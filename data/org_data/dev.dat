lower bounds for bmrm and faster rates for training svms
regularized risk minimization with the binary hinge loss and its variants lies at the heart of many machine learning problems . bundle methods for regularized risk minimization ( bmrm ) and the closely related svmstruct are considered the best general purpose solvers to tackle this problem . it was recently shown that bmrm requires $ o ( 1/\epsilon ) $ iterations to converge to an $ \epsilon $ accurate solution . in the first part of the paper we use the hadamard matrix to construct a regularized risk minimization problem and show that these rates can not be improved . we then show how one can exploit the structure of the objective function to devise an algorithm for the binary hinge loss which converges to an $ \epsilon $ accurate solution in $ o ( 1/\sqrt { \epsilon } ) $ iterations .

the information-collecting vehicle routing problem : stochastic optimization for emergency storm response
utilities face the challenge of responding to power outages due to storms and ice damage , but most power grids are not equipped with sensors to pinpoint the precise location of the faults causing the outage . instead , utilities have to depend primarily on phone calls ( trouble calls ) from customers who have lost power to guide the dispatching of utility trucks . in this paper , we develop a policy that routes a utility truck to restore outages in the power grid as quickly as possible , using phone calls to create beliefs about outages , but also using utility trucks as a mechanism for collecting additional information . this means that routing decisions change not only the physical state of the truck ( as it moves from one location to another ) and the grid ( as the truck performs repairs ) , but also our belief about the network , creating the first stochastic vehicle routing problem that explicitly models information collection and belief modeling . we address the problem of managing a single utility truck , which we start by formulating as a sequential stochastic optimization model which captures our belief about the state of the grid . we propose a stochastic lookahead policy , and use monte carlo tree search ( mcts ) to produce a practical policy that is asymptotically optimal . simulation results show that the developed policy restores the power grid much faster compared to standard industry heuristics .

contact state analysis using nfis and som
this paper reports application of neuro- fuzzy inference system ( nfis ) and self organizing feature map neural networks ( som ) on detection of contact state in a block system . in this manner , on a simple system , the evolution of contact states , by parallelization of dda , has been investigated . so , a comparison between nfis and som results has been presented . the results show applicability of the proposed methods , by different accuracy , on detection of contact 's distribution .

hybrid influence diagrams using mixtures of truncated exponentials
mixtures of truncated exponentials ( mte ) potentials are an alternative to discretization for representing continuous chance variables in influence diagrams . also , mte potentials can be used to approximate utility functions . this paper introduces mte influence diagrams , which can represent decision problems without restrictions on the relationships between continuous and discrete chance variables , without limitations on the distributions of continuous chance variables , and without limitations on the nature of the utility functions . in mte influence diagrams , all probability distributions and the joint utility function ( or its multiplicative factors ) are represented by mte potentials and decision nodes are assumed to have discrete state spaces . mte influence diagrams are solved by variable elimination using a fusion algorithm .

cognitive dynamic systems : a technical review of cognitive radar
we start with the history of cognitive radar , where origins of the pac , fuster research on cognition and principals of cognition are provided . fuster describes five cognitive functions : perception , memory , attention , language , and intelligence . we describe the perception-action cyclec as it applies to cognitive radar , and then discuss long-term memory , memory storage , memory retrieval and working memory . a comparison between memory in human cognition and cognitive radar is given as well . attention is another function described by fuster , and we have given the comparison of attention in human cognition and cognitive radar . we talk about the four functional blocks from the pac : bayesian filter , feedback information , dynamic programming and state-space model for the radar environment . then , to show that the pac improves the tracking accuracy of cognitive radar over traditional active radar , we have provided simulation results . in the simulation , three nonlinear filters : cubature kalman filter , unscented kalman filter and extended kalman filter are compared . based on the results , radars implemented with ckf perform better than the radars implemented with ukf or radars implemented with ekf . further , radar with ekf has the worst accuracy and has the biggest computation load because of derivation and evaluation of jacobian matrices . we suggest using the concept of risk management to better control parameters and improve performance in cognitive radar . we believe , spectrum sensing can be seen as a potential interest to be used in cognitive radar and we propose a new approach probabilistic ica which will presumably reduce noise based on estimation error in cognitive radar . parallel computing is a concept based on divide and conquers mechanism , and we suggest using the parallel computing approach in cognitive radar by doing complicated calculations or tasks to reduce processing time .

sub-optimal multi-phase path planning : a method for solving rubik 's revenge
rubik 's revenge , a 4x4x4 variant of the rubik 's puzzles , remains to date as an unsolved puzzle . that is to say , we do not have a method or successful categorization to optimally solve every one of its approximately $ 7.401 \times 10^ { 45 } $ possible configurations . rubik 's cube , rubik 's revenge 's predecessor ( 3x3x3 ) , with its approximately $ 4.33 \times 10^ { 19 } $ possible configurations , has only recently been completely solved by rokicki et . al , further finding that any configuration requires no more than 20 moves . with the sheer dimension of rubik 's revenge and its total configuration space , a brute-force method of finding all optimal solutions would be in vain . similar to the methods used by rokicki et . al on rubik 's cube , in this paper we develop a method for solving arbitrary configurations of rubik 's revenge in phases , using a combination of a powerful algorithm known as ida* and a useful definition of distance in the cube space . while time-series results were not successfully gathered , it will be shown that this method far outweighs current human-solving methods and can be used to determine loose upper bounds for the cube space . discussion will suggest that this method can also be applied to other puzzles with the proper transformations .

gazegan - unpaired adversarial image generation for gaze estimation
recent research has demonstrated the ability to estimate gaze on mobile devices by performing inference on the image from the phone 's front-facing camera , and without requiring specialized hardware . while this offers wide potential applications such as in human-computer interaction , medical diagnosis and accessibility ( e.g. , hands free gaze as input for patients with motor disorders ) , current methods are limited as they rely on collecting data from real users , which is a tedious and expensive process that is hard to scale across devices . there have been some attempts to synthesize eye region data using 3d models that can simulate various head poses and camera settings , however these lack in realism . in this paper , we improve upon a recently suggested method , and propose a generative adversarial framework to generate a large dataset of high resolution colorful images with high diversity ( e.g. , in subjects , head pose , camera settings ) and realism , while simultaneously preserving the accuracy of gaze labels . the proposed approach operates on extended regions of the eye , and even completes missing parts of the image . using this rich synthesized dataset , and without using any additional training data from real users , we demonstrate improvements over state-of-the-art for estimating 2d gaze position on mobile devices . we further demonstrate cross-device generalization of model performance , as well as improved robustness to diverse head pose , blur and distance .

discriminant projection representation-based classification for vision recognition
representation-based classification methods such as sparse representation-based classification ( src ) and linear regression classification ( lrc ) have attracted a lot of attentions . in order to obtain the better representation , a novel method called projection representation-based classification ( prc ) is proposed for image recognition in this paper . prc is based on a new mathematical model . this model denotes that the 'ideal projection ' of a sample point $ x $ on the hyper-space $ h $ may be gained by iteratively computing the projection of $ x $ on a line of hyper-space $ h $ with the proper strategy . therefore , prc is able to iteratively approximate the 'ideal representation ' of each subject for classification . moreover , the discriminant prc ( dprc ) is further proposed , which obtains the discriminant information by maximizing the ratio of the between-class reconstruction error over the within-class reconstruction error . experimental results on five typical databases show that the proposed prc and dprc are effective and outperform other state-of-the-art methods on several vision recognition tasks .

cognitive surveillance : why does it never appear among the avss conferences topics ?
video surveillance is a fast evolving field of research and development ( r & d ) driven by the urgent need for public security and safety ( due to the growing threats of terrorism , vandalism , and anti-social behavior ) . traditionally , surveillance systems are comprised of two components - video cameras distributed over the guarded area and human observer watching and analyzing the incoming video . explosive growth of installed cameras and limited human operator 's ability to process the delivered video content raise an urgent demand for developing surveillance systems with human like cognitive capabilities , that is - cognitive surveillance systems . the growing interest in this issue is testified by the tens of workshops , symposiums and conferences held over the world each year . the ieee international conference on advanced video and signal-based surveillance ( avss ) is certainly one of them . however , for unknown reasons , the term cognitive surveillance does never appear among its topics . as to me , the explanation for this is simple - the complexity and the indefinable nature of the term `` cognition '' . in this paper , i am trying to resolve the problem providing a novel definition of cognition equally suitable for biological as well as technological applications . i hope my humble efforts will be helpful .

combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning
deep learning has achieved impressive results on many problems . however , it requires high degree of expertise or a lot of experience to tune well the hyperparameters , and such manual tuning process is likely to be biased . moreover , it is not practical to try out as many different hyperparameter configurations in deep learning as in other machine learning scenarios , because evaluating each single hyperparameter configuration in deep learning would mean training a deep neural network , which usually takes quite long time . hyperband algorithm achieves state-of-the-art performance on various hyperparameter optimization problems in the field of deep learning . however , hyperband algorithm does not utilize history information of previous explored hyperparameter configurations , thus the solution found is suboptimal . we propose to combine hyperband algorithm with bayesian optimization ( which does not ignore history when sampling next trial configuration ) . experimental results show that our combination approach is superior to other hyperparameter optimization approaches including hyperband algorithm .

on reasoning with rdf statements about statements using singleton property triples
the singleton property ( sp ) approach has been proposed for representing and querying metadata about rdf triples such as provenance , time , location , and evidence . in this approach , one singleton property is created to uniquely represent a relationship in a particular context , and in general , generates a large property hierarchy in the schema . it has become the subject of important questions from semantic web practitioners . can an existing reasoner recognize the singleton property triples ? and how ? if the singleton property triples describe a data triple , then how can a reasoner infer this data triple from the singleton property triples ? or would the large property hierarchy affect the reasoners in some way ? we address these questions in this paper and present our study about the reasoning aspects of the singleton properties . we propose a simple mechanism to enable existing reasoners to recognize the singleton property triples , as well as to infer the data triples described by the singleton property triples . we evaluate the effect of the singleton property triples in the reasoning processes by comparing the performance on rdf datasets with and without singleton properties . our evaluation uses as benchmark the lubm datasets and the lubm-sp datasets derived from lubm with temporal information added through singleton properties .

detecting and correcting for label shift with black box predictors
faced with distribution shift between training and test set , we wish to detect and quantify the shift , and to correct our classifiers without test set labels . motivated by medical diagnosis , where diseases ( targets ) , cause symptoms ( observations ) , we focus on label shift , where the label marginal $ p ( y ) $ changes but the conditional $ p ( x|y ) $ does not . we propose black box shift estimation ( bbse ) to estimate the test distribution $ p ( y ) $ . bbse exploits arbitrary black box predictors to reduce dimensionality prior to shift correction . while better predictors give tighter estimates , bbse works even when predictors are biased , inaccurate , or uncalibrated , so long as their confusion matrices are invertible . we prove bbse 's consistency , bound its error , and introduce a statistical test that uses bbse to detect shift . we also leverage bbse to correct classifiers . experiments demonstrate accurate estimates and improved prediction , even on high-dimensional datasets of natural images .

clause/term resolution and learning in the evaluation of quantified boolean formulas
resolution is the rule of inference at the basis of most procedures for automated reasoning . in these procedures , the input formula is first translated into an equisatisfiable formula in conjunctive normal form ( cnf ) and then represented as a set of clauses . deduction starts by inferring new clauses by resolution , and goes on until the empty clause is generated or satisfiability of the set of clauses is proven , e.g. , because no new clauses can be generated . in this paper , we restrict our attention to the problem of evaluating quantified boolean formulas ( qbfs ) . in this setting , the above outlined deduction process is known to be sound and complete if given a formula in cnf and if a form of resolution , called q-resolution , is used . we introduce q-resolution on terms , to be used for formulas in disjunctive normal form . we show that the computation performed by most of the available procedures for qbfs -- based on the davis-logemann-loveland procedure ( dll ) for propositional satisfiability -- corresponds to a tree in which q-resolution on terms and clauses alternate . this poses the theoretical bases for the introduction of learning , corresponding to recording q-resolution formulas associated with the nodes of the tree . we discuss the problems related to the introduction of learning in dll based procedures , and present solutions extending state-of-the-art proposals coming from the literature on propositional satisfiability . finally , we show that our dll based solver extended with learning , performs significantly better on benchmarks used in the 2003 qbf solvers comparative evaluation .

quantifying natural and artificial intelligence in robots and natural systems with an algorithmic behavioural test
one of the most important aims of the fields of robotics , artificial intelligence and artificial life is the design and construction of systems and machines as versatile and as reliable as living organisms at performing high level human-like tasks . but how are we to evaluate artificial systems if we are not certain how to measure these capacities in living systems , let alone how to define life or intelligence ? here i survey a concrete metric towards measuring abstract properties of natural and artificial systems , such as the ability to react to the environment and to control one 's own behaviour .

quadratic unconstrained binary optimization problem preprocessing : theory and empirical analysis
the quadratic unconstrained binary optimization problem ( qubo ) has become a unifying model for representing a wide range of combinatorial optimization problems , and for linking a variety of disciplines that face these problems . a new class of quantum annealing computer that maps qubo onto a physical qubit network structure with specific size and edge density restrictions is generating a growing interest in ways to transform the underlying qubo structure into an equivalent graph having fewer nodes and edges . in this paper we present rules for reducing the size of the qubo matrix by identifying variables whose value at optimality can be predetermined . we verify that the reductions improve both solution quality and time to solution and , in the case of metaheuristic methods where optimal solutions can not be guaranteed , the quality of solutions obtained within reasonable time limits . we discuss the general qubo structural characteristics that can take advantage of these reduction techniques and perform careful experimental design and analysis to identify and quantify the specific characteristics most affecting reduction . the rules make it possible to dramatically improve solution times on a new set of problems using both the exact cplex solver and a tabu search metaheuristic .

learning to factor policies and action-value functions : factored action space representations for deep reinforcement learning
deep reinforcement learning ( drl ) methods have performed well in an increasing numbering of high-dimensional visual decision making domains . among all such visual decision making problems , those with discrete action spaces often tend to have underlying compositional structure in the said action space . such action spaces often contain actions such as go left , go up as well as go diagonally up and left ( which is a composition of the former two actions ) . the representations of control policies in such domains have traditionally been modeled without exploiting this inherent compositional structure in the action spaces . we propose a new learning paradigm , factored action space representations ( far ) wherein we decompose a control policy learned using a deep reinforcement learning algorithm into independent components , analogous to decomposing a vector in terms of some orthogonal basis vectors . this architectural modification of the control policy representation allows the agent to learn about multiple actions simultaneously , while executing only one of them . we demonstrate that far yields considerable improvements on top of two drl algorithms in atari 2600 : fara3c outperforms a3c ( asynchronous advantage actor critic ) in 9 out of 14 tasks and faraql outperforms aql ( asynchronous n-step q-learning ) in 9 out of 13 tasks .

compiling relational database schemata into probabilistic graphical models
instead of requiring a domain expert to specify the probabilistic dependencies of the data , in this work we present an approach that uses the relational db schema to automatically construct a bayesian graphical model for a database . this resulting model contains customized distributions for columns , latent variables that cluster the data , and factors that reflect and represent the foreign key links . experiments demonstrate the accuracy of the model and the scalability of inference on synthetic and real-world data .

truth maintenance under uncertainty
this paper addresses the problem of resolving errors under uncertainty in a rule-based system . a new approach has been developed that reformulates this problem as a neural-network learning problem . the strength and the fundamental limitations of this approach are explored and discussed . the main result is that neural heuristics can be applied to solve some but not all problems in rule-based systems .

restart strategy selection using machine learning techniques
restart strategies are an important factor in the performance of conflict-driven davis putnam style sat solvers . selecting a good restart strategy for a problem instance can enhance the performance of a solver . inspired by recent success applying machine learning techniques to predict the runtime of sat solvers , we present a method which uses machine learning to boost solver performance through a smart selection of the restart strategy . based on easy to compute features , we train both a satisfiability classifier and runtime models . we use these models to choose between restart strategies . we present experimental results comparing this technique with the most commonly used restart strategies . our results demonstrate that machine learning is effective in improving solver performance .

a convex formulation for learning task relationships in multi-task learning
multi-task learning is a learning paradigm which seeks to improve the generalization performance of a learning task with the help of some other related tasks . in this paper , we propose a regularization formulation for learning the relationships between tasks in multi-task learning . this formulation can be viewed as a novel generalization of the regularization framework for single-task learning . besides modeling positive task correlation , our method , called multi-task relationship learning ( mtrl ) , can also describe negative task correlation and identify outlier tasks based on the same underlying principle . under this regularization framework , the objective function of mtrl is convex . for efficiency , we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks . we study mtrl in the symmetric multi-task learning setting and then generalize it to the asymmetric setting as well . we also study the relationships between mtrl and some existing multi-task learning methods . experiments conducted on a toy problem as well as several benchmark data sets demonstrate the effectiveness of mtrl .

classification accuracy as a proxy for two sample testing
when data analysts train a classifier and check if its accuracy is significantly different from random guessing , they are implicitly and indirectly performing a hypothesis test ( two sample testing ) and it is of importance to ask whether this indirect method for testing is statistically optimal or not . given that hypothesis tests attempt to maximize statistical power subject to a bound on the allowable false positive rate , while prediction attempts to minimize statistical risk on future predictions on unseen data , we wish to study whether a predictive approach for an ultimate aim of testing is prudent . we formalize this problem by considering the two-sample mean-testing setting where one must determine if the means of two gaussians ( with known and equal covariance ) are the same or not , but the analyst indirectly does so by checking whether the accuracy achieved by fisher 's lda classifier is significantly different from chance or not . unexpectedly , we find that the asymptotic power of lda 's sample-splitting classification accuracy is actually minimax rate-optimal in terms of problem-dependent parameters . since prediction is commonly thought to be harder than testing , it might come as a surprise to some that solving a harder problem does not create a information-theoretic bottleneck for the easier one . on the flip side , even though the power is rate-optimal , our derivation suggests that it may be worse by a small constant factor ; hence practitioners must be wary of using ( admittedly flexible ) prediction methods on disguised testing problems .

fuzzy logic in narrow sense with hedges
classical logic has a serious limitation in that it can not cope with the issues of vagueness and uncertainty into which fall most modes of human reasoning . in order to provide a foundation for human knowledge representation and reasoning in the presence of vagueness , imprecision , and uncertainty , fuzzy logic should have the ability to deal with linguistic hedges , which play a very important role in the modification of fuzzy predicates . in this paper , we extend fuzzy logic in narrow sense with graded syntax , introduced by novak et al. , with many hedge connectives . in one case , each hedge does not have any dual one . in the other case , each hedge can have its own dual one . the resulting logics are shown to also have the pavelka-style completeness

experts fusion and multilayer perceptron based on belief learning for sonar image classification
the sonar images provide a rapid view of the seabed in order to characterize it . however , in such as uncertain environment , real seabed is unknown and the only information we can obtain , is the interpretation of different human experts , sometimes in conflict . in this paper , we propose to manage this conflict in order to provide a robust reality for the learning step of classification algorithms . the classification is conducted by a multilayer perceptron , taking into account the uncertainty of the reality in the learning stage . the results of this seabed characterization are presented on real sonar images .

coins and logic
we establish fun parallels between coin-weighing puzzles and knights-and-knaves puzzles .

simulated annealing for weighted polygon packing
in this paper we present a new algorithm for a layout optimization problem : this concerns the placement of weighted polygons inside a circular container , the two objectives being to minimize imbalance of mass and to minimize the radius of the container . this problem carries real practical significance in industrial applications ( such as the design of satellites ) , as well as being of significant theoretical interest . previous work has dealt with circular or rectangular objects , but here we deal with the more realistic case where objects may be represented as polygons and the polygons are allowed to rotate . we present a solution based on simulated annealing and first test it on instances with known optima . our results show that the algorithm obtains container radii that are close to optimal . we also compare our method with existing algorithms for the ( special ) rectangular case . experimental results show that our approach out-performs these methods in terms of solution quality .

phase transitions of plan modification in conformant planning
we explore phase transitions of plan modification , which mainly focus on the conformant planning problems . by analyzing features of plan modification in conformant planning problems , quantitative results are obtained . if the number of operators is less than , almost all conformant planning problems ca n't be solved with plan modification . if the number of operators is more than , almost all conformant planning problems can be solved with plan modification . the results of the experiments also show that there exists an experimental threshold of density ( ratio of number of operators to number of propositions ) , which separates the region where almost all conformant planning problems ca n't be solved with plan modification from the region where almost all conformant planning problems can be solved with plan modification .

lazy evaluation of symmetric bayesian decision problems
solving symmetric bayesian decision problems is a computationally intensive task to perform regardless of the algorithm used . in this paper we propose a method for improving the efficiency of algorithms for solving bayesian decision problems . the method is based on the principle of lazy evaluation - a principle recently shown to improve the efficiency of inference in bayesian networks . the basic idea is to maintain decompositions of potentials and to postpone computations for as long as possible . the efficiency improvements obtained with the lazy evaluation based method is emphasized through examples . finally , the lazy evaluation based method is compared with the hugin and valuation-based systems architectures for solving symmetric bayesian decision problems .

toward a robust diversity-based model to detect changes of context
being able to automatically and quickly understand the user context during a session is a main issue for recommender systems . as a first step toward achieving that goal , we propose a model that observes in real time the diversity brought by each item relatively to a short sequence of consultations , corresponding to the recent user history . our model has a complexity in constant time , and is generic since it can apply to any type of items within an online service ( e.g . profiles , products , music tracks ) and any application domain ( e-commerce , social network , music streaming ) , as long as we have partial item descriptions . the observation of the diversity level over time allows us to detect implicit changes . in the long term , we plan to characterize the context , i.e . to find common features among a contiguous sub-sequence of items between two changes of context determined by our model . this will allow us to make context-aware and privacy-preserving recommendations , to explain them to users . as this is an ongoing research , the first step consists here in studying the robustness of our model while detecting changes of context . in order to do so , we use a music corpus of 100 users and more than 210,000 consultations ( number of songs played in the global history ) . we validate the relevancy of our detections by finding connections between changes of context and events , such as ends of session . of course , these events are a subset of the possible changes of context , since there might be several contexts within a session . we altered the quality of our corpus in several manners , so as to test the performances of our model when confronted with sparsity and different types of items . the results show that our model is robust and constitutes a promising approach .

causal discovery in the presence of measurement error : identifiability conditions
measurement error in the observed values of the variables can greatly change the output of various causal discovery methods . this problem has received much attention in multiple fields , but it is not clear to what extent the causal model for the measurement-error-free variables can be identified in the presence of measurement error with unknown variance . in this paper , we study precise sufficient identifiability conditions for the measurement-error-free causal model and show what information of the causal model can be recovered from observed data . in particular , we present two different sets of identifiability conditions , based on the second-order statistics and higher-order statistics of the data , respectively . the former was inspired by the relationship between the generating model of the measurement-error-contaminated data and the factor analysis model , and the latter makes use of the identifiability result of the over-complete independent component analysis problem .

emphatic temporal-difference learning
emphatic algorithms are temporal-difference learning algorithms that change their effective state distribution by selectively emphasizing and de-emphasizing their updates on different time steps . recent works by sutton , mahmood and white ( 2015 ) , and yu ( 2015 ) show that by varying the emphasis in a particular way , these algorithms become stable and convergent under off-policy training with linear function approximation . this paper serves as a unified summary of the available results from both works . in addition , we demonstrate the empirical benefits from the flexibility of emphatic algorithms , including state-dependent discounting , state-dependent bootstrapping , and the user-specified allocation of function approximation resources .

deep face deblurring
blind deblurring consists a long studied task , however the outcomes of generic methods are not effective in real world blurred images . domain-specific methods for deblurring targeted object categories , e.g . text or faces , frequently outperform their generic counterparts , hence they are attracting an increasing amount of attention . in this work , we develop such a domain-specific method to tackle deblurring of human faces , henceforth referred to as face deblurring . studying faces is of tremendous significance in computer vision , however face deblurring has yet to demonstrate some convincing results . this can be partly attributed to the combination of i ) poor texture and ii ) highly structure shape that yield the contour/gradient priors ( that are typically used ) sub-optimal . in our work instead of making assumptions over the prior , we adopt a learning approach by inserting weak supervision that exploits the well-documented structure of the face . namely , we utilise a deep network to perform the deblurring and employ a face alignment technique to pre-process each face . we additionally surpass the requirement of the deep network for thousands training samples , by introducing an efficient framework that allows the generation of a large dataset . we utilised this framework to create 2mf2 , a dataset of over two million frames . we conducted experiments with real world blurred facial images and report that our method returns a result close to the sharp natural latent image .

extended graded modalities in strategy logic
strategy logic ( sl ) is a logical formalism for strategic reasoning in multi-agent systems . its main feature is that it has variables for strategies that are associated to specific agents with a binding operator . we introduce graded strategy logic ( gradedsl ) , an extension of sl by graded quantifiers over tuples of strategy variables , i.e. , `` there exist at least g different tuples ( x_1 , ... , x_n ) of strategies '' where g is a cardinal from the set n union { aleph_0 , aleph_1 , 2^aleph_0 } . we prove that the model-checking problem of gradedsl is decidable . we then turn to the complexity of fragments of gradedsl . when the g 's are restricted to finite cardinals , written gradednsl , the complexity of model-checking is no harder than for sl , i.e. , it is non-elementary in the quantifier rank . we illustrate our formalism by showing how to count the number of different strategy profiles that are nash equilibria ( ne ) , or subgame-perfect equilibria ( spe ) . by analyzing the structure of the specific formulas involved , we conclude that the important problems of checking for the existence of a unique ne or spe can both be solved in 2exptime , which is not harder than merely checking for the existence of such equilibria .

assisting drivers during overtaking using car-2-car communication and multi-agent systems
a warning system for assisting drivers during overtaking maneuvers is proposed . the system relies on car-2-car communication technologies and multi-agent systems . a protocol for safety overtaking is proposed based on acl communicative acts . the mathematical model for safety overtaking used kalman filter to minimize localization error .

automation of android applications testing using machine learning activities classification
mobile applications are being used every day by more than half of the world 's population to perform a great variety of tasks . with the increasingly widespread usage of these applications , the need arises for efficient techniques to test them . many frameworks allow automating the process of application testing , however existing frameworks mainly rely on the application developer for providing testing scripts for each developed application , thus preventing reuse of these tests for similar applications . in this paper , we present a novel approach for the automation of testing android applications by leveraging machine learning techniques and reusing popular test scenarios . we discuss and demonstrate the potential benefits of our approach in an empirical study where we show that our developed testing tool , based on the proposed approach , outperforms standard methods in realistic settings .

online speedup learning for optimal planning
domain-independent planning is one of the foundational areas in the field of artificial intelligence . a description of a planning task consists of an initial world state , a goal , and a set of actions for modifying the world state . the objective is to find a sequence of actions , that is , a plan , that transforms the initial world state into a goal state . in optimal planning , we are interested in finding not just a plan , but one of the cheapest plans . a prominent approach to optimal planning these days is heuristic state-space search , guided by admissible heuristic functions . numerous admissible heuristics have been developed , each with its own strengths and weaknesses , and it is well known that there is no single `` best heuristic for optimal planning in general . thus , which heuristic to choose for a given planning task is a difficult question . this difficulty can be avoided by combining several heuristics , but that requires computing numerous heuristic estimates at each state , and the tradeoff between the time spent doing so and the time saved by the combined advantages of the different heuristics might be high . we present a novel method that reduces the cost of combining admissible heuristics for optimal planning , while maintaining its benefits . using an idealized search space model , we formulate a decision rule for choosing the best heuristic to compute at each state . we then present an active online learning approach for learning a classifier with that decision rule as the target concept , and employ the learned classifier to decide which heuristic to compute at each state . we evaluate this technique empirically , and show that it substantially outperforms the standard method for combining several heuristics via their pointwise maximum .

belief revision by examples
a common assumption in belief revision is that the reliability of the information sources is either given , derived from temporal information , or the same for all . this article does not describe a new semantics for integration but the problem of obtaining the reliability of the sources given the result of a previous merging . as an example , the relative reliability of two sensors can be assessed given some certain observation , and allows for subsequent mergings of data coming from them .

sroiqsigma is decidable
we consider a dynamic extension of the description logic $ \mathcal { sroiq } $ . this means that interpretations could evolve thanks to some actions such as addition and/or deletion of an element ( respectively , a pair of elements ) of a concept ( respectively , of a role ) . the obtained logic is called $ \mathcal { sroiq } $ with explicit substitutions and is written $ \mathcal { sroiq^\sigma } $ . substitution is not treated as meta-operation that is carried out immediately , but the operation of substitution may be delayed , so that sub-formulae of $ \mathcal { sroiq } ^\sigma $ are of the form $ \phi\sigma $ , where $ \phi $ is a $ \mathcal { sroiq } $ formula and $ \sigma $ is a substitution which encodes changes of concepts and roles . in this paper , we particularly prove that the satisfiability problem of $ \mathcal { sroiq } ^\sigma $ is decidable .

micro-interventions in urban transport from pattern discovery on the flow of passengers and on the bus network
in this paper , we describe a case study in a big metropolis , in which from data collected by digital sensors , we tried to understand mobility patterns of persons using buses and how this can generate knowledge to suggest interventions that are applied incrementally into the transportation network in use . we have first estimated an origin-destination matrix of buses users from datasets about the ticket validation and gps positioning of buses . then we represent the supply of buses with their routes through bus stops as a complex network , which allowed us to understand the bottlenecks of the current scenario and , in particular , applying community discovery techniques , to identify clusters that the service supply infrastructure has . finally , from the superimposing of the flow of people represented in the origindestination matrix in the supply network , we exemplify how micro-interventions can be prospected by means of an example of the introduction of express routes .

bandit-based model selection for deformable object manipulation
we present a novel approach to deformable object manipulation that does not rely on highly-accurate modeling . the key contribution of this paper is to formulate the task as a multi-armed bandit problem , with each arm representing a model of the deformable object . to `` pull '' an arm and evaluate its utility , we use the arm 's model to generate a velocity command for the gripper ( s ) holding the object and execute it . as the task proceeds and the object deforms , the utility of each model can change . our framework estimates these changes and balances exploration of the model set with exploitation of high-utility models . we also propose an approach based on kalman filtering for non-stationary multi-armed normal bandits ( kf-manb ) to leverage the coupling between models to learn more from each arm pull . we demonstrate that our method outperforms previous methods on synthetic trials , and performs competitively on several manipulation tasks in simulation .

evidential relational clustering using medoids
in real clustering applications , proximity data , in which only pairwise similarities or dissimilarities are known , is more general than object data , in which each pattern is described explicitly by a list of attributes . medoid-based clustering algorithms , which assume the prototypes of classes are objects , are of great value for partitioning relational data sets . in this paper a new prototype-based clustering method , named evidential c-medoids ( ecmdd ) , which is an extension of fuzzy c-medoids ( fcmdd ) on the theoretical framework of belief functions is proposed . in ecmdd , medoids are utilized as the prototypes to represent the detected classes , including specific classes and imprecise classes . specific classes are for the data which are distinctly far from the prototypes of other classes , while imprecise classes accept the objects that may be close to the prototypes of more than one class . this soft decision mechanism could make the clustering results more cautious and reduce the misclassification rates . experiments in synthetic and real data sets are used to illustrate the performance of ecmdd . the results show that ecmdd could capture well the uncertainty in the internal data structure . moreover , it is more robust to the initializations compared with fcmdd .

autonomous quadrotor landing using deep reinforcement learning
landing an unmanned aerial vehicle ( uav ) on a ground marker is an open problem despite the effort of the research community . previous attempts mostly focused on the analysis of hand-crafted geometric features and the use of external sensors in order to allow the vehicle to approach the land-pad . in this article , we propose a method based on deep reinforcement learning that only requires low-resolution images taken from a down-looking camera in order to identify the position of the marker and land the uav on it . the proposed approach is based on a hierarchy of deep q-networks ( dqns ) used as high-level control policy for the navigation toward the marker . we implemented different technical solutions , such as the combination of vanilla and double dqns , and a partitioned buffer replay . using domain randomization we trained the vehicle on uniform textures and we tested it on a large variety of simulated and real-world environments . the overall performance is comparable with a state-of-the-art algorithm and human pilots .

time-dependent utility and action under uncertainty
we discuss representing and reasoning with knowledge about the time-dependent utility of an agent 's actions . time-dependent utility plays a crucial role in the interaction between computation and action under bounded resources . we present a semantics for time-dependent utility and describe the use of time-dependent information in decision contexts . we illustrate our discussion with examples of time-pressured reasoning in protos , a system constructed to explore the ideal control of inference by reasoners with limit abilities .

user intent classification using memory networks : a comparative analysis for a limited data scenario
in this report , we provide a comparative analysis of different techniques for user intent classification towards the task of app recommendation . we analyse the performance of different models and architectures for multi-label classification over a dataset with a relative large number of classes and only a handful examples of each class . we focus , in particular , on memory network architectures , and compare how well the different versions perform under the task constraints . since the classifier is meant to serve as a module in a practical dialog system , it needs to be able to work with limited training data and incorporate new data on the fly . we devise a 1-shot learning task to test the models under the above constraint . we conclude that relatively simple versions of memory networks perform better than other approaches . although , for tasks with very limited data , simple non-parametric methods perform comparably , without needing the extra training data .

expert gate : lifelong learning with a network of experts
in this paper we introduce a model of lifelong learning , based on a network of experts . new tasks / experts are learned and added to the model sequentially , building on what was learned before . to ensure scalability of this process , data from previous tasks can not be stored and hence is not available when learning a new task . a critical issue in such context , not addressed in the literature so far , relates to the decision which expert to deploy at test time . we introduce a set of gating autoencoders that learn a representation for the task at hand , and , at test time , automatically forward the test sample to the relevant expert . this also brings memory efficiency as only one expert network has to be loaded into memory at any given time . further , the autoencoders inherently capture the relatedness of one task to another , based on which the most relevant prior model to be used for training a new expert , with finetuning or learning without-forgetting , can be selected . we evaluate our method on image classification and video prediction problems .

alime assist : an intelligent assistant for creating an innovative e-commerce experience
we present alime assist , an intelligent assistant designed for creating an innovative online shopping experience in e-commerce . based on question answering ( qa ) , alime assist offers assistance service , customer service , and chatting service . it is able to take voice and text input , incorporate context to qa , and support multi-round interaction . currently , it serves millions of customer questions per day and is able to address 85 % of them . in this paper , we demonstrate the system , present the underlying techniques , and share our experience in dealing with real-world qa in the e-commerce field .

asynchronous decentralized algorithm for space-time cooperative pathfinding
cooperative pathfinding is a multi-agent path planning problem where a group of vehicles searches for a corresponding set of non-conflicting space-time trajectories . many of the practical methods for centralized solving of cooperative pathfinding problems are based on the prioritized planning strategy . however , in some domains ( e.g. , multi-robot teams of unmanned aerial vehicles , autonomous underwater vehicles , or unmanned ground vehicles ) a decentralized approach may be more desirable than a centralized one due to communication limitations imposed by the domain and/or privacy concerns . in this paper we present an asynchronous decentralized variant of prioritized planning adpp and its interruptible version iadpp . the algorithm exploits the inherent parallelism of distributed systems and allows for a speed up of the computation process . unlike the synchronized planning approaches , the algorithm allows an agent to react to updates about other agents ' paths immediately and invoke its local spatio-temporal path planner to find the best trajectory , as response to the other agents ' choices . we provide a proof of correctness of the algorithms and experimentally evaluate them on synthetic domains .

convolutional neural associative memories : massive capacity with noise tolerance
the task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions using a network of neurons . an ideal network should have the ability to 1 ) learn a set of patterns as they arrive , 2 ) retrieve the correct patterns from noisy queries , and 3 ) maximize the pattern retrieval capacity while maintaining the reliability in responding to queries . the majority of work on neural associative memories has focused on designing networks capable of memorizing any set of randomly chosen patterns at the expense of limiting the retrieval capacity . in this paper , we show that if we target memorizing only those patterns that have inherent redundancy ( i.e. , belong to a subspace ) , we can obtain all the aforementioned properties . this is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third . more specifically , we propose framework based on a convolutional neural network along with an iterative algorithm that learns the redundancy among the patterns . the resulting network has a retrieval capacity that is exponential in the size of the network . moreover , the asymptotic error correction performance of our network is linear in the size of the patterns . we then ex- tend our approach to deal with patterns lie approximately in a subspace . this extension allows us to memorize datasets containing natural patterns ( e.g. , images ) . finally , we report experimental results on both synthetic and real datasets to support our claims .

theory refinement on bayesian networks
theory refinement is the task of updating a domain theory in the light of new cases , to be done automatically or with some expert assistance . the problem of theory refinement under uncertainty is reviewed here in the context of bayesian statistics , a theory of belief revision . the problem is reduced to an incremental learning task as follows : the learning system is initially primed with a partial theory supplied by a domain expert , and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data . algorithms for refinement of bayesian networks are presented to illustrate what is meant by `` partial theory '' , `` alternative theory representation '' , etc . the algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode .

driven to distraction : self-supervised distractor learning for robust monocular visual odometry in urban environments
we present a self-supervised approach to ignoring `` distractors '' in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments . we leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image , which we use to train a deep convolutional network . at run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry ( vo ) pipeline , using either sparse features or dense photometric matching . our approach yields metric-scale vo using only a single camera and can recover the correct egomotion even when 90 % of the image is obscured by dynamic , independently moving objects . we evaluate our robust vo methods on more than 400km of driving from the oxford robotcar dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic .

social network processes in the isabelle and coq theorem proving communities
we identify the main actors in the isabelle and coq communities and describe how they affect and influence their peers . this work explores selected foundations of social networking analysis that we expect to be useful in the context of the proofpeer project , which is developing a new model for interactive theorem proving based on collaboration and social interactions .

a review and comparison of strategies for multi-step ahead time series forecasting based on the nn5 forecasting competition
multi-step ahead forecasting is still an open challenge in time series forecasting . several approaches that deal with this complex problem have been proposed in the literature but an extensive comparison on a large number of tasks is still missing . this paper aims to fill this gap by reviewing existing strategies for multi-step ahead forecasting and comparing them in theoretical and practical terms . to attain such an objective , we performed a large scale comparison of these different strategies using a large experimental benchmark ( namely the 111 series from the nn5 forecasting competition ) . in addition , we considered the effects of deseasonalization , input variable selection , and forecast combination on these strategies and on multi-step ahead forecasting at large . the following three findings appear to be consistently supported by the experimental results : multiple-output strategies are the best performing approaches , deseasonalization leads to uniformly improved forecast accuracy , and input selection is more effective when performed in conjunction with deseasonalization .

opinion mining for relating subjective expressions and annual earnings in us financial statements
financial statements contain quantitative information and manager 's subjective evaluation of firm 's financial status . using information released in u.s. 10-k filings . both qualitative and quantitative appraisals are crucial for quality financial decisions . to extract such opinioned statements from the reports , we built tagging models based on the conditional random field ( crf ) techniques , considering a variety of combinations of linguistic factors including morphology , orthography , predicate-argument structure , syntax , and simple semantics . our results show that the crf models are reasonably effective to find opinion holders in experiments when we adopted the popular mpqa corpus for training and testing . the contribution of our paper is to identify opinion patterns in multiword expressions ( mwes ) forms rather than in single word forms . we find that the managers of corporations attempt to use more optimistic words to obfuscate negative financial performance and to accentuate the positive financial performance . our results also show that decreasing earnings were often accompanied by ambiguous and mild statements in the reporting year and that increasing earnings were stated in assertive and positive way .

implicit robot-human communication in adversarial and collaborative environments
users of ai systems may rely upon them to produce plans for achieving desired objectives . such ai systems should be able to compute obfuscated plans whose execution in adversarial situations protects privacy as well as legible plans which are easy for team-members to understand in collaborative situations . we develop a unified framework that addresses these dual problems by computing plans with a desired level of comprehensibility from the point of view of a partially informed observer . our approach produces obfuscated plans with observations that are consistent with at least 'k ' goals from a given set of decoy goals . in addition , when the goal is known to the observer , our approach generates obfuscated plans with observations that are diverse with at least 'l ' candidate plans . our approach for plan legibility produces plans that achieve a goal while being consistent with at most 'j ' goals in a given set of confounding goals . we provide an empirical evaluation to show the feasibility and usefulness of our approaches .

compact value-function representations for qualitative preferences
we consider the challenge of preference elicitation in systems that help users discover the most desirable item ( s ) within a given database . past work on preference elicitation focused on structured models that provide a factored representation of users ' preferences . such models require less information to construct and support efficient reasoning algorithms . this paper makes two substantial contributions to this area : ( 1 ) strong representation theorems for factored value functions . ( 2 ) a methodology that utilizes our representation results to address the problem of optimal item selection .

a roadmap towards machine intelligence
the development of intelligent machines is one of the biggest unsolved challenges in computer science . in this paper , we propose some fundamental properties these machines should have , focusing in particular on communication and learning . we discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication , as a prerequisite to more complex interaction with human users . we also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment .

a cure for pathological behavior in games that use minimax
the traditional approach to choosing moves in game-playing programs is the minimax procedure . the general belief underlying its use is that increasing search depth improves play . recent research has shown that given certain simplifying assumptions about a game tree 's structure , this belief is erroneous : searching deeper decreases the probability of making a correct move . this phenomenon is called game tree pathology . among these simplifying assumptions is uniform depth of win/loss ( terminal ) nodes , a condition which is not true for most real games . analytic studies in [ 10 ] have shown that if every node in a pathological game tree is made terminal with probability exceeding a certain threshold , the resulting tree is nonpathological . this paper considers a new evaluation function which recognizes increasing densities of forced wins at deeper levels in the tree . this property raises two points that strengthen the hypothesis that uniform win depth causes pathology . first , it proves mathematically that as search deepens , an evaluation function that does not explicitly check for certain forced win patterns becomes decreasingly likely to force wins . this failing predicts the pathological behavior of the original evaluation function . second , it shows empirically that despite recognizing fewer mid-game wins than the theoretically predicted minimum , the new function is nonpathological .

an epsilon hierarchical fuzzy twin support vector regression
the research presents epsilon hierarchical fuzzy twin support vector regression based on epsilon fuzzy twin support vector regression and epsilon twin support vector regression . epsilon ftsvr is achieved by incorporating trapezoidal fuzzy numbers to epsilon tsvr which takes care of uncertainty existing in forecasting problems . epsilon ftsvr determines a pair of epsilon insensitive proximal functions by solving two related quadratic programming problems . the structural risk minimization principle is implemented by introducing regularization term in primal problems of epsilon ftsvr . this yields dual stable positive definite problems which improves regression performance . epsilon ftsvr is then reformulated as epsilon hftsvr consisting of a set of hierarchical layers each containing epsilon ftsvr . experimental results on both synthetic and real datasets reveal that epsilon hftsvr has remarkable generalization performance with minimum training time .

exploiting evidence-dependent sensitivity bounds
studying the effects of one-way variation of any number of parameters on any number of output probabilities quickly becomes infeasible in practice , especially if various evidence profiles are to be taken into consideration . to provide for identifying the parameters that have a potentially large effect prior to actually performing the analysis , we need properties of sensitivity functions that are independent of the network under study , of the available evidence , or of both . in this paper , we study properties that depend upon just the probability of the entered evidence . we demonstrate that these properties provide for establishing an upper bound on the sensitivity value for a parameter ; they further provide for establishing the region in which the vertex of the sensitivity function resides , thereby serving to identify parameters with a low sensitivity value that may still have a large impact on the probability of interest for relatively small parameter variations .

learning structural weight uncertainty for sequential decision-making
learning probability distributions on the weights of neural networks ( nns ) has recently proven beneficial in many applications . bayesian methods , such as stein variational gradient descent ( svgd ) , offer an elegant framework to reason about nn model uncertainty . however , by assuming independent gaussian priors for the individual nn weights ( as often applied ) , svgd does not impose prior knowledge that there is often structural information ( dependence ) among weights . we propose efficient posterior learning of structural weight uncertainty , within an svgd framework , by employing matrix variate gaussian priors on nn parameters . we further investigate the learned structural uncertainty in sequential decision-making problems , including contextual bandits and reinforcement learning . experiments on several synthetic and real datasets indicate the superiority of our model , compared with state-of-the-art methods .

a unit selection methodology for music generation using deep neural networks
several methods exist for a computer to generate music based on data including markov chains , recurrent neural networks , recombinancy , and grammars . we explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking , where , we consider a unit to be a variable length number of measures of music . we first examine whether a unit selection method , that is restricted to a finite size unit library , can be sufficient for encompassing a wide spectrum of music . we do this by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library . we then describe a generative model that combines a deep structured semantic model ( dssm ) with an lstm to predict the next unit , where units consist of four , two , and one measures of music . we evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task . we compare our model to a note-level generative baseline that consists of a stacked lstm trained to predict forward by one note .

predicting performance during tutoring with models of recent performance
in educational technology and learning sciences , there are multiple uses for a predictive model of whether a student will perform a task correctly or not . for example , an intelligent tutoring system may use such a model to estimate whether or not a student has mastered a skill . we analyze the significance of data recency in making such predictions , i.e. , asking whether relatively more recent observations of a student 's performance matter more than relatively older observations . we develop a new recent-performance factors analysis model that takes data recency into account . the new model significantly improves predictive accuracy over both existing logistic-regression performance models and over novel baseline models in evaluations on real-world and synthetic datasets . as a secondary contribution , we demonstrate how the widely used cross-validation with 0-1 loss is inferior to aic and to cross-validation with l1 prediction error loss as a measure of model performance .

an efficient triplet-based algorithm for evidential reasoning
linear-time computational techniques have been developed for combining evidence which is available on a number of contending hypotheses . they offer a means of making the computation-intensive calculations involved more efficient in certain circumstances . unfortunately , they restrict the orthogonal sum of evidential functions to the dichotomous structure applies only to elements and their complements . in this paper , we present a novel evidence structure in terms of a triplet and a set of algorithms for evidential reasoning . the merit of this structure is that it divides a set of evidence into three subsets , distinguishing trivial evidential elements from important ones focusing some particular elements . it avoids the deficits of the dichotomous structure in representing the preference of evidence and estimating the basic probability assignment of evidence . we have established a formalism for this structure and the general formulae for combining pieces of evidence in the form of the triplet , which have been theoretically justified .

artificial intelligence in reverse supply chain management : the state of the art
product take-back legislation forces manufacturers to bear the costs of collection and disposal of products that have reached the end of their useful lives . in order to reduce these costs , manufacturers can consider reuse , remanufacturing and/or recycling of components as an alternative to disposal . the implementation of such alternatives usually requires an appropriate reverse supply chain management . with the concepts of reverse supply chain are gaining popularity in practice , the use of artificial intelligence approaches in these areas is also becoming popular . as a result , the purpose of this paper is to give an overview of the recent publications concerning the application of artificial intelligence techniques to reverse supply chain with emphasis on certain types of product returns .

beyond shared hierarchies : deep multitask learning through soft layer ordering
existing deep multitask learning ( mtl ) approaches align layers shared between tasks in a parallel ordering . such an organization significantly constricts the types of shared structure that can be learned . the necessity of parallel ordering for deep mtl is first tested by comparing it with permuted ordering of shared layers . the results indicate that a flexible ordering can enable more effective sharing , thus motivating the development of a soft ordering approach , which learns how shared layers are applied in different ways for different tasks . deep mtl with soft ordering outperforms parallel ordering methods across a series of domains . these results suggest that the power of deep mtl comes from learning highly general building blocks that can be assembled to meet the demands of each task .

probabilistic reasoning about actions in nonmonotonic causal theories
we present the language { m p } { cal c } + for probabilistic reasoning about actions , which is a generalization of the action language { cal c } + that allows to deal with probabilistic as well as nondeterministic effects of actions . we define a formal semantics of { m p } { cal c } + in terms of probabilistic transitions between sets of states . using a concept of a history and its belief state , we then show how several important problems in reasoning about actions can be concisely formulated in our formalism .

nothing else matters : model-agnostic explanations by identifying prediction invariance
at the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model 's behavior . assumed in this question are three properties of the interpretable output : coverage , precision , and effort . coverage refers to how often humans think they can predict the model 's behavior , precision to how accurate humans are in those predictions , and effort is either the up-front effort required in interpreting the model , or the effort required to make predictions about a model 's behavior . in this work , we propose anchor-lime ( alime ) , a model-agnostic technique that produces high-precision rule-based explanations for which the coverage boundaries are very clear . we compare alime to linear lime with simulated experiments , and demonstrate the flexibility of alime with qualitative examples from a variety of domains and tasks .

safe exploration in finite markov decision processes with gaussian processes
in classical reinforcement learning , when exploring an environment , agents accept arbitrary short term loss for long term gain . this is infeasible for safety critical applications , such as robotics , where even a single unsafe action may cause system failure . in this paper , we address the problem of safely exploring finite markov decision processes ( mdp ) . we define safety in terms of an , a priori unknown , safety constraint that depends on states and actions . we aim to explore the mdp under this constraint , assuming that the unknown function satisfies regularity conditions expressed via a gaussian process prior . we develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the mdp without violating the safety constraint . to achieve this , it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment . moreover , the algorithm explicitly considers reachability when exploring the mdp , ensuring that it does not get stuck in any state with no safe way out . we demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover .

an mlp based approach for recognition of handwritten ` bangla ' numerals
the work presented here involves the design of a multi layer perceptron ( mlp ) based pattern classifier for recognition of handwritten bangla digits using a 76 element feature vector . bangla is the second most popular script and language in the indian subcontinent and the fifth most popular language in the world . the feature set developed for representing handwritten bangla numerals here includes 24 shadow features , 16 centroid features and 36 longest-run features . on experimentation with a database of 6000 samples , the technique yields an average recognition rate of 96.67 % evaluated after three-fold cross validation of results . it is useful for applications related to ocr of handwritten bangla digit and can also be extended to include ocr of handwritten characters of bangla alphabet .

gotcha password hackers !
we introduce gotchas ( generating panoptic turing tests to tell computers and humans apart ) as a way of preventing automated offline dictionary attacks against user selected passwords . a gotcha is a randomized puzzle generation protocol , which involves interaction between a computer and a human . informally , a gotcha should satisfy two key properties : ( 1 ) the puzzles are easy for the human to solve . ( 2 ) the puzzles are hard for a computer to solve even if it has the random bits used by the computer to generate the final puzzle -- - unlike a captcha . our main theorem demonstrates that gotchas can be used to mitigate the threat of offline dictionary attacks against passwords by ensuring that a password cracker must receive constant feedback from a human being while mounting an attack . finally , we provide a candidate construction of gotchas based on inkblot images . our construction relies on the usability assumption that users can recognize the phrases that they originally used to describe each inkblot image -- - a much weaker usability assumption than previous password systems based on inkblots which required users to recall their phrase exactly . we conduct a user study to evaluate the usability of our gotcha construction . we also generate a gotcha challenge where we encourage artificial intelligence and security researchers to try to crack several passwords protected with our scheme .

semantics derived automatically from language corpora contain human-like biases
artificial intelligence and machine learning are in a period of astounding growth . however , there are concerns that these technologies may be used , either with or without intention , to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions . here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language -- -the same sort of language humans are exposed to every day . we replicate a spectrum of standard human biases as exposed by the implicit association test and other well-known psychological studies . we replicate these using a widely used , purely statistical machine-learning model -- -namely , the glove word embedding -- -trained on a corpus of text from the web . our results indicate that language itself contains recoverable and accurate imprints of our historic biases , whether these are morally neutral as towards insects or flowers , problematic as towards race or gender , or even simply veridical , reflecting the { \em status quo } for the distribution of gender with respect to careers or first names . these regularities are captured by machine learning along with the rest of semantics . in addition to our empirical findings concerning language , we also contribute new methods for evaluating bias in text , the word embedding association test ( weat ) and the word embedding factual association test ( wefat ) . our results have implications not only for ai and machine learning , but also for the fields of psychology , sociology , and human ethics , since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here .

learning to execute
recurrent neural networks ( rnns ) with long short-term memory units ( lstm ) are widely used because they are expressive and are easy to train . our interest lies in empirically evaluating the expressiveness and the learnability of lstms in the sequence-to-sequence regime by training them to evaluate short computer programs , a domain that has traditionally been seen as too complex for neural networks . we consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory . our main result is that lstms can learn to map the character-level representations of such programs to their correct outputs . notably , it was necessary to use curriculum learning , and while conventional curriculum learning proved ineffective , we developed a new variant of curriculum learning that improved our networks ' performance in all experimental conditions . the improved curriculum had a dramatic impact on an addition problem , making it possible to train an lstm to add two 9-digit numbers with 99 % accuracy .

`` in vivo '' spam filtering : a challenge problem for data mining
spam , also known as unsolicited commercial email ( uce ) , is the bane of email communication . many data mining researchers have addressed the problem of detecting spam , generally by treating it as a static text classification problem . true in vivo spam filtering has characteristics that make it a rich and challenging domain for data mining . indeed , real-world datasets with these characteristics are typically difficult to acquire and to share . this paper demonstrates some of these characteristics and argues that researchers should pursue in vivo spam filtering as an accessible domain for investigating them .

xflow : 1d-2d cross-modal deep neural networks for audiovisual classification
we propose two multimodal deep learning architectures that allow for cross-modal dataflow ( xflow ) between the feature extractors , thereby extracting more interpretable features and obtaining a better representation than through unimodal learning , for the same amount of training data . these models can usefully exploit correlations between audio and visual data , which have a different dimensionality and are therefore nontrivially exchangeable . our work improves on existing multimodal deep learning metholodogies in two essential ways : ( 1 ) it presents a novel method for performing cross-modality ( before features are learned from individual modalities ) and ( 2 ) extends the previously proposed cross-connections , which only transfer information between streams that process compatible data . both cross-modal architectures outperformed their baselines ( by up to 7.5 % ) when evaluated on the avletters dataset .

towards an automated query modification assistant
users who need several queries before finding what they need can benefit from an automatic search assistant that provides feedback on their query modification strategies . we present a method to learn from a search log which types of query modifications have and have not been effective in the past . the method analyses query modifications along two dimensions : a traditional term-based dimension and a semantic dimension , for which queries are enriches with linked data entities . applying the method to the search logs of two search engines , we identify six opportunities for a query modification assistant to improve search : modification strategies that are commonly used , but that often do not lead to satisfactory results .

compositional operators in distributional semantics
this survey presents in some detail the main advances that have been recently taking place in computational linguistics towards the unification of the two prominent semantic paradigms : the compositional formal semantics view and the distributional models of meaning based on vector spaces . after an introduction to these two approaches , i review the most important models that aim to provide compositionality in distributional semantics . then i proceed and present in more detail a particular framework by coecke , sadrzadeh and clark ( 2010 ) based on the abstract mathematical setting of category theory , as a more complete example capable to demonstrate the diversity of techniques and scientific disciplines that this kind of research can draw from . this paper concludes with a discussion about important open issues that need to be addressed by the researchers in the future .

mining biclusters of similar values with triadic concept analysis
biclustering numerical data became a popular data-mining task in the beginning of 2000 's , especially for analysing gene expression data . a bicluster reflects a strong association between a subset of objects and a subset of attributes in a numerical object/attribute data-table . so called biclusters of similar values can be thought as maximal sub-tables with close values . only few methods address a complete , correct and non redundant enumeration of such patterns , which is a well-known intractable problem , while no formal framework exists . in this paper , we introduce important links between biclustering and formal concept analysis . more specifically , we originally show that triadic concept analysis ( tca ) , provides a nice mathematical framework for biclustering . interestingly , existing algorithms of tca , that usually apply on binary data , can be used ( directly or with slight modifications ) after a preprocessing step for extracting maximal biclusters of similar values .

higher-order partial least squares ( hopls ) : a generalized multi-linear regression method
a new generalized multilinear regression model , termed the higher-order partial least squares ( hopls ) , is introduced with the aim to predict a tensor ( multiway array ) $ \tensor { y } $ from a tensor $ \tensor { x } $ through projecting the data onto the latent space and performing regression on the corresponding latent variables . hopls differs substantially from other regression models in that it explains the data by a sum of orthogonal tucker tensors , while the number of orthogonal loadings serves as a parameter to control model complexity and prevent overfitting . the low dimensional latent space is optimized sequentially via a deflation operation , yielding the best joint subspace approximation for both $ \tensor { x } $ and $ \tensor { y } $ . instead of decomposing $ \tensor { x } $ and $ \tensor { y } $ individually , higher order singular value decomposition on a newly defined generalized cross-covariance tensor is employed to optimize the orthogonal loadings . a systematic comparison on both synthetic data and real-world decoding of 3d movement trajectories from electrocorticogram ( ecog ) signals demonstrate the advantages of hopls over the existing methods in terms of better predictive ability , suitability to handle small sample sizes , and robustness to noise .

graph colouring problem based on discrete imperialist competitive algorithm
in graph theory , graph colouring problem ( gcp ) is an assignment of colours to vertices of any given graph such that the colours on adjacent vertices are different . the gcp is known to be an optimization and np-hard problem . imperialist competitive algorithm ( ica ) is a meta-heuristic optimization and stochastic search strategy which is inspired from socio-political phenomenon of imperialistic competition . the ica contains two main operators : the assimilation and the imperialistic competition . the ica has excellent capabilities such as high convergence rate and better global optimum achievement . in this research , a discrete version of ica is proposed to deal with the solution of gcp . we call this algorithm as the dica . the performance of the proposed method is compared with genetic algorithm ( ga ) on seven well-known graph colouring benchmarks . experimental results demonstrate the superiority of the dica for the benchmarks . this means dica can produce optimal and valid solutions for different gcp instances .

deep learning : a critical appraisal
although deep learning has historical roots going back decades , neither the term `` deep learning '' nor the approach was popular just over five years ago , when the field was reignited by papers such as krizhevsky , sutskever and hinton 's now classic ( 2012 ) deep network model of imagenet . what has the field discovered in the five subsequent years ? against a background of considerable progress in areas such as speech recognition , image recognition , and game playing , and considerable enthusiasm in the popular press , i present ten concerns for deep learning , and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence .

digital stylometry : linking profiles across social networks
there is an ever growing number of users with accounts on multiple social media and networking sites . consequently , there is increasing interest in matching user accounts and profiles across different social networks in order to create aggregate profiles of users . in this paper , we present models for digital stylometry , which is a method for matching users through stylometry inspired techniques . we experimented with linguistic , temporal , and combined temporal-linguistic models for matching user accounts , using standard and novel techniques . using publicly available data , our best model , a combined temporal-linguistic one , was able to correctly match the accounts of 31 % of 5,612 distinct users across twitter and facebook .

noise-tolerant learning , the parity problem , and the statistical query model
we describe a slightly sub-exponential time algorithm for learning parity functions in the presence of random classification noise . this results in a polynomial-time algorithm for the case of parity functions that depend on only the first o ( log n log log n ) bits of input . this is the first known instance of an efficient noise-tolerant algorithm for a concept class that is provably not learnable in the statistical query model of kearns . thus , we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the pac model . in coding-theory terms , what we give is a poly ( n ) -time algorithm for decoding linear k by n codes in the presence of random noise for the case of k = c log n loglog n for some c > 0 . ( the case of k = o ( log n ) is trivial since one can just individually check each of the 2^k possible messages and choose the one that yields the closest codeword . ) a natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples ( as opposed to single examples ) . the second result of this paper is to show that any class of functions learnable ( strongly or weakly ) with t-wise queries for t = o ( log n ) is also weakly learnable with standard unary queries . hence this natural extension to the statistical query model does not increase the set of weakly learnable functions .

a minimax algorithm better than alpha-beta ? : no and yes
this paper has three main contributions to our understanding of fixed-depth minimax search : ( a ) a new formulation for stockman 's sss* algorithm , based on alpha-beta , is presented . it solves all the perceived drawbacks of sss* , finally transforming it into a practical algorithm . in effect , we show that sss* = alpha-beta + ransposition tables . the crucial step is the realization that transposition tables contain so-called solution trees , structures that are used in best-first search algorithms like sss* . having created a practical version , we present performance measurements with tournament game-playing programs for three different minimax games , yielding results that contradict a number of publications . ( b ) based on the insights gained in our attempts at understanding sss* , we present a framework that facilitates the construction of several best-first fixed- depth game-tree search algorithms , known and new . the framework is based on depth-first null-window alpha-beta search , enhanced with storage to allow for the refining of previous search results . it focuses attention on the essential differences between algorithms . ( c ) we present a new instance of the framework , mtd ( f ) . it is well-suited for use with iterative deepening , and performs better than algorithms that are currently used in most state-of-the-art game-playing programs . we provide experimental evidence to explain why mtd ( f ) performs better than the other fixed-depth minimax algorithms .

beyond the technical challenges for deploying machine learning solutions in a software company
recently software development companies started to embrace machine learning ( ml ) techniques for introducing a series of advanced functionality in their products such as personalisation of the user experience , improved search , content recommendation and automation . the technical challenges for tackling these problems are heavily researched in literature . a less studied area is a pragmatic approach to the role of humans in a complex modern industrial environment where ml based systems are developed . key stakeholders affect the system from inception and up to operation and maintenance . product managers want to embed `` smart '' experiences for their users and drive the decisions on what should be built next ; software engineers are challenged to build or utilise ml software tools that require skills that are well outside of their comfort zone ; legal and risk departments may influence design choices and data access ; operations teams are requested to maintain ml systems which are non-stationary in their nature and change behaviour over time ; and finally ml practitioners should communicate with all these stakeholders to successfully build a reliable system . this paper discusses some of the challenges we faced in atlassian as we started investing more in the ml space .

evaluation of interactive machine learning systems
the evaluation of interactive machine learning systems remains a difficult task . these systems learn from and adapt to the human , but at the same time , the human receives feedback and adapts to the system . getting a clear understanding of these subtle mechanisms of co-operation and co-adaptation is challenging . in this chapter , we report on our experience in designing and evaluating various interactive machine learning applications from different domains . we argue for coupling two types of validation : algorithm-centered analysis , to study the computational behaviour of the system ; and human-centered evaluation , to observe the utility and effectiveness of the application for end-users . we use a visual analytics application for guided search , built using an interactive evolutionary approach , as an exemplar of our work . our observation is that human-centered design and evaluation complement algorithmic analysis , and can play an important role in addressing the `` black-box '' effect of machine learning . finally , we discuss research opportunities that require human-computer interaction methodologies , in order to support both the visible and hidden roles that humans play in interactive machine learning .

intrinsically motivated goal exploration processes with automatic curriculum learning
intrinsically motivated spontaneous exploration is a key enabler of autonomous lifelong learning in human children . it allows them to discover and acquire large repertoires of skills through self-generation , self-selection , self-ordering and self-experimentation of learning goals . we present the unsupervised multi-goal reinforcement learning formal framework as well as an algorithmic approach called intrinsically motivated goal exploration processes ( imgep ) to enable similar properties of autonomous learning in machines . the imgep algorithmic architecture relies on several principles : 1 ) self-generation of goals as parameterized reinforcement learning problems ; 2 ) selection of goals based on intrinsic rewards ; 3 ) exploration with parameterized time-bounded policies and fast incremental goal-parameterized policy search ; 4 ) systematic reuse of information acquired when targeting a goal for improving other goals . we present a particularly efficient form of imgep that uses a modular representation of goal spaces as well as intrinsic rewards based on learning progress . we show how imgeps automatically generate a learning curriculum within an experimental setup where a real humanoid robot can explore multiple spaces of goals with several hundred continuous dimensions . while no particular target goal is provided to the system beforehand , this curriculum allows the discovery of skills of increasing complexity , that act as stepping stone for learning more complex skills ( like nested tool use ) . we show that learning several spaces of diverse problems can be more efficient for learning complex skills than only trying to directly learn these complex skills . we illustrate the computational efficiency of imgeps as these robotic experiments use a simple memory-based low-level policy representations and search algorithm , enabling the whole system to learn online and incrementally on a raspberry pi 3 .

deep neural networks for bot detection
the problem of detecting bots , automated social media accounts governed by software but disguising as human users , has strong implications . for example , bots have been used to sway political elections by distorting online discourse , to manipulate the stock market , or to push anti-vaccine conspiracy theories that caused health epidemics . most techniques proposed to date detect bots at the account level , by processing large amount of social media posts , and leveraging information from network structure , temporal dynamics , sentiment analysis , etc . in this paper , we propose a deep neural network based on contextual long short-term memory ( lstm ) architecture that exploits both content and metadata to detect bots at the tweet level : contextual features are extracted from user metadata and fed as auxiliary input to lstm deep nets processing the tweet text . another contribution that we make is proposing a technique based on synthetic minority oversampling to generate a large labeled dataset , suitable for deep nets training , from a minimal amount of labeled data ( roughly 3,000 examples of sophisticated twitter bots ) . we demonstrate that , from just one single tweet , our architecture can achieve high classification accuracy ( auc > 96 % ) in separating bots from humans . we apply the same architecture to account-level bot detection , achieving nearly perfect classification accuracy ( auc > 99 % ) . our system outperforms previous state of the art while leveraging a small and interpretable set of features yet requiring minimal training data .

modeling cultural dynamics
evoc ( for evolution of culture ) is a computer model of culture that enables us to investigate how various factors such as barriers to cultural diffusion , the presence and choice of leaders , or changes in the ratio of innovation to imitation affect the diversity and effectiveness of ideas . it consists of neural network based agents that invent ideas for actions , and imitate neighbors ' actions . the model is based on a theory of culture according to which what evolves through culture is not memes or artifacts , but the internal models of the world that give rise to them , and they evolve not through a darwinian process of competitive exclusion but a lamarckian process involving exchange of innovation protocols . evoc shows an increase in mean fitness of actions over time , and an increase and then decrease in the diversity of actions . diversity of actions is positively correlated with population size and density , and with barriers between populations . slowly eroding borders increase fitness without sacrificing diversity by fostering specialization followed by sharing of fit actions . introducing a leader that broadcasts its actions throughout the population increases the fitness of actions but reduces diversity of actions . increasing the number of leaders reduces this effect . efforts are underway to simulate the conditions under which an agent immigrating from one culture to another contributes new ideas while still fitting in .

parameterized compilation lower bounds for restricted cnf-formulas
we show unconditional parameterized lower bounds in the area of knowledge compilation , more specifically on the size of circuits in decomposable negation normal form ( dnnf ) that encode cnf-formulas restricted by several graph width measures . in particular , we show that - there are cnf formulas of size $ n $ and modular incidence treewidth $ k $ whose smallest dnnf-encoding has size $ n^ { \omega ( k ) } $ , and - there are cnf formulas of size $ n $ and incidence neighborhood diversity $ k $ whose smallest dnnf-encoding has size $ n^ { \omega ( \sqrt { k } ) } $ . these results complement recent upper bounds for compiling cnf into dnnf and strengthen -- -quantitatively and qualitatively -- -known conditional low\-er bounds for cliquewidth . moreover , they show that , unlike for many graph problems , the parameters considered here behave significantly differently from treewidth .

satisfaction of assumptions is a weak predictor of performance
this paper demonstrates a methodology for examining the accuracy of uncertain inference systems ( uis ) , after their parameters have been optimized , and does so for several common uis 's . this methodology may be used to test the accuracy when either the prior assumptions or updating formulae are not exactly satisfied . surprisingly , these uis 's were revealed to be no more accurate on the average than a simple linear regression . moreover , even on prior distributions which were deliberately biased so as give very good accuracy , they were less accurate than the simple probabilistic model which assumes marginal independence between inputs . this demonstrates that the importance of updating formulae can outweigh that of prior assumptions . thus , when uis 's are judged by their final accuracy after optimization , we get completely different results than when they are judged by whether or not their prior assumptions are perfectly satisfied .

zero-resource machine translation by multimodal encoder-decoder network with multimedia pivot
we propose an approach to build a neural machine translation system with no supervised resources ( i.e. , no parallel corpora ) using multimodal embedded representation over texts and images . based on the assumption that text documents are often likely to be described with other multimedia information ( e.g. , images ) somewhat related to the content , we try to indirectly estimate the relevance between two languages . using multimedia as the `` pivot '' , we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other , whatever the observed space of each sample is . this modality-agnostic representation is the key to bridging the gap between different modalities . putting a decoder on top of it , our network can flexibly draw the outputs from any input modality . notably , in the testing phase , we need only source language texts as the input for translation . in experiments , we tested our method on two benchmarks to show that it can achieve reasonable translation performance . we compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best .

a hybrid lp-rpg heuristic for modelling numeric resource flows in planning
although the use of metric fluents is fundamental to many practical planning problems , the study of heuristics to support fully automated planners working with these fluents remains relatively unexplored . the most widely used heuristic is the relaxation of metric fluents into interval-valued variables -- - an idea first proposed a decade ago . other heuristics depend on domain encodings that supply additional information about fluents , such as capacity constraints or other resource-related annotations . a particular challenge to these approaches is in handling interactions between metric fluents that represent exchange , such as the transformation of quantities of raw materials into quantities of processed goods , or trading of money for materials . the usual relaxation of metric fluents is often very poor in these situations , since it does not recognise that resources , once spent , are no longer available to be spent again . we present a heuristic for numeric planning problems building on the propositional relaxed planning graph , but using a mathematical program for numeric reasoning . we define a class of producer -- consumer planning problems and demonstrate how the numeric constraints in these can be modelled in a mixed integer program ( mip ) . this mip is then combined with a metric relaxed planning graph ( rpg ) heuristic to produce an integrated hybrid heuristic . the mip tracks resource use more accurately than the usual relaxation , but relaxes the ordering of actions , while the rpg captures the causal propositional aspects of the problem . we discuss how these two components interact to produce a single unified heuristic and go on to explore how further numeric features of planning problems can be integrated into the mip . we show that encoding a limited subset of the propositional problem to augment the mip can yield more accurate guidance , partly by exploiting structure such as propositional landmarks and propositional resources . our results show that the use of this heuristic enhances scalability on problems where numeric resource interaction is key in finding a solution .

enhanced discrete particle swarm optimization path planning for uav vision-based surface inspection
in built infrastructure monitoring , an efficient path planning algorithm is essential for robotic inspection of large surfaces using computer vision . in this work , we first formulate the inspection path planning problem as an extended travelling salesman problem ( tsp ) in which both the coverage and obstacle avoidance were taken into account . an enhanced discrete particle swarm optimization ( dpso ) algorithm is then proposed to solve the tsp , with performance improvement by using deterministic initialization , random mutation , and edge exchange . finally , we take advantage of parallel computing to implement the dpso in a gpu-based framework so that the computation time can be significantly reduced while keeping the hardware requirement unchanged . to show the effectiveness of the proposed algorithm , experimental results are included for datasets obtained from uav inspection of an office building and a bridge .

discrete dynamical genetic programming in xcs
a number of representation schemes have been presented for use within learning classifier systems , ranging from binary encodings to neural networks . this paper presents results from an investigation into using a discrete dynamical system representation within the xcs learning classifier system . in particular , asynchronous random boolean networks are used to represent the traditional condition-action production system rules . it is shown possible to use self-adaptive , open-ended evolution to design an ensemble of such discrete dynamical systems within xcs to solve a number of well-known test problems .

fuzzy dynamical genetic programming in xcsf
a number of representation schemes have been presented for use within learning classifier systems , ranging from binary encodings to neural networks , and more recently dynamical genetic programming ( dgp ) . this paper presents results from an investigation into using a fuzzy dgp representation within the xcsf learning classifier system . in particular , asynchronous fuzzy logic networks are used to represent the traditional condition-action production system rules . it is shown possible to use self-adaptive , open-ended evolution to design an ensemble of such fuzzy dynamical systems within xcsf to solve several well-known continuous-valued test problems .

quantified degrees of group responsibility ( extended abstract )
this paper builds on an existing notion of group responsibility and proposes two ways to define the degree of group responsibility : structural and functional degrees of responsibility . these notions measure the potential responsibilities of ( agent ) groups for avoiding a state of affairs . according to these notions , a degree of responsibility for a state of affairs can be assigned to a group of agents if , and to the extent that , the group has the potential to preclude the state of affairs .

a new framework for distributed submodular maximization
a wide variety of problems in machine learning , including exemplar clustering , document summarization , and sensor placement , can be cast as constrained submodular maximization problems . a lot of recent effort has been devoted to developing distributed algorithms for these problems . however , these results suffer from high number of rounds , suboptimal approximation ratios , or both . we develop a framework for bringing existing algorithms in the sequential setting to the distributed setting , achieving near optimal approximation ratios for many settings in only a constant number of mapreduce rounds . our techniques also give a fast sequential algorithm for non-monotone maximization subject to a matroid constraint .

a historical review of forty years of research on cmac
the cerebellar model articulation controller ( cmac ) is an influential brain-inspired computing model in many relevant fields . since its inception in the 1970s , the model has been intensively studied and many variants of the prototype , such as kernel-cmac , self-organizing map cmac , and linguistic cmac , have been proposed . this review article focus on how the cmac model is gradually developed and refined to meet the demand of fast , adaptive , and robust control . two perspective , cmac as a neural network and cmac as a table look-up technique are presented . three aspects of the model : the architecture , learning algorithms and applications are discussed . in the end , some potential future research directions on this model are suggested .

revisiting algebra and complexity of inference in graphical models
this paper studies the form and complexity of inference in graphical models using the abstraction offered by algebraic structures . in particular , we broadly formalize inference problems in graphical models by viewing them as a sequence of operations based on commutative semigroups . we then study the computational complexity of inference by organizing various problems into an `` inference hierarchy '' . when the underlying structure of an inference problem is a commutative semiring -- i.e . a combination of two commutative semigroups with the distributive law -- a message passing procedure called belief propagation can leverage this distributive law to perform polynomial-time inference for certain problems . after establishing the np-hardness of inference in any commutative semiring , we investigate the relation between algebraic properties in this setting and further show that polynomial-time inference using distributive law does not ( trivially ) extend to inference problems that are expressed using more than two commutative semigroups . we then extend the algebraic treatment of message passing procedures to survey propagation , providing a novel perspective using a combination of two commutative semirings . this formulation generalizes the application of survey propagation to new settings .

variational inference with normalizing flows
the choice of approximate posterior distribution is one of the core problems in variational inference . most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference , focusing on mean-field or other simple structured approximations . this restriction has a significant impact on the quality of inferences made using variational methods . we introduce a new approach for specifying flexible , arbitrarily complex and scalable approximate posterior distributions . our approximations are distributions constructed through a normalizing flow , whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained . we use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations . we demonstrate that the theoretical advantages of having posteriors that better match the true posterior , combined with the scalability of amortized variational approaches , provides a clear improvement in performance and applicability of variational inference .

hybrid intrusion detection and prediction multiagent system hidpas
this paper proposes an intrusion detection and prediction system based on uncertain and imprecise inference networks and its implementation . giving a historic of sessions , it is about proposing a method of supervised learning doubled of a classifier permitting to extract the necessary knowledge in order to identify the presence or not of an intrusion in a session and in the positive case to recognize its type and to predict the possible intrusions that will follow it . the proposed system takes into account the uncertainty and imprecision that can affect the statistical data of the historic . the systematic utilization of an unique probability distribution to represent this type of knowledge supposes a too rich subjective information and risk to be in part arbitrary . one of the first objectives of this work was therefore to permit the consistency between the manner of which we represent information and information which we really dispose .

do reichenbachian common cause systems of arbitrary finite size exist ?
the principle of common cause asserts that positive correlations between causally unrelated events ought to be explained through the action of some shared causal factors . reichenbachian common cause systems are probabilistic structures aimed at accounting for cases where correlations of the aforesaid sort can not be explained through the action of a single common cause . the existence of reichenbachian common cause systems of arbitrary finite size for each pair of non-causally correlated events was allegedly demonstrated by hofer-szab\'o and r\'edei in 2006. this paper shows that their proof is logically deficient , and we propose an improved proof .

optimisation using natural language processing : personalized tour recommendation for museums
this paper proposes a new method to provide personalized tour recommendation for museum visits . it combines an optimization of preference criteria of visitors with an automatic extraction of artwork importance from museum information based on natural language processing using textual energy . this project includes researchers from computer and social sciences . some results are obtained with numerical experiments . they show that our model clearly improves the satisfaction of the visitor who follows the proposed tour . this work foreshadows some interesting outcomes and applications about on-demand personalized visit of museums in a very near future .

evolutionary solving of the debts ' clearing problem
the debts ' clearing problem is about clearing all the debts in a group of n entities ( persons , companies etc . ) using a minimal number of money transaction operations . the problem is known to be np-hard in the strong sense . as for many intractable problems , techniques from the field of artificial intelligence are useful in finding solutions close to optimum for large inputs . an evolutionary algorithm for solving the debts ' clearing problem is proposed .

how important is syntactic parsing accuracy ? an empirical evaluation on rule-based sentiment analysis
syntactic parsing , the process of obtaining the internal structure of sentences in natural languages , is a crucial task for artificial intelligence applications that need to extract meaning from natural language text or speech . sentiment analysis is one example of application for which parsing has recently proven useful . in recent years , there have been significant advances in the accuracy of parsing algorithms . in this article , we perform an empirical , task-oriented evaluation to determine how parsing accuracy influences the performance of a state-of-the-art rule-based sentiment analysis system that determines the polarity of sentences from their parse trees . in particular , we evaluate the system using four well-known dependency parsers , including both current models with state-of-the-art accuracy and more innacurate models which , however , require less computational resources . the experiments show that all of the parsers produce similarly good results in the sentiment analysis task , without their accuracy having any relevant influence on the results . since parsing is currently a task with a relatively high computational cost that varies strongly between algorithms , this suggests that sentiment analysis researchers and users should prioritize speed over accuracy when choosing a parser ; and parsing researchers should investigate models that improve speed further , even at some cost to accuracy .

data set operations to hide decision tree rules
this paper focuses on preserving the privacy of sensitive patterns when inducing decision trees . we adopt a record augmentation approach for hiding sensitive classification rules in binary datasets . such a hiding methodology is preferred over other heuristic solutions like output perturbation or cryptographic techniques - which restrict the usability of the data - since the raw data itself is readily available for public use . we show some key lemmas which are related to the hiding process and we also demonstrate the methodology with an example and an indicative experiment using a prototype hiding tool .

consensus message passing for layered graphical models
generative models provide a powerful framework for probabilistic reasoning . however , in many domains their use has been hampered by the practical difficulties of inference . this is particularly the case in computer vision , where models of the imaging process tend to be large , loopy and layered . for this reason bottom-up conditional models have traditionally dominated in such domains . we find that widely-used , general-purpose message passing inference algorithms such as expectation propagation ( ep ) and variational message passing ( vmp ) fail on the simplest of vision models . with these models in mind , we introduce a modification to message passing that learns to exploit their layered structure by passing 'consensus ' messages that guide inference towards good solutions . experiments on a variety of problems show that the proposed technique leads to significantly more accurate inference results , not only when compared to standard ep and vmp , but also when compared to competitive bottom-up conditional models .

simultaneous object detection , tracking , and event recognition
the common internal structure and algorithmic organization of object detection , detection-based tracking , and event recognition facilitates a general approach to integrating these three components . this supports multidirectional information flow between these components allowing object detection to influence tracking and event recognition and event recognition to influence tracking and object detection . the performance of the combination can exceed the performance of the components in isolation . this can be done with linear asymptotic complexity .

optimized look-ahead tree policies : a bridge between look-ahead tree policies and direct policy search
direct policy search ( dps ) and look-ahead tree ( lt ) policies are two widely used classes of techniques to produce high performance policies for sequential decision-making problems . to make dps approaches work well , one crucial issue is to select an appropriate space of parameterized policies with respect to the targeted problem . a fundamental issue in lt approaches is that , to take good decisions , such policies must develop very large look-ahead trees which may require excessive online computational resources . in this paper , we propose a new hybrid policy learning scheme that lies at the intersection of dps and lt , in which the policy is an algorithm that develops a small look-ahead tree in a directed way , guided by a node scoring function that is learned through dps . the lt-based representation is shown to be a versatile way of representing policies in a dps scheme , while at the same time , dps enables to significantly reduce the size of the look-ahead trees that are required to take high-quality decisions . we experimentally compare our method with two other state-of-the-art dps techniques and four common lt policies on four benchmark domains and show that it combines the advantages of the two techniques from which it originates . in particular , we show that our method : ( 1 ) produces overall better performing policies than both pure dps and pure lt policies , ( 2 ) requires a substantially smaller number of policy evaluations than other dps techniques , ( 3 ) is easy to tune and ( 4 ) results in policies that are quite robust with respect to perturbations of the initial conditions .

application of the sp theory of intelligence to the understanding of natural vision and the development of computer vision
the sp theory of intelligence aims to simplify and integrate concepts in computing and cognition , with information compression as a unifying theme . this article discusses how it may be applied to the understanding of natural vision and the development of computer vision . the theory , which is described quite fully elsewhere , is described here in outline but with enough detail to ensure that the rest of the article makes sense . low level perceptual features such as edges or corners may be identified by the extraction of redundancy in uniform areas in a manner that is comparable with the run-length encoding technique for information compression . the concept of multiple alignment in the sp theory may be applied to the recognition of objects , and to scene analysis , with a hierarchy of parts and sub-parts , and at multiple levels of abstraction . the theory has potential for the unsupervised learning of visual objects and classes of objects , and suggests how coherent concepts may be derived from fragments . as in natural vision , both recognition and learning in the sp system is robust in the face of errors of omission , commission and substitution . the theory suggests how , via vision , we may piece together a knowledge of the three-dimensional structure of objects and of our environment , it provides an account of how we may see things that are not objectively present in an image , and how we recognise something despite variations in the size of its retinal image . and it has things to say about the phenomena of lightness constancy and colour constancy , the role of context in recognition , and ambiguities in visual perception . a strength of the sp theory is that it provides for the integration of vision with other sensory modalities and with other aspects of intelligence .

time hopping technique for faster reinforcement learning in simulations
this preprint has been withdrawn by the author for revision

optimization of anemia treatment in hemodialysis patients via reinforcement learning
objective : anemia is a frequent comorbidity in hemodialysis patients that can be successfully treated by administering erythropoiesis-stimulating agents ( esas ) . esas dosing is currently based on clinical protocols that often do not account for the high inter- and intra-individual variability in the patient 's response . as a result , the hemoglobin level of some patients oscillates around the target range , which is associated with multiple risks and side-effects . this work proposes a methodology based on reinforcement learning ( rl ) to optimize esa therapy . methods : rl is a data-driven approach for solving sequential decision-making problems that are formulated as markov decision processes ( mdps ) . computing optimal drug administration strategies for chronic diseases is a sequential decision-making problem in which the goal is to find the best sequence of drug doses . mdps are particularly suitable for modeling these problems due to their ability to capture the uncertainty associated with the outcome of the treatment and the stochastic nature of the underlying process . the rl algorithm employed in the proposed methodology is fitted q iteration , which stands out for its ability to make an efficient use of data . results : the experiments reported here are based on a computational model that describes the effect of esas on the hemoglobin level . the performance of the proposed method is evaluated and compared with the well-known q-learning algorithm and with a standard protocol . simulation results show that the performance of q-learning is substantially lower than fqi and the protocol . conclusion : although prospective validation is required , promising results demonstrate the potential of rl to become an alternative to current protocols .

compositional model repositories via dynamic constraint satisfaction with order-of-magnitude preferences
the predominant knowledge-based approach to automated model construction , compositional modelling , employs a set of models of particular functional components . its inference mechanism takes a scenario describing the constituent interacting components of a system and translates it into a useful mathematical model . this paper presents a novel compositional modelling approach aimed at building model repositories . it furthers the field in two respects . firstly , it expands the application domain of compositional modelling to systems that can not be easily described in terms of interacting functional components , such as ecological systems . secondly , it enables the incorporation of user preferences into the model selection process . these features are achieved by casting the compositional modelling problem as an activity-based dynamic preference constraint satisfaction problem , where the dynamic constraints describe the restrictions imposed over the composition of partial models and the preferences correspond to those of the user of the automated modeller . in addition , the preference levels are represented through the use of symbolic values that differ in orders of magnitude .

on implementing usual values
in many cases commonsense knowledge consists of knowledge of what is usual . in this paper we develop a system for reasoning with usual information . this system is based upon the fact that these pieces of commonsense information involve both a probabilistic aspect and a granular aspect . we implement this system with the aid of possibility-probability granules .

a partial taxonomy of judgment aggregation rules , and their properties
the literature on judgment aggregation is moving from studying impossibility results regarding aggregation rules towards studying specific judgment aggregation rules . here we give a structured list of most rules that have been proposed and studied recently in the literature , together with various properties of such rules . we first focus on the majority-preservation property , which generalizes condorcet-consistency , and identify which of the rules satisfy it . we study the inclusion relationships that hold between the rules . finally , we consider two forms of unanimity , monotonicity , homogeneity , and reinforcement , and we identify which of the rules satisfy these properties .

inverses , conditionals and compositional operators in separative valuation algebra
compositional models were introduce by jirousek and shenoy in the general framework of valuation-based systems . they based their theory on an axiomatic system of valuations involving not only the operations of combination and marginalisation , but also of removal . they claimed that this systems covers besides the classical case of discrete probability distributions , also the cases of gaussian densities and belief functions , and many other systems . whereas their results on the compositional operator are correct , the axiomatic basis is not sufficient to cover the examples claimed above . we propose here a different axiomatic system of valuation algebras , which permits a rigorous mathematical theory of compositional operators in valuation-based systems and covers all the examples mentioned above . it extends the classical theory of inverses in semigroup theory and places thereby the present theory into its proper mathematical frame . also this theory sheds light on the different structures of valuation-based systems , like regular algebras ( represented by probability potentials ) , canncellative algebras ( gaussian potentials ) and general separative algebras ( density functions ) .

monotone conditional complexity bounds on future prediction errors
we bound the future loss when predicting any ( computably ) stochastic sequence online . solomonoff finitely bounded the total deviation of his universal predictor m from the true distribution m by the algorithmic complexity of m. here we assume we are at a time t > 1 and already observed x=x_1 ... x_t . we bound the future prediction performance on x_ { t+1 } x_ { t+2 } ... by a new variant of algorithmic complexity of m given x , plus the complexity of the randomness deficiency of x. the new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged . we also briefly discuss potential generalizations to bayesian model classes and to classification problems .

a generalization of the noisy-or model
the noisy-or model is convenient for describing a class of uncertain relationships in bayesian networks [ pearl 1988 ] . pearl describes the noisy-or model for boolean variables . here we generalize the model to nary input and output variables and to arbitrary functions other than the boolean or function . this generalization is a useful modeling aid for construction of bayesian networks . we illustrate with some examples including digital circuit diagnosis and network reliability analysis .

a qualitative linear utility theory for spohn 's theory of epistemic beliefs
in this paper , we formulate a qualitative `` linear '' utility theory for lotteries in which uncertainty is expressed qualitatively using a spohnian disbelief function . we argue that a rational decision maker facing an uncertain decision problem in which the uncertainty is expressed qualitatively should behave so as to maximize `` qualitative expected utility . '' our axiomatization of the qualitative utility is similar to the axiomatization developed by von neumann and morgenstern for probabilistic lotteries . we compare our results with other recent results in qualitative decision making .

deductive nonmonotonic inference operations : antitonic representations
we provide a characterization of those nonmonotonic inference operations c for which c ( x ) may be described as the set of all logical consequences of x together with some set of additional assumptions s ( x ) that depends anti-monotonically on x ( i.e. , x is a subset of y implies that s ( y ) is a subset of s ( x ) ) . the operations represented are exactly characterized in terms of properties most of which have been studied in freund-lehmann ( cs.ai/0202031 ) . similar characterizations of right-absorbing and cumulative operations are also provided . for cumulative operations , our results fit in closely with those of freund . we then discuss extending finitary operations to infinitary operations in a canonical way and discuss co-compactness properties . our results provide a satisfactory notion of pseudo-compactness , generalizing to deductive nonmonotonic operations the notion of compactness for monotonic operations . they also provide an alternative , more elegant and more general , proof of the existence of an infinitary deductive extension for any finitary deductive operation ( theorem 7.9 of freund-lehmann ) .

d0 data handling operational experience
we report on the production experience of the d0 experiment at the fermilab tevatron , using the sam data handling system with a variety of computing hardware configurations , batch systems , and mass storage strategies . we have stored more than 300 tb of data in the fermilab enstore mass storage system . we deliver data through this system at an average rate of more than 2 tb/day to analysis programs , with a substantial multiplication factor in the consumed data through intelligent cache management . we handle more than 1.7 million files in this system and provide data delivery to user jobs at fermilab on four types of systems : a reconstruction farm , a large smp system , a linux batch cluster , and a linux desktop cluster . in addition , we import simulation data generated at 6 sites worldwide , and deliver data to jobs at many more sites . we describe the scope of the data handling deployment worldwide , the operational experience with this system , and the feedback of that experience .

ambiguity-driven fuzzy c-means clustering : how to detect uncertain clustered records
as a well-known clustering algorithm , fuzzy c-means ( fcm ) allows each input sample to belong to more than one cluster , providing more flexibility than non-fuzzy clustering methods . however , the accuracy of fcm is subject to false detections caused by noisy records , weak feature selection and low certainty of the algorithm in some cases . the false detections are very important in some decision-making application domains like network security and medical diagnosis , where weak decisions based on such false detections may lead to catastrophic outcomes . they are mainly emerged from making decisions about a subset of records that do not provide enough evidence to make a good decision . in this paper , we propose a method for detecting such ambiguous records in fcm by introducing a certainty factor to decrease invalid detections . this approach enables us to send the detected ambiguous records to another discrimination method for a deeper investigation , thus increasing the accuracy by lowering the error rate . most of the records are still processed quickly and with low error rate which prevents performance loss compared to similar hybrid methods . experimental results of applying the proposed method on several datasets from different domains show a significant decrease in error rate as well as improved sensitivity of the algorithm .

compiling quantum circuits to realistic hardware architectures using temporal planners
to run quantum algorithms on emerging gate-model quantum hardware , quantum circuits must be compiled to take into account constraints on the hardware . for near-term hardware , with only limited means to mitigate decoherence , it is critical to minimize the duration of the circuit . we investigate the application of temporal planners to the problem of compiling quantum circuits to newly emerging quantum hardware . while our approach is general , we focus on compiling to superconducting hardware architectures with nearest neighbor constraints . our initial experiments focus on compiling quantum alternating operator ansatz ( qaoa ) circuits whose high number of commuting gates allow great flexibility in the order in which the gates can be applied . that freedom makes it more challenging to find optimal compilations but also means there is a greater potential win from more optimized compilation than for less flexible circuits . we map this quantum circuit compilation problem to a temporal planning problem , and generated a test suite of compilation problems for qaoa circuits of various sizes to a realistic hardware architecture . we report compilation results from several state-of-the-art temporal planners on this test set . this early empirical evaluation demonstrates that temporal planning is a viable approach to quantum circuit compilation .

on the implementation of gnu prolog
gnu prolog is a general-purpose implementation of the prolog language , which distinguishes itself from most other systems by being , above all else , a native-code compiler which produces standalone executables which do n't rely on any byte-code emulator or meta-interpreter . other aspects which stand out include the explicit organization of the prolog system as a multipass compiler , where intermediate representations are materialized , in unix compiler tradition . gnu prolog also includes an extensible and high-performance finite domain constraint solver , integrated with the prolog language but implemented using independent lower-level mechanisms . this article discusses the main issues involved in designing and implementing gnu prolog : requirements , system organization , performance and portability issues as well as its position with respect to other prolog system implementations and the iso standardization initiative .

a multi-gene genetic programming application for predicting students failure at school
several efforts to predict student failure rate ( sfr ) at school accurately still remains a core problem area faced by many in the educational sector . the procedure for forecasting sfr are rigid and most often times require data scaling or conversion into binary form such as is the case of the logistic model which may lead to lose of information and effect size attenuation . also , the high number of factors , incomplete and unbalanced dataset , and black boxing issues as in artificial neural networks and fuzzy logic systems exposes the need for more efficient tools . currently the application of genetic programming ( gp ) holds great promises and has produced tremendous positive results in different sectors . in this regard , this study developed gpsfarps , a software application to provide a robust solution to the prediction of sfr using an evolutionary algorithm known as multi-gene genetic programming . the approach is validated by feeding a testing data set to the evolved gp models . result obtained from gpsfarps simulations show its unique ability to evolve a suitable failure rate expression with a fast convergence at 30 generations from a maximum specified generation of 500. the multi-gene system was also able to minimize the evolved model expression and accurately predict student failure rate using a subset of the original expression

argument-based belief in topological structures
this paper combines two studies : a topological semantics for epistemic notions and abstract argumentation theory . in our combined setting , we use a topological semantics to represent the structure of an agent 's collection of evidence , and we use argumentation theory to single out the relevant sets of evidence through which a notion of beliefs grounded on arguments is defined . we discuss the formal properties of this newly defined notion , providing also a formal language with a matching modality together with a sound and complete axiom system for it . despite the fact that our agent can combine her evidence in a 'rational ' way ( captured via the topological structure ) , argument-based beliefs are not closed under conjunction . this illustrates the difference between an agent 's reasoning abilities ( i.e . the way she is able to combine her available evidence ) and the closure properties of her beliefs . we use this point to argue for why the failure of closure under conjunction of belief should not bear the burden of the failure of rationality .

solution methods for constrained markov decision process with continuous probability modulation
we propose solution methods for previously-unsolved constrained mdps in which actions can continuously modify the transition probabilities within some acceptable sets . while many methods have been proposed to solve regular mdps with large state sets , there are few practical approaches for solving constrained mdps with large action sets . in particular , we show that the continuous action sets can be replaced by their extreme points when the rewards are linear in the modulation . we also develop a tractable optimization formulation for concave reward functions and , surprisingly , also extend it to non- concave reward functions by using their concave envelopes . we evaluate the effectiveness of the approach on the problem of managing delinquencies in a portfolio of loans .

reasoning about beliefs and actions under computational resource constraints
although many investigators affirm a desire to build reasoning systems that behave consistently with the axiomatic basis defined by probability theory and utility theory , limited resources for engineering and computation can make a complete normative analysis impossible . we attempt to move discussion beyond the debate over the scope of problems that can be handled effectively to cases where it is clear that there are insufficient computational resources to perform an analysis deemed as complete . under these conditions , we stress the importance of considering the expected costs and benefits of applying alternative approximation procedures and heuristics for computation and knowledge acquisition . we discuss how knowledge about the structure of user utility can be used to control value tradeoffs for tailoring inference to alternative contexts . we address the notion of real-time rationality , focusing on the application of knowledge about the expected timewise-refinement abilities of reasoning strategies to balance the benefits of additional computation with the costs of acting with a partial result . we discuss the benefits of applying decision theory to control the solution of difficult problems given limitations and uncertainty in reasoning resources .

transfer learning , soft distance-based bias , and the hierarchical boa
an automated technique has recently been proposed to transfer learning in the hierarchical bayesian optimization algorithm ( hboa ) based on distance-based statistics . the technique enables practitioners to improve hboa efficiency by collecting statistics from probabilistic models obtained in previous hboa runs and using the obtained statistics to bias future hboa runs on similar problems . the purpose of this paper is threefold : ( 1 ) test the technique on several classes of np-complete problems , including maxsat , spin glasses and minimum vertex cover ; ( 2 ) demonstrate that the technique is effective even when previous runs were done on problems of different size ; ( 3 ) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often yield nearly multiplicative speedups .

artificial intelligence and pediatrics : a synthetic mini review
the use of artificial intelligence intelligencein medicine can be traced back to 1968 when paycha published his paper le diagnostic a l'aide d'intelligences artificielle , presentation de la premiere machine diagnostri . few years later shortliffe et al . presented an expert system named mycin which was able to identify bacteria causing severe blood infections and to recommend antibiotics . despite the fact that mycin outperformed members of the stanford medical school in the reliability of diagnosis it was never used in practice due to a legal issue who do you sue if it gives a wrong diagnosis ? . however only in 2016 when the artificial intelligence software built into the ibm watson ai platform correctly diagnosed and proposed an effective treatment for a 60-year-old womans rare form of leukemia the ai use in medicine become really popular.on of first papers presenting the use of ai in paediatrics was published in 1984. the paper introduced a computer-assisted medical decision making system called shelp .

approximate decomposition : a method for bounding and estimating probabilistic and deterministic queries
in this paper , we introduce a method for approximating the solution to inference and optimization tasks in uncertain and deterministic reasoning . such tasks are in general intractable for exact algorithms because of the large number of dependency relationships in their structure . our method effectively maps such a dense problem to a sparser one which is in some sense `` closest '' . exact methods can be run on the sparser problem to derive bounds on the original answer , which can be quite sharp . we present empirical results demonstrating that our method works well on the tasks of belief inference and finding the probability of the most probable explanation in belief networks , and finding the cost of the solution that violates the smallest number of constraints in constraint satisfaction problems . on one large cpcs network , for example , we were able to calculate upper and lower bounds on the conditional probability of a variable , given evidence , that were almost identical in the average case .

how controlled english can improve semantic wikis
the motivation of semantic wikis is to make acquisition , maintenance , and mining of formal knowledge simpler , faster , and more flexible . however , most existing semantic wikis have a very technical interface and are restricted to a relatively low level of expressivity . in this paper , we explain how acewiki uses controlled english - concretely attempto controlled english ( ace ) - to provide a natural and intuitive interface while supporting a high degree of expressivity . we introduce recent improvements of the acewiki system and user studies that indicate that acewiki is usable and useful .

bayes networks for sonar sensor fusion
wide-angle sonar mapping of the environment by mobile robot is nontrivial due to several sources of uncertainty : dropouts due to `` specular '' reflections , obstacle location uncertainty due to the wide beam , and distance measurement error . earlier papers address the latter problems , but dropouts remain a problem in many environments . we present an approach that lifts the overoptimistic independence assumption used in earlier work , and use bayes nets to represent the dependencies between objects of the model . objects of the model consist of readings , and of regions in which `` quasi location invariance '' of the ( possible ) obstacles exists , with respect to the readings . simulation supports the method 's feasibility . the model is readily extensible to allow for prior distributions , as well as other types of sensing operations .

bayesian inference with posterior regularization and applications to infinite latent svms
existing bayesian models , especially nonparametric bayesian methods , rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations . while priors can affect posterior distributions through bayes ' rule , imposing posterior regularization is arguably more direct and in some cases more natural and general . in this paper , we present regularized bayesian inference ( regbayes ) , a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation . regbayes is more flexible than the procedure that elicits expert knowledge via priors , and it covers both directed bayesian networks and undirected markov networks whose bayesian formulation results in hybrid chain graph models . when the regularization is induced from a linear operator on the posterior distributions , such as the expectation operator , we present a general convex-analysis theorem to characterize the solution of regbayes . furthermore , we present two concrete examples of regbayes , infinite latent support vector machines ( ilsvm ) and multi-task infinite latent support vector machines ( mt-ilsvm ) , which explore the large-margin idea in combination with a nonparametric bayesian model for discovering predictive latent features for classification and multi-task learning , respectively . we present efficient inference methods and report empirical studies on several benchmark datasets , which appear to demonstrate the merits inherited from both large-margin learning and bayesian nonparametrics . such results were not available until now , and contribute to push forward the interface between these two important subfields , which have been largely treated as isolated in the community .

least generalizations and greatest specializations of sets of clauses
the main operations in inductive logic programming ( ilp ) are generalization and specialization , which only make sense in a generality order . in ilp , the three most important generality orders are subsumption , implication and implication relative to background knowledge . the two languages used most often are languages of clauses and languages of only horn clauses . this gives a total of six different ordered languages . in this paper , we give a systematic treatment of the existence or non-existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets . we survey results already obtained by others and also contribute some answers of our own . our main new results are , firstly , the existence of a computable least generalization under implication of every finite set of clauses containing at least one non-tautologous function-free clause ( among other , not necessarily function-free clauses ) . secondly , we show that such a least generalization need not exist under relative implication , not even if both the set that is to be generalized and the background knowledge are function-free . thirdly , we give a complete discussion of existence and non-existence of greatest specializations in each of the six ordered languages .

possibilistic pertinence feedback and semantic networks for goal 's extraction
pertinence feedback is a technique that enables a user to interactively express his information requirement by modifying his original query formulation with further information . this information is provided by explicitly confirming the pertinent of some indicating objects and/or goals extracted by the system . obviously the user can not mark objects and/or goals as pertinent until some are extracted , so the first search has to be initiated by a query and the initial query specification has to be good enough to pick out some pertinent objects and/or goals from the semantic network . in this paper we present a short survey of fuzzy and semantic approaches to knowledge extraction . the goal of such approaches is to define flexible knowledge extraction systems able to deal with the inherent vagueness and uncertainty of the extraction process . it has long been recognised that interactivity improves the effectiveness of knowledge extraction systems . novice user 's queries are the most natural and interactive medium of communication and recent progress in recognition is making it possible to build systems that interact with the user . however , given the typical novice user 's queries submitted to knowledge extraction systems , it is easy to imagine that the effects of goal recognition errors in novice user 's queries must be severely destructive on the system 's effectiveness . the experimental work reported in this paper shows that the use of possibility theory in classical knowledge extraction techniques for novice user 's query processing is more robust than the use of the probability theory . moreover , both possibilistic and probabilistic pertinence feedback can be effectively employed to improve the effectiveness of novice user 's query processing .

chess pure strategies are probably chaotic
it is odd that chess grandmasters often disagree in their analysis of positions , sometimes even of simple ones , and that a grandmaster can hold his own against an powerful analytic machine such as deep blue . the fact that there must exist pure winning strategies for chess is used to construct a control strategy function . it is then shown that chess strategy is equivalent to an autonomous system of differential equations , and conjectured that the system is chaotic . if true the conjecture would explain the forenamed peculiarities and would also imply that there can not exist a static evaluator for chess .

markov logic in infinite domains
combining first-order logic and probability has long been a goal of ai . markov logic ( richardson & domingos , 2006 ) accomplishes this by attaching weights to first-order formulas and viewing them as templates for features of markov networks . unfortunately , it does not have the full power of first-order logic , because it is only defined for finite domains . this paper extends markov logic to infinite domains , by casting it in the framework of gibbs measures ( georgii , 1988 ) . we show that a markov logic network ( mln ) admits a gibbs measure as long as each ground atom has a finite number of neighbors . many interesting cases fall in this category . we also show that an mln admits a unique measure if the weights of its non-unit clauses are small enough . we then examine the structure of the set of consistent measures in the non-unique case . many important phenomena , including systems with phase transitions , are represented by mlns with non-unique measures . we relate the problem of satisfiability in first-order logic to the properties of mln measures , and discuss how markov logic relates to previous infinite models .

exploring bayesian models for multi-level clustering of hierarchically grouped sequential data
a wide range of bayesian models have been proposed for data that is divided hierarchically into groups . these models aim to cluster the data at different levels of grouping , by assigning a mixture component to each datapoint , and a mixture distribution to each group . multi-level clustering is facilitated by the sharing of these components and distributions by the groups . in this paper , we introduce the concept of degree of sharing ( dos ) for the mixture components and distributions , with an aim to analyze and classify various existing models . next we introduce a generalized hierarchical bayesian model , of which the existing models can be shown to be special cases . unlike most of these models , our model takes into account the sequential nature of the data , and various other temporal structures at different levels while assigning mixture components and distributions . we show one specialization of this model aimed at hierarchical segmentation of news transcripts , and present a gibbs sampling based inference algorithm for it . we also show experimentally that the proposed model outperforms existing models for the same task .

a neural network and iterative optimization hybrid for dempster-shafer clustering
in this paper we extend an earlier result within dempster-shafer theory [ `` fast dempster-shafer clustering using a neural network structure , '' in proc . seventh int . conf . information processing and management of uncertainty in knowledge-based systems ( ipmu 98 ) ] where a large number of pieces of evidence are clustered into subsets by a neural network structure . the clustering is done by minimizing a metaconflict function . previously we developed a method based on iterative optimization . while the neural method had a much lower computation time than iterative optimization its average clustering performance was not as good . here , we develop a hybrid of the two methods . we let the neural structure do the initial clustering in order to achieve a high computational performance . its solution is fed as the initial state to the iterative optimization in order to improve the clustering performance .

properties of bayesian belief network learning algorithms
bayesian belief network learning algorithms have three basic components : a measure of a network structure and a database , a search heuristic that chooses network structures to be considered , and a method of estimating the probability tables from the database . this paper contributes to all these three topics . the behavior of the bayesian measure of cooper and herskovits and a minimum description length ( mdl ) measure are compared with respect to their properties for both limiting size and finite size databases . it is shown that the mdl measure has more desirable properties than the bayesian measure when a distribution is to be learned . it is shown that selecting belief networks with certain minimallity properties is np-hard . this result justifies the use of search heuristics instead of exact algorithms for choosing network structures to be considered . in some cases , a collection of belief networks can be represented by a single belief network which leads to a new kind of probability table estimation called smoothing . we argue that smoothing can be efficiently implemented by incorporating it in the search heuristic . experimental results suggest that for learning probabilities of belief networks smoothing is helpful .

constraint solvers : an empirical evaluation of design decisions
this paper presents an evaluation of the design decisions made in four state-of-the-art constraint solvers ; choco , eclipse , gecode , and minion . to assess the impact of design decisions , instances of the five problem classes n-queens , golomb ruler , magic square , social golfers , and balanced incomplete block design are modelled and solved with each solver . the results of the experiments are not meant to give an indication of the performance of a solver , but rather investigate what influence the choice of algorithms and data structures has . the analysis of the impact of the design decisions focuses on the different ways of memory management , behaviour with increasing problem size , and specialised algorithms for specific types of variables . it also briefly considers other , less significant decisions .

effect of tuned parameters on a lsa mcq answering model
this paper presents the current state of a work in progress , whose objective is to better understand the effects of factors that significantly influence the performance of latent semantic analysis ( lsa ) . a difficult task , which consists in answering ( french ) biology multiple choice questions , is used to test the semantic properties of the truncated singular space and to study the relative influence of main parameters . a dedicated software has been designed to fine tune the lsa semantic space for the multiple choice questions task . with optimal parameters , the performances of our simple model are quite surprisingly equal or superior to those of 7th and 8th grades students . this indicates that semantic spaces were quite good despite their low dimensions and the small sizes of training data sets . besides , we present an original entropy global weighting of answers ' terms of each question of the multiple choice questions which was necessary to achieve the model 's success .

stochastic patching process
stochastic partition models tailor a product space into a number of rectangular regions such that the data within each region exhibit certain types of homogeneity . due to constraints of partition strategy , existing models may cause unnecessary dissections in sparse regions when fitting data in dense regions . to alleviate this limitation , we propose a parsimonious partition model , named stochastic patching process ( spp ) , to deal with multi-dimensional arrays . spp adopts an `` enclosing '' strategy to attach rectangular patches to dense regions . spp is self-consistent such that it can be extended to infinite arrays . we apply spp to relational modeling and the experimental results validate its merit compared to the state-of-the-arts .

r-extreme signalling for congestion control
in many `` smart city '' applications , congestion arises in part due to the nature of signals received by individuals from a central authority . in the model of marecek et al . [ arxiv:1406.7639 , int . j. control 88 ( 10 ) , 2015 ] , each agent uses one out of multiple resources at each time instant . the per-use cost of a resource depends on the number of concurrent users . a central authority has up-to-date knowledge of the congestion across all resources and uses randomisation to provide a scalar or an interval for each resource at each time . in this paper , the interval to broadcast per resource is obtained by taking the minima and maxima of costs observed within a time window of length r , rather than by randomisation . we show that the resulting distribution of agents across resources also converges in distribution , under plausible assumptions about the evolution of the population over time .

bayesian network learning by compiling to weighted max-sat
the problem of learning discrete bayesian networks from data is encoded as a weighted max-sat problem and the maxwalksat local search algorithm is used to address it . for each dataset , the per-variable summands of the ( bdeu ) marginal likelihood for different choices of parents ( 'family scores ' ) are computed prior to applying maxwalksat . each permissible choice of parents for each variable is encoded as a distinct propositional atom and the associated family score encoded as a 'soft ' weighted single-literal clause . two approaches to enforcing acyclicity are considered : either by encoding the ancestor relation or by attaching a total order to each graph and encoding that . the latter approach gives better results . learning experiments have been conducted on 21 synthetic datasets sampled from 7 bns . the largest dataset has 10,000 datapoints and 60 variables producing ( for the 'ancestor ' encoding ) a weighted cnf input file with 19,932 atoms and 269,367 clauses . for most datasets , maxwalksat quickly finds bns with higher bdeu score than the 'true ' bn . the effect of adding prior information is assessed . it is further shown that bayesian model averaging can be effected by collecting bns generated during the search .

implementation of continuous bayesian networks using sums of weighted gaussians
bayesian networks provide a method of representing conditional independence between random variables and computing the probability distributions associated with these random variables . in this paper , we extend bayesian network structures to compute probability density functions for continuous random variables . we make this extension by approximating prior and conditional densities using sums of weighted gaussian distributions and then finding the propagation rules for updating the densities in terms of these weights . we present a simple example that illustrates the bayesian network for continuous variables ; this example shows the effect of the network structure and approximation errors on the computation of densities for variables in the network .

dlv - a system for declarative problem solving
dlv is an efficient logic programming and non-monotonic reasoning ( lpnmr ) system with advanced knowledge representation mechanisms and interfaces to classic relational database systems . its core language is disjunctive datalog ( function-free disjunctive logic programming ) under the answer set semantics with integrity constraints , both default and strong ( or explicit ) negation , and queries . integer arithmetics and various built-in predicates are also supported . in addition dlv has several frontends , namely brave and cautious reasoning , abductive diagnosis , consistency-based diagnosis , a subset of sql3 , planning with action languages , and logic programming with inheritance .

adaptive submodular optimization under matroid constraints
many important problems in discrete optimization require maximization of a monotonic submodular function subject to matroid constraints . for these problems , a simple greedy algorithm is guaranteed to obtain near-optimal solutions . in this article , we extend this classic result to a general class of adaptive optimization problems under partial observability , where each choice can depend on observations resulting from past choices . specifically , we prove that a natural adaptive greedy algorithm provides a $ 1/ ( p+1 ) $ approximation for the problem of maximizing an adaptive monotone submodular function subject to $ p $ matroid constraints , and more generally over arbitrary $ p $ -independence systems . we illustrate the usefulness of our result on a complex adaptive match-making application .

compositional distributional cognition
we accommodate the integrated connectionist/symbolic architecture ( ics ) of [ 32 ] within the categorical compositional semantics ( catco ) of [ 13 ] , forming a model of categorical compositional cognition ( catcog ) . this resolves intrinsic problems with ics such as the fact that representations inhabit an unbounded space and that sentences with differing tree structures can not be directly compared . we do so in a way that makes the most of the grammatical structure available , in contrast to strategies like circular convolution . using the catco model also allows us to make use of tools developed for catco such as the representation of ambiguity and logical reasoning via density matrices , structural meanings for words such as relative pronouns , and addressing over- and under-extension , all of which are present in cognitive processes . moreover the catcog framework is sufficiently flexible to allow for entirely different representations of meaning , such as conceptual spaces . interestingly , since the catco model was largely inspired by categorical quantum mechanics , so is catcog .

loss-sensitive training of probabilistic conditional random fields
we consider the problem of training probabilistic conditional random fields ( crfs ) in the context of a task where performance is measured using a specific loss function . while maximum likelihood is the most common approach to training crfs , it ignores the inherent structure of the task 's loss function . we describe alternatives to maximum likelihood which take that loss into account . these include a novel adaptation of a loss upper bound from the structured svms literature to the crf context , as well as a new loss-inspired kl divergence objective which relies on the probabilistic nature of crfs . these loss-sensitive objectives are compared to maximum likelihood using ranking as a benchmark task . this comparison confirms the importance of incorporating loss information in the probabilistic training of crfs , with the loss-inspired kl outperforming all other objectives .

electricity demand and energy consumption management system
this project describes the electricity demand and energy consumption management system and its application to southern peru smelter . it is composed of an hourly demand-forecasting module and of a simulation component for a plant electrical system . the first module was done using dynamic neural networks with backpropagation training algorithm ; it is used to predict the electric power demanded every hour , with an error percentage below of 1 % . this information allows efficient management of energy peak demands before this happen , distributing the raise of electric load to other hours or improving those equipments that increase the demand . the simulation module is based in advanced estimation techniques , such as : parametric estimation , neural network modeling , statistic regression and previously developed models , which simulates the electric behavior of the smelter plant . these modules facilitate electricity demand and consumption proper planning , because they allow knowing the behavior of the hourly demand and the consumption patterns of the plant , including the bill components , but also energy deficiencies and opportunities for improvement , based on analysis of information about equipments , processes and production plans , as well as maintenance programs . finally the results of its application in southern peru smelter are presented .

learning bounded treewidth bayesian networks with thousands of variables
we present a method for learning treewidth-bounded bayesian networks from data sets containing thousands of variables . bounding the treewidth of a bayesian greatly reduces the complexity of inferences . yet , being a global property of the graph , it considerably increases the difficulty of the learning process . we propose a novel algorithm for this task , able to scale to large domains and large treewidths . our novel approach consistently outperforms the state of the art on data sets with up to ten thousand variables .

a hierarchy of tractable subsets for computing stable models
finding the stable models of a knowledge base is a significant computational problem in artificial intelligence . this task is at the computational heart of truth maintenance systems , autoepistemic logic , and default logic . unfortunately , it is np-hard . in this paper we present a hierarchy of classes of knowledge bases , omega_1 , omega_2 , ... , with the following properties : first , omega_1 is the class of all stratified knowledge bases ; second , if a knowledge base pi is in omega_k , then pi has at most k stable models , and all of them may be found in time o ( lnk ) , where l is the length of the knowledge base and n the number of atoms in pi ; third , for an arbitrary knowledge base pi , we can find the minimum k such that pi belongs to omega_k in time polynomial in the size of pi ; and , last , where k is the class of all knowledge bases , it is the case that union { i=1 to infty } omega_i = k , that is , every knowledge base belongs to some class in the hierarchy .

an efficient training algorithm for kernel survival support vector machines
survival analysis is a fundamental tool in medical research to identify predictors of adverse events and develop systems for clinical decision support . in order to leverage large amounts of patient data , efficient optimisation routines are paramount . we propose an efficient training algorithm for the kernel survival support vector machine ( ssvm ) . we directly optimise the primal objective function and employ truncated newton optimisation and order statistic trees to significantly lower computational costs compared to previous training algorithms , which require $ o ( n^4 ) $ space and $ o ( p n^6 ) $ time for datasets with $ n $ samples and $ p $ features . our results demonstrate that our proposed optimisation scheme allows analysing data of a much larger scale with no loss in prediction performance . experiments on synthetic and 5 real-world datasets show that our technique outperforms existing kernel ssvm formulations if the amount of right censoring is high ( $ \geq85\ % $ ) , and performs comparably otherwise .

reachable set computation and safety verification for neural networks with relu activations
neural networks have been widely used to solve complex real-world problems . due to the complicate , nonlinear , non-convex nature of neural networks , formal safety guarantees for the output behaviors of neural networks will be crucial for their applications in safety-critical systems.in this paper , the output reachable set computation and safety verification problems for a class of neural networks consisting of rectified linear unit ( relu ) activation functions are addressed . a layer-by-layer approach is developed to compute output reachable set . the computation is formulated in the form of a set of manipulations for a union of polyhedra , which can be efficiently applied with the aid of polyhedron computation tools . based on the output reachable set computation results , the safety verification for a relu neural network can be performed by checking the intersections of unsafe regions and output reachable set described by a union of polyhedra . a numerical example of a randomly generated relu neural network is provided to show the effectiveness of the approach developed in this paper .

shedding light on the asymmetric learning capability of adaboost
in this paper , we propose a different insight to analyze adaboost . this analysis reveals that , beyond some preconceptions , adaboost can be directly used as an asymmetric learning algorithm , preserving all its theoretical properties . a novel class-conditional description of adaboost , which models the actual asymmetric behavior of the algorithm , is presented .

unsupervised grammar induction with depth-bounded pcfg
there has been recent interest in applying cognitively or empirically motivated bounds on recursion depth to limit the search space of grammar induction models ( ponvert et al. , 2011 ; noji and johnson , 2016 ; shain et al. , 2016 ) . this work extends this depth-bounding approach to probabilistic context-free grammar induction ( db-pcfg ) , which has a smaller parameter space than hierarchical sequence models , and therefore more fully exploits the space reductions of depth-bounding . results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy . moreover , gram- mars acquired from this model demonstrate a consistent use of category labels , something which has not been demonstrated by other acquisition models .

acewiki : a natural and expressive semantic wiki
we present acewiki , a prototype of a new kind of semantic wiki using the controlled natural language attempto controlled english ( ace ) for representing its content . ace is a subset of english with a restricted grammar and a formal semantics . the use of ace has two important advantages over existing semantic wikis . first , we can improve the usability and achieve a shallow learning curve . second , ace is more expressive than the formal languages of existing semantic wikis . our evaluation shows that people who are not familiar with the formal foundations of the semantic web are able to deal with acewiki after a very short learning phase and without the help of an expert .

general-purpose mcmc inference over relational structures
tasks such as record linkage and multi-target tracking , which involve reconstructing the set of objects that underlie some observed data , are particularly challenging for probabilistic inference . recent work has achieved efficient and accurate inference on such problems using markov chain monte carlo ( mcmc ) techniques with customized proposal distributions . currently , implementing such a system requires coding mcmc state representations and acceptance probability calculations that are specific to a particular application . an alternative approach , which we pursue in this paper , is to use a general-purpose probabilistic modeling language ( such as blog ) and a generic metropolis-hastings mcmc algorithm that supports user-supplied proposal distributions . our algorithm gains flexibility by using mcmc states that are only partial descriptions of possible worlds ; we provide conditions under which mcmc over partial worlds yields correct answers to queries . we also show how to use a context-specific bayes net to identify the factors in the acceptance probability that need to be computed for a given proposed move . experimental results on a citation matching task show that our general-purpose mcmc engine compares favorably with an application-specific system .

complexity results and practical algorithms for logics in knowledge representation
description logics ( dls ) are used in knowledge-based systems to represent and reason about terminological knowledge of the application domain in a semantically well-defined manner . in this thesis , we establish a number of novel complexity results and give practical algorithms for expressive dls that provide different forms of counting quantifiers . we show that , in many cases , adding local counting in the form of qualifying number restrictions to dls does not increase the complexity of the inference problems , even if binary coding of numbers in the input is assumed . on the other hand , we show that adding different forms of global counting restrictions to a logic may increase the complexity of the inference problems dramatically . we provide exact complexity results and a practical , tableau based algorithm for the dl shiq , which forms the basis of the highly optimized dl system ifact . finally , we describe a tableau algorithm for the clique guarded fragment ( cgf ) , which we hope will serve as the basis for an efficient implementation of a cgf reasoner .

latent belief theory and belief dependencies : a solution to the recovery problem in the belief set theories
the agm recovery postulate says : assume a set of propositions x ; assume that it is consistent and that it is closed under logical consequences ; remove a belief p from the set minimally , but make sure that the resultant set is again some set of propositions x ' which is closed under the logical consequences ; now add p again and close the set under the logical consequences ; and we should get a set of propositions that contains all the propositions that were in x. this postulate has since met objections ; many have observed that it could bear counter-intuitive results . nevertheless , the attempts that have been made so far to amend it either recovered the postulate in full , had to relinquish the assumption of the logical closure altogether , or else had to introduce fresh controversies of their own . we provide a solution to the recovery paradox in this work . our theoretical basis is the recently proposed belief theory with latent beliefs ( simply the latent belief theory for short ) . firstly , through examples , we will illustrate that the vanilla latent belief theory can be made more expressive . we will identify that a latent belief , when it becomes visible , may remain visible only while the beliefs that triggered it into the agent 's consciousness are in the agent 's belief set . in order that such situations can be also handled , we will enrich the latent belief theory with belief dependencies among attributive beliefs , recording the information as to which belief is supported of its existence by which beliefs . we will show that the enriched latent belief theory does not possess the recovery property . the closure by logical consequences is maintained in the theory , however . hence it serves as a solution to the open problem in the belief set theories .

how the landscape of random job shop scheduling instances depends on the ratio of jobs to machines
we characterize the search landscape of random instances of the job shop scheduling problem ( jsp ) . specifically , we investigate how the expected values of ( 1 ) backbone size , ( 2 ) distance between near-optimal schedules , and ( 3 ) makespan of random schedules vary as a function of the job to machine ratio ( n/m ) . for the limiting cases n/m approaches 0 and n/m approaches infinity we provide analytical results , while for intermediate values of n/m we perform experiments . we prove that as n/m approaches 0 , backbone size approaches 100 % , while as n/m approaches infinity the backbone vanishes . in the process we show that as n/m approaches 0 ( resp . n/m approaches infinity ) , simple priority rules almost surely generate an optimal schedule , providing theoretical evidence of an `` easy-hard-easy '' pattern of typical-case instance difficulty in job shop scheduling . we also draw connections between our theoretical results and the `` big valley '' picture of jsp landscapes .

memory enriched big bang big crunch optimization algorithm for data clustering
cluster analysis plays an important role in decision making process for many knowledge-based systems . there exist a wide variety of different approaches for clustering applications including the heuristic techniques , probabilistic models , and traditional hierarchical algorithms . in this paper , a novel heuristic approach based on big bang-big crunch algorithm is proposed for clustering problems . the proposed method not only takes advantage of heuristic nature to alleviate typical clustering algorithms such as k-means , but it also benefits from the memory based scheme as compared to its similar heuristic techniques . furthermore , the performance of the proposed algorithm is investigated based on several benchmark test functions as well as on the well-known datasets . the experimental results show the significant superiority of the proposed method over the similar algorithms .

metacognitive learning approach for online tool condition monitoring
as manufacturing processes become increasingly automated , so should tool condition monitoring ( tcm ) as it is impractical to have human workers monitor the state of the tools continuously . tool condition is crucial to ensure the good quality of products : worn tools affect not only the surface quality but also the dimensional accuracy , which means higher reject rate of the products . therefore , there is an urgent need to identify tool failures before it occurs on the fly . while various versions of intelligent tool condition monitoring have been proposed , most of them suffer from a cognitive nature of traditional machine learning algorithms . they focus on the how to learn process without paying attention to other two crucial issues : what to learn , and when to learn . the what to learn and the when to learn provide self regulating mechanisms to select the training samples and to determine time instants to train a model . a novel tool condition monitoring approach based on a psychologically plausible concept , namely the metacognitive scaffolding theory , is proposed and built upon a recently published algorithm , recurrent classifier ( rclass ) . the learning process consists of three phases : what to learn , how to learn , when to learn and makes use of a generalized recurrent network structure as a cognitive component . experimental studies with real-world manufacturing data streams were conducted where rclass demonstrated the highest accuracy while retaining the lowest complexity over its counterparts .

fast k-nearest neighbour search via dynamic continuous indexing
existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality . we argue this is caused in part by inherent deficiencies of space partitioning , which is the underlying strategy used by most existing methods . we devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset . the proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis , automatically adapts to variations in data density , supports dynamic updates to the dataset and is easy-to-implement . we show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing ( lsh ) in terms of approximation quality , speed and space efficiency .

a new approach to draw detection by move repetition in computer chess programming
we will try to tackle both the theoretical and practical aspects of a very important problem in chess programming as stated in the title of this article - the issue of draw detection by move repetition . the standard approach that has so far been employed in most chess programs is based on utilising positional matrices in original and compressed format as well as on the implementation of the so-called bitboard format . the new approach that we will be trying to introduce is based on using variant strings generated by the search algorithm ( searcher ) during the tree expansion in decision making . we hope to prove that this approach is more efficient than the standard treatment of the issue , especially in positions with few pieces ( endgames ) . to illustrate what we have in mind a machine language routine that implements our theoretical assumptions is attached . the routine is part of the axon chess program , developed by the authors . axon , in its current incarnation , plays chess at master strength ( ca . 2400-2450 elo , based on both axon vs computer programs and axon vs human masters in over 3000 games altogether ) .

high throughput virtual screening with data level parallelism in multi-core processors
improving the throughput of molecular docking , a computationally intensive phase of the virtual screening process , is a highly sought area of research since it has a significant weight in the drug designing process . with such improvements , the world might find cures for incurable diseases like hiv disease and cancer sooner . our approach presented in this paper is to utilize a multi-core environment to introduce data level parallelism ( dlp ) to the autodock vina software , which is a widely used for molecular docking software . autodock vina already exploits instruction level parallelism ( ilp ) in multi-core environments and therefore optimized for such environments . however , with the results we have obtained , it can be clearly seen that our approach has enhanced the throughput of the already optimized software by more than six times . this will dramatically reduce the time consumed for the lead identification phase in drug designing along with the shift in the processor technology from multi-core to many-core of the current era . therefore , we believe that the contribution of this project will effectively make it possible to expand the number of small molecules docked against a drug target and improving the chances to design drugs for incurable diseases .

predictive modelling of football injuries
the goal of this thesis is to investigate the potential of predictive modelling for football injuries . this work was conducted in close collaboration with tottenham hotspurs fc ( thfc ) , the pga european tour and the participation of wolverhampton wanderers ( ww ) . three investigations were conducted : 1. predicting the recovery time of football injuries using the uefa injury recordings : the uefa recordings is a common standard for recording injuries in professional football . for this investigation , three datasets of uefa injury recordings were available . different machine learning algorithms were used in order to build a predictive model . the performance of the machine learning models is then improved by using feature selection conducted through correlation-based subset feature selection and random forests . 2. predicting injuries in professional football using exposure records : the relationship between exposure ( in training hours and match hours ) in professional football athletes and injury incidence was studied . a common problem in football is understanding how the training schedule of an athlete can affect the chance of him getting injured . the task was to predict the number of days a player can train before he gets injured . 3. predicting intrinsic injury incidence using in-training gps measurements : a significant percentage of football injuries can be attributed to overtraining and fatigue . gps data collected during training sessions might provide indicators of fatigue , or might be used to detect very intense training sessions which can lead to overtraining . this research used gps data gathered during training sessions of the first team of thfc , in order to predict whether an injury would take place during a week .

mura dataset : towards radiologist-level abnormality detection in musculoskeletal radiographs
we introduce mura , a large dataset of musculoskeletal radiographs containing 40,895 images from 14,982 studies , where each study is manually labeled by radiologists as either normal or abnormal . on this dataset , we train a 169-layer densely connected convolutional network to detect and localize abnormalities . to evaluate our model robustly and to get an estimate of radiologist performance , we collect additional labels from board-certified stanford radiologists on the test set , consisting of 209 musculoskeletal studies . we compared our model and radiologists on the cohen 's kappa statistic , which expresses the agreement of our model and of each radiologist with the gold standard , defined as the majority vote of a disjoint group of radiologists . we find that our model achieves performance comparable to that of radiologists . model performance is higher than the best radiologist performance in detecting abnormalities on finger studies and equivalent on wrist studies . however , model performance is lower than best radiologist performance in detecting abnormalities on elbow , forearm , hand , humerus , and shoulder studies , indicating that the task is a good challenge for future research . to encourage advances , we have made our dataset freely available at https : //stanfordmlgroup.github.io/projects/mura

a recommender system based on idiotypic artificial immune networks
the immune system is a complex biological system with a highly distributed , adaptive and self-organising nature . this paper presents an artificial immune system ( ais ) that exploits some of these characteristics and is applied to the task of film recommendation by collaborative filtering ( cf ) . natural evolution and in particular the immune system have not been designed for classical optimisation . however , for this problem , we are not interested in finding a single optimum . rather we intend to identify a sub-set of good matches on which recommendations can be based . it is our hypothesis that an ais built on two central aspects of the biological immune system will be an ideal candidate to achieve this : antigen-antibody interaction for matching and idiotypic antibody-antibody interaction for diversity . computational results are presented in support of this conjecture and compared to those found by other cf techniques .

stick-breaking policy learning in dec-pomdps
expectation maximization ( em ) has recently been shown to be an efficient algorithm for learning finite-state controllers ( fscs ) in large decentralized pomdps ( dec-pomdps ) . however , current methods use fixed-size fscs and often converge to maxima that are far from optimal . this paper considers a variable-size fsc to represent the local policy of each agent . these variable-size fscs are constructed using a stick-breaking prior , leading to a new framework called \emph { decentralized stick-breaking policy representation } ( dec-sbpr ) . this approach learns the controller parameters with a variational bayesian algorithm without having to assume that the dec-pomdp model is available . the performance of dec-sbpr is demonstrated on several benchmark problems , showing that the algorithm scales to large problems while outperforming other state-of-the-art methods .

composite task-completion dialogue policy learning via hierarchical deep reinforcement learning
building a dialogue agent to fulfill complex tasks , such as travel planning , is challenging because the agent has to learn to collectively complete multiple subtasks . for example , the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in . this paper addresses this challenge by formulating the task in the mathematical framework of options over markov decision processes ( mdps ) , and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales . the dialogue manager consists of : ( 1 ) a top-level dialogue policy that selects among subtasks or options , ( 2 ) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy , and ( 3 ) a global state tracker that helps ensure all cross-subtask constraints be satisfied . experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines , two based on handcrafted rules and the other based on flat deep reinforcement learning .

a brief network analysis of artificial intelligence publication
in this paper , we present an illustration to the history of artificial intelligence ( ai ) with a statistical analysis of publish since 1940. we collected and mined through the ieee publish data base to analysis the geological and chronological variance of the activeness of research in ai . the connections between different institutes are showed . the result shows that the leading community of ai research are mainly in the usa , china , the europe and japan . the key institutes , authors and the research hotspots are revealed . it is found that the research institutes in the fields like data mining , computer vision , pattern recognition and some other fields of machine learning are quite consistent , implying a strong interaction between the community of each field . it is also showed that the research of electronic engineering and industrial or commercial applications are very active in california . japan is also publishing a lot of papers in robotics . due to the limitation of data source , the result might be overly influenced by the number of published articles , which is to our best improved by applying network keynode analysis on the research community instead of merely count the number of publish .

parameter learning in prism programs with continuous random variables
probabilistic logic programming ( plp ) , exemplified by sato and kameya 's prism , poole 's icl , de raedt et al 's problog and vennekens et al 's lpad , combines statistical and logical knowledge representation and inference . inference in these languages is based on enumerative construction of proofs over logic programs . consequently , these languages permit very limited use of random variables with continuous distributions . in this paper , we extend prism with gaussian random variables and linear equality constraints , and consider the problem of parameter learning in the extended language . many statistical models such as finite mixture models and kalman filter can be encoded in extended prism . our em-based learning algorithm uses a symbolic inference procedure that represents sets of derivations without enumeration . this permits us to learn the distribution parameters of extended prism programs with discrete as well as gaussian variables . the learning algorithm naturally generalizes the ones used for prism and hybrid bayesian networks .

premise selection for mathematics by corpus analysis and kernel methods
smart premise selection is essential when using automated reasoning as a tool for large-theory formal proof development . a good method for premise selection in complex mathematical libraries is the application of machine learning to large corpora of proofs . this work develops learning-based premise selection in two ways . first , a newly available minimal dependency analysis of existing high-level formal mathematical proofs is used to build a large knowledge base of proof dependencies , providing precise data for atp-based re-verification and for training premise selection algorithms . second , a new machine learning algorithm for premise selection based on kernel methods is proposed and implemented . to evaluate the impact of both techniques , a benchmark consisting of 2078 large-theory mathematical problems is constructed , extending the older mptp challenge benchmark . the combined effect of the techniques results in a 50 % improvement on the benchmark over the vampire/sine state-of-the-art system for automated reasoning in large theories .

consistency techniques for flow-based projection-safe global cost functions in weighted constraint satisfaction
many combinatorial problems deal with preferences and violations , the goal of which is to find solutions with the minimum cost . weighted constraint satisfaction is a framework for modeling such problems , which consists of a set of cost functions to measure the degree of violation or preferences of different combinations of variable assignments . typical solution methods for weighted constraint satisfaction problems ( wcsps ) are based on branch-and-bound search , which are made practical through the use of powerful consistency techniques such as ac* , fdac* , edac* to deduce hidden cost information and value pruning during search . these techniques , however , are designed to be efficient only on binary and ternary cost functions which are represented in table form . in tackling many real-life problems , high arity ( or global ) cost functions are required . we investigate efficient representation scheme and algorithms to bring the benefits of the consistency techniques to also high arity cost functions , which are often derived from hard global constraints from classical constraint satisfaction . the literature suggests some global cost functions can be represented as flow networks , and the minimum cost flow algorithm can be used to compute the minimum costs of such networks in polynomial time . we show that naive adoption of this flow-based algorithmic method for global cost functions can result in a stronger form of null-inverse consistency . we further show how the method can be modified to handle cost projections and extensions to maintain generalized versions of ac* and fdac* for cost functions with more than two variables . similar generalization for the stronger edac* is less straightforward . we reveal the oscillation problem when enforcing edac* on cost functions sharing more than one variable . to avoid oscillation , we propose a weak version of edac* and generalize it to weak edgac* for non-binary cost functions . using various benchmarks involving the soft variants of hard global constraints alldifferent , gcc , same , and regular , empirical results demonstrate that our proposal gives improvements of up to an order of magnitude when compared with the traditional constraint optimization approach , both in terms of time and pruning .

mce reasoning in recursive causal networks
a probabilistic method of reasoning under uncertainty is proposed based on the principle of minimum cross entropy ( mce ) and concept of recursive causal model ( rcm ) . the dependency and correlations among the variables are described in a special language bndl ( belief networks description language ) . beliefs are propagated among the clauses of the bndl programs representing the underlying probabilistic distributions . bndl interpreters in both prolog and c has been developed and the performance of the method is compared with those of the others .

generalizing the dempster-shafer theory to fuzzy sets
with the desire to apply the dempster-shafer theory to complex real world problems where the evidential strength is often imprecise and vague , several attempts have been made to generalize the theory . however , the important concept in the d-s theory that the belief and plausibility functions are lower and upper probabilities is no longer preserved in these generalizations . in this paper , we describe a generalized theory of evidence where the degree of belief in a fuzzy set is obtained by minimizing the probability of the fuzzy set under the constraints imposed by a basic probability assignment . to formulate the probabilistic constraint of a fuzzy focal element , we decompose it into a set of consonant non-fuzzy focal elements . by generalizing the compatibility relation to a possibility theory , we are able to justify our generalization to dempster 's rule based on possibility distribution . our generalization not only extends the application of the d-s theory but also illustrates a way that probability theory and fuzzy set theory can be combined to deal with different kinds of uncertain information in ai systems .

robust logistic regression using shift parameters ( long version )
annotation errors can significantly hurt classifier performance , yet datasets are only growing noisier with the increased use of amazon mechanical turk and techniques like distant supervision that automatically generate labels . in this paper , we present a robust extension of logistic regression that incorporates the possibility of mislabelling directly into the objective . our model can be trained through nearly the same means as logistic regression , and retains its efficiency on high-dimensional datasets . through named entity recognition experiments , we demonstrate that our approach can provide a significant improvement over the standard model when annotation errors are present .

fast exact k-means , k-medians and bregman divergence clustering in 1d
the $ k $ -means clustering problem on $ n $ points is np-hard for any dimension $ d\ge 2 $ , however , for the 1d case there exist exact polynomial time algorithms . previous literature reported an $ o ( kn^2 ) $ time dynamic programming algorithm that uses $ o ( kn ) $ space . we present a new algorithm computing the optimal clustering in only $ o ( kn ) $ time using linear space . for $ k = \omega ( \lg n ) $ , we improve this even further to $ n 2^ { o ( \sqrt { \lg \lg n \lg k } ) } $ time . we generalize the new algorithm ( s ) to work for the absolute distance instead of squared distance and to work for any bregman divergence as well .

conservative inference rule for uncertain reasoning under incompleteness
in this paper we formulate the problem of inference under incomplete information in very general terms . this includes modelling the process responsible for the incompleteness , which we call the incompleteness process . we allow the process behaviour to be partly unknown . then we use walleys theory of coherent lower previsions , a generalisation of the bayesian theory to imprecision , to derive the rule to update beliefs under incompleteness that logically follows from our assumptions , and that we call conservative inference rule . this rule has some remarkable properties : it is an abstract rule to update beliefs that can be applied in any situation or domain ; it gives us the opportunity to be neither too optimistic nor too pessimistic about the incompleteness process , which is a necessary condition to draw reliable while strong enough conclusions ; and it is a coherent rule , in the sense that it can not lead to inconsistencies . we give examples to show how the new rule can be applied in expert systems , in parametric statistical inference , and in pattern classification , and discuss more generally the view of incompleteness processes defended here as well as some of its consequences .

cost-sensitive c4.5 with post-pruning and competition
decision tree is an effective classification approach in data mining and machine learning . in applications , test costs and misclassification costs should be considered while inducing decision trees . recently , some cost-sensitive learning algorithms based on id3 such as cs-id3 , idx , \lambda-id3 have been proposed to deal with the issue . these algorithms deal with only symbolic data . in this paper , we develop a decision tree algorithm inspired by c4.5 for numeric data . there are two major issues for our algorithm . first , we develop the test cost weighted information gain ratio as the heuristic information . according to this heuristic information , our algorithm is to pick the attribute that provides more gain ratio and costs less for each selection . second , we design a post-pruning strategy through considering the tradeoff between test costs and misclassification costs of the generated decision tree . in this way , the total cost is reduced . experimental results indicate that ( 1 ) our algorithm is stable and effective ; ( 2 ) the post-pruning technique reduces the total cost significantly ; ( 3 ) the competition strategy is effective to obtain a cost-sensitive decision tree with low cost .

sufficiency , separability and temporal probabilistic models
suppose we are given the conditional probability of one variable given some other variables.normally the full joint distribution over the conditioning variablesis required to determine the probability of the conditioned variable.under what circumstances are the marginal distributions over the conditioning variables sufficient to determine the probability ofthe conditioned variable ? sufficiency in this sense is equivalent to additive separability ofthe conditional probability distribution.such separability structure is natural and can be exploited forefficient inference.separability has a natural generalization to conditional separability.separability provides a precise notion of weaklyinteracting subsystems in temporal probabilistic models.given a system that is decomposed into separable subsystems , exactmarginal probabilities over subsystems at future points in time can becomputed by propagating marginal subsystem probabilities , rather thancomplete system joint probabilities.thus , separability can make exact prediction tractable.however , observations can break separability , so exact monitoring of dynamic systems remains hard .

perspectival knowledge in psoa ruleml : representation , model theory , and translation
in positional-slotted object-applicative ( psoa ) ruleml , a predicate application ( atom ) can have an object identifier ( oid ) and descriptors that may be positional arguments ( tuples ) or attribute-value pairs ( slots ) . psoa ruleml 1.0 specifies for each descriptor whether it is to be interpreted under the perspective of the predicate in whose scope it occurs . this perspectivity dimension refines the space between oidless , positional atoms ( relationships ) and oidful , slotted atoms ( frames ) : while relationships use only a predicate-scope-sensitive ( predicate-dependent ) tuple and frames use only predicate-scope-insensitive ( predicate-independent ) slots , psoa ruleml 1.0 uses a systematics of orthogonal constructs also permitting atoms with ( predicate- ) independent tuples and atoms with ( predicate- ) dependent slots . this supports data and knowledge representation where a slot attribute can have different values depending on the predicate . psoa thus extends object-oriented multi-membership and multiple inheritance . based on objectification , psoa laws are given : besides unscoping and centralization , the semantic restriction and transformation of describution permits rescoping of one atom 's independent descriptors to another atom with the same oid but a different predicate . for inheritance , default descriptors are realized by rules . on top of a metamodel and a grailog visualization , psoa 's atom systematics for facts , queries , and rules is explained . the presentation and ( xml- ) serialization syntaxes of psoa ruleml 1.0 are introduced . its model-theoretic semantics is formalized by extending the earlier interpretation functions for dependent descriptors . the open-source psoatransrun 1.3 system realizes psoa ruleml 1.0 by a translator to runtime predicates , including for dependent tuples ( prdtupterm ) and slots ( prdsloterm ) . our tests show efficiency advantages of dependent and tupled modeling .

obstacle evasion using fuzzy logic in a sliding blades problem environment
this paper discusses obstacle avoidance using fuzzy logic and shortest path algorithm . this paper also introduces the sliding blades problem and illustrates how a drone can navigate itself through the swinging blade obstacles while tracing a semi-optimal path and also maintaining constant velocity

active neural localization
localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment . traditional methods of localization , which filter the belief based on the observations , are sub-optimal in the number of steps required , as they do not decide the actions taken by the agent . we propose `` active neural localizer '' , a fully differentiable neural network that learns to localize accurately and efficiently . the proposed model incorporates ideas of traditional filtering-based localization methods , by using a structured belief of the state with multiplicative interactions to propagate belief , and combines it with a policy model to localize accurately while minimizing the number of steps required for localization . active neural localizer is trained end-to-end with reinforcement learning . we use a variety of simulation environments for our experiments which include random 2d mazes , random mazes in the doom game engine and a photo-realistic environment in the unreal game engine . the results on the 2d environments show the effectiveness of the learned policy in an idealistic setting while results on the 3d environments demonstrate the model 's capability of learning the policy and perceptual model jointly from raw-pixel based rgb observations . we also show that a model trained on random textures in the doom environment generalizes well to a photo-realistic office space environment in the unreal engine .

embodied evolution in collective robotics : a review
this paper provides an overview of evolutionary robotics techniques applied to on-line distributed evolution for robot collectives -- namely , embodied evolution . it provides a definition of embodied evolution as well as a thorough description of the underlying concepts and mechanisms . the paper also presents a comprehensive summary of research published in the field since its inception ( 1999-2017 ) , providing various perspectives to identify the major trends . in particular , we identify a shift from considering embodied evolution as a parallel search method within small robot collectives ( fewer than 10 robots ) to embodied evolution as an on-line distributed learning method for designing collective behaviours in swarm-like collectives . the paper concludes with a discussion of applications and open questions , providing a milestone for past and an inspiration for future research .

using neural network to propose solutions to threats in attack patterns
in the last decade , a lot of effort has been put into securing software application during development in the software industry . software security is a research field in this area which looks at how security can be weaved into software at each phase of software development lifecycle ( sdlc ) . the use of attack patterns is one of the approaches that have been proposed for integrating security during the design phase of sdlc . while this approach help developers in identify security flaws in their software designs , the need to apply the proper security capability that will mitigate the threat identified is very important . to assist in this area , the uses of security patterns have been proposed to help developers to identify solutions to recurring security problems . however due to different types of security patterns and their taxonomy , software developers are faced with the challenge of finding and selecting appropriate security patterns that addresses the security risks in their design . in this paper , we propose a tool based on neural network for proposing solutions in form of security patterns to threats in attack patterns matching attacking patterns . from the result of performance of the neural network , we found out that the neural network was able to match attack patterns to security patterns that can mitigate the threat in the attack pattern . with this information developers are better informed in making decision on the solution for securing their application .

quantum entanglement in concept combinations
research in the application of quantum structures to cognitive science confirms that these structures quite systematically appear in the dynamics of concepts and their combinations and quantum-based models faithfully represent experimental data of situations where classical approaches are problematical . in this paper , we analyze the data we collected in an experiment on a specific conceptual combination , showing that bell 's inequalities are violated in the experiment . we present a new refined entanglement scheme to model these data within standard quantum theory rules , where 'entangled measurements and entangled evolutions ' occur , in addition to the expected 'entangled states ' , and present a full quantum representation in complex hilbert space of the data . this stronger form of entanglement in measurements and evolutions might have relevant applications in the foundations of quantum theory , as well as in the interpretation of nonlocality tests . it could indeed explain some non-negligible 'anomalies ' identified in epr-bell experiments .

evolving genes to balance a pole
we discuss how to use a genetic regulatory network as an evolutionary representation to solve a typical gp reinforcement problem , the pole balancing . the network is a modified version of an artificial regulatory network proposed a few years ago , and the task could be solved only by finding a proper way of connecting inputs and outputs to the network . we show that the representation is able to generalize well over the problem domain , and discuss the performance of different models of this kind .

minimally naturalistic artificial intelligence
the rapid advancement of machine learning techniques has re-energized research into general artificial intelligence . while the idea of domain-agnostic meta-learning is appealing , this emerging field must come to terms with its relationship to human cognition and the statistics and structure of the tasks humans perform . the position of this article is that only by aligning our agents ' abilities and environments with those of humans do we stand a chance at developing general artificial intelligence ( gai ) . a broad reading of the famous 'no free lunch ' theorem is that there is no universally optimal inductive bias or , equivalently , bias-free learning is impossible . this follows from the fact that there are an infinite number of ways to extrapolate data , any of which might be the one used by the data generating environment ; an inductive bias prefers some of these extrapolations to others , which lowers performance in environments using these adversarial extrapolations . we may posit that the optimal gai is the one that maximally exploits the statistics of its environment to create its inductive bias ; accepting the fact that this agent is guaranteed to be extremely sub-optimal for some alternative environments . this trade-off appears benign when thinking about the environment as being the physical universe , as performance on any fictive universe is obviously irrelevant . but , we should expect a sharper inductive bias if we further constrain our environment . indeed , we implicitly do so by defining gai in terms of accomplishing that humans consider useful . one common version of this is need the for 'common-sense reasoning ' , which implicitly appeals to the statistics of physical universe as perceived by humans .

belief induced by the partial knowledge of the probabilities
we construct the belief function that quantifies the agent , beliefs about which event of q will occurred when he knows that the event is selected by a chance set-up and that the probability function associated to the chance set up is only partially known .

on sets of graded attribute implications with witnessed non-redundancy
we study properties of particular non-redundant sets of if-then rules describing dependencies between graded attributes . we introduce notions of saturation and witnessed non-redundancy of sets of graded attribute implications are show that bases of graded attribute implications given by systems of pseudo-intents correspond to non-redundant sets of graded attribute implications with saturated consequents where the non-redundancy is witnessed by antecedents of the contained graded attribute implications . we introduce an algorithm which transforms any complete set of graded attribute implications parameterized by globalization into a base given by pseudo-intents . experimental evaluation is provided to compare the method of obtaining bases for general parameterizations by hedges with earlier graph-based approaches .

applying cooperative machine learning to speed up the annotation of social signals in large multi-modal corpora
scientific disciplines , such as behavioural psychology , anthropology and recently social signal processing are concerned with the systematic exploration of human behaviour . a typical work-flow includes the manual annotation ( also called coding ) of social signals in multi-modal corpora of considerable size . for the involved annotators this defines an exhausting and time-consuming task . in the article at hand we present a novel method and also provide the tools to speed up the coding procedure . to this end , we suggest and evaluate the use of cooperative machine learning ( cml ) techniques to reduce manual labelling efforts by combining the power of computational capabilities and human intelligence . the proposed cml strategy starts with a small number of labelled instances and concentrates on predicting local parts first . afterwards , a session-independent classification model is created to finish the remaining parts of the database . confidence values are computed to guide the manual inspection and correction of the predictions . to bring the proposed approach into application we introduce nova - an open-source tool for collaborative and machine-aided annotations . in particular , it gives labellers immediate access to cml strategies and directly provides visual feedback on the results . our experiments show that the proposed method has the potential to significantly reduce human labelling efforts .

low-dimensional data embedding via robust ranking
we describe a new method called t-ete for finding a low-dimensional embedding of a set of objects in euclidean space . we formulate the embedding problem as a joint ranking problem over a set of triplets , where each triplet captures the relative similarities between three objects in the set . by exploiting recent advances in robust ranking , t-ete produces high-quality embeddings even in the presence of a significant amount of noise and better preserves local scale than known methods , such as t-ste and t-sne . in particular , our method produces significantly better results than t-sne on signature datasets while also being faster to compute .

intuitions about ordered beliefs leading to probabilistic models
the general use of subjective probabilities to model belief has been justified using many axiomatic schemes . for example , ? consistent betting behavior ' arguments are well-known . to those not already convinced of the unique fitness and generality of probability models , such justifications are often unconvincing . the present paper explores another rationale for probability models . ? qualitative probability , ' which is known to provide stringent constraints on belief representation schemes , is derived from five simple assumptions about relationships among beliefs . while counterparts of familiar rationality concepts such as transitivity , dominance , and consistency are used , the betting context is avoided . the gap between qualitative probability and probability proper can be bridged by any of several additional assumptions . the discussion here relies on results common in the recent ai literature , introducing a sixth simple assumption . the narrative emphasizes models based on unique complete orderings , but the rationale extends easily to motivate set-valued representations of partial orderings as well .

weakly submodular maximization beyond cardinality constraints : does randomization help greedy ?
submodular functions are a broad class of set functions , which naturally arise in diverse areas . many algorithms have been suggested for the maximization of these functions . unfortunately , once the function deviates from submodularity , the known algorithms may perform arbitrarily poorly . amending this issue , by obtaining approximation results for set functions generalizing submodular functions , has been the focus of recent works . one such class , known as weakly submodular functions , has received a lot of attention . a key result proved by das and kempe ( 2011 ) showed that the approximation ratio of the greedy algorithm for weakly submodular maximization subject to a cardinality constraint degrades smoothly with the distance from submodularity . however , no results have been obtained for maximization subject to constraints beyond cardinality . in particular , it is not known whether the greedy algorithm achieves any non-trivial approximation ratio for such constraints . in this paper , we prove that a randomized version of the greedy algorithm ( previously used by buchbinder et al . ( 2014 ) for a different problem ) achieves an approximation ratio of $ ( 1 + 1/\gamma ) ^ { -2 } $ for the maximization of a weakly submodular function subject to a general matroid constraint , where $ \gamma $ is a parameter measuring the distance of the function from submodularity . moreover , we also experimentally compare the performance of this version of the greedy algorithm on real world problems against natural benchmarks , and show that the algorithm we study performs well also in practice . to the best of our knowledge , this is the first algorithm with a non-trivial approximation guarantee for maximizing a weakly submodular function subject to a constraint other than the simple cardinality constraint . in particular , it is the first algorithm with such a guarantee for the important and broad class of matroid constraints .

tightening lp relaxations for map using message passing
linear programming ( lp ) relaxations have become powerful tools for finding the most probable ( map ) configuration in graphical models . these relaxations can be solved efficiently using message-passing algorithms such as belief propagation and , when the relaxation is tight , provably find the map configuration . the standard lp relaxation is not tight enough in many real-world problems , however , and this has lead to the use of higher order cluster-based lp relaxations . the computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can use . we propose to solve the cluster selection problem monotonically in the dual lp , iteratively selecting clusters with guaranteed improvement , and quickly re-solving with the added clusters by reusing the existing solution . our dual message-passing algorithm finds the map configuration in protein sidechain placement , protein design , and stereo problems , in cases where the standard lp relaxation fails .

huto : an human time ontology for semantic web applications
the temporal phenomena have many facets that are studied by different communities . in semantic web , large heterogeneous data are handled and produced . these data often have informal , semi-formal or formal temporal information which must be interpreted by software agents . in this paper we present human time ontology ( huto ) an rdfs ontology to annotate and represent temporal data . a major contribution of huto is the modeling of non-convex intervals giving the ability to write queries for this kind of interval . huto also incorporates normalization and reasoning rules to explicit certain information . huto also proposes an approach which associates a temporal dimension to the knowledge base content . this facilitates information retrieval by considering or not the temporal aspect .

efficient mrf energy minimization via adaptive diminishing smoothing
we consider the linear programming relaxation of an energy minimization problem for markov random fields . the dual objective of this problem can be treated as a concave and unconstrained , but non-smooth function . the idea of smoothing the objective prior to optimization was recently proposed in a series of papers . some of them suggested the idea to decrease the amount of smoothing ( so called temperature ) while getting closer to the optimum . however , no theoretical substantiation was provided . we propose an adaptive smoothing diminishing algorithm based on the duality gap between relaxed primal and dual objectives and demonstrate the efficiency of our approach with a smoothed version of sequential tree-reweighted message passing ( trw-s ) algorithm . the strategy is applicable to other algorithms as well , avoids adhoc tuning of the smoothing during iterations , and provably guarantees convergence to the optimum .

determining the consistency factor of autopilot using rough set theory
autopilot is a system designed to guide a vehicle without aid . due to increase in flight hours and complexity of modern day flight it has become imperative to equip the aircrafts with autopilot . thus reliability and consistency of an autopilot system becomes a crucial role in a flight . but the increased complexity and demand for better accuracy has made the process of evaluating the autopilot for consistency a difficult process .a vast amount of imprecise data has been involved . rough sets can be a potent tool for such kind of applications containing vague data . this paper proposes an approach towards consistency factor determination using rough set theory . the seventeen basic factors , that are crucial in determining the consistency of an autopilot system , are grouped into five payloads based on their functionality . consistency factor is evaluated through these payloads , using rough set theory . consistency factor determines the consistency and reliability of an autopilot system and the conditions under which manual override becomes imperative . using rough set theory the most and the least influential factors towards autopilot system are also determined .

guided policy exploration for markov decision processes using an uncertainty-based value-of-information criterion
reinforcement learning in environments with many action-state pairs is challenging . at issue is the number of episodes needed to thoroughly search the policy space . most conventional heuristics address this search problem in a stochastic manner . this can leave large portions of the policy space unvisited during the early training stages . in this paper , we propose an uncertainty-based , information-theoretic approach for performing guided stochastic searches that more effectively cover the policy space . our approach is based on the value of information , a criterion that provides the optimal trade-off between expected costs and the granularity of the search process . the value of information yields a stochastic routine for choosing actions during learning that can explore the policy space in a coarse to fine manner . we augment this criterion with a state-transition uncertainty factor , which guides the search process into previously unexplored regions of the policy space .

towards automatic learning of heuristics for mechanical transformations of procedural code
the current trends in next-generation exascale systems go towards integrating a wide range of specialized ( co- ) processors into traditional supercomputers . due to the efficiency of heterogeneous systems in terms of watts and flops per surface unit , opening the access of heterogeneous platforms to a wider range of users is an important problem to be tackled . however , heterogeneous platforms limit the portability of the applications and increase development complexity due to the programming skills required . program transformation can help make programming heterogeneous systems easier by defining a step-wise transformation process that translates a given initial code into a semantically equivalent final code , but adapted to a specific platform . program transformation systems require the definition of efficient transformation strategies to tackle the combinatorial problem that emerges due to the large set of transformations applicable at each step of the process . in this paper we propose a machine learning-based approach to learn heuristics to define program transformation strategies . our approach proposes a novel combination of reinforcement learning and classification methods to efficiently tackle the problems inherent to this type of systems . preliminary results demonstrate the suitability of this approach .

exact reasoning under uncertainty
this paper focuses on designing expert systems to support decision making in complex , uncertain environments . in this context , our research indicates that strictly probabilistic representations , which enable the use of decision-theoretic reasoning , are highly preferable to recently proposed alternatives ( e.g. , fuzzy set theory and dempster-shafer theory ) . furthermore , we discuss the language of influence diagrams and a corresponding methodology -decision analysis -- that allows decision theory to be used effectively and efficiently as a decision-making aid . finally , we use rachel , a system that helps infertile couples select medical treatments , to illustrate the methodology of decision analysis as basis for expert decision systems .

giraffe : using deep reinforcement learning to play chess
this report presents giraffe , a chess engine that uses self-play to discover all its domain-specific knowledge , with minimal hand-crafted knowledge given by the programmer . unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions , giraffe 's learning system also performs automatic feature extraction and pattern recognition . the trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers , tuned over many years by both computer chess experts and human chess masters . giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess .

the ditmarsch tale of wonders - the dynamics of lying
we propose a dynamic logic of lying , wherein a 'lie that phi ' ( where phi is a formula in the logic ) is an action in the sense of dynamic modal logic , that is interpreted as a state transformer relative to the formula phi . the states that are being transformed are pointed kripke models encoding the uncertainty of agents about their beliefs . lies can be about factual propositions but also about modal formulas , such as the beliefs of other agents or the belief consequences of the lies of other agents . we distinguish ( i ) an outside observer who is lying to an agent that is modelled in the system , from ( ii ) one agent who is lying to another agent , and where both are modelled in the system . for either case , we further distinguish ( iii ) the agent who believes everything that it is told ( even at the price of inconsistency ) , from ( iv ) the agent who only believes what it is told if that is consistent with its current beliefs , and from ( v ) the agent who believes everything that it is told by consistently revising its current beliefs . the logics have complete axiomatizations , which can most elegantly be shown by way of their embedding in what is known as action model logic or the extension of that logic to belief revision .

a proof challenge : multiple alignment and information compression
these notes pose a `` proof challenge '' : a proof , or disproof , of the proposition that `` for any given body of information , i , expressed as a one-dimensional sequence of atomic symbols , a multiple alignment concept , described in the document , provides a means of encoding all the redundancy that may exist in i. aspects of the challenge are described .

filtering algorithms for the multiset ordering constraint
constraint programming ( cp ) has been used with great success to tackle a wide variety of constraint satisfaction problems which are computationally intractable in general . global constraints are one of the important factors behind the success of cp . in this paper , we study a new global constraint , the multiset ordering constraint , which is shown to be useful in symmetry breaking and searching for leximin optimal solutions in cp . we propose efficient and effective filtering algorithms for propagating this global constraint . we show that the algorithms are sound and complete and we discuss possible extensions . we also consider alternative propagation methods based on existing constraints in cp toolkits . our experimental results on a number of benchmark problems demonstrate that propagating the multiset ordering constraint via a dedicated algorithm can be very beneficial .

intuitions and the modelling of defeasible reasoning : some case studies
the purpose of this paper is to address some criticisms recently raised by john horty in two articles against the validity of two commonly accepted defeasible reasoning patterns , viz . reinstatement and floating conclusions . i shall argue that horty 's counterexamples , although they significantly raise our understanding of these reasoning patterns , do not show their invalidity . some of them reflect patterns which , if made explicit in the formalisation , avoid the unwanted inference without having to give up the criticised inference principles . other examples seem to involve hidden assumptions about the specific problem which , if made explicit , are nothing but extra information that defeat the defeasible inference . these considerations will be put in a wider perspective by reflecting on the nature of defeasible reasoning principles as principles of justified acceptance rather than ` real ' logical inference .

hcrs : a hybrid clothes recommender system based on user ratings and product features
nowadays , online clothes-selling business has become popular and extremely attractive because of its convenience and cheap-and-fine price . good examples of these successful web sites include yintai.com , vancl.com and shop.vipshop.com which provide thousands of clothes for online shoppers . the challenge for online shoppers lies on how to find a good product from lots of options . in this article , we propose a collaborative clothes recommender for easy shopping . one of the unique features of this system is the ability to recommend clothes in terms of both user ratings and clothing attributes . experiments in our simulation environment show that the proposed recommender can better satisfy the needs of users .

distributing knowledge into simple bases
understanding the behavior of belief change operators for fragments of classical logic has received increasing interest over the last years . results in this direction are mainly concerned with adapting representation theorems . however , fragment-driven belief change also leads to novel research questions . in this paper we propose the concept of belief distribution , which can be understood as the reverse task of merging . more specifically , we are interested in the following question : given an arbitrary knowledge base $ k $ and some merging operator $ \delta $ , can we find a profile $ e $ and a constraint $ \mu $ , both from a given fragment of classical logic , such that $ \delta_\mu ( e ) $ yields a result equivalent to $ k $ ? in other words , we are interested in seeing if $ k $ can be distributed into knowledge bases of simpler structure , such that the task of merging allows for a reconstruction of the original knowledge . our initial results show that merging based on drastic distance allows for an easy distribution of knowledge , while the power of distribution for operators based on hamming distance relies heavily on the fragment of choice .

a comprehensive trainable error model for sung music queries
we propose a model for errors in sung queries , a variant of the hidden markov model ( hmm ) . this is a solution to the problem of identifying the degree of similarity between a ( typically error-laden ) sung query and a potential target in a database of musical works , an important problem in the field of music information retrieval . similarity metrics are a critical component of query-by-humming ( qbh ) applications which search audio and multimedia databases for strong matches to oral queries . our model comprehensively expresses the types of error or variation between target and query : cumulative and non-cumulative local errors , transposition , tempo and tempo changes , insertions , deletions and modulation . the model is not only expressive , but automatically trainable , or able to learn and generalize from query examples . we present results of simulations , designed to assess the discriminatory potential of the model , and tests with real sung queries , to demonstrate relevance to real-world applications .

hybridminer : mining maximal frequent itemsets using hybrid database representation approach
in this paper we present a novel hybrid ( arraybased layout and vertical bitmap layout ) database representation approach for mining complete maximal frequent itemset ( mfi ) on sparse and large datasets . our work is novel in terms of scalability , item search order and two horizontal and vertical projection techniques . we also present a maximal algorithm using this hybrid database representation approach . different experimental results on real and sparse benchmark datasets show that our approach is better than previous state of art maximal algorithms .

artificial intelligence markup language : a brief tutorial
the purpose of this paper is to serve as a reference guide for the development of chatterbots implemented with the aiml language . in order to achieve this , the main concepts in pattern recognition area are described because the aiml uses such theoretical framework in their syntactic and semantic structures . after that , aiml language is described and each aiml command/tag is followed by an application example . also , the usage of aiml embedded tags for the handling of sequence dialogue limitations between humans and machines is shown . finally , computer systems that assist in the design of chatterbots with the aiml language are classified and described .

optimizing real-time rdf data streams
the resource description framework ( rdf ) provides a common data model for the integration of `` real-time '' social and sensor data streams with the web and with each other . while there exist numerous protocols and data formats for exchanging dynamic rdf data , or rdf updates , these options should be examined carefully in order to enable a semantic web equivalent of the high-throughput , low-latency streams of typical web 2.0 , multimedia , and gaming applications . this paper contains a brief survey of rdf update formats and a high-level discussion of both tcp and udp-based transport protocols for updates . its main contribution is the experimental evaluation of a udp-based architecture which serves as a real-world example of a high-performance rdf streaming application in an internet-scale distributed environment .

quantum and concept combination , entangled measurements and prototype theory
we analyze the meaning of the violation of the marginal probability law for situations of correlation measurements where entanglement is identified . we show that for quantum theory applied to the cognitive realm such a violation does not lead to the type of problems commonly believed to occur in situations of quantum theory applied to the physical realm . we briefly situate our quantum approach for modeling concepts and their combinations with respect to the notions of 'extension ' and 'intension ' in theories of meaning , and in existing concept theories .

collusion in unrepeated , first-price auctions with an uncertain number of participants
we consider the question of whether collusion among bidders ( a `` bidding ring '' ) can be supported in equilibrium of unrepeated first-price auctions . unlike previous work on the topic such as that by mcafee and mcmillan [ 1992 ] and marshall and marx [ 2007 ] , we do not assume that non-colluding agents have perfect knowledge about the number of colluding agents whose bids are suppressed by the bidding ring , and indeed even allow for the existence of multiple cartels . furthermore , while we treat the association of bidders with bidding rings as exogenous , we allow bidders to make strategic decisions about whether to join bidding rings when invited . we identify a bidding ring protocol that results in an efficient allocation in bayes { nash equilibrium , under which non-colluding agents bid straightforwardly , and colluding agents join bidding rings when invited and truthfully declare their valuations to the ring center . we show that bidding rings benefit ring centers and all agents , both members and non-members of bidding rings , at the auctioneer 's expense . the techniques we introduce in this paper may also be useful for reasoning about other problems in which agents have asymmetric information about a setting .

generating redundant features with unsupervised multi-tree genetic programming
recently , feature selection has become an increasingly important area of research due to the surge in high-dimensional datasets in all areas of modern life . a plethora of feature selection algorithms have been proposed , but it is difficult to truly analyse the quality of a given algorithm . ideally , an algorithm would be evaluated by measuring how well it removes known bad features . acquiring datasets with such features is inherently difficult , and so a common technique is to add synthetic bad features to an existing dataset . while adding noisy features is an easy task , it is very difficult to automatically add complex , redundant features . this work proposes one of the first approaches to generating redundant features , using a novel genetic programming approach . initial experiments show that our proposed method can automatically create difficult , redundant features which have the potential to be used for creating high-quality feature selection benchmark datasets .

object-oriented dynamic networks
this paper contains description of such knowledge representation model as object-oriented dynamic network ( oodn ) , which gives us an opportunity to represent knowledge , which can be modified in time , to build new relations between objects and classes of objects and to represent results of their modifications . the model is based on representation of objects via their properties and methods . it gives us a possibility to classify the objects and , in a sense , to build hierarchy of their types . furthermore , it enables to represent relation of modification between concepts , to build new classes of objects based on existing classes and to create sets and multisets of concepts . oodn can be represented as a connected and directed graph , where nodes are concepts and edges are relations between them . using such model of knowledge representation , we can consider modifications of knowledge and movement through the graph of network as a process of logical reasoning or finding the right solutions or creativity , etc . the proposed approach gives us an opportunity to model some aspects of human knowledge system and main mechanisms of human thought , in particular getting a new experience and knowledge .

exploiting points and lines in regression forests for rgb-d camera relocalization
camera relocalization plays a vital role in many robotics and computer vision tasks , such as global localization , recovery from tracking failure and loop closure detection . recent random forests based methods exploit randomly sampled pixel comparison features to predict 3d world locations for 2d image locations to guide the camera pose optimization . however , these image features are only sampled randomly in the images , without considering the spatial structures or geometric information , leading to large errors or failure cases with the existence of poorly textured areas or in motion blur . line segment features are more robust in these environments . in this work , we propose to jointly exploit points and lines within the framework of uncertainty driven regression forests . the proposed approach is thoroughly evaluated on three publicly available datasets against several strong state-of-the-art baselines in terms of several different error metrics . experimental results prove the efficacy of our method , showing superior or on-par state-of-the-art performance .

learning complex swarm behaviors by exploiting local communication protocols with deep reinforcement learning
swarm systems constitute a challenging problem for reinforcement learning ( rl ) as the algorithm needs to learn decentralized control policies that can cope with limited local sensing and communication abilities of the agents . although there have been recent advances of deep rl algorithms applied to multi-agent systems , learning communication protocols while simultaneously learning the behavior of the agents is still beyond the reach of deep rl algorithms . however , while it is often difficult to directly define the behavior of the agents , simple communication protocols can be defined more easily using prior knowledge about the given task . in this paper , we propose a number of simple communication protocols that can be exploited by deep reinforcement learning to find decentralized control policies in a multi-robot swarm environment . the protocols are based on histograms that encode the local neighborhood relations of the agents and can also transmit task-specific information , such as the shortest distance and direction to a desired target . in our framework , we use an adaptation of trust region policy optimization to learn complex collaborative tasks , such as formation building , building a communication link , and pushing an intruder . we evaluate our findings in a simulated 2d-physics environment , and compare the implications of different communication protocols .

robust filtering and smoothing with gaussian processes
we propose a principled algorithm for robust bayesian filtering and smoothing in nonlinear stochastic dynamic systems when both the transition function and the measurement function are described by non-parametric gaussian process ( gp ) models . gps are gaining increasing importance in signal processing , machine learning , robotics , and control for representing unknown system functions by posterior probability distributions . this modern way of `` system identification '' is more robust than finding point estimates of a parametric function representation . in this article , we present a principled algorithm for robust analytic smoothing in gp dynamic systems , which are increasingly used in robotics and control . our numerical evaluations demonstrate the robustness of the proposed approach in situations where other state-of-the-art gaussian filters and smoothers can fail .

distributed constraint optimization problems and applications : a survey
the field of multi-agent system ( mas ) is an active area of research within artificial intelligence , with an increasingly important impact in industrial and other real-world applications . within a mas , autonomous agents interact to pursue personal interests and/or to achieve common objectives . distributed constraint optimization problems ( dcops ) have emerged as one of the prominent agent architectures to govern the agents ' autonomous behavior , where both algorithms and communication models are driven by the structure of the specific problem . during the last decade , several extensions to the dcop model have enabled them to support mas in complex , real-time , and uncertain environments . this survey aims at providing an overview of the dcop model , giving a classification of its multiple extensions and addressing both resolution methods and applications that find a natural mapping within each class of dcops . the proposed classification suggests several future perspectives for dcop extensions , and identifies challenges in the design of efficient resolution algorithms , possibly through the adaptation of strategies from different areas .

message passing multi-agent gans
communicating and sharing intelligence among agents is an important facet of achieving artificial general intelligence . as a first step towards this challenge , we introduce a novel framework for image generation : message passing multi-agent generative adversarial networks ( mpm gans ) . while gans have recently been shown to be very effective for image generation and other tasks , these networks have been limited to mostly single generator-discriminator networks . we show that we can obtain multi-agent gans that communicate through message passing to achieve better image generation . the objectives of the individual agents in this framework are two fold : a co-operation objective and a competing objective . the co-operation objective ensures that the message sharing mechanism guides the other generator to generate better than itself while the competing objective encourages each generator to generate better than its counterpart . we analyze and visualize the messages that these gans share among themselves in various scenarios . we quantitatively show that the message sharing formulation serves as a regularizer for the adversarial training . qualitatively , we show that the different generators capture different traits of the underlying data distribution .

improving feature selection algorithms using normalised feature histograms
the proposed feature selection method builds a histogram of the most stable features from random subsets of a training set and ranks the features based on a classifier based cross-validation . this approach reduces the instability of features obtained by conventional feature selection methods that occur with variation in training data and selection criteria . classification results on four microarray and three image datasets using three major feature selection criteria and a naive bayes classifier show considerable improvement over benchmark results .

is a good offensive always the best defense ?
a checkers-like model game with a simplified set of rules is studied through extensive simulations of agents with different expertise and strategies . the introduction of complementary strategies , in a quite general way , provides a tool to mimic the basic ingredients of a wide scope of real games . we find that only for the player having the higher offensive expertise ( the dominant player ) , maximizing the offensive always increases the probability to win . for the non-dominant player , interestingly , a complete minimization of the offensive becomes the best way to win in many situations , depending on the relative values of the defense expertise . further simulations on the interplay of defense expertise were done separately , in the context of a fully-offensive scenario , offering a starting point for analytical treatments . in particular , we established that in this scenario the total number of moves is defined only by the player with the lower defensive expertise . we believe that these results stand for a first step towards a new way to improve decisions-making in a large number of zero-sum real games .

a new algorithm for identity verification based on the analysis of a handwritten dynamic signature
identity verification based on authenticity assessment of a handwritten signature is an important issue in biometrics . there are many effective methods for signature verification taking into account dynamics of a signing process . methods based on partitioning take a very important place among them . in this paper we propose a new approach to signature partitioning . its most important feature is the possibility of selecting and processing of hybrid partitions in order to increase a precision of the test signature analysis . partitions are formed by a combination of vertical and horizontal sections of the signature . vertical sections correspond to the initial , middle , and final time moments of the signing process . in turn , horizontal sections correspond to the signature areas associated with high and low pen velocity and high and low pen pressure on the surface of a graphics tablet . our previous research on vertical and horizontal sections of the dynamic signature ( created independently ) led us to develop the algorithm presented in this paper . selection of sections , among others , allows us to define the stability of the signing process in the partitions , promoting signature areas of greater stability ( and vice versa ) . in the test of the proposed method two databases were used : public mcyt-100 and paid biosecure .

a web-based tool for identifying strategic intervention points in complex systems
steering a complex system towards a desired outcome is a challenging task . the lack of clarity on the system 's exact architecture and the often scarce scientific data upon which to base the operationalisation of the dynamic rules that underpin the interactions between participant entities are two contributing factors . we describe an analytical approach that builds on fuzzy cognitive mapping ( fcm ) to address the latter and represent the system as a complex network . we apply results from network controllability to address the former and determine minimal control configurations - subsets of factors , or system levers , which comprise points for strategic intervention in steering the system . we have implemented the combination of these techniques in an analytical tool that runs in the browser , and generates all minimal control configurations of a complex network . we demonstrate our approach by reporting on our experience of working alongside industrial , local-government , and ngo stakeholders in the humber region , uk . our results are applied to the decision-making process involved in the transition of the region to a bio-based economy .

induction and uncertainty management techniques applied to veterinary medical diagnosis
this paper discusses a project undertaken between the departments of computing science , statistics , and the college of veterinary medicine to design a medical diagnostic system . on-line medical data has been collected in the hospital database system for several years . a number of induction methods are being used to extract knowledge from the data in an attempt to improve upon simple diagnostic charts used by the clinicians . they also enhance the results of classical statistical methods - finding many more significant variables . the second part of the paper describes an essentially bayesian method of evidence combination using fuzzy events at an initial step . results are presented and comparisons are made with other methods .

towards combinatorial clustering : preliminary research survey
the paper describes clustering problems from the combinatorial viewpoint . a brief systemic survey is presented including the following : ( i ) basic clustering problems ( e.g. , classification , clustering , sorting , clustering with an order over cluster ) , ( ii ) basic approaches to assessment of objects and object proximities ( i.e. , scales , comparison , aggregation issues ) , ( iii ) basic approaches to evaluation of local quality characteristics for clusters and total quality characteristics for clustering solutions , ( iv ) clustering as multicriteria optimization problem , ( v ) generalized modular clustering framework , ( vi ) basic clustering models/methods ( e.g. , hierarchical clustering , k-means clustering , minimum spanning tree based clustering , clustering as assignment , detection of clisue/quasi-clique based clustering , correlation clustering , network communities based clustering ) , special attention is targeted to formulation of clustering as multicriteria optimization models . combinatorial optimization models are used as auxiliary problems ( e.g. , assignment , partitioning , knapsack problem , multiple choice problem , morphological clique problem , searching for consensus/median for structures ) . numerical examples illustrate problem formulations , solving methods , and applications . the material can be used as follows : ( a ) a research survey , ( b ) a fundamental for designing the structure/architecture of composite modular clustering software , ( c ) a bibliography reference collection , and ( d ) a tutorial .

genetic algorithms for multiple objective vehicle routing
the talk describes a general approach of a genetic algorithm for multiple objective optimization problems . a particular dominance relation between the individuals of the population is used to define a fitness operator , enabling the genetic algorithm to adress even problems with efficient , but convex-dominated alternatives . the algorithm is implemented in a multilingual computer program , solving vehicle routing problems with time windows under multiple objectives . the graphical user interface of the program shows the progress of the genetic algorithm and the main parameters of the approach can be easily modified . in addition to that , the program provides powerful decision support to the decision maker . the software has proved it 's excellence at the finals of the european academic software award easa , held at the keble college/ university of oxford/ great britain .

dropoutdagger : a bayesian approach to safe imitation learning
while imitation learning is becoming common practice in robotics , this approach often suffers from data mismatch and compounding errors . dagger is an iterative algorithm that addresses these issues by continually aggregating training data from both the expert and novice policies , but does not consider the impact of safety . we present a probabilistic extension to dagger , which uses the distribution over actions provided by the novice policy , for a given observation . our method , which we call dropoutdagger , uses dropout to train the novice as a bayesian neural network that provides insight to its confidence . using the distribution over the novice 's actions , we estimate a probabilistic measure of safety with respect to the expert action , tuned to balance exploration and exploitation . the utility of this approach is evaluated on the mujoco halfcheetah and in a simple driving experiment , demonstrating improved performance and safety compared to other dagger variants and classic imitation learning .

contextualizing concepts using a mathematical generalization of the quantum formalism
we outline the rationale and preliminary results of using the state context property ( scop ) formalism , originally developed as a generalization of quantum mechanics , to describe the contextual manner in which concepts are evoked , used and combined to generate meaning . the quantum formalism was developed to cope with problems arising in the description of ( i ) the measurement process , and ( ii ) the generation of new states with new properties when particles become entangled . similar problems arising with concepts motivated the formal treatment introduced here . concepts are viewed not as fixed representations , but entities existing in states of potentiality that require interaction with a context-a stimulus or another concept-to 'collapse ' to an instantiated form ( e.g . exemplar , prototype , or other possibly imaginary instance ) . the stimulus situation plays the role of the measurement in physics , acting as context that induces a change of the cognitive state from superposition state to collapsed state . the collapsed state is more likely to consist of a conjunction of concepts for associative than analytic thought because more stimulus or concept properties take part in the collapse . we provide two contextual measures of conceptual distance-one using collapse probabilities and the other weighted properties-and show how they can be applied to conjunctions using the pet fish problem .

on macroscopic complexity and perceptual coding
the theoretical limits of 'lossy ' data compression algorithms are considered . the complexity of an object as seen by a macroscopic observer is the size of the perceptual code which discards all information that can be lost without altering the perception of the specified observer . the complexity of this macroscopically observed state is the simplest description of any microstate comprising that macrostate . inference and pattern recognition based on macrostate rather than microstate complexities will take advantage of the complexity of the macroscopic observer to ignore irrelevant noise .

probabilistic programs for inferring the goals of autonomous agents
intelligent systems sometimes need to infer the probable goals of people , cars , and robots , based on partial observations of their motion . this paper introduces a class of probabilistic programs for formulating and solving these problems . the formulation uses randomized path planning algorithms as the basis for probabilistic models of the process by which autonomous agents plan to achieve their goals . because these path planning algorithms do not have tractable likelihood functions , new inference algorithms are needed . this paper proposes two monte carlo techniques for these `` likelihood-free '' models , one of which can use likelihood estimates from neural networks to accelerate inference . the paper demonstrates efficacy on three simple examples , each using under 50 lines of probabilistic code .

meta learning framework for automated driving
the success of automated driving deployment is highly depending on the ability to develop an efficient and safe driving policy . the problem is well formulated under the framework of optimal control as a cost optimization problem . model based solutions using traditional planning are efficient , but require the knowledge of the environment model . on the other hand , model free solutions suffer sample inefficiency and require too many interactions with the environment , which is infeasible in practice . methods under the reinforcement learning framework usually require the notion of a reward function , which is not available in the real world . imitation learning helps in improving sample efficiency by introducing prior knowledge obtained from the demonstrated behavior , on the risk of exact behavior cloning without generalizing to unseen environments . in this paper we propose a meta learning framework , based on data set aggregation , to improve generalization of imitation learning algorithms . under the proposed framework , we propose metadagger , a novel algorithm which tackles the generalization issues in traditional imitation learning . we use the open race car simulator ( torcs ) to test our algorithm . results on unseen test tracks show significant improvement over traditional imitation learning algorithms , improving the learning time and sample efficiency in the same time . the results are also supported by visualization of the learnt features to prove generalization of the captured details .

truth as utility : a conceptual synthesis
this paper introduces conceptual relations that synthesize utilitarian and logical concepts , extending the logics of preference of rescher . we define first , in the context of a possible worlds model , constraint-dependent measures that quantify the relative quality of alternative solutions of reasoning problems or the relative desirability of various policies in control , decision , and planning problems . we show that these measures may be interpreted as truth values in a multi valued logic and propose mechanisms for the representation of complex constraints as combinations of simpler restrictions . these extended logical operations permit also the combination and aggregation of goal-specific quality measures into global measures of utility . we identify also relations that represent differential preferences between alternative solutions and relate them to the previously defined desirability measures . extending conventional modal logic formulations , we introduce structures for the representation of ignorance about the utility of alternative solutions . finally , we examine relations between these concepts and similarity based semantic models of fuzzy logic .

factored contextual policy search with bayesian optimization
scarce data is a major challenge to scaling robot learning to truly complex tasks , as we need to generalize locally learned policies over different `` contexts '' . bayesian optimization approaches to contextual policy search ( cps ) offer data-efficient policy learning that generalize over a context space . we propose to improve data- efficiency by factoring typically considered contexts into two components : target- type contexts that correspond to a desired outcome of the learned behavior , e.g . target position for throwing a ball ; and environment type contexts that correspond to some state of the environment , e.g . initial ball position or wind speed . our key observation is that experience can be directly generalized over target-type contexts . based on that we introduce factored contextual policy search with bayesian optimization for both passive and active learning settings . preliminary results show faster policy generalization on a simulated toy problem .

can the internet cope with stress ?
when will the internet become aware of itself ? in this note the problem is approached by asking an alternative question : can the internet cope with stress ? by extrapolating the psychological difference between coping and defense mechanisms a distributed software experiment is outlined which could reject the hypothesis that the internet is not a conscious entity .

continuous occurrence theory
usually gradual and continuous changes in entities will lead to appear events . but usually it is supposed that an event is occurred at once . in this research an integrated framework called continuous occurrence theory ( cot ) is presented to investigate respective path leading to occurrence of the events in the real world . for this purpose initially fundamental concepts are defined . afterwards , the appropriate tools such as occurrence variables computations , occurrence dependency function and occurrence model are introduced and explained in a systematic manner . indeed , cot provides the possibility to : ( a ) monitor occurrence of events during time ; ( b ) study background of the events ; ( c ) recognize the relevant issues of each event ; and ( d ) understand how these issues affect on the considered event . the developed framework ( cot ) provides the necessary context to analyze accurately continual changes of the issues and the relevant events in the various branches of science and business . finally , typical applications of cot and an applied modeling example of it have been explained and a mathematical programming example is modeled in the occurrence based environment .

learning heuristic search via imitation
robotic motion planning problems are typically solved by constructing a search tree of valid maneuvers from a start to a goal configuration . limited onboard computation and real-time planning constraints impose a limit on how large this search tree can grow . heuristics play a crucial role in such situations by guiding the search towards potentially good directions and consequently minimizing search effort . moreover , it must infer such directions in an efficient manner using only the information uncovered by the search up until that time . however , state of the art methods do not address the problem of computing a heuristic that explicitly minimizes search effort . in this paper , we do so by training a heuristic policy that maps the partial information from the search to decide which node of the search tree to expand . unfortunately , naively training such policies leads to slow convergence and poor local minima . we present sail , an efficient algorithm that trains heuristic policies by imitating `` clairvoyant oracles '' - oracles that have full information about the world and demonstrate decisions that minimize search effort . we leverage the fact that such oracles can be efficiently computed using dynamic programming and derive performance guarantees for the learnt heuristic . we validate the approach on a spectrum of environments which show that sail consistently outperforms state of the art algorithms . our approach paves the way forward for learning heuristics that demonstrate an anytime nature - finding feasible solutions quickly and incrementally refining it over time .

practical issues in constructing a bayes ' belief network
bayes belief networks and influence diagrams are tools for constructing coherent probabilistic representations of uncertain knowledge . the process of constructing such a network to represent an expert 's knowledge is used to illustrate a variety of techniques which can facilitate the process of structuring and quantifying uncertain relationships . these include some generalizations of the `` noisy or gate '' concept . sensitivity analysis of generic elements of bayes ' networks provides insight into when rough probability assessments are sufficient and when greater precision may be important .

deep tracking on the move : learning to track the world from a moving vehicle using recurrent neural networks
this paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments . unlike traditional approaches to tracking , this method is learned end-to-end , and is able to directly predict a full unoccluded occupancy grid map from raw laser input data . inspired by the recently presented deeptracking approach [ ondruska , 2016 ] , we employ a recurrent neural network ( rnn ) to capture the temporal evolution of the state of the environment , and propose to use spatial transformer modules to exploit estimates of the egomotion of the vehicle . our results demonstrate the ability to track a range of objects , including cars , buses , pedestrians , and cyclists through occlusion , from both moving and stationary platforms , using a single learned model . experimental results demonstrate that the model can also predict the future states of objects from current inputs , with greater accuracy than previous work .

adapting improved upper confidence bounds for monte-carlo tree search
the uct algorithm , which combines the ucb algorithm and monte-carlo tree search ( mcts ) , is currently the most widely used variant of mcts . recently , a number of investigations into applying other bandit algorithms to mcts have produced interesting results . in this research , we will investigate the possibility of combining the improved ucb algorithm , proposed by auer et al . ( 2010 ) , with mcts . however , various characteristics and properties of the improved ucb algorithm may not be ideal for a direct application to mcts . therefore , some modifications were made to the improved ucb algorithm , making it more suitable for the task of game tree search . the mi-uct algorithm is the application of the modified ucb algorithm applied to trees . the performance of mi-uct is demonstrated on the games of $ 9\times 9 $ go and $ 9\times 9 $ nogo , and has shown to outperform the plain uct algorithm when only a small number of playouts are given , and rougly on the same level when more playouts are available .

context-specific independence in bayesian networks
bayesian networks provide a language for qualitatively representing the conditional independence properties of a distribution . this allows a natural and compact representation of the distribution , eases knowledge acquisition , and supports effective inference algorithms . it is well-known , however , that there are certain independencies that we can not capture qualitatively within the bayesian network structure : independencies that hold only in certain contexts , i.e. , given a specific assignment of values to certain variables . in this paper , we propose a formal notion of context-specific independence ( csi ) , based on regularities in the conditional probability tables ( cpts ) at a node . we present a technique , analogous to ( and based on ) d-separation , for determining when such independence holds in a given network . we then focus on a particular qualitative representation scheme - tree-structured cpts - for capturing csi . we suggest ways in which this representation can be used to support effective inference algorithms . in particular , we present a structural decomposition of the resulting network which can improve the performance of clustering algorithms , and an alternative algorithm based on cutset conditioning .

evaluating influence diagrams using limids
we present a new approach to the solution of decision problems formulated as influence diagrams . the approach converts the influence diagram into a simpler structure , the limited memory influence diagram ( limid ) , where only the requisite information for the computation of optimal policies is depicted . because the requisite information is explicitly represented in the diagram , the evaluation procedure can take advantage of it . in this paper we show how to convert an influence diagram to a limid and describe the procedure for finding an optimal strategy . our approach can yield significant savings of memory and computational time when compared to traditional methods .

a spatio-temporal representation for the orienteering problem with time-varying profits
we consider an orienteering problem ( op ) where an agent needs to visit a series ( possibly a subset ) of depots , from which the maximal accumulated profits are desired within given limited time budget . different from most existing works where the profits are assumed to be static , in this work we investigate a variant that has arbitrary time-dependent profits . specifically , the profits to be collected change over time and they follow different ( e.g. , independent ) time-varying functions . the problem is of inherent nonlinearity and difficult to solve by existing methods . to tackle the challenge , we present a simple and effective framework that incorporates time-variations into the fundamental planning process . specifically , we propose a deterministic spatio-temporal representation where both spatial description and temporal logic are unified into one routing topology . by employing existing basic sorting and searching algorithms , the routing solutions can be computed in an extremely efficient way . the proposed method is easy to implement and extensive numerical results show that our approach is time efficient and generates near-optimal solutions .

survey of modern fault diagnosis methods in networks
with the advent of modern computer networks , fault diagnosis has been a focus of research activity . this paper reviews the history of fault diagnosis in networks and discusses the main methods in information gathering section , information analyzing section and diagnosing and revolving section of fault diagnosis in networks . emphasis will be placed upon knowledge-based methods with discussing the advantages and shortcomings of the different methods . the survey is concluded with a description of some open problems .

tensorizing generative adversarial nets
generative adversarial network ( gan ) and its variants demonstrate state-of-the-art performance in the class of generative models . to capture higher dimensional distributions , the common learning procedure requires high computational complexity and large number of parameters . in this paper , we present a new generative adversarial framework by representing each layer as a tensor structure connected by multilinear operations , aiming to reduce the number of model parameters by a large factor while preserving the quality of generalized performance . to learn the model , we develop an efficient algorithm by alternating optimization of the mode connections . experimental results demonstrate that our model can achieve high compression rate for model parameters up to 40 times as compared to the existing gan .

a logic and adaptive approach for efficient diagnosis systems using cbr
case based reasoning ( cbr ) is an intelligent way of thinking based on experience and capitalization of already solved cases ( source cases ) to find a solution to a new problem ( target case ) . retrieval phase consists on identifying source cases that are similar to the target case . this phase may lead to erroneous results if the existing knowledge imperfections are not taken into account . this work presents a novel solution based on fuzzy logic techniques and adaptation measures which aggregate weighted similarities to improve the retrieval results . to confirm the efficiency of our solution , we have applied it to the industrial diagnosis domain . the obtained results are more efficient results than those obtained by applying typical measures .

intriguing properties of randomly weighted networks : generalizing while learning next to nothing
training deep neural networks results in strong learned representations that show good generalization capabilities . in most cases , training involves iterative modification of all weights inside the network via back-propagation . in extreme learning machines , it has been suggested to set the first layer of a network to fixed random values instead of learning it . in this paper , we propose to take this approach a step further and fix almost all layers of a deep convolutional neural network , allowing only a small portion of the weights to be learned . as our experiments show , fixing even the majority of the parameters of the network often results in performance which is on par with the performance of learning all of them . the implications of this intriguing property of deep neural networks are discussed and we suggest ways to harness it to create more robust representations .

what can you do with a rock ? affordance extraction via word embeddings
autonomous agents must often detect affordances : the set of behaviors enabled by a situation . affordance detection is particularly helpful in domains with large action spaces , allowing the agent to prune its search space by avoiding futile behaviors . this paper presents a method for affordance extraction via word embeddings trained on a wikipedia corpus . the resulting word vectors are treated as a common knowledge database which can be queried using linear algebra . we apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance most of the time . our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed . in addition , the agent 's action selections begin to resemble those a human would choose .

geometry of interest ( goi ) : spatio-temporal destination extraction and partitioning in gps trajectory data
nowadays large amounts of gps trajectory data is being continuously collected by gps-enabled devices such as vehicles navigation systems and mobile phones . gps trajectory data is useful for applications such as traffic management , location forecasting , and itinerary planning . such applications often need to extract the time-stamped sequence of visited locations ( svls ) of the mobile objects . the nearest neighbor query ( nnq ) is the most applied method for labeling the visited locations based on the ids of the pois in the process of svl generation . nnq in some scenarios is not accurate enough . to improve the quality of the extracted svls , instead of using nnq , we label the visited locations as the ids of the pois which geometrically intersect with the gps observations . intersection operator requires the accurate geometry of the points of interest which we refer to them as the geometries of interest ( gois ) . in some application domains ( e.g . movement trajectories of animals ) , adequate information about the pois and their gois may not be available a priori , or they may not be publicly accessible and , therefore , they need to be derived from gps trajectory data . in this paper we propose a novel method for estimating the pois and their gois , which consists of three phases : ( i ) extracting the geometries of the stay regions ; ( ii ) constructing the geometry of destination regions based on the extracted stay regions ; and ( iii ) constructing the gois based on the geometries of the destination regions . using the geometric similarity to known gois as the major evaluation criterion , the experiments we performed using long-term gps trajectory data show that our method outperforms the existing approaches .

towards visual ego-motion learning in robots
many model-based visual odometry ( vo ) algorithms have been proposed in the past decade , often restricted to the type of camera optics , or the underlying motion manifold observed . we envision robots to be able to learn and perform these tasks , in a minimally supervised setting , as they gain more experience . to this end , we propose a fully trainable solution to visual ego-motion estimation for varied camera optics . we propose a visual ego-motion learning architecture that maps observed optical flow vectors to an ego-motion density estimate via a mixture density network ( mdn ) . by modeling the architecture as a conditional variational autoencoder ( c-vae ) , our model is able to provide introspective reasoning and prediction for ego-motion induced scene-flow . additionally , our proposed model is especially amenable to bootstrapped ego-motion learning in robots where the supervision in ego-motion estimation for a particular camera sensor can be obtained from standard navigation-based sensor fusion strategies ( gps/ins and wheel-odometry fusion ) . through experiments , we show the utility of our proposed approach in enabling the concept of self-supervised learning for visual ego-motion estimation in autonomous robots .

semantics for probabilistic inference
a number of writers ( joseph halpern and fahiem bacchus among them ) have offered semantics for formal languages in which inferences concerning probabilities can be made . our concern is different . this paper provides a formalization of nonmonotonic inferences in which the conclusion is supported only to a certain degree . such inferences are clearly 'invalid ' since they must allow the falsity of a conclusion even when the premises are true . nevertheless , such inferences can be characterized both syntactically and semantically . the 'premises ' of probabilistic arguments are sets of statements ( as in a database or knowledge base ) , the conclusions categorical statements in the language . we provide standards for both this form of inference , for which high probability is required , and for an inference in which the conclusion is qualified by an intermediate interval of support .

octopus : a framework for cost-quality-time optimization in crowdsourcing
we present octopus , an ai agent to jointly balance three conflicting task objectives on a micro-crowdsourcing marketplace - the quality of work , total cost incurred , and time to completion . previous control agents have mostly focused on cost-quality , or cost-time tradeoffs , but not on directly controlling all three in concert . a naive formulation of three-objective optimization is intractable ; octopus takes a hierarchical pomdp approach , with three different components responsible for setting the pay per task , selecting the next task , and controlling task-level quality . we demonstrate that octopus significantly outperforms existing state-of-the-art approaches on real experiments . we also deploy octopus on amazon mechanical turk , showing its ability to manage tasks in a real-world dynamic setting .

coupling control and human-centered automation in mathematical models of complex systems
in this paper we analyze mathematically how human factors can be effectively incorporated into the analysis and control of complex systems . as an example , we focus our discussion around one of the key problems in the intelligent transportation systems ( its ) theory and practice , the problem of speed control , considered here as a decision making process with limited information available . the problem is cast mathematically in the general framework of control problems and is treated in the context of dynamically changing environments where control is coupled to human-centered automation . since in this case control might not be limited to a small number of control settings , as it is often assumed in the control literature , serious difficulties arise in the solution of this problem . we demonstrate that the problem can be reduced to a set of hamilton-jacobi-bellman equations where human factors are incorporated via estimations of the system hamiltonian . in the its context , these estimations can be obtained with the use of on-board equipment like sensors/receivers/actuators , in-vehicle communication devices , etc . the proposed methodology provides a way to integrate human factor into the solving process of the models for other complex dynamic systems .

applicability of crisp and fuzzy logic in intelligent response generation
this paper discusses the merits and demerits of crisp logic and fuzzy logic with respect to their applicability in intelligent response generation by a human being and by a robot . intelligent systems must have the capability of taking decisions that are wise and handle situations intelligently . a direct relationship exists between the level of perfection in handling a situation and the level of completeness of the available knowledge or information or data required to handle the situation . the paper concludes that the use of crisp logic with complete knowledge leads to perfection in handling situations whereas fuzzy logic can handle situations imperfectly only . however , in the light of availability of incomplete knowledge fuzzy theory is more effective but may be disadvantageous as compared to crisp logic .

safe probability
we formalize the idea of probability distributions that lead to reliable predictions about some , but not all aspects of a domain . the resulting notion of ` safety ' provides a fresh perspective on foundational issues in statistics , providing a middle ground between imprecise probability and multiple-prior models on the one hand and strictly bayesian approaches on the other . it also allows us to formalize fiducial distributions in terms of the set of random variables that they can safely predict , thus taking some of the sting out of the fiducial idea . by restricting probabilistic inference to safe uses , one also automatically avoids paradoxes such as the monty hall problem . safety comes in a variety of degrees , such as `` validity '' ( the strongest notion ) , `` calibration '' , `` confidence safety '' and `` unbiasedness '' ( almost the weakest notion ) .

knowledge engineering for large belief networks
we present several techniques for knowledge engineering of large belief networks ( bns ) based on the our experiences with a network derived from a large medical knowledge base . the noisymax , a generalization of the noisy-or gate , is used to model causal in dependence in a bn with multi-valued variables . we describe the use of leak probabilities to enforce the closed-world assumption in our model . we present netview , a visualization tool based on causal independence and the use of leak probabilities . the netview software allows knowledge engineers to dynamically view sub-networks for knowledge engineering , and it provides version control for editing a bn . netview generates sub-networks in which leak probabilities are dynamically updated to reflect the missing portions of the network .

inverse classification for comparison-based interpretability in machine learning
in the context of post-hoc interpretability , this paper addresses the task of explaining the prediction of a classifier , considering the case where no information is available , neither on the classifier itself , nor on the processed data ( neither the training nor the test data ) . it proposes an instance-based approach whose principle consists in determining the minimal changes needed to alter a prediction : given a data point whose classification must be explained , the proposed method consists in identifying a close neighbour classified differently , where the closeness definition integrates a sparsity constraint . this principle is implemented using observation generation in the growing spheres algorithm . experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier .

powerplay : training an increasingly general problem solver by continually searching for the simplest still unsolvable problem
most of computer science focuses on automatically solving given computational problems . i focus on automatically inventing or discovering problems in a way inspired by the playful behavior of animals and humans , to train a more and more general problem solver from scratch in an unsupervised fashion . consider the infinite set of all computable descriptions of tasks with possibly computable solutions . the novel algorithmic framework powerplay ( 2011 ) continually searches the space of possible pairs of new tasks and modifications of the current problem solver , until it finds a more powerful problem solver that provably solves all previously learned tasks plus the new one , while the unmodified predecessor does not . wow-effects are achieved by continually making previously learned skills more efficient such that they require less time and space . new skills may ( partially ) re-use previously learned skills . powerplay 's search orders candidate pairs of tasks and solver modifications by their conditional computational ( time & space ) complexity , given the stored experience so far . the new task and its corresponding task-solving skill are those first found and validated . the computational costs of validating new tasks need not grow with task repertoire size . powerplay 's ongoing search for novelty keeps breaking the generalization abilities of its present solver . this is related to goedel 's sequence of increasingly powerful formal theories based on adding formerly unprovable statements to the axioms without affecting previously provable theorems . the continually increasing repertoire of problem solving procedures can be exploited by a parallel search for solutions to additional externally posed tasks . powerplay may be viewed as a greedy but practical implementation of basic principles of creativity . a first experimental analysis can be found in separate papers [ 53,54 ] .

enhancing sentence relation modeling with auxiliary character-level embedding
neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs . however , the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction . to address this challenge , we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings . the two kinds of word sequence representations as inputs into multi-layer bidirectional lstm to learn enhanced sentence representation . after that , we construct matching features followed by another temporal cnn to learn high-level hidden matching feature representations . experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets .

managing inconsistent intelligence
in this paper we demonstrate that it is possible to manage intelligence in constant time as a pre-process to information fusion through a series of processes dealing with issues such as clustering reports , ranking reports with respect to importance , extraction of prototypes from clusters and immediate classification of newly arriving intelligence reports . these methods are used when intelligence reports arrive which concerns different events which should be handled independently , when it is not known a priori to which event each intelligence report is related . we use clustering that runs as a back-end process to partition the intelligence into subsets representing the events , and in parallel , a fast classification that runs as a front-end process in order to put the newly arriving intelligence into its correct information fusion process .

refining reasoning in qualitative probabilistic networks
in recent years there has been a spate of papers describing systems for probabilisitic reasoning which do not use numerical probabilities . in some cases the simple set of values used by these systems make it impossible to predict how a probability will change or which hypothesis is most likely given certain evidence . this paper concentrates on such situations , and suggests a number of ways in which they may be resolved by refining the representation .

copula component analysis
a framework named copula component analysis ( cca ) for blind source separation is proposed as a generalization of independent component analysis ( ica ) . it differs from ica which assumes independence of sources that the underlying components may be dependent with certain structure which is represented by copula . by incorporating dependency structure , much accurate estimation can be made in principle in the case that the assumption of independence is invalidated . a two phrase inference method is introduced for cca which is based on the notion of multidimensional ica .

shaping proto-value functions via rewards
in this paper , we combine task-dependent reward shaping and task-independent proto-value functions to obtain reward dependent proto-value functions ( rpvfs ) . in constructing the rpvfs we are making use of the immediate rewards which are available during the sampling phase but are not used in the pvf construction . we show via experiments that learning with an rpvf based representation is better than learning with just reward shaping or pvfs . in particular , when the state space is symmetrical and the rewards are asymmetrical , the rpvf capture the asymmetry better than the pvfs .

the computational power of dynamic bayesian networks
this paper considers the computational power of constant size , dynamic bayesian networks . although discrete dynamic bayesian networks are no more powerful than hidden markov models , dynamic bayesian networks with continuous random variables and discrete children of continuous parents are capable of performing turing-complete computation . with modified versions of existing algorithms for belief propagation , such a simulation can be carried out in real time . this result suggests that dynamic bayesian networks may be more powerful than previously considered . relationships to causal models and recurrent neural networks are also discussed .

query expansion in information retrieval systems using a bayesian network-based thesaurus
information retrieval ( ir ) is concerned with the identification of documents in a collection that are relevant to a given information need , usually represented as a query containing terms or keywords , which are supposed to be a good description of what the user is looking for . ir systems may improve their effectiveness ( i.e. , increasing the number of relevant documents retrieved ) by using a process of query expansion , which automatically adds new terms to the original query posed by an user . in this paper we develop a method of query expansion based on bayesian networks . using a learning algorithm , we construct a bayesian network that represents some of the relationships among the terms appearing in a given document collection ; this network is then used as a thesaurus ( specific for that collection ) . we also report the results obtained by our method on three standard test collections .

unsupervised grammar induction in a framework of information compression by multiple alignment , unification and search
this paper describes a novel approach to grammar induction that has been developed within a framework designed to integrate learning with other aspects of computing , ai , mathematics and logic . this framework , called `` information compression by multiple alignment , unification and search '' ( icmaus ) , is founded on principles of minimum length encoding pioneered by solomonoff and others . most of the paper describes sp70 , a computer model of the icmaus framework that incorporates processes for unsupervised learning of grammars . an example is presented to show how the model can infer a plausible grammar from appropriate input . limitations of the current model and how they may be overcome are briefly discussed .

pairwise choice markov chains
as datasets capturing human choices grow in richness and scale -- -particularly in online domains -- -there is an increasing need for choice models that escape traditional choice-theoretic axioms such as regularity , stochastic transitivity , and luce 's choice axiom . in this work we introduce the pairwise choice markov chain ( pcmc ) model of discrete choice , an inferentially tractable model that does not assume any of the above axioms while still satisfying the foundational axiom of uniform expansion , a considerably weaker assumption than luce 's choice axiom . we show that the pcmc model significantly outperforms the multinomial logit ( mnl ) model in prediction tasks on both synthetic and empirical datasets known to exhibit violations of luce 's axiom . our analysis also synthesizes several recent observations connecting the multinomial logit model and markov chains ; the pcmc model retains the multinomial logit model as a special case .

decision trees for helpdesk advisor graphs
we use decision trees to build a helpdesk agent reference network to facilitate the on-the-job advising of junior or less experienced staff on how to better address telecommunication customer fault reports . such reports generate field measurements and remote measurements which , when coupled with location data and client attributes , and fused with organization-level statistics , can produce models of how support should be provided . beyond decision support , these models can help identify staff who can act as advisors , based on the quality , consistency and predictability of dealing with complex troubleshooting reports . advisor staff models are then used to guide less experienced staff in their decision making ; thus , we advocate the deployment of a simple mechanism which exploits the availability of staff with a sound track record at the helpdesk to act as dormant tutors .

intelligent personal assistant with knowledge navigation
an intelligent personal agent ( ipa ) is an agent that has the purpose of helping the user to gain information through reliable resources with the help of knowledge navigation techniques and saving time to search the best content . the agent is also responsible for responding to the chat-based queries with the help of conversation corpus . we will be testing different methods for optimal query generation . to felicitate the ease of usage of the application , the agent will be able to accept the input through text ( keyboard ) , voice ( speech recognition ) and server ( facebook ) and output responses using the same method . existing chat bots reply by making changes in the input , but we will give responses based on multiple srt files . the model will learn using the human dialogs dataset and will be able respond human-like . responses to queries about famous things ( places , people , and words ) can be provided using web scraping which will enable the bot to have knowledge navigation features . the agent will even learn from its past experiences supporting semi-supervised learning .

implementing integrity constraints in an existing belief revision system
sneps is a mature knowledge representation , reasoning , and acting system that has long contained a belief revision subsystem , called snebr . snebr is triggered when an explicit contradiction is introduced into the sneps belief space , either because of a user 's new assertion , or because of a user 's query . snebr then makes the user decide what belief to remove from the belief space in order to restore consistency , although it provides information to help the user in making that decision . we have recently added automatic belief revision to snebr , by which , under certain circumstances , snebr decides by itself which belief to remove , and then informs the user of the decision and its consequences . we have used the well-known belief revision integrity constraints as a guide in designing automatic belief revision , taking into account , however , that sneps 's belief space is not deductively closed , and that it would be infeasible to form the deductive closure in order to decide what belief to remove . this paper briefly describes snebr both before and after this revision , discusses how we adapted the integrity constraints for this purpose , and gives an example of the new snebr in action .

influence and dynamic behavior in random boolean networks
we present a rigorous mathematical framework for analyzing dynamics of a broad class of boolean network models . we use this framework to provide the first formal proof of many of the standard critical transition results in boolean network analysis , and offer analogous characterizations for novel classes of random boolean networks . we precisely connect the short-run dynamic behavior of a boolean network to the average influence of the transfer functions . we show that some of the assumptions traditionally made in the more common mean-field analysis of boolean networks do not hold in general . for example , we offer some evidence that imbalance , or expected internal inhomogeneity , of transfer functions is a crucial feature that tends to drive quiescent behavior far more strongly than previously observed .

lattices for dynamic , hierarchic & overlapping categorization : the case of epistemic communities
we present a method for hierarchic categorization and taxonomy evolution description . we focus on the structure of epistemic communities ( ecs ) , or groups of agents sharing common knowledge concerns . introducing a formal framework based on galois lattices , we categorize ecs in an automated and hierarchically structured way and propose criteria for selecting the most relevant epistemic communities - for instance , ecs gathering a certain proportion of agents and thus prototypical of major fields . this process produces a manageable , insightful taxonomy of the community . then , the longitudinal study of these static pictures makes possible an historical description . in particular , we capture stylized facts such as field progress , decline , specialization , interaction ( merging or splitting ) , and paradigm emergence . the detection of such patterns in social networks could fruitfully be applied to other contexts .

some problems for convex bayesians
we discuss problems for convex bayesian decision making and uncertainty representation . these include the inability to accommodate various natural and useful constraints and the possibility of an analog of the classical dutch book being made against an agent behaving in accordance with convex bayesian prescriptions . a more general set-based bayesianism may be as tractable and would avoid the difficulties we raise .

mining generalized patterns from large databases using ontologies
formal concept analysis ( fca ) is a mathematical theory based on the formalization of the notions of concept and concept hierarchies . it has been successfully applied to several computer science fields such as data mining , software engineering , and knowledge engineering , and in many domains like medicine , psychology , linguistics and ecology . for instance , it has been exploited for the design , mapping and refinement of ontologies . in this paper , we show how fca can benefit from a given domain ontology by analyzing the impact of a taxonomy ( on objects and/or attributes ) on the resulting concept lattice . we willmainly concentrate on the usage of a taxonomy to extract generalized patterns ( i.e. , knowledge generated from data when elements of a given domain ontology are used ) in the form of concepts and rules , and improve navigation through these patterns . to that end , we analyze three generalization cases and show their impact on the size of the generalized pattern set . different scenarios of simultaneous generalizations on both objects and attributes are also discussed

counterexample-guided planning
planning in adversarial and uncertain environments can be modeled as the problem of devising strategies in stochastic perfect information games . these games are generalizations of markov decision processes ( mdps ) : there are two ( adversarial ) players , and a source of randomness . the main practical obstacle to computing winning strategies in such games is the size of the state space . in practice therefore , one typically works with abstractions of the model . the diffculty is to come up with an abstraction that is neither too coarse to remove all winning strategies ( plans ) , nor too fine to be intractable . in verification , the paradigm of counterexample-guided abstraction refinement has been successful to construct useful but parsimonious abstractions automatically . we extend this paradigm to probabilistic models ( namely , perfect information games and , as a special case , mdps ) . this allows us to apply the counterexample-guided abstraction paradigm to the ai planning problem . as special cases , we get planning algorithms for mdps and deterministic systems that automatically construct system abstractions .

data generator based on rbf network
there are plenty of problems where the data available is scarce and expensive . we propose a generator of semi-artificial data with similar properties to the original data which enables development and testing of different data mining algorithms and optimization of their parameters . the generated data allow a large scale experimentation and simulations without danger of overfitting . the proposed generator is based on rbf networks which learn sets of gaussian kernels . learned gaussian kernels can be used in a generative mode to generate the data from the same distributions . to asses quality of the generated data we developed several workflows and used them to evaluate the statistical properties of the generated data , structural similarity , and predictive similarity using supervised and unsupervised learning techniques . to determine usability of the proposed generator we conducted a large scale evaluation using 51 uci data sets . the results show a considerable similarity between the original and generated data and indicate that the method can be useful in several development and simulation scenarios .

partial knowledge in embeddings
representing domain knowledge is crucial for any task . there has been a wide range of techniques developed to represent this knowledge , from older logic based approaches to the more recent deep learning based techniques ( i.e . embeddings ) . in this paper , we discuss some of these methods , focusing on the representational expressiveness tradeoffs that are often made . in particular , we focus on the the ability of various techniques to encode ` partial knowledge ' - a key component of successful knowledge systems . we introduce and describe the concepts of ` ensembles of embeddings ' and ` aggregate embeddings ' and demonstrate how they allow for partial knowledge .

limitations of skeptical default reasoning
poole has shown that nonmonotonic logics do not handle the lottery paradox correctly . in this paper we will show that pollock 's theory of defeasible reasoning fails for the same reason : defeasible reasoning is incompatible with the skeptical notion of derivability .

axiomatizing causal reasoning
causal models defined in terms of a collection of equations , as defined by pearl , are axiomatized here . axiomatizations are provided for three successively more general classes of causal models : ( 1 ) the class of recursive theories ( those without feedback ) , ( 2 ) the class of theories where the solutions to the equations are unique , ( 3 ) arbitrary theories ( where the equations may not have solutions and , if they do , they are not necessarily unique ) . it is shown that to reason about causality in the most general third class , we must extend the language used by galles and pearl . in addition , the complexity of the decision procedures is characterized for all the languages and classes of models considered .

evidence for the size principle in semantic and perceptual domains
shepard 's universal law of generalization offered a compelling case for the first physics-like law in cognitive science that should hold for all intelligent agents in the universe . shepard 's account is based on a rational bayesian model of generalization , providing an answer to the question of why such a law should emerge . extending this account to explain how humans use multiple examples to make better generalizations requires an additional assumption , called the size principle : hypotheses that pick out fewer objects should make a larger contribution to generalization . the degree to which this principle warrants similarly law-like status is far from conclusive . typically , evaluating this principle has not been straightforward , requiring additional assumptions . we present a new method for evaluating the size principle that is more direct , and apply this method to a diverse array of datasets . our results provide support for the broad applicability of the size principle .

fuzzy integer linear programming mathematical models for examination timetable problem
etp is np hard combinatorial optimization problem . it has received tremendous research attention during the past few years given its wide use in universities . in this paper , we develop three mathematical models for nsou , kolkata , india using filp technique . to deal with impreciseness and vagueness we model various allocation variables through fuzzy numbers . the solution to the problem is obtained using fuzzy number ranking method . each feasible solution has fuzzy number obtained by fuzzy objective function . the different filp technique performance are demonstrated by experimental data generated through extensive simulation from nsou , kolkata , india in terms of its execution times . the proposed filp models are compared with commonly used heuristic viz . ilp approach on experimental data which gives an idea about quality of heuristic . the techniques are also compared with different artificial intelligence based heuristics for etp with respect to best and mean cost as well as execution time measures on carter benchmark datasets to illustrate its effectiveness . filp takes an appreciable amount of time to generate satisfactory solution in comparison to other heuristics . the formulation thus serves as good benchmark for other heuristics . the experimental study presented here focuses on producing a methodology that generalizes well over spectrum of techniques that generates significant results for one or more datasets . the performance of filp model is finally compared to the best results cited in literature for carter benchmarks to assess its potential . the problem can be further reduced by formulating with lesser number of allocation variables it without affecting optimality of solution obtained . flip model for etp can also be adapted to solve other etp as well as combinatorial optimization problems .

complexity results and approximation strategies for map explanations
map is the problem of finding a most probable instantiation of a set of variables given evidence . map has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation pr , or the problem of computing the most probable explanation ( mpe ) . this paper investigates the complexity of map in bayesian networks . specifically , we show that map is complete for np^pp and provide further negative complexity results for algorithms based on variable elimination . we also show that map remains hard even when mpe and pr become easy . for example , we show that map is np-complete when the networks are restricted to polytrees , and even then can not be effectively approximated . given the difficulty of computing map exactly , and the difficulty of approximating map while providing useful guarantees on the resulting approximation , we investigate best effort approximations . we introduce a generic map approximation framework . we provide two instantiations of the framework ; one for networks which are amenable to exact inference pr , and one for networks for which even exact inference is too hard . this allows map approximation on networks that are too complex to even exactly solve the easier problems , pr and mpe . experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques , and provide accurate map estimates in many cases .

i 'm sorry to say , but your understanding of image processing fundamentals is absolutely wrong
the ongoing discussion whether modern vision systems have to be viewed as visually-enabled cognitive systems or cognitively-enabled vision systems is groundless , because perceptual and cognitive faculties of vision are separate components of human ( and consequently , artificial ) information processing system modeling .

bridging the gap between reinforcement learning and knowledge representation : a logical off- and on-policy framework
knowledge representation is important issue in reinforcement learning . in this paper , we bridge the gap between reinforcement learning and knowledge representation , by providing a rich knowledge representation framework , based on normal logic programs with answer set semantics , that is capable of solving model-free reinforcement learning problems for more complex do-mains and exploits the domain-specific knowledge . we prove the correctness of our approach . we show that the complexity of finding an offline and online policy for a model-free reinforcement learning problem in our approach is np-complete . moreover , we show that any model-free reinforcement learning problem in mdp environment can be encoded as a sat problem . the importance of that is model-free reinforcement

bayesian poker
poker is ideal for testing automated reasoning under uncertainty . it introduces uncertainty both by physical randomization and by incomplete information about opponents hands.another source of uncertainty is the limited information available to construct psychological models of opponents , their tendencies to bluff , play conservatively , reveal weakness , etc . and the relation between their hand strengths and betting behaviour . all of these uncertainties must be assessed accurately and combined effectively for any reasonable level of skill in the game to be achieved , since good decision making is highly sensitive to those tasks.we describe our bayesian poker program ( bpp ) , which uses a bayesian network to model the programs poker hand , the opponents hand and the opponents playing behaviour conditioned upon the hand , and betting curves which govern play given a probability of winning . the history of play with opponents is used to improve bpps understanding of their behaviour.we compare bpp experimentally with : a simple rule - based system ; a program which depends exclusively on hand probabilities ( i.e. , without opponent modeling ) ; and with human players.bpp has shown itself to be an effective player against all these opponents , barring the better humans.we also sketch out some likely ways of improving play .

scope for machine learning in digital manufacturing
this provocation paper provides an overview of the underlying optimisation problem in the emerging field of digital manufacturing . initially , this paper discusses how the notion of digital manufacturing is transforming from a term describing a suite of software tools for the integration of production and design functions towards a more general concept incorporating computerised manufacturing and supply chain processes , as well as information collection and utilisation across the product life cycle . on this basis , we use the example of one such manufacturing process , additive manufacturing , to identify an integrated multi-objective optimisation problem underlying digital manufacturing . forming an opportunity for a concurrent application of data science and optimisation , a set of challenges arising from this problem is outlined .

an axiomatic approach to the roughness measure of rough sets
in pawlak 's rough set theory , a set is approximated by a pair of lower and upper approximations . to measure numerically the roughness of an approximation , pawlak introduced a quantitative measure of roughness by using the ratio of the cardinalities of the lower and upper approximations . although the roughness measure is effective , it has the drawback of not being strictly monotonic with respect to the standard ordering on partitions . recently , some improvements have been made by taking into account the granularity of partitions . in this paper , we approach the roughness measure in an axiomatic way . after axiomatically defining roughness measure and partition measure , we provide a unified construction of roughness measure , called strong pawlak roughness measure , and then explore the properties of this measure . we show that the improved roughness measures in the literature are special instances of our strong pawlak roughness measure and introduce three more strong pawlak roughness measures as well . the advantage of our axiomatic approach is that some properties of a roughness measure follow immediately as soon as the measure satisfies the relevant axiomatic definition .

learning to select computations
efficient use of limited computational resources is essential to intelligence . selecting computations optimally according to rational metareasoning would achieve this , but rational metareasoning is computationally intractable . inspired by psychology and neuroscience , we propose the first learning algorithm for approximating the optimal selection of computations . we derive a general , sample-efficient reinforcement learning algorithm for learning to select computations from the insight that the value of computation lies between the myopic value of computation and the value of perfect information . we evaluate the performance of our method against two state-of-the-art methods for approximate metareasoning -- the meta-greedy heuristic and the blinkered policy -- on three increasingly difficult metareasoning problems : metareasoning about when to terminate computation , metareasoning about how to choose between multiple actions , and metareasoning about planning . across all three domains , our method achieved near-optimal performance and significantly outperformed the meta-greedy heuristic . the blinkered policy performed on par with our method in metareasoning about decision-making , but it is not directly applicable to metareasoning about planning where our method outperformed both the meta-greedy heuristic and a generalization of the blinkered policy . our results are a step towards building self-improving ai systems that can learn to make optimal use of their limited computational resources to efficiently solve complex problems in real-time .

managing sparsity , time , and quality of inference in topic models
inference is an integral part of probabilistic topic models , but is often non-trivial to derive an efficient algorithm for a specific model . it is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents . in this article , we introduce a simple framework for inference in probabilistic topic models , denoted by fw . this framework is general and flexible enough to be easily adapted to mixture models . it has a linear convergence rate , offers an easy way to incorporate prior knowledge , and provides us an easy way to directly trade off sparsity against quality and time . we demonstrate the goodness and flexibility of fw over existing inference methods by a number of tasks . finally , we show how inference in topic models with nonconjugate priors can be done efficiently .

linking makinson and kraus-lehmann-magidor preferential entailments
about ten years ago , various notions of preferential entailment have been introduced . the main reference is a paper by kraus , lehmann and magidor ( klm ) , one of the main competitor being a more general version defined by makinson ( mak ) . these two versions have already been compared , but it is time to revisit these comparisons . here are our three main results : ( 1 ) these two notions are equivalent , provided that we restrict our attention , as done in klm , to the cases where the entailment respects logical equivalence ( on the left and on the right ) . ( 2 ) a serious simplification of the description of the fundamental cases in which mak is equivalent to klm , including a natural passage in both ways . ( 3 ) the two previous results are given for preferential entailments more general than considered in some of the original texts , but they apply also to the original definitions and , for this particular case also , the models can be simplified .

committment-based data-aware multi-agent-contexts systems
communication and interaction among agents have been the subject of extensive investigation since many years . commitment-based communication , where communicating agents are seen as a debtor agent who is committed to a creditor agent to bring about something ( possibly under some conditions ) is now very well-established . the approach of dacmas ( data-aware commitment-based mas ) lifts commitment-related approaches proposed in the literature from a propositional to a first-order setting via the adoption the drl-lite description logic . notably , dacmass provide , beyond commitments , simple forms of inter-agent event-based communication . yet , the aspect is missing of making a mas able to acquire knowledge from contexts which are not agents and which are external to the mas . this topic is coped with in managed mcss ( managed multi-context systems ) , where however exchanges are among knowledge bases and not agents . in this paper , we propose the new approach of dacmmcmass ( data-aware commitment-based managed multi- context mas ) , so as to obtain a commitment-based first-order agent system which is able to interact with heterogeneous external information sources . we show that dacmmcmass retain the nice formal properties of the original approaches .

on the uniform one-dimensional fragment
the uniform one-dimensional fragment of first-order logic , u1 , is a recently introduced formalism that extends two-variable logic in a natural way to contexts with relations of all arities . we survey properties of u1 and investigate its relationship to description logics designed to accommodate higher arity relations , with particular attention given to dlr_reg . we also define a description logic version of a variant of u1 and prove a range of new results concerning the expressivity of u1 and related logics .

probabilistic conflict resolution in hierarchical hypothesis spaces
artificial intelligence applications such as industrial robotics , military surveillance , and hazardous environment clean-up , require situation understanding based on partial , uncertain , and ambiguous or erroneous evidence . it is necessary to evaluate the relative likelihood of multiple possible hypotheses of the ( current ) situation faced by the decision making program . often , the evidence and hypotheses are hierarchical in nature . in image understanding tasks , for example , evidence begins with raw imagery , from which ambiguous features are extracted which have multiple possible aggregations providing evidential support for the presence of multiple hypothesis of objects and terrain , which in turn aggregate in multiple ways to provide partial evidence for different interpretations of the ambient scene . information fusion for military situation understanding has a similar evidence/hypothesis hierarchy from multiple sensor through message level interpretations , and also provides evidence at multiple levels of the doctrinal hierarchy of military forces .

particle value functions
the policy gradients of the expected return objective can react slowly to rare rewards . yet , in some cases agents may wish to emphasize the low or high returns regardless of their probability . borrowing from the economics and control literature , we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example . this risk-sensitive value function is not always applicable to reinforcement learning problems , so we introduce the particle value function defined by a particle filter over the distributions of an agent 's experience , which bounds the risk-sensitive one . we illustrate the benefit of the policy gradients of this objective in cliffworld .

the most advantageous bangla keyboard layout using data mining technique
bangla alphabet has a large number of letters , for this it is complicated to type faster using bangla keyboard . the proposed keyboard will maximize the speed of operator as they can type with both hands parallel . association rule of data mining to distribute the bangla characters in the keyboard is used here . the frequencies of data consisting of monograph , digraph and trigraph are analyzed , which are derived from data wire-house , and then used association rule of data mining to distribute the bangla characters in the layout . experimental results on several data show the effectiveness of the proposed approach with better performance . this paper presents an optimal bangla keyboard layout , which distributes the load equally on both hands so that maximizing the ease and minimizing the effort .

optimizations for decision making and planning in description logic dynamic knowledge bases
artifact-centric models for business processes recently raised a lot of attention , as they manage to combine structural ( i.e . data related ) with dynamical ( i.e . process related ) aspects in a seamless way . many frameworks developed under this approach , although , are not built explicitly for planning , one of the most prominent operations related to business processes . in this paper , we try to overcome this by proposing a framework named dynamic knowledge bases , aimed at describing rich business domains through description logic-based ontologies , and where a set of actions allows the system to evolve by modifying such ontologies . this framework , by offering action rewriting and knowledge partialization , represents a viable and formal environment to develop decision making and planning techniques for dl-based artifact-centric business domains .

causality and responsibility for formal verification and beyond
the theory of actual causality , defined by halpern and pearl , and its quantitative measure - the degree of responsibility - was shown to be extremely useful in various areas of computer science due to a good match between the results it produces and our intuition . in this paper , i describe the applications of causality to formal verification , namely , explanation of counterexamples , refinement of coverage metrics , and symbolic trajectory evaluation . i also briefly discuss recent applications of causality to legal reasoning .

continuous adaptation via meta-learning in nonstationary and competitive environments
ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence . in this paper , we cast the problem of continuous adaptation into the learning-to-learn framework . we develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios . additionally , we design a new multi-agent competitive environment , robosumo , and define iterated adaptation games for testing various aspects of continuous adaptation strategies . we demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime . our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest .

feature importance in bayesian assessment of newborn brain maturity from eeg
the methodology of bayesian model averaging ( bma ) is applied for assessment of newborn brain maturity from sleep eeg . in theory this methodology provides the most accurate assessments of uncertainty in decisions . however , the existing bma techniques have been shown providing biased assessments in the absence of some prior information enabling to explore model parameter space in details within a reasonable time . the lack in details leads to disproportional sampling from the posterior distribution . in case of the eeg assessment of brain maturity , bma results can be biased because of the absence of information about eeg feature importance . in this paper we explore how the posterior information about eeg features can be used in order to reduce a negative impact of disproportional sampling on bma performance . we use eeg data recorded from sleeping newborns to test the efficiency of the proposed bma technique .

safe reinforcement learning via shielding
reinforcement learning algorithms discover policies that maximize reward , but do not necessarily guarantee safety during learning or execution phases . we introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic . to this end , given the temporal logic specification that is to be obeyed by the learning system , we propose to synthesize a reactive system called a shield . the shield is introduced in the traditional learning process in two alternative ways , depending on the location at which the shield is implemented . in the first one , the shield acts each time the learning agent is about to make a decision and provides a list of safe actions . in the second way , the shield is introduced after the learning agent . the shield monitors the actions from the learner and corrects them only if the chosen action causes a violation of the specification . we discuss which requirements a shield must meet to preserve the convergence guarantees of the learner . finally , we demonstrate the versatility of our approach on several challenging reinforcement learning scenarios .

ontological multidimensional data models and contextual data qality
data quality assessment and data cleaning are context-dependent activities . motivated by this observation , we propose the ontological multidimensional data model ( omd model ) , which can be used to model and represent contexts as logic-based ontologies . the data under assessment is mapped into the context , for additional analysis , processing , and quality data extraction . the resulting contexts allow for the representation of dimensions , and multidimensional data quality assessment becomes possible . at the core of a multidimensional context we include a generalized multidimensional data model and a datalog+/- ontology with provably good properties in terms of query answering . these main components are used to represent dimension hierarchies , dimensional constraints , dimensional rules , and define predicates for quality data specification . query answering relies upon and triggers navigation through dimension hierarchies , and becomes the basic tool for the extraction of quality data . the omd model is interesting per se , beyond applications to data quality . it allows for a logic-based , and computationally tractable representation of multidimensional data , extending previous multidimensional data models with additional expressive power and functionalities .

the logical meaning of expansion
the expansion property considered by researchers in social choice is shown to correspond to a logical property of nonmonotonic consequence relations that is the { \em pure } , i.e. , not involving connectives , version of a previously known weak rationality condition . the assumption that the union of two definable sets of models is definable is needed for the soundness part of the result .

the kb paradigm and its application to interactive configuration
the knowledge base paradigm aims to express domain knowledge in a rich formal language , and to use this domain knowledge as a knowledge base to solve various problems and tasks that arise in the domain by applying multiple forms of inference . as such , the paradigm applies a strict separation of concerns between information and problem solving . in this paper , we analyze the principles and feasibility of the knowledge base paradigm in the context of an important class of applications : interactive configuration problems . in interactive configuration problems , a configuration of interrelated objects under constraints is searched , where the system assists the user in reaching an intended configuration . it is widely recognized in industry that good software solutions for these problems are very difficult to develop . we investigate such problems from the perspective of the kb paradigm . we show that multiple functionalities in this domain can be achieved by applying different forms of logical inferences on a formal specification of the configuration domain . we report on a proof of concept of this approach in a real-life application with a banking company . to appear in theory and practice of logic programming ( tplp ) .

application of fuzzy logic in design of smart washing machine
washing machine is of great domestic necessity as it frees us from the burden of washing our clothes and saves ample of our time . this paper will cover the aspect of designing and developing of fuzzy logic based , smart washing machine . the regular washing machine ( timer based ) makes use of multi-turned timer based start-stop mechanism which is mechanical as is prone to breakage . in addition to its starting and stopping issues , the mechanical timers are not efficient with respect of maintenance and electricity usage . recent developments have shown that merger of digital electronics in optimal functionality of this machine is possible and nowadays in practice . a number of international renowned companies have developed the machine with the introduction of smart artificial intelligence . such a machine makes use of sensors and smartly calculates the amount of run-time ( washing time ) for the main machine motor . realtime calculations and processes are also catered in optimizing the run-time of the machine . the obvious result is smart time management , better economy of electricity and efficiency of work . this paper deals with the indigenization of flc ( fuzzy logic controller ) based washing machine , which is capable of automating the inputs and getting the desired output ( wash-time ) .

a stochastic temporal model of polyphonic midi performance with ornaments
we study indeterminacies in realization of ornaments and how they can be incorporated in a stochastic performance model applicable for music information processing such as score-performance matching . we point out the importance of temporal information , and propose a hidden markov model which describes it explicitly and represents ornaments with several state types . following a review of the indeterminacies , they are carefully incorporated into the model through its topology and parameters , and the state construction for quite general polyphonic scores is explained in detail . by analyzing piano performance data , we find significant overlaps in inter-onset-interval distributions of chordal notes , ornaments , and inter-chord events , and the data is used to determine details of the model . the model is applied for score following and offline score-performance matching , yielding highly accurate matching for performances with many ornaments and relatively frequent errors , repeats , and skips .

a logical characterization of iterated admissibility
brandenburger , friedenberg , and keisler provide an epistemic characterization of iterated admissibility ( i.e. , iterated deletion of weakly dominated strategies ) where uncertainty is represented using lpss ( lexicographic probability sequences ) . their characterization holds in a rich structure called a complete structure , where all types are possible . here , a logical charaacterization of iterated admisibility is given that involves only standard probability and holds in all structures , not just complete structures . a stronger notion of strong admissibility is then defined . roughly speaking , strong admissibility is meant to capture the intuition that `` all the agent knows '' is that the other agents satisfy the appropriate rationality assumptions . strong admissibility makes it possible to relate admissibility , canonical structures ( as typically considered in completeness proofs in modal logic ) , complete structures , and the notion of `` all i know '' .

model-agnostic meta-learning for fast adaptation of deep networks
we propose an algorithm for meta-learning that is model-agnostic , in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems , including classification , regression , and reinforcement learning . the goal of meta-learning is to train a model on a variety of learning tasks , such that it can solve new learning tasks using only a small number of training samples . in our approach , the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task . in effect , our method trains the model to be easy to fine-tune . we demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks , produces good results on few-shot regression , and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies .

understanding and comparing deep neural networks for age and gender classification
recently , deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images . however , these models were applied in a black-box manner with no information provided about which facial features are actually used for prediction and how these features depend on image preprocessing , model initialization and architecture choice . we present a study investigating these different effects . in detail , our work compares four popular neural network architectures , studies the effect of pretraining , evaluates the robustness of the considered alignment preprocessings via cross-method test set swapping and intuitively visualizes the model 's prediction strategies in given preprocessing conditions using the recent layer-wise relevance propagation ( lrp ) algorithm . our evaluations on the challenging adience benchmark show that suitable parameter initialization leads to a holistic perception of the input , compensating artefactual data representations . with a combination of simple preprocessing steps , we reach state of the art performance in gender recognition .

defining concepts of emotion : from philosophy to science
this paper is motivated by a series of ( related ) questions as to whether a computer can have pleasure and pain , what pleasure ( and intensity of pleasure ) is , and , ultimately , what concepts of emotion are . to determine what an emotion is , is a matter of conceptualization , namely , understanding and explicitly encoding the concept of emotion as people use it in everyday life . this is a notoriously difficult problem ( frijda , 1986 , fehr \ & russell , 1984 ) . this paper firstly shows why this is a difficult problem by aligning it with the conceptualization of a few other so called semantic primitives such as `` exist '' , `` force '' , `` big '' ( plus `` limit '' ) . the definitions of these thought-to-be-indefinable concepts , given in this paper , show what formal definitions of concepts look like and how concepts are constructed . as a by-product , owing to the explicit account of the meaning of `` exist '' , the famous dispute between einstein and bohr is naturally resolved from linguistic point of view . secondly , defending frijda 's view that emotion is action tendency ( or ryle 's behavioral disposition ( propensity ) ) , we give a list of emotions defined in terms of action tendency . in particular , the definitions of pleasure and the feeling of beauty are presented . further , we give a formal definition of `` action tendency '' , from which the concept of `` intensity '' of emotions ( including pleasure ) is naturally derived in a formal fashion . the meanings of `` wish '' , `` wait '' , `` good '' , `` hot '' are analyzed .

principles and examples of plausible reasoning and propositional plausible logic
plausible reasoning concerns situations whose inherent lack of precision is not quantified ; that is , there are no degrees or levels of precision , and hence no use of numbers like probabilities . a hopefully comprehensive set of principles that clarifies what it means for a formal logic to do plausible reasoning is presented . a new propositional logic , called propositional plausible logic ( ppl ) , is defined and applied to some important examples . ppl is the only non-numeric non-monotonic logic we know of that satisfies all the principles and correctly reasons with all the examples . some important results about ppl are proved .

bayesian networks aplied to therapy monitoring
we propose a general bayesian network model for application in a wide class of problems of therapy monitoring . we discuss the use of stochastic simulation as a computational approach to inference on the proposed class of models . as an illustration we present an application to the monitoring of cytotoxic chemotherapy in breast cancer .

multi-label learning with global and local label correlation
it is well-known that exploiting label correlations is important to multi-label learning . existing approaches either assume that the label correlations are global and shared by all instances ; or that the label correlations are local and shared only by a data subset . in fact , in the real-world applications , both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances . moreover , it is also a usual case that only partial labels are observed , which makes the exploitation of the label correlations much more difficult . that is , it is hard to estimate the label correlations when many labels are absent . in this paper , we propose a new multi-label approach glocal dealing with both the full-label and the missing-label cases , exploiting global and local label correlations simultaneously , through learning a latent label representation and optimizing label manifolds . the extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data .

similarity assessment through blocking and affordance assignment in textual cbr
it has been conceived that children learn new objects through their affordances , that is , the actions that can be taken on them . we suggest that web pages also have affordances defined in terms of the users ' information need they meet . an assumption of the proposed approach is that different parts of a text may not be equally important / relevant to a given query . judgment on the relevance of a web document requires , therefore , a thorough look into its parts , rather than treating it as a monolithic content . we propose a method to extract and assign affordances to texts and then use these affordances to retrieve the corresponding web pages . the overall approach presented in the paper relies on case-based representations that bridge the queries to the affordances of web documents . we tested our method on the tourism domain and the results are promising .

hybrid metaheuristics for the clustered vehicle routing problem
the clustered vehicle routing problem ( cluvrp ) is a variant of the capacitated vehicle routing problem in which customers are grouped into clusters . each cluster has to be visited once , and a vehicle entering a cluster can not leave it until all customers have been visited . this article presents two alternative hybrid metaheuristic algorithms for the cluvrp . the first algorithm is based on an iterated local search algorithm , in which only feasible solutions are explored and problem-specific local search moves are utilized . the second algorithm is a hybrid genetic search , for which the shortest hamiltonian path between each pair of vertices within each cluster should be precomputed . using this information , a sequence of clusters can be used as a solution representation and large neighborhoods can be efficiently explored by means of bi-directional dynamic programming , sequence concatenations , by using appropriate data structures . extensive computational experiments are performed on benchmark instances from the literature , as well as new large scale ones . recommendations on promising algorithm choices are provided relatively to average cluster size .

well-definedness and efficient inference for probabilistic logic programming under the distribution semantics
the distribution semantics is one of the most prominent approaches for the combination of logic programming and probability theory . many languages follow this semantics , such as independent choice logic , prism , pd , logic programs with annotated disjunctions ( lpads ) and problog . when a program contains functions symbols , the distribution semantics is well-defined only if the set of explanations for a query is finite and so is each explanation . well-definedness is usually either explicitly imposed or is achieved by severely limiting the class of allowed programs . in this paper we identify a larger class of programs for which the semantics is well-defined together with an efficient procedure for computing the probability of queries . since lpads offer the most general syntax , we present our results for them , but our results are applicable to all languages under the distribution semantics . we present the algorithm `` probabilistic inference with tabling and answer subsumption '' ( pita ) that computes the probability of queries by transforming a probabilistic program into a normal program and then applying slg resolution with answer subsumption . pita has been implemented in xsb and tested on six domains : two with function symbols and four without . the execution times are compared with those of problog , cplint and cve , pita was almost always able to solve larger problems in a shorter time , on domains with and without function symbols .

decision flexibility
the development of new methods and representations for temporal decision-making requires a principled basis for characterizing and measuring the flexibility of decision strategies in the face of uncertainty . our goal in this paper is to provide a framework - not a theory - for observing how decision policies behave in the face of informational perturbations , to gain clues as to how they might behave in the face of unanticipated , possibly unarticulated uncertainties . to this end , we find it beneficial to distinguish between two types of uncertainty : `` small world '' and `` large world '' uncertainty . the first type can be resolved by posing an unambiguous question to a `` clairvoyant , '' and is anchored on some well-defined aspect of a decision frame . the second type is more troublesome , yet it is often of greater interest when we address the issue of flexibility ; this type of uncertainty can be resolved only by consulting a `` psychic . '' we next observe that one approach to flexibility used in the economics literature is already implicitly accounted for in the maximum expected utility ( meu ) principle from decision theory . though simple , the observation establishes the context for a more illuminating notion of flexibility , what we term flexibility with respect to information revelation . we show how to perform flexibility analysis of a static ( i.e. , single period ) decision problem using a simple example , and we observe that the most flexible alternative thus identified is not necessarily the meu alternative . we extend our analysis for a dynamic ( i.e. , multi-period ) model , and we demonstrate how to calculate the value of flexibility for decision strategies that allow downstream revision of an upstream commitment decision .

conversion rate optimization through evolutionary computation
conversion optimization means designing a web interface so that as many users as possible take a desired action on it , such as register or purchase . such design is usually done by hand , testing one change at a time through a/b testing , or a limited number of combinations through multivariate testing , making it possible to evaluate only a small fraction of designs in a vast design space . this paper describes sentient ascend , an automatic conversion optimization system that uses evolutionary optimization to create effective web interface designs . ascend makes it possible to discover and utilize interactions between the design elements that are difficult to identify otherwise . moreover , evaluation of design candidates is done in parallel online , i.e . with a large number of real users interacting with the system . a case study on an existing media site shows that significant improvements ( i.e . over 43 % ) are possible beyond human design . ascend can therefore be seen as an approach to massively multivariate conversion optimization , based on a massively parallel interactive evolution .

exploration potential
we introduce exploration potential , a quantity that measures how much a reinforcement learning agent has explored its environment class . in contrast to information gain , exploration potential takes the problem 's reward structure into account . this leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality ( learning to act optimally across the entire environment class ) . our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation .

the cognitive processing of causal knowledge
there is a brief description of the probabilistic causal graph model for representing , reasoning with , and learning causal structure using bayesian networks . it is then argued that this model is closely related to how humans reason with and learn causal structure . it is shown that studies in psychology on discounting ( reasoning concerning how the presence of one cause of an effect makes another cause less probable ) support the hypothesis that humans reach the same judgments as algorithms for doing inference in bayesian networks . next , it is shown how studies by piaget indicate that humans learn causal structure by observing the same independencies and dependencies as those used by certain algorithms for learning the structure of a bayesian network . based on this indication , a subjective definition of causality is forwarded . finally , methods for further testing the accuracy of these claims are discussed .

interacting attention-gated recurrent networks for recommendation
capturing the temporal dynamics of user preferences over items is important for recommendation . existing methods mainly assume that all time steps in user-item interaction history are equally relevant to recommendation , which however does not apply in real-world scenarios where user-item interactions can often happen accidentally . more importantly , they learn user and item dynamics separately , thus failing to capture their joint effects on user-item interactions . to better model user and item dynamics , we present the interacting attention-gated recurrent network ( iarn ) which adopts the attention model to measure the relevance of each time step . in particular , we propose a novel attention scheme to learn the attention scores of user and item history in an interacting way , thus to account for the dependencies between user and item dynamics in shaping user-item interactions . by doing so , iarn can selectively memorize different time steps of a user 's history when predicting her preferences over different items . our model can therefore provide meaningful interpretations for recommendation results , which could be further enhanced by auxiliary features . extensive validation on real-world datasets shows that iarn consistently outperforms state-of-the-art methods .

attacker and defender counting approach for abstract argumentation
in dung 's abstract argumentation , arguments are either acceptable or unacceptable , given a chosen notion of acceptability . this gives a coarse way to compare arguments . in this paper , we propose a counting approach for a more fine-gained assessment to arguments by counting the number of their respective attackers and defenders based on argument graph and argument game . an argument is more acceptable if the proponent puts forward more number of defenders for it and the opponent puts forward less number of attackers against it . we show that our counting model has two well-behaved properties : normalization and convergence . then , we define a counting semantics based on this model , and investigate some general properties of the semantics .

random projections for $ k $ -means clustering
this paper discusses the topic of dimensionality reduction for $ k $ -means clustering . we prove that any set of $ n $ points in $ d $ dimensions ( rows in a matrix $ a \in \rr^ { n \times d } $ ) can be projected into $ t = \omega ( k / \eps^2 ) $ dimensions , for any $ \eps \in ( 0,1/3 ) $ , in $ o ( n d \lceil \eps^ { -2 } k/ \log ( d ) \rceil ) $ time , such that with constant probability the optimal $ k $ -partition of the point set is preserved within a factor of $ 2+\eps $ . the projection is done by post-multiplying $ a $ with a $ d \times t $ random matrix $ r $ having entries $ +1/\sqrt { t } $ or $ -1/\sqrt { t } $ with equal probability . a numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results .

anytime decision making with imprecise probabilities
this paper examines methods of decision making that are able to accommodate limitations on both the form in which uncertainty pertaining to a decision problem can be realistically represented and the amount of computing time available before a decision must be made . the methods are anytime algorithms in the sense of boddy and dean 1991. techniques are presented for use with frisch and haddawy 's [ 1992 ] anytime deduction system , with an anytime adaptation of nilsson 's [ 1986 ] probabilistic logic , and with a probabilistic database model .

covariance plasticity and regulated criticality
we propose that a regulation mechanism based on hebbian covariance plasticity may cause the brain to operate near criticality . we analyze the effect of such a regulation on the dynamics of a network with excitatory and inhibitory neurons and uniform connectivity within and across the two populations . we show that , under broad conditions , the system converges to a critical state lying at the common boundary of three regions in parameter space ; these correspond to three modes of behavior : high activity , low activity , oscillation .

boundary properties of the inconsistency of pairwise comparisons in group decisions
this paper proposes an analysis of the effects of consensus and preference aggregation on the consistency of pairwise comparisons . we define some boundary properties for the inconsistency of group preferences and investigate their relation with different inconsistency indices . some results are presented on more general dependencies between properties of inconsistency indices and the satisfaction of boundary properties . in the end , given three boundary properties and nine indices among the most relevant ones , we will be able to present a complete analysis of what indices satisfy what properties and offer a reflection on the interpretation of the inconsistency of group preferences .

fast exact search in hamming space with multi-index hashing
there is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search . although binary codes are motivated by their use as direct indices ( addresses ) into a hash table , codes longer than 32 bits are not being used as such , as it was thought to be ineffective . we introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in hamming space . the approach is storage efficient and straightforward to implement . theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes . empirical results show dramatic speedups over a linear scan baseline for datasets of up to one billion codes of 64 , 128 , or 256 bits .

belief update in clg bayesian networks with lazy propagation
in recent years bayesian networks ( bns ) with a mixture of continuous and discrete variables have received an increasing level of attention . we present an architecture for exact belief update in conditional linear gaussian bns ( clg bns ) . the architecture is an extension of lazy propagation using operations of lauritzen & jensen [ 6 ] and cowell [ 2 ] . by decomposing clique and separator potentials into sets of factors , the proposed architecture takes advantage of independence and irrelevance properties induced by the structure of the graph and the evidence . the resulting benefits are illustrated by examples . results of a preliminary empirical performance evaluation indicate a significant potential of the proposed architecture .

description logics with fuzzy concrete domains
we present a fuzzy version of description logics with concrete domains . main features are : ( i ) concept constructors are based on t-norm , t-conorm , negation and implication ; ( ii ) concrete domains are fuzzy sets ; ( iii ) fuzzy modifiers are allowed ; and ( iv ) the reasoning algorithm is based on a mixture of completion rules and bounded mixed integer programming .

epistemic foundation of stable model semantics
stable model semantics has become a very popular approach for the management of negation in logic programming . this approach relies mainly on the closed world assumption to complete the available knowledge and its formulation has its basis in the so-called gelfond-lifschitz transformation . the primary goal of this work is to present an alternative and epistemic-based characterization of stable model semantics , to the gelfond-lifschitz transformation . in particular , we show that stable model semantics can be defined entirely as an extension of the kripke-kleene semantics . indeed , we show that the closed world assumption can be seen as an additional source of ` falsehood ' to be added cumulatively to the kripke-kleene semantics . our approach is purely algebraic and can abstract from the particular formalism of choice as it is based on monotone operators ( under the knowledge order ) over bilattices only .

on minimal constraint networks
in a minimal binary constraint network , every tuple of a constraint relation can be extended to a solution . the tractability or intractability of computing a solution to such a minimal network was a long standing open question . dechter conjectured this computation problem to be np-hard . we prove this conjecture . we also prove a conjecture by dechter and pearl stating that for k\geq2 it is np-hard to decide whether a single constraint can be decomposed into an equivalent k-ary constraint network . we show that this holds even in case of bi-valued constraints where k\geq3 , which proves another conjecture of dechter and pearl . finally , we establish the tractability frontier for this problem with respect to the domain cardinality and the parameter k .

the diamond system for argumentation : preliminary report
abstract dialectical frameworks ( adfs ) are a powerful generalisation of dung 's abstract argumentation frameworks . in this paper we present an answer set programming based software system , called diamond ( dialectical models encoding ) . it translates adfs into answer set programs whose stable models correspond to models of the adf with respect to several semantics ( i.e . admissible , complete , stable , grounded ) .

automated inference system for end-to-end diagnosis of network performance issues in client-terminal devices
traditional network diagnosis methods of client-terminal device ( ctd ) problems tend to be laborintensive , time consuming , and contribute to increased customer dissatisfaction . in this paper , we propose an automated solution for rapidly diagnose the root causes of network performance issues in ctd . based on a new intelligent inference technique , we create the intelligent automated client diagnostic ( iacd ) system , which only relies on collection of transmission control protocol ( tcp ) packet traces . using soft-margin support vector machine ( svm ) classifiers , the system ( i ) distinguishes link problems from client problems and ( ii ) identifies characteristics unique to the specific fault to report the root cause . the modular design of the system enables support for new access link and fault types . experimental evaluation demonstrated the capability of the iacd system to distinguish between faulty and healthy links and to diagnose the client faults with 98 % accuracy . the system can perform fault diagnosis independent of the user 's specific tcp implementation , enabling diagnosis of diverse range of client devices

set unification
the unification problem in algebras capable of describing sets has been tackled , directly or indirectly , by many researchers and it finds important applications in various research areas -- e.g. , deductive databases , theorem proving , static analysis , rapid software prototyping . the various solutions proposed are spread across a large literature . in this paper we provide a uniform presentation of unification of sets , formalizing it at the level of set theory . we address the problem of deciding existence of solutions at an abstract level . this provides also the ability to classify different types of set unification problems . unification algorithms are uniformly proposed to solve the unification problem in each of such classes . the algorithms presented are partly drawn from the literature -- and properly revisited and analyzed -- and partly novel proposals . in particular , we present a new goal-driven algorithm for general aci1 unification and a new simpler algorithm for general ( ab ) ( cl ) unification .

intelligent search of correlated alarms for gsm networks with model-based constraints
in order to control the process of data mining and focus on the things of interest to us , many kinds of constraints have been added into the algorithms of data mining . however , discovering the correlated alarms in the alarm database needs deep domain constraints . because the correlated alarms greatly depend on the logical and physical architecture of networks . thus we use the network model as the constraints of algorithms , including scope constraint , inter-correlated constraint and intra-correlated constraint , in our proposed algorithm called smc ( search with model-based constraints ) . the experiments show that the smc algorithm with inter-correlated or intra-correlated constraint is about two times faster than the algorithm with no constraints .

the extended parameter filter
the parameters of temporal models , such as dynamic bayesian networks , may be modelled in a bayesian context as static or atemporal variables that influence transition probabilities at every time step . particle filters fail for models that include such variables , while methods that use gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence . storvik devised a method for incremental computation of exact sufficient statistics that , for some cases , reduces the per-sample cost to a constant . in this paper , we demonstrate a connection between storvik 's filter and a kalman filter in parameter space and establish more general conditions under which storvik 's filter works . drawing on an analogy to the extended kalman filter , we develop and analyze , both theoretically and experimentally , a taylor approximation to the parameter posterior that allows storvik 's method to be applied to a broader class of models . our experiments on both synthetic examples and real applications show improvement over existing methods .

applications of probabilistic programming ( master 's thesis , 2015 )
this thesis describes work on two applications of probabilistic programming : the learning of probabilistic program code given specifications , in particular program code of one-dimensional samplers ; and the facilitation of sequential monte carlo inference with help of data-driven proposals . the latter is presented with experimental results on a linear gaussian model and a non-parametric dependent dirichlet process mixture of objects model for object recognition and tracking . in chapter 1 we provide a brief introduction to probabilistic programming . in chapter 3 we present an approach to automatic discovery of samplers in the form of probabilistic programs . we formulate a bayesian approach to this problem by specifying a grammar-based prior over probabilistic program code . we use an approximate bayesian computation method to learn the programs , whose executions generate samples that statistically match observed data or analytical characteristics of distributions of interest . in our experiments we leverage different probabilistic programming systems to perform markov chain monte carlo sampling over the space of programs . experimental results have demonstrated that , using the proposed methodology , we can learn approximate and even some exact samplers . finally , we show that our results are competitive with regard to genetic programming methods . in chapter 3 , we describe a way to facilitate sequential monte carlo inference in probabilistic programming using data-driven proposals . in particular , we develop a distance-based proposal for the non-parametric dependent dirichlet process mixture of objects model . we implement this approach in the probabilistic programming system anglican , and show that for that model data-driven proposals provide significant performance improvements . we also explore the possibility of using neural networks to improve data-driven proposals .

case base mining for adaptation knowledge acquisition
in case-based reasoning , the adaptation of a source case in order to solve the target problem is at the same time crucial and difficult to implement . the reason for this difficulty is that , in general , adaptation strongly depends on domain-dependent knowledge . this fact motivates research on adaptation knowledge acquisition ( aka ) . this paper presents an approach to aka based on the principles and techniques of knowledge discovery from databases and data-mining . it is implemented in cabamaka , a system that explores the variations within the case base to elicit adaptation knowledge . this system has been successfully tested in an application of case-based reasoning to decision support in the domain of breast cancer treatment .

a case study of algorithm selection for the traveling thief problem
many real-world problems are composed of several interacting components . in order to facilitate research on such interactions , the traveling thief problem ( ttp ) was created in 2013 as the combination of two well-understood combinatorial optimization problems . with this article , we contribute in four ways . first , we create a comprehensive dataset that comprises the performance data of 21 ttp algorithms on the full original set of 9720 ttp instances . second , we define 55 characteristics for all tpp instances that can be used to select the best algorithm on a per-instance basis . third , we use these algorithms and features to construct the first algorithm portfolios for ttp , clearly outperforming the single best algorithm . finally , we study which algorithms contribute most to this portfolio .

reactive policies with planning for action languages
we describe a representation in a high-level transition system for policies that express a reactive behavior for the agent . we consider a target decision component that figures out what to do next and an ( online ) planning capability to compute the plans needed to reach these targets . our representation allows one to analyze the flow of executing the given reactive policy , and to determine whether it works as expected . additionally , the flexibility of the representation opens a range of possibilities for designing behaviors .

an empirical analysis of proximal policy optimization with kronecker-factored natural gradients
in this technical report , we consider an approach that combines the ppo objective and k-fac natural gradient optimization , for which we call ppokfac . we perform a range of empirical analysis on various aspects of the algorithm , such as sample complexity , training speed , and sensitivity to batch size and training epochs . we observe that ppokfac is able to outperform ppo in terms of sample complexity and speed in a range of mujoco environments , while being scalable in terms of batch size . in spite of this , it seems that adding more epochs is not necessarily helpful for sample efficiency , and ppokfac seems to be worse than its a2c counterpart , acktr .

applying dynamic model for multiple manoeuvring target tracking using particle filtering
in this paper , we applied a dynamic model for manoeuvring targets in sir particle filter algorithm for improving tracking accuracy of multiple manoeuvring targets . in our proposed approach , a color distribution model is used to detect changes of target 's model . our proposed approach controls deformation of target 's model . if deformation of target 's model is larger than a predetermined threshold , then the model will be updated . global nearest neighbor ( gnn ) algorithm is used as data association algorithm . we named our proposed method as deformation detection particle filter ( ddpf ) . ddpf approach is compared with basic sir-pf algorithm on real airshow videos . comparisons results show that , the basic sir-pf algorithm is not able to track the manoeuvring targets when the rotation or scaling is occurred in target ' s model . however , ddpf approach updates target 's model when the rotation or scaling is occurred . thus , the proposed approach is able to track the manoeuvring targets more efficiently and accurately .

the singularity may be near
toby walsh in 'the singularity may never be near ' gives six arguments to support his point of view that technological singularity may happen but that it is unlikely . in this paper , we provide analysis of each one of his arguments and arrive at similar conclusions , but with more weight given to the 'likely to happen ' probability .

speeding up sor solvers for constraint-based guis with a warm-start strategy
many computer programs have graphical user interfaces ( guis ) , which need good layout to make efficient use of the available screen real estate . most guis do not have a fixed layout , but are resizable and able to adapt themselves . constraints are a powerful tool for specifying adaptable gui layouts : they are used to specify a layout in a general form , and a constraint solver is used to find a satisfying concrete layout , e.g.\ for a specific gui size . the constraint solver has to calculate a new layout every time a gui is resized or changed , so it needs to be efficient to ensure a good user experience . one approach for constraint solvers is based on the gauss-seidel algorithm and successive over-relaxation ( sor ) . our observation is that a solution after resizing or changing is similar in structure to a previous solution . thus , our hypothesis is that we can increase the computational performance of an sor-based constraint solver if we reuse the solution of a previous layout to warm-start the solving of a new layout . in this paper we report on experiments to test this hypothesis experimentally for three common use cases : big-step resizing , small-step resizing and constraint change . in our experiments , we measured the solving time for randomly generated gui layout specifications of various sizes . for all three cases we found that the performance is improved if an existing solution is used as a starting solution for a new layout .

learning robust options
robust reinforcement learning aims to produce policies that have strong guarantees even in the face of environments/transition models whose parameters have strong uncertainty . existing work uses value-based methods and the usual primitive action setting . in this paper , we propose robust methods for learning temporally abstract actions , in the framework of options . we present a robust options policy iteration ( ropi ) algorithm with convergence guarantees , which learns options that are robust to model uncertainty . we utilize ropi to learn robust options with the robust options deep q network ( ro-dqn ) that solves multiple tasks and mitigates model misspecification due to model uncertainty . we present experimental results which suggest that policy iteration with linear features may have an inherent form of robustness when using coarse feature representations . in addition , we present experimental results which demonstrate that robustness helps policy iteration implemented on top of deep neural networks to generalize over a much broader range of dynamics than non-robust policy iteration .

exploiting the pruning power of strong local consistencies through parallelization
local consistencies stronger than arc consistency have received a lot of attention since the early days of csp research . % because of the strong pruning they can achieve . however , they have not been widely adopted by csp solvers . this is because applying such consistencies can sometimes result in considerably smaller search tree sizes and therefore in important speed-ups , but in other cases the search space reduction may be small , causing severe run time penalties . taking advantage of recent advances in parallelization , we propose a novel approach for the application of strong local consistencies ( slcs ) that can improve their performance by largely preserving the speed-ups they offer in cases where they are successful , and eliminating the run time penalties in cases where they are unsuccessful . this approach is presented in the form of two search algorithms . both algorithms consist of a master search process , which is a typical csp solver , and a number of slave processes , with each one implementing a slc method . the first algorithm runs the different slcs synchronously at each node of the search tree explored in the master process , while the second one can run them asynchronously at different nodes of the search tree . experimental results demonstrate the benefits of the proposed method .

disjunctive answer set solvers via templates
answer set programming is a declarative programming paradigm oriented towards difficult combinatorial search problems . a fundamental task in answer set programming is to compute stable models , i.e. , solutions of logic programs . answer set solvers are the programs that perform this task . the problem of deciding whether a disjunctive program has a stable model is $ \sigma^p_2 $ -complete . the high complexity of reasoning within disjunctive logic programming is responsible for few solvers capable of dealing with such programs , namely dlv , gnt , cmodels , clasp and wasp . in this paper we show that transition systems introduced by nieuwenhuis , oliveras , and tinelli to model and analyze satisfiability solvers can be adapted for disjunctive answer set solvers . transition systems give a unifying perspective and bring clarity in the description and comparison of solvers . they can be effectively used for analyzing , comparing and proving correctness of search algorithms as well as inspiring new ideas in the design of disjunctive answer set solvers . in this light , we introduce a general template , which accounts for major techniques implemented in disjunctive solvers . we then illustrate how this general template captures solvers dlv , gnt and cmodels . we also show how this framework provides a convenient tool for designing new solving algorithms by means of combinations of techniques employed in different solvers .

non-gaussian random generators in bacteria foraging algorithm for multiobjective optimization
random generators or stochastic engines are a key component in the structure of metaheuristic algorithms . this work investigates the effects of non-gaussian stochastic engines on the performance of metaheuristics when solving a real-world optimization problem . in this work , the bacteria foraging algorithm ( bfa ) was employed in tandem with four random generators ( stochastic engines ) . the stochastic engines operate using the weibull distribution , gamma distribution , gaussian distribution and a chaotic mechanism . the two non-gaussian distributions are the weibull and gamma distributions . in this work , the approaches developed were implemented on the real-world multi-objective resin bonded sand mould problem . the pareto frontiers obtained were benchmarked using two metrics ; the hyper volume indicator ( hvi ) and the proposed average explorative rate ( aer ) metric . detail discussions from various perspectives on the effects of non-gaussian random generators in metaheuristics are provided .

interactive learning of state representation through natural language instruction and explanation
one significant simplification in most previous work on robot learning is the closed-world assumption where the robot is assumed to know ahead of time a complete set of predicates describing the state of the physical world . however , robots are not likely to have a complete model of the world especially when learning a new task . to address this problem , this extended abstract gives a brief introduction to our on-going work that aims to enable the robot to acquire new state representations through language communication with humans .

scalable relaxations of sparse packing constraints : optimal biocontrol in predator-prey network
cascades represent rapid changes in networks . a cascading phenomenon of ecological and economic impact is the spread of invasive species in geographic landscapes . the most promising management strategy is often biocontrol , which entails introducing a natural predator able to control the invading population , a setting that can be treated as two interacting cascades of predator and prey populations . we formulate and study a nonlinear problem of optimal biocontrol : optimally seeding the predator cascade over time to minimize the harmful prey population . recurring budgets , which typically face conservation organizations , naturally leads to sparse constraints which make the problem amenable to approximation algorithms . available methods based on continuous relaxations scale poorly , to remedy this we develop a novel and scalable randomized algorithm based on a width relaxation , applicable to a broad class of combinatorial optimization problems . we evaluate our contributions in the context of biocontrol for the insect pest hemlock wolly adelgid ( hwa ) in eastern north america . our algorithm outperforms competing methods in terms of scalability and solution quality , and finds near optimal strategies for the control of the hwa for fine-grained networks -- an important problem in computational sustainability .

a hypercat-enabled semantic internet of things data hub : technical report
an increasing amount of information is generated from the rapidly increasing number of sensor networks and smart devices . a wide variety of sources generate and publish information in different formats , thus highlighting interoperability as one of the key prerequisites for the success of internet of things ( iot ) . the bt hypercat data hub provides a focal point for the sharing and consumption of available datasets from a wide range of sources . in this work , we propose a semantic enrichment of the bt hypercat data hub , using well-accepted semantic web standards and tools . we propose an ontology that captures the semantics of the imported data and present the bt sparql endpoint by means of a mapping between sparql and sql queries . furthermore , federated sparql queries allow queries over multiple hub-based and external data sources . finally , we provide two use cases in order to illustrate the advantages afforded by our semantic approach .

extension of boolean algebra by a bayesian operator ; application to the definition of a deterministic bayesian logic
this work contributes to the domains of boolean algebra and of bayesian probability , by proposing an algebraic extension of boolean algebras , which implements an operator for the bayesian conditional inference and is closed under this operator . it is known since the work of lewis ( lewis ' triviality ) that it is not possible to construct such conditional operator within the space of events . nevertheless , this work proposes an answer which complements lewis ' triviality , by the construction of a conditional operator outside the space of events , thus resulting in an algebraic extension . in particular , it is proved that any probability defined on a boolean algebra may be extended to its algebraic extension in compliance with the multiplicative definition of the conditional probability . in the last part of this paper , a new bivalent logic is introduced on the basis of this algebraic extension , and basic properties are derived .

arguments using ontological and causal knowledge
we investigate an approach to reasoning about causes through argumentation . we consider a causal model for a physical system , and look for arguments about facts . some arguments are meant to provide explanations of facts whereas some challenge these explanations and so on . at the root of argumentation here , are causal links ( { a_1 , ... , a_n } causes b ) and ontological links ( o_1 is_a o_2 ) . we present a system that provides a candidate explanation ( { a_1 , ... , a_n } explains { b_1 , ... , b_m } ) by resorting to an underlying causal link substantiated with appropriate ontological links . argumentation is then at work from these various explaining links . a case study is developed : a severe storm xynthia that devastated part of france in 2010 , with an unaccountably high number of casualties .

toward a combination rule to deal with partial conflict and specificity in belief functions theory
we present and discuss a mixed conjunctive and disjunctive rule , a generalization of conflict repartition rules , and a combination of these two rules . in the belief functions theory one of the major problem is the conflict repartition enlightened by the famous zadeh 's example . to date , many combination rules have been proposed in order to solve a solution to this problem . moreover , it can be important to consider the specificity of the responses of the experts . since few year some unification rules are proposed . we have shown in our previous works the interest of the proportional conflict redistribution rule . we propose here a mixed combination rule following the proportional conflict redistribution rule modified by a discounting procedure . this rule generalizes many combination rules .

z specification for the w3c editor 's draft core shacl semantics
this article provides a formalization of the w3c draft core shacl semantics specification using z notation . this formalization exercise has identified a number of quality issues in the draft . it has also established that the recursive definitions in the draft are well-founded . further formal validation of the draft will require the use of an executable specification technology .

decision support with belief functions theory for seabed characterization
the seabed characterization from sonar images is a very hard task because of the produced data and the unknown environment , even for an human expert . in this work we propose an original approach in order to combine binary classifiers arising from different kinds of strategies such as one-versus-one or one-versus-rest , usually used in the svm-classification . the decision functions coming from these binary classifiers are interpreted in terms of belief functions in order to combine these functions with one of the numerous operators of the belief functions theory . moreover , this interpretation of the decision function allows us to propose a process of decisions by taking into account the rejected observations too far removed from the learning data , and the imprecise decisions given in unions of classes . this new approach is illustrated and evaluated with a svm in order to classify the different kinds of sediment on image sonar .

augmenting alc ( d ) ( atemporal ) roles and ( aspatial ) concrete domain with temporal roles and a spatial concrete domain -first results
we consider the well-known family alc ( d ) of description logics with a concrete domain , and provide first results on a framework obtained by augmenting alc ( d ) atemporal roles and aspatial concrete domain with temporal roles and a spatial concrete domain .

incremental recompilation of knowledge
approximating a general formula from above and below by horn formulas ( its horn envelope and horn core , respectively ) was proposed by selman and kautz ( 1991 , 1996 ) as a form of `` knowledge compilation , '' supporting rapid approximate reasoning ; on the negative side , this scheme is static in that it supports no updates , and has certain complexity drawbacks pointed out by kavvadias , papadimitriou and sideri ( 1993 ) . on the other hand , the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments , even in the horn case , as was pointed out by eiter and gottlob ( 1992 ) , and is further demonstrated in the present paper . more fundamentally , these schemes are not inductive , in that they may lose in a single update any positive properties of the represented sets of formulas ( small size , horn structure , etc. ) . in this paper we propose a new scheme , incremental recompilation , which combines horn approximation and model-based updates ; this scheme is inductive and very efficient , free of the problems facing its constituents . a set of formulas is represented by an upper and lower horn approximation . to update , we replace the upper horn formula by the horn envelope of its minimum-change update , and similarly the lower one by the horn core of its update ; the key fact which enables this scheme is that horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a horn formula by a clause . we conjecture that efficient algorithms are possible for more complex updates .

type-elimination-based reasoning for the description logic shiqbs using decision diagrams and disjunctive datalog
we propose a novel , type-elimination-based method for reasoning in the description logic shiqbs including dl-safe rules . to this end , we first establish a knowledge compilation method converting the terminological part of an alcib knowledge base into an ordered binary decision diagram ( obdd ) which represents a canonical model . this obdd can in turn be transformed into disjunctive datalog and merged with the assertional part of the knowledge base in order to perform combined reasoning . in order to leverage our technique for full shiqbs , we provide a stepwise reduction from shiqbs to alcib that preserves satisfiability and entailment of positive and negative ground facts . the proposed technique is shown to be worst case optimal w.r.t . combined and data complexity and easily admits extensions with ground conjunctive queries .

developing corpus-based translation methods between informal and formal mathematics : project description
the goal of this project is to ( i ) accumulate annotated informal/formal mathematical corpora suitable for training semi-automated translation between informal and formal mathematics by statistical machine-translation methods , ( ii ) to develop such methods oriented at the formalization task , and in particular ( iii ) to combine such methods with learning-assisted automated reasoning that will serve as a strong semantic component . we describe these ideas , the initial set of corpora , and some initial experiments done over them .

mining arguments from cancer documents using natural language processing and ontologies
in the medical domain , the continuous stream of scientific research contains contradictory results supported by arguments and counter-arguments . as medical expertise occurs at different levels , part of the human agents have difficulties to face the huge amount of studies , but also to understand the reasons and pieces of evidences claimed by the proponents and the opponents of the debated topic . to better understand the supporting arguments for new findings related to current state of the art in the medical domain we need tools able to identify arguments in scientific papers . our work here aims to fill the above technological gap . quite aware of the difficulty of this task , we embark to this road by relying on the well-known interleaving of domain knowledge with natural language processing . to formalise the existing medical knowledge , we rely on ontologies . to structure the argumentation model we use also the expressivity and reasoning capabilities of description logics . to perform argumentation mining we formalise various linguistic patterns in a rule-based language . we tested our solution against a corpus of scientific papers related to breast cancer . the run experiments show a f-measure between 0.71 and 0.86 for identifying conclusions of an argument and between 0.65 and 0.86 for identifying premises of an argument .

the method of artificial systems
this document is written with the intention to describe in detail a method and means by which a computer program can reason about the world and in so doing , increase its analogue to a living system . as the literature is rife and it is apparent we , as scientists and engineers , have not found the solution , this document will attempt the solution by grounding its intellectual arguments within tenets of human cognition in western philosophy . the result will be a characteristic description of a method to describe an artificial system analogous to that performed for a human . the approach was the substance of my master 's thesis , explored more deeply during the course of my postdoc research . it focuses primarily on context awareness and choice set within a boundary of available epistemology , which serves to describe it . expanded upon , such a description strives to discover agreement with kant 's critique of reason to understand how it could be applied to define the architecture of its design . the intention has never been to mimic human or biological systems , rather , to understand the profoundly fundamental rules , when leveraged correctly , results in an artificial consciousness as noumenon while in keeping with the perception of it as phenomenon .

d-flat : declarative problem solving using tree decompositions and answer-set programming
in this work , we propose answer-set programming ( asp ) as a tool for rapid prototyping of dynamic programming algorithms based on tree decompositions . in fact , many such algorithms have been designed , but only a few of them found their way into implementation . the main obstacle is the lack of easy-to-use systems which ( i ) take care of building a tree decomposition and ( ii ) provide an interface for declarative specifications of dynamic programming algorithms . in this paper , we present d-flat , a novel tool that relieves the user of having to handle all the technical details concerned with parsing , tree decomposition , the handling of data structures , etc . instead , it is only the dynamic programming algorithm itself which has to be specified in the asp language . d-flat employs an asp solver in order to compute the local solutions in the dynamic programming algorithm . in the paper , we give a few examples illustrating the use of d-flat and describe the main features of the system . moreover , we report experiments which show that asp-based d-flat encodings for some problems outperform monolithic asp encodings on instances of small treewidth .

on mining complex sequential data by means of fca and pattern structures
nowadays data sets are available in very complex and heterogeneous ways . mining of such data collections is essential to support many real-world applications ranging from healthcare to marketing . in this work , we focus on the analysis of `` complex '' sequential data by means of interesting sequential patterns . we approach the problem using the elegant mathematical framework of formal concept analysis ( fca ) and its extension based on `` pattern structures '' . pattern structures are used for mining complex data ( such as sequences or graphs ) and are based on a subsumption operation , which in our case is defined with respect to the partial order on sequences . we show how pattern structures along with projections ( i.e. , a data reduction of sequential structures ) , are able to enumerate more meaningful patterns and increase the computing efficiency of the approach . finally , we show the applicability of the presented method for discovering and analyzing interesting patient patterns from a french healthcare data set on cancer . the quantitative and qualitative results ( with annotations and analysis from a physician ) are reported in this use case which is the main motivation for this work . keywords : data mining ; formal concept analysis ; pattern structures ; projections ; sequences ; sequential data .

a way out of the odyssey : analyzing and combining recent insights for lstms
lstms have become a basic building block for many deep nlp models . in recent years , many improvements and variations have been proposed for deep sequence models in general , and lstms in particular . we propose and analyze a series of augmentations and modifications to lstm networks resulting in improved performance for text classification datasets . we observe compounding improvements on traditional lstms using monte carlo test-time model averaging , average pooling , and residual connections , along with four other suggested modifications . our analysis provides a simple , reliable , and high quality baseline model .

differences between industrial models of autonomy and systemic models of autonomy
this paper discusses the idea of levels of autonomy of systems - be this technical or organic - and compares the insights with models employed by industries used to describe maturity and capability of their products .

hp-gan : probabilistic 3d human motion prediction via gan
predicting and understanding human motion dynamics has many applications , such as motion synthesis , augmented reality , security , and autonomous vehicles . due to the recent success of generative adversarial networks ( gan ) , there has been much interest in probabilistic estimation and synthetic data generation using deep neural network architectures and learning algorithms . we propose a novel sequence-to-sequence model for probabilistic human motion prediction , trained with a modified version of improved wasserstein generative adversarial networks ( wgan-gp ) , in which we use a custom loss function designed for human motion prediction . our model , which we call hp-gan , learns a probability density function of future human poses conditioned on previous poses . it predicts multiple sequences of possible future human poses , each from the same input sequence but a different vector z drawn from a random distribution . furthermore , to quantify the quality of the non-deterministic predictions , we simultaneously train a motion-quality-assessment model that learns the probability that a given skeleton sequence is a real human motion . we test our algorithm on two of the largest skeleton datasets : nturgb-d and human3.6m . we train our model on both single and multiple action types . its predictive power for long-term motion estimation is demonstrated by generating multiple plausible futures of more than 30 frames from just 10 frames of input . we show that most sequences generated from the same input have more than 50\ % probabilities of being judged as a real human sequence . we will release all the code used in this paper to github .

an n-ary constraint for the stable marriage problem
we present an n-ary constraint for the stable marriage problem . this constraint acts between two sets of integer variables where the domains of those variables represent preferences . our constraint enforces stability and disallows bigamy . for a stable marriage instance with $ n $ men and $ n $ women we require only one of these constraints , and the complexity of enforcing arc-consistency is $ o ( n^2 ) $ which is optimal in the size of input . our computational studies show that our n-ary constraint is significantly faster and more space efficient than the encodings presented in \cite { cp01 } . we also introduce a new problem to the constraint community , the sex-equal stable marriage problem .

a global constraint for closed itemset mining
discovering the set of closed frequent patterns is one of the fundamental problems in data mining . recent constraint programming ( cp ) approaches for declarative itemset mining have proven their usefulness and flexibility . but the wide use of reified constraints in current cp approaches raises many difficulties to cope with high dimensional datasets . this paper proposes closed pattern global constraint which does not require any reified constraints nor any extra variables to encode efficiently the closed frequent pattern mining ( cfpm ) constraint . closed-pattern captures the particular semantics of the cfpm problem in order to ensure a polynomial pruning algorithm ensuring domain consistency . the computational properties of our constraint are analyzed and their practical effectiveness is experimentally evaluated .

neutrality and many-valued logics
in this book , we consider various many-valued logics : standard , linear , hyperbolic , parabolic , non-archimedean , p-adic , interval , neutrosophic , etc . we survey also results which show the tree different proof-theoretic frameworks for many-valued logics , e.g . frameworks of the following deductive calculi : hilbert 's style , sequent , and hypersequent . we present a general way that allows to construct systematically analytic calculi for a large family of non-archimedean many-valued logics : hyperrational-valued , hyperreal-valued , and p-adic valued logics characterized by a special format of semantics with an appropriate rejection of archimedes ' axiom . these logics are built as different extensions of standard many-valued logics ( namely , lukasiewicz 's , goedel 's , product , and post 's logics ) . the informal sense of archimedes ' axiom is that anything can be measured by a ruler . also logical multiple-validity without archimedes ' axiom consists in that the set of truth values is infinite and it is not well-founded and well-ordered . on the base of non-archimedean valued logics , we construct non-archimedean valued interval neutrosophic logic inl by which we can describe neutrality phenomena .

belief propagation for linear programming
belief propagation ( bp ) is a popular , distributed heuristic for performing map computations in graphical models . bp can be interpreted , from a variational perspective , as minimizing the bethe free energy ( bfe ) . bp can also be used to solve a special class of linear programming ( lp ) problems . for this class of problems , map inference can be stated as an integer lp with an lp relaxation that coincides with minimization of the bfe at `` zero temperature '' . we generalize these prior results and establish a tight characterization of the lp problems that can be formulated as an equivalent lp relaxation of map inference . moreover , we suggest an efficient , iterative annealing bp algorithm for solving this broader class of lp problems . we demonstrate the algorithm 's performance on a set of weighted matching problems by using it as a cutting plane method to solve a sequence of lps tightened by adding `` blossom '' inequalities .

untangling adaboost-based cost-sensitive classification . part i : theoretical perspective
boosting algorithms have been widely used to tackle a plethora of problems . in the last few years , a lot of approaches have been proposed to provide standard adaboost with cost-sensitive capabilities , each with a different focus . however , for the researcher , these algorithms shape a tangled set with diffuse differences and properties , lacking a unifying analysis to jointly compare , classify , evaluate and discuss those approaches on a common basis . in this series of two papers we aim to revisit the various proposals , both from theoretical ( part i ) and practical ( part ii ) perspectives , in order to analyze their specific properties and behavior , with the final goal of identifying the algorithm providing the best and soundest results .

mean actor critic
we propose a new algorithm , mean actor-critic ( mac ) , for discrete-action continuous-state reinforcement learning . mac is a policy gradient algorithm that uses the agent 's explicit representation of all action values to estimate the gradient of the policy , rather than using only the actions that were actually executed . this significantly reduces variance in the gradient updates and removes the need for a variance reduction baseline . we show empirical results on two control domains where mac performs as well as or better than other policy gradient approaches , and on five atari games , where mac is competitive with state-of-the-art policy search algorithms .

on identifying total effects in the presence of latent variables and selection bias
assume that cause-effect relationships between variables can be described as a directed acyclic graph and the corresponding linear structural equation model.we consider the identification problem of total effects in the presence of latent variables and selection bias between a treatment variable and a response variable . pearl and his colleagues provided the back door criterion , the front door criterion ( pearl , 2000 ) and the conditional instrumental variable method ( brito and pearl , 2002 ) as identifiability criteria for total effects in the presence of latent variables , but not in the presence of selection bias . in order to solve this problem , we propose new graphical identifiability criteria for total effects based on the identifiable factor models . the results of this paper are useful to identify total effects in observational studies and provide a new viewpoint to the identification conditions of factor models .

a stochastic process model of classical search
among classical search algorithms with the same heuristic information , with sufficient memory a* is essentially as fast as possible in finding a proven optimal solution . however , in many situations optimal solutions are simply infeasible , and thus search algorithms that trade solution quality for speed are desirable . in this paper , we formalize the process of classical search as a metalevel decision problem , the abstract search mdp . for any given optimization criterion , this establishes a well-defined notion of the best possible behaviour for a search algorithm and offers a theoretical approach to the design of algorithms for that criterion . we proceed to approximately solve a version of the abstract search mdp for anytime algorithms and thus derive a novel search algorithm , search by maximizing the incremental rate of improvement ( smiri ) . smiri is shown to outperform current state-of-the-art anytime search algorithms on a parametrized stochastic tree model for most of the tested parameter values .

toward an automaton constraint for local search
we explore the idea of using finite automata to implement new constraints for local search ( this is already a successful technique in constraint-based global search ) . we show how it is possible to maintain incrementally the violations of a constraint and its decision variables from an automaton that describes a ground checker for that constraint . we establish the practicality of our approach idea on real-life personnel rostering problems , and show that it is competitive with the approach of [ pralong , 2007 ] .

promoting scientific thinking with robots
this article describes an exemplary robot exercise which was conducted in a class for mechatronics students . the goal of this exercise was to engage students in scientific thinking and reasoning , activities which do not always play an important role in their curriculum . the robotic platform presented here is simple in its construction and is customizable to the needs of the teacher . therefore , it can be used for exercises in many different fields of science , not necessarily related to robotics . here we present a situation where the robot is used like an alien creature from which we want to understand its behavior , resembling an ethological research activity . this robot exercise is suited for a wide range of courses , from general introduction to science , to hardware oriented lectures .

a framework for searching and/or graphs with cycles
search in cyclic and/or graphs was traditionally known to be an unsolved problem . in the recent past several important studies have been reported in this domain . in this paper , we have taken a fresh look at the problem . first , a new and comprehensive theoretical framework for cyclic and/or graphs has been presented , which was found missing in the recent literature . based on this framework , two best-first search algorithms , s1 and s2 , have been developed . s1 does uninformed search and is a simple modification of the bottom-up algorithm by martelli and montanari . s2 performs a heuristically guided search and replicates the modification in bottom-up 's successors , namely hs and ao* . both s1 and s2 solve the problem of searching and/or graphs in presence of cycles . we then present a detailed analysis for the correctness and complexity results of s1 and s2 , using the proposed framework . we have observed through experiments that s1 and s2 output correct results in all cases .

on modeling vagueness and uncertainty in data-to-text systems through fuzzy sets
vagueness and uncertainty management is counted among one of the challenges that remain unresolved in systems that generate texts from non-linguistic data , known as data-to-text systems . in the last decade , work in fuzzy linguistic summarization and description of data has raised the interest of using fuzzy sets to model and manage the imprecision of human language in data-to-text systems . however , despite some research in this direction , there has not been an actual clear discussion and justification on how fuzzy sets can contribute to data-to-text for modeling vagueness and uncertainty in words and expressions . this paper intends to bridge this gap by answering the following questions : what does vagueness mean in fuzzy sets theory ? what does vagueness mean in data-to-text contexts ? in what ways can fuzzy sets theory contribute to improve data-to-text systems ? what are the challenges that researchers from both disciplines need to address for a successful integration of fuzzy sets into data-to-text systems ? in what cases should the use of fuzzy sets be avoided in d2t ? for this , we review and discuss the state of the art of vagueness modeling in natural language generation and data-to-text , describe potential and actual usages of fuzzy sets in data-to-text contexts , and provide some additional insights about the engineering of data-to-text systems that make use of fuzzy set-based techniques .

performance bounds for lambda policy iteration and application to the game of tetris
we consider the discrete-time infinite-horizon optimal control problem formalized by markov decision processes . we revisit the work of bertsekas and ioffe , that introduced $ \lambda $ policy iteration , a family of algorithms parameterized by $ \lambda $ that generalizes the standard algorithms value iteration and policy iteration , and has some deep connections with the temporal differences algorithm td ( $ \lambda $ ) described by sutton and barto . we deepen the original theory developped by the authors by providing convergence rate bounds which generalize standard bounds for value iteration described for instance by puterman . then , the main contribution of this paper is to develop the theory of this algorithm when it is used in an approximate form and show that this is sound . doing so , we extend and unify the separate analyses developped by munos for approximate value iteration and approximate policy iteration . eventually , we revisit the use of this algorithm in the training of a tetris playing controller as originally done by bertsekas and ioffe . we provide an original performance bound that can be applied to such an undiscounted control problem . our empirical results are different from those of bertsekas and ioffe ( which were originally qualified as `` paradoxical '' and `` intriguing '' ) , and much more conform to what one would expect from a learning experiment . we discuss the possible reason for such a difference .

integrating probabilistic rules into neural networks : a stochastic em learning algorithm
the em-algorithm is a general procedure to get maximum likelihood estimates if part of the observations on the variables of a network are missing . in this paper a stochastic version of the algorithm is adapted to probabilistic neural networks describing the associative dependency of variables . these networks have a probability distribution , which is a special case of the distribution generated by probabilistic inference networks . hence both types of networks can be combined allowing to integrate probabilistic rules as well as unspecified associations in a sound way . the resulting network may have a number of interesting features including cycles of probabilistic rules , hidden 'unobservable ' variables , and uncertain and contradictory evidence .

an empirical evaluation of portfolios approaches for solving csps
recent research in areas such as sat solving and integer linear programming has shown that the performances of a single arbitrarily efficient solver can be significantly outperformed by a portfolio of possibly slower on-average solvers . we report an empirical evaluation and comparison of portfolio approaches applied to constraint satisfaction problems ( csps ) . we compared models developed on top of off-the-shelf machine learning algorithms with respect to approaches used in the sat field and adapted for csps , considering different portfolio sizes and using as evaluation metrics the number of solved problems and the time taken to solve them . results indicate that the best sat approaches have top performances also in the csp field and are slightly more competitive than simple models built on top of classification algorithms .

learning implicitly in reasoning in pac-semantics
we consider the problem of answering queries about formulas of propositional logic based on background knowledge partially represented explicitly as other formulas , and partially represented as partially obscured examples independently drawn from a fixed probability distribution , where the queries are answered with respect to a weaker semantics than usual -- pac-semantics , introduced by valiant ( 2000 ) -- that is defined using the distribution of examples . we describe a fairly general , efficient reduction to limited versions of the decision problem for a proof system ( e.g. , bounded space treelike resolution , bounded degree polynomial calculus , etc . ) from corresponding versions of the reasoning problem where some of the background knowledge is not explicitly given as formulas , only learnable from the examples . crucially , we do not generate an explicit representation of the knowledge extracted from the examples , and so the `` learning '' of the background knowledge is only done implicitly . as a consequence , this approach can utilize formulas as background knowledge that are not perfectly valid over the distribution -- -essentially the analogue of agnostic learning here .

antifragility for intelligent autonomous systems
antifragile systems grow measurably better in the presence of hazards . this is in contrast to fragile systems which break down in the presence of hazards , robust systems that tolerate hazards up to a certain degree , and resilient systems that -- like self-healing systems -- revert to their earlier expected behavior after a period of convalescence . the notion of antifragility was introduced by taleb for economics systems , but its applicability has been illustrated in biological and engineering domains as well . in this paper , we propose an architecture that imparts antifragility to intelligent autonomous systems , specifically those that are goal-driven and based on ai-planning . we argue that this architecture allows the system to self-improve by uncovering new capabilities obtained either through the hazards themselves ( opportunistic ) or through deliberation ( strategic ) . an ai planning-based case study of an autonomous wheeled robot is presented . we show that with the proposed architecture , the robot develops antifragile behaviour with respect to an oil spill hazard .

a sensitivity analysis of pathfinder : a follow-up study
at last year ? s uncertainty in ai conference , we reported the results of a sensitivity analysis study of pathfinder . our findings were quite unexpected-slight variations to pathfinder ? s parameters appeared to lead to substantial degradations in system performance . a careful look at our first analysis , together with the valuable feedback provided by the participants of last year ? s conference , led us to conduct a follow-up study . our follow-up differs from our initial study in two ways : ( i ) the probabilities 0.0 and 1.0 remained unchanged , and ( ii ) the variations to the probabilities that are close to both ends ( 0.0 or 1.0 ) were less than the ones close to the middle ( 0.5 ) . the results of the follow-up study look more reasonable-slight variations to pathfinder ? s parameters now have little effect on its performance . taken together , these two sets of results suggest a viable extension of a common decision analytic sensitivity analysis to the larger , more complex settings generally encountered in artificial intelligence .

spatio-temporal data mining : a survey of problems and methods
large volumes of spatio-temporal data are increasingly collected and studied in diverse domains including , climate science , social sciences , neuroscience , epidemiology , transportation , mobile health , and earth sciences . spatio-temporal data differs from relational data for which computational approaches are developed in the data mining community for multiple decades , in that both spatial and temporal attributes are available in addition to the actual measurements/attributes . the presence of these attributes introduces additional challenges that needs to be dealt with . approaches for mining spatio-temporal data have been studied for over a decade in the data mining community . in this article we present a broad survey of this relatively young field of spatio-temporal data mining . we discuss different types of spatio-temporal data and the relevant data mining questions that arise in the context of analyzing each of these datasets . based on the nature of the data mining problem studied , we classify literature on spatio-temporal data mining into six major categories : clustering , predictive learning , change detection , frequent pattern mining , anomaly detection , and relationship mining . we discuss the various forms of spatio-temporal data mining problems in each of these categories .

an interesting uncertainty-based combinatoric problem in spare parts forecasting : the fred system
the domain of spare parts forecasting is examined , and is found to present unique uncertainty based problems in the architectural design of a knowledge-based system . a mixture of different uncertainty paradigms is required for the solution , with an intriguing combinatoric problem arising from an uncertain choice of inference engines . thus , uncertainty in the system is manifested in two different meta-levels . the different uncertainty paradigms and meta-levels must be integrated into a functioning whole . fred is an example of a difficult real-world domain to which no existing uncertainty approach is completely appropriate . this paper discusses the architecture of fred , highlighting : the points of uncertainty and other interesting features of the domain , the specific implications of those features on the system design ( including the combinatoric explosions ) , their current implementation & future plans , and other problems and issues with the architecture .

a survey of parallel a*
a* is a best-first search algorithm for finding optimal-cost paths in graphs . a* benefits significantly from parallelism because in many applications , a* is limited by memory usage , so distributed memory implementations of a* that use all of the aggregate memory on the cluster enable problems that can not be solved by serial , single-machine implementations to be solved . we survey approaches to parallel a* , focusing on decentralized approaches to a* which partition the state space among processors . we also survey approaches to parallel , limited-memory variants of a* such as parallel ida* .

causal inference in the presence of latent variables and selection bias
we show that there is a general , informative and reliable procedure for discovering causal relations when , for all the investigator knows , both latent variables and selection bias may be at work . given information about conditional independence and dependence relations between measured variables , even when latent variables and selection bias may be present , there are sufficient conditions for reliably concluding that there is a causal path from one variable to another , and sufficient conditions for reliably concluding when no such causal path exists .

memetic search for identifying critical nodes in sparse graphs
critical node problems involve identifying a subset of critical nodes from an undirected graph whose removal results in optimizing a pre-defined measure over the residual graph . as useful models for a variety of practical applications , these problems are computational challenging . in this paper , we study the classic critical node problem ( cnp ) and introduce an effective memetic algorithm for solving cnp . the proposed algorithm combines a double backbone-based crossover operator ( to generate promising offspring solutions ) , a component-based neighborhood search procedure ( to find high-quality local optima ) and a rank-based pool updating strategy ( to guarantee a healthy population ) . specially , the component-based neighborhood search integrates two key techniques , i.e. , two-phase node exchange strategy and node weighting scheme . the double backbone-based crossover extends the idea of general backbone-based crossovers . extensive evaluations on 42 synthetic and real-world benchmark instances show that the proposed algorithm discovers 21 new upper bounds and matches 18 previous best-known upper bounds . we also demonstrate the relevance of our algorithm for effectively solving a variant of the classic cnp , called the cardinality-constrained critical node problem . finally , we investigate the usefulness of each key algorithmic component .

robust inference of trees
this paper is concerned with the reliable inference of optimal tree-approximations to the dependency structure of an unknown distribution generating data . the traditional approach to the problem measures the dependency strength between random variables by the index called mutual information . in this paper reliability is achieved by walley 's imprecise dirichlet model , which generalizes bayesian learning with dirichlet priors . adopting the imprecise dirichlet model results in posterior interval expectation for mutual information , and in a set of plausible trees consistent with the data . reliable inference about the actual tree is achieved by focusing on the substructure common to all the plausible trees . we develop an exact algorithm that infers the substructure in time o ( m^4 ) , m being the number of random variables . the new algorithm is applied to a set of data sampled from a known distribution . the method is shown to reliably infer edges of the actual tree even when the data are very scarce , unlike the traditional approach . finally , we provide lower and upper credibility limits for mutual information under the imprecise dirichlet model . these enable the previous developments to be extended to a full inferential method for trees .

recurrent generative adversarial networks for proximal learning and automated compressive image recovery
recovering images from undersampled linear measurements typically leads to an ill-posed linear inverse problem , that asks for proper statistical priors . building effective priors is however challenged by the low train and test overhead dictated by real-time tasks ; and the need for retrieving visually `` plausible '' and physically `` feasible '' images with minimal hallucination . to cope with these challenges , we design a cascaded network architecture that unrolls the proximal gradient iterations by permeating benefits from generative residual networks ( resnet ) to modeling the proximal operator . a mixture of pixel-wise and perceptual costs is then deployed to train proximals . the overall architecture resembles back-and-forth projection onto the intersection of feasible and plausible images . extensive computational experiments are examined for a global task of reconstructing mr images of pediatric patients , and a more local task of superresolving celeba faces , that are insightful to design efficient architectures . our observations indicate that for mri reconstruction , a recurrent resnet with a single residual block effectively learns the proximal . this simple architecture appears to significantly outperform the alternative deep resnet architecture by 2db snr , and the conventional compressed-sensing mri by 4db snr with 100x faster inference . for image superresolution , our preliminary results indicate that modeling the denoising proximal demands deep resnets .

unsupervised neural machine translation
in spite of the recent success of neural machine translation ( nmt ) in standard benchmarks , the lack of large parallel corpora poses a major practical problem for many language pairs . there have been several proposals to alleviate this issue with , for instance , triangulation and semi-supervised learning techniques , but they still require a strong cross-lingual signal . in this work , we completely remove the need of parallel data and propose a novel method to train an nmt system in a completely unsupervised manner , relying on nothing but monolingual corpora . our model builds upon the recent work on unsupervised embedding mappings , and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation . despite the simplicity of the approach , our system obtains 15.56 and 10.21 bleu points in wmt 2014 french-to-english and german-to-english translation . the model can also profit from small parallel corpora , and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences , respectively . our implementation is released as an open source project .

multi-agent only knowing
levesque introduced a notion of `` only knowing '' , with the goal of capturing certain types of nonmonotonic reasoning . levesque 's logic dealt with only the case of a single agent . recently , both halpern and lakemeyer independently attempted to extend levesque 's logic to the multi-agent case . although there are a number of similarities in their approaches , there are some significant differences . in this paper , we reexamine the notion of only knowing , going back to first principles . in the process , we simplify levesque 's completeness proof , and point out some problems with the earlier definitions . this leads us to reconsider what the properties of only knowing ought to be . we provide an axiom system that captures our desiderata , and show that it has a semantics that corresponds to it . the axiom system has an added feature of interest : it includes a modal operator for satisfiability , and thus provides a complete axiomatization for satisfiability in the logic k45 .

experimental analysis of data-driven control for a building heating system
driven by the opportunity to harvest the flexibility related to building climate control for demand response applications , this work presents a data-driven control approach building upon recent advancements in reinforcement learning . more specifically , model assisted batch reinforcement learning is applied to the setting of building climate control subjected to a dynamic pricing . the underlying sequential decision making problem is cast on a markov decision problem , after which the control algorithm is detailed . in this work , fitted q-iteration is used to construct a policy from a batch of experimental tuples . in those regions of the state space where the experimental sample density is low , virtual support samples are added using an artificial neural network . finally , the resulting policy is shaped using domain knowledge . the control approach has been evaluated quantitatively using a simulation and qualitatively in a living lab . from the quantitative analysis it has been found that the control approach converges in approximately 20 days to obtain a control policy with a performance within 90 % of the mathematical optimum . the experimental analysis confirms that within 10 to 20 days sensible policies are obtained that can be used for different outside temperature regimes .

perspective alignment in spatial language
it is well known that perspective alignment plays a major role in the planning and interpretation of spatial language . in order to understand the role of perspective alignment and the cognitive processes involved , we have made precise complete cognitive models of situated embodied agents that self-organise a communication system for dialoging about the position and movement of real world objects in their immediate surroundings . we show in a series of robotic experiments which cognitive mechanisms are necessary and sufficient to achieve successful spatial language and why and how perspective alignment can take place , either implicitly or based on explicit marking .

boolean equi-propagation for optimized sat encoding
we present an approach to propagation based solving , boolean equi-propagation , where constraints are modelled as propagators of information about equalities between boolean literals . propagation based solving applies this information as a form of partial evaluation resulting in optimized sat encodings . we demonstrate for a variety of benchmarks that our approach results in smaller cnf encodings and leads to speed-ups in solving times .

landau theory of adaptive integration in computational intelligence
computational intelligence ( ci ) is a sub-branch of artificial intelligence paradigm focusing on the study of adaptive mechanisms to enable or facilitate intelligent behavior in complex and changing environments . there are several paradigms of ci [ like artificial neural networks , evolutionary computations , swarm intelligence , artificial immune systems , fuzzy systems and many others ] , each of these has its origins in biological systems [ biological neural systems , natural darwinian evolution , social behavior , immune system , interactions of organisms with their environment ] . most of those paradigms evolved into separate machine learning ( ml ) techniques , where probabilistic methods are used complementary with ci techniques in order to effectively combine elements of learning , adaptation , evolution and fuzzy logic to create heuristic algorithms that are , in some sense , intelligent . the current trend is to develop consensus techniques , since no single machine learning algorithms is superior to others in all possible situations . in order to overcome this problem several meta-approaches were proposed in ml focusing on the integration of results from different methods into single prediction . we discuss here the landau theory for the nonlinear equation that can describe the adaptive integration of information acquired from an ensemble of independent learning agents . the influence of each individual agent on other learners is described similarly to the social impact theory . the final decision outcome for the consensus system is calculated using majority rule in the stationary limit , yet the minority solutions can survive inside the majority population as the complex intermittent clusters of opposite opinion .

quantum structure in cognition : fundamentals and applications
experiments in cognitive science and decision theory show that the ways in which people combine concepts and make decisions can not be described by classical logic and probability theory . this has serious implications for applied disciplines such as information retrieval , artificial intelligence and robotics . inspired by a mathematical formalism that generalizes quantum mechanics the authors have constructed a contextual framework for both concept representation and decision making , together with quantum models that are in strong alignment with experimental data . the results can be interpreted by assuming the existence in human thought of a double-layered structure , a 'classical logical thought ' and a 'quantum conceptual thought ' , the latter being responsible of the above paradoxes and nonclassical effects . the presence of a quantum structure in cognition is relevant , for it shows that quantum mechanics provides not only a useful modeling tool for experimental data but also supplies a structural model for human and artificial thought processes . this approach has strong connections with theories formalizing meaning , such as semantic analysis , and has also a deep impact on computer science , information retrieval and artificial intelligence . more specifically , the links with information retrieval are discussed in this paper .

componentwise least squares support vector machines
this chapter describes componentwise least squares support vector machines ( ls-svms ) for the estimation of additive models consisting of a sum of nonlinear components . the primal-dual derivations characterizing ls-svms for the estimation of the additive model result in a single set of linear equations with size growing in the number of data-points . the derivation is elaborated for the classification as well as the regression case . furthermore , different techniques are proposed to discover structure in the data by looking for sparse components in the model based on dedicated regularization schemes on the one hand and fusion of the componentwise ls-svms training with a validation criterion on the other hand . ( keywords : ls-svms , additive models , regularization , structure detection )

can you fool ai with adversarial examples on a visual turing test ?
deep learning has achieved impressive results in many areas of computer vision and natural language pro- cessing . among others , visual question answering ( vqa ) , also referred to a visual turing test , is considered one of the most compelling problems , and recent deep learning models have reported significant progress in vision and language modeling . although artificial intelligence ( ai ) is getting closer to passing the visual turing test , at the same time the existence of adversarial examples to deep learning systems may hinder the practical application of such systems . in this work , we conduct the first extensive study on adversarial examples for vqa systems . in particular , we focus on generating targeted adversarial examples for a vqa system while the target is considered to be a question-answer pair . our evaluation shows that the success rate of whether a targeted adversarial example can be generated is mostly dependent on the choice of the target question-answer pair , and less on the choice of images to which the question refers . we also report the language prior phenomenon of a vqa model , which can explain why targeted adversarial examples are hard to generate for some question-answer targets . we also demonstrate that a compositional vqa architecture is slightly more resilient to adversarial attacks than a non-compositional one . our study sheds new light on how to build deep vision and language resilient models robust against adversarial examples .

binary encodings of non-binary constraint satisfaction problems : algorithms and experimental results
a non-binary constraint satisfaction problem ( csp ) can be solved directly using extended versions of binary techniques . alternatively , the non-binary problem can be translated into an equivalent binary one . in this case , it is generally accepted that the translated problem can be solved by applying well-established techniques for binary csps . in this paper we evaluate the applicability of the latter approach . we demonstrate that the use of standard techniques for binary csps in the encodings of non-binary problems is problematic and results in models that are very rarely competitive with the non-binary representation . to overcome this , we propose specialized arc consistency and search algorithms for binary encodings , and we evaluate them theoretically and empirically . we consider three binary representations ; the hidden variable encoding , the dual encoding , and the double encoding . theoretical and empirical results show that , for certain classes of non-binary constraints , binary encodings are a competitive option , and in many cases , a better one than the non-binary representation .

understanding negations in information processing : learning from replicating human behavior
information systems experience an ever-growing volume of unstructured data , particularly in the form of textual materials . this represents a rich source of information from which one can create value for people , organizations and businesses . for instance , recommender systems can benefit from automatically understanding preferences based on user reviews or social media . however , it is difficult for computer programs to correctly infer meaning from narrative content . one major challenge is negations that invert the interpretation of words and sentences . as a remedy , this paper proposes a novel learning strategy to detect negations : we apply reinforcement learning to find a policy that replicates the human perception of negations based on an exogenous response , such as a user rating for reviews . our method yields several benefits , as it eliminates the former need for expensive and subjective manual labeling in an intermediate stage . moreover , the inferred policy can be used to derive statistical inferences and implications regarding how humans process and act on negations .

recurrent neural network based modeling of gene regulatory network using bat algorithm
correct inference of genetic regulations inside a cell is one of the greatest challenges in post genomic era for the biologist and researchers . several intelligent techniques and models were already proposed to identify the regulatory relations among genes from the biological database like time series microarray data . recurrent neural network ( rnn ) is one of the most popular and simple approach to model the dynamics as well as to infer correct dependencies among genes . in this paper , bat algorithm ( ba ) is applied to optimize the model parameters of rnn model of gene regulatory network ( grn ) . initially the proposed method is tested against small artificial network without any noise and the efficiency is observed in term of number of iteration , number of population and ba optimization parameters . the model is also validated in presence of different level of random noise for the small artificial network and that proved its ability to infer the correct inferences in presence of noise like real world dataset . in the next phase of this research , ba based rnn is applied to real world benchmark time series microarray dataset of e. coli . the results prove that it can able to identify the maximum number of true positive regulation but also include some false positive regulations . therefore , ba is very suitable for identifying biological plausible grn with the help rnn model .

learning and real-time classification of hand-written digits with spiking neural networks
we describe a novel spiking neural network ( snn ) for automated , real-time handwritten digit classification and its implementation on a gp-gpu platform . information processing within the network , from feature extraction to classification is implemented by mimicking the basic aspects of neuronal spike initiation and propagation in the brain . the feature extraction layer of the snn uses fixed synaptic weight maps to extract the key features of the image and the classifier layer uses the recently developed normad approximate gradient descent based supervised learning algorithm for spiking neural networks to adjust the synaptic weights . on the standard mnist database images of handwritten digits , our network achieves an accuracy of 99.80 % on the training set and 98.06 % on the test set , with nearly 7x fewer parameters compared to the state-of-the-art spiking networks . we further use this network in a gpu based user-interface system demonstrating real-time snn simulation to infer digits written by different users . on a test set of 500 such images , this real-time platform achieves an accuracy exceeding 97 % while making a prediction within an snn emulation time of less than 100ms .

image segmentation in video sequences : a probabilistic approach
`` background subtraction '' is an old technique for finding moving objects in a video sequence for example , cars driving on a freeway . the idea is that subtracting the current image from a timeaveraged background image will leave only nonstationary objects . it is , however , a crude approximation to the task of classifying each pixel of the current image ; it fails with slow-moving objects and does not distinguish shadows from moving objects . the basic idea of this paper is that we can classify each pixel using a model of how that pixel looks when it is part of different classes . we learn a mixture-of-gaussians classification model for each pixel using an unsupervised technique- an efficient , incremental version of em . unlike the standard image-averaging approach , this automatically updates the mixture component for each class according to likelihood of membership ; hence slow-moving objects are handled perfectly . our approach also identifies and eliminates shadows much more effectively than other techniques such as thresholding . application of this method as part of the roadwatch traffic surveillance project is expected to result in significant improvements in vehicle identification and tracking .

continuous value function approximation for sequential bidding policies
market-based mechanisms such as auctions are being studied as an appropriate means for resource allocation in distributed and mulitagent decision problems . when agents value resources in combination rather than in isolation , they must often deliberate about appropriate bidding strategies for a sequence of auctions offering resources of interest . we briefly describe a discrete dynamic programming model for constructing appropriate bidding policies for resources exhibiting both complementarities and substitutability . we then introduce a continuous approximation of this model , assuming that money ( or the numeraire good ) is infinitely divisible . though this has the potential to reduce the computational cost of computing policies , value functions in the transformed problem do not have a convenient closed form representation . we develop { em grid-based } approximation for such value functions , representing value functions using piecewise linear approximations . we show that these methods can offer significant computational savings with relatively small cost in solution quality .

random graph generator for bipartite networks modeling
the purpose of this article is to introduce a new iterative algorithm with properties resembling real life bipartite graphs . the algorithm enables us to generate wide range of random bigraphs , which features are determined by a set of parameters.we adapt the advances of last decade in unipartite complex networks modeling to the bigraph setting . this data structure can be observed in several situations . however , only a few datasets are freely available to test the algorithms ( e.g . community detection , influential nodes identification , information retrieval ) which operate on such data . therefore , artificial datasets are needed to enhance development and testing of the algorithms . we are particularly interested in applying the generator to the analysis of recommender systems . therefore , we focus on two characteristics that , besides simple statistics , are in our opinion responsible for the performance of neighborhood based collaborative filtering algorithms . the features are node degree distribution and local clustering coeficient .

an importance sampling algorithm based on evidence pre-propagation
precision achieved by stochastic sampling algorithms for bayesian networks typically deteriorates in face of extremely unlikely evidence . to address this problem , we propose the evidence pre-propagation importance sampling algorithm ( epis-bn ) , an importance sampling algorithm that computes an approximate importance function by the heuristic methods : loopy belief propagation and e-cutoff . we tested the performance of e-cutoff on three large real bayesian networks : andes , cpcs , and pathfinder . we observed that on each of these networks the epis-bn algorithm gives us a considerable improvement over the current state of the art algorithm , the ais-bn algorithm . in addition , it avoids the costly learning stage of the ais-bn algorithm .

tournament versus fitness uniform selection
in evolutionary algorithms a critical parameter that must be tuned is that of selection pressure . if it is set too low then the rate of convergence towards the optimum is likely to be slow . alternatively if the selection pressure is set too high the system is likely to become stuck in a local optimum due to a loss of diversity in the population . the recent fitness uniform selection scheme ( fuss ) is a conceptually simple but somewhat radical approach to addressing this problem - rather than biasing the selection towards higher fitness , fuss biases selection towards sparsely populated fitness levels . in this paper we compare the relative performance of fuss with the well known tournament selection scheme on a range of problems .

reasoning in the owl 2 full ontology language using first-order automated theorem proving
owl 2 has been standardized by the world wide web consortium ( w3c ) as a family of ontology languages for the semantic web . the most expressive of these languages is owl 2 full , but to date no reasoner has been implemented for this language . consistency and entailment checking are known to be undecidable for owl 2 full . we have translated a large fragment of the owl 2 full semantics into first-order logic , and used automated theorem proving systems to do reasoning based on this theory . the results are promising , and indicate that this approach can be applied in practice for effective owl reasoning , beyond the capabilities of current semantic web reasoners . this is an extended version of a paper with the same title that has been published at cade 2011 , lnai 6803 , pp . 446-460. the extended version provides appendices with additional resources that were used in the reported evaluation .

online learnability of statistical relational learning in anomaly detection
statistical relational learning ( srl ) methods for anomaly detection are introduced via a security-related application . operational requirements for online learning stability are outlined and compared to mathematical definitions as applied to the learning process of a representative srl method - bayesian logic programs ( blp ) . since a formal proof of online stability appears to be impossible , tentative common sense requirements are formulated and tested by theoretical and experimental analysis of a simple and analytically tractable blp model . it is found that learning algorithms in initial stages of online learning can lock on unstable false predictors that nevertheless comply with our tentative stability requirements and thus masquerade as bona fide solutions . the very expressiveness of srl seems to cause significant stability issues in settings with many variables and scarce data . we conclude that reliable anomaly detection with srl-methods requires monitoring by an overarching framework that may involve a comprehensive context knowledge base or human supervision .

a tutor agent for moba games
digital games have become a key player in the entertainment industry , attracting millions of new players each year . in spite of that , novice players may have a hard time when playing certain types of games , such as mobas and mmorpgs , due to their steep learning curves and not so friendly online communities . in this paper , we present an approach to help novice players in moba games overcome these problems . an artificial intelligence agent plays alongside the player analyzing his/her performance and giving tips about the game . experiments performed with the game { \em league of legends } show the potential of this approach .

on the definition of a confounder
the causal inference literature has provided a clear formal definition of confounding expressed in terms of counterfactual independence . the literature has not , however , come to any consensus on a formal definition of a confounder , as it has given priority to the concept of confounding over that of a confounder . we consider a number of candidate definitions arising from various more informal statements made in the literature . we consider the properties satisfied by each candidate definition , principally focusing on ( i ) whether under the candidate definition control for all `` confounders '' suffices to control for `` confounding '' and ( ii ) whether each confounder in some context helps eliminate or reduce confounding bias . several of the candidate definitions do not have these two properties . only one candidate definition of those considered satisfies both properties . we propose that a `` confounder '' be defined as a pre-exposure covariate c for which there exists a set of other covariates x such that effect of the exposure on the outcome is unconfounded conditional on ( x , c ) but such that for no proper subset of ( x , c ) is the effect of the exposure on the outcome unconfounded given the subset . we also provide a conditional analogue of the above definition ; and we propose a variable that helps reduce bias but not eliminate bias be referred to as a `` surrogate confounder . '' these definitions are closely related to those given by robins and morgenstern [ comput . math . appl . 14 ( 1987 ) 869-916 ] . the implications that hold among the various candidate definitions are discussed .

generalizing boolean satisfiability ii : theory
this is the second of three planned papers describing zap , a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high performance solvers . the fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the boolean representation used ; our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance . this paper presents the theoretical basis for the ideas underlying zap , arguing that existing ideas in this area exploit a single , recurring structure in that multiple database axioms can be obtained by operating on a single axiom using a subgroup of the group of permutations on the literals in the problem . we argue that the group structure precisely captures the general structure at which earlier approaches hinted , and give numerous examples of its use . we go on to extend the davis-putnam-logemann-loveland inference procedure to this broader setting , and show that earlier computational improvements are either subsumed or left intact by the new method . the third paper in this series discusses zaps implementation and presents experimental performance results .

a constraint satisfaction approach to the robust spanning tree problem with interval data
robust optimization is one of the fundamental approaches to deal with uncertainty in combinatorial optimization . this paper considers the robust spanning tree problem with interval data , which arises in a variety of telecommunication applications . it proposes a constraint satisfaction approach using a combinatorial lower bound , a pruning component that removes infeasible and suboptimal edges , as well as a search strategy exploring the most uncertain edges first . the resulting algorithm is shown to produce very dramatic improvements over the mathematical programming approach of yaman et al . and to enlarge considerably the class of problems amenable to effective solutions

a fuzzy syllogistic reasoning schema for generalized quantifiers
in this paper , a new approximate syllogistic reasoning schema is described that expands some of the approaches expounded in the literature into two ways : ( i ) a number of different types of quantifiers ( logical , absolute , proportional , comparative and exception ) taken from theory of generalized quantifiers and similarity quantifiers , taken from statistics , are considered and ( ii ) any number of premises can be taken into account within the reasoning process . furthermore , a systematic reasoning procedure to solve the syllogism is also proposed , interpreting it as an equivalent mathematical optimization problem , where the premises constitute the constraints of the searching space for the quantifier in the conclusion .

tree-to-tree neural networks for program translation
program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language . in this work , we are the first to consider employing deep neural networks toward tackling this problem . we observe that program translation is a modular procedure , in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step . to capture this intuition , we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one . meanwhile , we develop an attention mechanism for the tree-to-tree model , so that when the decoder expands one non-terminal in the target tree , the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder . we evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches . compared against other neural translation models , we observe that our approach is consistently better than the baselines with a margin of up to 15 points . further , our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects .

objective function designing led by user preferences acquisition
many real world problems can be defined as optimisation problems in which the aim is to maximise an objective function . the quality of obtained solution is directly linked to the pertinence of the used objective function . however , designing such function , which has to translate the user needs , is usually fastidious . in this paper , a method to help user objective functions designing is proposed . our approach , which is highly interactive , is based on man machine dialogue and more particularly on the comparison of problem instance solutions by the user . we propose an experiment in the domain of cartographic generalisation that shows promising results .

hybrid automated reasoning tools : from black-box to clear-box integration
recently , researchers in answer set programming and constraint programming spent significant efforts in the development of hybrid languages and solving algorithms combining the strengths of these traditionally separate fields . these efforts resulted in a new research area : constraint answer set programming ( casp ) . casp languages and systems proved to be largely successful at providing efficient solutions to problems involving hybrid reasoning tasks , such as scheduling problems with elements of planning . yet , the development of casp systems is difficult , requiring non-trivial expertise in multiple areas . this suggests a need for a study identifying general development principles of hybrid systems . once these principles and their implications are well understood , the development of hybrid languages and systems may become a well-established and well-understood routine process . as a step in this direction , in this paper we conduct a case study aimed at evaluating various integration schemas of casp methods .

on defining 'i ' `` i logy ''
could we define i ? throughout this article we give a negative answer to this question . more exactly , we show that there is no definition for i in a certain way . but this negative answer depends on our definition of definability . here , we try to consider sufficient generalized definition of definability . in the middle of paper a paradox will arise which makes us to modify the way we use the concept of property and definability .

laplace 's method approximations for probabilistic inference in belief networks with continuous variables
laplace 's method , a family of asymptotic methods used to approximate integrals , is presented as a potential candidate for the tool box of techniques used for knowledge acquisition and probabilistic inference in belief networks with continuous variables . this technique approximates posterior moments and marginal posterior distributions with reasonable accuracy [ errors are o ( n^-2 ) for posterior means ] in many interesting cases . the method also seems promising for computing approximations for bayes factors for use in the context of model selection , model uncertainty and mixtures of pdfs . the limitations , regularity conditions and computational difficulties for the implementation of laplace 's method are comparable to those associated with the methods of maximum likelihood and posterior mode analysis .

symmetry breaking predicates for sat-based dfa identification
it was shown before that the np-hard problem of deterministic finite automata ( dfa ) identification can be effectively translated to boolean satisfiability ( sat ) . modern sat-solvers can tackle hard dfa identification instances efficiently . we present a technique to reduce the problem search space by enforcing an enumeration of dfa states in depth-first search ( dfs ) or breadth-first search ( bfs ) order . we propose symmetry breaking predicates , which can be added to boolean formulae representing various dfa identification problems . we show how to apply this technique to dfa identification from both noiseless and noisy data . also we propose a method to identify all automata of the desired size . the proposed approach outperforms the current state-of-the-art dfasat method for dfa identification from noiseless data . a big advantage of the proposed approach is that it allows to determine exactly the existence or non-existence of a solution of the noisy dfa identification problem unlike metaheuristic approaches such as genetic algorithms .

sentiment predictability for stocks
in this work , we present our findings and experiments for stock-market prediction using various textual sentiment analysis tools , such as mood analysis and event extraction , as well as prediction models , such as lstms and specific convolutional architectures .

penalty logic and its link with dempster-shafer theory
penalty logic , introduced by pinkas , associates to each formula of a knowledge base the price to pay if this formula is violated . penalties may be used as a criterion for selecting preferred consistent subsets in an inconsistent knowledge base , thus inducing a non-monotonic inference relation . a precise formalization and the main properties of penalty logic and of its associated non-monotonic inference relation are given in the first part . we also show that penalty logic and dempster-shafer theory are related , especially in the infinitesimal case .

structural regularities in text-based entity vector spaces
entity retrieval is the task of finding entities such as people or products in response to a query , based solely on the textual documents they are associated with . recent semantic entity retrieval algorithms represent queries and experts in finite-dimensional vector spaces , where both are constructed from text sequences . we investigate entity vector spaces and the degree to which they capture structural regularities . such vector spaces are constructed in an unsupervised manner without explicit information about structural aspects . for concreteness , we address these questions for a specific type of entity : experts in the context of expert finding . we discover how clusterings of experts correspond to committees in organizations , the ability of expert representations to encode the co-author graph , and the degree to which they encode academic rank . we compare latent , continuous representations created using methods based on distributional semantics ( lsi ) , topic models ( lda ) and neural networks ( word2vec , doc2vec , sert ) . vector spaces created using neural methods , such as doc2vec and sert , systematically perform better at clustering than lsi , lda and word2vec . when it comes to encoding entity relations , sert performs best .

distributed autonomous online learning : regrets and intrinsic privacy-preserving properties
online learning has become increasingly popular on handling massive data . the sequential nature of online learning , however , requires a centralized learner to store data and update parameters . in this paper , we consider online learning with { \em distributed } data sources . the autonomous learners update local parameters based on local data sources and periodically exchange information with a small subset of neighbors in a communication network . we derive the regret bound for strongly convex functions that generalizes the work by ram et al . ( 2010 ) for convex functions . most importantly , we show that our algorithm has \emph { intrinsic } privacy-preserving properties , and we prove the sufficient and necessary conditions for privacy preservation in the network . these conditions imply that for networks with greater-than-one connectivity , a malicious learner can not reconstruct the subgradients ( and sensitive raw data ) of other learners , which makes our algorithm appealing in privacy sensitive applications .

a monte carlo aixi approximation
this paper introduces a principled approach for the design of a scalable general reinforcement learning agent . our approach is based on a direct approximation of aixi , a bayesian optimality notion for general reinforcement learning agents . previously , it has been unclear whether the theory of aixi could motivate the design of practical algorithms . we answer this hitherto open question in the affirmative , by providing the first computationally feasible approximation to the aixi agent . to develop our approximation , we introduce a new monte-carlo tree search algorithm along with an agent-specific extension to the context tree weighting algorithm . empirically , we present a set of encouraging results on a variety of stochastic and partially observable domains . we conclude by proposing a number of directions for future research .

solving stable matching problems using answer set programming
since the introduction of the stable marriage problem ( smp ) by gale and shapley ( 1962 ) , several variants and extensions have been investigated . while this variety is useful to widen the application potential , each variant requires a new algorithm for finding the stable matchings . to address this issue , we propose an encoding of the smp using answer set programming ( asp ) , which can straightforwardly be adapted and extended to suit the needs of specific applications . the use of asp also means that we can take advantage of highly efficient off-the-shelf solvers . to illustrate the flexibility of our approach , we show how our asp encoding naturally allows us to select optimal stable matchings , i.e . matchings that are optimal according to some user-specified criterion . to the best of our knowledge , our encoding offers the first exact implementation to find sex-equal , minimum regret , egalitarian or maximum cardinality stable matchings for smp instances in which individuals may designate unacceptable partners and ties between preferences are allowed . this paper is under consideration in theory and practice of logic programming ( tplp ) .

variational inference for uncertainty on the inputs of gaussian process models
the gaussian process latent variable model ( gp-lvm ) provides a flexible approach for non-linear dimensionality reduction that has been widely applied . however , the current approach for training gp-lvms is based on maximum likelihood , where the latent projection variables are maximized over rather than integrated out . in this paper we present a bayesian method for training gp-lvms by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a gp-lvm by maximizing an analytic lower bound on the exact marginal likelihood . we apply this method for learning a gp-lvm from iid observations and for learning non-linear dynamical systems where the observations are temporally correlated . we show that a benefit of the variational bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space . the resulting framework is generic , flexible and easy to extend for other purposes , such as gaussian process regression with uncertain inputs and semi-supervised gaussian processes . we demonstrate our method on synthetic data and standard machine learning benchmarks , as well as challenging real world datasets , including high resolution video data .

development of knowledge base expert system for natural treatment of diabetes disease
the development of expert system for treatment of diabetes disease by using natural methods is new information technology derived from artificial intelligent research using esta ( expert system text animation ) system . the proposed expert system contains knowledge about various methods of natural treatment methods ( massage , herbal/proper nutrition , acupuncture , gems ) for diabetes diseases of human beings . the system is developed in the esta ( expert system shell for text animation ) which is visual prolog 7.3 application . the knowledge for the said system will be acquired from domain experts , texts and other related sources .

a semantic loss function for deep learning with symbolic knowledge
this paper develops a novel methodology for using symbolic knowledge in deep learning . from first principles , we derive a semantic loss function that bridges between neural output vectors and logical constraints . this loss function captures how close the neural network is to satisfying the constraints on its output . an experimental evaluation shows that our semantic loss function effectively guides the learner to achieve ( near- ) state-of-the-art results on semi-supervised multi-class classification . moreover , it significantly increases the ability of the neural network to predict structured objects , such as rankings and paths . these discrete concepts are tremendously difficult to learn , and benefit from a tight integration of deep learning and symbolic reasoning methods .

introducer concepts in n-dimensional contexts
concept lattices are well-known conceptual structures that organise interesting patterns-the concepts-extracted from data . in some applications , such as software engineering or data mining , the size of the lattice can be a problem , as it is often too large to be efficiently computed , and too complex to be browsed . for this reason , the galois sub-hierarchy , a restriction of the concept lattice to introducer concepts , has been introduced as a smaller alternative . in this paper , we generalise the galois sub-hierarchy to n-lattices , conceptual structures obtained from multidimensional data in the same way that concept lattices are obtained from binary relations .

efficient planning under uncertainty with macro-actions
deciding how to act in partially observable environments remains an active area of research . identifying good sequences of decisions is particularly challenging when good control performance requires planning multiple steps into the future in domains with many states . towards addressing this challenge , we present an online , forward-search algorithm called the posterior belief distribution ( pbd ) . pbd leverages a novel method for calculating the posterior distribution over beliefs that result after a sequence of actions is taken , given the set of observation sequences that could be received during this process . this method allows us to efficiently evaluate the expected reward of a sequence of primitive actions , which we refer to as macro-actions . we present a formal analysis of our approach , and examine its performance on two very large simulation experiments : scientific exploration and a target monitoring domain . we also demonstrate our algorithm being used to control a real robotic helicopter in a target monitoring experiment , which suggests that our approach has practical potential for planning in real-world , large partially observable domains where a multi-step lookahead is required to achieve good performance .

algorithms for learning decomposable models and chordal graphs
decomposable dependency models and their graphical counterparts , i.e. , chordal graphs , possess a number of interesting and useful properties . on the basis of two characterizations of decomposable models in terms of independence relationships , we develop an exact algorithm for recovering the chordal graphical representation of any given decomposable model . we also propose an algorithm for learning chordal approximations of dependency models isomorphic to general undirected graphs .

a decision-making support system based on know-how
the research results described are concerned with : - developing a domain modeling method and tools to provide the design and implementation of decision-making support systems for computer integrated manufacturing ; - building a decision-making support system based on know-how and its software environment . the research is funded by nedo , japan .

vagueness of linguistic variable
in the area of computer science focusing on creating machines that can engage on behaviors that humans consider intelligent . the ability to create intelligent machines has intrigued humans since ancient times and today with the advent of the computer and 50 years of research into various programming techniques , the dream of smart machines is becoming a reality . researchers are creating systems which can mimic human thought , understand speech , beat the best human chessplayer , and countless other feats never before possible . ability of the human to estimate the information is most brightly shown in using of natural languages . using words of a natural language for valuation qualitative attributes , for example , the person pawns uncertainty in form of vagueness in itself estimations . vague sets , vague judgments , vague conclusions takes place there and then , where and when the reasonable subject exists and also is interested in something . the vague sets theory has arisen as the answer to an illegibility of language the reasonable subject speaks . language of a reasonable subject is generated by vague events which are created by the reason and which are operated by the mind . the theory of vague sets represents an attempt to find such approximation of vague grouping which would be more convenient , than the classical theory of sets in situations where the natural language plays a significant role . such theory has been offered by known american mathematician gau and buehrer .in our paper we are describing how vagueness of linguistic variables can be solved by using the vague set theory.this paper is mainly designed for one of directions of the eventology ( the theory of the random vague events ) , which has arisen within the limits of the probability theory and which pursue the unique purpose to describe eventologically a movement of reason .

message-passing algorithms for quadratic programming formulations of map estimation
computing maximum a posteriori ( map ) estimation in graphical models is an important inference problem with many applications . we present message-passing algorithms for quadratic programming ( qp ) formulations of map estimation for pairwise markov random fields . in particular , we use the concave-convex procedure ( cccp ) to obtain a locally optimal algorithm for the non-convex qp formulation . a similar technique is used to derive a globally convergent algorithm for the convex qp relaxation of map . we also show that a recently developed expectation-maximization ( em ) algorithm for the qp formulation of map can be derived from the cccp perspective . experiments on synthetic and real-world problems confirm that our new approach is competitive with max-product and its variations . compared with cplex , we achieve more than an order-of-magnitude speedup in solving optimally the convex qp relaxation .

constraint processing in lifted probabilistic inference
first-order probabilistic models combine representational power of first-order logic with graphical models . there is an ongoing effort to design lifted inference algorithms for first-order probabilistic models . we analyze lifted inference from the perspective of constraint processing and , through this viewpoint , we analyze and compare existing approaches and expose their advantages and limitations . our theoretical results show that the wrong choice of constraint processing method can lead to exponential increase in computational complexity . our empirical tests confirm the importance of constraint processing in lifted inference . this is the first theoretical and empirical study of constraint processing in lifted inference .

a case of combination of evidence in the dempster-shafer theory inconsistent with evaluation of probabilities
the dempster-shafer theory of evidence accumulation is one of the main tools for combining data obtained from multiple sources . in this paper a special case of combination of two bodies of evidence with non-zero conflict coefficient is considered . it is shown that application of the dempster-shafer rule of combination in this case leads to an evaluation of masses of the combined bodies that is different from the evaluation of the corresponding probabilities obtained by application of the law of total probability . this finding supports the view that probabilistic interpretation of results of the dempster-shafer analysis in the general case is not appropriate .

the bag semantics of ontology-based data access
ontology-based data access ( obda ) is a popular approach for integrating and querying multiple data sources by means of a shared ontology . the ontology is linked to the sources using mappings , which assign views over the data to ontology predicates . motivated by the need for obda systems supporting database-style aggregate queries , we propose a bag semantics for obda , where duplicate tuples in the views defined by the mappings are retained , as is the case in standard databases . we show that bag semantics makes conjunctive query answering in obda conp-hard in data complexity . to regain tractability , we consider a rather general class of queries and show its rewritability to a generalisation of the relational calculus to bags .

constrained bayesian networks : theory , optimization , and applications
we develop the theory and practice of an approach to modelling and probabilistic inference in causal networks that is suitable when application-specific or analysis-specific constraints should inform such inference or when little or no data for the learning of causal network structure or probability values at nodes are available . constrained bayesian networks generalize a bayesian network such that probabilities can be symbolic , arithmetic expressions and where the meaning of the network is constrained by finitely many formulas from the theory of the reals . a formal semantics for constrained bayesian networks over first-order logic of the reals is given , which enables non-linear and non-convex optimisation algorithms that rely on decision procedures for this logic , and supports the composition of several constrained bayesian networks . a non-trivial case study in arms control , where few or no data are available to assess the effectiveness of an arms inspection process , evaluates our approach . an open-access prototype implementation of these foundations and their algorithms uses the smt solver z3 as decision procedure , leverages an open-source package for bayesian inference to symbolic computation , and is evaluated experimentally .

feature extraction using latent dirichlet allocation and neural networks : a case study on movie synopses
feature extraction has gained increasing attention in the field of machine learning , as in order to detect patterns , extract information , or predict future observations from big data , the urge of informative features is crucial . the process of extracting features is highly linked to dimensionality reduction as it implies the transformation of the data from a sparse high-dimensional space , to higher level meaningful abstractions . this dissertation employs neural networks for distributed paragraph representations , and latent dirichlet allocation to capture higher level features of paragraph vectors . although neural networks for distributed paragraph representations are considered the state of the art for extracting paragraph vectors , we show that a quick topic analysis model such as latent dirichlet allocation can provide meaningful features too . we evaluate the two methods on the cmu movie summary corpus , a collection of 25,203 movie plot summaries extracted from wikipedia . finally , for both approaches , we use k-nearest neighbors to discover similar movies , and plot the projected representations using t-distributed stochastic neighbor embedding to depict the context similarities . these similarities , expressed as movie distances , can be used for movies recommendation . the recommended movies of this approach are compared with the recommended movies from imdb , which use a collaborative filtering recommendation approach , to show that our two models could constitute either an alternative or a supplementary recommendation approach .

computational results for extensive-form adversarial team games
we provide , to the best of our knowledge , the first computational study of extensive-form adversarial team games . these games are sequential , zero-sum games in which a team of players , sharing the same utility function , faces an adversary . we define three different scenarios according to the communication capabilities of the team . in the first , the teammates can communicate and correlate their actions both before and during the play . in the second , they can only communicate before the play . in the third , no communication is possible at all . we define the most suitable solution concepts , and we study the inefficiency caused by partial or null communication , showing that the inefficiency can be arbitrarily large in the size of the game tree . furthermore , we study the computational complexity of the equilibrium-finding problem in the three scenarios mentioned above , and we provide , for each of the three scenarios , an exact algorithm . finally , we empirically evaluate the scalability of the algorithms in random games and the inefficiency caused by partial or null communication .

using causal information and local measures to learn bayesian networks
in previous work we developed a method of learning bayesian network models from raw data . this method relies on the well known minimal description length ( mdl ) principle . the mdl principle is particularly well suited to this task as it allows us to tradeoff , in a principled way , the accuracy of the learned network against its practical usefulness . in this paper we present some new results that have arisen from our work . in particular , we present a new local way of computing the description length . this allows us to make significant improvements in our search algorithm . in addition , we modify our algorithm so that it can take into account partial domain information that might be provided by a domain expert . the local computation of description length also opens the door for local refinement of an existent network . the feasibility of our approach is demonstrated by experiments involving networks of a practical size .

on the behavioural formalization of the cognitive middleware awdrat
we present our ongoing work and initial results towards the ( behavioral ) correctness analysis of the cognitive middleware awdrat . since , the ( provable ) behavioral correctness of a software system is a fundamental pre-requisite of the system 's security . therefore , the goal of the work is to first formalize the behavioral semantics of the middleware as a pre-requisite for our proof of the behavioral correctness . however , in this paper , we focus only on the core and critical component of the middleware , i.e . execution monitor which is a part of the module `` architectural differencer '' of awdrat . the role of the execution monitor is to identify inconsistencies between runtime observations of the target system and predictions of the specification system architectural model of the system . as a starting point we have defined the formal ( denotational ) semantics of the observations ( runtime events ) and predictions ( executable specifications as of system architectural model ) ; then based on the aforementioned formal semantices , we have formalized the behavior of the `` execution monitor '' of the middleware .

grounding bound founded answer set programs
to appear in theory and practice of logic programming ( tplp ) bound founded answer set programming ( bfasp ) is an extension of answer set programming ( asp ) that extends stable model semantics to numeric variables . while the theory of bfasp is defined on ground rules , in practice bfasp programs are written as complex non-ground expressions . flattening of bfasp is a technique used to simplify arbitrary expressions of the language to a small and well defined set of primitive expressions . in this paper , we first show how we can flatten arbitrary bfasp rule expressions , to give equivalent bfasp programs . next , we extend the bottom-up grounding technique and magic set transformation used by asp to bfasp programs . our implementation shows that for bfasp problems , these techniques can significantly reduce the ground program size , and improve subsequent solving .

a contextual-bandit approach to personalized news article recommendation
personalized web services strive to adapt their services ( advertisements , news articles , etc ) to individual users by making use of both content and user information . despite a few recent advances , this problem remains challenging for at least two reasons . first , web service is featured with dynamically changing pools of content , rendering traditional collaborative filtering methods inapplicable . second , the scale of most web services of practical interest calls for solutions that are both fast in learning and computation . in this work , we model personalized recommendation of news articles as a contextual bandit problem , a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles , while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks . the contributions of this work are three-fold . first , we propose a new , general contextual bandit algorithm that is computationally efficient and well motivated from learning theory . second , we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic . finally , using this offline evaluation method , we successfully applied our new algorithm to a yahoo ! front page today module dataset containing over 33 million events . results showed a 12.5 % click lift compared to a standard context-free bandit algorithm , and the advantage becomes even greater when data gets more scarce .

an argumentative agent-based model of scientific inquiry
in this paper we present an agent-based model ( abm ) of scientific inquiry aimed at investigating how different social networks impact the efficiency of scientists in acquiring knowledge . as such , the abm is a computational tool for tackling issues in the domain of scientific methodology and science policy . in contrast to existing abms of science , our model aims to represent the argumentative dynamics that underlies scientific practice . to this end we employ abstract argumentation theory as the core design feature of the model . this helps to avoid a number of problematic idealizations which are present in other abms of science and which impede their relevance for actual scientific practice .

keyphrase extraction using sequential labeling
keyphrases efficiently summarize a document 's content and are used in various document processing and retrieval tasks . several unsupervised techniques and classifiers exist for extracting keyphrases from text documents . most of these methods operate at a phrase-level and rely on part-of-speech ( pos ) filters for candidate phrase generation . in addition , they do not directly handle keyphrases of varying lengths . we overcome these modeling shortcomings by addressing keyphrase extraction as a sequential labeling task in this paper . we explore a basic set of features commonly used in nlp tasks as well as predictions from various unsupervised methods to train our taggers . in addition to a more natural modeling for the keyphrase extraction problem , we show that tagging models yield significant performance benefits over existing state-of-the-art extraction methods .

structured sparse method for hyperspectral unmixing
hyperspectral unmixing ( hu ) has received increasing attention in the past decades due to its ability of unveiling information latent in hyperspectral data . unfortunately , most existing methods fail to take advantage of the spatial information in data . to overcome this limitation , we propose a structured sparse regularized nonnegative matrix factorization ( ss-nmf ) method from the following two aspects . first , we incorporate a graph laplacian to encode the manifold structures embedded in the hyperspectral data space . in this way , the highly similar neighboring pixels can be grouped together . second , the lasso penalty is employed in ss-nmf for the fact that pixels in the same manifold structure are sparsely mixed by a common set of relevant bases . these two factors act as a new structured sparse constraint . with this constraint , our method can learn a compact space , where highly similar pixels are grouped to share correlated sparse representations . experiments on real hyperspectral data sets with different noise levels demonstrate that our method outperforms the state-of-the-art methods significantly .

efficient informative sensing using multiple robots
the need for efficient monitoring of spatio-temporal dynamics in large environmental applications , such as the water quality monitoring in rivers and lakes , motivates the use of robotic sensors in order to achieve sufficient spatial coverage . typically , these robots have bounded resources , such as limited battery or limited amounts of time to obtain measurements . thus , careful coordination of their paths is required in order to maximize the amount of information collected , while respecting the resource constraints . in this paper , we present an efficient approach for near-optimally solving the np-hard optimization problem of planning such informative paths . in particular , we first develop esip ( efficient single-robot informative path planning ) , an approximation algorithm for optimizing the path of a single robot . hereby , we use a gaussian process to model the underlying phenomenon , and use the mutual information between the visited locations and remainder of the space to quantify the amount of information collected . we prove that the mutual information collected using paths obtained by using esip is close to the information obtained by an optimal solution . we then provide a general technique , sequential allocation , which can be used to extend any single robot planning algorithm , such as esip , for the multi-robot problem . this procedure approximately generalizes any guarantees for the single-robot problem to the multi-robot case . we extensively evaluate the effectiveness of our approach on several experiments performed in-field for two important environmental sensing applications , lake and river monitoring , and simulation experiments performed using several real world sensor network data sets .

second order probabilities for uncertain and conflicting evidence
in this paper the elicitation of probabilities from human experts is considered as a measurement process , which may be disturbed by random 'measurement noise ' . using bayesian concepts a second order probability distribution is derived reflecting the uncertainty of the input probabilities . the algorithm is based on an approximate sample representation of the basic probabilities . this sample is continuously modified by a stochastic simulation procedure , the metropolis algorithm , such that the sequence of successive samples corresponds to the desired posterior distribution . the procedure is able to combine inconsistent probabilities according to their reliability and is applicable to general inference networks with arbitrary structure . dempster-shafer probability mass functions may be included using specific measurement distributions . the properties of the approach are demonstrated by numerical experiments .

a fuzzy petri nets model for computing with words
motivated by zadeh 's paradigm of computing with words rather than numbers , several formal models of computing with words have recently been proposed . these models are based on automata and thus are not well-suited for concurrent computing . in this paper , we incorporate the well-known model of concurrent computing , petri nets , together with fuzzy set theory and thereby establish a concurrency model of computing with words -- fuzzy petri nets for computing with words ( fpncws ) . the new feature of such fuzzy petri nets is that the labels of transitions are some special words modeled by fuzzy sets . by employing the methodology of fuzzy reasoning , we give a faithful extension of an fpncw which makes it possible for computing with more words . the language expressiveness of the two formal models of computing with words , fuzzy automata for computing with words and fpncws , is compared as well . a few small examples are provided to illustrate the theoretical development .

multilingual and cross-lingual timeline extraction
in this paper we present an approach to extract ordered timelines of events , their participants , locations and times from a set of multilingual and cross-lingual data sources . based on the assumption that event-related information can be recovered from different documents written in different languages , we extend the cross-document event ordering task presented at semeval 2015 by specifying two new tasks for , respectively , multilingual and cross-lingual timeline extraction . we then develop three deterministic algorithms for timeline extraction based on two main ideas . first , we address implicit temporal relations at document level since explicit time-anchors are too scarce to build a wide coverage timeline extraction system . second , we leverage several multilingual resources to obtain a single , inter-operable , semantic representation of events across documents and across languages . the result is a highly competitive system that strongly outperforms the current state-of-the-art . nonetheless , further analysis of the results reveals that linking the event mentions with their target entities and time-anchors remains a difficult challenge . the systems , resources and scorers are freely available to facilitate its use and guarantee the reproducibility of results .

outlier detection by logic programming
the development of effective knowledge discovery techniques has become in the recent few years a very active research area due to the important impact it has in several relevant application areas . one interesting task thereof is that of singling out anomalous individuals from a given population , e.g. , to detect rare events in time-series analysis settings , or to identify objects whose behavior is deviant w.r.t . a codified standard set of `` social '' rules . such exceptional individuals are usually referred to as outliers in the literature . recently , outlier detection has also emerged as a relevant kr & r problem . in this paper , we formally state the concept of outliers by generalizing in several respects an approach recently proposed in the context of default logic , for instance , by having outliers not being restricted to single individuals but , rather , in the more general case , to correspond to entire ( sub ) theories . we do that within the context of logic programming and , mainly through examples , we discuss its potential practical impact in applications . the formalization we propose is a novel one and helps in shedding some light on the real nature of outliers . moreover , as a major contribution of this work , we illustrate the exploitation of minimality criteria in outlier detection . the computational complexity of outlier detection problems arising in this novel setting is thoroughly investigated and accounted for in the paper as well . finally , we also propose a rewriting algorithm that transforms any outlier detection problem into an equivalent inference problem under the stable model semantics , thereby making outlier computation effective and realizable on top of any stable model solver .

statistical-mechanical analysis of pre-training and fine tuning in deep learning
in this paper , we present a statistical-mechanical analysis of deep learning . we elucidate some of the essential components of deep learning -- -pre-training by unsupervised learning and fine tuning by supervised learning . we formulate the extraction of features from the training data as a margin criterion in a high-dimensional feature-vector space . the self-organized classifier is then supplied with small amounts of labelled data , as in deep learning . although we employ a simple single-layer perceptron model , rather than directly analyzing a multi-layer neural network , we find a nontrivial phase transition that is dependent on the number of unlabelled data in the generalization error of the resultant classifier . in this sense , we evaluate the efficacy of the unsupervised learning component of deep learning . the analysis is performed by the replica method , which is a sophisticated tool in statistical mechanics . we validate our result in the manner of deep learning , using a simple iterative algorithm to learn the weight vector on the basis of belief propagation .

translation of pronominal anaphora between english and spanish : discrepancies and evaluation
this paper evaluates the different tasks carried out in the translation of pronominal anaphora in a machine translation ( mt ) system . the mt interlingua approach named agir ( anaphora generation with an interlingua representation ) improves upon other proposals presented to date because it is able to translate intersentential anaphors , detect co-reference chains , and translate spanish zero pronouns into english -- -issues hardly considered by other systems . the paper presents the resolution and evaluation of these anaphora problems in agir with the use of different kinds of knowledge ( lexical , morphological , syntactic , and semantic ) . the translation of english and spanish anaphoric third-person personal pronouns ( including spanish zero pronouns ) into the target language has been evaluated on unrestricted corpora . we have obtained a precision of 80.4 % and 84.8 % in the translation of spanish and english pronouns , respectively . although we have only studied the spanish and english languages , our approach can be easily extended to other languages such as portuguese , italian , or japanese .

inferring cognitive models from data using approximate bayesian computation
an important problem for hci researchers is to estimate the parameter values of a cognitive model from behavioral data . this is a difficult problem , because of the substantial complexity and variety in human behavioral strategies . we report an investigation into a new approach using approximate bayesian computation ( abc ) to condition model parameters to data and prior knowledge . as the case study we examine menu interaction , where we have click time data only to infer a cognitive model that implements a search behaviour with parameters such as fixation duration and recall probability . our results demonstrate that abc ( i ) improves estimates of model parameter values , ( ii ) enables meaningful comparisons between model variants , and ( iii ) supports fitting models to individual users . abc provides ample opportunities for theoretical hci research by allowing principled inference of model parameter values and their uncertainty .

towards a unified taxonomy of biclustering methods
being an unsupervised machine learning and data mining technique , biclustering and its multimodal extensions are becoming popular tools for analysing object-attribute data in different domains . apart from conventional clustering techniques , biclustering is searching for homogeneous groups of objects while keeping their common description , e.g. , in binary setting , their shared attributes . in bioinformatics , biclustering is used to find genes , which are active in a subset of situations , thus being candidates for biomarkers . however , the authors of those biclustering techniques that are popular in gene expression analysis , may overlook the existing methods . for instance , bimax algorithm is aimed at finding biclusters , which are well-known for decades as formal concepts . moreover , even if bioinformatics classify the biclustering methods according to reasonable domain-driven criteria , their classification taxonomies may be different from survey to survey and not full as well . so , in this paper we propose to use concept lattices as a tool for taxonomy building ( in the biclustering domain ) and attribute exploration as means for cross-domain taxonomy completion .

task-based end-to-end model learning in stochastic optimization
with the increasing popularity of machine learning techniques , it has become common to see prediction algorithms operating within some larger process . however , the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them . this paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used , within the context of stochastic programming . we present three experimental evaluations of the proposed approach : a classical inventory stock problem , a real-world electrical grid scheduling task , and a real-world energy storage arbitrage task . we show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications .

alternative markov and causal properties for acyclic directed mixed graphs
we extend andersson-madigan-perlman chain graphs by ( i ) relaxing the semidirected acyclity constraint so that only directed cycles are forbidden , and ( ii ) allowing up to two edges between any pair of nodes . we introduce global , and ordered local and pairwise markov properties for the new models . we show the equivalence of these properties for strictly positive probability distributions . we also show that when the random variables are continuous , the new models can be interpreted as systems of structural equations with correlated errors . this enables us to adapt pearl 's do-calculus to them . finally , we describe an exact algorithm for learning the new models from observational and interventional data via answer set programming .

calibrating chemical multisensory devices for real world applications : an in-depth comparison of quantitative machine learning approaches
chemical multisensor devices need calibration algorithms to estimate gas concentrations . their possible adoption as indicative air quality measurements devices poses new challenges due to the need to operate in continuous monitoring modes in uncontrolled environments . several issues , including slow dynamics , continue to affect their real world performances . at the same time , the need for estimating pollutant concentrations on board the devices , espe- cially for wearables and iot deployments , is becoming highly desirable . in this framework , several calibration approaches have been proposed and tested on a variety of proprietary devices and datasets ; still , no thorough comparison is available to researchers . this work attempts a benchmarking of the most promising calibration algorithms according to recent literature with a focus on machine learning approaches . we test the techniques against absolute and dynamic performances , generalization capabilities and computational/storage needs using three different datasets sharing continuous monitoring operation methodology . our results can guide researchers and engineers in the choice of optimal strategy . they show that non-linear multivariate techniques yield reproducible results , outperforming lin- ear approaches . specifically , the support vector regression method consistently shows good performances in all the considered scenarios . we highlight the enhanced suitability of shallow neural networks in a trade-off between performance and computational/storage needs . we confirm , on a much wider basis , the advantages of dynamic approaches with respect to static ones that only rely on instantaneous sensor array response . the latter have been shown to be best choice whenever prompt and precise response is needed .

learning to transfer
transfer learning borrows knowledge from a source domain to facilitate learning in a target domain . two primary issues to be addressed in transfer learning are what and how to transfer . for a pair of domains , adopting different transfer learning algorithms results in different knowledge transferred between them . to discover the optimal transfer learning algorithm that maximally improves the learning performance in the target domain , researchers have to exhaustively explore all existing transfer learning algorithms , which is computationally intractable . as a trade-off , a sub-optimal algorithm is selected , which requires considerable expertise in an ad-hoc way . meanwhile , it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices . motivated by this , we propose a novel transfer learning framework known as learning to transfer ( l2t ) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences . we establish the l2t framework in two stages : 1 ) we first learn a reflection function encrypting transfer learning skills from experiences ; and 2 ) we infer what and how to transfer for a newly arrived pair of domains by optimizing the reflection function . extensive experiments demonstrate the l2t 's superiority over several state-of-the-art transfer learning algorithms and its effectiveness on discovering more transferable knowledge .

computing rational decisions in extensive games with limited foresight
we introduce a class of extensive form games where players might not be able to foresee the possible consequences of their decisions and form a model of their opponents which they exploit to achieve a more profitable outcome . we improve upon existing models of games with limited foresight , endowing players with the ability of higher-order reasoning and proposing a novel solution concept to address intuitions coming from real game play . we analyse the resulting equilibria , devising an effective procedure to compute them .

klog : a language for logical and relational learning with kernels
we introduce klog , a novel approach to statistical relational learning . unlike standard approaches , klog does not represent a probability distribution directly . it is rather a language to perform kernel-based learning on expressive logical and relational representations . klog allows users to specify learning problems declaratively . it builds on simple but powerful concepts : learning from interpretations , entity/relationship data modeling , logic programming , and deductive databases . access by the kernel to the rich representation is mediated by a technique we call graphicalization : the relational representation is first transformed into a graph -- - in particular , a grounded entity/relationship diagram . subsequently , a choice of graph kernel defines the feature space . klog supports mixed numerical and symbolic data , as well as background knowledge in the form of prolog or datalog programs as in inductive logic programming systems . the klog framework can be applied to tackle the same range of tasks that has made statistical relational learning so popular , including classification , regression , multitask learning , and collective classification . we also report about empirical comparisons , showing that klog can be either more accurate , or much faster at the same level of accuracy , than tilde and alchemy . klog is gplv3 licensed and is available at http : //klog.dinfo.unifi.it along with tutorials .

can machines think in radio language ?
people can think in auditory , visual and tactile forms of language , so can machines principally . but is it possible for them to think in radio language ? according to a first principle presented for general intelligence , i.e . the principle of language 's relativity , the answer may give an exceptional solution for robot astronauts to talk with each other in space exploration .

an evaluation of naive bayesian anti-spam filtering
it has recently been argued that a naive bayesian classifier can be used to filter unsolicited bulk e-mail ( `` spam '' ) . we conduct a thorough evaluation of this proposal on a corpus that we make publicly available , contributing towards standard benchmarks . at the same time we investigate the effect of attribute-set size , training-corpus size , lemmatization , and stop-lists on the filter 's performance , issues that had not been previously explored . after introducing appropriate cost-sensitive evaluation measures , we reach the conclusion that additional safety nets are needed for the naive bayesian anti-spam filter to be viable in practice .

tree-cnn : a deep convolutional neural network for lifelong learning
in recent years , convolutional neural networks ( cnns ) have shown remarkable performance in many computer vision tasks such as object recognition and detection . however , complex training issues , such as `` catastrophic forgetting '' and hyper-parameter tuning , make incremental learning in cnns a difficult challenge . in this paper , we propose a hierarchical deep neural network , with cnns at multiple levels , and a corresponding training method for lifelong learning . the network grows in a tree-like manner to accommodate the new classes of data without losing the ability to identify the previously trained classes . the proposed network was tested on cifar-10 and cifar-100 datasets , and compared against the method of fine tuning specific layers of a conventional cnn . we obtained comparable accuracies and achieved 40 % and 20 % reduction in training effort in cifar-10 and cifar 100 respectively . the network was able to organize the incoming classes of data into feature-driven super-classes . our model improves upon existing hierarchical cnn models by adding the capability of self-growth and also yields important observations on feature selective classification .

quantum robot : structure , algorithms and applications
this paper has been withdrawn .

evidence algorithm and system for automated deduction : a retrospective view
a research project aimed at the development of an automated theorem proving system was started in kiev ( ukraine ) in early 1960s . the mastermind of the project , academician v.glushkov , baptized it `` evidence algorithm '' , ea . the work on the project lasted , off and on , more than 40 years . in the framework of the project , the russian and english versions of the system for automated deduction , sad , were constructed . they may be already seen as powerful theorem-proving assistants .

automated game design learning
while general game playing is an active field of research , the learning of game design has tended to be either a secondary goal of such research or it has been solely the domain of humans . we propose a field of research , automated game design learning ( agdl ) , with the direct purpose of learning game designs directly through interaction with games in the mode that most people experience games : via play . we detail existing work that touches the edges of this field , describe current successful projects in agdl and the theoretical foundations that enable them , point to promising applications enabled by agdl , and discuss next steps for this exciting area of study . the key moves of agdl are to use game programs as the ultimate source of truth about their own design , and to make these design properties available to other systems and avenues of inquiry .

the gtr-model : a universal framework for quantum-like measurements
we present a very general geometrico-dynamical description of physical or more abstract entities , called the 'general tension-reduction ' ( gtr ) model , where not only states , but also measurement-interactions can be represented , and the associated outcome probabilities calculated . underlying the model is the hypothesis that indeterminism manifests as a consequence of unavoidable fluctuations in the experimental context , in accordance with the 'hidden-measurements interpretation ' of quantum mechanics . when the structure of the state space is hilbertian , and measurements are of the 'universal ' kind , i.e. , are the result of an average over all possible ways of selecting an outcome , the gtr-model provides the same predictions of the born rule , and therefore provides a natural completed version of quantum mechanics . however , when the structure of the state space is non-hilbertian and/or not all possible ways of selecting an outcome are available to be actualized , the predictions of the model generally differ from the quantum ones , especially when sequential measurements are considered . some paradigmatic examples will be discussed , taken from physics and human cognition . particular attention will be given to some known psychological effects , like question order effects and response replicability , which we show are able to generate non-hilbertian statistics . we also suggest a realistic interpretation of the gtr-model , when applied to human cognition and decision , which we think could become the generally adopted interpretative framework in quantum cognition research .

feature representation for icu mortality
good predictors of icu mortality have the potential to identify high-risk patients earlier , improve icu resource allocation , or create more accurate population-level risk models . machine learning practitioners typically make choices about how to represent features in a particular model , but these choices are seldom evaluated quantitatively . this study compares the performance of different representations of clinical event data from mimic ii in a logistic regression model to predict 36-hour icu mortality . the most common representations are linear ( normalized counts ) and binary ( yes/no ) . these , along with a new representation termed `` hill '' , are compared using both l1 and l2 regularization . results indicate that the introduced `` hill '' representation outperforms both the binary and linear representations , the hill representation thus has the potential to improve existing models of icu mortality .

graphical representations of consensus belief
graphical models based on conditional independence support concise encodings of the subjective belief of a single agent . a natural question is whether the consensus belief of a group of agents can be represented with equal parsimony . we prove , under relatively mild assumptions , that even if everyone agrees on a common graph topology , no method of combining beliefs can maintain that structure . even weaker conditions rule out local aggregation within conditional probability tables . on a more positive note , we show that if probabilities are combined with the logarithmic opinion pool ( logop ) , then commonly held markov independencies are maintained . this suggests a straightforward procedure for constructing a consensus markov network . we describe an algorithm for computing the logop with time complexity comparable to that of exact bayesian inference .

automatic rule extraction from long short term memory networks
although deep learning models have proven effective at solving problems in natural language processing , the mechanism by which they come to their conclusions is often unclear . as a result , these models are generally treated as black boxes , yielding no insight of the underlying learned patterns . in this paper we consider long short term memory networks ( lstms ) and demonstrate a new approach for tracking the importance of a given input to the lstm for a given output . by identifying consistently important patterns of words , we are able to distill state of the art lstms on sentiment analysis and question answering into a set of representative phrases . this representation is then quantitatively validated by using the extracted phrases to construct a simple , rule-based classifier which approximates the output of the lstm .

tensorlog : deep learning meets probabilistic dbs
we present an implementation of a probabilistic first-order logic called tensorlog , in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as tensorflow or theano . this leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure : in particular , it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic . experimental results show that tensorlog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples .

machine learning , clustering , and polymorphy
this paper describes a machine induction program ( witt ) that attempts to model human categorization . properties of categories to which human subjects are sensitive includes best or prototypical members , relative contrasts between putative categories , and polymorphy ( neither necessary or sufficient features ) . this approach represents an alternative to usual artificial intelligence approaches to generalization and conceptual clustering which tend to focus on necessary and sufficient feature rules , equivalence classes , and simple search and match schemes . witt is shown to be more consistent with human categorization while potentially including results produced by more traditional clustering schemes . applications of this approach in the domains of expert systems and information retrieval are also discussed .

computoser - rule-based , probability-driven algorithmic music composition
this paper presents the computoser hybrid probability/rule based algorithm for music composition ( http : //computoser.com ) and provides a reference implementation . it addresses the issues of unpleasantness and lack of variation exhibited by many existing approaches by combining the two methods ( basing the parameters of the rules on data obtained from preliminary analysis ) . a sample of 500+ musical pieces was analyzed to derive probabilities for musical characteristics and events ( e.g . scale , tempo , intervals ) . the algorithm was constructed to produce musical pieces using the derived probabilities combined with a large set of composition rules , which were obtained and structured after studying established composition practices . generated pieces were published on the computoser website where evaluation was performed by listeners . the feedback was positive ( 58.4 % approval ) , asserting the merits of the undertaken approach . the paper compares this hybrid approach to other approaches to algorithmic composition and presents a survey of the pleasantness of the resulting music .

taxonomy , structure , and implementation of evidential reasoning
the fundamental elements of evidential reasoning problems are described , followed by a discussion of the structure of various types of problems . bayesian inference networks and state space formalism are used as the tool for problem representation . a human-oriented decision making cycle for solving evidential reasoning problems is described and illustrated for a military situation assessment problem . the implementation of this cycle may serve as the basis for an expert system shell for evidential reasoning ; i.e . a situation assessment processor .

a new data representation based on training data characteristics to extract drug named-entity in medical text
one essential task in information extraction from the medical corpus is drug name recognition . compared with text sources come from other domains , the medical text is special and has unique characteristics . in addition , the medical text mining poses more challenges , e.g. , more unstructured text , the fast growing of new terms addition , a wide range of name variation for the same drug . the mining is even more challenging due to the lack of labeled dataset sources and external knowledge , as well as multiple token representations for a single drug name that is more common in the real application setting . although many approaches have been proposed to overwhelm the task , some problems remained with poor f-score performance ( less than 0.75 ) . this paper presents a new treatment in data representation techniques to overcome some of those challenges . we propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training . the first technique is evaluated with the standard nn model , i.e. , mlp ( multi-layer perceptrons ) . the second technique involves two deep network classifiers , i.e. , dbn ( deep belief networks ) , and sae ( stacked denoising encoders ) . the third technique represents the sentence as a sequence that is evaluated with a recurrent nn model , i.e. , lstm ( long short term memory ) . in extracting the drug name entities , the third technique gives the best f-score performance compared to the state of the art , with its average f-score being 0.8645 .

best of both worlds : transferring knowledge from discriminative learning to a generative visual dialog model
we present a novel training framework for neural sequence models , particularly for grounded dialog generation . the standard training paradigm for these models is maximum likelihood estimation ( mle ) , or minimizing the cross-entropy of the human responses . across a variety of domains , a recurring problem with mle trained generative neural dialog models ( g ) is that they tend to produce 'safe ' and generic responses ( `` i do n't know '' , `` i ca n't tell '' ) . in contrast , discriminative dialog models ( d ) that are trained to rank a list of candidate human responses outperform their generative counterparts ; in terms of automatic metrics , diversity , and informativeness of the responses . however , d is not useful in practice since it can not be deployed to have real conversations with users . our work aims to achieve the best of both worlds -- the practical usefulness of g and the strong performance of d -- via knowledge transfer from d to g. our primary contribution is an end-to-end trainable generative visual dialog model , where g receives gradients from d as a perceptual ( not adversarial ) loss of the sequence sampled from g. we leverage the recently proposed gumbel-softmax ( gs ) approximation to the discrete distribution -- specifically , an rnn augmented with a sequence of gs samplers , coupled with the straight-through gradient estimator to enable end-to-end differentiability . we also introduce a stronger encoder for visual dialog , and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid d in better capturing semantic similarities in answer responses . overall , our proposed model outperforms state-of-the-art on the visdial dataset by a significant margin ( 2.67 % on recall @ 10 ) . the source code can be downloaded from https : //github.com/jiasenlu/visdial.pytorch .

a behavioural foundation for natural computing and a programmability test
what does it mean to claim that a physical or natural system computes ? one answer , endorsed here , is that computing is about programming a system to behave in different ways . this paper offers an account of what it means for a physical system to compute based on this notion . it proposes a behavioural characterisation of computing in terms of a measure of programmability , which reflects a system 's ability to react to external stimuli . the proposed measure of programmability is useful for classifying computers in terms of the apparent algorithmic complexity of their evolution in time . i make some specific proposals in this connection and discuss this approach in the context of other behavioural approaches , notably turing 's test of machine intelligence . i also anticipate possible objections and consider the applicability of these proposals to the task of relating abstract computation to nature-like computation .

qualitative models for decision under uncertainty without the commensurability assumption
this paper investigates a purely qualitative version of savage 's theory for decision making under uncertainty . until now , most representation theorems for preference over acts rely on a numerical representation of utility and uncertainty where utility and uncertainty are commensurate . disrupting the tradition , we relax this assumption and introduce a purely ordinal axiom requiring that the decision maker ( dm ) preference between two acts only depends on the relative position of their consequences for each state . within this qualitative framework , we determine the only possible form of the decision rule and investigate some instances compatible with the transitivity of the strict preference . finally we propose a mild relaxation of our ordinality axiom , leaving room for a new family of qualitative decision rules compatible with transitivity .

network of bandits insure privacy of end-users
in order to distribute the best arm identification task as close as possible to the user 's devices , on the edge of the radio access network , we propose a new problem setting , where distributed players collaborate to find the best arm . this architecture guarantees privacy to end-users since no events are stored . the only thing that can be observed by an adversary through the core network is aggregated information across users . we provide a first algorithm , distributed median elimination , which is optimal in term of number of transmitted bits and near optimal in term of speed-up factor with respect to an optimal algorithm run independently on each player . in practice , this first algorithm can not handle the trade-off between the communication cost and the speed-up factor , and requires some knowledge about the distribution of players . extended distributed median elimination overcomes these limitations , by playing in parallel different instances of distributed median elimination and selecting the best one . experiments illustrate and complete the analysis . according to the analysis , in comparison to median elimination performed on each player , the proposed algorithm shows significant practical improvements .

artificial immune systems
the biological immune system is a robust , complex , adaptive system that defends the body from foreign pathogens . it is able to categorize all cells ( or molecules ) within the body as self-cells or non-self cells . it does this with the help of a distributed task force that has the intelligence to take action from a local and also a global perspective using its network of chemical messengers for communication . there are two major branches of the immune system . the innate immune system is an unchanging mechanism that detects and destroys certain invading organisms , whilst the adaptive immune system responds to previously unknown foreign cells and builds a response to them that can remain in the body over a long period of time . this remarkable information processing biological system has caught the attention of computer science in recent years . a novel computational intelligence technique , inspired by immunology , has emerged , called artificial immune systems . several concepts from the immune have been extracted and applied for solution to real world science and engineering problems . in this tutorial , we briefly describe the immune system metaphors that are relevant to existing artificial immune systems methods . we will then show illustrative real-world problems suitable for artificial immune systems and give a step-by-step algorithm walkthrough for one such problem . a comparison of the artificial immune systems to other well-known algorithms , areas for future work , tips & tricks and a list of resources will round this tutorial off . it should be noted that as artificial immune systems is still a young and evolving field , there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from time to time and from those examples given here .

use of python and phoenix-m interface in robotics
in this paper i will show how to use python programming with a computer interface such as phoenix-m 1 to drive simple robots . in my quest towards artificial intelligence ( ai ) i am experimenting with a lot of different possibilities in robotics . this one will try to mimic the working of a simple insect 's nervous system using hard wiring and some minimal software usage . this is the precursor to my advanced robotics and ai integration where i plan to use a new paradigm of ai based on machine learning and self consciousness via knowledge feedback and update process .

architecture of a fuzzy expert system used for dyslalic children therapy
in this paper we present architecture of a fuzzy expert system used for therapy of dyslalic children . with fuzzy approach we can create a better model for speech therapist decisions . a software interface was developed for validation of the system . the main objectives of this task are : personalized therapy ( the therapy must be in according with child 's problems level , context and possibilities ) , speech therapist assistant ( the expert system offer some suggestion regarding what exercises are better for a specific moment and from a specific child ) , ( self ) teaching ( when system 's conclusion is different that speech therapist 's conclusion the last one must have the knowledge base change possibility ) .

an online mechanism for ridesharing in autonomous mobility-on-demand systems
with proper management , autonomous mobility-on-demand ( amod ) systems have great potential to satisfy the transport demands of urban populations by providing safe , convenient , and affordable ridesharing services . meanwhile , such systems can substantially decrease private car ownership and use , and thus significantly reduce traffic congestion , energy consumption , and carbon emissions . to achieve this objective , an amod system requires private information about the demand from passengers . however , due to self-interestedness , passengers are unlikely to cooperate with the service providers in this regard . therefore , an online mechanism is desirable if it incentivizes passengers to truthfully report their actual demand . for the purpose of promoting ridesharing , we hereby introduce a posted-price , integrated online ridesharing mechanism ( iors ) that satisfies desirable properties such as ex-post incentive compatibility , individual rationality , and budget-balance . numerical results indicate the competitiveness of iors compared with two benchmarks , namely the optimal assignment and an offline , auction-based mechanism .

planning with pixels in ( almost ) real time
recently , width-based planning methods have been shown to yield state-of-the-art results in the atari 2600 video games . for this , the states were associated with the ( ram ) memory states of the simulator . in this work , we consider the same planning problem but using the screen instead . by using the same visual inputs , the planning results can be compared with those of humans and learning methods . we show that the planning approach , out of the box and without training , results in scores that compare well with those obtained by humans and learning methods , and moreover , by developing an episodic , rollout version of the iw ( k ) algorithm , we show that such scores can be obtained in almost real time .

set constraint model and automated encoding into sat : application to the social golfer problem
on the one hand , constraint satisfaction problems allow one to declaratively model problems . on the other hand , propositional satisfiability problem ( sat ) solvers can handle huge sat instances . we thus present a technique to declaratively model set constraint problems and to encode them automatically into sat instances . we apply our technique to the social golfer problem and we also use it to break symmetries of the problem . our technique is simpler , more declarative , and less error-prone than direct and improved hand modeling . the sat instances that we automatically generate contain less clauses than improved hand-written instances such as in [ 20 ] , and with unit propagation they also contain less variables . moreover , they are well-suited for sat solvers and they are solved faster as shown when solving difficult instances of the social golfer problem .

an optimal bayesian network based solution scheme for the constrained stochastic on-line equi-partitioning problem
a number of intriguing decision scenarios revolve around partitioning a collection of objects to optimize some application specific objective function . this problem is generally referred to as the object partitioning problem ( opp ) and is known to be np-hard . we here consider a particularly challenging version of opp , namely , the stochastic on-line equi-partitioning problem ( so-epp ) . in so-epp , the target partitioning is unknown and has to be inferred purely from observing an on-line sequence of object pairs . the paired objects belong to the same partition with probability $ p $ and to different partitions with probability $ 1-p $ , with $ p $ also being unknown . as an additional complication , the partitions are required to be of equal cardinality . previously , only sub-optimal solution strategies have been proposed for so- epp . in this paper , we propose the first optimal solution strategy . in brief , the scheme that we propose , bn-epp , is founded on a bayesian network representation of so-epp problems . based on probabilistic reasoning , we are not only able to infer the underlying object partitioning with optimal accuracy . we are also able to simultaneously infer $ p $ , allowing us to accelerate learning as object pairs arrive . furthermore , our scheme is the first to support arbitrary constraints on the partitioning ( constrained so-epp ) . being optimal , bn-epp provides superior performance compared to existing solution schemes . we additionally introduce walk-bn-epp , a novel walksat inspired algorithm for solving large scale bn-epp problems . finally , we provide a bn-epp based solution to the problem of order picking , a representative real-life application of bn-epp .

general problem solving with category theory
this paper proposes a formal cognitive framework for problem solving based on category theory . we introduce cognitive categories , which are categories with exactly one morphism between any two objects . objects in these categories are interpreted as states and morphisms as transformations between states . moreover , cognitive problems are reduced to the specification of two objects in a cognitive category : an outset ( i.e . the current state of the system ) and a goal ( i.e . the desired state ) . cognitive systems transform the target system by means of generators and evaluators . generators realize cognitive operations over a system by grouping morphisms , whilst evaluators group objects as a way to generalize outsets and goals to partially defined states . meta-cognition emerges when the whole cognitive system is self-referenced as sub-states in the cognitive category , whilst learning must always be considered as a meta-cognitive process to maintain consistency . several examples grounded in basic ai methods are provided as well .

regulating highly automated robot ecologies : insights from three user studies
highly automated robot ecologies ( hare ) , or societies of independent autonomous robots or agents , are rapidly becoming an important part of much of the world 's critical infrastructure . as with human societies , regulation , wherein a governing body designs rules and processes for the society , plays an important role in ensuring that hare meet societal objectives . however , to date , a careful study of interactions between a regulator and hare is lacking . in this paper , we report on three user studies which give insights into how to design systems that allow people , acting as the regulatory authority , to effectively interact with hare . as in the study of political systems in which governments regulate human societies , our studies analyze how interactions between hare and regulators are impacted by regulatory power and individual ( robot or agent ) autonomy . our results show that regulator power , decision support , and adaptive autonomy can each diminish the social welfare of hare , and hint at how these seemingly desirable mechanisms can be designed so that they become part of successful hare .

bargaining for revenue shares on tree trading networks
we study trade networks with a tree structure , where a seller with a single indivisible good is connected to buyers , each with some value for the good , via a unique path of intermediaries . agents in the tree make multiplicative revenue share offers to their parent nodes , who choose the best offer and offer part of it to their parent , and so on ; the winning path is determined by who finally makes the highest offer to the seller . in this paper , we investigate how these revenue shares might be set via a natural bargaining process between agents on the tree , specifically , egalitarian bargaining between endpoints of each edge in the tree . we investigate the fixed point of this system of bargaining equations and prove various desirable for this solution concept , including ( i ) existence , ( ii ) uniqueness , ( iii ) efficiency , ( iv ) membership in the core , ( v ) strict monotonicity , ( vi ) polynomial-time computability to any given accuracy . finally , we present numerical evidence that asynchronous dynamics with randomly ordered updates always converges to the fixed point , indicating that the fixed point shares might arise from decentralized bargaining amongst agents on the trade network .

self-supervised visual planning with temporal skip connections
in order to autonomously learn wide repertoires of complex skills , robots must be able to learn from their own autonomously collected data , without human supervision . one learning signal that is always available for autonomously collected data is prediction : if a robot can learn to predict the future , it can use this predictive model to take actions to produce desired outcomes , such as moving an object to a particular location . however , in complex open-world scenarios , designing a representation for prediction is difficult . in this work , we instead aim to enable self-supervised robotic learning through direct video prediction : instead of attempting to design a good representation , we directly predict what the robot will see next , and then use this model to achieve desired goals . a key challenge in video prediction for robotic manipulation is handling complex spatial arrangements such as occlusions . to that end , we introduce a video prediction model that can keep track of objects through occlusion by incorporating temporal skip-connections . together with a novel planning criterion and action space formulation , we demonstrate that this model substantially outperforms prior work on video prediction-based control . our results show manipulation of objects not seen during training , handling multiple objects , and pushing objects around obstructions . these results represent a significant advance in the range and complexity of skills that can be performed entirely with self-supervised robotic learning .

compatible extensions and consistent closures : a fuzzy approach
in this paper $ \ast $ -- compatible extensions of fuzzy relations are studied , generalizing some results obtained by duggan in case of crisp relations . from this general result are obtained as particular cases fuzzy versions of some important extension theorems for crisp relations ( szpilrajn , hansson , suzumura ) . two notions of consistent closure of a fuzzy relation are introduced .

asf+ -- - eine asf-aehnliche spezifikationssprache
maintaining the main aspects of the algebraic specification language asf as presented in [ bergstra & al.89 ] we have extend asf with the following concepts : while once exported names in asf must stay visible up to the top the module hierarchy , asf+ permits a more sophisticated hiding of signature names . the erroneous merging of distinct structures that occurs when importing different actualizations of the same parameterized module in asf is avoided in asf+ by a more adequate form of parameter binding . the new `` namensraum '' -concept of asf+ permits the specifier on the one hand directly to identify the origin of hidden names and on the other to decide whether an imported module is only to be accessed or whether an important property of it is to be modified . in the first case he can access one single globally provided version ; in the second he has to import a copy of the module . finally asf+ permits semantic conditions on parameters and the specification of tasks for a theorem prover .

declarative sequential pattern mining of care pathways
sequential pattern mining algorithms are widely used to explore care pathways database , but they generate a deluge of patterns , mostly redundant or useless . clinicians need tools to express complex mining queries in order to generate less but more significant patterns . these algorithms are not versatile enough to answer complex clinician queries . this article proposes to apply a declarative pattern mining approach based on answer set programming paradigm . it is exemplified by a pharmaco-epidemiological study investigating the possible association between hospitalization for seizure and antiepileptic drug switch from a french medico-administrative database .

learning equilibria with partial information in decentralized wireless networks
in this article , a survey of several important equilibrium concepts for decentralized networks is presented . the term decentralized is used here to refer to scenarios where decisions ( e.g. , choosing a power allocation policy ) are taken autonomously by devices interacting with each other ( e.g. , through mutual interference ) . the iterative long-term interaction is characterized by stable points of the wireless network called equilibria . the interest in these equilibria stems from the relevance of network stability and the fact that they can be achieved by letting radio devices to repeatedly interact over time . to achieve these equilibria , several learning techniques , namely , the best response dynamics , fictitious play , smoothed fictitious play , reinforcement learning algorithms , and regret matching , are discussed in terms of information requirements and convergence properties . most of the notions introduced here , for both equilibria and learning schemes , are illustrated by a simple case study , namely , an interference channel with two transmitter-receiver pairs .

cbinfer : change-based inference for convolutional neural networks on video data
extracting per-frame features using convolutional neural networks for real-time processing of video data is currently mainly performed on powerful gpu-accelerated workstations and compute clusters . however , there are many applications such as smart surveillance cameras that require or would benefit from on-site processing . to this end , we propose and evaluate a novel algorithm for change-based evaluation of cnns for video data recorded with a static camera setting , exploiting the spatio-temporal sparsity of pixel changes . we achieve an average speed-up of 8.6x over a cudnn baseline on a realistic benchmark with a negligible accuracy loss of less than 0.1 % and no retraining of the network . the resulting energy efficiency is 10x higher than that of per-frame evaluation and reaches an equivalent of 328 gop/s/w on the tegra x1 platform .

sequence-based multimodal apprenticeship learning for robot perception and decision making
apprenticeship learning has recently attracted a wide attention due to its capability of allowing robots to learn physical tasks directly from demonstrations provided by human experts . most previous techniques assumed that the state space is known a priori or employed simple state representations that usually suffer from perceptual aliasing . different from previous research , we propose a novel approach named sequence-based multimodal apprenticeship learning ( smal ) , which is capable to simultaneously fusing temporal information and multimodal data , and to integrate robot perception with decision making . to evaluate the smal approach , experiments are performed using both simulations and real-world robots in the challenging search and rescue scenarios . the empirical study has validated that our smal approach can effectively learn plans for robots to make decisions using sequence of multimodal observations . experimental results have also showed that smal outperforms the baseline methods using individual images .

a transformational characterization of markov equivalence for directed acyclic graphs with latent variables
different directed acyclic graphs ( dags ) may be markov equivalent in the sense that they entail the same conditional independence relations among the observed variables . chickering ( 1995 ) provided a transformational characterization of markov equivalence for dags ( with no latent variables ) , which is useful in deriving properties shared by markov equivalent dags , and , with certain generalization , is needed to prove the asymptotic correctness of a search procedure over markov equivalence classes , known as the ges algorithm . for dag models with latent variables , maximal ancestral graphs ( mags ) provide a neat representation that facilitates model search . however , no transformational characterization -- analogous to chickering 's -- of markov equivalent mags is yet available . this paper establishes such a characterization for directed mags , which we expect will have similar uses as it does for dags .

deep spatio-temporal residual networks for citywide crowd flows prediction
forecasting the flow of crowds is of great importance to traffic management and public safety , yet a very challenging task affected by many complex factors , such as inter-region traffic , events and weather . in this paper , we propose a deep-learning-based approach , called st-resnet , to collectively forecast the in-flow and out-flow of crowds in each and every region through a city . we design an end-to-end structure of st-resnet based on unique properties of spatio-temporal data . more specifically , we employ the framework of the residual neural networks to model the temporal closeness , period , and trend properties of the crowd traffic , respectively . for each property , we design a branch of residual convolutional units , each of which models the spatial properties of the crowd traffic . st-resnet learns to dynamically aggregate the output of the three residual neural networks based on data , assigning different weights to different branches and regions . the aggregation is further combined with external factors , such as weather and day of the week , to predict the final traffic of crowds in each and every region . we evaluate st-resnet based on two types of crowd flows in beijing and nyc , finding that its performance exceeds six well-know methods .

a spacetime approach to generalized cognitive reasoning in multi-scale learning
in modern machine learning , pattern recognition replaces realtime semantic reasoning . the mapping from input to output is learned with fixed semantics by training outcomes deliberately . this is an expensive and static approach which depends heavily on the availability of a very particular kind of prior raining data to make inferences in a single step . conventional semantic network approaches , on the other hand , base multi-step reasoning on modal logics and handcrafted ontologies , which are ad hoc , expensive to construct , and fragile to inconsistency . both approaches may be enhanced by a hybrid approach , which completely separates reasoning from pattern recognition . in this report , a quasi-linguistic approach to knowledge representation is discussed , motivated by spacetime structure . tokenized patterns from diverse sources are integrated to build a lightly constrained and approximately scale-free network . this is then be parsed with very simple recursive algorithms to generate ` brainstorming ' sets of reasoned knowledge .

qualitative shape representation based on the qualitative relative direction and distance calculus eopram
this document serves as a brief technical report , detailing the processes used to represent and reconstruct simplified polygons using qualitative spatial descriptions , as defined by the eopram qualitative spatial calculus .

boolean kernels for collaborative filtering in top-n item recommendation
in many personalized recommendation problems available data consists only of positive interactions ( implicit feedback ) between users and items . this problem is also known as one-class collaborative filtering ( oc-cf ) . linear models usually achieve state-of-the-art performances on oc-cf problems and many efforts have been devoted to build more expressive and complex representations able to improve the recommendations . recent analysis show that collaborative filtering ( cf ) datasets have peculiar characteristics such as high sparsity and a long tailed distribution of the ratings . in this paper we propose a boolean kernel , called disjunctive kernel , which is less expressive than the linear one but it is able to alleviate the sparsity issue in cf contexts . the embedding of this kernel is composed by all the combinations of a certain arity d of the input variables , and these combined features are semantically interpreted as disjunctions of the input variables . experiments on several cf datasets show the effectiveness and the efficiency of the proposed kernel .

relative expressiveness of defeasible logics
we address the relative expressiveness of defeasible logics in the framework dl . relative expressiveness is formulated as the ability to simulate the reasoning of one logic within another logic . we show that such simulations must be modular , in the sense that they also work if applied only to part of a theory , in order to achieve a useful notion of relative expressiveness . we present simulations showing that logics in dl with and without the capability of team defeat are equally expressive . we also show that logics that handle ambiguity differently -- ambiguity blocking versus ambiguity propagating -- have distinct expressiveness , with neither able to simulate the other under a different formulation of expressiveness .

about updating
survey of several forms of updating , with a practical illustrative example . we study several updating ( conditioning ) schemes that emerge naturally from a common scenarion to provide some insights into their meaning . updating is a subtle operation and there is no single method , no single 'good ' rule . the choice of the appropriate rule must always be given due consideration . planchet ( 1989 ) presents a mathematical survey of many rules . we focus on the practical meaning of these rules . after summarizing the several rules for conditioning , we present an illustrative example in which the various forms of conditioning can be explained .

sensitivities : an alternative to conditional probabilities for bayesian belief networks
we show an alternative way of representing a bayesian belief network by sensitivities and probability distributions . this representation is equivalent to the traditional representation by conditional probabilities , but makes dependencies between nodes apparent and intuitively easy to understand . we also propose a qr matrix representation for the sensitivities and/or conditional probabilities which is more efficient , in both memory requirements and computational speed , than the traditional representation for computer-based implementations of probabilistic inference . we use sensitivities to show that for a certain class of binary networks , the computation time for approximate probabilistic inference with any positive upper bound on the error of the result is independent of the size of the network . finally , as an alternative to traditional algorithms that use conditional probabilities , we describe an exact algorithm for probabilistic inference that uses the qr-representation for sensitivities and updates probability distributions of nodes in a network according to messages from the neighbors .

the largest compatible subset problem for phylogenetic data
the phylogenetic tree construction is to infer the evolutionary relationship between species from the experimental data . however , the experimental data are often imperfect and conflicting each others . therefore , it is important to extract the motif from the imperfect data . the largest compatible subset problem is that , given a set of experimental data , we want to discard the minimum such that the remaining is compatible . the largest compatible subset problem can be viewed as the vertex cover problem in the graph theory that has been proven to be np-hard . in this paper , we propose a hybrid evolutionary computing ( ec ) method for this problem . the proposed method combines the ec approach and the algorithmic approach for special structured graphs . as a result , the complexity of the problem is dramatically reduced . experiments were performed on randomly generated graphs with different edge densities . the vertex covers produced by the proposed method were then compared to the vertex covers produced by a 2-approximation algorithm . the experimental results showed that the proposed method consistently outperformed a classical 2- approximation algorithm . furthermore , a significant improvement was found when the graph density was small .

what is learning ? a primary discussion about information and representation
nowadays , represented by deep learning techniques , the field of machine learning is experiencing unprecedented prosperity and its influence is demonstrated in academia , industry and civil society . `` intelligent '' has become a label which could not be neglected for most applications ; celebrities and scientists also warned that the development of full artificial intelligence may spell the end of the human race . it seems that the answer to building a computer system that could automatically improve with experience is right on the next corner . while for ai and machine learning researchers , it is a consensus that we are not anywhere near the core technique which could bring the terminator , number 5 or r2d2 into real life , and there is not even a formal definition about what is intelligence , or one of its basic properties : learning . therefore , even though researchers know these concerns are not necessary currently , there is no generalized explanation about why these concerns are not necessary , and what properties people should take into account that would make these concerns to be necessary . in this paper , starts from analysing the relation between information and its representation , a necessary condition for a model to be a learning model is proposed . this condition and related future works could be used to verify whether a system is able to learn or not , and enrich our understanding of learning : one important property of intelligence .

data acquisition and database management system for samsung superconductor test facility
in order to fulfill the test requirement of kstar ( korea superconducting tokamak advanced research ) superconducting magnet system , a large scale superconducting magnet and conductor test facility , sstf ( samsung superconductor test facility ) , has been constructed at samsung advanced institute of technology . the computer system for sstf dac ( data acquisition and control ) is based on unix system and vxworks is used for the real-time os of the vme system . epics ( experimental physics and industrial control system ) is used for the communication between ioc server and client . a database program has been developed for the efficient management of measured data and a linux workstation with pentium-4 cpu is used for the database server . in this paper , the current status of sstf dac system , the database management system and recent test results are presented .

linking search space structure , run-time dynamics , and problem difficulty : a step toward demystifying tabu search
tabu search is one of the most effective heuristics for locating high-quality solutions to a diverse array of np-hard combinatorial optimization problems . despite the widespread success of tabu search , researchers have a poor understanding of many key theoretical aspects of this algorithm , including models of the high-level run-time dynamics and identification of those search space features that influence problem difficulty . we consider these questions in the context of the job-shop scheduling problem ( jsp ) , a domain where tabu search algorithms have been shown to be remarkably effective . previously , we demonstrated that the mean distance between random local optima and the nearest optimal solution is highly correlated with problem difficulty for a well-known tabu search algorithm for the jsp introduced by taillard . in this paper , we discuss various shortcomings of this measure and develop a new model of problem difficulty that corrects these deficiencies . we show that taillards algorithm can be modeled with high fidelity as a simple variant of a straightforward random walk . the random walk model accounts for nearly all of the variability in the cost required to locate both optimal and sub-optimal solutions to random jsps , and provides an explanation for differences in the difficulty of random versus structured jsps . finally , we discuss and empirically substantiate two novel predictions regarding tabu search algorithm behavior . first , the method for constructing the initial solution is highly unlikely to impact the performance of tabu search . second , tabu tenure should be selected to be as small as possible while simultaneously avoiding search stagnation ; values larger than necessary lead to significant degradations in performance .

content-based text categorization using wikitology
a major computational burden , while performing document clustering , is the calculation of similarity measure between a pair of documents . similarity measure is a function that assign a real number between 0 and 1 to a pair of documents , depending upon the degree of similarity between them . a value of zero means that the documents are completely dissimilar whereas a value of one indicates that the documents are practically identical . traditionally , vector-based models have been used for computing the document similarity . the vector-based models represent several features present in documents . these approaches to similarity measures , in general , can not account for the semantics of the document . documents written in human languages contain contexts and the words used to describe these contexts are generally semantically related . motivated by this fact , many researchers have proposed semantic-based similarity measures by utilizing text annotation through external thesauruses like wordnet ( a lexical database ) . in this paper , we define a semantic similarity measure based on documents represented in topic maps . topic maps are rapidly becoming an industrial standard for knowledge representation with a focus for later search and extraction . the documents are transformed into a topic map based coded knowledge and the similarity between a pair of documents is represented as a correlation between the common patterns . the experimental studies on the text mining datasets reveal that this new similarity measure is more effective as compared to commonly used similarity measures in text clustering .

rapid feature learning with stacked linear denoisers
we investigate unsupervised pre-training of deep architectures as feature generators for `` shallow '' classifiers . stacked denoising autoencoders ( sda ) , when used as feature pre-processing tools for svm classification , can lead to significant improvements in accuracy - however , at the price of a substantial increase in computational cost . in this paper we create a simple algorithm which mimics the layer by layer training of sdas . however , in contrast to sdas , our algorithm requires no training through gradient descent as the parameters can be computed in closed-form . it can be implemented in less than 20 lines of matlabtmand reduces the computation time from several hours to mere seconds . we show that our feature transformation reliably improves the results of svm classification significantly on all our data sets - often outperforming sdas and even deep neural networks in three out of four deep learning benchmarks .

deep neural networks under stress
in recent years , deep architectures have been used for transfer learning with state-of-the-art performance in many datasets . the properties of their features remain , however , largely unstudied under the transfer perspective . in this work , we present an extensive analysis of the resiliency of feature vectors extracted from deep models , with special focus on the trade-off between performance and compression rate . by introducing perturbations to image descriptions extracted from a deep convolutional neural network , we change their precision and number of dimensions , measuring how it affects the final score . we show that deep features are more robust to these disturbances when compared to classical approaches , achieving a compression rate of 98.4 % , while losing only 0.88 % of their original score for pascal voc 2007 .

networked intelligence : towards autonomous cyber physical systems
developing intelligent systems requires combining results from both industry and academia . in this report you find an overview of relevant research fields and industrially applicable technologies for building very large scale cyber physical systems . a concept architecture is used to illustrate how existing pieces may fit together , and the maturity of the subsystems is estimated . the goal is to structure the developments and the challenge of machine intelligence for consumer and industrial internet technologists , cyber physical systems researchers and people interested in the convergence of data & internet of things . it can be used for planning developments of intelligent systems .

planning , scheduling , and uncertainty in the sequence of future events
scheduling in the factory setting is compounded by computational complexity and temporal uncertainty . together , these two factors guarantee that the process of constructing an optimal schedule will be costly and the chances of executing that schedule will be slight . temporal uncertainty in the task execution time can be offset by several methods : eliminate uncertainty by careful engineering , restore certainty whenever it is lost , reduce the uncertainty by using more accurate sensors , and quantify and circumscribe the remaining uncertainty . unfortunately , these methods focus exclusively on the sources of uncertainty and fail to apply knowledge of the tasks which are to be scheduled . a complete solution must adapt the schedule of activities to be performed according to the evolving state of the production world . the example of vision-directed assembly is presented to illustrate that the principle of least commitment , in the creation of a plan , in the representation of a schedule , and in the execution of a schedule , enables a robot to operate intelligently and efficiently , even in the presence of considerable uncertainty in the sequence of future events .

vain : attentional multi-agent predictive modeling
multi-agent predictive modeling is an essential step for understanding physical , social and team-play systems . recently , interaction networks ( ins ) were proposed for the task of modeling multi-agent physical systems , ins scale with the number of interactions in the system ( typically quadratic or higher order in the number of agents ) . in this paper we introduce vain , a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents . we show that vain is effective for multi-agent predictive modeling . our method is evaluated on tasks from challenging multi-agent prediction domains : chess and soccer , and outperforms competing multi-agent approaches .

towards a general framework for an observation and knowledge based model of occupant behaviour in office buildings
this paper proposes a new general approach based on bayesian networks to model the human behaviour . this approach represents human behaviour withprobabilistic cause-effect relations based not only on previous works , but also with conditional probabilities coming either from expert knowledge or deduced from observations . the approach has been used in the co-simulation of building physics and human behaviour in order to assess the co 2 concentration in an office .

on the ontological modeling of trees
trees -- i.e. , the type of data structure known under this name -- are central to many aspects of knowledge organization . we investigate some central design choices concerning the ontological modeling of such trees . in particular , we consider the limits of what is expressible in the web ontology language , and provide a reusable ontology design pattern for trees .

machine comprehension using match-lstm and answer pointer
machine comprehension of text is an important problem in natural language processing . a recently released dataset , the stanford question answering dataset ( squad ) , offers a large number of real questions and their answers created by humans through crowdsourcing . squad provides a challenging testbed for evaluating machine comprehension algorithms , partly because compared with previous datasets , in squad the answers do not come from a small set of candidate answers and they have variable lengths . we propose an end-to-end neural architecture for the task . the architecture is based on match-lstm , a model we proposed previously for textual entailment , and pointer net , a sequence-to-sequence model proposed by vinyals et al . ( 2015 ) to constrain the output tokens to be from the input sequences . we propose two ways of using pointer net for our task . our experiments show that both of our two models substantially outperform the best results obtained by rajpurkar et al . ( 2016 ) using logistic regression and manually crafted features .

exploring implicit human responses to robot mistakes in a learning from demonstration task
as robots enter human environments , they will be expected to accomplish a tremendous range of tasks . it is not feasible for robot designers to pre-program these behaviors or know them in advance , so one way to address this is through end-user programming , such as via learning from demonstration ( lfd ) . while significant work has been done on the mechanics of enabling robot learning from human teachers , one unexplored aspect is enabling mutual feedback between both the human teacher and robot during the learning process , i.e. , implicit learning . in this paper , we explore one aspect of this mutual understanding , grounding sequences , where both a human and robot provide non-verbal feedback to signify their mutual understanding during interaction . we conducted a study where people taught an autonomous humanoid robot a dance , and performed gesture analysis to measure people 's responses to the robot during correct and incorrect demonstrations .

building end-to-end dialogue systems using generative hierarchical neural network models
we investigate the task of building open domain , conversational dialogue systems based on large dialogue corpora using generative models . generative models produce system responses that are autonomously generated word-by-word , opening up the possibility for realistic , flexible interactions . in support of this goal , we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain , and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models . we investigate the limitations of this and similar approaches , and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings .

surrogate scoring rules and a dominant truth serum for information elicitation
we study information elicitation without verification ( iewv ) and ask the following question : can we achieve truthfulness in dominant strategy in iewv ? this paper considers two elicitation settings . the first setting is when the mechanism designer has access to a random variable that is a noisy or proxy version of the ground truth , with known biases . the second setting is the standard peer prediction setting where agents ' reports are the only source of information that the mechanism designer has . we introduce surrogate scoring rules ( ssr ) for the first setting , which use the noisy ground truth to evaluate quality of elicited information , and show that ssr achieve truthful elicitation in dominant strategy . built upon ssr , we develop a multi-task mechanism , dominant truth serum ( dts ) , to achieve truthful elicitation in dominant strategy when the mechanism designer only has access to agents ' reports ( the second setting ) . the method relies on an estimation procedure to accurately estimate the average bias in the reports of other agents . with the accurate estimation , a random peer agent 's report serves as a noisy ground truth and ssr can then be applied to achieve truthfulness in dominant strategy . a salient feature of ssr and dts is that they both quantify the quality or value of information despite lack of ground truth , just as proper scoring rules do for the with verification setting . our work complements both the strictly proper scoring rule literature by solving the case where the mechanism designer only has access to a noisy or proxy version of the ground truth , and the peer prediction literature by achieving truthful elicitation in dominant strategy .

proceedings of the eleventh conference on uncertainty in artificial intelligence ( 1995 )
this is the proceedings of the eleventh conference on uncertainty in artificial intelligence , which was held in montreal , qu , august 18-20 , 1995

conceptual modelling and the quality of ontologies : endurantism vs. perdurantism
ontologies are key enablers for sharing precise and machine-understandable semantics among different applications and parties . yet , for ontologies to meet these expectations , their quality must be of a good standard . the quality of an ontology is strongly based on the design method employed . this paper addresses the design problems related to the modelling of ontologies , with specific concentration on the issues related to the quality of the conceptualisations produced . the paper aims to demonstrate the impact of the modelling paradigm adopted on the quality of ontological models and , consequently , the potential impact that such a decision can have in relation to the development of software applications . to this aim , an ontology that is conceptualised based on the object-role modelling ( orm ) approach ( a representative of endurantism ) is re-engineered into a one modelled on the basis of the object paradigm ( op ) ( a representative of perdurantism ) . next , the two ontologies are analytically compared using the specified criteria . the conducted comparison highlights that using the op for ontology conceptualisation can provide more expressive , reusable , objective and temporal ontologies than those conceptualised on the basis of the orm approach .

expectation particle belief propagation
we propose an original particle-based implementation of the loopy belief propagation ( lpb ) algorithm for pairwise markov random fields ( mrf ) on a continuous state space . the algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the mrf . this is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an expectation propagation ( ep ) framework . the proposed particle scheme provides consistent estimation of the lbp marginals as the number of particles increases . we demonstrate that it provides more accurate results than the particle belief propagation ( pbp ) algorithm of ihler and mcallester ( 2009 ) at a fraction of the computational cost and is additionally more robust empirically . the computational complexity of our algorithm at each iteration is quadratic in the number of particles . we also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy bp marginal distributions and performs almost as well as the original procedure .

learning from complex systems : on the roles of entropy and fisher information in pairwise isotropic gaussian markov random fields
markov random field models are powerful tools for the study of complex systems . however , little is known about how the interactions between the elements of such systems are encoded , especially from an information-theoretic perspective . in this paper , our goal is to enlight the connection between fisher information , shannon entropy , information geometry and the behavior of complex systems modeled by isotropic pairwise gaussian markov random fields . we propose analytical expressions to compute local and global versions of these measures using besag 's pseudo-likelihood function , characterizing the system 's behavior through its \emph { fisher curve } , a parametric trajectory accross the information space that provides a geometric representation for the study of complex systems . computational experiments show how the proposed tools can be useful in extrating relevant information from complex patterns . the obtained results quantify and support our main conclusion , which is : in terms of information , moving towards higher entropy states ( a -- > b ) is different from moving towards lower entropy states ( b -- > a ) , since the \emph { fisher curves } are not the same given a natural orientation ( the direction of time ) .

regularizing face verification nets for pain intensity regression
limited labeled data are available for the research of estimating facial expression intensities . for instance , the ability to train deep networks for automated pain assessment is limited by small datasets with labels of patient-reported pain intensities . fortunately , fine-tuning from a data-extensive pre-trained domain , such as face verification , can alleviate this problem . in this paper , we propose a network that fine-tunes a state-of-the-art face verification network using a regularized regression loss and additional data with expression labels . in this way , the expression intensity regression task can benefit from the rich feature representations trained on a huge amount of data for face verification . the proposed regularized deep regressor is applied to estimate the pain expression intensity and verified on the widely-used unbc-mcmaster shoulder-pain dataset , achieving the state-of-the-art performance . a weighted evaluation metric is also proposed to address the imbalance issue of different pain intensities .

a comparative study of arithmetic constraints on integer intervals
we propose here a number of approaches to implement constraint propagation for arithmetic constraints on integer intervals . to this end we introduce integer interval arithmetic . each approach is explained using appropriate proof rules that reduce the variable domains . we compare these approaches using a set of benchmarks .

multi-objective approaches to markov decision processes with uncertain transition parameters
markov decision processes ( mdps ) are a popular model for performance analysis and optimization of stochastic systems . the parameters of stochastic behavior of mdps are estimates from empirical observations of a system ; their values are not known precisely . different types of mdps with uncertain , imprecise or bounded transition rates or probabilities and rewards exist in the literature . commonly , analysis of models with uncertainties amounts to searching for the most robust policy which means that the goal is to generate a policy with the greatest lower bound on performance ( or , symmetrically , the lowest upper bound on costs ) . however , hedging against an unlikely worst case may lead to losses in other situations . in general , one is interested in policies that behave well in all situations which results in a multi-objective view on decision making . in this paper , we consider policies for the expected discounted reward measure of mdps with uncertain parameters . in particular , the approach is defined for bounded-parameter mdps ( bmdps ) [ 8 ] . in this setting the worst , best and average case performances of a policy are analyzed simultaneously , which yields a multi-scenario multi-objective optimization problem . the paper presents and evaluates approaches to compute the pure pareto optimal policies in the value vector space .

towards end-to-end learning for dialog state tracking and management using deep reinforcement learning
this paper presents an end-to-end framework for task-oriented dialog systems using a variant of deep recurrent q-networks ( drqn ) . the model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy . moreover , we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed . we evaluated the proposed model on a 20 question game conversational game simulator . results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state .

a novel model of working set selection for smo decomposition methods
in the process of training support vector machines ( svms ) by decomposition methods , working set selection is an important technique , and some exciting schemes were employed into this field . to improve working set selection , we propose a new model for working set selection in sequential minimal optimization ( smo ) decomposition methods . in this model , it selects b as working set without reselection . some properties are given by simple proof , and experiments demonstrate that the proposed method is in general faster than existing methods .

estimation of passenger route choice pattern using smart card data for complex metro systems
nowadays , metro systems play an important role in meeting the urban transportation demand in large cities . the understanding of passenger route choice is critical for public transit management . the wide deployment of automated fare collection ( afc ) systems opens up a new opportunity . however , only each trip 's tap-in and tap-out timestamp and stations can be directly obtained from afc system records ; the train and route chosen by a passenger are unknown , which are necessary to solve our problem . while existing methods work well in some specific situations , they do n't work for complicated situations . in this paper , we propose a solution that needs no additional equipment or human involvement than the afc systems . we develop a probabilistic model that can estimate from empirical analysis how the passenger flows are dispatched to different routes and trains . we validate our approach using a large scale data set collected from the shenzhen metro system . the measured results provide us with useful inputs when building the passenger path choice model .

why is compiling lifted inference into a low-level language so effective ?
first-order knowledge compilation techniques have proven efficient for lifted inference . they compile a relational probability model into a target circuit on which many inference queries can be answered efficiently . early methods used data structures as their target circuit . in our kr-2016 paper , we showed that compiling to a low-level program instead of a data structure offers orders of magnitude speedup , resulting in the state-of-the-art lifted inference technique . in this paper , we conduct experiments to address two questions regarding our kr-2016 results : 1- does the speedup come from more efficient compilation or more efficient reasoning with the target circuit ? , and 2- why are low-level programs more efficient target circuits than data structures ?

map estimation , linear programming and belief propagation with convex free energies
finding the most probable assignment ( map ) in a general graphical model is known to be np hard but good approximations have been attained with max-product belief propagation ( bp ) and its variants . in particular , it is known that using bp on a single-cycle graph or tree reweighted bp on an arbitrary graph will give the map solution if the beliefs have no ties . in this paper we extend the setting under which bp can be used to provably extract the map . we define convex bp as bp algorithms based on a convex free energy approximation and show that this class includes ordinary bp with single-cycle , tree reweighted bp and many other bp variants . we show that when there are no ties , fixed-points of convex max-product bp will provably give the map solution . we also show that convex sum-product bp at sufficiently small temperatures can be used to solve linear programs that arise from relaxing the map problem . finally , we derive a novel condition that allows us to derive the map solution even if some of the convex bp beliefs have ties . in experiments , we show that our theorems allow us to find the map in many real-world instances of graphical models where exact inference using junction-tree is impossible .

logic programming with ordered disjunction
logic programs with ordered disjunction ( lpods ) combine ideas underlying qualitative choice logic ( brewka et al . kr 2002 ) and answer set programming . logic programming under answer set semantics is extended with a new connective called ordered disjunction . the new connective allows us to represent alternative , ranked options for problem solutions in the heads of rules : a \times b intuitively means : if possible a , but if a is not possible then at least b. the semantics of logic programs with ordered disjunction is based on a preference relation on answer sets . lpods are useful for applications in design and configuration and can serve as a basis for qualitative decision making .

embodied question answering
we present a new ai task -- embodied question answering ( embodiedqa ) -- where an agent is spawned at a random location in a 3d environment and asked a question ( `` what color is the car ? '' ) . in order to answer , the agent must first intelligently navigate to explore the environment , gather information through first-person ( egocentric ) vision , and then answer the question ( `` orange '' ) . this challenging task requires a range of ai skills -- active perception , language understanding , goal-driven navigation , commonsense reasoning , and grounding of language into actions . in this work , we develop the environments , end-to-end-trained reinforcement learning agents , and evaluation protocols for embodiedqa .

a comparison between decision trees and decision tree forest models for software development effort estimation
accurate software effort estimation has been a challenge for many software practitioners and project managers . underestimation leads to disruption in the projects estimated cost and delivery . on the other hand , overestimation causes outbidding and financial losses in business . many software estimation models exist ; however , none have been proven to be the best in all situations . in this paper , a decision tree forest ( dtf ) model is compared to a traditional decision tree ( dt ) model , as well as a multiple linear regression model ( mlr ) . the evaluation was conducted using isbsg and desharnais industrial datasets . results show that the dtf model is competitive and can be used as an alternative in software effort prediction .

towards `` propagation = logic + control ''
constraint propagation algorithms implement logical inference . for efficiency , it is essential to control whether and in what order basic inference steps are taken . we provide a high-level framework that clearly differentiates between information needed for controlling propagation versus that needed for the logical semantics of complex constraints composed from primitive ones . we argue for the appropriateness of our controlled propagation framework by showing that it captures the underlying principles of manually designed propagation algorithms , such as literal watching for unit clause propagation and the lexicographic ordering constraint . we provide an implementation and benchmark results that demonstrate the practicality and efficiency of our framework .

reducing uncertainty in navigation and exploration
a significant problem in designing mobile robot control systems involves coping with the uncertainty that arises in moving about in an unknown or partially unknown environment and relying on noisy or ambiguous sensor data to acquire knowledge about that environment . we describe a control system that chooses what activity to engage in next on the basis of expectations about how the information re- turned as a result of a given activity will improve 2 its knowledge about the spatial layout of its environment . certain of the higher-level components of the control system are specified in terms of probabilistic decision models whose output is used to mediate the behavior of lower-level control components responsible for movement and sensing .

inequality constraints in causal models with hidden variables
we present a class of inequality constraints on the set of distributions induced by local interventions on variables governed by a causal bayesian network , in which some of the variables remain unmeasured . we derive bounds on causal effects that are not directly measured in randomized experiments . we derive instrumental inequality type of constraints on nonexperimental distributions . the results have applications in testing causal models with observational or experimental data .

towards grounding conceptual spaces in neural representations
the highly influential framework of conceptual spaces provides a geometric way of representing knowledge . it aims at bridging the gap between symbolic and subsymbolic processing . instances are represented by points in a high-dimensional space and concepts are represented by convex regions in this space . in this paper , we present our approach towards grounding the dimensions of a conceptual space in latent spaces learned by an infogan from unlabeled data .

multiscale probability transformation of basic probability assignment
decision making is still an open issue in the application of dempster-shafer evidence theory . a lot of works have been presented for it . in the transferable belief model ( tbm ) , pignistic probabilities based on the basic probability as- signments are used for decision making . in this paper , multiscale probability transformation of basic probability assignment based on the belief function and the plausibility function is proposed , which is a generalization of the pignistic probability transformation . in the multiscale probability function , a factor q based on the tsallis entropy is used to make the multiscale prob- abilities diversified . an example is shown that the multiscale probability transformation is more reasonable in the decision making .

a k-fold method for baseline estimation in policy gradient algorithms
the high variance issue in unbiased policy-gradient methods such as vpg and reinforce is typically mitigated by adding a baseline . however , the baseline fitting itself suffers from the underfitting or the overfitting problem . in this paper , we develop a k-fold method for baseline estimation in policy gradient algorithms . the parameter k is the baseline estimation hyperparameter that can adjust the bias-variance trade-off in the baseline estimates . we demonstrate the usefulness of our approach via two state-of-the-art policy gradient algorithms on three mujoco locomotive control tasks .

automated assignment of backbone nmr data using artificial intelligence
nuclear magnetic resonance ( nmr ) spectroscopy is a powerful method for the investigation of three-dimensional structures of biological molecules such as proteins . determining a protein structure is essential for understanding its function and alterations in function which lead to disease . one of the major challenges of the post-genomic era is to obtain structural and functional information on the many unknown proteins encoded by thousands of newly identified genes . the goal of this research is to design an algorithm capable of automating the analysis of backbone protein nmr data by implementing ai strategies such as greedy and a* search .

darknet and deepnet mining for proactive cybersecurity threat intelligence
in this paper , we present an operational system for cyber threat intelligence gathering from various social platforms on the internet particularly sites on the darknet and deepnet . we focus our attention to collecting information from hacker forum discussions and marketplaces offering products and services focusing on malicious hacking . we have developed an operational system for obtaining information from these sites for the purposes of identifying emerging cyber threats . currently , this system collects on average 305 high-quality cyber threat warnings each week . these threat warnings include information on newly developed malware and exploits that have not yet been deployed in a cyber-attack . this provides a significant service to cyber-defenders . the system is significantly augmented through the use of various data mining and machine learning techniques . with the use of machine learning models , we are able to recall 92 % of products in marketplaces and 80 % of discussions on forums relating to malicious hacking with high precision . we perform preliminary analysis on the data collected , demonstrating its application to aid a security expert for better threat analysis .

solver scheduling via answer set programming
although boolean constraint technology has made tremendous progress over the last decade , the efficacy of state-of-the-art solvers is known to vary considerably across different types of problem instances and is known to depend strongly on algorithm parameters . this problem was addressed by means of a simple , yet effective approach using handmade , uniform and unordered schedules of multiple solvers in ppfolio , which showed very impressive performance in the 2011 sat competition . inspired by this , we take advantage of the modeling and solving capacities of answer set programming ( asp ) to automatically determine more refined , that is , non-uniform and ordered solver schedules from existing benchmarking data . we begin by formulating the determination of such schedules as multi-criteria optimization problems and provide corresponding asp encodings . the resulting encodings are easily customizable for different settings and the computation of optimum schedules can mostly be done in the blink of an eye , even when dealing with large runtime data sets stemming from many solvers on hundreds to thousands of instances . also , the fact that our approach can be customized easily enabled us to swiftly adapt it to generate parallel schedules for multi-processor machines .

value based argumentation frameworks
this paper introduces the notion of value-based argumentation frameworks , an extension of the standard argumentation frameworks proposed by dung , which are able toshow how rational decision is possible in cases where arguments derive their force from the social values their acceptance would promote .

geometric enclosing networks
training model to generate data has increasingly attracted research attention and become important in modern world applications . we propose in this paper a new geometry-based optimization approach to address this problem . orthogonal to current state-of-the-art density-based approaches , most notably vae and gan , we present a fresh new idea that borrows the principle of minimal enclosing ball to train a generator g\left ( \bz\right ) in such a way that both training and generated data , after being mapped to the feature space , are enclosed in the same sphere . we develop theory to guarantee that the mapping is bijective so that its inverse from feature space to data space results in expressive nonlinear contours to describe the data manifold , hence ensuring data generated are also lying on the data manifold learned from training data . our model enjoys a nice geometric interpretation , hence termed geometric enclosing networks ( gen ) , and possesses some key advantages over its rivals , namely simple and easy-to-control optimization formulation , avoidance of mode collapsing and efficiently learn data manifold representation in a completely unsupervised manner . we conducted extensive experiments on synthesis and real-world datasets to illustrate the behaviors , strength and weakness of our proposed gen , in particular its ability to handle multi-modal data and quality of generated data .

distilling a neural network into a soft decision tree
deep neural networks have proved to be a very effective way to perform classification tasks . they excel when the input data is high dimensional , the relationship between the input and the output is complicated , and the number of labeled training examples is large . but it is hard to explain why a learned network makes a particular classification decision on a particular test case . this is due to their reliance on distributed hierarchical representations . if we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead , explaining a particular decision would be much easier . we describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data .

measuring cultural relativity of emotional valence and arousal using semantic clustering and twitter
researchers since at least darwin have debated whether and to what extent emotions are universal or culture-dependent . however , previous studies have primarily focused on facial expressions and on a limited set of emotions . given that emotions have a substantial impact on human lives , evidence for cultural emotional relativity might be derived by applying distributional semantics techniques to a text corpus of self-reported behaviour . here , we explore this idea by measuring the valence and arousal of the twelve most popular emotion keywords expressed on the micro-blogging site twitter . we do this in three geographical regions : europe , asia and north america . we demonstrate that in our sample , the valence and arousal levels of the same emotion keywords differ significantly with respect to these geographical regions -- - europeans are , or at least present themselves as more positive and aroused , north americans are more negative and asians appear to be more positive but less aroused when compared to global valence and arousal levels of the same emotion keywords . our work is the first in kind to programatically map large text corpora to a dimensional model of affect .

the dlvhex system for knowledge representation : recent advances ( system description )
the dlvhex system implements the hex-semantics , which integrates answer set programming ( asp ) with arbitrary external sources . since its first release ten years ago , significant advancements were achieved . most importantly , the exploitation of properties of external sources led to efficiency improvements and flexibility enhancements of the language , and technical improvements on the system side increased user 's convenience . in this paper , we present the current status of the system and point out the most important recent enhancements over early versions . while existing literature focuses on theoretical aspects and specific components , a bird 's eye view of the overall system is missing . in order to promote the system for real-world applications , we further present applications which were already successfully realized on top of dlvhex . this paper is under consideration for acceptance in theory and practice of logic programming .

lifted inference for relational continuous models
relational continuous models ( rcms ) represent joint probability densities over attributes of objects , when the attributes have continuous domains . with relational representations , they can model joint probability distributions over large numbers of variables compactly in a natural way . this paper presents a new exact lifted inference algorithm for rcms , thus it scales up to large models of real world applications . the algorithm applies to relational pairwise models which are ( relational ) products of potentials of arity 2. our algorithm is unique in two ways . first , it substantially improves the efficiency of lifted inference with variables of continuous domains . when a relational model has gaussian potentials , it takes only linear-time compared to cubic time of previous methods . second , it is the first exact inference algorithm which handles rcms in a lifted way . the algorithm is illustrated over an example from econometrics . experimental results show that our algorithm outperforms both a groundlevel inference algorithm and an algorithm built with previously-known lifted methods .

unicalc.lin : a linear constraint solver for the unicalc system
in this short paper we present a linear constraint solver for the unicalc system , an environment for reliable solution of mathematical modeling problems .

essence ' description
a description of the essence ' language as used by the tool savile row .

analysing sensitivity data from probabilistic networks
with the advance of efficient analytical methods for sensitivity analysis ofprobabilistic networks , the interest in the sensitivities revealed by real-life networks is rekindled . as the amount of data resulting from a sensitivity analysis of even a moderately-sized network is alreadyoverwhelming , methods for extracting relevant information are called for . one such methodis to study the derivative of the sensitivity functions yielded for a network 's parameters . we further propose to build upon the concept of admissible deviation , that is , the extent to which a parameter can deviate from the true value without inducing a change in the most likely outcome . we illustrate these concepts by means of a sensitivity analysis of a real-life probabilistic network in oncology .

deep knowledge tracing
knowledge tracing -- -where a machine models the knowledge of a student as they interact with coursework -- -is a well established problem in computer supported education . though effectively modeling student knowledge would have high educational impact , the task has many inherent challenges . in this paper we explore the utility of using recurrent neural networks ( rnns ) to model student learning . the rnn family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge , and can capture more complex representations of student knowledge . using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets . moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks . these results suggest a promising new line of research for knowledge tracing and an exemplary application task for rnns .

rerepresenting and restructuring domain theories : a constructive induction approach
theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory . there are two challenges that theory revision and other theory-guided systems face . first , a representation language appropriate for the initial theory may be inappropriate for an improved theory . while the original representation may concisely express the initial theory , a more accurate theory forced to use that same representation may be bulky , cumbersome , and difficult to reach . second , a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory . systems that produce only small , local changes to a theory have limited value for accomplishing complex structural alterations that may be required . consequently , advanced theory-guided learning systems require flexible representation and flexible structure . an analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties . designed to capture the underlying qualities of each system , a new system uses theory-guided constructive induction . experiments in three domains show improvement over previous theory-guided systems . this leads to a study of the behavior , limitations , and potential of theory-guided constructive induction .

higher order probabilities
a number of writers have supposed that for the full specification of belief , higher order probabilities are required . some have even supposed that there may be an unending sequence of higher order probabilities of probabilities of probabilities ... . in the present paper we show that higher order probabilities can always be replaced by the marginal distributions of joint probability distributions . we consider both the case in which higher order probabilities are of the same sort as lower order probabilities and that in which higher order probabilities are distinct in character , as when lower order probabilities are construed as frequencies and higher order probabilities are construed as subjective degrees of belief . in neither case do higher order probabilities appear to offer any advantages , either conceptually or computationally .

zm-net : real-time zero-shot image manipulation network
many problems in image processing and computer vision ( e.g . colorization , style transfer ) can be posed as 'manipulating ' an input image into a corresponding output image given a user-specified guiding signal . a holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals ( even signals unseen during training ) , such as diverse paintings and arbitrary descriptive attributes . however , existing methods are either inefficient to simultaneously process multiple signals ( let alone generalize to unseen signals ) , or unable to handle signals from other modalities . in this paper , we make the first attempt to address the zero-shot image manipulation task . we cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal ( even unseen ones ) . to this end , we propose the zero-shot manipulation net ( zm-net ) , a fully-differentiable architecture that jointly optimizes an image-transformation network ( tnet ) and a parameter network ( pnet ) . the pnet learns to generate key transformation parameters for the tnet given any guiding signal while the tnet performs fast zero-shot image manipulation according to both signal-dependent parameters from the pnet and signal-invariant parameters from the tnet itself . extensive experiments show that our zm-net can perform high-quality image manipulation conditioned on different forms of guiding signals ( e.g . style images and attributes ) in real-time ( tens of milliseconds per image ) even for unseen signals . moreover , a large-scale style dataset with over 20,000 style images is also constructed to promote further research .

minimum model semantics for extensional higher-order logic programming with negation
extensional higher-order logic programming has been introduced as a generalization of classical logic programming . an important characteristic of this paradigm is that it preserves all the well-known properties of traditional logic programming . in this paper we consider the semantics of negation in the context of the new paradigm . using some recent results from non-monotonic fixed-point theory , we demonstrate that every higher-order logic program with negation has a unique minimum infinite-valued model . in this way we obtain the first purely model-theoretic semantics for negation in extensional higher-order logic programming . using our approach , we resolve an old paradox that was introduced by w. w. wadge in order to demonstrate the semantic difficulties of higher-order logic programming .

visualizing and exploring dynamic high-dimensional datasets with lion-tsne
t-distributed stochastic neighbor embedding ( tsne ) is a popular and prize-winning approach for dimensionality reduction and visualizing high-dimensional data . however , tsne is non-parametric : once visualization is built , tsne is not designed to incorporate additional data into existing representation . it highly limits the applicability of tsne to the scenarios where data are added or updated over time ( like dashboards or series of data snapshots ) . in this paper we propose , analyze and evaluate lion-tsne ( local interpolation with outlier control ) - a novel approach for incorporating new data into tsne representation . lion-tsne is based on local interpolation in the vicinity of training data , outlier detection and a special outlier mapping algorithm . we show that lion-tsne method is robust both to outliers and to new samples from existing clusters . we also discuss multiple possible improvements for special cases . we compare lion-tsne to a comprehensive list of possible benchmark approaches that include multiple interpolation techniques , gradient descent for new data , and neural network approximation .

quantifying the probable approximation error of probabilistic inference programs
this paper introduces a new technique for quantifying the approximation error of a broad class of probabilistic inference programs , including ones based on both variational and monte carlo approaches . the key idea is to derive a subjective bound on the symmetrized kl divergence between the distribution achieved by an approximate inference program and its true target distribution . the bound 's validity ( and subjectivity ) rests on the accuracy of two auxiliary probabilistic programs : ( i ) a `` reference '' inference program that defines a gold standard of accuracy and ( ii ) a `` meta-inference '' program that answers the question `` what internal random choices did the original approximate inference program probably make given that it produced a particular result ? '' the paper includes empirical results on inference problems drawn from linear regression , dirichlet process mixture modeling , hmms , and bayesian networks . the experiments show that the technique is robust to the quality of the reference inference program and that it can detect implementation bugs that are not apparent from predictive performance .

the sea exploration problem : data-driven orienteering on a continuous surface
this paper describes a problem arising in sea exploration , where the aim is to schedule the expedition of a ship for collecting information about the resources on the seafloor . the aim is to collect data by probing on a set of carefully chosen locations , so that the information available is optimally enriched . this problem has similarities with the orienteering problem , where the aim is to plan a time-limited trip for visiting a set of vertices , collecting a prize at each of them , in such a way that the total value collected is maximum . in our problem , the score at each vertex is associated with an estimation of the level of the resource on the given surface , which is done by regression using gaussian processes . hence , there is a correlation among scores on the selected vertices ; this is a first difference with respect to the standard orienteering problem . the second difference is the location of each vertex , which in our problem is a freely chosen point on a given surface .

external validity : from do-calculus to transportability across populations
the generalizability of empirical findings to new environments , settings or populations , often called `` external validity , '' is essential in most scientific explorations . this paper treats a particular problem of generalizability , called `` transportability , '' defined as a license to transfer causal effects learned in experimental studies to a new population , in which only observational studies can be conducted . we introduce a formal representation called `` selection diagrams '' for expressing knowledge about differences and commonalities between populations of interest and , using this representation , we reduce questions of transportability to symbolic derivations in the do-calculus . this reduction yields graph-based procedures for deciding , prior to observing any data , whether causal effects in the target population can be inferred from experimental findings in the study population . when the answer is affirmative , the procedures identify what experimental and observational findings need be obtained from the two populations , and how they can be combined to ensure bias-free transport .

a convergent online single time scale actor critic algorithm
actor-critic based approaches were among the first to address reinforcement learning in a general setting . recently , these algorithms have gained renewed interest due to their generality , good convergence properties , and possible biological relevance . in this paper , we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward . linear function approximation is used by the critic in order estimate the value function , and the temporal difference signal , which is passed from the critic to the actor . the main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale , while in most current convergence proofs they are required to have very different time scales in order to converge . moreover , the same temporal difference signal is used to update the parameters of both the actor and the critic . a limitation of the proposed approach , compared to results available for two time scale convergence , is that convergence is guaranteed only to a neighborhood of an optimal value , rather to an optimal value itself . the single time scale and identical temporal difference signal used by the actor and the critic , may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain .

on the problem of computing the well-founded semantics
the well-founded semantics is one of the most widely studied and used semantics of logic programs with negation . in the case of finite propositional programs , it can be computed in polynomial time , more specifically , in o ( |at ( p ) |size ( p ) ) steps , where size ( p ) denotes the total number of occurrences of atoms in a logic program p. this bound is achieved by an algorithm introduced by van gelder and known as the alternating-fixpoint algorithm . improving on the alternating-fixpoint algorithm turned out to be difficult . in this paper we study extensions and modifications of the alternating-fixpoint approach . we then restrict our attention to the class of programs whose rules have no more than one positive occurrence of an atom in their bodies . for programs in that class we propose a new implementation of the alternating-fixpoint method in which false atoms are computed in a top-down fashion . we show that our algorithm is faster than other known algorithms and that for a wide class of programs it is linear and so , asymptotically optimal .

integrating probabilistic , taxonomic and causal knowledge in abductive diagnosis
we propose an abductive diagnosis theory that integrates probabilistic , causal and taxonomic knowledge . probabilistic knowledge allows us to select the most likely explanation ; causal knowledge allows us to make reasonable independence assumptions ; taxonomic knowledge allows causation to be modeled at different levels of detail , and allows observations be described in different levels of precision . unlike most other approaches where a causal explanation is a hypothesis that one or more causative events occurred , we define an explanation of a set of observations to be an occurrence of a chain of causation events . these causation events constitute a scenario where all the observations are true . we show that the probabilities of the scenarios can be computed from the conditional probabilities of the causation events . abductive reasoning is inherently complex even if only modest expressive power is allowed . however , our abduction algorithm is exponential only in the number of observations to be explained , and is polynomial in the size of the knowledge base . this contrasts with many other abduction procedures that are exponential in the size of the knowledge base .

perceptual reward functions
reinforcement learning problems are often described through rewards that indicate if an agent has completed some task . this specification can yield desirable behavior , however many problems are difficult to specify in this manner , as one often needs to know the proper configuration for the agent . when humans are learning to solve tasks , we often learn from visual instructions composed of images or videos . such representations motivate our development of perceptual reward functions , which provide a mechanism for creating visual task descriptions . we show that this approach allows an agent to learn from rewards that are based on raw pixels rather than internal parameters .

on the complexity of solving markov decision problems
markov decision problems ( mdps ) provide the foundations for a number of problems of interest to ai researchers studying automated planning and reinforcement learning . in this paper , we summarize results regarding the complexity of solving mdps and the running time of mdp solution algorithms . we argue that , although mdps can be solved efficiently in theory , more study is needed to reveal practical algorithms for solving large problems quickly . to encourage future research , we sketch some alternative methods of analysis that rely on the structure of mdps .

representing knowledge base into database for wap and web-based expert system
expert system is developed as consulting service for users spread or public requires affordable access . the internet has become a medium for such services , but presence of mobile devices make the access becomes more widespread by utilizing mobile web and wap ( wireless application protocol ) . applying expert systems applications over the web and wap requires a knowledge base representation that can be accessed simultaneously . this paper proposes single database to accommodate the knowledge representation with decision tree mapping approach . because of the database exist , consulting application through both web and wap can access it to provide expert system services options for more affordable for public .

bridging cognitive programs and machine learning
while great advances are made in pattern recognition and machine learning , the successes of such fields remain restricted to narrow applications and seem to break down when training data is scarce , a shift in domain occurs , or when intelligent reasoning is required for rapid adaptation to new environments . in this work , we list several of the shortcomings of modern machine-learning solutions , specifically in the contexts of computer vision and in reinforcement learning and suggest directions to explore in order to try to ameliorate these weaknesses .

from query-by-keyword to query-by-example : linkedin talent search approach
one key challenge in talent search is to translate complex criteria of a hiring position into a search query , while it is relatively easy for a searcher to list examples of suitable candidates for a given position . to improve search efficiency , we propose the next generation of talent search at linkedin , also referred to as search by ideal candidates . in this system , a searcher provides one or several ideal candidates as the input to hire for a given position . the system then generates a query based on the ideal candidates and uses it to retrieve and rank results . shifting from the traditional query-by-keyword to this new query-by-example system poses a number of challenges : how to generate a query that best describes the candidates ? when moving to a completely different paradigm , how does one leverage previous product logs to learn ranking models and/or evaluate the new system with no existing usage logs ? finally , given the different nature between the two search paradigms , the ranking features typically used for query-by-keyword systems might not be optimal for query-by-example . this paper describes our approach to solving these challenges . we present experimental results confirming the effectiveness of the proposed solution , particularly on query building and search ranking tasks . as of writing this paper , the new system has been available to all linkedin members .

extracting lifted mutual exclusion invariants from temporal planning domains
we present a technique for automatically extracting mutual exclusion invariants from temporal planning instances . it first identifies a set of invariant templates by inspecting the lifted representation of the domain and then checks these templates against properties that assure invariance . our technique builds on other approaches to invariant synthesis presented in the literature , but departs from their limited focus on instantaneous actions by addressing temporal domains . to deal with time , we formulate invariance conditions that account for the entire structure of the actions and the possible concurrent interactions between them . as a result , we construct a significantly more comprehensive technique than previous methods , which is able to find not only invariants for temporal domains , but also a broader set of invariants for non-temporal domains . the experimental results reported in this paper provide evidence that identifying a broader set of invariants results in the generation of fewer multi-valued state variables with larger domains . we show that , in turn , this reduction in the number of variables reflects positively on the performance of a number of temporal planners that use a variable/value representation by significantly reducing their running time .

transferring autonomous driving knowledge on simulated and real intersections
we view intersection handling on autonomous vehicles as a reinforcement learning problem , and study its behavior in a transfer learning setting . we show that a network trained on one type of intersection generally is not able to generalize to other intersections . however , a network that is pre-trained on one intersection and fine-tuned on another performs better on the new task compared to training in isolation . this network also retains knowledge of the prior task , even though some forgetting occurs . finally , we show that the benefits of fine-tuning hold when transferring simulated intersection handling knowledge to a real autonomous vehicle .

a rule-based approach for aligning japanese-spanish sentences from a comparable corpora
the performance of a statistical machine translation system ( smt ) system is proportionally directed to the quality and length of the parallel corpus it uses . however for some pair of languages there is a considerable lack of them . the long term goal is to construct a japanese-spanish parallel corpus to be used for smt , whereas , there are a lack of useful japanese-spanish parallel corpus . to address this problem , in this study we proposed a method for extracting japanese-spanish parallel sentences from wikipedia using pos tagging and rule-based approach . the main focus of this approach is the syntactic features of both languages . human evaluation was performed over a sample and shows promising results , in comparison with the baseline .

adaptive learning with binary neurons
a efficient incremental learning algorithm for classification tasks , called netlines , well adapted for both binary and real-valued input patterns is presented . it generates small compact feedforward neural networks with one hidden layer of binary units and binary output units . a convergence theorem ensures that solutions with a finite number of hidden units exist for both binary and real-valued input patterns . an implementation for problems with more than two classes , valid for any binary classifier , is proposed . the generalization error and the size of the resulting networks are compared to the best published results on well-known classification benchmarks . early stopping is shown to decrease overfitting , without improving the generalization performance .

boost phrase-level polarity labelling with review-level sentiment classification
sentiment analysis on user reviews helps to keep track of user reactions towards products , and make advices to users about what to buy . state-of-the-art review-level sentiment classification techniques could give pretty good precisions of above 90 % . however , current phrase-level sentiment analysis approaches might only give sentiment polarity labelling precisions of around 70 % ~80 % , which is far from satisfaction and restricts its application in many practical tasks . in this paper , we focus on the problem of phrase-level sentiment polarity labelling and attempt to bridge the gap between phrase-level and review-level sentiment analysis . we investigate the inconsistency between the numerical star ratings and the sentiment orientation of textual user reviews . although they have long been treated as identical , which serves as a basic assumption in previous work , we find that this assumption is not necessarily true . we further propose to leverage the results of review-level sentiment classification to boost the performance of phrase-level polarity labelling using a novel constrained convex optimization framework . besides , the framework is capable of integrating various kinds of information sources and heuristics , while giving the global optimal solution due to its convexity . experimental results on both english and chinese reviews show that our framework achieves high labelling precisions of up to 89 % , which is a significant improvement from current approaches .

modelling creativity : identifying key components through a corpus-based approach
creativity is a complex , multi-faceted concept encompassing a variety of related aspects , abilities , properties and behaviours . if we wish to study creativity scientifically , then a tractable and well-articulated model of creativity is required . such a model would be of great value to researchers investigating the nature of creativity and in particular , those concerned with the evaluation of creative practice . this paper describes a unique approach to developing a suitable model of how creative behaviour emerges that is based on the words people use to describe the concept . using techniques from the field of statistical natural language processing , we identify a collection of fourteen key components of creativity through an analysis of a corpus of academic papers on the topic . words are identified which appear significantly often in connection with discussions of the concept . using a measure of lexical similarity to help cluster these words , a number of distinct themes emerge , which collectively contribute to a comprehensive and multi-perspective model of creativity . the components provide an ontology of creativity : a set of building blocks which can be used to model creative practice in a variety of domains . the components have been employed in two case studies to evaluate the creativity of computational systems and have proven useful in articulating achievements of this work and directions for further research .

an agent based architecture ( using planning ) for dynamic and semantic web services composition in an ebxml context
the process-based semantic composition of web services is gaining a considerable momentum as an approach for the effective integration of distributed , heterogeneous , and autonomous applications . to compose web services semantically , we need an ontology . there are several ways of inserting semantics in web services . one of them consists of using description languages like owl-s. in this paper , we introduce our work which consists in the proposition of a new model and the use of semantic matching technology for semantic and dynamic composition of ebxml business processes .

universal empathy and ethical bias for artificial general intelligence
rational agents are usually built to maximize rewards . however , agi agents can find undesirable ways of maximizing any prior reward function . therefore value learning is crucial for safe agi . we assume that generalized states of the world are valuable - not rewards themselves , and propose an extension of aixi , in which rewards are used only to bootstrap hierarchical value learning . the modified aixi agent is considered in the multi-agent environment , where other agents can be either humans or other `` mature '' agents , which values should be revealed and adopted by the `` infant '' agi agent . general framework for designing such empathic agent with ethical bias is proposed also as an extension of the universal intelligence model . moreover , we perform experiments in the simple markov environment , which demonstrate feasibility of our approach to value learning in safe agi .

network as a computer : ranking paths to find flows
we explore a simple mathematical model of network computation , based on markov chains . similar models apply to a broad range of computational phenomena , arising in networks of computers , as well as in genetic , and neural nets , in social networks , and so on . the main problem of interaction with such spontaneously evolving computational systems is that the data are not uniformly structured . an interesting approach is to try to extract the semantical content of the data from their distribution among the nodes . a concept is then identified by finding the community of nodes that share it . the task of data structuring is thus reduced to the task of finding the network communities , as groups of nodes that together perform some non-local data processing . towards this goal , we extend the ranking methods from nodes to paths . this allows us to extract some information about the likely flow biases from the available static information about the network .

handwriting profiling using generative adversarial networks
handwriting is a skill learned by humans from a very early age . the ability to develop one 's own unique handwriting as well as mimic another person 's handwriting is a task learned by the brain with practice . this paper deals with this very problem where an intelligent system tries to learn the handwriting of an entity using generative adversarial networks ( gans ) . we propose a modified architecture of dcgan ( radford , metz , and chintala 2015 ) to achieve this . we also discuss about applying reinforcement learning techniques to achieve faster learning . our algorithm hopes to give new insights in this area and its uses include identification of forged documents , signature verification , computer generated art , digitization of documents among others . our early implementation of the algorithm illustrates a good performance with mnist datasets .

combinatorial structure of the deterministic seriation method with multiple subset solutions
seriation methods order a set of descriptions given some criterion ( e.g. , unimodality or minimum distance between similarity scores ) . seriation is thus inherently a problem of finding the optimal solution among a set of permutations of objects . in this short technical note , we review the combinatorial structure of the classical seriation problem , which seeks a single solution out of a set of objects . we then extend those results to the iterative frequency seriation approach introduced by lipo ( 1997 ) , which finds optimal subsets of objects which each satisfy the unimodality criterion within each subset . the number of possible solutions across multiple solution subsets is larger than $ n ! $ , which underscores the need to find new algorithms and heuristics to assist in the deterministic frequency seriation problem .

pddl 2.1 : representation vs. computation
i comment on the pddl 2.1 language and its use in the planning competition , focusing on the choices made for accommodating time and concurrency . i also discuss some methodological issues that have to do with the move toward more expressive planning languages and the balance needed in planning research between semantics and computation .

providing effective real-time feedback in simulation-based surgical training
virtual reality simulation is becoming popular as a training platform in surgical education . however , one important aspect of simulation-based surgical training that has not received much attention is the provision of automated real-time performance feedback to support the learning process . performance feedback is actionable advice that improves novice behaviour . in simulation , automated feedback is typically extracted from prediction models trained using data mining techniques . existing techniques suffer from either low effectiveness or low efficiency resulting in their inability to be used in real-time . in this paper , we propose a random forest based method that finds a balance between effectiveness and efficiency . experimental results in a temporal bone surgery simulation show that the proposed method is able to extract highly effective feedback at a high level of efficiency .

cross-lingual predicate mapping between linked data ontologies
ontologies in different natural languages often differ in quality in terms of richness of schema or richness of internal links . this difference is markedly visible when comparing a rich english language ontology with a non-english language counterpart . discovering alignment between them is a useful endeavor as it serves as a starting point in bridging the disparity . in particular , our work is motivated by the absence of inter-language links for predicates in the localised versions of dbpedia . in this paper , we propose and demonstrate an ad-hoc system to find possible owl : equivalentproperty links between predicates in ontologies of different natural languages . we seek to achieve this mapping by using pre-existing inter-language links of the resources connected by the given predicate . thus , our methodology stresses on semantic similarity rather than lexical . moreover , through an evaluation , we show that our system is capable of outperforming a baseline system that is similar to the one used in recent oaei campaigns .

changing the environment based on empowerment as intrinsic motivation
one aspect of intelligence is the ability to restructure your own environment so that the world you live in becomes more beneficial to you . in this paper we investigate how the information-theoretic measure of agent empowerment can provide a task-independent , intrinsic motivation to restructure the world . we show how changes in embodiment and in the environment change the resulting behaviour of the agent and the artefacts left in the world . for this purpose , we introduce an approximation of the established empowerment formalism based on sparse sampling , which is simpler and significantly faster to compute for deterministic dynamics . sparse sampling also introduces a degree of randomness into the decision making process , which turns out to beneficial for some cases . we then utilize the measure to generate agent behaviour for different agent embodiments in a minecraft-inspired three dimensional block world . the paradigmatic results demonstrate that empowerment can be used as a suitable generic intrinsic motivation to not only generate actions in given static environments , as shown in the past , but also to modify existing environmental conditions . in doing so , the emerging strategies to modify an agent 's environment turn out to be meaningful to the specific agent capabilities , i.e. , de facto to its embodiment .

temporal-difference learning to assist human decision making during the control of an artificial limb
in this work we explore the use of reinforcement learning ( rl ) to help with human decision making , combining state-of-the-art rl algorithms with an application to prosthetics . managing human-machine interaction is a problem of considerable scope , and the simplification of human-robot interfaces is especially important in the domains of biomedical technology and rehabilitation medicine . for example , amputees who control artificial limbs are often required to quickly switch between a number of control actions or modes of operation in order to operate their devices . we suggest that by learning to anticipate ( predict ) a user 's behaviour , artificial limbs could take on an active role in a human 's control decisions so as to reduce the burden on their users . recently , we showed that rl in the form of general value functions ( gvfs ) could be used to accurately detect a user 's control intent prior to their explicit control choices . in the present work , we explore the use of temporal-difference learning and gvfs to predict when users will switch their control influence between the different motor functions of a robot arm . experiments were performed using a multi-function robot arm that was controlled by muscle signals from a user 's body ( similar to conventional artificial limb control ) . our approach was able to acquire and maintain forecasts about a user 's switching decisions in real time . it also provides an intuitive and reward-free way for users to correct or reinforce the decisions made by the machine learning system . we expect that when a system is certain enough about its predictions , it can begin to take over switching decisions from the user to streamline control and potentially decrease the time and effort needed to complete tasks . this preliminary study therefore suggests a way to naturally integrate human- and machine-based decision making systems .

rollout sampling approximate policy iteration
several researchers have recently investigated the connection between reinforcement learning and classification . we are motivated by proposals of approximate policy iteration schemes without value functions which focus on policy representation using classifiers and address policy learning as a supervised learning problem . this paper proposes variants of an improved policy iteration scheme which addresses the core sampling problem in evaluating a policy through simulation as a multi-armed bandit machine . the resulting algorithm offers comparable performance to the previous algorithm achieved , however , with significantly less computational effort . an order of magnitude improvement is demonstrated experimentally in two standard reinforcement learning domains : inverted pendulum and mountain-car .

applying fuzzy id3 decision tree for software effort estimation
web effort estimation is a process of predicting the efforts and cost in terms of money , schedule and staff for any software project system . many estimation models have been proposed over the last three decades and it is believed that it is a must for the purpose of : budgeting , risk analysis , project planning and control , and project improvement investment analysis . in this paper , we investigate the use of fuzzy id3 decision tree for software cost estimation ; it is designed by integrating the principles of id3 decision tree and the fuzzy set-theoretic concepts , enabling the model to handle uncertain and imprecise data when describing the software projects , which can improve greatly the accuracy of obtained estimates . mmre and pred are used as measures of prediction accuracy for this study . a series of experiments is reported using two different software projects datasets namely , tukutuku and cocomo'81 datasets . the results are compared with those produced by the crisp version of the id3 decision tree .

value-function approximations for partially observable markov decision processes
partially observable markov decision processes ( pomdps ) provide an elegant mathematical framework for modeling complex decision and planning problems in stochastic domains in which states of the system are observable only indirectly , via a set of imperfect or noisy observations . the modeling advantage of pomdps , however , comes at a price -- exact methods for solving them are computationally very expensive and thus applicable in practice only to very simple problems . we focus on efficient approximation ( heuristic ) methods that attempt to alleviate the computational problem and trade off accuracy for speed . we have two objectives here . first , we survey various approximation methods , analyze their properties and relations and provide some new insights into their differences . second , we present a number of new approximation methods and novel refinements of existing techniques . the theoretical results are supported by experiments on a problem from the agent navigation domain .

parallelizing linear recurrent neural nets over sequence length
recurrent neural networks ( rnns ) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length . we show the training of rnns with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm , leading to rapid training on long sequences even with small minibatch size . we develop a parallel linear recurrence cuda kernel and show that it can be applied to immediately speed up training and inference of several state of the art rnn architectures by up to 9x . we abstract recent work on linear rnns into a new framework of linear surrogate rnns and develop a linear surrogate model for the long short-term memory unit , the gilr-lstm , that utilizes parallel linear recurrence . we extend sequence learning to new extremely long sequence regimes that were previously out of reach by successfully training a gilr-lstm on a synthetic sequence classification task with a one million timestep dependency .

synergistic team composition
effective teams are crucial for organisations , especially in environments that require teams to be constantly created and dismantled , such as software development , scientific experiments , crowd-sourcing , or the classroom . key factors influencing team performance are competences and personality of team members . hence , we present a computational model to compose proficient and congenial teams based on individuals ' personalities and their competences to perform tasks of different nature . with this purpose , we extend wilde 's post-jungian method for team composition , which solely employs individuals ' personalities . the aim of this study is to create a model to partition agents into teams that are balanced in competences , personality and gender . finally , we present some preliminary empirical results that we obtained when analysing student performance . results show the benefits of a more informed team composition that exploits individuals ' competences besides information about their personalities .

heinrich behmann 's contributions to second-order quantifier elimination from the view of computational logic
for relational monadic formulas ( the l\ '' owenheim class ) second-order quantifier elimination , which is closely related to computation of uniform interpolants , projection and forgetting - operations that currently receive much attention in knowledge processing - always succeeds . the decidability proof for this class by heinrich behmann from 1922 explicitly proceeds by elimination with equivalence preserving formula rewriting . here we reconstruct the results from behmann 's publication in detail and discuss related issues that are relevant in the context of modern approaches to second-order quantifier elimination in computational logic . in addition , an extensive documentation of the letters and manuscripts in behmann 's bequest that concern second-order quantifier elimination is given , including a commented register and english abstracts of the german sources with focus on technical material . in the late 1920s behmann attempted to develop an elimination-based decision method for formulas with predicates whose arity is larger than one . his manuscripts and the correspondence with wilhelm ackermann show technical aspects that are still of interest today and give insight into the genesis of ackermann 's landmark paper `` untersuchungen \ '' uber das eliminationsproblem der mathematischen logik '' from 1935 , which laid the foundation of the two prevailing modern approaches to second-order quantifier elimination .

minimum weight perfect matching via blossom belief propagation
max-product belief propagation ( bp ) is a popular message-passing algorithm for computing a maximum-a-posteriori ( map ) assignment over a distribution represented by a graphical model ( gm ) . it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching , shortest path , network flow and vertex cover under the following common assumption : the respective linear programming ( lp ) relaxation is tight , i.e. , no integrality gap is present . however , when lp shows an integrality gap , no model has been known which can be solved systematically via sequential applications of bp . in this paper , we develop the first such algorithm , coined blossom-bp , for solving the minimum weight matching problem over arbitrary graphs . each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms , i.e. , odd sets of vertices . our scheme guarantees termination in o ( n^2 ) of bp runs , where n is the number of vertices in the original graph . in essence , the blossom-bp offers a distributed version of the celebrated edmonds ' blossom algorithm by jumping at once over many sub-steps with a single bp . moreover , our result provides an interpretation of the edmonds ' algorithm as a sequence of lps .

inference in probabilistic logic programs using weighted cnf 's
probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities . several classical probabilistic inference tasks ( such as map and computing marginals ) have not yet received a lot of attention for this formalism . the contribution of this paper is that we develop efficient inference algorithms for these tasks . this is based on a conversion of the probabilistic logic program and the query and evidence to a weighted cnf formula . this allows us to reduce the inference tasks to well-studied tasks such as weighted model counting . to solve such tasks , we employ state-of-the-art methods . we consider multiple methods for the conversion of the programs as well as for inference on the weighted cnf . the resulting approach is evaluated experimentally and shown to improve upon the state-of-the-art in probabilistic logic programming .

low impact artificial intelligences
there are many goals for an ai that could become dangerous if the ai becomes superintelligent or otherwise powerful . much work on the ai control problem has been focused on constructing ai goals that are safe even for such ais . this paper looks at an alternative approach : defining a general concept of ` low impact ' . the aim is to ensure that a powerful ai which implements low impact will not modify the world extensively , even if it is given a simple or dangerous goal . the paper proposes various ways of defining and grounding low impact , and discusses methods for ensuring that the ai can still be allowed to have a ( desired ) impact despite the restriction . the end of the paper addresses known issues with this approach and avenues for future research .

combat models for rts games
game tree search algorithms , such as monte carlo tree search ( mcts ) , require access to a forward model ( or `` simulator '' ) of the game at hand . however , in some games such forward model is not readily available . this paper presents three forward models for two-player attrition games , which we call `` combat models '' , and show how they can be used to simulate combat in rts games . we also show how these combat models can be learned from replay data . we use starcraft as our application domain . we report experiments comparing our combat models predicting a combat output and their impact when used for tactical decisions during a real game .

kgan : how to break the minimax game in gan
generative adversarial networks ( gans ) were intuitively and attractively explained under the perspective of game theory , wherein two involving parties are a discriminator and a generator . in this game , the task of the discriminator is to discriminate the real and generated ( i.e. , fake ) data , whilst the task of the generator is to generate the fake data that maximally confuses the discriminator . in this paper , we propose a new viewpoint for gans , which is termed as the minimizing general loss viewpoint . this viewpoint shows a connection between the general loss of a classification problem regarding a convex loss function and a f-divergence between the true and fake data distributions . mathematically , we proposed a setting for the classification problem of the true and fake data , wherein we can prove that the general loss of this classification problem is exactly the negative f-divergence for a certain convex function f. this allows us to interpret the problem of learning the generator for dismissing the f-divergence between the true and fake data distributions as that of maximizing the general loss which is equivalent to the min-max problem in gan if the logistic loss is used in the classification problem . however , this viewpoint strengthens gans in two ways . first , it allows us to employ any convex loss function for the discriminator . second , it suggests that rather than limiting ourselves in nn-based discriminators , we can alternatively utilize other powerful families . bearing this viewpoint , we then propose using the kernel-based family for discriminators . this family has two appealing features : i ) a powerful capacity in classifying non-linear nature data and ii ) being convex in the feature space . using the convexity of this family , we can further develop fenchel duality to equivalently transform the max-min problem to the max-max dual problem .

emergence of locomotion behaviours in rich environments
the reinforcement learning paradigm allows , in principle , for complex behaviours to be learned directly from simple reward signals . in practice , however , it is common to carefully hand-design the reward function to encourage a particular solution , or to derive it from demonstration data . in this paper explore how a rich environment can help to promote the learning of complex behavior . specifically , we train agents in diverse environmental contexts , and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks . we demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward . we train several simulated bodies on a diverse set of challenging terrains and obstacles , using a simple reward function based on forward progress . using a novel scalable variant of policy gradient reinforcement learning , our agents learn to run , jump , crouch and turn as required by the environment without explicit reward-based guidance . a visual depiction of highlights of the learned behavior can be viewed following https : //youtu.be/hx_bgotf7bs .

semantically decomposing the latent spaces of generative adversarial networks
we propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities ( e.g . individual humans ) and observations ( e.g . specific photographs ) . by fixing the identity portion of the latent codes , we can generate diverse images of the same subject , and by fixing the observation portion , we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose . our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code . corresponding samples from the real dataset consist of two distinct photographs of the same subject . in order to fool the discriminator , the generator must produce pairs that are photorealistic , distinct , and appear to depict the same individual . we augment both the dcgan and began approaches with siamese discriminators to facilitate pairwise training . experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm 's ability to generate convincing , identity-matched photographs .

intelligent fault analysis in electrical power grids
power grids are one of the most important components of infrastructure in today 's world . every nation is dependent on the security and stability of its own power grid to provide electricity to the households and industries . a malfunction of even a small part of a power grid can cause loss of productivity , revenue and in some cases even life . thus , it is imperative to design a system which can detect the health of the power grid and take protective measures accordingly even before a serious anomaly takes place . to achieve this objective , we have set out to create an artificially intelligent system which can analyze the grid information at any given time and determine the health of the grid through the usage of sophisticated formal models and novel machine learning techniques like recurrent neural networks . our system simulates grid conditions including stimuli like faults , generator output fluctuations , load fluctuations using siemens pss/e software and this data is trained using various classifiers like svm , lstm and subsequently tested . the results are excellent with our methods giving very high accuracy for the data . this model can easily be scaled to handle larger and more complex grid architectures .

processing uncertainty and indeterminacy in information systems success mapping
is success is a complex concept , and its evaluation is complicated , unstructured and not readily quantifiable . numerous scientific publications address the issue of success in the is field as well as in other fields . but , little efforts have been done for processing indeterminacy and uncertainty in success research . this paper shows a formal method for mapping success using neutrosophic success map . this is an emerging tool for processing indeterminacy and uncertainty in success research . eis success have been analyzed using this tool .

verification of agent-based artifact systems
artifact systems are a novel paradigm for specifying and implementing business processes described in terms of interacting modules called artifacts . artifacts consist of data and lifecycles , accounting respectively for the relational structure of the artifacts ' states and their possible evolutions over time . in this paper we put forward artifact-centric multi-agent systems , a novel formalisation of artifact systems in the context of multi-agent systems operating on them . differently from the usual process-based models of services , the semantics we give explicitly accounts for the data structures on which artifact systems are defined . we study the model checking problem for artifact-centric multi-agent systems against specifications written in a quantified version of temporal-epistemic logic expressing the knowledge of the agents in the exchange . we begin by noting that the problem is undecidable in general . we then identify two noteworthy restrictions , one syntactical and one semantical , that enable us to find bisimilar finite abstractions and therefore reduce the model checking problem to the instance on finite models . under these assumptions we show that the model checking problem for these systems is expspace-complete . we then introduce artifact-centric programs , compact and declarative representations of the programs governing both the artifact system and the agents . we show that , while these in principle generate infinite-state systems , under natural conditions their verification problem can be solved on finite abstractions that can be effectively computed from the programs . finally we exemplify the theoretical results of the paper through a mainstream procurement scenario from the artifact systems literature .

credulous and skeptical argument games for complete semantics in conflict resolution based argumentation
argumentation is one of the most popular approaches of defining a~non-monotonic formalism and several argumentation based semantics were proposed for defeasible logic programs . recently , a new approach based on notions of conflict resolutions was proposed , however with declarative semantics only . this paper gives a more procedural counterpart by developing skeptical and credulous argument games for complete semantics and soundness and completeness theorems for both games are provided . after that , distribution of defeasible logic program into several contexts is investigated and both argument games are adapted for multi-context system .

deep reinforcement learning from raw pixels in doom
using current reinforcement learning methods , it has recently become possible to learn to play unknown 3d games from raw pixels . in this work , we study the challenges that arise in such complex environments , and summarize current methods to approach these . we choose a task within the doom game , that has not been approached yet . the goal for the agent is to fight enemies in a 3d world consisting of five rooms . we train the dqn and lstm-a3c algorithms on this task . results show that both algorithms learn sensible policies , but fail to achieve high scores given the amount of training . we provide insights into the learned behavior , which can serve as a valuable starting point for further research in the doom domain .

modular continual learning in a unified visual environment
a core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly . here , we describe a modular continual reinforcement learning paradigm inspired by these abilities . we first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework . we then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment . we investigate how properties of module architecture influence efficiency of task learning , showing that a module motif incorporating specific design principles ( e.g . early bottlenecks , low-order polynomial nonlinearities , and symmetry ) significantly outperforms more standard neural network motifs , needing fewer training examples and fewer neurons to achieve high levels of performance . finally , we present a meta-controller architecture for task switching based on a dynamic neural voting scheme , which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency .

an efficient metric of automatic weight generation for properties in instance matching technique
the proliferation of heterogeneous data sources of semantic knowledge base intensifies the need of an automatic instance matching technique . however , the efficiency of instance matching is often influenced by the weight of a property associated to instances . automatic weight generation is a non-trivial , however an important task in instance matching technique . therefore , identifying an appropriate metric for generating weight for a property automatically is nevertheless a formidable task . in this paper , we investigate an approach of generating weights automatically by considering hypotheses : ( 1 ) the weight of a property is directly proportional to the ratio of the number of its distinct values to the number of instances contain the property , and ( 2 ) the weight is also proportional to the ratio of the number of distinct values of a property to the number of instances in a training dataset . the basic intuition behind the use of our approach is the classical theory of information content that infrequent words are more informative than frequent ones . our mathematical model derives a metric for generating property weights automatically , which is applied in instance matching system to produce re-conciliated instances efficiently . our experiments and evaluations show the effectiveness of our proposed metric of automatic weight generation for properties in an instance matching technique .

affect control processes : intelligent affective interaction using a partially observable markov decision process
this paper describes a novel method for building affectively intelligent human-interactive agents . the method is based on a key sociological insight that has been developed and extensively verified over the last twenty years , but has yet to make an impact in artificial intelligence . the insight is that resource bounded humans will , by default , act to maintain affective consistency . humans have culturally shared fundamental affective sentiments about identities , behaviours , and objects , and they act so that the transient affective sentiments created during interactions confirm the fundamental sentiments . humans seek and create situations that confirm or are consistent with , and avoid and supress situations that disconfirm or are inconsistent with , their culturally shared affective sentiments . this `` affect control principle '' has been shown to be a powerful predictor of human behaviour . in this paper , we present a probabilistic and decision-theoretic generalisation of this principle , and we demonstrate how it can be leveraged to build affectively intelligent artificial agents . the new model , called bayesact , can maintain multiple hypotheses about sentiments simultaneously as a probability distribution , and can make use of an explicit utility function to make value-directed action choices . this allows the model to generate affectively intelligent interactions with people by learning about their identity , predicting their behaviours using the affect control principle , and taking actions that are simultaneously goal-directed and affect-sensitive . we demonstrate this generalisation with a set of simulations . we then show how our model can be used as an emotional `` plug-in '' for artificially intelligent systems that interact with humans in two different settings : an exam practice assistant ( tutor ) and an assistive device for persons with a cognitive disability .

comparison of selection methods in on-line distributed evolutionary robotics
in this paper , we study the impact of selection methods in the context of on-line on-board distributed evolutionary algorithms . we propose a variant of the medea algorithm in which we add a selection operator , and we apply it in a taskdriven scenario . we evaluate four selection methods that induce different intensity of selection pressure in a multi-robot navigation with obstacle avoidance task and a collective foraging task . experiments show that a small intensity of selection pressure is sufficient to rapidly obtain good performances on the tasks at hand . we introduce different measures to compare the selection methods , and show that the higher the selection pressure , the better the performances obtained , especially for the more challenging food foraging task .

simultaneous policy learning and latent state inference for imitating driver behavior
in this work , we propose a method for learning driver models that account for variables that can not be observed directly . when trained on a synthetic dataset , our models are able to learn encodings for vehicle trajectories that distinguish between four distinct classes of driver behavior . such encodings are learned without any knowledge of the number of driver classes or any objective that directly requires the models to learn encodings for each class . we show that driving policies trained with knowledge of latent variables are more effective than baseline methods at imitating the driver behavior that they are trained to replicate . furthermore , we demonstrate that the actions chosen by our policy are heavily influenced by the latent variable settings that are provided to them .

composite strategy for multicriteria ranking/sorting ( methodological issues , examples )
the paper addresses the modular design of composite solving strategies for multicriteria ranking ( sorting ) . here a 'scale of creativity ' that is close to creative levels proposed by altshuller is used as the reference viewpoint : ( i ) a basic object , ( ii ) a selected object , ( iii ) a modified object , and ( iv ) a designed object ( e.g. , composition of object components ) . these levels maybe used in various parts of decision support systems ( dss ) ( e.g. , information , operations , user ) . the paper focuses on the more creative above-mentioned level ( i.e. , composition or combinatorial synthesis ) for the operational part ( i.e. , composite solving strategy ) . this is important for a search/exploration mode of decision making process with usage of various procedures and techniques and analysis/integration of obtained results . the paper describes methodological issues of decision technology and synthesis of composite strategy for multicriteria ranking . the synthesis of composite strategies is based on 'hierarchical morphological multicriteria design ' ( hmmd ) which is based on selection and combination of design alternatives ( das ) ( here : local procedures or techniques ) while taking into account their quality and quality of their interconnections ( ic ) . a new version of hmmd with interval multiset estimates for das is used . the operational environment of dss combi for multicriteria ranking , consisting of a morphology of local procedures or techniques ( as design alternatives das ) , is examined as a basic one .

normative design using inductive learning
in this paper we propose a use-case-driven iterative design methodology for normative frameworks , also called virtual institutions , which are used to govern open systems . our computational model represents the normative framework as a logic program under answer set semantics ( asp ) . by means of an inductive logic programming approach , implemented using asp , it is possible to synthesise new rules and revise the existing ones . the learning mechanism is guided by the designer who describes the desired properties of the framework through use cases , comprising ( i ) event traces that capture possible scenarios , and ( ii ) a state that describes the desired outcome . the learning process then proposes additional rules , or changes to current rules , to satisfy the constraints expressed in the use cases . thus , the contribution of this paper is a process for the elaboration and revision of a normative framework by means of a semi-automatic and iterative process driven from specifications of ( un ) desirable behaviour . the process integrates a novel and general methodology for theory revision based on asp .

a flexible framework for defeasible logics
logics for knowledge representation suffer from over-specialization : while each logic may provide an ideal representation formalism for some problems , it is less than optimal for others . a solution to this problem is to choose from several logics and , when necessary , combine the representations . in general , such an approach results in a very difficult problem of combination . however , if we can choose the logics from a uniform framework then the problem of combining them is greatly simplified . in this paper , we develop such a framework for defeasible logics . it supports all defeasible logics that satisfy a strong negation principle . we use logic meta-programs as the basis for the framework .

reliable force aggregation using a refined evidence specification from dempster-shafer clustering
in this paper we develop methods for selection of templates and use these templates to recluster an already performed dempster-shafer clustering taking into account intelligence to template fit during the reclustering phase . by this process the risk of erroneous force aggregation based on some misplace pieces of evidence from the first clustering process is greatly reduced . finally , a more reliable force aggregation is performed using the result of the second clustering . these steps are taken in order to maintain most of the excellent computational performance of dempster-shafer clustering , while at the same time improve on the clustering result by including some higher relations among intelligence reports described by the templates . the new improved algorithm has a computational complexity of o ( n**3 log**2 n ) compared to o ( n**2 log**2 n ) of standard dempster-shafer clustering using potts spin mean field theory .

supporting temporal reasoning by mapping calendar expressions to minimal periodic sets
in the recent years several research efforts have focused on the concept of time granularity and its applications . a first stream of research investigated the mathematical models behind the notion of granularity and the algorithms to manage temporal data based on those models . a second stream of research investigated symbolic formalisms providing a set of algebraic operators to define granularities in a compact and compositional way . however , only very limited manipulation algorithms have been proposed to operate directly on the algebraic representation making it unsuitable to use the symbolic formalisms in applications that need manipulation of granularities . this paper aims at filling the gap between the results from these two streams of research , by providing an efficient conversion from the algebraic representation to the equivalent low-level representation based on the mathematical models . in addition , the conversion returns a minimal representation in terms of period length . our results have a major practical impact : users can more easily define arbitrary granularities in terms of algebraic operators , and then access granularity reasoning and other services operating efficiently on the equivalent , minimal low-level representation . as an example , we illustrate the application to temporal constraint reasoning with multiple granularities . from a technical point of view , we propose an hybrid algorithm that interleaves the conversion of calendar subexpressions into periodical sets with the minimization of the period length . the algorithm returns set-based granularity representations having minimal period length , which is the most relevant parameter for the performance of the considered reasoning services . extensive experimental work supports the techniques used in the algorithm , and shows the efficiency and effectiveness of the algorithm .

first results from using game refinement measure and learning coefficient in scrabble
this paper explores the entertainment experience and learning experience in scrabble . it proposes a new measure from the educational point of view , which we call learning coefficient , based on the balance between the learner 's skill and the challenge in scrabble . scrabble variants , generated using different size of board and dictionary , are analyzed with two measures of game refinement and learning coefficient . the results show that 13x13 scrabble yields the best entertainment experience and 15x15 ( standard ) scrabble with 4 % of original dictionary size yields the most effective environment for language learners . moreover , 15x15 scrabble with 10 % of original dictionary size has a good balance between entertainment and learning experience .

citlab argus for historical handwritten documents
we describe citlab 's recognition system for the htrts competition attached to the 13. international conference on document analysis and recognition , icdar 2015. the task comprises the recognition of historical handwritten documents . the core algorithms of our system are based on multi-dimensional recurrent neural networks ( mdrnn ) and connectionist temporal classification ( ctc ) . the software modules behind that as well as the basic utility technologies are essentially powered by planet 's argus framework for intelligent text recognition and image processing .

emergence of grounded compositional language in multi-agent populations
by capturing statistical patterns in large corpora , machine learning has enabled significant advances in natural language processing , including in machine translation , question answering , and sentiment analysis . however , for agents to intelligently interact with humans , simply capturing the statistical patterns is insufficient . in this paper we investigate if , and how , grounded compositional language can emerge as a means to achieve goals in multi-agent populations . towards this end , we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language . this language is represented as streams of abstract discrete symbols uttered by agents over time , but nonetheless has a coherent structure that possesses a defined vocabulary and syntax . we also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable .

combining observational and experimental data to find heterogeneous treatment effects
every design choice will have different effects on different units . however traditional a/b tests are often underpowered to identify these heterogeneous effects . this is especially true when the set of unit-level attributes is high-dimensional and our priors are weak about which particular covariates are important . however , there are often observational data sets available that are orders of magnitude larger . we propose a method to combine these two data sources to estimate heterogeneous treatment effects . first , we use observational time series data to estimate a mapping from covariates to unit-level effects . these estimates are likely biased but under some conditions the bias preserves unit-level relative rank orderings . if these conditions hold , we only need sufficient experimental data to identify a monotonic , one-dimensional transformation from observationally predicted treatment effects to real treatment effects . this reduces power demands greatly and makes the detection of heterogeneous effects much easier . as an application , we show how our method can be used to improve facebook page recommendations .

learning to communicate to solve riddles with deep distributed recurrent q-networks
we propose deep distributed recurrent q-networks ( ddrqn ) , which enable teams of agents to learn to solve communication-based coordination tasks . in these tasks , the agents are not given any pre-designed communication protocol . therefore , in order to successfully communicate , they must first automatically develop and agree upon their own communication protocol . we present empirical results on two multi-agent learning problems based on well-known riddles , demonstrating that ddrqn can successfully solve such tasks and discover elegant communication protocols to do so . to our knowledge , this is the first time deep reinforcement learning has succeeded in learning communication protocols . in addition , we present ablation experiments that confirm that each of the main components of the ddrqn architecture are critical to its success .

smart augmentation - learning an optimal data augmentation strategy
a recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks ( dnn ) . there are many techniques to address this , including data augmentation , dropout , and transfer learning . in this paper , we introduce an additional method which we call smart augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network . smart augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss . this allows us to learn augmentations that minimize the error of that network . smart augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested . in addition , it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases .

quantitative fine-grained human evaluation of machine translation systems : a case study on english to croatian
this paper presents a quantitative fine-grained manual evaluation approach to comparing the performance of different machine translation ( mt ) systems . we build upon the well-established multidimensional quality metrics ( mqm ) error taxonomy and implement a novel method that assesses whether the differences in performance for mqm error types between different mt systems are statistically significant . we conduct a case study for english-to-croatian , a language direction that involves translating into a morphologically rich language , for which we compare three mt systems belonging to different paradigms : pure phrase-based , factored phrase-based and neural . first , we design an mqm-compliant error taxonomy tailored to the relevant linguistic phenomena of slavic languages , which made the annotation process feasible and accurate . errors in mt outputs were then annotated by two annotators following this taxonomy . subsequently , we carried out a statistical analysis which showed that the best-performing system ( neural ) reduces the errors produced by the worst system ( pure phrase-based ) by more than half ( 54\ % ) . moreover , we conducted an additional analysis of agreement errors in which we distinguished between short ( phrase-level ) and long distance ( sentence-level ) errors . we discovered that phrase-based mt approaches are of limited use for long distance agreement phenomena , for which neural mt was found to be especially effective .

incremental cardinality constraints for maxsat
maximum satisfiability ( maxsat ) is an optimization variant of the boolean satisfiability ( sat ) problem . in general , maxsat algorithms perform a succession of sat solver calls to reach an optimum solution making extensive use of cardinality constraints . many of these algorithms are non-incremental in nature , i.e . at each iteration the formula is rebuilt and no knowledge is reused from one iteration to another . in this paper , we exploit the knowledge acquired across iterations using novel schemes to use cardinality constraints in an incremental fashion . we integrate these schemes with several maxsat algorithms . our experimental results show a significant performance boost for these algo- rithms as compared to their non-incremental counterparts . these results suggest that incremental cardinality constraints could be beneficial for other constraint solving domains .

optimal bangla keyboard layout using association rule of data mining
in this paper we present an optimal bangla keyboard layout , which distributes the load equally on both hands so that maximizing the ease and minimizing the effort . bangla alphabet has a large number of letters , for this it is difficult to type faster using bangla keyboard . our proposed keyboard will maximize the speed of operator as they can type with both hands parallel . here we use the association rule of data mining to distribute the bangla characters in the keyboard . first , we analyze the frequencies of data consisting of monograph , digraph and trigraph , which are derived from data wire-house , and then used association rule of data mining to distribute the bangla characters in the layout . finally , we propose a bangla keyboard layout . experimental results on several keyboard layout shows the effectiveness of the proposed approach with better performance .

dempster-shafer vs. probabilistic logic
the combination of evidence in dempster-shafer theory is compared with the combination of evidence in probabilistic logic . sufficient conditions are stated for these two methods to agree . it is then shown that these conditions are minimal in the sense that disagreement can occur when any one of them is removed . an example is given in which the traditional assumption of conditional independence of evidence on hypotheses holds and a uniform prior is assumed , but probabilistic logic and dempster 's rule give radically different results for the combination of two evidence events .

using soft computer techniques on smart devices for monitoring chronic diseases : the chronious case
chronious is an open , ubiquitous and adaptive chronic disease management platform for chronic obstructive pulmonary disease ( copd ) chronic kidney disease ( ckd ) and renal insufficiency . it consists of several modules : an ontology based literature search engine , a rule based decision support system , remote sensors interacting with lifestyle interfaces ( pda , monitor touchscreen ) and a machine learning module . all these modules interact each other to allow the monitoring of two types of chronic diseases and to help clinician in taking decision for cure purpose . this paper illustrates how some machine learning algorithms and a rule based decision support system can be used in smart devices , to monitor chronic patient . we will analyse how a set of machine learning algorithms can be used in smart devices to alert the clinician in case of a patient health condition worsening trend .

selecting attributes for sport forecasting using formal concept analysis
in order to address complex systems , apply pattern recongnition on their evolution could play an key role to understand their dynamics . global patterns are required to detect emergent concepts and trends , some of them with qualitative nature . formal concept analysis ( fca ) is a theory whose goal is to discover and to extract knowledge from qualitative data . it provides tools for reasoning with implication basis ( and association rules ) . implications and association rules are usefull to reasoning on previously selected attributes , providing a formal foundation for logical reasoning . in this paper we analyse how to apply fca reasoning to increase confidence in sports betting , by means of detecting temporal regularities from data . it is applied to build a knowledge-based system for confidence reasoning .

order-distance and other metric-like functions on jointly distributed random variables
we construct a class of real-valued nonnegative binary functions on a set of jointly distributed random variables , which satisfy the triangle inequality and vanish at identical arguments ( pseudo-quasi-metrics ) . these functions are useful in dealing with the problem of selective probabilistic causality encountered in behavioral sciences and in quantum physics . the problem reduces to that of ascertaining the existence of a joint distribution for a set of variables with known distributions of certain subsets of this set . any violation of the triangle inequality or its consequences by one of our functions when applied to such a set rules out the existence of this joint distribution . we focus on an especially versatile and widely applicable pseudo-quasi-metric called an order-distance and its special case called a classification distance .

using hankel matrices for dynamics-based facial emotion recognition and pain detection
this paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions . to this purpose , a sequence of face image descriptors ( fid ) is regarded as the output of a linear time invariant ( lti ) system . the temporal dynamics of such sequence of descriptors are represented by means of a hankel matrix . the paper presents different strategies to compute dynamics-based representation of a sequence of fid , and reports classification accuracy values of the proposed representations within different standard classification frameworks . the representations have been validated in two very challenging application domains : emotion recognition and pain detection . experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based fid representation attains competitive performance when off-the-shelf classification tools are adopted .

using a distributional semantic vector space with a knowledge base for reasoning in uncertain conditions
the inherent inflexibility and incompleteness of commonsense knowledge bases ( kb ) has limited their usefulness . we describe a system called displacer for performing kb queries extended with the analogical capabilities of the word2vec distributional semantic vector space ( dsvs ) . this allows the system to answer queries with information which was not contained in the original kb in any form . by performing analogous queries on semantically related terms and mapping their answers back into the context of the original query using displacement vectors , we are able to give approximate answers to many questions which , if posed to the kb alone , would return no results . we also show how the hand-curated knowledge in a kb can be used to increase the accuracy of a dsvs in solving analogy problems . in these ways , a kb and a dsvs can make up for each other 's weaknesses .

explaining the unexplained : a class-enhanced attentive response ( clear ) approach to understanding deep neural networks
in this work , we propose class-enhanced attentive response ( clear ) : an approach to visualize and understand the decisions made by deep neural networks ( dnns ) given a specific input . clear facilitates the visualization of attentive regions and levels of interest of dnns during the decision-making process . it also enables the visualization of the most dominant classes associated with these attentive regions of interest . as such , clear can mitigate some of the shortcomings of heatmap-based methods associated with decision ambiguity , and allows for better insights into the decision-making process of dnns . quantitative and qualitative experiments across three different datasets demonstrate the efficacy of clear for gaining a better understanding of the inner workings of dnns during the decision-making process .

interference effects in quantum belief networks
probabilistic graphical models such as bayesian networks are one of the most powerful structures known by the computer science community for deriving probabilistic inferences . however , modern cognitive psychology has revealed that human decisions could not follow the rules of classical probability theory , because humans can not process large amounts of data in order to make judgements . consequently , the inferences performed are based on limited data coupled with several heuristics , leading to violations of the law of total probability . this means that probabilistic graphical models based on classical probability theory are too limited to fully simulate and explain various aspects of human decision making . quantum probability theory was developed in order to accommodate the paradoxical findings that the classical theory could not explain . recent findings in cognitive psychology revealed that quantum probability can fully describe human decisions in an elegant framework . their findings suggest that , before taking a decision , human thoughts are seen as superposed waves that can interfere with each other , influencing the final decision . in this work , we propose a new bayesian network based on the psychological findings of cognitive scientists . we made experiments with two very well known bayesian networks from the literature . the results obtained revealed that the quantum like bayesian network can affect drastically the probabilistic inferences , specially when the levels of uncertainty of the network are very high ( no pieces of evidence observed ) . when the levels of uncertainty are very low , then the proposed quantum like network collapses to its classical counterpart .

a variational approach for approximating bayesian networks by edge deletion
we consider in this paper the formulation of approximate inference in bayesian networks as a problem of exact inference on an approximate network that results from deleting edges ( to reduce treewidth ) . we have shown in earlier work that deleting edges calls for introducing auxiliary network parameters to compensate for lost dependencies , and proposed intuitive conditions for determining these parameters . we have also shown that our method corresponds to ibp when enough edges are deleted to yield a polytree , and corresponds to some generalizations of ibp when fewer edges are deleted . in this paper , we propose a different criteria for determining auxiliary parameters based on optimizing the kl-divergence between the original and approximate networks . we discuss the relationship between the two methods for selecting parameters , shedding new light on ibp and its generalizations . we also discuss the application of our new method to approximating inference problems which are exponential in constrained treewidth , including map and nonmyopic value of information .

a tabu search algorithm for the multi-period inspector scheduling problem
this paper introduces a multi-period inspector scheduling problem ( mpisp ) , which is a new variant of the multi-trip vehicle routing problem with time windows ( vrptw ) . in the mpisp , each inspector is scheduled to perform a route in a given multi-period planning horizon . at the end of each period , each inspector is not required to return to the depot but has to stay at one of the vertices for recuperation . if the remaining time of the current period is insufficient for an inspector to travel from his/her current vertex $ a $ to a certain vertex b , he/she can choose either waiting at vertex a until the start of the next period or traveling to a vertex c that is closer to vertex b. therefore , the shortest transit time between any vertex pair is affected by the length of the period and the departure time . we first describe an approach of computing the shortest transit time between any pair of vertices with an arbitrary departure time . to solve the mpisp , we then propose several local search operators adapted from classical operators for the vrptw and integrate them into a tabu search framework . in addition , we present a constrained knapsack model that is able to produce an upper bound for the problem . finally , we evaluate the effectiveness of our algorithm with extensive experiments based on a set of test instances . our computational results indicate that our approach generates high-quality solutions .

an iterative school decomposition algorithm for solving the multi-school bus routing and scheduling problem
servicing the school transportation demand safely with a minimum number of buses is one of the highest financial goals for school transportation directors . to achieve that objective , a good and efficient way to solve the routing and scheduling problem is required . due to the growth of the computing power , the spotlight has been shed on solving the combined problem of the school bus routing and scheduling . a recent attempt tried to model the routing problem by maximizing the trip compatibilities with the hope of requiring fewer buses in the scheduling problem . however , an over-counting problem associated with trip compatibility could diminish the performance of this approach . an extended model is proposed in this paper to resolve this issue along with an iterative solution algorithm . this extended model is an integrated model for multi-school bus routing and scheduling problem . the result shows better solutions for 8 test problems can be found with a fewer number of buses ( up to 25 % ) and shorter travel time ( up to 7 % per trip ) .

towards a self-organized agent-based simulation model for exploration of human synaptic connections
in this paper , the early design of our self-organized agent-based simulation model for exploration of synaptic connections that faithfully generates what is observed in natural situation is given . while we take inspiration from neuroscience , our intent is not to create a veridical model of processes in neurodevelopmental biology , nor to represent a real biological system . instead , our goal is to design a simulation model that learns acting in the same way of human nervous system by using findings on human subjects using reflex methodologies in order to estimate unknown connections .

learning to order things
there are many applications in which it is desirable to order rather than classify instances . here we consider the problem of learning how to order instances given feedback in the form of preference judgments , i.e. , statements to the effect that one instance should be ranked ahead of another . we outline a two-stage approach in which one first learns by conventional means a binary preference function indicating whether it is advisable to rank one instance before another . here we consider an on-line algorithm for learning preference functions that is based on freund and schapire 's 'hedge ' algorithm . in the second stage , new instances are ordered so as to maximize agreement with the learned preference function . we show that the problem of finding the ordering that agrees best with a learned preference function is np-complete . nevertheless , we describe simple greedy algorithms that are guaranteed to find a good approximation . finally , we show how metasearch can be formulated as an ordering problem , and present experimental results on learning a combination of 'search experts ' , each of which is a domain-specific query expansion strategy for a web search engine .

where is my forearm ? clustering of body parts from simultaneous tactile and linguistic input using sequential mapping
humans and animals are constantly exposed to a continuous stream of sensory information from different modalities . at the same time , they form more compressed representations like concepts or symbols . in species that use language , this process is further structured by this interaction , where a mapping between the sensorimotor concepts and linguistic elements needs to be established . there is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts ( cross-situational learning ) . in existing models , the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co-occurrences . in this paper , we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available matlab implementation . for demonstration , we have chosen a less typical scenario : instead of learning to associate objects with their names , we focus on body representations . a humanoid robot is receiving tactile stimulations on its body , while at the same time listening to utterances of the body part names ( e.g. , hand , forearm and torso ) . with the goal at arriving at the correct `` body categories '' , we demonstrate how a sequential mapping algorithm outperforms one-step mapping . in addition , the effect of data set size and noise in the linguistic input are studied .

extracting actionability from machine learning models by sub-optimal deterministic planning
a main focus of machine learning research has been improving the generalization accuracy and efficiency of prediction models . many models such as svm , random forest , and deep neural nets have been proposed and achieved great success . however , what emerges as missing in many applications is actionability , i.e. , the ability to turn prediction results into actions . for example , in applications such as customer relationship management , clinical prediction , and advertisement , the users need not only accurate prediction , but also actionable instructions which can transfer an input to a desirable goal ( e.g. , higher profit repays , lower morbidity rates , higher ads hit rates ) . existing effort in deriving such actionable knowledge is few and limited to simple action models which restricted to only change one attribute for each action . the dilemma is that in many real applications those action models are often more complex and harder to extract an optimal solution . in this paper , we propose a novel approach that achieves actionability by combining learning with planning , two core areas of ai . in particular , we propose a framework to extract actionable knowledge from random forest , one of the most widely used and best off-the-shelf classifiers . we formulate the actionability problem to a sub-optimal action planning ( soap ) problem , which is to find a plan to alter certain features of a given input so that the random forest would yield a desirable output , while minimizing the total costs of actions . technically , the soap problem is formulated in the sas+ planning formalism , and solved using a max-sat based approach . our experimental results demonstrate the effectiveness and efficiency of the proposed approach on a personal credit dataset and other benchmarks . our work represents a new application of automated planning on an emerging and challenging machine learning paradigm .

recursion in rdf data shape languages
an rdf data shape is a description of the expected contents of an rdf document ( aka graph ) or dataset . a major part of this description is the set of constraints that the document or dataset is required to satisfy . w3c recently ( 2014 ) chartered the rdf data shapes working group to define shacl , a standard rdf data shape language . we refer to the ability to name and reference shape language elements as recursion . this article provides a precise definition of the meaning of recursion as used in resource shape 2.0. the definition of recursion presented in this article is largely independent of language-specific details . we speculate that it also applies to shex and to all three of the current proposals for shacl . in particular , recursion is not permitted in the shacl-sparql proposal , but we conjecture that recursion could be added by using the definition proposed here as a top-level control structure .

people on drugs : credibility of user statements in health communities
online health communities are a valuable source of information for patients and physicians . however , such user-generated resources are often plagued by inaccuracies and misinformation . in this work we propose a method for automatically establishing the credibility of user-generated medical statements and the trustworthiness of their authors by exploiting linguistic cues and distant supervision from expert sources . to this end we introduce a probabilistic graphical model that jointly learns user trustworthiness , statement credibility , and language objectivity . we apply this methodology to the task of extracting rare or unknown side-effects of medical drugs -- - this being one of the problems where large scale non-expert data has the potential to complement expert medical knowledge . we show that our method can reliably extract side-effects and filter out false statements , while identifying trustworthy users that are likely to contribute valuable medical information .

numerical sensitivity and efficiency in the treatment of epistemic and aleatory uncertainty
the treatment of both aleatory and epistemic uncertainty by recent methods often requires an high computational effort . in this abstract , we propose a numerical sampling method allowing to lighten the computational burden of treating the information by means of so-called fuzzy random variables .

learning belief networks in domains with recursively embedded pseudo independent submodels
a pseudo independent ( pi ) model is a probabilistic domain model ( pdm ) where proper subsets of a set of collectively dependent variables display marginal independence . pi models can not be learned correctly by many algorithms that rely on a single link search . earlier work on learning pi models has suggested a straightforward multi-link search algorithm . however , when a domain contains recursively embedded pi submodels , it may escape the detection of such an algorithm . in this paper , we propose an improved algorithm that ensures the learning of all embedded pi submodels whose sizes are upper bounded by a predetermined parameter . we show that this improved learning capability only increases the complexity slightly beyond that of the previous algorithm . the performance of the new algorithm is demonstrated through experiment .

aclp : integrating abduction and constraint solving
aclp is a system which combines abductive reasoning and constraint solving by integrating the frameworks of abductive logic programming ( alp ) and constraint logic programming ( clp ) . it forms a general high-level knowledge representation environment for abductive problems in artificial intelligence and other areas . in aclp , the task of abduction is supported and enhanced by its non-trivial integration with constraint solving facilitating its application to complex problems . the aclp system is currently implemented on top of the clp language of eclipse as a meta-interpreter exploiting its underlying constraint solver for finite domains . it has been applied to the problems of planning and scheduling in order to test its computational effectiveness compared with the direct use of the ( lower level ) constraint solving framework of clp on which it is built . these experiments provide evidence that the abductive framework of aclp does not compromise significantly the computational efficiency of the solutions . other experiments show the natural ability of aclp to accommodate easily and in a robust way new or changing requirements of the original problem .

belief decision support and reject for textured images characterization
the textured images ' classification assumes to consider the images in terms of area with the same texture . in uncertain environment , it could be better to take an imprecise decision or to reject the area corresponding to an unlearning class . moreover , on the areas that are the classification units , we can have more than one texture . these considerations allows us to develop a belief decision model permitting to reject an area as unlearning and to decide on unions and intersections of learning classes . the proposed approach finds all its justification in an application of seabed characterization from sonar images , which contributes to an illustration .

antids : self-organized ant-based clustering model for intrusion detection system
security of computers and the networks that connect them is increasingly becoming of great significance . computer security is defined as the protection of computing systems against threats to confidentiality , integrity , and availability . there are two types of intruders : the external intruders who are unauthorized users of the machines they attack , and internal intruders , who have permission to access the system with some restrictions . due to the fact that it is more and more improbable to a system administrator to recognize and manually intervene to stop an attack , there is an increasing recognition that id systems should have a lot to earn on following its basic principles on the behavior of complex natural systems , namely in what refers to self-organization , allowing for a real distributed and collective perception of this phenomena . with that aim in mind , the present work presents a self-organized ant colony based intrusion detection system ( antids ) to detect intrusions in a network infrastructure . the performance is compared among conventional soft computing paradigms like decision trees , support vector machines and linear genetic programming to model fast , online and efficient intrusion detection systems .

a paraconsistent tableau algorithm based on sign transformation in semantic web
in an open , constantly changing and collaborative environment like the forthcoming semantic web , it is reasonable to expect that knowledge sources will contain noise and inaccuracies . it is well known , as the logical foundation of the semantic web , description logic is lack of the ability of tolerating inconsistent or incomplete data . recently , the ability of paraconsistent approaches in semantic web is weaker in this paper , we present a tableau algorithm based on sign transformation in semantic web which holds the stronger ability of reasoning . we prove that the tableau algorithm is decidable which hold the same function of classical tableau algorithm for consistent knowledge bases .

cooperation between top-down and bottom-up theorem provers
top-down and bottom-up theorem proving approaches each have specific advantages and disadvantages . bottom-up provers profit from strong redundancy control but suffer from the lack of goal-orientation , whereas top-down provers are goal-oriented but often have weak calculi when their proof lengths are considered . in order to integrate both approaches , we try to achieve cooperation between a top-down and a bottom-up prover in two different ways : the first technique aims at supporting a bottom-up with a top-down prover . a top-down prover generates subgoal clauses , they are then processed by a bottom-up prover . the second technique deals with the use of bottom-up generated lemmas in a top-down prover . we apply our concept to the areas of model elimination and superposition . we discuss the ability of our techniques to shorten proofs as well as to reorder the search space in an appropriate manner . furthermore , in order to identify subgoal clauses and lemmas which are actually relevant for the proof task , we develop methods for a relevancy-based filtering . experiments with the provers setheo and spass performed in the problem library tptp reveal the high potential of our cooperation approaches .

a human-grounded evaluation benchmark for local explanations of machine learning
in order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making , people need to be able to understand the machine rationale for given output . research in explain artificial intelligence ( xai ) addresses the aim , but there is a need for evaluation of human relevance and understandability of explanations . our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models . we present an evaluation benchmark for instance explanations from text and image classifiers . the explanation meta-data in this benchmark is generated from user annotations of image and text samples . we describe the benchmark and demonstrate its utility by a quantitative evaluation on explanations generated from a recent machine learning algorithm . this research demonstrates how human-grounded evaluation could be used as a measure to qualify local machine-learning explanations .

active learning with statistical models
for many types of machine learning algorithms , one can compute the statistically ` optimal ' way to select training data . in this paper , we review how optimal data selection techniques have been used with feedforward neural networks . we then show how the same principles may be used to select data for two alternative , statistically-based learning architectures : mixtures of gaussians and locally weighted regression . while the techniques for neural networks are computationally expensive and approximate , the techniques for mixtures of gaussians and locally weighted regression are both efficient and accurate . empirically , we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance .

analogy perception applied to seven tests of word comprehension
it has been argued that analogy is the core of cognition . in ai research , algorithms for analogy are often limited by the need for hand-coded high-level representations as input . an alternative approach is to use high-level perception , in which high-level representations are automatically generated from raw data . analogy perception is the process of recognizing analogies using high-level perception . we present pairclass , an algorithm for analogy perception that recognizes lexical proportional analogies using representations that are automatically generated from a large corpus of raw textual data . a proportional analogy is an analogy of the form a : b : :c : d , meaning `` a is to b as c is to d '' . a lexical proportional analogy is a proportional analogy with words , such as carpenter : wood : :mason : stone . pairclass represents the semantic relations between two words using a high-dimensional feature vector , in which the elements are based on frequencies of patterns in the corpus . pairclass recognizes analogies by applying standard supervised machine learning techniques to the feature vectors . we show how seven different tests of word comprehension can be framed as problems of analogy perception and we then apply pairclass to the seven resulting sets of analogy perception problems . we achieve competitive results on all seven tests . this is the first time a uniform approach has handled such a range of tests of word comprehension .

quantification of sand fraction from seismic attributes using neuro-fuzzy approach
in this paper , we illustrate the modeling of a reservoir property ( sand fraction ) from seismic attributes namely seismic impedance , seismic amplitude , and instantaneous frequency using neuro-fuzzy ( nf ) approach . input dataset includes 3d post-stacked seismic attributes and six well logs acquired from a hydrocarbon field located in the western coast of india . presence of thin sand and shale layers in the basin area makes the modeling of reservoir characteristic a challenging task . though seismic data is helpful in extrapolation of reservoir properties away from boreholes ; yet , it could be challenging to delineate thin sand and shale reservoirs using seismic data due to its limited resolvability . therefore , it is important to develop state-of-art intelligent methods for calibrating a nonlinear mapping between seismic data and target reservoir variables . neural networks have shown its potential to model such nonlinear mappings ; however , uncertainties associated with the model and datasets are still a concern . hence , introduction of fuzzy logic ( fl ) is beneficial for handling these uncertainties . more specifically , hybrid variants of artificial neural network ( ann ) and fuzzy logic , i.e. , nf methods , are capable for the modeling reservoir characteristics by integrating the explicit knowledge representation power of fl with the learning ability of neural networks . the documented results in this study demonstrate acceptable resemblance between target and predicted variables , and hence , encourage the application of integrated machine learning approaches such as neuro-fuzzy in reservoir characterization domain . furthermore , visualization of the variation of sand probability in the study area would assist in identifying placement of potential wells for future drilling operations .

semantic code repair using neuro-symbolic transformation networks
we study the problem of semantic code repair , which can be broadly defined as automatically fixing non-syntactic bugs in source code . the majority of past work in semantic code repair assumed access to unit tests against which candidate repairs could be validated . in contrast , the goal here is to develop a strong statistical model to accurately predict both bug locations and exact fixes without access to information about the intended correct behavior of the program . achieving such a goal requires a robust contextual repair model , which we train on a large corpus of real-world source code that has been augmented with synthetically injected bugs . our framework adopts a two-stage approach where first a large set of repair candidates are generated by rule-based processors , and then these candidates are scored by a statistical model using a novel neural network architecture which we refer to as share , specialize , and compete . specifically , the architecture ( 1 ) generates a shared encoding of the source code using an rnn over the abstract syntax tree , ( 2 ) scores each candidate repair using specialized network modules , and ( 3 ) then normalizes these scores together so they can compete against one another in comparable probability space . we evaluate our model on a real-world test set gathered from github containing four common categories of bugs . our model is able to predict the exact correct repair 41\ % of the time with a single guess , compared to 13\ % accuracy for an attentional sequence-to-sequence model .

probabilistic relational planning with first order decision diagrams
dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations , in particular algebraic decision diagrams , to capture domain dynamics and value functions . work on symbolic dynamic programming lifted these ideas to first order logic using several representation schemes . recent work introduced a first order variant of decision diagrams ( fodd ) and developed a value iteration algorithm for this representation . this paper develops several improvements to the fodd algorithm that make the approach practical . these include , new reduction operators that decrease the size of the representation , several speedup techniques , and techniques for value approximation . incorporating these , the paper presents a planning system , fodd-planner , for solving relational stochastic planning problems . the system is evaluated on several domains , including problems from the recent international planning competition , and shows competitive performance with top ranking systems . this is the first demonstration of feasibility of this approach and it shows that abstraction through compact representation is a promising approach to stochastic planning .

what does a belief function believe in ?
the conditioning in the dempster-shafer theory of evidence has been defined ( by shafer \cite { shafer:90 } as combination of a belief function and of an `` event '' via dempster rule . on the other hand shafer \cite { shafer:90 } gives a `` probabilistic '' interpretation of a belief function ( hence indirectly its derivation from a sample ) . given the fact that conditional probability distribution of a sample-derived probability distribution is a probability distribution derived from a subsample ( selected on the grounds of a conditioning event ) , the paper investigates the empirical nature of the dempster- rule of combination . it is demonstrated that the so-called `` conditional '' belief function is not a belief function given an event but rather a belief function given manipulation of original empirical data.\\ given this , an interpretation of belief function different from that of shafer is proposed . algorithms for construction of belief networks from data are derived for this interpretation .

combining fuzzy cognitive maps and discrete random variables
in this paper we propose an extension to the fuzzy cognitive maps ( fcms ) that aims at aggregating a number of reasoning tasks into a one parallel run . the described approach consists in replacing real-valued activation levels of concepts ( and further influence weights ) by random variables . such extension , followed by the implemented software tool , allows for determining ranges reached by concept activation levels , sensitivity analysis as well as statistical analysis of multiple reasoning results . we replace multiplication and addition operators appearing in the fcm state equation by appropriate convolutions applicable for discrete random variables . to make the model computationally feasible , it is further augmented with aggregation operations for discrete random variables . we discuss four implemented aggregators , as well as we report results of preliminary tests .

the assumptions behind dempster 's rule
this paper examines the concept of a combination rule for belief functions . it is shown that two fairly simple and apparently reasonable assumptions determine dempster 's rule , giving a new justification for it .

knowledge-based programs as plans : succinctness and the complexity of plan existence
knowledge-based programs ( kbps ) are high-level protocols describing the course of action an agent should perform as a function of its knowledge . the use of kbps for expressing action policies in ai planning has been surprisingly overlooked . given that to each kbp corresponds an equivalent plan and vice versa , kbps are typically more succinct than standard plans , but imply more on-line computation time . here we make this argument formal , and prove that there exists an exponential succinctness gap between knowledge-based programs and standard plans . then we address the complexity of plan existence . some results trivially follow from results already known from the literature on planning under incomplete knowledge , but many were unknown so far .

deep learning for identifying potential conceptual shifts for co-creative drawing
we present a system for identifying conceptual shifts between visual categories , which will form the basis for a co-creative drawing system to help users draw more creative sketches . the system recognizes human sketches and matches them to structurally similar sketches from categories to which they do not belong . this would allow a co-creative drawing system to produce an ambiguous sketch that blends features from both categories .

generating natural questions about an image
there has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription , and answering questions about images . these tasks have focused on literal descriptions of the image . to move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image . in this paper , we introduce the novel task of visual question generation ( vqg ) , where the system is tasked with asking a natural and engaging question when shown an image . we provide three datasets which cover a variety of images from object-centric to event-centric , with considerably more abstract training data than provided to state-of-the-art captioning systems thus far . we train and test several generative and retrieval models to tackle the task of vqg . evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics . our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language .

towards full automated drive in urban environments : a demonstration in gomentum station , california
each year , millions of motor vehicle traffic accidents all over the world cause a large number of fatalities , injuries and significant material loss . automated driving ( ad ) has potential to drastically reduce such accidents . in this work , we focus on the technical challenges that arise from ad in urban environments . we present the overall architecture of an ad system and describe in detail the perception and planning modules . the ad system , built on a modified acura rlx , was demonstrated in a course in gomentum station in california . we demonstrated autonomous handling of 4 scenarios : traffic lights , cross-traffic at intersections , construction zones and pedestrians . the ad vehicle displayed safe behavior and performed consistently in repeated demonstrations with slight variations in conditions . overall , we completed 44 runs , encompassing 110km of automated driving with only 3 cases where the driver intervened the control of the vehicle , mostly due to error in gps positioning . our demonstration showed that robust and consistent behavior in urban scenarios is possible , yet more investigation is necessary for full scale roll-out on public roads .

knowledge representation for robots through human-robot interaction
the representation of the knowledge needed by a robot to perform complex tasks is restricted by the limitations of perception . one possible way of overcoming this situation and designing `` knowledgeable '' robots is to rely on the interaction with the user . we propose a multi-modal interaction framework that allows to effectively acquire knowledge about the environment where the robot operates . in particular , in this paper we present a rich representation framework that can be automatically built from the metric map annotated with the indications provided by the user . such a representation , allows then the robot to ground complex referential expressions for motion commands and to devise topological navigation plans to achieve the target locations .

the representation of legal contracts
the paper outlines ongoing research on logic-based tools for the analysis and representation of legal contracts of the kind frequently encountered in large-scale engineering projects and complex , long-term trading agreements . we consider both contract formation and contract performance , in each case identifying the representational issues and the prospects for providing automated support tools .

regret-based multi-agent coordination with uncertain task rewards
many multi-agent coordination problems can be represented as dcops . motivated by task allocation in disaster response , we extend standard dcop models to consider uncertain task rewards where the outcome of completing a task depends on its current state , which is randomly drawn from unknown distributions . the goal of solving this problem is to find a solution for all agents that minimizes the overall worst-case loss . this is a challenging problem for centralized algorithms because the search space grows exponentially with the number of agents and is nontrivial for standard dcop algorithms we have . to address this , we propose a novel decentralized algorithm that incorporates max-sum with iterative constraint generation to solve the problem by passing messages among agents . by so doing , our approach scales well and can solve instances of the task allocation problem with hundreds of agents and tasks .

fast stochastic variance reduced gradient method with momentum acceleration for machine learning
recently , research on accelerated stochastic gradient descent methods ( e.g. , svrg ) has made exciting progress ( e.g. , linear convergence for strongly convex problems ) . however , the best-known methods ( e.g. , katyusha ) requires at least two auxiliary variables and two momentum parameters . in this paper , we propose a fast stochastic variance reduction gradient ( fsvrg ) method , in which we design a novel update rule with the nesterov 's momentum and incorporate the technique of growing epoch size . fsvrg has only one auxiliary variable and one momentum weight , and thus it is much simpler and has much lower per-iteration complexity . we prove that fsvrg achieves linear convergence for strongly convex problems and the optimal $ \mathcal { o } ( 1/t^2 ) $ convergence rate for non-strongly convex problems , where $ t $ is the number of outer-iterations . we also extend fsvrg to directly solve the problems with non-smooth component functions , such as svm . finally , we empirically study the performance of fsvrg for solving various machine learning problems such as logistic regression , ridge regression , lasso and svm . our results show that fsvrg outperforms the state-of-the-art stochastic methods , including katyusha .

embedding default logic in propositional argumentation systems
in this paper we present a transformation of finite propositional default theories into so-called propositional argumentation systems . this transformation allows to characterize all notions of reiter 's default logic in the framework of argumentation systems . as a consequence , computing extensions , or determining wether a given formula belongs to one extension or all extensions can be answered without leaving the field of classical propositional logic . the transformation proposed is linear in the number of defaults .

text-based lstm networks for automatic music composition
in this paper , we introduce new methods and discuss results of text-based lstm ( long short-term memory ) networks for automatic music composition . the proposed network is designed to learn relationships within text documents that represent chord progressions and drum tracks in two case studies . in the experiments , word-rnns ( recurrent neural networks ) show good results for both cases , while character-based rnns ( char-rnns ) only succeed to learn chord progressions . the proposed system can be used for fully automatic composition or as semi-automatic systems that help humans to compose music by controlling a diversity parameter of the model .

multi-dimensional parametric mincuts for constrained map inference
in this paper , we propose novel algorithms for inferring the maximum a posteriori ( map ) solution of discrete pairwise random field models under multiple constraints . we show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its lagrangian dual , and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly . these multiple solutions enable us to even deal with ` soft constraints ' ( higher order penalty functions ) . moreover , we propose two practical variants of our algorithm to solve problems with hard constraints . we also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation . experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods .

proceedings of the sixteenth conference on uncertainty in artificial intelligence ( 2000 )
this is the proceedings of the sixteenth conference on uncertainty in artificial intelligence , which was held in san francisco , ca , june 30 - july 3 , 2000

people are strange when you 're a stranger : impact and influence of bots on social networks
bots are , for many web and social media users , the source of many dangerous attacks or the carrier of unwanted messages , such as spam . nevertheless , crawlers and software agents are a precious tool for analysts , and they are continuously executed to collect data or to test distributed applications . however , no one knows which is the real potential of a bot whose purpose is to control a community , to manipulate consensus , or to influence user behavior . it is commonly believed that the better an agent simulates human behavior in a social network , the more it can succeed to generate an impact in that community . we contribute to shed light on this issue through an online social experiment aimed to study to what extent a bot with no trust , no profile , and no aims to reproduce human behavior , can become popular and influential in a social media . results show that a basic social probing activity can be used to acquire social relevance on the network and that the so-acquired popularity can be effectively leveraged to drive users in their social connectivity choices . we also register that our bot activity unveiled hidden social polarization patterns in the community and triggered an emotional response of individuals that brings to light subtle privacy hazards perceived by the user base .

on the semantics and automated deduction for plfc , a logic of possibilistic uncertainty and fuzziness
possibilistic logic is a well-known graded logic of uncertainty suitable to reason under incomplete information and partially inconsistent knowledge , which is built upon classical first order logic . there exists for possibilistic logic a proof procedure based on a refutation complete resolution-style calculus . recently , a syntactical extension of first order possibilistic logic ( called plfc ) dealing with fuzzy constants and fuzzily restricted quantifiers has been proposed . our aim is to present steps towards both the formalization of plfc itself and an automated deduction system for it by ( i ) providing a formal semantics ; ( ii ) defining a sound resolution-style calculus by refutation ; and ( iii ) describing a first-order proof procedure for plfc clauses based on ( ii ) and on a novel notion of most general substitution of two literals in a resolution step . in contrast to standard possibilistic logic semantics , truth-evaluation of formulas with fuzzy constants are many-valued instead of boolean , and consequently an extended notion of possibilistic uncertainty is also needed .

an experimental comparison of numerical and qualitative probabilistic reasoning
qualitative and infinitesimal probability schemes are consistent with the axioms of probability theory , but avoid the need for precise numerical probabilities . using qualitative probabilities could substantially reduce the effort for knowledge engineering and improve the robustness of results . we examine experimentally how well infinitesimal probabilities ( the kappa-calculus of goldszmidt and pearl ) perform a diagnostic task - troubleshooting a car that will not start by comparison with a conventional numerical belief network . we found the infinitesimal scheme to be as good as the numerical scheme in identifying the true fault . the performance of the infinitesimal scheme worsens significantly for prior fault probabilities greater than 0.03. these results suggest that infinitesimal probability methods may be of substantial practical value for machine diagnosis with small prior fault probabilities .

exploiting layerwise convexity of rectifier networks with sign constrained weights
by introducing sign constraints on the weights , this paper proposes sign constrained rectifier networks ( scrns ) , whose training can be solved efficiently by the well known majorization-minimization ( mm ) algorithms . we prove that the proposed two-hidden-layer scrns , which exhibit negative weights in the second hidden layer and negative weights in the output layer , are capable of separating any two ( or more ) disjoint pattern sets . furthermore , the proposed two-hidden-layer scrns can decompose the patterns of each class into several clusters so that each cluster is convexly separable from all the patterns from the other classes . this provides a means to learn the pattern structures and analyse the discriminant factors between different classes of patterns .

cooperative multi-agent planning : a survey
cooperative multi-agent planning ( map ) is a relatively recent research field that combines technologies , algorithms and techniques developed by the artificial intelligence planning and multi-agent systems communities . while planning has been generally treated as a single-agent task , map generalizes this concept by considering multiple intelligent agents that work cooperatively to develop a course of action that satisfies the goals of the group . this paper reviews the most relevant approaches to map , putting the focus on the solvers that took part in the 2015 competition of distributed and multi-agent planning , and classifies them according to their key features and relative performance .

case-based merging techniques in oakplan
case-based planning can take advantage of former problem-solving experiences by storing in a plan library previously generated plans that can be reused to solve similar planning problems in the future . although comparative worst-case complexity analyses of plan generation and reuse techniques reveal that it is not possible to achieve provable efficiency gain of reuse over generation , we show that the case-based planning approach can be an effective alternative to plan generation when similar reuse candidates can be chosen .

progressive reinforcement learning with distillation for multi-skilled motion control
deep reinforcement learning has demonstrated increasing capabilities for continuous control problems , including agents that can move with skill and agility through their environment . an open problem in this setting is that of developing good strategies for integrating or merging policies for multiple skills , where each individual skill is a specialist in a specific skill and its associated state distribution . we extend policy distillation methods to the continuous action setting and leverage this technique to combine expert policies , as evaluated in the domain of simulated bipedal locomotion across different classes of terrain . we also introduce an input injection method for augmenting an existing policy network to exploit new input features . lastly , our method uses transfer learning to assist in the efficient acquisition of new skills . the combination of these methods allows a policy to be incrementally augmented with new skills . we compare our progressive learning and integration via distillation ( plaid ) method against three alternative baselines .

efficient stepwise selection in decomposable models
in this paper , we present an efficient way of performing stepwise selection in the class of decomposable models . the main contribution of the paper is a simple characterization of the edges that canbe added to a decomposable model while keeping the resulting model decomposable and an efficient algorithm for enumerating all such edges for a given model in essentially o ( 1 ) time per edge . we also discuss how backward selection can be performed efficiently using our data structures.we also analyze the complexity of the complete stepwise selection procedure , including the complexity of choosing which of the eligible dges to add to ( or delete from ) the current model , with the aim ofminimizing the kullback-leibler distance of the resulting model from the saturated model for the data .

moments in time dataset : one million videos for event understanding
we present the moments in time dataset , a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds . modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges : meaningful events do not include only people , but also objects , animals , and natural phenomena ; visual and auditory events can be symmetrical or not in time ( `` opening '' means `` closing '' in reverse order ) , and transient or sustained . we describe the annotation process of our dataset ( each video is tagged with one action or activity label among 339 different classes ) , analyze its scale and diversity in comparison to other large-scale video datasets for action recognition , and report results of several baseline models addressing separately and jointly three modalities : spatial , temporal and auditory . the moments in time dataset designed to have a large coverage and diversity of events in both visual and auditory modalities , can serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis .

general ai challenge - round one : gradual learning
the general ai challenge is an initiative to encourage the wider artificial intelligence community to focus on important problems in building intelligent machines with more general scope than is currently possible . the challenge comprises of multiple rounds , with the first round focusing on gradual learning , i.e . the ability to re-use already learned knowledge for efficiently learning to solve subsequent problems . in this article , we will present details of the first round of the challenge , its inspiration and aims . we also outline a more formal description of the challenge and present a preliminary analysis of its curriculum , based on ideas from computational mechanics . we believe , that such formalism will allow for a more principled approach towards investigating tasks in the challenge , building new curricula and for potentially improving consequent challenge rounds .

dempster-shafer clustering using potts spin mean field theory
in this article we investigate a problem within dempster-shafer theory where 2**q - 1 pieces of evidence are clustered into q clusters by minimizing a metaconflict function , or equivalently , by minimizing the sum of weight of conflict over all clusters . previously one of us developed a method based on a hopfield and tank model . however , for very large problems we need a method with lower computational complexity . we demonstrate that the weight of conflict of evidence can , as an approximation , be linearized and mapped to an antiferromagnetic potts spin model . this facilitates efficient numerical solution , even for large problem sizes . optimal or nearly optimal solutions are found for dempster-shafer clustering benchmark tests with a time complexity of approximately o ( n**2 log**2 n ) . furthermore , an isomorphism between the antiferromagnetic potts spin model and a graph optimization problem is shown . the graph model has dynamic variables living on the links , which have a priori probabilities that are directly related to the pairwise conflict between pieces of evidence . hence , the relations between three different models are shown .

augmenting end-to-end dialog systems with commonsense knowledge
building dialog agents that can converse naturally with humans is a challenging yet intriguing problem of artificial intelligence . in open-domain human-computer conversation , where the conversational agent is expected to respond to human responses in an interesting and engaging way , commonsense knowledge has to be integrated into the model effectively . in this paper , we investigate the impact of providing commonsense knowledge about the concepts covered in the dialog . our model represents the first attempt to integrating a large commonsense knowledge base into end-to-end conversational models . in the retrieval-based scenario , we propose the tri-lstm model to jointly take into account message and commonsense for selecting an appropriate response . our experiments suggest that the knowledge-augmented models are superior to their knowledge-free counterparts in automatic evaluation .

lazily adapted constant kinky inference for nonparametric regression and model-reference adaptive control
techniques known as nonlinear set membership prediction , lipschitz interpolation or kinky inference are approaches to machine learning that utilise presupposed lipschitz properties to compute inferences over unobserved function values . provided a bound on the true best lipschitz constant of the target function is known a priori they offer convergence guarantees as well as bounds around the predictions . considering a more general setting that builds on hoelder continuity relative to pseudo-metrics , we propose an online method for estimating the hoelder constant online from function value observations that possibly are corrupted by bounded observational errors . utilising this to compute adaptive parameters within a kinky inference rule gives rise to a nonparametric machine learning method , for which we establish strong universal approximation guarantees . that is , we show that our prediction rule can learn any continuous function in the limit of increasingly dense data to within a worst-case error bound that depends on the level of observational uncertainty . we apply our method in the context of nonparametric model-reference adaptive control ( mrac ) . across a range of simulated aircraft roll-dynamics and performance metrics our approach outperforms recently proposed alternatives that were based on gaussian processes and rbf-neural networks . for discrete-time systems , we provide stability guarantees for our learning-based controllers both for the batch and the online learning setting .

management of uncertainty in the multi-level monitoring and diagnosis of the time of flight scintillation array
we present a general architecture for the monitoring and diagnosis of large scale sensor-based systems with real time diagnostic constraints . this architecture is multileveled , combining a single monitoring level based on statistical methods with two model based diagnostic levels . at each level , sources of uncertainty are identified , and integrated methodologies for uncertainty management are developed . the general architecture was applied to the monitoring and diagnosis of a specific nuclear physics detector at lawrence berkeley national laboratory that contained approximately 5000 components and produced over 500 channels of output data . the general architecture is scalable , and work is ongoing to apply it to detector systems one and two orders of magnitude more complex .

message-passing algorithms : reparameterizations and splittings
the max-product algorithm , a local message-passing scheme that attempts to compute the most probable assignment ( map ) of a given probability distribution , has been successfully employed as a method of approximate inference for applications arising in coding theory , computer vision , and machine learning . however , the max-product algorithm is not guaranteed to converge to the map assignment , and if it does , is not guaranteed to recover the map assignment . alternative convergent message-passing schemes have been proposed to overcome these difficulties . this work provides a systematic study of such message-passing algorithms that extends the known results by exhibiting new sufficient conditions for convergence to local and/or global optima , providing a combinatorial characterization of these optima based on graph covers , and describing a new convergent and correct message-passing algorithm whose derivation unifies many of the known convergent message-passing algorithms . while convergent and correct message-passing algorithms represent a step forward in the analysis of max-product style message-passing algorithms , the conditions needed to guarantee convergence to a global optimum can be too restrictive in both theory and practice . this limitation of convergent and correct message-passing schemes is characterized by graph covers and illustrated by example .

interval structure : a framework for representing uncertain information
in this paper , a unified framework for representing uncertain information based on the notion of an interval structure is proposed . it is shown that the lower and upper approximations of the rough-set model , the lower and upper bounds of incidence calculus , and the belief and plausibility functions all obey the axioms of an interval structure . an interval structure can be used to synthesize the decision rules provided by the experts . an efficient algorithm to find the desirable set of rules is developed from a set of sound and complete inference axioms .

$ \mathbf { d^3 } $ : deep dual-domain based fast restoration of jpeg-compressed images
in this paper , we design a deep dual-domain ( $ \mathbf { d^3 } $ ) based fast restoration model to remove artifacts of jpeg compressed images . it leverages the large learning capacity of deep networks , as well as the problem-specific expertise that was hardly incorporated in the past design of deep architectures . for the latter , we take into consideration both the prior knowledge of the jpeg compression scheme , and the successful practice of the sparsity-based dual-domain approach . we further design the one-step sparse inference ( 1-si ) module , as an efficient and light-weighted feed-forward approximation of sparse coding . extensive experiments verify the superiority of the proposed $ d^3 $ model over several state-of-the-art methods . specifically , our best model is capable of outperforming the latest deep model for around 1 db in psnr , and is 30 times faster .

estimating 3d trajectories from 2d projections via disjunctive factored four-way conditional restricted boltzmann machines
estimation , recognition , and near-future prediction of 3d trajectories based on their two dimensional projections available from one camera source is an exceptionally difficult problem due to uncertainty in the trajectories and environment , high dimensionality of the specific trajectory states , lack of enough labeled data and so on . in this article , we propose a solution to solve this problem based on a novel deep learning model dubbed disjunctive factored four-way conditional restricted boltzmann machine ( dffw-crbm ) . our method improves state-of-the-art deep learning techniques for high dimensional time-series modeling by introducing a novel tensor factorization capable of driving forth order boltzmann machines to considerably lower energy levels , at no computational costs . dffw-crbms are capable of accurately estimating , recognizing , and performing near-future prediction of three-dimensional trajectories from their 2d projections while requiring limited amount of labeled data . we evaluate our method on both simulated and real-world data , showing its effectiveness in predicting and classifying complex ball trajectories and human activities .

the semantics of kalah game
the present work consisted in developing a plateau game . there are the traditional ones ( monopoly , cluedo , ect . ) but those which interest us leave less place at the chance ( luck ) than to the strategy such that the chess game . kallah is an old african game , its rules are simple but the strategies to be used are very complex to implement . of course , they are based on a strongly mathematical basis as in the film `` rain-man '' where one can see that gambling can be payed with strategies based on mathematical theories . the artificial intelligence gives the possibility `` of thinking '' to a machine and , therefore , allows it to make decisions . in our work , we use it to give the means to the computer choosing its best movement .

learning like humans with deep symbolic networks
we introduce the deep symbolic network ( dsn ) model , which aims at becoming the white-box version of deep neural networks ( dnn ) . the dsn model provides a simple , universal yet powerful structure , similar to dnn , to represent any knowledge of the world , which is transparent to humans . the conjecture behind the dsn model is that any type of real world objects sharing enough common features are mapped into human brains as a symbol . those symbols are connected by links , representing the composition , correlation , causality , or other relationships between them , forming a deep , hierarchical symbolic network structure . powered by such a structure , the dsn model is expected to learn like humans , because of its unique characteristics . first , it is universal , using the same structure to store any knowledge . second , it can learn symbols from the world and construct the deep symbolic networks automatically , by utilizing the fact that real world objects have been naturally separated by singularities . third , it is symbolic , with the capacity of performing causal deduction and generalization . fourth , the symbols and the links between them are transparent to us , and thus we will know what it has learned or not - which is the key for the security of an ai system . fifth , its transparency enables it to learn with relatively small data . sixth , its knowledge can be accumulated . last but not least , it is more friendly to unsupervised learning than dnn . we present the details of the model , the algorithm powering its automatic learning ability , and describe its usefulness in different use cases . the purpose of this paper is to generate broad interest to develop it within an open source project centered on the deep symbolic network ( dsn ) model towards the development of general ai .

master algorithms for active experts problems based on increasing loss values
we specify an experts algorithm with the following characteristics : ( a ) it uses only feedback from the actions actually chosen ( bandit setup ) , ( b ) it can be applied with countably infinite expert classes , and ( c ) it copes with losses that may grow in time appropriately slowly . we prove loss bounds against an adaptive adversary . from this , we obtain master algorithms for `` active experts problems '' , which means that the master 's actions may influence the behavior of the adversary . our algorithm can significantly outperform standard experts algorithms on such problems . finally , we combine it with a universal expert class . this results in a ( computationally infeasible ) universal master algorithm which performs - in a certain sense - almost as well as any computable strategy , for any online problem .

extracting biomolecular interactions using semantic parsing of biomedical text
we advance the state of the art in biomolecular interaction extraction with three contributions : ( i ) we show that deep , abstract meaning representations ( amr ) significantly improve the accuracy of a biomolecular interaction extraction system when compared to a baseline that relies solely on surface- and syntax-based features ; ( ii ) in contrast with previous approaches that infer relations on a sentence-by-sentence basis , we expand our framework to enable consistent predictions over sets of sentences ( documents ) ; ( iii ) we further modify and expand a graph kernel learning framework to enable concurrent exploitation of automatically induced amr ( semantic ) and dependency structure ( syntactic ) representations . our experiments show that our approach yields interaction extraction systems that are more robust in environments where there is a significant mismatch between training and test conditions .

interpolation in equilibrium logic and answer set programming : the propositional case
interpolation is an important property of classical and many non classical logics that has been shown to have interesting applications in computer science and ai . here we study the interpolation property for the propositional version of the non-monotonic system of equilibrium logic , establishing weaker or stronger forms of interpolation depending on the precise interpretation of the inference relation . these results also yield a form of interpolation for ground logic programs under the answer sets semantics . for disjunctive logic programs we also study the property of uniform interpolation that is closely related to the concept of variable forgetting .

unsupervised submodular rank aggregation on score-based permutations
unsupervised rank aggregation on score-based permutations , which is widely used in many applications , has not been deeply explored yet . this work studies the use of submodular optimization for rank aggregation on score-based permutations in an unsupervised way . specifically , we propose an unsupervised approach based on the lovasz bregman divergence for setting up linear structured convex and nested structured concave objective functions . in addition , stochastic optimization methods are applied in the training process and efficient algorithms for inference can be guaranteed . the experimental results from information retrieval , combining distributed neural networks , influencers in social networks , and distributed automatic speech recognition tasks demonstrate the effectiveness of the proposed methods .

hiding satisfying assignments : two are better than one
the evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances . a plausible source of such instances consists of random k-sat formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment a. unfortunately , instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics . roughly speaking , as the formula 's density increases , for a number of different algorithms , a acts as a stronger and stronger attractor . motivated by recent results on the geometry of the space of satisfying truth assignments of random k-sat and nae-k-sat formulas , we introduce a simple twist on this basic model , which appears to dramatically increase its hardness . namely , in addition to forbidding the clauses violated by the hidden assignment a , we also forbid the clauses violated by its complement , so that both a and complement of a are satisfying . it appears that under this `` symmetrization '' the effects of the two attractors largely cancel out , making it much harder for algorithms to find any truth assignment . we give theoretical and experimental evidence supporting this assertion .

webapirec : recommending web apis to software projects via personalized ranking
application programming interfaces ( apis ) offer a plethora of functionalities for developers to reuse without reinventing the wheel . identifying the appropriate apis given a project requirement is critical for the success of a project , as many functionalities can be reused to achieve faster development . however , the massive number of apis would often hinder the developers ' ability to quickly find the right apis . in this light , we propose a new , automated approach called webapirec that takes as input a project profile and outputs a ranked list of { web } apis that can be used to implement the project . at its heart , webapirec employs a personalized ranking model that ranks web apis specific ( personalized ) to a project . based on the historical data of { web } api usages , webapirec learns a model that minimizes the incorrect ordering of web apis , i.e. , when a used { web } api is ranked lower than an unused ( or a not-yet-used ) web api . we have evaluated our approach on a dataset comprising 9,883 web apis and 4,315 web application projects from programmableweb with promising results . for 84.0 % of the projects , webapirec is able to successfully return correct apis that are used to implement the projects in the top-5 positions . this is substantially better than the recommendations provided by programmableweb 's native search functionality . webapirec also outperforms mcmillan et al . 's application search engine and popularity-based recommendation .

a definition of artificial intelligence
in this paper we offer a formal definition of artificial intelligence and this directly gives us an algorithm for construction of this object . really , this algorithm is useless due to the combinatory explosion . the main innovation in our definition is that it does not include the knowledge as a part of the intelligence . so according to our definition a newly born baby also is an intellect . here we differs with turing 's definition which suggests that an intellect is a person with knowledge gained through the years .

flica : a framework for leader identification in coordinated activity
leadership is an important aspect of social organization that affects the processes of group formation , coordination , and decision-making in human societies , as well as in the social system of many other animal species . the ability to identify leaders based on their behavior and the subsequent reactions of others opens opportunities to explore how group decisions are made . understanding who exerts influence provides key insights into the structure of social organizations . in this paper , we propose a simple yet powerful leadership inference framework extracting group coordination periods and determining leadership based on the activity of individuals within a group . we are able to not only identify a leader or leaders but also classify the type of leadership model that is consistent with observed patterns of group decision-making . the framework performs well in differentiating a variety of leadership models ( e.g . dictatorship , linear hierarchy , or local influence ) . we propose five simple features that can be used to categorize characteristics of each leadership model , and thus make model classification possible . the proposed approach automatically ( 1 ) identifies periods of coordinated group activity , ( 2 ) determines the identities of leaders , and ( 3 ) classifies the likely mechanism by which the group coordination occurred . we demonstrate our framework on both simulated and real-world data : gps tracks of a baboon troop and video-tracking of fish schools , as well as stock market closing price data of the nasdaq index . the results of our leadership model are consistent with ground-truthed biological data and the framework finds many known events in financial data which are not otherwise reflected in the aggregate nasdaq index . our approach is easily generalizable to any coordinated activity data from interacting entities .

3d model retrieval based on semantic and shape indexes
the size of 3d models used on the web or stored in databases is becoming increasingly high . then , an efficient method that allows users to find similar 3d objects for a given 3d model query has become necessary . keywords and the geometry of a 3d model can not meet the needs of users ' retrieval because they do not include the semantic information . in this paper , a new method has been proposed to 3d models retrieval using semantic concepts combined with shape indexes . to obtain these concepts , we use the machine learning methods to label 3d models by k-means algorithm in measures and shape indexes space . moreover , semantic concepts have been organized and represented by ontology language owl and spatial relationships are used to disambiguate among models of similar appearance . the sparql query language has been used to question the information displayed in this language and to compute the similarity between two 3d models . we interpret our results using the princeton shape benchmark database and the results show the performance of the proposed new approach to retrieval 3d models . keywords : 3d model , 3d retrieval , measures , shape indexes , semantic , ontology

evidence aggregation for answer re-ranking in open-domain question answering
a popular recent approach to answering open-domain questions is to first search for question-related passages and then apply reading comprehension models to extract answers . existing methods usually extract answers from single passages independently . but some questions require a combination of evidence from across different sources to answer correctly . in this paper , we propose two models which make use of multiple passages to generate their answers . both use an answer-reranking approach which reorders the answer candidates generated by an existing state-of-the-art qa model . we propose two methods , namely , strength-based re-ranking and coverage-based re-ranking , to make use of the aggregated evidence from different passages to better determine the answer . our models have achieved state-of-the-art results on three public open-domain qa datasets : quasar-t , searchqa and the open-domain version of triviaqa , with about 8 percentage points of improvement over the former two datasets .

what to talk about and how ? selective generation using lstms with coarse-to-fine alignment
we propose an end-to-end , domain-independent neural encoder-aligner-decoder model for selective generation , i.e. , the joint task of content selection and surface realization . our model first encodes a full set of over-determined database event records via an lstm-based recurrent neural network , then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about , and finally employs a decoder to generate free-form descriptions of the aligned , selected records . our model achieves the best selection and generation results reported to-date ( with 59 % relative improvement in generation ) on the benchmark weathergov dataset , despite using no specialized features or linguistic resources . using an improved k-nearest neighbor beam filter helps further . we also perform a series of ablations and visualizations to elucidate the contributions of our key model components . lastly , we evaluate the generalizability of our model on the robocup dataset , and get results that are competitive with or better than the state-of-the-art , despite being severely data-starved .

discriminative probabilistic models for relational data
in many supervised learning tasks , the entities to be labeled are related to each other in complex ways and their labels are not independent . for example , in hypertext classification , the labels of linked pages are highly correlated . a standard approach is to classify each entity independently , ignoring the correlations between them . recently , probabilistic relational models , a relational version of bayesian networks , were used to define a joint probabilistic model for a collection of related entities . in this paper , we present an alternative framework that builds on ( conditional ) markov networks and addresses two limitations of the previous approach . first , undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models . second , undirected models are well suited for discriminative training , where we optimize the conditional likelihood of the labels given the features , which generally improves classification accuracy . we show how to train these models effectively , and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities . we provide experimental results on a webpage classification task , showing that accuracy can be significantly improved by modeling relational dependencies .

monte-carlo optimizations for resource allocation problems in stochastic network systems
real-world distributed systems and networks are often unreliable and subject to random failures of its components . such a stochastic behavior affects adversely the complexity of optimization tasks performed routinely upon such systems , in particular , various resource allocation tasks . in this work we investigate and develop monte carlo solutions for a class of two-stage optimization problems in stochastic networks in which the expected value of resource allocations before and after stochastic failures needs to be optimized . the limitation of these problems is that their exact solutions are exponential in the number of unreliable network components : thus , exact methods do not scale-up well to large networks often seen in practice . we first prove that monte carlo optimization methods can overcome the exponential bottleneck of exact methods . next we support our theoretical findings on resource allocation experiments and show a very good scale-up potential of the new methods to large stochastic networks .

deep fruit detection in orchards
an accurate and reliable image based fruit detection system is critical for supporting higher level agriculture tasks such as yield mapping and robotic harvesting . this paper presents the use of a state-of-the-art object detection framework , faster r-cnn , in the context of fruit detection in orchards , including mangoes , almonds and apples . ablation studies are presented to better understand the practical deployment of the detection network , including how much training data is required to capture variability in the dataset . data augmentation techniques are shown to yield significant performance gains , resulting in a greater than two-fold reduction in the number of training images required . in contrast , transferring knowledge between orchards contributed to negligible performance gain over initialising the deep convolutional neural network directly from imagenet features . finally , to operate over orchard data containing between 100-1000 fruit per image , a tiling approach is introduced for the faster r-cnn framework . the study has resulted in the best yet detection performance for these orchards relative to previous works , with an f1-score of > 0.9 achieved for apples and mangoes .

systematic verification of the modal logic cube in isabelle/hol
we present an automated verification of the well-known modal logic cube in isabelle/hol , in which we prove the inclusion relations between the cube 's logics using automated reasoning tools . prior work addresses this problem but without restriction to the modal logic cube , and using encodings in first-order logic in combination with first-order automated theorem provers . in contrast , our solution is more elegant , transparent and effective . it employs an embedding of quantified modal logic in classical higher-order logic . automated reasoning tools , such as sledgehammer with leo-ii , satallax and cvc4 , metis and nitpick , are employed to achieve full automation . though successful , the experiments also motivate some technical improvements in the isabelle/hol tool .

multimodal content analysis for effective advertisements on youtube
the rapid advances in e-commerce and web 2.0 technologies have greatly increased the impact of commercial advertisements on the general public . as a key enabling technology , a multitude of recommender systems exists which analyzes user features and browsing patterns to recommend appealing advertisements to users . in this work , we seek to study the characteristics or attributes that characterize an effective advertisement and recommend a useful set of features to aid the designing and production processes of commercial advertisements . we analyze the temporal patterns from multimedia content of advertisement videos including auditory , visual and textual components , and study their individual roles and synergies in the success of an advertisement . the objective of this work is then to measure the effectiveness of an advertisement , and to recommend a useful set of features to advertisement designers to make it more successful and approachable to users . our proposed framework employs the signal processing technique of cross modality feature learning where data streams from different components are employed to train separate neural network models and are then fused together to learn a shared representation . subsequently , a neural network model trained on this joint feature embedding representation is utilized as a classifier to predict advertisement effectiveness . we validate our approach using subjective ratings from a dedicated user study , the sentiment strength of online viewer comments , and a viewer opinion metric of the ratio of the likes and views received by each advertisement from an online platform .

a proposal to design expert system for the calculations in the domain of qft
main purposes of the paper are followings : 1 ) to show examples of the calculations in domain of qft via `` derivative rules '' of an expert system ; 2 ) to consider advantages and disadvantage that technology of the calculations ; 3 ) to reflect about how one would develop new physical theories , what knowledge would be useful in their investigations and how this problem can be connected with designing an expert system .

a general framework for equivalences in answer-set programming by countermodels in the logic of here-and-there
different notions of equivalence , such as the prominent notions of strong and uniform equivalence , have been studied in answer-set programming , mainly for the purpose of identifying programs that can serve as substitutes without altering the semantics , for instance in program optimization . such semantic comparisons are usually characterized by various selections of models in the logic of here-and-there ( ht ) . for uniform equivalence however , correct characterizations in terms of ht-models can only be obtained for finite theories , respectively programs . in this article , we show that a selection of countermodels in ht captures uniform equivalence also for infinite theories . this result is turned into coherent characterizations of the different notions of equivalence by countermodels , as well as by a mixture of ht-models and countermodels ( so-called equivalence interpretations ) . moreover , we generalize the so-called notion of relativized hyperequivalence for programs to propositional theories , and apply the same methodology in order to obtain a semantic characterization which is amenable to infinite settings . this allows for a lifting of the results to first-order theories under a very general semantics given in terms of a quantified version of ht . we thus obtain a general framework for the study of various notions of equivalence for theories under answer-set semantics . moreover , we prove an expedient property that allows for a simplified treatment of extended signatures , and provide further results for non-ground logic programs . in particular , uniform equivalence coincides under open and ordinary answer-set semantics , and for finite non-ground programs under these semantics , also the usual characterization of uniform equivalence in terms of maximal and total ht-models of the grounding is correct , even for infinite domains , when corresponding ground programs are infinite .

dependence maximizing temporal alignment via squared-loss mutual information
the goal of temporal alignment is to establish time correspondence between two sequences , which has many applications in a variety of areas such as speech processing , bioinformatics , computer vision , and computer graphics . in this paper , we propose a novel temporal alignment method called least-squares dynamic time warping ( lsdtw ) . lsdtw finds an alignment that maximizes statistical dependency between sequences , measured by a squared-loss variant of mutual information . the benefit of this novel information-theoretic formulation is that lsdtw can align sequences with different lengths , different dimensionality , high non-linearity , and non-gaussianity in a computationally efficient manner . in addition , model parameters such as an initial alignment matrix can be systematically optimized by cross-validation . we demonstrate the usefulness of lsdtw through experiments on synthetic and real-world kinect action recognition datasets .

on the construction of the inclusion boundary neighbourhood for markov equivalence classes of bayesian network structures
the problem of learning markov equivalence classes of bayesian network structures may be solved by searching for the maximum of a scoring metric in a space of these classes . this paper deals with the definition and analysis of one such search space . we use a theoretically motivated neighbourhood , the inclusion boundary , and represent equivalence classes by essential graphs . we show that this search space is connected and that the score of the neighbours can be evaluated incrementally . we devise a practical way of building this neighbourhood for an essential graph that is purely graphical and does not explicitely refer to the underlying independences . we find that its size can be intractable , depending on the complexity of the essential graph of the equivalence class . the emphasis is put on the potential use of this space with greedy hill -climbing search

investigation of a collective decision making system of different neighbourhood-size based on hyper-geometric distribution
the study of collective decision making system has become the central part of the swarm- intelligence related research in recent years . the most challenging task of modelling a collec- tive decision making system is to develop the macroscopic stochastic equation from its microscopic model . in this report we have investigated the behaviour of a collective decision making system with specified microscopic rules that resemble the chemical reaction and used different group size . then we ventured to derive a generalized analytical model of a collective-decision system using hyper-geometric distribution . index terms-swarm ; collective decision making ; noise ; group size ; hyper-geometric distribution

safer classification by synthesis
the discriminative approach to classification using deep neural networks has become the de-facto standard in various fields . complementing recent reservations about safety against adversarial examples , we show that conventional discriminative methods can easily be fooled to provide incorrect labels with very high confidence to out of distribution examples . we posit that a generative approach is the natural remedy for this problem , and propose a method for classification using generative models . at training time , we learn a generative model for each class , while at test time , given an example to classify , we query each generator for its most similar generation , and select the class corresponding to the most similar one . our approach is general and can be used with expressive models such as gans and vaes . at test time , our method accurately `` knows when it does not know , '' and provides resilience to out of distribution examples while maintaining competitive performance for standard examples .

a novel clustering algorithm based on quantum games
enormous successes have been made by quantum algorithms during the last decade . in this paper , we combine the quantum game with the problem of data clustering , and then develop a quantum-game-based clustering algorithm , in which data points in a dataset are considered as players who can make decisions and implement quantum strategies in quantum games . after each round of a quantum game , each player 's expected payoff is calculated . later , he uses a link-removing-and-rewiring ( lrr ) function to change his neighbors and adjust the strength of links connecting to them in order to maximize his payoff . further , algorithms are discussed and analyzed in two cases of strategies , two payoff matrixes and two lrr functions . consequently , the simulation results have demonstrated that data points in datasets are clustered reasonably and efficiently , and the clustering algorithms have fast rates of convergence . moreover , the comparison with other algorithms also provides an indication of the effectiveness of the proposed approach .

definition and properties to assess multi-agent environments as social intelligence tests
social intelligence in natural and artificial systems is usually measured by the evaluation of associated traits or tasks that are deemed to represent some facets of social behaviour . the amalgamation of these traits is then used to configure the intuitive notion of social intelligence . instead , in this paper we start from a parametrised definition of social intelligence as the expected performance in a set of environments with several agents , and we assess and derive tests from it . this definition makes several dependencies explicit : ( 1 ) the definition depends on the choice ( and weight ) of environments and agents , ( 2 ) the definition may include both competitive and cooperative behaviours depending on how agents and rewards are arranged into teams , ( 3 ) the definition mostly depends on the abilities of other agents , and ( 4 ) the actual difference between social intelligence and general intelligence ( or other abilities ) depends on these choices . as a result , we address the problem of converting this definition into a more precise one where some fundamental properties ensuring social behaviour ( such as action and reward dependency and anticipation on competitive/cooperative behaviours ) are met as well as some other more instrumental properties ( such as secernment , boundedness , symmetry , validity , reliability , efficiency ) , which are convenient to convert the definition into a practical test . from the definition and the formalised properties , we take a look at several representative multi-agent environments , tests and games to see whether they meet these properties .

an order of magnitude calculus
this paper develops a simple calculus for order of magnitude reasoning . a semantics is given with soundness and completeness results . order of magnitude probability functions are easily defined and turn out to be equivalent to kappa functions , which are slight generalizations of spohn 's natural conditional functions . the calculus also gives rise to an order of magnitude decision theory , which can be used to justify an amended version of pearl 's decision theory for kappa functions , although the latter is weaker and less expressive .

principle of development
today , science have a powerful tool for the description of reality - the numbers . however , the concept of number was not immediately , lets try to trace the evolution of the concept . the numbers emerged as the need for accurate estimates of the amount in order to permit a comparison of some objects . so if you see to it how many times a day a person uses the numbers and compare , it becomes evident that the comparison is used much more frequently . however , the comparison is not possible without two opposite basic standards . thus , to introduce the concept of comparison , must have two opposing standards , in turn , the operation of comparison is necessary to introduce the concept of number . arguably , the scientific description of reality is impossible without the concept of opposites . in this paper analyzes the concept of opposites , as the basis for the introduction of the principle of development .

reformulating the situation calculus and the event calculus in the general theory of stable models and in answer set programming
circumscription and logic programs under the stable model semantics are two well-known nonmonotonic formalisms . the former has served as a basis of classical logic based action formalisms , such as the situation calculus , the event calculus and temporal action logics ; the latter has served as a basis of a family of action languages , such as language a and several of its descendants . based on the discovery that circumscription and the stable model semantics coincide on a class of canonical formulas , we reformulate the situation calculus and the event calculus in the general theory of stable models . we also present a translation that turns the reformulations further into answer set programs , so that efficient answer set solvers can be applied to compute the situation calculus and the event calculus .

equilibrium graphs
in this paper we present an extension of peirce 's existential graphs to provide a diagrammatic representation of expressions in quantified equilibrium logic ( qel ) . using this formalisation , logical connectives are replaced by encircled regions ( circles and squares ) and quantified variables are represented as `` identity '' lines . although the expressive power is equivalent to that of qel , the new representation can be useful for illustrative or educational purposes .

learning visual reasoning without strong priors
achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step , high-level process - is an important step towards artificial general intelligence . this multi-modal task requires learning a question-dependent , structured reasoning process over images from language . standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure , while leading methods learn to visually reason successfully but are hand-crafted for reasoning . we show that a general-purpose , conditional batch normalization approach achieves state-of-the-art results on the clevr visual reasoning benchmark with a 2.4 % error rate . we outperform the next best end-to-end method ( 4.5 % ) and even methods that use extra supervision ( 3.1 % ) . we probe our model to shed light on how it reasons , showing it has learned a question-dependent , multi-step process . previous work has operated under the assumption that visual reasoning calls for a specialized architecture , but we show that a general architecture with proper conditioning can learn to visually reason effectively .

one-shot imitation learning
imitation learning has been commonly applied to solve different tasks in isolation . this usually requires either careful feature engineering , or a significant number of samples . this is far from what we desire : ideally , robots should be able to learn from very few demonstrations of any given task , and instantly generalize to new situations of the same task , without requiring task-specific engineering . in this paper , we propose a meta-learning framework for achieving such capability , which we call one-shot imitation learning . specifically , we consider the setting where there is a very large set of tasks , and each task has many instantiations . for example , a task could be to stack all blocks on a table into a single tower , another task could be to place all blocks on a table into two-block towers , etc . in each case , different instances of the task would consist of different sets of blocks with different initial states . at training time , our algorithm is presented with pairs of demonstrations for a subset of all tasks . a neural net is trained that takes as input one demonstration and the current state ( which initially is the initial state of the other demonstration of the pair ) , and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration . at test time , a demonstration of a single instance of a new task is presented , and the neural net is expected to perform well on new instances of this new task . the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data . we anticipate that by training this model on a much greater variety of tasks and settings , we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks . videos available at https : //bit.ly/nips2017-oneshot .

extending the harper identity to iterated belief change
the field of iterated belief change has focused mainly on revision , with the other main operator of agm belief change theory , i.e . contraction , receiving relatively little attention . in this paper we extend the harper identity from single-step change to define iterated contraction in terms of iterated revision . specifically , just as the harper identity provides a recipe for defining the belief set resulting from contracting a in terms of ( i ) the initial belief set and ( ii ) the belief set resulting from revision by not-a , we look at ways to define the plausibility ordering over worlds resulting from contracting a in terms of ( iii ) the initial plausibility ordering , and ( iv ) the plausibility ordering resulting from revision by not-a . after noting that the most straightforward such extension leads to a trivialisation of the space of permissible orderings , we provide a family of operators for combining plausibility orderings that avoid such a result . these operators are characterised in our domain of interest by a pair of intuitively compelling properties , which turn out to enable the derivation of a number of iterated contraction postulates from postulates for iterated revision . we finish by observing that a salient member of this family allows for the derivation of counterparts for contraction of some well known iterated revision operators , as well as for defining new iterated contraction operators .

empirical study on deep learning models for question answering
in this paper we explore deep learning models with memory component or attention mechanism for question answering task . we combine and compare three models , neural machine translation , neural turing machine , and memory networks for a simulated qa data set . this paper is the first one that uses neural machine translation and neural turing machines for solving qa tasks . our results suggest that the combination of attention and memory have potential to solve certain qa problem .

a joint many-task model : growing a neural network for multiple nlp tasks
transfer and multi-task learning have traditionally focused on either a single source-target pair or very few , similar tasks . ideally , the linguistic levels of morphology , syntax and semantics would benefit each other by being trained in a single model . we introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks . higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies . we use a simple regularization term to allow for optimizing all model weights to improve one task 's loss without exhibiting catastrophic interference of the other tasks . our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging , parsing , relatedness , and entailment tasks .

local contrast learning
learning a deep model from small data is yet an opening and challenging problem . we focus on one-shot classification by deep learning approach based on a small quantity of training samples . we proposed a novel deep learning approach named local contrast learning ( lcl ) based on the key insight about a human cognitive behavior that human recognizes the objects in a specific context by contrasting the objects in the context or in her/his memory . lcl is used to train a deep model that can contrast the recognizing sample with a couple of contrastive samples randomly drawn and shuffled . on one-shot classification task on omniglot , the deep model based lcl with 122 layers and 1.94 millions of parameters , which was trained on a tiny dataset with only 60 classes and 20 samples per class , achieved the accuracy 97.99 % that outperforms human and state-of-the-art established by bayesian program learning ( bpl ) trained on 964 classes . lcl is a fundamental idea which can be applied to alleviate parametric model 's overfitting resulted by lack of training samples .

sample complexity of episodic fixed-horizon reinforcement learning
recently , there has been significant progress in understanding reinforcement learning in discounted infinite-horizon markov decision processes ( mdps ) by deriving tight sample complexity bounds . however , in many real-world applications , an interactive learning agent operates for a fixed or bounded period of time , for example tutoring students for exams or handling customer service requests . such scenarios can often be better treated as episodic fixed-horizon mdps , for which only looser bounds on the sample complexity exist . a natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability ( pac guarantee ) . in this paper , we derive an upper pac bound $ \tilde o ( \frac { |\mathcal s|^2 |\mathcal a| h^2 } { \epsilon^2 } \ln\frac 1 \delta ) $ and a lower pac bound $ \tilde \omega ( \frac { |\mathcal s| |\mathcal a| h^2 } { \epsilon^2 } \ln \frac 1 { \delta + c } ) $ that match up to log-terms and an additional linear dependency on the number of states $ |\mathcal s| $ . the lower bound is the first of its kind for this setting . our upper bound leverages bernstein 's inequality to improve on previous bounds for episodic finite-horizon mdps which have a time-horizon dependency of at least $ h^3 $ .

front-to-end bidirectional heuristic search with near-optimal node expansions
it is well-known that any admissible unidirectional heuristic search algorithm must expand all states whose $ f $ -value is smaller than the optimal solution cost when using a consistent heuristic . such states are called `` surely expanded '' ( s.e. ) . a recent study characterized s.e . pairs of states for bidirectional search with consistent heuristics : if a pair of states is s.e . then at least one of the two states must be expanded . this paper derives a lower bound , vc , on the minimum number of expansions required to cover all s.e . pairs , and present a new admissible front-to-end bidirectional heuristic search algorithm , near-optimal bidirectional search ( nbs ) , that is guaranteed to do no more than 2vc expansions . we further prove that no admissible front-to-end algorithm has a worst case better than 2vc . experimental results show that nbs competes with or outperforms existing bidirectional search algorithms , and often outperforms a* as well .

a practical ontology for the large-scale modeling of scholarly artifacts and their usage
the large-scale analysis of scholarly artifact usage is constrained primarily by current practices in usage data archiving , privacy issues concerned with the dissemination of usage data , and the lack of a practical ontology for modeling the usage domain . as a remedy to the third constraint , this article presents a scholarly ontology that was engineered to represent those classes for which large-scale bibliographic and usage data exists , supports usage research , and whose instantiation is scalable to the order of 50 million articles along with their associated artifacts ( e.g . authors and journals ) and an accompanying 1 billion usage events . the real world instantiation of the presented abstract ontology is a semantic network model of the scholarly community which lends the scholarly process to statistical analysis and computational support . we present the ontology , discuss its instantiation , and provide some example inference rules for calculating various scholarly artifact metrics .

learning regular languages over large ordered alphabets
this work is concerned with regular languages defined over large alphabets , either infinite or just too large to be expressed enumeratively . we define a generic model where transitions are labeled by elements of a finite partition of the alphabet . we then extend angluin 's l* algorithm for learning regular languages from examples for such automata . we have implemented this algorithm and we demonstrate its behavior where the alphabet is a subset of the natural or real numbers . we sketch the extension of the algorithm to a class of languages over partially ordered alphabets .

a $ o ( \log m ) $ , deterministic , polynomial-time computable approximation of lewis carroll 's scoring rule
we provide deterministic , polynomial-time computable voting rules that approximate dodgson 's and ( the `` minimization version '' of ) young 's scoring rules to within a logarithmic factor . our approximation of dodgson 's rule is tight up to a constant factor , as dodgson 's rule is $ \np $ -hard to approximate to within some logarithmic factor . the `` maximization version '' of young 's rule is known to be $ \np $ -hard to approximate by any constant factor . both approximations are simple , and natural as rules in their own right : given a candidate we wish to score , we can regard either its dodgson or young score as the edit distance between a given set of voter preferences and one in which the candidate to be scored is the condorcet winner . ( the difference between the two scoring rules is the type of edits allowed . ) we regard the marginal cost of a sequence of edits to be the number of edits divided by the number of reductions ( in the candidate 's deficit against any of its opponents in the pairwise race against that opponent ) that the edits yield . over a series of rounds , our scoring rules greedily choose a sequence of edits that modify exactly one voter 's preferences and whose marginal cost is no greater than any other such single-vote-modifying sequence .

accelerating innovation through analogy mining
the availability of large idea repositories ( e.g. , the u.s. patent database ) could significantly accelerate innovation and discovery by providing people with inspiration from solutions to analogous problems . however , finding useful analogies in these large , messy , real-world repositories remains a persistent challenge for either human or automated methods . previous approaches include costly hand-created databases that have high relational structure ( e.g. , predicate calculus representations ) but are very sparse . simpler machine-learning/information-retrieval similarity metrics can scale to large , natural-language datasets , but struggle to account for structural similarity , which is central to analogy . in this paper we explore the viability and value of learning simpler structural representations , specifically , `` problem schemas '' , which specify the purpose of a product and the mechanisms by which it achieves that purpose . our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions . we demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional information-retrieval methods . in an ideation experiment , analogies retrieved by our models significantly increased people 's likelihood of generating creative ideas compared to analogies retrieved by traditional methods . our results suggest a promising approach to enabling computational analogy at scale is to learn and leverage weaker structural representations .

generating explanations for evidential reasoning
in this paper , we present two methods to provide explanations for reasoning with belief functions in the valuation-based systems . one approach , inspired by strat 's method , is based on sensitivity analysis , but its computation is simpler thus easier to implement than strat 's . the other one is to examine the impact of evidence on the conclusion based on the measure of the information content in the evidence . we show the property of additivity for the pieces of evidence that are conditional independent within the context of the valuation-based systems . we will give an example to show how these approaches are applied in an evidential network .

preliminary report on wasp 2.0
answer set programming ( asp ) is a declarative programming paradigm . the intrinsic complexity of the evaluation of asp programs makes the development of more effective and faster systems a challenging research topic . this paper reports on the recent improvements of the asp solver wasp . wasp is undergoing a refactoring process which will end up in the release of a new and more performant version of the software . in particular the paper focus on the improvements to the core evaluation algorithms working on normal programs . a preliminary experiment on benchmarks from the 3rd asp competition belonging to the np class is reported . the previous version of wasp was often not competitive with alternative solutions on this class . the new version of wasp shows a substantial increase in performance .

a scalable method for solving high-dimensional continuous pomdps using local approximation
partially-observable markov decision processes ( pomdps ) are typically solved by finding an approximate global solution to a corresponding belief-mdp . in this paper , we offer a new planning algorithm for pomdps with continuous state , action and observation spaces . since such domains have an inherent notion of locality , we can find an approximate solution using local optimization methods . we parameterize the belief distribution as a gaussian mixture , and use the extended kalman filter ( ekf ) to approximate the belief update . since the ekf is a first-order filter , we can marginalize over the observations analytically . by using feedback control and state estimation during policy execution , we recover a behavior that is effectively conditioned on incoming observations despite the unconditioned planning . local optimization provides no guarantees of global optimality , but it allows us to tackle domains that are at least an order of magnitude larger than the current state-of-the-art . we demonstrate the scalability of our algorithm by considering a simulated hand-eye coordination domain with 16 continuous state dimensions and 6 continuous action dimensions .

social browsing on flickr
the new social media sites - blogs , wikis , del.icio.us and flickr , among others - underscore the transformation of the web to a participatory medium in which users are actively creating , evaluating and distributing information . the photo-sharing site flickr , for example , allows users to upload photographs , view photos created by others , comment on those photos , etc . as is common to other social media sites , flickr allows users to designate others as `` contacts '' and to track their activities in real time . the contacts ( or friends ) lists form the social network backbone of social media sites . we claim that these social networks facilitate new ways of interacting with information , e.g. , through what we call social browsing . the contacts interface on flickr enables users to see latest images submitted by their friends . through an extensive analysis of flickr data , we show that social browsing through the contacts ' photo streams is one of the primary methods by which users find new images on flickr . this finding has implications for creating personalized recommendation systems based on the user 's declared contacts lists .

a model of multi-agent consensus for vague and uncertain beliefs
consensus formation is investigated for multi-agent systems in which agents ' beliefs are both vague and uncertain . vagueness is represented by a third truth state meaning \emph { borderline } . this is combined with a probabilistic model of uncertainty . a belief combination operator is then proposed which exploits borderline truth values to enable agents with conflicting beliefs to reach a compromise . a number of simulation experiments are carried out in which agents apply this operator in pairwise interactions , under the bounded confidence restriction that the two agents ' beliefs must be sufficiently consistent with each other before agreement can be reached . as well as studying the consensus operator in isolation we also investigate scenarios in which agents are influenced either directly or indirectly by the state of the world . for the former we conduct simulations which combine consensus formation with belief updating based on evidence . for the latter we investigate the effect of assuming that the closer an agent 's beliefs are to the truth the more visible they are in the consensus building process . in all cases applying the consensus operators results in the population converging to a single shared belief which is both crisp and certain . furthermore , simulations which combine consensus formation with evidential updating converge faster to a shared opinion which is closer to the actual state of the world than those in which beliefs are only changed as a result of directly receiving new evidence . finally , if agent interactions are guided by belief quality measured as similarity to the true state of the world , then applying the consensus operator alone results in the population converging to a high quality shared belief .

representing independence models with elementary triplets
in an independence model , the triplets that represent conditional independences between singletons are called elementary . it is known that the elementary triplets represent the independence model unambiguously under some conditions . in this paper , we show how this representation helps performing some operations with independence models , such as finding the dominant triplets or a minimal independence map of an independence model , or computing the union or intersection of a pair of independence models , or performing causal reasoning . for the latter , we rephrase in terms of conditional independences some of pearl 's results for computing causal effects .

predicting opponent team activity in a robocup environment
the goal of this project is to predict the opponent 's configuration in a robocup ssl environment . for simplicity , a markov model assumption is made such that the predicted formation of the opponent team only depends on its current formation . the field is divided into a grid and a robot state per player is created with information about its position and its velocity . to gather a more general sense of what the opposing team is doing , the state also incorporates the team 's average position ( centroid ) . all possible state transitions are stored in a hash table that requires minimum storage space . the table is populated with transition probabilities that are learned by reading vision packages and counting the state transitions regardless of the specific robot player . therefore , the computation during the game is reduced to interpreting a given vision package to assign each player to a state , and looking for the most likely state it will transition to . the confidence of the predicted team 's formation is the product of each individual player 's probability . the project is noteworthy in that it minimizes the time and space complexity requirements for opponent 's moves prediction .

domain-dependent knowledge in answer set planning
in this paper we consider three different kinds of domain-dependent control knowledge ( temporal , procedural and htn-based ) that are useful in planning . our approach is declarative and relies on the language of logic programming with answer set semantics ( ansprolog* ) . ansprolog* is designed to plan without control knowledge . we show how temporal , procedural and htn-based control knowledge can be incorporated into ansprolog* by the modular addition of a small number of domain-dependent rules , without the need to modify the planner . we formally prove the correctness of our planner , both in the absence and presence of the control knowledge . finally , we perform some initial experimentation that demonstrates the potential reduction in planning time that can be achieved when procedural domain knowledge is used to solve planning problems with large plan length .

the fractal dimension of sat formulas
modern sat solvers have experienced a remarkable progress on solving industrial instances . most of the techniques have been developed after an intensive experimental testing process . recently , there have been some attempts to analyze the structure of these formulas in terms of complex networks , with the long-term aim of explaining the success of these sat solving techniques , and possibly improving them . we study the fractal dimension of sat formulas , and show that most industrial families of formulas are self-similar , with a small fractal dimension . we also show that this dimension is not affected by the addition of learnt clauses . we explore how the dimension of a formula , together with other graph properties can be used to characterize sat instances . finally , we give empirical evidence that these graph properties can be used in state-of-the-art portfolios .

heart rate variability and respiration signal as diagnostic tools for late onset sepsis in neonatal intensive care units
apnea-bradycardia is one of the major clinical early indicators of late-onset sepsis occurring in approximately 7 % to 10 % of all neonates and in more than 25 % of very low birth weight infants in nicu . the objective of this paper was to determine if hrv , respiration and their relationships help to diagnose infection in premature infants via non-invasive ways in nicu . therefore , we implement mono-channel ( mc ) and bi-channel ( bc ) analysis in two groups : sepsis ( s ) vs. non-sepsis ( ns ) . firstly , we studied rr series not only by linear methods : time domain and frequency domain , but also by non-linear methods : chaos theory and information theory . the results show that alpha slow , alpha fast and sample entropy are significant parameters to distinguish s from ns . secondly , the question about the functional coupling of hrv and nasal respiration is addressed . local linear correlation coefficient r2t , f has been explored , while non-linear regression coefficient h2 was calculated in two directions . it is obvious that r2t , f within the third frequency band ( 0.2 < f < 0.4 hz ) and h2 in two directions were complementary approaches to diagnose sepsis . thirdly , feasibility study is carried out on the candidate parameters selected from mc and bc respectively . we discovered that the proposed test based on optimal fusion of 6 features shows good performance with the largest auc and a reduced probability of false alarm ( pfa ) .

toward a market model for bayesian inference
we present a methodology for representing probabilistic relationships in a general-equilibrium economic model . specifically , we define a precise mapping from a bayesian network with binary nodes to a market price system where consumers and producers trade in uncertain propositions . we demonstrate the correspondence between the equilibrium prices of goods in this economy and the probabilities represented by the bayesian network . a computational market model such as this may provide a useful framework for investigations of belief aggregation , distributed probabilistic inference , resource allocation under uncertainty , and other problems of decentralized uncertainty .

adversarial patrolling with spatially uncertain alarm signals
when securing complex infrastructures or large environments , constant surveillance of every area is not affordable . to cope with this issue , a common countermeasure is the usage of cheap but wide-ranged sensors , able to detect suspicious events that occur in large areas , supporting patrollers to improve the effectiveness of their strategies . however , such sensors are commonly affected by uncertainty . in the present paper , we focus on spatially uncertain alarm signals . that is , the alarm system is able to detect an attack but it is uncertain on the exact position where the attack is taking place . this is common when the area to be secured is wide such as in border patrolling and fair site surveillance . we propose , to the best of our knowledge , the first patrolling security game model where a defender is supported by a spatially uncertain alarm system which non-deterministically generates signals once a target is under attack . we show that finding the optimal strategy in arbitrary graphs is apx-hard even in zero-sum games and we provide two ( exponential time ) exact algorithms and two ( polynomial time ) approximation algorithms . furthermore , we analyse what happens in environments with special topologies , showing that in linear and cycle graphs the optimal patrolling strategy can be found in polynomial time , de facto allowing our algorithms to be used in real-life scenarios , while in trees the problem is np-hard . finally , we show that without false positives and missed detections , the best patrolling strategy reduces to stay in a place , wait for a signal , and respond to it at best . this strategy is optimal even with non-negligible missed detection rates , which , unfortunately , affect every commercial alarm system . we evaluate our methods in simulation , assessing both quantitative and qualitative aspects .

computational intelligence characterization method of semiconductor device
characterization of semiconductor devices is used to gather as much data about the device as possible to determine weaknesses in design or trends in the manufacturing process . in this paper , we propose a novel multiple trip point characterization concept to overcome the constraint of single trip point concept in device characterization phase . in addition , we use computational intelligence techniques ( e.g . neural network , fuzzy and genetic algorithm ) to further manipulate these sets of multiple trip point values and tests based on semiconductor test equipments , our experimental results demonstrate an excellent design parameter variation analysis in device characterization phase , as well as detection of a set of worst case tests that can provoke the worst case variation , while traditional approach was not capable of detecting them .

proceedings of the second conference on uncertainty in artificial intelligence ( 1986 )
this is the proceedings of the second conference on uncertainty in artificial intelligence , which was held in philadelphia , pa , august 8-10 , 1986

a decidable very expressive description logic for databases ( extended version )
we introduce $ \mathcal { dlr } ^+ $ , an extension of the n-ary propositionally closed description logic $ \mathcal { dlr } $ to deal with attribute-labelled tuples ( generalising the positional notation ) , projections of relations , and global and local objectification of relations , able to express inclusion , functional , key , and external uniqueness dependencies . the logic is equipped with both tbox and abox axioms . we show how a simple syntactic restriction on the appearance of projections sharing common attributes in a $ \mathcal { dlr } ^+ $ knowledge base makes reasoning in the language decidable with the same computational complexity as $ \mathcal { dlr } $ . the obtained $ \mathcal { dlr } ^\pm $ n-ary description logic is able to encode more thoroughly conceptual data models such as eer , uml , and orm .

expert and non-expert opinion about technological unemployment
there is significant concern that technological advances , especially in robotics and artificial intelligence ( ai ) , could lead to high levels of unemployment in the coming decades . studies have estimated that around half of all current jobs are at risk of automation . to look into this issue in more depth , we surveyed experts in robotics and ai about the risk , and compared their views with those of non-experts . whilst the experts predicted a significant number of occupations were at risk of automation in the next two decades , they were more cautious than people outside the field in predicting occupations at risk . their predictions were consistent with their estimates for when computers might be expected to reach human level performance across a wide range of skills . these estimates were typically decades later than those of the non-experts . technological barriers may therefore provide society with more time to prepare for an automated future than the public fear . in addition , public expectations may need to be dampened about the speed of progress to be expected in robotics and ai .

learning finite-state controllers for partially observable environments
reactive ( memoryless ) policies are sufficient in completely observable markov decision processes ( mdps ) , but some kind of memory is usually necessary for optimal control of a partially observable mdp . policies with finite memory can be represented as finite-state automata . in this paper , we extend baird and moore 's vaps algorithm to the problem of learning general finite-state automata . because it performs stochastic gradient descent , this algorithm can be shown to converge to a locally optimal finite-state controller . we provide the details of the algorithm and then consider the question of under what conditions stochastic gradient descent will outperform exact gradient descent . we conclude with empirical results comparing the performance of stochastic and exact gradient descent , and showing the ability of our algorithm to extract the useful information contained in the sequence of past observations to compensate for the lack of observability at each time-step .

mining for trees in a graph is np-complete
mining for trees in a graph is shown to be np-complete .

expressibility of norms in temporal logic
in this short note we address the issue of expressing norms ( such as obligations and prohibitions ) in temporal logic . in particular , we address the argument from [ governatori 2015 ] that norms can not be expressed in linear time temporal logic ( ltl ) .

sat-based analysis of large real-world feature models is easy
modern conflict-driven clause-learning ( cdcl ) boolean sat solvers provide efficient automatic analysis of real-world feature models ( fm ) of systems ranging from cars to operating systems . it is well-known that solver-based analysis of real-world fms scale very well even though sat instances obtained from such fms are large , and the corresponding analysis problems are known to be np-complete . to better understand why sat solvers are so effective , we systematically studied many syntactic and semantic characteristics of a representative set of large real-world fms . we discovered that a key reason why large real-world fms are easy-to-analyze is that the vast majority of the variables in these models are unrestricted , i.e. , the models are satisfiable for both true and false assignments to such variables under the current partial assignment . given this discovery and our understanding of cdcl sat solvers , we show that solvers can easily find satisfying assignments for such models without too many backtracks relative to the model size , explaining why solvers scale so well . further analysis showed that the presence of unrestricted variables in these real-world models can be attributed to their high-degree of variability . additionally , we experimented with a series of well-known non-backtracking simplifications that are particularly effective in solving fms . the remaining variables/clauses after simplifications , called the core , are so few that they are easily solved even with backtracking , further strengthening our conclusions .

dcs : an implementation of datalog with constraints
answer-set programming ( asp ) has emerged recently as a viable programming paradigm . we describe here an asp system , datalog with constraints or dc , based on non-monotonic logic . informally , dc theories consist of propositional clauses ( constraints ) and of horn rules . the semantics is a simple and natural extension of the semantics of the propositional logic . however , thanks to the presence of horn rules in the system , modeling of transitive closure becomes straightforward . we describe the syntax , use and implementation of dc and provide experimental results .

inducing honest reporting without observing outcomes : an application to the peer-review process
when eliciting opinions from a group of experts , traditional devices used to promote honest reporting assume that there is an observable future outcome . in practice , however , this assumption is not always reasonable . in this paper , we propose a scoring method built on strictly proper scoring rules to induce honest reporting without assuming observable outcomes . our method provides scores based on pairwise comparisons between the reports made by each pair of experts in the group . for ease of exposition , we introduce our scoring method by illustrating its application to the peer-review process . in order to do so , we start by modeling the peer-review process using a bayesian model where the uncertainty regarding the quality of the manuscript is taken into account . thereafter , we introduce our scoring method to evaluate the reported reviews . under the assumptions that reviewers are bayesian decision-makers and that they can not influence the reviews of other reviewers , we show that risk-neutral reviewers strictly maximize their expected scores by honestly disclosing their reviews . we also show how the group 's scores can be used to find a consensual review . experimental results show that encouraging honest reporting through the proposed scoring method creates more accurate reviews than the traditional peer-review process .

why artificial intelligence needs a task theory -- - and what it might look like
the concept of `` task '' is at the core of artificial intelligence ( ai ) : tasks are used for training and evaluating ai systems , which are built in order to perform and automatize tasks we deem useful . in other fields of engineering theoretical foundations allow thorough evaluation of designs by methodical manipulation of well understood parameters with a known role and importance ; this allows an aeronautics engineer , for instance , to systematically assess the effects of wind speed on an airplane 's performance and stability . no framework exists in ai that allows this kind of methodical manipulation : performance results on the few tasks in current use ( cf . board games , question-answering ) can not be easily compared , however similar or different . the issue is even more acute with respect to artificial *general* intelligence systems , which must handle unanticipated tasks whose specifics can not be known beforehand . a *task theory* would enable addressing tasks at the *class* level , bypassing their specifics , providing the appropriate formalization and classification of tasks , environments , and their parameters , resulting in more rigorous ways of measuring , comparing , and evaluating intelligent behavior . even modest improvements in this direction would surpass the current ad-hoc nature of machine learning and ai evaluation . here we discuss the main elements of the argument for a task theory and present an outline of what it might look like for physical tasks .

solving linear constraints in elementary abelian p-groups of symmetries
symmetries occur naturally in csp or sat problems and are not very difficult to discover , but using them to prune the search space tends to be very challenging . indeed , this usually requires finding specific elements in a group of symmetries that can be huge , and the problem of their very existence is np-hard . we formulate such an existence problem as a constraint problem on one variable ( the symmetry to be used ) ranging over a group , and try to find restrictions that may be solved in polynomial time . by considering a simple form of constraints ( restricted by a cardinality k ) and the class of groups that have the structure of fp-vector spaces , we propose a partial algorithm based on linear algebra . this polynomial algorithm always applies when k=p=2 , but may fail otherwise as we prove the problem to be np-hard for all other values of k and p. experiments show that this approach though restricted should allow for an efficient use of at least some groups of symmetries . we conclude with a few directions to be explored to efficiently solve this problem on the general case .

indicators of good student performance in moodle activity data
in this paper we conduct an analysis of moodle activity data focused on identifying early predictors of good student performance . the analysis shows that three relevant hypotheses are largely supported by the data . these hypotheses are : early submission is a good sign , a high level of activity is predictive of good results and evening activity is even better than daytime activity . we highlight some pathological examples where high levels of activity correlates with bad results .

sensitivity analysis in decision circuits
decision circuits have been developed to perform efficient evaluation of influence diagrams [ bhattacharjya and shachter , 2007 ] , building on the advances in arithmetic circuits for belief network inference [ darwiche,2003 ] . in the process of model building and analysis , we perform sensitivity analysis to understand how the optimal solution changes in response to changes in the model . when sequential decision problems under uncertainty are represented as decision circuits , we can exploit the efficient solution process embodied in the decision circuit and the wealth of derivative information available to compute the value of information for the uncertainties in the problem and the effects of changes to model parameters on the value and the optimal strategy .

meta-learning of exploration/exploitation strategies : the multi-armed bandit case
the exploration/exploitation ( e/e ) dilemma arises naturally in many subfields of science . multi-armed bandit problems formalize this dilemma in its canonical form . most current research in this field focuses on generic solutions that can be applied to a wide range of problems . however , in practice , it is often the case that a form of prior information is available about the specific class of target problems . prior knowledge is rarely used in current solutions due to the lack of a systematic approach to incorporate it into the e/e strategy . to address a specific class of e/e problems , we propose to proceed in three steps : ( i ) model prior knowledge in the form of a probability distribution over the target class of e/e problems ; ( ii ) choose a large hypothesis space of candidate e/e strategies ; and ( iii ) , solve an optimization problem to find a candidate e/e strategy of maximal average performance over a sample of problems drawn from the prior distribution . we illustrate this meta-learning approach with two different hypothesis spaces : one where e/e strategies are numerically parameterized and another where e/e strategies are represented as small symbolic formulas . we propose appropriate optimization algorithms for both cases . our experiments , with two-armed bernoulli bandit problems and various playing budgets , show that the meta-learnt e/e strategies outperform generic strategies of the literature ( ucb1 , ucb1-tuned , ucb-v , kl-ucb and epsilon greedy ) ; they also evaluate the robustness of the learnt e/e strategies , by tests carried out on arms whose rewards follow a truncated gaussian distribution .

incremental truncated lstd
balancing between computational efficiency and sample efficiency is an important goal in reinforcement learning . temporal difference ( td ) learning algorithms stochastically update the value function , with a linear time complexity in the number of features , whereas least-squares temporal difference ( lstd ) algorithms are sample efficient but can be quadratic in the number of features . in this work , we develop an efficient incremental low-rank lstd ( { \lambda } ) algorithm that progresses towards the goal of better balancing computation and sample efficiency . the algorithm reduces the computation and storage complexity to the number of features times the chosen rank parameter while summarizing past samples efficiently to nearly obtain the sample complexity of lstd . we derive a simulation bound on the solution given by truncated low-rank approximation , illustrating a bias- variance trade-off dependent on the choice of rank . we demonstrate that the algorithm effectively balances computational complexity and sample efficiency for policy evaluation in a benchmark task and a high-dimensional energy allocation domain .

allocating indivisible items in categorized domains
we formulate a general class of allocation problems called categorized domain allocation problems ( cdaps ) , where indivisible items from multiple categories are allocated to agents without monetary transfer and each agent gets at least one item per category . we focus on basic cdaps , where the number of items in each category is equal to the number of agents . we characterize serial dictatorships for basic cdaps by a minimal set of three axiomatic properties : strategy-proofness , non-bossiness , and category-wise neutrality . then , we propose a natural extension of serial dictatorships called categorial sequential allocation mechanisms ( csams ) , which allocate the items in multiple rounds : in each round , the active agent chooses an item from a designated category . we fully characterize the worst-case rank efficiency of csams for optimistic and pessimistic agents , and provide a bound for strategic agents . we also conduct experiments to compare expected rank efficiency of various csams w.r.t . random generated data .

freeway merging in congested traffic based on multipolicy decision making with passive actor critic
freeway merging in congested traffic is a significant challenge toward fully automated driving . merging vehicles need to decide not only how to merge into a spot , but also where to merge . we present a method for the freeway merging based on multi-policy decision making with a reinforcement learning method called { \em passive actor-critic } ( pac ) , which learns with less knowledge of the system and without active exploration . the method selects a merging spot candidate by using the state value learned with pac . we evaluate our method using real traffic data . our experiments show that pac achieves 92\ % success rate to merge into a freeway , which is comparable to human decision making .

map segmentation by colour cube genetic k-mean clustering
segmentation of a colour image composed of different kinds of texture regions can be a hard problem , namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields . in this work , a method is described for evolving adaptive procedures for these problems . in many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be mapped into topological domains . we formulate the segmentation problem upon such images as an optimisation problem and adopt evolutionary strategy of genetic algorithms for the clustering of small regions in colour feature space . the present approach uses k-means unsupervised clustering methods into genetic algorithms , namely for guiding this last evolutionary algorithm in his search for finding the optimal or sub-optimal data partition , task that as we know , requires a non-trivial search because of its np-complete nature . to solve this task , the appropriate genetic coding is also discussed , since this is a key aspect in the implementation . our purpose is to demonstrate the efficiency of genetic algorithms to automatic and unsupervised texture segmentation . some examples in colour maps are presented and overall results discussed . keywords : genetic algorithms , artificial neoteny , dynamic mutation rates , faster convergence , colour image segmentation , classification , clustering .

speaking the same language : matching machine to human captions by adversarial training
while strong progress has been made in image captioning over the last years , machine and human captions are still quite distinct . a closer look reveals that this is due to the deficiencies in the generated word distribution , vocabulary size , and strong bias in the generators towards frequent captions . furthermore , humans -- rightfully so -- generate multiple , diverse captions , due to the inherent ambiguity in the captioning task which is not considered in today 's systems . to address these challenges , we change the training objective of the caption generator from reproducing groundtruth captions to generating a set of captions that is indistinguishable from human generated captions . instead of handcrafting such a learning target , we employ adversarial training in combination with an approximate gumbel sampler to implicitly match the generated distribution to the human one . while our method achieves comparable performance to the state-of-the-art in terms of the correctness of the captions , we generate a set of diverse captions , that are significantly less biased and match the word statistics better in several aspects .

a corpus and evaluation framework for deeper understanding of commonsense stories
representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding . this issue is particularly challenging for understanding casual and correlational relationships between events . while this topic has received a lot of interest in the nlp community , research has been hindered by the lack of a proper evaluation framework . this paper attempts to address this problem with a new framework for evaluating story understanding and script learning : the 'story cloze test ' . this test requires a system to choose the correct ending to a four-sentence story . we created a new corpus of ~50k five-sentence commonsense stories , rocstories , to enable this evaluation . this corpus is unique in two ways : ( 1 ) it captures a rich set of causal and temporal commonsense relations between daily events , and ( 2 ) it is a high quality collection of everyday life stories that can also be used for story generation . experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the story cloze test . we discuss these implications for script and story learning , and offer suggestions for deeper language understanding .

a game-theoretic model motivated by the darpa network challenge
in this paper we propose a game-theoretic model to analyze events similar to the 2009 \emph { darpa network challenge } , which was organized by the defense advanced research projects agency ( darpa ) for exploring the roles that the internet and social networks play in incentivizing wide-area collaborations . the challenge was to form a group that would be the first to find the locations of ten moored weather balloons across the united states . we consider a model in which $ n $ people ( who can form groups ) are located in some topology with a fixed coverage volume around each person 's geographical location . we consider various topologies where the players can be located such as the euclidean $ d $ -dimension space and the vertices of a graph . a balloon is placed in the space and a group wins if it is the first one to report the location of the balloon . a larger team has a higher probability of finding the balloon , but we assume that the prize money is divided equally among the team members . hence there is a competing tension to keep teams as small as possible . \emph { risk aversion } is the reluctance of a person to accept a bargain with an uncertain payoff rather than another bargain with a more certain , but possibly lower , expected payoff . in our model we consider the \emph { isoelastic } utility function derived from the arrow-pratt measure of relative risk aversion . the main aim is to analyze the structures of the groups in nash equilibria for our model . for the $ d $ -dimensional euclidean space ( $ d\geq 1 $ ) and the class of bounded degree regular graphs we show that in any nash equilibrium the \emph { richest } group ( having maximum expected utility per person ) covers a constant fraction of the total volume .

plan explicability and predictability for robot task planning
intelligent robots and machines are becoming pervasive in human populated environments . a desirable capability of these agents is to respond to goal-oriented commands by autonomously constructing task plans . however , such autonomy can add significant cognitive load and potentially introduce safety risks to humans when agents behave unexpectedly . hence , for such agents to be helpful , one important requirement is for them to synthesize plans that can be easily understood by humans . while there exists previous work that studied socially acceptable robots that interact with humans in `` natural ways '' , and work that investigated legible motion planning , there lacks a general solution for high level task planning . to address this issue , we introduce the notions of plan { \it explicability } and { \it predictability } . to compute these measures , first , we postulate that humans understand agent plans by associating abstract tasks with agent actions , which can be considered as a labeling process . we learn the labeling scheme of humans for agent plans from training examples using conditional random fields ( crfs ) . then , we use the learned model to label a new plan to compute its explicability and predictability . these measures can be used by agents to proactively choose or directly synthesize plans that are more explicable and predictable to humans . we provide evaluations on a synthetic domain and with human subjects using physical robots to show the effectiveness of our approach

simulated tornado optimization
we propose a swarm-based optimization algorithm inspired by air currents of a tornado . two main air currents - spiral and updraft - are mimicked . spiral motion is designed for exploration of new search areas and updraft movements is deployed for exploitation of a promising candidate solution . assignment of just one search direction to each particle at each iteration , leads to low computational complexity of the proposed algorithm respect to the conventional algorithms . regardless of the step size parameters , the only parameter of the proposed algorithm , called tornado diameter , can be efficiently adjusted by randomization . numerical results over six different benchmark cost functions indicate comparable and , in some cases , better performance of the proposed algorithm respect to some other metaheuristics .

generating nontrivial melodies for music as a service
we present a hybrid neural network and rule-based system that generates pop music . music produced by pure rule-based systems often sounds mechanical . music produced by machine learning sounds better , but still lacks hierarchical temporal structure . we restore temporal hierarchy by augmenting machine learning with a temporal production grammar , which generates the music 's overall structure and chord progressions . a compatible melody is then generated by a conditional variational recurrent autoencoder . the autoencoder is trained with eight-measure segments from a corpus of 10,000 midi files , each of which has had its melody track and chord progressions identified heuristically . the autoencoder maps melody into a multi-dimensional feature space , conditioned by the underlying chord progression . a melody is then generated by feeding a random sample from that space to the autoencoder 's decoder , along with the chord progression generated by the grammar . the autoencoder can make musically plausible variations on an existing melody , suitable for recurring motifs . it can also reharmonize a melody to a new chord progression , keeping the rhythm and contour . the generated music compares favorably with that generated by other academic and commercial software designed for the music-as-a-service industry .

causal decision trees
uncovering causal relationships in data is a major objective of data analytics . causal relationships are normally discovered with designed experiments , e.g . randomised controlled trials , which , however are expensive or infeasible to be conducted in many cases . causal relationships can also be found using some well designed observational studies , but they require domain experts ' knowledge and the process is normally time consuming . hence there is a need for scalable and automated methods for causal relationship exploration in data . classification methods are fast and they could be practical substitutes for finding causal signals in data . however , classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones . in this paper , we develop a causal decision tree where nodes have causal interpretations . our method follows a well established causal inference framework and makes use of a classic statistical test . the method is practical for finding causal signals in large data sets .

neural network influence in group technology : a chronological survey and critical analysis
this article portrays a chronological review of the influence of artificial neural network in group technology applications in the vicinity of cellular manufacturing systems . the research trend is identified and the evolvement is captured through a critical analysis of the literature accessible from the very beginning of its practice in the early 90 's till the 2010. analysis of the diverse ann approaches , spotted research pattern , comparison of the clustering efficiencies , the solutions obtained and the tools used make this study exclusive in its class .

bringing salary transparency to the world : computing robust compensation insights via linkedin salary
the recently launched linkedin salary product has been designed with the goal of providing compensation insights to the world 's professionals and thereby helping them optimize their earning potential . we describe the overall design and architecture of the statistical modeling system underlying this product . we focus on the unique data mining challenges while designing and implementing the system , and describe the modeling components such as bayesian hierarchical smoothing that help to compute and present robust compensation insights to users . we report on extensive evaluation with nearly one year of de-identified compensation data collected from over one million linkedin users , thereby demonstrating the efficacy of the statistical models . we also highlight the lessons learned through the deployment of our system at linkedin .

lower bounds for exact model counting and applications in probabilistic databases
the best current methods for exactly computing the number of satisfying assignments , or the satisfying probability , of boolean formulas can be seen , either directly or indirectly , as building 'decision-dnnf ' ( decision decomposable negation normal form ) representations of the input boolean formulas . decision-dnnfs are a special case of 'd-dnnf 's where 'd ' stands for 'deterministic ' . we show that any decision-dnnf can be converted into an equivalent 'fbdd ' ( free binary decision diagram ) -- also known as a 'read-once branching program ' ( robp or 1-bp ) -- with only a quasipolynomial increase in representation size in general , and with only a polynomial increase in size in the special case of monotone k-dnf formulas . leveraging known exponential lower bounds for fbdds , we then obtain similar exponential lower bounds for decision-dnnfs which provide lower bounds for the recent algorithms . we also separate the power of decision-dnnfs from d-dnnfs and a generalization of decision-dnnfs known as and-fbdds . finally we show how these imply exponential lower bounds for natural problems associated with probabilistic databases .

efficient open world reasoning for planning
we consider the problem of reasoning and planning with incomplete knowledge and deterministic actions . we introduce a knowledge representation scheme called psiplan that can effectively represent incompleteness of an agent 's knowledge while allowing for sound , complete and tractable entailment in domains where the set of all objects is either unknown or infinite . we present a procedure for state update resulting from taking an action in psiplan that is correct , complete and has only polynomial complexity . state update is performed without considering the set of all possible worlds corresponding to the knowledge state . as a result , planning with psiplan is done without direct manipulation of possible worlds . psiplan representation underlies the psipop planning algorithm that handles quantified goals with or without exceptions that no other domain independent planner has been shown to achieve . psiplan has been implemented in common lisp and used in an application on planning in a collaborative interface .

model-based value estimation for efficient model-free reinforcement learning
recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity . such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks . unfortunately , they rely on heuristics that limit usage of the dynamics model . we present model-based value expansion , which controls for uncertainty in the model by only allowing imagination to fixed depth . by enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm , we improve value estimation , which , in turn , reduces the sample complexity of learning .

node discovery in a networked organization
in this paper , i present a method to solve a node discovery problem in a networked organization . covert nodes refer to the nodes which are not observable directly . they affect social interactions , but do not appear in the surveillance logs which record the participants of the social interactions . discovering the covert nodes is defined as identifying the suspicious logs where the covert nodes would appear if the covert nodes became overt . a mathematical model is developed for the maximal likelihood estimation of the network behind the social interactions and for the identification of the suspicious logs . precision , recall , and f measure characteristics are demonstrated with the dataset generated from a real organization and the computationally synthesized datasets . the performance is close to the theoretical limit for any covert nodes in the networks of any topologies and sizes if the ratio of the number of observation to the number of possible communication patterns is large .

a geometric framework for convolutional neural networks
in this paper , a geometric framework for neural networks is proposed . this framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component-based form , but in a coordinate-free manner . convolutional neural networks are described in this framework in a compact form , with the gradients of standard -- - and higher-order -- - loss functions calculated for each layer of the network . this approach can be applied to other network structures and provides a basis on which to create new networks .

integrating conflict driven clause learning to local search
this article introduces sathys ( sat hybrid solver ) , a novel hybrid approach for propositional satisfiability . it combines local search and conflict driven clause learning ( cdcl ) scheme . each time the local search part reaches a local minimum , the cdcl is launched . for sat problems it behaves like a tabu list , whereas for unsat ones , the cdcl part tries to focus on minimum unsatisfiable sub-formula ( mus ) . experimental results show good performances on many classes of sat instances from the last sat competitions .

personalized fuzzy text search using interest prediction and word vectorization
in this paper we study the personalized text search problem . the keyword based search method in conventional algorithms has a low efficiency in understanding users ' intention since the semantic meaning , user profile , user interests are not always considered . firstly , we propose a novel text search algorithm using a inverse filtering mechanism that is very efficient for label based item search . secondly , we adopt the bayesian network to implement the user interest prediction for an improved personalized search . according to user input , it searches the related items using keyword information , predicted user interest . thirdly , the word vectorization is used to discover potential targets according to the semantic meaning . experimental results show that the proposed search engine has an improved efficiency and accuracy and it can operate on embedded devices with very limited computational resources .

on coreferring text-extracted event descriptions with the aid of ontological reasoning
systems for automatic extraction of semantic information about events from large textual resources are now available : these tools are capable to generate rdf datasets about text extracted events and this knowledge can be used to reason over the recognized events . on the other hand , text based tasks for event recognition , as for example event coreference ( i.e . recognizing whether two textual descriptions refer to the same event ) , do not take into account ontological information of the extracted events in their process . in this paper , we propose a method to derive event coreference on text extracted event data using semantic based rule reasoning . we demonstrate our method considering a limited ( yet representative ) set of event types : we introduce a formal analysis on their ontological properties and , on the base of this , we define a set of coreference criteria . we then implement these criteria as rdf-based reasoning rules to be applied on text extracted event data . we evaluate the effectiveness of our approach over a standard coreference benchmark dataset .

how , what and why to test an ontology
ontology development relates to software development in that they both involve the production of formal computational knowledge . it is possible , therefore , that some of the techniques used in software engineering could also be used for ontologies ; for example , in software engineering testing is a well-established process , and part of many different methodologies . the application of testing to ontologies , therefore , seems attractive . the karyotype ontology is developed using the novel tawny-owl library . this provides a fully programmatic environment for ontology development , which includes a complete test harness . in this paper , we describe how we have used this harness to build an extensive series of tests as well as used a commodity continuous integration system to link testing deeply into our development process ; this environment , is applicable to any owl ontology whether written using tawny-owl or not . moreover , we present a novel analysis of our tests , introducing a new classification of what our different tests are . for each class of test , we describe why we use these tests , also by comparison to software tests . we believe that this systematic comparison between ontology and software development will help us move to a more agile form of ontology development .

prioritized sweeping neural dynaq with multiple predecessors , and hippocampal replays
during sleep and awake rest , the hippocampus replays sequences of place cells that have been activated during prior experiences . these have been interpreted as a memory consolidation process , but recent results suggest a possible interpretation in terms of reinforcement learning . the dyna reinforcement learning algorithms use off-line replays to improve learning . under limited replay budget , a prioritized sweeping approach , which requires a model of the transitions to the predecessors , can be used to improve performance . we investigate whether such algorithms can explain the experimentally observed replays . we propose a neural network version of prioritized sweeping q-learning , for which we developed a growing multiple expert algorithm , able to cope with multiple predecessors . the resulting architecture is able to improve the learning of simulated agents confronted to a navigation task . we predict that , in animals , learning the world model should occur during rest periods , and that the corresponding replays should be shuffled .

intent expression using eye robot for mascot robot system
an intent expression system using eye robots is proposed for a mascot robot system from a viewpoint of humatronics . the eye robot aims at providing a basic interface method for an information terminal robot system . to achieve better understanding of the displayed information , the importance and the degree of certainty of the information should be communicated along with the main content . the proposed intent expression system aims at conveying this additional information using the eye robot system . eye motions are represented as the states in a pleasure-arousal space model . changes in the model state are calculated by fuzzy inference according to the importance and degree of certainty of the displayed information . these changes influence the arousal-sleep coordinates in the space that corresponds to levels of liveliness during communication . the eye robot provides a basic interface for the mascot robot system that is easy to be understood as an information terminal for home environments in a humatronics society .

bethe-admm for tree decomposition based parallel map inference
we consider the problem of maximum a posteriori ( map ) inference in discrete graphical models . we present a parallel map inference algorithm called bethe-admm based on two ideas : tree-decomposition of the graph and the alternating direction method of multipliers ( admm ) . however , unlike the standard admm , we use an inexact admm augmented with a bethe-divergence based proximal function , which makes each subproblem in admm easy to solve in parallel using the sum-product algorithm . we rigorously prove global convergence of bethe-admm . the proposed algorithm is extensively evaluated on both synthetic and real datasets to illustrate its effectiveness . further , the parallel bethe-admm is shown to scale almost linearly with increasing number of cores .

knowledge engineering for planning-based hypothesis generation
in this paper , we address the knowledge engineering problems for hypothesis generation motivated by applications that require timely exploration of hypotheses under unreliable observations . we looked at two applications : malware detection and intensive care delivery . in intensive care , the goal is to generate plausible hypotheses about the condition of the patient from clinical observations and further refine these hypotheses to create a recovery plan for the patient . similarly , preventing malware spread within a corporate network involves generating hypotheses from network traffic data and selecting preventive actions . to this end , building on the already established characterization and use of ai planning for similar problems , we propose use of planning for the hypothesis generation problem . however , to deal with uncertainty , incomplete model description and unreliable observations , we need to use a planner capable of generating multiple high-quality plans . to capture the model description we propose a language called lts++ and a web-based tool that enables the specification of the lts++ model and a set of observations . we also proposed a 9-step process that helps provide guidance to the domain expert in specifying the lts++ model . the hypotheses are then generated by running a planner on the translated lts++ model and the provided trace . the hypotheses can be visualized and shown to the analyst or can be further investigated automatically .

raising a hardness result
this article presents a technique for proving problems hard for classes of the polynomial hierarchy or for pspace . the rationale of this technique is that some problem restrictions are able to simulate existential or universal quantifiers . if this is the case , reductions from quantified boolean formulae ( qbf ) to these restrictions can be transformed into reductions from qbfs having one more quantifier in the front . this means that a proof of hardness of a problem at level n in the polynomial hierarchy can be split into n separate proofs , which may be simpler than a proof directly showing a reduction from a class of qbfs to the considered problem .

sparse stochastic finite-state controllers for pomdps
bounded policy iteration is an approach to solving infinite-horizon pomdps that represents policies as stochastic finite-state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming . in the original algorithm , the size of the linear programs , and thus the complexity of policy improvement , depends on the number of parameters of each node , which grows with the size of the controller . but in practice , the number of parameters of a node with non-zero values is often very small , and does not grow with the size of the controller . based on this observation , we develop a version of bounded policy iteration that leverages the sparse structure of a stochastic finite-state controller . in each iteration , it improves a policy by the same amount as the original algorithm , but with much better scalability .

pipe : personalizing recommendations via partial evaluation
it is shown that personalization of web content can be advantageously viewed as a form of partial evaluation -- - a technique well known in the programming languages community . the basic idea is to model a recommendation space as a program , then partially evaluate this program with respect to user preferences ( and features ) to obtain specialized content . this technique supports both content-based and collaborative approaches , and is applicable to a range of applications that require automatic information integration from multiple web sources . the effectiveness of this methodology is illustrated by two example applications -- - ( i ) personalizing content for visitors to the blacksburg electronic village ( http : //www.bev.net ) , and ( ii ) locating and selecting scientific software on the internet . the scalability of this technique is demonstrated by its ability to interface with online web ontologies that index thousands of web pages .

computing strong game-theoretic strategies in jotto
we develop a new approach that computes approximate equilibrium strategies in jotto , a popular word game . jotto is an extremely large two-player game of imperfect information ; its game tree has many orders of magnitude more states than games previously studied , including no-limit texas hold 'em . to address the fact that the game is so large , we propose a novel strategy representation called oracular form , in which we do not explicitly represent a strategy , but rather appeal to an oracle that quickly outputs a sample move from the strategy 's distribution . our overall approach is based on an extension of the fictitious play algorithm to this oracular setting . we demonstrate the superiority of our computed strategies over the strategies computed by a benchmark algorithm , both in terms of head-to-head and worst-case performance .

ncbo ontology recommender 2.0 : an enhanced approach for biomedical ontology recommendation
biomedical researchers use ontologies to annotate their data with ontology terms , enabling better data integration and interoperability . however , the number , variety and complexity of current biomedical ontologies make it cumbersome for researchers to determine which ones to reuse for their specific needs . to overcome this problem , in 2010 the national center for biomedical ontology ( ncbo ) released the ontology recommender , which is a service that receives a biomedical text corpus or a list of keywords and suggests ontologies appropriate for referencing the indicated terms . we developed a new version of the ncbo ontology recommender . called ontology recommender 2.0 , it uses a new recommendation approach that evaluates the relevance of an ontology to biomedical text data according to four criteria : ( 1 ) the extent to which the ontology covers the input data ; ( 2 ) the acceptance of the ontology in the biomedical community ; ( 3 ) the level of detail of the ontology classes that cover the input data ; and ( 4 ) the specialization of the ontology to the domain of the input data . our evaluation shows that the enhanced recommender provides higher quality suggestions than the original approach , providing better coverage of the input data , more detailed information about their concepts , increased specialization for the domain of the input data , and greater acceptance and use in the community . in addition , it provides users with more explanatory information , along with suggestions of not only individual ontologies but also groups of ontologies . it also can be customized to fit the needs of different scenarios . ontology recommender 2.0 combines the strengths of its predecessor with a range of adjustments and new features that improve its reliability and usefulness . ontology recommender 2.0 recommends over 500 biomedical ontologies from the ncbo bioportal platform , where it is openly available .

auto-adaptative laplacian pyramids for high-dimensional data analysis
non-linear dimensionality reduction techniques such as manifold learning algorithms have become a common way for processing and analyzing high-dimensional patterns that often have attached a target that corresponds to the value of an unknown function . their application to new points consists in two steps : first , embedding the new data point into the low dimensional space and then , estimating the function value on the test point from its neighbors in the embedded space . however , finding the low dimension representation of a test point , while easy for simple but often not powerful enough procedures such as pca , can be much more complicated for methods that rely on some kind of eigenanalysis , such as spectral clustering ( sc ) or diffusion maps ( dm ) . similarly , when a target function is to be evaluated , averaging methods like nearest neighbors may give unstable results if the function is noisy . thus , the smoothing of the target function with respect to the intrinsic , low-dimensional representation that describes the geometric structure of the examined data is a challenging task . in this paper we propose auto-adaptive laplacian pyramids ( alp ) , an extension of the standard laplacian pyramids model that incorporates a modified loocv procedure that avoids the large cost of the standard one and offers the following advantages : ( i ) it selects automatically the optimal function resolution ( stopping time ) adapted to the data and its noise , ( ii ) it is easy to apply as it does not require parameterization , ( iii ) it does not overfit the training set and ( iv ) it adds no extra cost compared to other classical interpolation methods . we illustrate numerically alp 's behavior on a synthetic problem and apply it to the computation of the dm projection of new patterns and to the extension to them of target function values on a radiation forecasting problem over very high dimensional patterns .

an application-oriented terminology evaluation : the case of back-of-the book indexes
this paper addresses the problem of computational terminology evaluation not per se but in a specific application context . this paper describes the evaluation procedure that has been used to assess the validity of our overall indexing approach and the quality of the inddoc indexing tool . even if user-oriented extended evaluation is irreplaceable , we argue that early evaluations are possible and they are useful for development guidance .

managing autonomous mobility on demand systems for better passenger experience
autonomous mobility on demand systems , though still in their infancy , have very promising prospects in providing urban population with sustainable and safe personal mobility in the near future . while much research has been conducted on both autonomous vehicles and mobility on demand systems , to the best of our knowledge , this is the first work that shows how to manage autonomous mobility on demand systems for better passenger experience . we introduce the expand and target algorithm which can be easily integrated with three different scheduling strategies for dispatching autonomous vehicles . we implement an agent-based simulation platform and empirically evaluate the proposed approaches with the new york city taxi data . experimental results demonstrate that the algorithm significantly improve passengers ' experience by reducing the average passenger waiting time by up to 29.82 % and increasing the trip success rate by up to 7.65 % .

bounded optimal exploration in mdp
within the framework of probably approximately correct markov decision processes ( pac-mdp ) , much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration . however , practical concerns require the attainment of satisfactory behavior within a short period of time . in this paper , we relax the pac-mdp conditions to reconcile theoretically driven exploration methods and practical needs . we propose simple algorithms for discrete and continuous state spaces , and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples . our algorithms also maintain anytime error bounds and average loss bounds . our approach accommodates both bayesian and non-bayesian methods .

the ai & m procedure for learning from incomplete data
we investigate methods for parameter learning from incomplete data that is not missing at random . likelihood-based methods then require the optimization of a profile likelihood that takes all possible missingness mechanisms into account . optimzing this profile likelihood poses two main difficulties : multiple ( local ) maxima , and its very high-dimensional parameter space . in this paper a new method is presented for optimizing the profile likelihood that addresses the second difficulty : in the proposed ai & m ( adjusting imputation and mazimization ) procedure the optimization is performed by operations in the space of data completions , rather than directly in the parameter space of the profile likelihood . we apply the ai & m method to learning parameters for bayesian networks . the method is compared against conservative inference , which takes into account each possible data completion , and against em . the results indicate that likelihood-based inference is still feasible in the case of unknown missingness mechanisms , and that conservative inference is unnecessarily weak . on the other hand , our results also provide evidence that the em algorithm is still quite effective when the data is not missing at random .

different goals in multiscale simulations and how to reach them
in this paper we sum up our works on multiscale programs , mainly simulations . we first start with describing what multiscaling is about , how it helps perceiving signal from a background noise in a ? ow of data for example , for a direct perception by a user or for a further use by another program . we then give three examples of multiscale techniques we used in the past , maintaining a summary , using an environmental marker introducing an history in the data and finally using a knowledge on the behavior of the different scales to really handle them at the same time .

indebted households profiling : a knowledge discovery from database approach
a major challenge in consumer credit risk portfolio management is to classify households according to their risk profile . in order to build such risk profiles it is necessary to employ an approach that analyses data systematically in order to detect important relationships , interactions , dependencies and associations amongst the available continuous and categorical variables altogether and accurately generate profiles of most interesting household segments according to their credit risk . the objective of this work is to employ a knowledge discovery from database process to identify groups of indebted households and describe their profiles using a database collected by the consumer credit counselling service ( cccs ) in the uk . employing a framework that allows the usage of both categorical and continuous data altogether to find hidden structures in unlabelled data it was established the ideal number of clusters and such clusters were described in order to identify the households who exhibit a high propensity of excessive debt levels .

ontologies and information extraction
this report argues that , even in the simplest cases , ie is an ontology-driven process . it is not a mere text filtering method based on simple pattern matching and keywords , because the extracted pieces of texts are interpreted with respect to a predefined partial domain model . this report shows that depending on the nature and the depth of the interpretation to be done for extracting the information , more or less knowledge must be involved . this report is mainly illustrated in biology , a domain in which there are critical needs for content-based exploration of the scientific literature and which becomes a major application domain for ie .

polynomial constraints in causal bayesian networks
we use the implicitization procedure to generate polynomial equality constraints on the set of distributions induced by local interventions on variables governed by a causal bayesian network with hidden variables . we show how we may reduce the complexity of the implicitization problem and make the problem tractable in certain causal bayesian networks . we also show some preliminary results on the algebraic structure of polynomial constraints . the results have applications in distinguishing between causal models and in testing causal models with combined observational and experimental data .

a statistical learning theory framework for supervised pattern discovery
this paper formalizes a latent variable inference problem we call { \em supervised pattern discovery } , the goal of which is to find sets of observations that belong to a single `` pattern . '' we discuss two versions of the problem and prove uniform risk bounds for both . in the first version , collections of patterns can be generated in an arbitrary manner and the data consist of multiple labeled collections . in the second version , the patterns are assumed to be generated independently by identically distributed processes . these processes are allowed to take an arbitrary form , so observations within a pattern are not in general independent of each other . the bounds for the second version of the problem are stated in terms of a new complexity measure , the quasi-rademacher complexity .

neuro-symbolic eda-based optimisation using ilp-enhanced dbns
we investigate solving discrete optimisation problems using the estimation of distribution ( eda ) approach via a novel combination of deep belief networks ( dbn ) and inductive logic programming ( ilp ) .while dbns are used to learn the structure of successively better feasible solutions , ilp enables the incorporation of domain-based background knowledge related to the goodness of solutions.recent work showed that ilp could be an effective way to use domain knowledge in an eda scenario.however , in a purely ilp-based eda , sampling successive populations is either inefficient or not straightforward.in our neuro-symbolic eda , an ilp engine is used to construct a model for good solutions using domain-based background knowledge.these rules are introduced as boolean features in the last hidden layer of dbns used for eda-based optimization.this incorporation of logical ilp features requires some changes while training and sampling from dbns : ( a ) our dbns need to be trained with data for units at the input layer as well as some units in an otherwise hidden layer , and ( b ) we would like the samples generated to be drawn from instances entailed by the logical model.we demonstrate the viability of our approach on instances of two optimisation problems : predicting optimal depth-of-win for the krk endgame , and jobshop scheduling.our results are promising : ( i ) on each iteration of distribution estimation , samples obtained with an ilp-assisted dbn have a substantially greater proportion of good solutions than samples generated using a dbn without ilp features , and ( ii ) on termination of distribution estimation , samples obtained using an ilp-assisted dbn contain more near-optimal samples than samples from a dbn without ilp features.these results suggest that the use of ilp-constructed theories could be useful for incorporating complex domain-knowledge into deep models for estimation of distribution based procedures .

quizz : targeted crowdsourcing with a billion ( potential ) users
we describe quizz , a gamified crowdsourcing system that simultaneously assesses the knowledge of users and acquires new knowledge from them . quizz operates by asking users to complete short quizzes on specific topics ; as a user answers the quiz questions , quizz estimates the user 's competence . to acquire new knowledge , quizz also incorporates questions for which we do not have a known answer ; the answers given by competent users provide useful signals for selecting the correct answers for these questions . quizz actively tries to identify knowledgeable users on the internet by running advertising campaigns , effectively leveraging the targeting capabilities of existing , publicly available , ad placement services . quizz quantifies the contributions of the users using information theory and sends feedback to the advertisingsystem about each user . the feedback allows the ad targeting mechanism to further optimize ad placement . our experiments , which involve over ten thousand users , confirm that we can crowdsource knowledge curation for niche and specialized topics , as the advertising network can automatically identify users with the desired expertise and interest in the given topic . we present controlled experiments that examine the effect of various incentive mechanisms , highlighting the need for having short-term rewards as goals , which incentivize the users to contribute . finally , our cost-quality analysis indicates that the cost of our approach is below that of hiring workers through paid-crowdsourcing platforms , while offering the additional advantage of giving access to billions of potential users all over the planet , and being able to reach users with specialized expertise that is not typically available through existing labor marketplaces .

performance evaluation of dca and src on a single bot detection
malicious users try to compromise systems using new techniques . one of the recent techniques used by the attacker is to perform complex distributed attacks such as denial of service and to obtain sensitive data such as password information . these compromised machines are said to be infected with malicious software termed a `` bot '' . in this paper , we investigate the correlation of behavioural attributes such as keylogging and packet flooding behaviour to detect the existence of a single bot on a compromised machine by applying ( 1 ) spearman 's rank correlation ( src ) algorithm and ( 2 ) the dendritic cell algorithm ( dca ) . we also compare the output results generated from these two methods to the detection of a single bot . the results show that the dca has a better performance in detecting malicious activities .

a computational model to disentangle semantic information embedded in word association norms
two well-known databases of semantic relationships between pairs of words used in psycholinguistics , feature-based and association-based , are studied as complex networks . we propose an algorithm to disentangle feature based relationships from free association semantic networks . the algorithm uses the rich topology of the free association semantic network to produce a new set of relationships between words similar to those observed in feature production norms .

from likelihood to plausibility
several authors have explained that the likelihood ratio measures the strength of the evidence represented by observations in statistical problems . this idea works fine when the goal is to evaluate the strength of the available evidence for a simple hypothesis versus another simple hypothesis . however , the applicability of this idea is limited to simple hypotheses because the likelihood function is primarily defined on points ( simple hypotheses ) of the parameter space . in this paper we define a general weight of evidence that is applicable to both simple and composite hypotheses . it is based on the dempster-shafer concept of plausibility and is shown to be a generalization of the likelihood ratio . functional models are of a fundamental importance for the general weight of evidence proposed in this paper . the relevant concepts and ideas are explained by means of a familiar urn problem and the general analysis of a real-world medical problem is presented .

geometric analogue of holographic reduced representation
holographic reduced representations ( hrr ) are based on superpositions of convolution-bound $ n $ -tuples , but the $ n $ -tuples can not be regarded as vectors since the formalism is basis dependent . this is why hrr can not be associated with geometric structures . replacing convolutions by geometric products one arrives at reduced representations analogous to hrr but interpretable in terms of geometry . variable bindings occurring in both hrr and its geometric analogue mathematically correspond to two different representations of $ z_2\times ... \times z_2 $ ( the additive group of binary $ n $ -tuples with addition modulo 2 ) . as opposed to standard hrr , variable binding performed by means of geometric product allows for computing exact inverses of all nonzero vectors , a procedure even simpler than approximate inverses employed in hrr . the formal structure of the new reduced representation is analogous to cartoon computation , a geometric analogue of quantum computation .

multi-agent inverse reinforcement learning for zero-sum games
in this paper we introduce a bayesian framework for solving a class of problems termed multi-agent inverse reinforcement learning ( mirl ) . compared to the well-known inverse reinforcement learning ( irl ) problem , mirl is formalized in the context of a stochastic game rather than a markov decision process ( mdp ) . games bring two primary challenges : first , the concept of optimality , central to mdps , loses its meaning and must be replaced with a more general solution concept , such as the nash equilibrium . second , the non-uniqueness of equilibria means that in mirl , in addition to multiple reasonable solutions for a given inversion model , there may be multiple inversion models that are all equally sensible approaches to solving the problem . we establish a theoretical foundation for competitive two-agent mirl problems and propose a bayesian optimization algorithm to solve the problem . we focus on the case of two-person zero-sum stochastic games , developing a generative model for the likelihood of unknown rewards of agents given observed game play assuming that the two agents follow a minimax bipolicy . as a numerical illustration , we apply our method in the context of an abstract soccer game . for the soccer game , we investigate relationships between the extent of prior information and the quality of learned rewards . results suggest that covariance structure is more important than mean value in reward priors .

deontic modality based on preference
deontic modalities are here defined in terms of the preference relation explored in our previous work ( osherson and weinstein , 2012 ) . some consequences of the system are discussed .

sensor scheduling for optimal observability using estimation entropy
we consider sensor scheduling as the optimal observability problem for partially observable markov decision processes ( pomdp ) . this model fits to the cases where a markov process is observed by a single sensor which needs to be dynamically adjusted or by a set of sensors which are selected one at a time in a way that maximizes the information acquisition from the process . similar to conventional pomdp problems , in this model the control action is based on all past measurements ; however here this action is not for the control of state process , which is autonomous , but it is for influencing the measurement of that process . this pomdp is a controlled version of the hidden markov process , and we show that its optimal observability problem can be formulated as an average cost markov decision process ( mdp ) scheduling problem . in this problem , a policy is a rule for selecting sensors or adjusting the measuring device based on the measurement history . given a policy , we can evaluate the estimation entropy for the joint state-measurement processes which inversely measures the observability of state process for that policy . considering estimation entropy as the cost of a policy , we show that the problem of finding optimal policy is equivalent to an average cost mdp scheduling problem where the cost function is the entropy function over the belief space . this allows the application of the policy iteration algorithm for finding the policy achieving minimum estimation entropy , thus optimum observability .

graph planning with expected finite horizon
graph planning gives rise to fundamental algorithmic questions such as shortest path , traveling salesman problem , etc . a classical problem in discrete planning is to consider a weighted graph and construct a path that maximizes the sum of weights for a given time horizon $ t $ . however , in many scenarios , the time horizon is not fixed , but the stopping time is chosen according to some distribution such that the expected stopping time is $ t $ . if the stopping time distribution is not known , then to ensure robustness , the distribution is chosen by an adversary , to represent the worst-case scenario . a stationary plan for every vertex always chooses the same outgoing edge . for fixed horizon or fixed stopping-time distribution , stationary plans are not sufficient for optimality . quite surprisingly we show that when an adversary chooses the stopping-time distribution with expected stopping time $ t $ , then stationary plans are sufficient . while computing optimal stationary plans for fixed horizon is np-complete , we show that computing optimal stationary plans under adversarial stopping-time distribution can be achieved in polynomial time . consequently , our polynomial-time algorithm for adversarial stopping time also computes an optimal plan among all possible plans .

resolving distributed knowledge
distributed knowledge is the sum of the knowledge in a group ; what someone who is able to discern between two possible worlds whenever any member of the group can discern between them , would know . sometimes distributed knowledge is referred to as the potential knowledge of a group , or the joint knowledge they could obtain if they had unlimited means of communication . in epistemic logic , the formula d_g { \phi } is intended to express the fact that group g has distributed knowledge of { \phi } , that there is enough information in the group to infer { \phi } . but this is not the same as reasoning about what happens if the members of the group share their information . in this paper we introduce an operator r_g , such that r_g { \phi } means that { \phi } is true after g have shared all their information with each other - after g 's distributed knowledge has been resolved . the r_g operators are called resolution operators . semantically , we say that an expression r_g { \phi } is true iff { \phi } is true in what van benthem [ 11 , p. 249 ] calls ( g 's ) communication core ; the model update obtained by removing links to states for members of g that are not linked by all members of g. we study logics with different combinations of resolution operators and operators for common and distributed knowledge . of particular interest is the relationship between distributed and common knowledge . the main results are sound and complete axiomatizations .

test set selection using active information acquisition for predictive models
in this paper , we consider active information acquisition when the prediction model is meant to be applied on a targeted subset of the population . the goal is to label a pre-specified fraction of customers in the target or test set by iteratively querying for information from the non-target or training set . the number of queries is limited by an overall budget . arising in the context of two rather disparate applications- banking and medical diagnosis , we pose the active information acquisition problem as a constrained optimization problem . we propose two greedy iterative algorithms for solving the above problem . we conduct experiments with synthetic data and compare results of our proposed algorithms with few other baseline approaches . the experimental results show that our proposed approaches perform better than the baseline schemes .

fundamentals of mathematical theory of emotional robots
in this book we introduce a mathematically formalized concept of emotion , robot 's education and other psychological parameters of intelligent robots . we also introduce unitless coefficients characterizing an emotional memory of a robot . besides , the effect of a robot 's memory upon its emotional behavior is studied , and theorems defining fellowship and conflicts in groups of robots are proved . also unitless parameters describing emotional states of those groups are introduced , and a rule of making alternative ( binary ) decisions based on emotional selection is given . we introduce a concept of equivalent educational process for robots and a concept of efficiency coefficient of an educational process , and suggest an algorithm of emotional contacts within a group of robots . and generally , we present and describe a model of a virtual reality with emotional robots . the book is meant for mathematical modeling specialists and emotional robot software developers .

anomaly detection using the knowledge-based temporal abstraction method
the rapid growth in stored time-oriented data necessitates the development of new methods for handling , processing , and interpreting large amounts of temporal data . one important example of such processing is detecting anomalies in time-oriented data . the knowledge-based temporal abstraction method was previously proposed for intelligent interpretation of temporal data based on predefined domain knowledge . in this study we propose a framework that integrates the kbta method with a temporal pattern mining process for anomaly detection . according to the proposed method a temporal pattern mining process is applied on a dataset of basic temporal abstraction database in order to extract patterns representing normal behavior . these patterns are then analyzed in order to identify abnormal time periods characterized by a significantly small number of normal patterns . the proposed approach was demonstrated using a dataset collected from a real server .

crime prediction based on crime types and using spatial and temporal criminal hotspots
this paper focuses on finding spatial and temporal criminal hotspots . it analyses two different real-world crimes datasets for denver , co and los angeles , ca and provides a comparison between the two datasets through a statistical analysis supported by several graphs . then , it clarifies how we conducted apriori algorithm to produce interesting frequent patterns for criminal hotspots . in addition , the paper shows how we used decision tree classifier and naive bayesian classifier in order to predict potential crime types . to further analyse crimes datasets , the paper introduces an analysis study by combining our findings of denver crimes dataset with its demographics information in order to capture the factors that might affect the safety of neighborhoods . the results of this solution could be used to raise awareness regarding the dangerous locations and to help agencies to predict future crimes in a specific location within a particular time .

evidence and plausibility in neighborhood structures
the intuitive notion of evidence has both semantic and syntactic features . in this paper , we develop an { \em evidence logic } for epistemic agents faced with possibly contradictory evidence from different sources . the logic is based on a neighborhood semantics , where a neighborhood $ n $ indicates that the agent has reason to believe that the true state of the world lies in $ n $ . further notions of relative plausibility between worlds and beliefs based on the latter ordering are then defined in terms of this evidence structure , yielding our intended models for evidence-based beliefs . in addition , we also consider a second more general flavor , where belief and plausibility are modeled using additional primitive relations , and we prove a representation theorem showing that each such general model is a $ p $ -morphic image of an intended one . this semantics invites a number of natural special cases , depending on how uniform we make the evidence sets , and how coherent their total structure . we give a structural study of the resulting ` uniform ' and ` flat ' models . our main result are sound and complete axiomatizations for the logics of all four major model classes with respect to the modal language of evidence , belief and safe belief . we conclude with an outlook toward logics for the dynamics of changing evidence , and the resulting language extensions and connections with logics of plausibility change .

empath : understanding topic signals in large-scale text
human language is colored by a broad range of topics , but existing text analysis tools only focus on a small number of them . we present empath , a tool that can generate and validate new lexical categories on demand from a small set of seed terms ( like `` bleed '' and `` punch '' to generate the category violence ) . empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction . given a small set of seed words that characterize a category , empath uses its neural embedding to discover new related terms , then validates the category with a crowd-powered filter . empath also analyzes text across 200 built-in , pre-validated categories we have generated from common topics in our web dataset , like neglect , government , and social media . we show that empath 's data-driven , human validated categories are highly correlated ( r=0.906 ) with similar categories in liwc .

low-autocorrelation binary sequences : on improved merit factors and runtime predictions to achieve them
the search for binary sequences with a high figure of merit , known as the low autocorrelation binary sequence ( $ labs $ } ) problem , represents a formidable computational challenge . to mitigate the computational constraints of the problem , we consider solvers that accept odd values of sequence length $ l $ and return solutions for skew-symmetric binary sequences only -- with the consequence that not all best solutions under this constraint will be optimal for each $ l $ . in order to improve both , the search for best merit factor $ and $ the asymptotic runtime performance , we instrumented three stochastic solvers , the first two are state-of-the-art solvers that rely on variants of memetic and tabu search ( $ lssmats $ and $ lssrrts $ ) , the third solver ( $ lssorel $ ) organizes the search as a sequence of independent contiguous self-avoiding walk segments . by adapting a rigorous statistical methodology to performance testing of all three combinatorial solvers , experiments show that the solver with the best asymptotic average-case performance , $ lssorel\_8 = 0.000032*1.1504^l $ , has the best chance of finding solutions that improve , as $ l $ increases , figures of merit reported to date . the same methodology can be applied to engineering new $ labs $ solvers that may return merit factors even closer to the conjectured asymptotic value of 12.3248 .

measuring the evolvability landscape to study neutrality
this theoretical work defines the measure of autocorrelation of evolvability in the context of neutral fitness landscape . this measure has been studied on the classical max-sat problem . this work highlight a new characteristic of neutral fitness landscapes which allows to design new adapted metaheuristic .

firefly algorithm : recent advances and applications
nature-inspired metaheuristic algorithms , especially those based on swarm intelligence , have attracted much attention in the last ten years . firefly algorithm appeared in about five years ago , its literature has expanded dramatically with diverse applications . in this paper , we will briefly review the fundamentals of firefly algorithm together with a selection of recent publications . then , we discuss the optimality associated with balancing exploration and exploitation , which is essential for all metaheuristic algorithms . by comparing with intermittent search strategy , we conclude that metaheuristics such as firefly algorithm are better than the optimal intermittent search strategy . we also analyse algorithms and their implications for higher-dimensional optimization problems .

a discrete state transition algorithm for generalized traveling salesman problem
generalized traveling salesman problem ( gtsp ) is an extension of classical traveling salesman problem ( tsp ) , which is a combinatorial optimization problem and an np-hard problem . in this paper , an efficient discrete state transition algorithm ( dsta ) for gtsp is proposed , where a new local search operator named \textit { k-circle } , directed by neighborhood information in space , has been introduced to dsta to shrink search space and strengthen search ability . a novel robust update mechanism , restore in probability and risk in probability ( double r-probability ) , is used in our work to escape from local minima . the proposed algorithm is tested on a set of gtsp instances . compared with other heuristics , experimental results have demonstrated the effectiveness and strong adaptability of dsta and also show that dsta has better search ability than its competitors .

message passing for quantified boolean formulas
we introduce two types of message passing algorithms for quantified boolean formulas ( qbf ) . the first type is a message passing based heuristics that can prove unsatisfiability of the qbf by assigning the universal variables in such a way that the remaining formula is unsatisfiable . in the second type , we use message passing to guide branching heuristics of a davis-putnam logemann-loveland ( dpll ) complete solver . numerical experiments show that on random qbfs our branching heuristics gives robust exponential efficiency gain with respect to the state-of-art solvers . we also manage to solve some previously unsolved benchmarks from the qbflib library . apart from this our study sheds light on using message passing in small systems and as subroutines in complete solvers .

extended asp tableaux and rule redundancy in normal logic programs
we introduce an extended tableau calculus for answer set programming ( asp ) . the proof system is based on the asp tableaux defined in [ gebser & schaub , iclp 2006 ] , with an added extension rule . we investigate the power of extended asp tableaux both theoretically and empirically . we study the relationship of extended asp tableaux with the extended resolution proof system defined by tseitin for sets of clauses , and separate extended asp tableaux from asp tableaux by giving a polynomial-length proof for a family of normal logic programs p_n for which asp tableaux has exponential-length minimal proofs with respect to n. additionally , extended asp tableaux imply interesting insight into the effect of program simplification on the lengths of proofs in asp . closely related to extended asp tableaux , we empirically investigate the effect of redundant rules on the efficiency of asp solving . to appear in theory and practice of logic programming ( tplp ) .

an algebraic programming style for numerical software and its optimization
the abstract mathematical theory of partial differential equations ( pdes ) is formulated in terms of manifolds , scalar fields , tensors , and the like , but these algebraic structures are hardly recognizable in actual pde solvers . the general aim of the sophus programming style is to bridge the gap between theory and practice in the domain of pde solvers . its main ingredients are a library of abstract datatypes corresponding to the algebraic structures used in the mathematical theory and an algebraic expression style similar to the expression style used in the mathematical theory . because of its emphasis on abstract datatypes , sophus is most naturally combined with object-oriented languages or other languages supporting abstract datatypes . the resulting source code patterns are beyond the scope of current compiler optimizations , but are sufficiently specific for a dedicated source-to-source optimizer . the limited , domain-specific , character of sophus is the key to success here . this kind of optimization has been tested on computationally intensive sophus style code with promising results . the general approach may be useful for other styles and in other application domains as well .

developing bug-free machine learning systems with formal mathematics
noisy data , non-convex objectives , model misspecification , and numerical instability can all cause undesired behaviors in machine learning systems . as a result , detecting actual implementation errors can be extremely difficult . we demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct . the process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail . as a case study , we implement a new system , certigrad , for optimizing over stochastic computation graphs , and we generate a formal ( i.e . machine-checkable ) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients . we train a variational autoencoder using certigrad and find the performance comparable to training the same model in tensorflow .

exact hybrid covariance thresholding for joint graphical lasso
this paper considers the problem of estimating multiple related gaussian graphical models from a $ p $ -dimensional dataset consisting of different classes . our work is based upon the formulation of this problem as group graphical lasso . this paper proposes a novel hybrid covariance thresholding algorithm that can effectively identify zero entries in the precision matrices and split a large joint graphical lasso problem into small subproblems . our hybrid covariance thresholding method is superior to existing uniform thresholding methods in that our method can split the precision matrix of each individual class using different partition schemes and thus split group graphical lasso into much smaller subproblems , each of which can be solved very fast . in addition , this paper establishes necessary and sufficient conditions for our hybrid covariance thresholding algorithm . the superior performance of our thresholding method is thoroughly analyzed and illustrated by a few experiments on simulated data and real gene expression data .

deep recurrent generative decoder for abstractive text summarization
we propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder ( drgn ) . latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality . neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables . abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states . extensive experiments on some benchmark datasets in different languages show that drgn achieves improvements over the state-of-the-art methods .

using summarization to discover argument facets in online ideological dialog
more and more of the information available on the web is dialogic , and a significant portion of it takes place in online forum conversations about current social and political topics . we aim to develop tools to summarize what these conversations are about . what are the central propositions associated with different stances on an issue , what are the abstract objects under discussion that are central to a speaker 's argument ? how can we recognize that two central propositions realize the same facet of the argument ? we hypothesize that the central propositions are exactly those arguments that people find most salient , and use human summarization as a probe for discovering them . we describe our corpus of human summaries of opinionated dialogs , then show how we can identify similar repeated arguments , and group them into facets across many discussions of a topic . we define a new task , argument facet similarity ( afs ) , and show that we can predict afs with a .54 correlation score , versus an ngram system baseline of .39 and a semantic textual similarity system baseline of .45 .

prefix-projection global constraint for sequential pattern mining
sequential pattern mining under constraints is a challenging data mining task . many efficient ad hoc methods have been developed for mining sequential patterns , but they are all suffering from a lack of genericity . recent works have investigated constraint programming ( cp ) methods , but they are not still effective because of their encoding . in this paper , we propose a global constraint based on the projected databases principle which remedies to this drawback . experiments show that our approach clearly outperforms cp approaches and competes well with ad hoc methods on large datasets .

uniform and partially uniform redistribution rules
this short paper introduces two new fusion rules for combining quantitative basic belief assignments . these rules although very simple have not been proposed in literature so far and could serve as useful alternatives because of their low computation cost with respect to the recent advanced proportional conflict redistribution rules developed in the dsmt framework .

a hybrid intelligent model for software cost estimation
accurate software development effort estimation is critical to the success of software projects . although many techniques and algorithmic models have been developed and implemented by practitioners , accurate software development effort prediction is still a challenging endeavor in the field of software engineering , especially in handling uncertain and imprecise inputs and collinear characteristics . in this paper , a hybrid in-telligent model combining a neural network model integrated with fuzzy model ( neuro-fuzzy model ) has been used to improve the accuracy of estimating software cost . the performance of the proposed model is assessed by designing and conducting evaluation with published project and industrial data . results have shown that the proposed model demonstrates the ability of improving the estimation accuracy by 18 % based on the mean magnitude of relative error ( mmre ) criterion .

generalising unit-refutation completeness and slur via nested input resolution
we introduce two hierarchies of clause-sets , slur_k and uc_k , based on the classes slur ( single lookahead unit refutation ) , introduced in 1995 , and uc ( unit refutation complete ) , introduced in 1994. the class slur , introduced in [ annexstein et al , 1995 ] , is the class of clause-sets for which unit-clause-propagation ( denoted by r_1 ) detects unsatisfiability , or where otherwise iterative assignment , avoiding obviously false assignments by look-ahead , always yields a satisfying assignment . it is natural to consider how to form a hierarchy based on slur . such investigations were started in [ cepek et al , 2012 ] and [ balyo et al , 2012 ] . we present what we consider the `` limit hierarchy '' slur_k , based on generalising r_1 by r_k , that is , using generalised unit-clause-propagation introduced in [ kullmann , 1999 , 2004 ] . the class uc , studied in [ del val , 1994 ] , is the class of unit refutation complete clause-sets , that is , those clause-sets for which unsatisfiability is decidable by r_1 under any falsifying assignment . for unsatisfiable clause-sets f , the minimum k such that r_k determines unsatisfiability of f is exactly the `` hardness '' of f , as introduced in [ ku 99 , 04 ] . for satisfiable f we use now an extension mentioned in [ ansotegui et al , 2008 ] : the hardness is the minimum k such that after application of any falsifying partial assignments , r_k determines unsatisfiability . the class uc_k is given by the clause-sets which have hardness < = k. we observe that uc_1 is exactly uc . uc_k has a proof-theoretic character , due to the relations between hardness and tree-resolution , while slur_k has an algorithmic character . the correspondence between r_k and k-times nested input resolution ( or tree resolution using clause-space k+1 ) means that r_k has a dual nature : both algorithmic and proof theoretic . this corresponds to a basic result of this paper , namely slur_k = uc_k .

simultaneous dempster-shafer clustering and gradual determination of number of clusters using a neural network structure
in this paper we extend an earlier result within dempster-shafer theory [ `` fast dempster-shafer clustering using a neural network structure , '' in proc . seventh int . conf . information processing and management of uncertainty in knowledge-based systems ( ipmu'98 ) ] where several pieces of evidence were clustered into a fixed number of clusters using a neural structure . this was done by minimizing a metaconflict function . we now develop a method for simultaneous clustering and determination of number of clusters during iteration in the neural structure . we let the output signals of neurons represent the degree to which a pieces of evidence belong to a corresponding cluster . from these we derive a probability distribution regarding the number of clusters , which gradually during the iteration is transformed into a determination of number of clusters . this gradual determination is fed back into the neural structure at each iteration to influence the clustering process .

computing probability intervals under independency constraints
many ai researchers argue that probability theory is only capable of dealing with uncertainty in situations where a full specification of a joint probability distribution is available , and conclude that it is not suitable for application in knowledge-based systems . probability intervals , however , constitute a means for expressing incompleteness of information . we present a method for computing such probability intervals for probabilities of interest from a partial specification of a joint probability distribution . our method improves on earlier approaches by allowing for independency relationships between statistical variables to be exploited .

combining finite and continuous solvers
combining efficiency with reliability within cp systems is one of the main concerns of cp developers . this paper presents a simple and efficient way to connect choco and ibex , two cp solvers respectively specialised on finite and continuous domains . this enables to take advantage of the most recent advances of the continuous community within choco while saving development and maintenance resources , hence ensuring a better software quality .

first-order modeling and stability analysis of illusory contours
in visual cognition , illusions help elucidate certain intriguing latent perceptual functions of the human vision system , and their proper mathematical modeling and computational simulation are therefore deeply beneficial to both biological and computer vision . inspired by existent prior works , the current paper proposes a first-order energy-based model for analyzing and simulating illusory contours . the lower complexity of the proposed model facilitates rigorous mathematical analysis on the detailed geometric structures of illusory contours . after being asymptotically approximated by classical active contours , the proposed model is then robustly computed using the celebrated level-set method of osher and sethian ( j. comput . phys. , 79:12-49 , 1988 ) with a natural supervising scheme . potential cognitive implications of the mathematical results are addressed , and generic computational examples are demonstrated and discussed .

planning based on classification by induction graph
in artificial intelligence , planning refers to an area of research that proposes to develop systems that can automatically generate a result set , in the form of an integrated decision-making system through a formal procedure , known as plan . instead of resorting to the scheduling algorithms to generate plans , it is proposed to operate the automatic learning by decision tree to optimize time . in this paper , we propose to build a classification model by induction graph from a learning sample containing plans that have an associated set of descriptors whose values change depending on each plan . this model will then operate for classifying new cases by assigning the appropriate plan .

optlayer - practical constrained optimization for deep reinforcement learning in the real world
while deep reinforcement learning techniques have recently produced considerable achievements on many decision-making problems , their use in robotics has largely been limited to simulated worlds or restricted motions , since unconstrained trial-and-error interactions in the real world can have undesirable consequences for the robot or its environment . to overcome such limitations , we propose a novel reinforcement learning architecture , optlayer , that takes as inputs possibly unsafe actions predicted by a neural network and outputs the closest actions that satisfy chosen constraints . while learning control policies often requires carefully crafted rewards and penalties while exploring the range of possible actions , optlayer ensures that only safe actions are actually executed and unsafe predictions are penalized during training . we demonstrate the effectiveness of our approach on robot reaching tasks , both simulated and in the real world .

labeled directed acyclic graphs : a generalization of context-specific independence in directed graphical models
we introduce a novel class of labeled directed acyclic graph ( ldag ) models for finite sets of discrete variables . ldags generalize earlier proposals for allowing local structures in the conditional probability distribution of a node , such that unrestricted label sets determine which edges can be deleted from the underlying directed acyclic graph ( dag ) for a given context . several properties of these models are derived , including a generalization of the concept of markov equivalence classes . efficient bayesian learning of ldags is enabled by introducing an ldag-based factorization of the dirichlet prior for the model parameters , such that the marginal likelihood can be calculated analytically . in addition , we develop a novel prior distribution for the model structures that can appropriately penalize a model for its labeling complexity . a non-reversible markov chain monte carlo algorithm combined with a greedy hill climbing approach is used for illustrating the useful properties of ldag models for both real and synthetic data sets .

hybrid genetic algorithm for cloud computing applications
in this paper with the aid of genetic algorithm and fuzzy theory , we present a hybrid job scheduling approach , which considers the load balancing of the system and reduces total execution time and execution cost . we try to modify the standard genetic algorithm and to reduce the iteration of creating population with the aid of fuzzy theory . the main goal of this research is to assign the jobs to the resources with considering the vm mips and length of jobs . the new algorithm assigns the jobs to the resources with considering the job length and resources capacities . we evaluate the performance of our approach with some famous cloud scheduling models . the results of the experiments show the efficiency of the proposed approach in term of execution time , execution cost and average degree of imbalance ( di ) .

mesa : maximum entropy by simulated annealing
probabilistic reasoning systems combine different probabilistic rules and probabilistic facts to arrive at the desired probability values of consequences . in this paper we describe the mesa-algorithm ( maximum entropy by simulated annealing ) that derives a joint distribution of variables or propositions . it takes into account the reliability of probability values and can resolve conflicts between contradictory statements . the joint distribution is represented in terms of marginal distributions and therefore allows to process large inference networks and to determine desired probability values with high precision . the procedure derives a maximum entropy distribution subject to the given constraints . it can be applied to inference networks of arbitrary topology and may be extended into a number of directions .

an algorithm for finding minimum d-separating sets in belief networks
the criterion commonly used in directed acyclic graphs ( dags ) for testing graphical independence is the well-known d-separation criterion . it allows us to build graphical representations of dependency models ( usually probabilistic dependency models ) in the form of belief networks , which make easy interpretation and management of independence relationships possible , without reference to numerical parameters ( conditional probabilities ) . in this paper , we study the following combinatorial problem : finding the minimum d-separating set for two nodes in a dag . this set would represent the minimum information ( in the sense of minimum number of variables ) necessary to prevent these two nodes from influencing each other . the solution to this basic problem and some of its extensions can be useful in several ways , as we shall see later . our solution is based on a two-step process : first , we reduce the original problem to the simpler one of finding a minimum separating set in an undirected graph , and second , we develop an algorithm for solving it .

towards automation of data quality system for cern cms experiment
daily operation of a large-scale experiment is a challenging task , particularly from perspectives of routine monitoring of quality for data being taken . we describe an approach that uses machine learning for the automated system to monitor data quality , which is based on partial use of data qualified manually by detector experts . the system automatically classifies marginal cases : both of good an bad data , and use human expert decision to classify remaining `` grey area '' cases . this study uses collision data collected by the cms experiment at lhc in 2010. we demonstrate that proposed workflow is able to automatically process at least 20\ % of samples without noticeable degradation of the result .

representing knowledge about norms
norms are essential to extend inference : inferences based on norms are far richer than those based on logical implications . in the recent decades , much effort has been devoted to reason on a domain , once its norms are represented . how to extract and express those norms has received far less attention . extraction is difficult : as the readers are supposed to know them , the norms of a domain are seldom made explicit . for one thing , extracting norms requires a language to represent them , and this is the topic of this paper . we apply this language to represent norms in the domain of driving , and show that it is adequate to reason on the causes of accidents , as described by car-crash reports .

fast meta-learning for adaptive hierarchical classifier design
we propose a new splitting criterion for a meta-learning approach to multiclass classifier design that adaptively merges the classes into a tree-structured hierarchy of increasingly difficult binary classification problems . the classification tree is constructed from empirical estimates of the henze-penrose bounds on the pairwise bayes misclassification rates that rank the binary subproblems in terms of difficulty of classification . the proposed empirical estimates of the bayes error rate are computed from the minimal spanning tree ( mst ) of the samples from each pair of classes . moreover , a meta-learning technique is presented for quantifying the one-vs-rest bayes error rate for each individual class from a single mst on the entire dataset . extensive simulations on benchmark datasets show that the proposed hierarchical method can often be learned much faster than competing methods , while achieving competitive accuracy .

a splitting set theorem for epistemic specifications
over the past decade a considerable amount of research has been done to expand logic programming languages to handle incomplete information . one such language is the language of epistemic specifications . as is usual with logic programming languages , the problem of answering queries is intractable in the general case . for extended disjunctive logic programs , an idea that has proven useful in simplifying the investigation of answer sets is the use of splitting sets . in this paper we will present an extended definition of splitting sets that will be applicable to epistemic specifications . furthermore , an extension of the splitting set theorem will be presented . also , a characterization of stratified epistemic specifications will be given in terms of splitting sets . this characterization leads us to an algorithmic method of computing world views of a subclass of epistemic logic programs .

generative moment matching networks
we consider the problem of learning deep generative models from data . we formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron , as in the recently proposed generative adversarial networks ( goodfellow et al. , 2014 ) . training a generative adversarial network , however , requires careful optimization of a difficult minimax program . instead , we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy ( mmd ) , which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model , and can be trained by backpropagation . we further boost the performance of this approach by combining our generative network with an auto-encoder network , using mmd to learn to generate codes that can then be decoded to produce samples . we show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on mnist and the toronto face database .

control neuronal por modelo inverso de un servosistema usando algoritmos de aprendizaje levenberg-marquardt y bayesiano
in this paper we present the experimental results of the neural network control of a servo-system in order to control its speed . the control strategy is implemented by using an inverse-model control based on artificial neural networks ( anns ) . the network training was performed using two learning algorithms : levenberg-marquardt and bayesian regularization . we evaluate the generalization capability for each method according to both the correct operation of the controller to follow the reference signal , and the control efforts developed by the ann-based controller .

adaptive measurement-based policy-driven qos management with fuzzy-rule-based resource allocation
fixed and wireless networks are increasingly converging towards common connectivity with ip-based core networks . providing effective end-to-end resource and qos management in such complex heterogeneous converged network scenarios requires unified , adaptive and scalable solutions to integrate and co-ordinate diverse qos mechanisms of different access technologies with ip-based qos . policy-based network management ( pbnm ) is one approach that could be employed to address this challenge . hence , a policy-based framework for end-to-end qos management in converged networks , cnqf ( converged networks qos management framework ) has been proposed within our project . in this paper , the cnqf architecture , a java implementation of its prototype and experimental validation of key elements are discussed . we then present a fuzzy-based cnqf resource management approach and study the performance of our implementation with real traffic flows on an experimental testbed . the results demonstrate the efficacy of our resource-adaptive approach for practical pbnm systems .

local expression languages for probabilistic dependence : a preliminary report
we present a generalization of the local expression language used in the symbolic probabilistic inference ( spi ) approach to inference in belief nets [ 1l , [ 8 ] . the local expression language in spi is the language in which the dependence of a node on its antecedents is described . the original language represented the dependence as a single monolithic conditional probability distribution . the extended language provides a set of operators ( * , + , and - ) which can be used to specify methods for combining partial conditional distributions . as one instance of the utility of this extension , we show how this extended language can be used to capture the semantics , representational advantages , and inferential complexity advantages of the `` noisy or '' relationship .

rdf2rules : learning rules from rdf knowledge bases by mining frequent predicate cycles
recently , several large-scale rdf knowledge bases have been built and applied in many knowledge-based applications . to further increase the number of facts in rdf knowledge bases , logic rules can be used to predict new facts based on the existing ones . therefore , how to automatically learn reliable rules from large-scale knowledge bases becomes increasingly important . in this paper , we propose a novel rule learning approach named rdf2rules for rdf knowledge bases . rdf2rules first mines frequent predicate cycles ( fpcs ) , a kind of interesting frequent patterns in knowledge bases , and then generates rules from the mined fpcs . because each fpc can produce multiple rules , and effective pruning strategy is used in the process of mining fpcs , rdf2rules works very efficiently . another advantage of rdf2rules is that it uses the entity type information when generates and evaluates rules , which makes the learned rules more accurate . experiments show that our approach outperforms the compared approach in terms of both efficiency and accuracy .

behavioural correlation for detecting p2p bots
in the past few years , irc bots , malicious programs which are remotely controlled by the attacker through irc servers , have become a major threat to the internet and users . these bots can be used in different malicious ways such as issuing distributed denial of services attacks to shutdown other networks and services , keystrokes logging , spamming , traffic sniffing cause serious disruption on networks and users . new bots use peer to peer ( p2p ) protocols start to appear as the upcoming threat to internet security due to the fact that p2p bots do not have a centralized point to shutdown or traceback , thus making the detection of p2p bots is a real challenge . in response to these threats , we present an algorithm to detect an individual p2p bot running on a system by correlating its activities . our evaluation shows that correlating different activities generated by p2p bots within a specified time period can detect these kind of bots .

submodular meets structured : finding diverse subsets in exponentially-large structured item sets
to cope with the high level of ambiguity faced in domains such as computer vision or natural language processing , robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals . in structured prediction problems , this becomes a daunting task , as the solution space ( image labelings , sentence parses , etc . ) is exponentially large . we study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and high-order potentials ( hops ) studied for graphical models . specifically , we show via examples that when marginal gains of submodular diversity functions allow structured representations , this enables efficient ( sub-linear time ) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed hops . we discuss benefits , tradeoffs , and show that our constructions lead to significantly better proposals .

cost and actual causation
i propose the purpose our concept of actual causation serves is minimizing various cost in intervention practice . actual causation has three features : nonredundant sufficiency , continuity and abnormality ; these features correspond to the minimization of exploitative cost , exploratory cost and risk cost in intervention practice . incorporating these three features , a definition of actual causation is given . i test the definition in 66 causal cases from actual causation literature and show that this definition 's application fit intuition better than some other causal modelling based definitions .

estimating the value of computation in flexible information refinement
we outline a method to estimate the value of computation for a flexible algorithm using empirical data . to determine a reasonable trade-off between cost and value , we build an empirical model of the value obtained through computation , and apply this model to estimate the value of computation for quite different problems . in particular , we investigate this trade-off for the problem of constructing policies for decision problems represented as influence diagrams . we show how two features of our anytime algorithm provide reasonable estimates of the value of computation in this domain .

high level path planning with uncertainty
for high level path planning , environments are usually modeled as distance graphs , and path planning problems are reduced to computing the shortest path in distance graphs . one major drawback of this modeling is the inability to model uncertainties , which are often encountered in practice . in this paper , a new tool , called u-yraph , is proposed for environment modeling . a u-graph is an extension of distance graphs with the ability to handle a kind of uncertainty . by modeling an uncertain environment as a u-graph , and a navigation problem as a markovian decision process , we can precisely define a new optimality criterion for navigation plans , and more importantly , we can come up with a general algorithm for computing optimal plans for navigation tasks .

a hybrid approach to query answering under expressive datalog+/-
datalog+/- is a family of ontology languages that combine good computational properties with high expressive power . datalog+/- languages are provably able to capture the most relevant semantic web languages . in this paper we consider the class of weakly-sticky ( ws ) datalog+/- programs , which allow for certain useful forms of joins in rule bodies as well as extending the well-known class of weakly-acyclic tgds . so far , only non-deterministic algorithms were known for answering queries on ws datalog+/- programs . we present novel deterministic query answering algorithms under ws datalog+/- . in particular , we propose : ( 1 ) a bottom-up grounding algorithm based on a query-driven chase , and ( 2 ) a hybrid approach based on transforming a ws program into a so-called sticky one , for which query rewriting techniques are known . we discuss how our algorithms can be optimized and effectively applied for query answering in real-world scenarios .

an asp-based architecture for autonomous uavs in dynamic environments : progress report
traditional ai reasoning techniques have been used successfully in many domains , including logistics , scheduling and game playing . this paper is part of a project aimed at investigating how such techniques can be extended to coordinate teams of unmanned aerial vehicles ( uavs ) in dynamic environments . specifically challenging are real-world environments where uavs and other network-enabled devices must communicate to coordinate -- -and communication actions are neither reliable nor free . such network-centric environments are common in military , public safety and commercial applications , yet most research ( even multi-agent planning ) usually takes communications among distributed agents as a given . we address this challenge by developing an agent architecture and reasoning algorithms based on answer set programming ( asp ) . asp has been chosen for this task because it enables high flexibility of representation , both of knowledge and of reasoning tasks . although asp has been used successfully in a number of applications , and asp-based architectures have been studied for about a decade , to the best of our knowledge this is the first practical application of a complete asp-based agent architecture . it is also the first practical application of asp involving a combination of centralized reasoning , decentralized reasoning , execution monitoring , and reasoning about network communications . this work has been empirically validated using a distributed network-centric software evaluation testbed and the results provide guidance to designers in how to understand and control intelligent systems that operate in these environments .

refining adverse drug reaction signals by incorporating interaction variables identified using emergent pattern mining
purpose : to develop a framework for identifying and incorporating candidate confounding interaction terms into a regularised cox regression analysis to refine adverse drug reaction signals obtained via longitudinal observational data . methods : we considered six drug families that are commonly associated with myocardial infarction in observational healthcare data , but where the causal relationship ground truth is known ( adverse drug reaction or not ) . we applied emergent pattern mining to find itemsets of drugs and medical events that are associated with the development of myocardial infarction . these are the candidate confounding interaction terms . we then implemented a cohort study design using regularised cox regression that incorporated and accounted for the candidate confounding interaction terms . results the methodology was able to account for signals generated due to confounding and a cox regression with elastic net regularisation correctly ranked the drug families known to be true adverse drug reactions above those .

knowledge representation
this work analyses main features that should be present in knowledge representation . it suggests a model for representation and a way to implement this model in software . representation takes care of both low-level sensor information and high-level concepts .

exploration of the dendritic cell algorithm using the duration calculus
as one of the newest members in artificial immune systems ( ais ) , the dendritic cell algorithm ( dca ) has been applied to a range of problems . these applications mainly belong to the field of anomaly detection . however , real-time detection , a new challenge to anomaly detection , requires improvement on the real-time capability of the dca . to assess such capability , formal methods in the research of rea-time systems can be employed . the findings of the assessment can provide guideline for the future development of the algorithm . therefore , in this paper we use an interval logic based method , named the duration calculus ( dc ) , to specify a simplified single-cell model of the dca . based on the dc specifications with further induction , we find that each individual cell in the dca can perform its function as a detector in real-time . since the dca can be seen as many such cells operating in parallel , it is potentially capable of performing real-time detection . however , the analysis process of the standard dca constricts its real-time capability . as a result , we conclude that the analysis process of the standard dca should be replaced by a real-time analysis component , which can perform periodic analysis for the purpose of real-time detection .

ooasp : connecting object-oriented and logic programming
most of contemporary software systems are implemented using an object-oriented approach . modeling phases -- during which software engineers analyze requirements to the future system using some modeling language -- are an important part of the development process , since modeling errors are often hard to recognize and correct . in this paper we present a framework which allows the integration of answer set programming into the object-oriented software development process . ooasp supports reasoning about object-oriented software models and their instantiations . preliminary results of the ooasp application in csl studio , which is a siemens internal modeling environment for product configurators , show that it can be used as a lightweight approach to verify , create and transform instantiations of object models at runtime and to support the software development process during design and testing .

medtq : dynamic topic discovery and query generation for medical ontologies
biomedical ontology refers to a shared conceptualization for a biomedical domain of interest that has vastly improved data management and data sharing through the open data movement . the rapid growth and availability of biomedical data make it impractical and computationally expensive to perform manual analysis and query processing with the large scale ontologies . the lack of ability in analyzing ontologies from such a variety of sources , and supporting knowledge discovery for clinical practice and biomedical research should be overcome with new technologies . in this study , we developed a medical topic discovery and query generation framework ( medtq ) , which was composed by a series of approaches and algorithms . a predicate neighborhood pattern-based approach introduced has the ability to compute the similarity of predicates ( relations ) in ontologies . given a predicate similarity metric , machine learning algorithms have been developed for automatic topic discovery and query generation . the topic discovery algorithm , called the hierarchical k-means algorithm was designed by extending an existing supervised algorithm ( k-means clustering ) for the construction of a topic hierarchy . in the hierarchical k-means algorithm , a level-by-level optimization strategy was selected for consistent with the strongly association between elements within a topic . automatic query generation was facilitated for discovered topic that could be guided users for interactive query design and processing . evaluation was conducted to generate topic hierarchy for drugbank ontology as a case study . results demonstrated that the medtq framework can enhance knowledge discovery by capturing underlying structures from domain specific data and ontologies .

worst-case upper bound for ( 1 , 2 ) -qsat
the rigorous theoretical analysis of the algorithm for a subclass of qsat , i.e . ( 1 , 2 ) -qsat , has been proposed in the literature . ( 1 , 2 ) -qsat , first introduced in sat'08 , can be seen as quantified extended 2-cnf formulas . until now , within our knowledge , there exists no algorithm presenting the worst upper bound for ( 1 , 2 ) -qsat . therefore in this paper , we present an exact algorithm to solve ( 1 , 2 ) -qsat . by analyzing the algorithms , we obtain a worst-case upper bound o ( 1.4142m ) , where m is the number of clauses .

comparing neural and attractiveness-based visual features for artwork recommendation
advances in image processing and computer vision in the latest years have brought about the use of visual features in artwork recommendation . recent works have shown that visual features obtained from pre-trained deep neural networks ( dnns ) perform very well for recommending digital art . other recent works have shown that explicit visual features ( evf ) based on attractiveness can perform well in preference prediction tasks , but no previous work has compared dnn features versus specific attractiveness-based visual features ( e.g . brightness , texture ) in terms of recommendation performance . in this work , we study and compare the performance of dnn and evf features for the purpose of physical artwork recommendation using transactional data from ugallery , an online store of physical paintings . in addition , we perform an exploratory analysis to understand if dnn embedded features have some relation with certain evf . our results show that dnn features outperform evf , that certain evf features are more suited for physical artwork recommendation and , finally , we show evidence that certain neurons in the dnn might be partially encoding visual features such as brightness , providing an opportunity for explaining recommendations based on visual neural models .

maximizing a nonnegative , monotone , submodular function constrained to matchings
submodular functions have many applications . matchings have many applications . the bitext word alignment problem can be modeled as the problem of maximizing a nonnegative , monotone , submodular function constrained to matchings in a complete bipartite graph where each vertex corresponds to a word in the two input sentences and each edge represents a potential word-to-word translation . we propose a more general problem of maximizing a nonnegative , monotone , submodular function defined on the edge set of a complete graph constrained to matchings ; we call this problem the csm-matching problem . csm-matching also generalizes the maximum-weight matching problem , which has a polynomial-time algorithm ; however , we show that it is np-hard to approximate csm-matching within a factor of e/ ( e-1 ) by reducing the max k-cover problem to it . our main result is a simple , greedy , 3-approximation algorithm for csm-matching . then we reduce csm-matching to maximizing a nonnegative , monotone , submodular function over two matroids , i.e. , csm-2-matroids . csm-2-matroids has a ( 2+epsilon ) -approximation algorithm - called lsv2 . we show that we can find a ( 4+epsilon ) -approximate solution to csm-matching using lsv2 . we extend this approach to similar problems .

dualing gans
generative adversarial nets ( gans ) are a promising technique for modeling a distribution from samples . it is however well known that gan training suffers from instability due to the nature of its maximin formulation . in this paper , we explore ways to tackle the instability problem by dualizing the discriminator . we start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem , such that both the generator and the discriminator of this 'dualing gan ' act in concert . we then demonstrate how to extend this intuition to non-linear formulations . for gans with linear discriminators our approach is able to remove the instability in training , while for gans with nonlinear discriminators our approach provides an alternative to the commonly used gan training algorithm .

memory augmented control networks
planning problems in partially observable environments can not be solved directly with convolutional networks and require some form of memory . but , even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan . to mitigate these challenges we introduce the memory augmented control network ( macn ) . the proposed network architecture consists of three main parts . the first part uses convolutions to extract features and the second part uses a neural network-based planning module to pre-plan in the environment . the third part uses a network controller that learns to store those specific instances of past information that are necessary for planning . the performance of the network is evaluated in discrete grid world environments for path planning in the presence of simple and complex obstacles . we show that our network learns to plan and can generalize to new environments .

decision-making under ordinal preferences and comparative uncertainty
this paper investigates the problem of finding a preference relation on a set of acts from the knowledge of an ordering on events ( subsets of states of the world ) describing the decision-maker ( dm ) s uncertainty and an ordering of consequences of acts , describing the dms preferences . however , contrary to classical approaches to decision theory , we try to do it without resorting to any numerical representation of utility nor uncertainty , and without even using any qualitative scale on which both uncertainty and preference could be mapped . it is shown that although many axioms of savage theory can be preserved and despite the intuitive appeal of the method for constructing a preference over acts , the approach is inconsistent with a probabilistic representation of uncertainty , but leads to the kind of uncertainty theory encountered in non-monotonic reasoning ( especially preferential and rational inference ) , closely related to possibility theory . moreover the method turns out to be either very little decisive or to lead to very risky decisions , although its basic principles look sound . this paper raises the question of the very possibility of purely symbolic approaches to savage-like decision-making under uncertainty and obtains preliminary negative results .

towards improving validation , verification , crash investigations , and event reconstruction of flight-critical systems with self-forensics
this paper introduces a novel concept of self-forensics to complement the standard autonomic self-chop properties of the self-managed systems , to be specified in the forensic lucid language . we argue that self-forensics , with the forensics taken out of the cybercrime domain , is applicable to `` self-dissection '' for the purpose of verification of autonomous software and hardware systems of flight-critical systems for automated incident and anomaly analysis and event reconstruction by the engineering teams in a variety of incident scenarios during design and testing as well as actual flight data .

go for a walk and arrive at the answer : reasoning over paths in knowledge bases using reinforcement learning
knowledge bases ( kb ) , both automatically and manually constructed , are often incomplete -- - many valid facts can be inferred from the kb by synthesizing existing information . a popular approach to kb completion is to infer new relations by combinatory reasoning over the information found along other paths connecting a pair of entities . given the enormous size of kbs and the exponential number of paths , previous path-based models have considered only the problem of predicting a missing relation given two entities or evaluating the truth of a proposed triple . additionally , these methods have traditionally used random paths between fixed entity pairs or more recently learned to pick paths between them . we propose a new algorithm minerva , which addresses the much more difficult and practical task of answering questions where the relation is known , but only one entity . since random walks are impractical in a setting with combinatorially many destinations from a start node , we present a neural reinforcement learning approach which learns how to navigate the graph conditioned on the input query to find predictive paths . empirically , this approach obtains state-of-the-art results on several datasets , significantly outperforming prior methods .

exact structure discovery in bayesian networks with less space
the fastest known exact algorithms for scorebased structure discovery in bayesian networks on n nodes run in time and space 2nno ( 1 ) . the usage of these algorithms is limited to networks on at most around 25 nodes mainly due to the space requirement . here , we study space-time tradeoffs for finding an optimal network structure . when little space is available , we apply the gurevich-shelah recurrence-originally proposed for the hamiltonian path problem-and obtain time 22n-sno ( 1 ) in space 2sno ( 1 ) for any s = n/2 , n/4 , n/8 , . . . ; we assume the indegree of each node is bounded by a constant . for the more practical setting with moderate amounts of space , we present a novel scheme . it yields running time 2n ( 3/2 ) pno ( 1 ) in space 2n ( 3/4 ) pno ( 1 ) for any p = 0 , 1 , . . . , n/2 ; these bounds hold as long as the indegrees are at most 0.238n . furthermore , the latter scheme allows easy and efficient parallelization beyond previous algorithms . we also explore empirically the potential of the presented techniques .

advances in probabilistic reasoning
this paper discuses multiple bayesian networks representation paradigms for encoding asymmetric independence assertions . we offer three contributions : ( 1 ) an inference mechanism that makes explicit use of asymmetric independence to speed up computations , ( 2 ) a simplified definition of similarity networks and extensions of their theory , and ( 3 ) a generalized representation scheme that encodes more types of asymmetric independence assertions than do similarity networks .

blind , greedy , and random : ordinal approximation algorithms for matching and clustering
we study matching and other related problems in a partial information setting where the agents ' utilities for being matched to other agents are hidden and the mechanism only has access to ordinal preference information . our model is motivated by the fact that in many settings , agents can not express the numerical values of their utility for different outcomes , but are still able to rank the outcomes in their order of preference . specifically , we study problems where the ground truth exists in the form of a weighted graph , and look to design algorithms that approximate the true optimum matching using only the preference orderings for each agent ( induced by the hidden weights ) as input . if no restrictions are placed on the weights , then one can not hope to do better than the simple greedy algorithm , which yields a half optimal matching . perhaps surprisingly , we show that by imposing a little structure on the weights , we can improve upon the trivial algorithm significantly : we design a 1.6-approximation algorithm for instances where the hidden weights obey the metric inequality . using our algorithms for matching as a black-box , we also design new approximation algorithms for other closely related problems : these include a a 3.2-approximation for the problem of clustering agents into equal sized partitions , a 4-approximation algorithm for densest k-subgraph , and a 2.14-approximation algorithm for max tsp . these results are the first non-trivial ordinal approximation algorithms for such problems , and indicate that we can design robust algorithms even when we are agnostic to the precise agent utilities .

improving the performance of neural networks in regression tasks using drawering
the method presented extends a given regression neural network to make its performance improve . the modification affects the learning procedure only , hence the extension may be easily omitted during evaluation without any change in prediction . it means that the modified model may be evaluated as quickly as the original one but tends to perform better . this improvement is possible because the modification gives better expressive power , provides better behaved gradients and works as a regularization . the knowledge gained by the temporarily extended neural network is contained in the parameters shared with the original neural network . the only cost is an increase in learning time .

collaborative creativity with monte-carlo tree search and convolutional neural networks
we investigate a human-machine collaborative drawing environment in which an autonomous agent sketches images while optionally allowing a user to directly influence the agent 's trajectory . we combine monte carlo tree search with image classifiers and test both shallow models ( e.g . multinomial logistic regression ) and deep convolutional neural networks ( e.g . lenet , inception v3 ) . we found that using the shallow model , the agent produces a limited variety of images , which are noticably recogonisable by humans . however , using the deeper models , the agent produces a more diverse range of images , and while the agent remains very confident ( 99.99 % ) in having achieved its objective , to humans they mostly resemble unrecognisable 'random ' noise . we relate this to recent research which also discovered that 'deep neural networks are easily fooled ' \cite { nguyen2015 } and we discuss possible solutions and future directions for the research .

a model of the mechanisms underlying exploratory behaviour
a model of the mechanisms underlying exploratory behaviour , based on empirical research and refined using a computer simulation , is presented . the behaviour of killifish from two lakes , one with killifish predators and one without , was compared in the laboratory . plotting average activity in a novel environment versus time resulted in an inverted-u-shaped curve for both groups ; however , the curve for killifish from the lake without predators was ( 1 ) steeper , ( 2 ) reached a peak value earlier , ( s ) reached a higher peak value , and ( 4 ) subsumed less area than the curve for killifish from the lake with predators . we hypothesize that the shape of the exploration curve reflects a competition between motivational subsystems that excite and inhibit exploratory behaviour in a way that is tuned to match the affordance probabilities of the animal 's environment . a computer implementation of this model produced curves which differed along the same four dimensions as differentiate the two killifish curves . all four differences were reproduced in the model by tuning a single parameter : the time-dependent component of the decay-rate of the exploration-inhibiting subsystem .

near-optimal active learning of halfspaces via query synthesis in the noisy setting
in this paper , we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries . this problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known . in such applications , queries can be constructed de novo to elicit information ( e.g. , automated science ) or to evade detection with minimal cost ( e.g. , adversarial reverse engineering ) . we develop a general framework , called dimension coupling ( dc ) , that 1 ) reduces a d-dimensional learning problem to d-1 low dimensional sub-problems , 2 ) solves each sub-problem efficiently , 3 ) appropriately aggregates the results and outputs a linear classifier , and 4 ) provides a theoretical guarantee for all possible schemes of aggregation . the proposed method is proved resilient to noise . we show that the dc framework avoids the curse of dimensionality : its computational complexity scales linearly with the dimension . moreover , we show that the query complexity of dc is near optimal ( within a constant factor of the optimum algorithm ) . to further support our theoretical analysis , we compare the performance of dc with the existing work . we observe that dc consistently outperforms the prior arts in terms of query complexity while often running orders of magnitude faster .

multi-agent reinforcement learning in sequential social dilemmas
matrix games like prisoner 's dilemma have guided research on social dilemmas for decades . however , they necessarily treat the choice to cooperate or defect as an atomic action . in real-world social dilemmas these choices are temporally extended . cooperativeness is a property that applies to policies , not elementary actions . we introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions . we analyze the dynamics of policies learned by multiple self-interested independent learning agents , each using its own deep q-network , on two markov games we introduce here : 1. a fruit gathering game and 2. a wolfpack hunting game . we characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance . our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation .

knowledge propagation in contextualized knowledge repositories : an experimental evaluation
as the interest in the representation of context dependent knowledge in the semantic web has been recognized , a number of logic based solutions have been proposed in this regard . in our recent works , in response to this need , we presented the description logic-based contextualized knowledge repository ( ckr ) framework . ckr is not only a theoretical framework , but it has been effectively implemented over state-of-the-art tools for the management of semantic web data : inference inside and across contexts has been realized in the form of forward sparql-based rules over different rdf named graphs . in this paper we present the first evaluation results for such ckr implementation . in particular , in first experiment we study its scalability with respect to different reasoning regimes . in a second experiment we analyze the effects of knowledge propagation on the computation of inferences .

a new approach to updating beliefs
we define a new notion of conditional belief , which plays the same role for dempster-shafer belief functions as conditional probability does for probability functions . our definition is different from the standard definition given by dempster , and avoids many of the well-known problems of that definition . just as the conditional probability pr ( lb ) is a probability function which is the result of conditioning on b being true , so too our conditional belief function bel ( lb ) is a belief function which is the result of conditioning on b being true . we define the conditional belief as the lower envelope ( that is , the inf ) of a family of conditional probability functions , and provide a closed form expression for it . an alternate way of understanding our definition of conditional belief is provided by considering ideas from an earlier paper [ fagin and halpern , 1989 ] , where we connect belief functions with inner measures . in particular , we show here how to extend the definition of conditional probability to non measurable sets , in order to get notions of inner and outer conditional probabilities , which can be viewed as best approximations to the true conditional probability , given our lack of information . our definition of conditional belief turns out to be an exact analogue of our definition of inner conditional probability .

inverse graphics with probabilistic cad models
recently , multiple formulations of vision problems as probabilistic inversions of generative models based on computer graphics have been proposed . however , applications to 3d perception from natural images have focused on low-dimensional latent scenes , due to challenges in both modeling and inference . accounting for the enormous variability in 3d object shape and 2d appearance via realistic generative models seems intractable , as does inverting even simple versions of the many-to-many computations that link 3d scenes to 2d images . this paper proposes and evaluates an approach that addresses key aspects of both these challenges . we show that it is possible to solve challenging , real-world 3d vision problems by approximate inference in generative models for images based on rendering the outputs of probabilistic cad ( pcad ) programs . our pcad object geometry priors generate deformable 3d meshes corresponding to plausible objects and apply affine transformations to place them in a scene . image likelihoods are based on similarity in a feature space based on standard mid-level image representations from the vision literature . our inference algorithm integrates single-site and locally blocked metropolis-hastings proposals , hamiltonian monte carlo and discriminative data-driven proposals learned from training data generated from our models . we apply this approach to 3d human pose estimation and object shape reconstruction from single images , achieving quantitative and qualitative performance improvements over state-of-the-art baselines .

popular ensemble methods : an empirical study
an ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances . previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble . bagging ( breiman , 1996c ) and boosting ( freund and shapire , 1996 ; shapire , 1990 ) are two relatively new but popular methods for producing ensembles . in this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm . our results clearly indicate a number of conclusions . first , while bagging is almost always more accurate than a single classifier , it is sometimes much less accurate than boosting . on the other hand , boosting can create ensembles that are less accurate than a single classifier -- especially when using neural networks . analysis indicates that the performance of the boosting methods is dependent on the characteristics of the data set being examined . in fact , further results show that boosting ensembles may overfit noisy data sets , thus decreasing its performance . finally , consistent with previous studies , our work suggests that most of the gain in an ensemble 's performance comes in the first few classifiers combined ; however , relatively large gains can be seen up to 25 classifiers when boosting decision trees .

the ariadne 's clew algorithm
we present a new approach to path planning , called the `` ariadne 's clew algorithm '' . it is designed to find paths in high-dimensional continuous spaces and applies to robots with many degrees of freedom in static , as well as dynamic environments - ones where obstacles may move . the ariadne 's clew algorithm comprises two sub-algorithms , called search and explore , applied in an interleaved manner . explore builds a representation of the accessible space while search looks for the target . both are posed as optimization problems . we describe a real implementation of the algorithm to plan paths for a six degrees of freedom arm in a dynamic environment where another six degrees of freedom arm is used as a moving obstacle . experimental results show that a path is found in about one second without any pre-processing .

deep learning interior tomography for region-of-interest reconstruction
interior tomography for the region-of-interest ( roi ) imaging has advantages of using a small detector and reducing x-ray radiation dose . however , standard analytic reconstruction suffers from severe cupping artifacts due to existence of null space in the truncated radon transform . existing penalized reconstruction methods may address this problem but they require extensive computations due to the iterative reconstruction . inspired by the recent deep learning approaches to low-dose and sparse view ct , here we propose a deep learning architecture that removes null space signals from the fbp reconstruction . experimental results have shown that the proposed method provides near-perfect reconstruction with about 7-10 db improvement in psnr over existing methods in spite of significantly reduced run-time complexity .

high-resolution multispectral dataset for semantic segmentation
unmanned aircraft have decreased the cost required to collect remote sensing imagery , which has enabled researchers to collect high-spatial resolution data from multiple sensor modalities more frequently and easily . the increase in data will push the need for semantic segmentation frameworks that are able to classify non-rgb imagery , but this type of algorithmic development requires an increase in publicly available benchmark datasets with class labels . in this paper , we introduce a high-resolution multispectral dataset with image labels . this new benchmark dataset has been pre-split into training/testing folds in order to standardize evaluation and continue to push state-of-the-art classification frameworks for non-rgb imagery .

a logic of graded possibility and certainty coping with partial inconsistency
a semantics is given to possibilistic logic , a logic that handles weighted classical logic formulae , and where weights are interpreted as lower bounds on degrees of certainty or possibility , in the sense of zadeh 's possibility theory . the proposed semantics is based on fuzzy sets of interpretations . it is tolerant to partial inconsistency . satisfiability is extended from interpretations to fuzzy sets of interpretations , each fuzzy set representing a possibility distribution describing what is known about the state of the world . a possibilistic knowledge base is then viewed as a set of possibility distributions that satisfy it . the refutation method of automated deduction in possibilistic logic , based on previously introduced generalized resolution principle is proved to be sound and complete with respect to the proposed semantics , including the case of partial inconsistency .

on a model for integrated information
in this paper we give a thorough presentation of a model proposed by tononi et al . for modeling \emph { integrated information } , i.e . how much information is generated in a system transitioning from one state to the next one by the causal interaction of its parts and \emph { above and beyond } the information given by the sum of its parts . we also provides a more general formulation of such a model , independent from the time chosen for the analysis and from the uniformity of the probability distribution at the initial time instant . finally , we prove that integrated information is null for disconnected systems .

variational gaussian process dynamical systems
high dimensional time series are endemic in applications of machine learning such as robotics ( sensor data ) , computational biology ( gene expression data ) , vision ( video sequences ) and graphics ( motion capture data ) . practical nonlinear probabilistic approaches to this data are required . in this paper we introduce the variational gaussian process dynamical system . our work builds on recent variational approximations for gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space . the approach also allows for the appropriate dimensionality of the latent space to be automatically determined . we demonstrate the model on a human motion capture data set and a series of high resolution video sequences .

averaged-dqn : variance reduction and stabilization for deep reinforcement learning
instability and variability of deep reinforcement learning ( drl ) algorithms tend to adversely affect their performance . averaged-dqn is a simple extension to the dqn algorithm , based on averaging previously learned q-values estimates , which leads to a more stable training procedure and improved performance by reducing approximation error variance in the target values . to understand the effect of the algorithm , we examine the source of value function estimation errors and provide an analytical comparison within a simplified model . we further present experiments on the arcade learning environment benchmark that demonstrate significantly improved stability and performance due to the proposed extension .

tighter linear program relaxations for high order graphical models
graphical models with high order potentials ( hops ) have received considerable interest in recent years . while there are a variety of approaches to inference in these models , nearly all of them amount to solving a linear program ( lp ) relaxation with unary consistency constraints between the hop and the individual variables . in many cases , the resulting relaxations are loose , and in these cases the results of inference can be poor . it is thus desirable to look for more accurate ways of performing inference in these models . in this work , we study the lp relaxations that result from enforcing additional consistency constraints between the hop and the rest of the model . we address theoretical questions about the strength of the resulting relaxations compared to the relaxations that arise in standard approaches , and we develop practical and efficient message passing algorithms for optimizing the lps . empirically , we show that the lps with additional consistency constraints lead to more accurate inference on some challenging problems that include a combination of low order and high order terms .

properties of bethe free energies and message passing in gaussian models
we address the problem of computing approximate marginals in gaussian probabilistic models by using mean field and fractional bethe approximations . we define the gaussian fractional bethe free energy in terms of the moment parameters of the approximate marginals , derive a lower and an upper bound on the fractional bethe free energy and establish a necessary condition for the lower bound to be bounded from below . it turns out that the condition is identical to the pairwise normalizability condition , which is known to be a sufficient condition for the convergence of the message passing algorithm . we show that stable fixed points of the gaussian message passing algorithm are local minima of the gaussian bethe free energy . by a counterexample , we disprove the conjecture stating that the unboundedness of the free energy implies the divergence of the message passing algorithm .

a new approach for revising logic programs
belief revision has been studied mainly with respect to background logics that are monotonic in character . in this paper we study belief revision when the underlying logic is non-monotonic instead -- an inherently interesting problem that is under explored . in particular , we will focus on the revision of a body of beliefs that is represented as a logic program under the answer set semantics , while the new information is also similarly represented as a logic program . our approach is driven by the observation that unlike in a monotonic setting where , when necessary , consistency in a revised body of beliefs is maintained by jettisoning some old beliefs , in a non-monotonic setting consistency can be restored by adding new beliefs as well . we will define a syntactic revision function and subsequently provide representation theorem for characterising it .

landmark guided probabilistic roadmap queries
a landmark based heuristic is investigated for reducing query phase run-time of the probabilistic roadmap ( \prm ) motion planning method . the heuristic is generated by storing minimum spanning trees from a small number of vertices within the \prm graph and using these trees to approximate the cost of a shortest path between any two vertices of the graph . the intermediate step of preprocessing the graph increases the time and memory requirements of the classical motion planning technique in exchange for speeding up individual queries making the method advantageous in multi-query applications . this paper investigates these trade-offs on \prm graphs constructed in randomized environments as well as a practical manipulator simulation.we conclude that the method is preferable to dijkstra 's algorithm or the $ { \rm a } ^* $ algorithm with conventional heuristics in multi-query applications .

improvements to inference compilation for probabilistic programming in large-scale scientific simulators
we consider the problem of bayesian inference in the family of probabilistic models implicitly defined by stochastic generative models of data . in scientific fields ranging from population biology to cosmology , low-level mechanistic components are composed to create complex generative models . these models lead to intractable likelihoods and are typically non-differentiable , which poses challenges for traditional approaches to inference . we extend previous work in `` inference compilation '' , which combines universal probabilistic programming and deep learning methods , to large-scale scientific simulators , and introduce a c++ based probabilistic programming library called cpprob . we successfully use cpprob to interface with sherpa , a large code-base used in particle physics . here we describe the technical innovations realized and planned for this library .

a ros multi-ontology references services : owl reasoners and application prototyping issues
the challenge of sharing and communicating information is crucial in complex human-robot interaction ( hri ) scenarios . ontologies and symbolic reasoning are the state-of-the-art approaches for a natural representation of knowledge , especially within the semantic web domain . in such a context , scripted paradigms have been adopted to achieve high expressiveness . nevertheless , since symbolic reasoning is a high complexity problem , optimizing its performance requires a careful design of the knowledge . specifically , a robot architecture requires the integration of several components implementing different behaviors and generating a series of beliefs . most of the components are expected to access , manipulate , and reason upon a run-time generated semantic representation of knowledge grounding robot behaviors and perceptions through formal axioms , with soft real-time requirements .

a study of foss'2013 survey data using clustering techniques
foss is an acronym for free and open source software . the foss 2013 survey primarily targets foss contributors and relevant anonymized dataset is publicly available under cc by sa license . in this study , the dataset is analyzed from a critical perspective using statistical and clustering techniques ( especially multiple correspondence analysis ) with a strong focus on women contributors towards discovering hidden trends and facts . important inferences are drawn about development practices and other facets of the free software and oss worlds .

fast hands-free writing by gaze direction
we describe a method for text entry based on inverse arithmetic coding that relies on gaze direction and which is faster and more accurate than using an on-screen keyboard . these benefits are derived from two innovations : the writing task is matched to the capabilities of the eye , and a language model is used to make predictable words and phrases easier to write .

network topology and time criticality effects in the modularised fleet mix problem
in this paper , we explore the interplay between network topology and time criticality in a military logistics system . a general goal of this work ( and previous work ) is to evaluate land transportation requirements or , more specifically , how to design appropriate fleets of military general service vehicles that are tasked with the supply and re-supply of military units dispersed in an area of operation . the particular focus of this paper is to gain a better understanding of how the logistics environment changes when current army vehicles with fixed transport characteristics are replaced by a new generation of modularised vehicles that can be configured task-specifically . the experimental work is conducted within a well developed strategic planning simulation environment which includes a scenario generation engine for automatically sampling supply and re-supply missions and a multi-objective meta-heuristic search algorithm ( i.e . evolutionary algorithm ) for solving the particular scheduling and routing problems . the results presented in this paper allow for a better understanding of how ( and under what conditions ) a modularised vehicle fleet can provide advantages over the currently implemented system .

a hybrid aco approach to the matrix bandwidth minimization problem
the evolution of the human society raises more and more difficult endeavors . for some of the real-life problems , the computing time-restriction enhances their complexity . the matrix bandwidth minimization problem ( mbmp ) seeks for a simultaneous permutation of the rows and the columns of a square matrix in order to keep its nonzero entries close to the main diagonal . the mbmp is a highly investigated p-complete problem , as it has broad applications in industry , logistics , artificial intelligence or information recovery . this paper describes a new attempt to use the ant colony optimization framework in tackling mbmp . the introduced model is based on the hybridization of the ant colony system technique with new local search mechanisms . computational experiments confirm a good performance of the proposed algorithm for the considered set of mbmp instances .

probabilistic active learning of functions in structural causal models
we consider the problem of learning the functions computing children from parents in a structural causal model once the underlying causal graph has been identified . this is in some sense the second step after causal discovery . taking a probabilistic approach to estimating these functions , we derive a natural myopic active learning scheme that identifies the intervention which is optimally informative about all of the unknown functions jointly , given previously observed data . we test the derived algorithms on simple examples , to demonstrate that they produce a structured exploration policy that significantly improves on unstructured base-lines .

stochastic separation theorems
the problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all artificial intelligence applications in the real world . its solution requires robust separation of samples with errors from samples where the system works properly . we demonstrate that in ( moderately ) high dimension this separation could be achieved with probability close to one by linear discriminants . surprisingly , separation of a new image from a very large set of known images is almost always possible even in moderately high dimensions by linear functionals , and coefficients of these functionals can be found explicitly . based on fundamental properties of measure concentration , we show that for $ m < a\exp ( b { n } ) $ random $ m $ -element sets in $ \mathbb { r } ^n $ are linearly separable with probability $ p $ , $ p > 1-\vartheta $ , where $ 1 > \vartheta > 0 $ is a given small constant . exact values of $ a , b > 0 $ depend on the probability distribution that determines how the random $ m $ -element sets are drawn , and on the constant $ \vartheta $ . these { \em stochastic separation theorems } provide a new instrument for the development , analysis , and assessment of machine learning methods and algorithms in high dimension . theoretical statements are illustrated with numerical examples .

relational models
we provide a survey on relational models . relational models describe complete networked { domains by taking into account global dependencies in the data } . relational models can lead to more accurate predictions if compared to non-relational machine learning approaches . relational models typically are based on probabilistic graphical models , e.g. , bayesian networks , markov networks , or latent variable models . relational models have applications in social networks analysis , the modeling of knowledge graphs , bioinformatics , recommendation systems , natural language processing , medical decision support , and linked data .

mean field for markov decision processes : from discrete to continuous optimization
we study the convergence of markov decision processes made of a large number of objects to optimization problems on ordinary differential equations ( ode ) . we show that the optimal reward of such a markov decision process , satisfying a bellman equation , converges to the solution of a continuous hamilton-jacobi-bellman ( hjb ) equation based on the mean field approximation of the markov decision process . we give bounds on the difference of the rewards , and a constructive algorithm for deriving an approximating solution to the markov decision process from a solution of the hjb equations . we illustrate the method on three examples pertaining respectively to investment strategies , population dynamics control and scheduling in queues are developed . they are used to illustrate and justify the construction of the controlled ode and to show the gain obtained by solving a continuous hjb equation rather than a large discrete bellman equation .

question asking as program generation
a hallmark of human intelligence is the ability to ask rich , creative , and revealing questions . here we introduce a cognitive model capable of constructing human-like questions . our approach treats questions as formal programs that , when executed on the state of the world , output an answer . the model specifies a probability distribution over a complex , compositional space of programs , favoring concise programs that help the agent learn in the current context . we evaluate our approach by modeling the types of open-ended questions generated by humans who were attempting to learn about an ambiguous situation in a game . we find that our model predicts what questions people will ask , and can creatively produce novel questions that were not present in the training set . in addition , we compare a number of model variants , finding that both question informativeness and complexity are important for producing human-like questions .

the configurable sat solver challenge ( cssc )
it is well known that different solution strategies work well for different types of instances of hard combinatorial problems . as a consequence , most solvers for the propositional satisfiability problem ( sat ) expose parameters that allow them to be customized to a particular family of instances . in the international sat competition series , these parameters are ignored : solvers are run using a single default parameter setting ( supplied by the authors ) for all benchmark instances in a given track . while this competition format rewards solvers with robust default settings , it does not reflect the situation faced by a practitioner who only cares about performance on one particular application and can invest some time into tuning solver parameters for this application . the new configurable sat solver competition ( cssc ) compares solvers in this latter setting , scoring each solver by the performance it achieved after a fully automated configuration step . this article describes the cssc in more detail , and reports the results obtained in its two instantiations so far , cssc 2013 and 2014 .

a real-time autonomous highway accident detection model based on big data processing and computational intelligence
due to increasing urban population and growing number of motor vehicles , traffic congestion is becoming a major problem of the 21st century . one of the main reasons behind traffic congestion is accidents which can not only result in casualties and losses for the participants , but also in wasted and lost time for the others that are stuck behind the wheels . early detection of an accident can save lives , provides quicker road openings , hence decreases wasted time and resources , and increases efficiency . in this study , we propose a preliminary real-time autonomous accident-detection system based on computational intelligence techniques . istanbul city traffic-flow data for the year 2015 from various sensor locations are populated using big data processing methodologies . the extracted features are then fed into a nearest neighbor model , a regression tree , and a feed-forward neural network model . for the output , the possibility of an occurrence of an accident is predicted . the results indicate that even though the number of false alarms dominates the real accident cases , the system can still provide useful information that can be used for status verification and early reaction to possible accidents .

learning unbelievable marginal probabilities
loopy belief propagation performs approximate inference on graphical models with loops . one might hope to compensate for the approximation by adjusting model parameters . learning algorithms for this purpose have been explored previously , and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model . on the contrary , here we show that many probability distributions have marginals that can not be reached by belief propagation using any set of model parameters or any learning algorithm . we call such marginals ` unbelievable . ' this problem occurs whenever the hessian of the bethe free energy is not positive-definite at the target marginals . all learning algorithms for belief propagation necessarily fail in these cases , producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation . we then show that averaging inaccurate beliefs , each obtained from belief propagation using model parameters perturbed about some learned mean values , can achieve the unbelievable marginals .

a method for implementing a probabilistic model as a relational database
this paper discusses a method for implementing a probabilistic inference system based on an extended relational data model . this model provides a unified approach for a variety of applications such as dynamic programming , solving sparse linear equations , and constraint propagation . in this framework , the probability model is represented as a generalized relational database . subsequent probabilistic requests can be processed as standard relational queries . conventional database management systems can be easily adopted for implementing such an approximate reasoning system .

blockchain and artificial intelligence
it is undeniable that artificial intelligence ( ai ) and blockchain concepts are spreading at a phenomenal rate . both technologies have distinct degree of technological complexity and multi-dimensional business implications . however , a common misunderstanding about blockchain concept , in particular , is that blockchain is decentralized and is not controlled by anyone . but the underlying development of a blockchain system is still attributed to a cluster of core developers . take smart contract as an example , it is essentially a collection of codes ( or functions ) and data ( or states ) that are programmed and deployed on a blockchain ( say , ethereum ) by different human programmers . it is thus , unfortunately , less likely to be free of loopholes and flaws . in this article , through a brief overview about how artificial intelligence could be used to deliver bug-free smart contract so as to achieve the goal of blockchain 2.0 , we to emphasize that the blockchain implementation can be assisted or enhanced via various ai techniques . the alliance of ai and blockchain is expected to create numerous possibilities .

recurrent reinforcement learning : a hybrid approach
successful applications of reinforcement learning in real-world problems often require dealing with partially observable states . it is in general very challenging to construct and infer hidden states as they often depend on the agent 's entire interaction history and may require substantial domain knowledge . in this work , we investigate a deep-learning approach to learning the representation of states in partially observable tasks , with minimal prior knowledge of the domain . in particular , we propose a new family of hybrid models that combines the strength of both supervised learning ( sl ) and reinforcement learning ( rl ) , trained in a joint fashion : the sl component can be a recurrent neural networks ( rnn ) or its long short-term memory ( lstm ) version , which is equipped with the desired property of being able to capture long-term dependency on history , thus providing an effective way of learning the representation of hidden states . the rl component is a deep q-network ( dqn ) that learns to optimize the control for maximizing long-term rewards . extensive experiments in a direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach , which performs the best among a set of previous state-of-the-art methods .

a novel method for comparative analysis of dna sequences by ramanujan-fourier transform
alignment-free sequence analysis approaches provide important alternatives over multiple sequence alignment ( msa ) in biological sequence analysis because alignment-free approaches have low computation complexity and are not dependent on high level of sequence identity , however , most of the existing alignment-free methods do not employ true full information content of sequences and thus can not accurately reveal similarities and differences among dna sequences . we present a novel alignment-free computational method for sequence analysis based on ramanujan-fourier transform ( rft ) , in which complete information of dna sequences is retained . we represent dna sequences as four binary indicator sequences and apply rft on the indicator sequences to convert them into frequency domain . the euclidean distance of the complete rft coefficients of dna sequences are used as similarity measure . to address the different lengths in euclidean space of rft coefficients , we pad zeros to short dna binary sequences so that the binary sequences equal the longest length in the comparison sequence data . thus , the dna sequences are compared in the same dimensional frequency space without information loss . we demonstrate the usefulness of the proposed method by presenting experimental results on hierarchical clustering of genes and genomes . the proposed method opens a new channel to biological sequence analysis , classification , and structural module identification .

natural language generation as planning under uncertainty using reinforcement learning
we present and evaluate a new model for natural language generation ( nlg ) in spoken dialogue systems , based on statistical planning , given noisy feedback from the current generation context ( e.g . a user and a surface realiser ) . we study its use in a standard nlg problem : how to present information ( in this case a set of search results ) to users , given the complex trade- offs between utterance length , amount of information conveyed , and cognitive load . we set these trade-offs by analysing existing match data . we then train a nlg pol- icy using reinforcement learning ( rl ) , which adapts its behaviour to noisy feed- back from the current generation context . this policy is compared to several base- lines derived from previous work in this area . the learned policy significantly out- performs all the prior approaches .

efficient algorithm for estimation of qualitative expected utility in possibilistic case-based reasoning
we propose an efficient algorithm for estimation of possibility based qualitative expected utility . it is useful for decision making mechanisms where each possible decision is assigned a multi-attribute possibility distribution . the computational complexity of ordinary methods calculating the expected utility based on discretization is growing exponentially with the number of attributes , and may become infeasible with a high number of these attributes . we present series of theorems and lemmas proving the correctness of our algorithm that exibits a linear computational complexity . our algorithm has been applied in the context of selecting the most prospective partners in multi-party multi-attribute negotiation , and can also be used in making decisions about potential offers during the negotiation as other similar problems .

raisonnement stratifié à base de normes pour inférer les causes dans un corpus textuel
to understand texts written in natural language ( ln ) , we use our knowledge about the norms of the domain . norms allow to infer more implicit information from the text . this kind of information can , in general , be defeasible , but it remains useful and acceptable while the text do not contradict it explicitly . in this paper we describe a non-monotonic reasoning system based on the norms of the car crash domain . the system infers the cause of an accident from its textual description . the cause of an accident is seen as the most specific norm which has been violated . the predicates and the rules of the system are stratified : organized on layers in order to obtain an efficient reasoning .

greedy gossip with eavesdropping
this paper presents greedy gossip with eavesdropping ( gge ) , a novel randomized gossip algorithm for distributed computation of the average consensus problem . in gossip algorithms , nodes in the network randomly communicate with their neighbors and exchange information iteratively . the algorithms are simple and decentralized , making them attractive for wireless network applications . in general , gossip algorithms are robust to unreliable wireless conditions and time varying network topologies . in this paper we introduce gge and demonstrate that greedy updates lead to rapid convergence . we do not require nodes to have any location information . instead , greedy updates are made possible by exploiting the broadcast nature of wireless communications . during the operation of gge , when a node decides to gossip , instead of choosing one of its neighbors at random , it makes a greedy selection , choosing the node which has the value most different from its own . in order to make this selection , nodes need to know their neighbors ' values . therefore , we assume that all transmissions are wireless broadcasts and nodes keep track of their neighbors ' values by eavesdropping on their communications . we show that the convergence of gge is guaranteed for connected network topologies . we also study the rates of convergence and illustrate , through theoretical bounds and numerical simulations , that gge consistently outperforms randomized gossip and performs comparably to geographic gossip on moderate-sized random geometric graph topologies .

crowdsourcing pareto-optimal object finding by pairwise comparisons
this is the first study on crowdsourcing pareto-optimal object finding , which has applications in public opinion collection , group decision making , and information exploration . departing from prior studies on crowdsourcing skyline and ranking queries , it considers the case where objects do not have explicit attributes and preference relations on objects are strict partial orders . the partial orders are derived by aggregating crowdsourcers ' responses to pairwise comparison questions . the goal is to find all pareto-optimal objects by the fewest possible questions . it employs an iterative question-selection framework . guided by the principle of eagerly identifying non-pareto optimal objects , the framework only chooses candidate questions which must satisfy three conditions . this design is both sufficient and efficient , as it is proven to find a short terminal question sequence . the framework is further steered by two ideas -- -macro-ordering and micro-ordering . by different micro-ordering heuristics , the framework is instantiated into several algorithms with varying power in pruning questions . experiment results using both real crowdsourcing marketplace and simulations exhibited not only orders of magnitude reductions in questions when compared with a brute-force approach , but also close-to-optimal performance from the most efficient instantiation .

an algorithmic framework to control bias in bandit-based personalization
personalization is pervasive in the online space as it leads to higher efficiency and revenue by allowing the most relevant content to be served to each user . however , recent studies suggest that personalization methods can propagate societal or systemic biases and polarize opinions ; this has led to calls for regulatory mechanisms and algorithms to combat bias and inequality . algorithmically , bandit optimization has enjoyed great success in learning user preferences and personalizing content or feeds accordingly . we propose an algorithmic framework that allows for the possibility to control bias or discrimination in such bandit-based personalization . our model allows for the specification of general fairness constraints on the sensitive types of the content that can be displayed to a user . the challenge , however , is to come up with a scalable and low regret algorithm for the constrained optimization problem that arises . our main technical contribution is a provably fast and low-regret algorithm for the fairness-constrained bandit optimization problem . our proofs crucially leverage the special structure of our problem . experiments on synthetic and real-world data sets show that our algorithmic framework can control bias with only a minor loss to revenue .

recruitment market trend analysis with sequential latent variable models
recruitment market analysis provides valuable understanding of industry-specific economic growth and plays an important role for both employers and job seekers . with the rapid development of online recruitment services , massive recruitment data have been accumulated and enable a new paradigm for recruitment market analysis . however , traditional methods for recruitment market analysis largely rely on the knowledge of domain experts and classic statistical models , which are usually too general to model large-scale dynamic recruitment data , and have difficulties to capture the fine-grained market trends . to this end , in this paper , we propose a new research paradigm for recruitment market analysis by leveraging unsupervised learning techniques for automatically discovering recruitment market trends based on large-scale recruitment data . specifically , we develop a novel sequential latent variable model , named mtlvm , which is designed for capturing the sequential dependencies of corporate recruitment states and is able to automatically learn the latent recruitment topics within a bayesian generative framework . in particular , to capture the variability of recruitment topics over time , we design hierarchical dirichlet processes for mtlvm . these processes allow to dynamically generate the evolving recruitment topics . finally , we implement a prototype system to empirically evaluate our approach based on real-world recruitment data in china . indeed , by visualizing the results from mtlvm , we can successfully reveal many interesting findings , such as the popularity of lbs related jobs reached the peak in the 2nd half of 2014 , and decreased in 2015 .

the tip-of-the-tongue phenomenon : irrelevant neural network localization or disruption of its interneuron links ?
on the base of recently proposed three-stage quantitative neural network model of the tip-of-the-tongue ( tot ) phenomenon a possibility to occur of tot states coursed by neural network interneuron links ' disruption has been studied . using a numerical example it was found that tots coursed by interneron links ' disruption are in ( 1.5 + - 0.3 ) x1000 times less probable then those coursed by irrelevant ( incomplete ) neural network localization . it was shown that delayed tot states ' etiology can not be related to neural network interneuron links ' disruption .

lpc ( id ) : a sequent calculus proof system for propositional logic extended with inductive definitions
the logic fo ( id ) uses ideas from the field of logic programming to extend first order logic with non-monotone inductive definitions . such logic formally extends logic programming , abductive logic programming and datalog , and thus formalizes the view on these formalisms as logics of ( generalized ) inductive definitions . the goal of this paper is to study a deductive inference method for pc ( id ) , which is the propositional fragment of fo ( id ) . we introduce a formal proof system based on the sequent calculus ( gentzen-style deductive system ) for this logic . as pc ( id ) is an integration of classical propositional logic and propositional inductive definitions , our sequent calculus proof system integrates inference rules for propositional calculus and definitions . we present the soundness and completeness of this proof system with respect to a slightly restricted fragment of pc ( id ) . we also provide some complexity results for pc ( id ) . by developing the proof system for pc ( id ) , it helps us to enhance the understanding of proof-theoretic foundations of fo ( id ) , and therefore to investigate useful proof systems for fo ( id ) .

fuzzy object-oriented dynamic networks . i
the concepts of fuzzy objects and their classes are described that make it possible to structurally represent knowledge about fuzzy and partially-defined objects and their classes . operations over such objects and classes are also proposed that make it possible to obtain sets and new classes of fuzzy objects and also to model variations in object structures under the influence of external factors .

hidden parameter markov decision processes : a semiparametric regression approach for discovering latent task parametrizations
control applications often feature tasks with similar , but not identical , dynamics . we introduce the hidden parameter markov decision process ( hip-mdp ) , a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors , and introduce a semiparametric regression approach for learning its structure from data . in the control setting , we show that a learned hip-mdp rapidly identifies the dynamics of a new task instance , allowing an agent to flexibly adapt to task variations .

reinforcement learning based local search for grouping problems : a case study on graph coloring
grouping problems aim to partition a set of items into multiple mutually disjoint subsets according to some specific criterion and constraints . grouping problems cover a large class of important combinatorial optimization problems that are generally computationally difficult . in this paper , we propose a general solution approach for grouping problems , i.e. , reinforcement learning based local search ( rls ) , which combines reinforcement learning techniques with descent-based local search . the viability of the proposed approach is verified on a well-known representative grouping problem ( graph coloring ) where a very simple descent-based coloring algorithm is applied . experimental studies on popular dimacs and color02 benchmark graphs indicate that rls achieves competitive performances compared to a number of well-known coloring algorithms .

flucap : a heuristic search planner for first-order mdps
we present a heuristic search algorithm for solving first-order markov decision processes ( fomdps ) . our approach combines first-order state abstraction that avoids evaluating states individually , and heuristic search that avoids evaluating all states . firstly , in contrast to existing systems , which start with propositionalizing the fomdp and then perform state abstraction on its propositionalized version we apply state abstraction directly on the fomdp avoiding propositionalization . this kind of abstraction is referred to as first-order state abstraction . secondly , guided by an admissible heuristic , the search is restricted to those states that are reachable from the initial state . we demonstrate the usefulness of the above techniques for solving fomdps with a system , referred to as flucap ( formerly , fcplanner ) , that entered the probabilistic track of the 2004 international planning competition ( ipc2004 ) and demonstrated an advantage over other planners on the problems represented in first-order terms .

wav2letter : an end-to-end convnet-based speech recognition system
this paper presents a simple end-to-end model for speech recognition , combining a convolutional network based acoustic model and a graph decoding . it is trained to output letters , with transcribed speech , without the need for force alignment of phonemes . we introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with ctc while being simpler . we show competitive results in word error rate on the librispeech corpus with mfcc features , and promising results from raw waveform .

ai evaluation : past , present and future
artificial intelligence develops techniques and systems whose performance must be evaluated on a regular basis in order to certify and foster progress in the discipline . we will describe and critically assess the different ways ai systems are evaluated . we first focus on the traditional task-oriented evaluation approach . we see that black-box ( behavioural evaluation ) is becoming more and more common , as ai systems are becoming more complex and unpredictable . we identify three kinds of evaluation : human discrimination , problem benchmarks and peer confrontation . we describe the limitations of the many evaluation settings and competitions in these three categories and propose several ideas for a more systematic and robust evaluation . we then focus on a less customary ( and challenging ) ability-oriented evaluation approach , where a system is characterised by its ( cognitive ) abilities , rather than by the tasks it is designed to solve . we discuss several possibilities : the adaptation of cognitive tests used for humans and animals , the development of tests derived from algorithmic information theory or more general approaches under the perspective of universal psychometrics .

a topological approach to meta-heuristics : analytical results on the bfs vs. dfs algorithm selection problem
search is a central problem in artificial intelligence , and bfs and dfs the two most fundamental ways to search . in this report we derive results for average bfs and dfs runtime : for tree search , we employ a probabilistic model of goal distribution ; for graph search , the analysis depends on an additional statistic of path redundancy and average branching factor . as an application , we use the results on two concrete grammar problems . the runtime estimates can be used to select the faster out of bfs and dfs for a given problem , and may form the basis for further analysis of more advanced search methods . finally , we verify our results experimentally ; the analytical approximations come surprisingly close to empirical reality .

evaluating race and sex diversity in the world 's largest companies using deep neural networks
diversity is one of the fundamental properties for the survival of species , populations , and organizations . recent advances in deep learning allow for the rapid and automatic assessment of organizational diversity and possible discrimination by race , sex , age and other parameters . automating the process of assessing the organizational diversity using the deep neural networks and eliminating the human factor may provide a set of real-time unbiased reports to all stakeholders . in this pilot study we applied the deep-learned predictors of race and sex to the executive management and board member profiles of the 500 largest companies from the 2016 forbes global 2000 list and compared the predicted ratios to the ratios within each company 's country of origin and ranked them by the sex- , age- and race- diversity index ( di ) . while the study has many limitations and no claims are being made concerning the individual companies , it demonstrates a method for the rapid and impartial assessment of organizational diversity using deep neural networks .

web robot detection in academic publishing
recent industry reports assure the rise of web robots which comprise more than half of the total web traffic . they not only threaten the security , privacy and efficiency of the web but they also distort analytics and metrics , doubting the veracity of the information being promoted . in the academic publishing domain , this can cause articles to be faulty presented as prominent and influential . in this paper , we present our approach on detecting web robots in academic publishing websites . we use different supervised learning algorithms with a variety of characteristics deriving from both the log files of the server and the content served by the website . our approach relies on the assumption that human users will be interested in specific domains or articles , while web robots crawl a web library incoherently . we experiment with features adopted in previous studies with the addition of novel semantic characteristics which derive after performing a semantic analysis using the latent dirichlet allocation ( lda ) algorithm . our real-world case study shows promising results , pinpointing the significance of semantic features in the web robot detection problem .

discovering reliable approximate functional dependencies
given a database and a target attribute of interest , how can we tell whether there exists a functional , or approximately functional dependence of the target on any set of other attributes in the data ? how can we reliably , without bias to sample size or dimensionality , measure the strength of such a dependence ? and , how can we efficiently discover the optimal or $ \alpha $ -approximate top- $ k $ dependencies ? these are exactly the questions we answer in this paper . as we want to be agnostic on the form of the dependence , we adopt an information-theoretic approach , and construct a reliable , bias correcting score that can be efficiently computed . moreover , we give an effective optimistic estimator of this score , by which for the first time we can mine the approximate functional dependencies from data with guarantees of optimality . empirical evaluation shows that the derived score achieves a good bias for variance trade-off , can be used within an efficient discovery algorithm , and indeed discovers meaningful dependencies . most important , it remains reliable in the face of data sparsity .

semantic web search based on ontology modeling using protege reasoner
the semantic web works on the existing web which presents the meaning of information as well-defined vocabularies understood by the people . semantic search , at the same time , works on improving the accuracy if a search by understanding the intent of the search and providing contextually relevant results . this paper describes a semantic approach toward web search through a php application . the goal was to parse through a user 's browsing history and return semantically relevant web pages for the search query provided .

what 's in a name ?
this paper describes experiments on identifying the language of a single name in isolation or in a document written in a different language . a new corpus has been compiled and made available , matching names against languages . this corpus is used in a series of experiments measuring the performance of general language models and names-only language models on the language identification task . conclusions are drawn from the comparison between using general language models and names-only language models and between identifying the language of isolated names and the language of very short document fragments . future research directions are outlined .

modified frank-wolfe algorithm for enhanced sparsity in support vector machine classifiers
this work proposes a new algorithm for training a re-weighted l2 support vector machine ( svm ) , inspired on the re-weighted lasso algorithm of cand\ ` es et al . and on the equivalence between lasso and svm shown recently by jaggi . in particular , the margin required for each training vector is set independently , defining a new weighted svm model . these weights are selected to be binary , and they are automatically adapted during the training of the model , resulting in a variation of the frank-wolfe optimization algorithm with essentially the same computational complexity as the original algorithm . as shown experimentally , this algorithm is computationally cheaper to apply since it requires less iterations to converge , and it produces models with a sparser representation in terms of support vectors and which are more stable with respect to the selection of the regularization hyper-parameter .

knowledge embedding and retrieval strategies in an informledge system
informledge system ( ils ) is a knowledge network with autonomous nodes and intelligent links that integrate and structure the pieces of knowledge . in this paper , we put forward the strategies for knowledge embedding and retrieval in an ils . ils is a powerful knowledge network system dealing with logical storage and connectivity of information units to form knowledge using autonomous nodes and multi-lateral links . in ils , the autonomous nodes known as knowledge network nodes ( knn ) s play vital roles which are not only used in storage , parsing and in forming the multi-lateral linkages between knowledge points but also in helping the realization of intelligent retrieval of linked information units in the form of knowledge . knowledge built in to the ils forms the shape of sphere . the intelligence incorporated into the links of a knn helps in retrieving various knowledge threads from a specific set of knns . a developed entity of information realized through knn forms in to the shape of a knowledge cone

modelling and analysis of temporal preference drifts using a component-based factorised latent approach
the changes in user preferences can originate from substantial reasons , like personality shift , or transient and circumstantial ones , like seasonal changes in item popularities . disregarding these temporal drifts in modelling user preferences can result in unhelpful recommendations . moreover , different temporal patterns can be associated with various preference domains , and preference components and their combinations . these components comprise preferences over features , preferences over feature values , conditional dependencies between features , socially-influenced preferences , and bias . for example , in the movies domain , the user can change his rating behaviour ( bias shift ) , her preference for genre over language ( feature preference shift ) , or start favouring drama over comedy ( feature value preference shift ) . in this paper , we first propose a novel latent factor model to capture the domain-dependent component-specific temporal patterns in preferences . the component-based approach followed in modelling the aspects of preferences and their temporal effects enables us to arbitrarily switch components on and off . we evaluate the proposed method on three popular recommendation datasets and show that it significantly outperforms the most accurate state-of-the-art static models . the experiments also demonstrate the greater robustness and stability of the proposed dynamic model in comparison with the most successful models to date . we also analyse the temporal behaviour of different preference components and their combinations and show that the dynamic behaviour of preference components is highly dependent on the preference dataset and domain . therefore , the results also highlight the importance of modelling temporal effects but also underline the advantages of a component-based architecture that is better suited to capture domain-specific balances in the contributions of the aspects .

revisiting unreasonable effectiveness of data in deep learning era
the success of deep learning in vision can be attributed to : ( a ) models with high capacity ; ( b ) increased computational power ; and ( c ) availability of large-scale labeled data . since 2012 , there have been significant advances in representation capabilities of the models and computational capabilities of gpus . but the size of the biggest dataset has surprisingly remained constant . what will happen if we increase the dataset size by 10x or 100x ? this paper takes a step towards clearing the clouds of mystery surrounding the relationship between ` enormous data ' and visual deep learning . by exploiting the jft-300m dataset which has more than 375m noisy labels for 300m images , we investigate how the performance of current vision tasks would change if this data was used for representation learning . our paper delivers some surprising ( and some expected ) findings . first , we find that the performance on vision tasks increases logarithmically based on volume of training data size . second , we show that representation learning ( or pre-training ) still holds a lot of promise . one can improve performance on many vision tasks by just training a better base model . finally , as expected , we present new state-of-the-art results for different vision tasks including image classification , object detection , semantic segmentation and human pose estimation . our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets .

asynchronous announcements
we propose a logic of asynchronous announcements , where truthful announcements are publicly sent but individually received by agents . additional to epistemic modalities , the logic therefore contains two types of dynamic modalities , for sending messages and for receiving messages . the semantics defines truth relative to the current state of reception of messages for all agents . this means that knowledge need not be truthful , because some messages may not have been received by the knowing agent . messages that are announcements may also result in partial synchronization , namely when an agent learns from receiving an announcement that other announcements must already have been received by other agents . we give detailed examples of the semantics , and prove several semantic results , including that : after an announcement an agent knows that a proposition is true , if and only if on condition of the truth of that announcement , the agent knows that after that announcement and after any number of other agents also receiving it , the proposition is true . we show that on multi-agent epistemic models , each formula in asynchronous announcement logic is equivalent to a formula in epistemic logic .

finding robust solutions to stable marriage
we study the notion of robustness in stable matching problems . we first define robustness by introducing ( a , b ) -supermatches . an $ ( a , b ) $ -supermatch is a stable matching in which if $ a $ pairs break up it is possible to find another stable matching by changing the partners of those $ a $ pairs and at most $ b $ other pairs . in this context , we define the most robust stable matching as a $ ( 1 , b ) $ -supermatch where b is minimum . we show that checking whether a given stable matching is a $ ( 1 , b ) $ -supermatch can be done in polynomial time . next , we use this procedure to design a constraint programming model , a local search approach , and a genetic algorithm to find the most robust stable matching . our empirical evaluation on large instances show that local search outperforms the other approaches .

a model-based active testing approach to sequential diagnosis
model-based diagnostic reasoning often leads to a large number of diagnostic hypotheses . the set of diagnoses can be reduced by taking into account extra observations ( passive monitoring ) , measuring additional variables ( probing ) or executing additional tests ( sequential diagnosis/test sequencing ) . in this paper we combine the above approaches with techniques from automated test pattern generation ( atpg ) and model-based diagnosis ( mbd ) into a framework called fractal ( framework for active testing algorithms ) . apart from the inputs and outputs that connect a system to its environment , in active testing we consider additional input variables to which a sequence of test vectors can be supplied . we address the computationally hard problem of computing optimal control assignments ( as defined in fractal ) in terms of a greedy approximation algorithm called fractal-g. we compare the decrease in the number of remaining minimal cardinality diagnoses of fractal-g to that of two more fractal algorithms : fractal-atpg and fractal-p. fractal-atpg is based on atpg and sequential diagnosis while fractal-p is based on probing and , although not an active testing algorithm , provides a baseline for comparing the lower bound on the number of reachable diagnoses for the fractal algorithms . we empirically evaluate the trade-offs of the three fractal algorithms by performing extensive experimentation on the iscas85/74xxx benchmark of combinational circuits .

joint learning of ontology and semantic parser from text
semantic parsing methods are used for capturing and representing semantic meaning of text . meaning representation capturing all the concepts in the text may not always be available or may not be sufficiently complete . ontologies provide a structured and reasoning-capable way to model the content of a collection of texts . in this work , we present a novel approach to joint learning of ontology and semantic parser from text . the method is based on semi-automatic induction of a context-free grammar from semantically annotated text . the grammar parses the text into semantic trees . both , the grammar and the semantic trees are used to learn the ontology on several levels -- classes , instances , taxonomic and non-taxonomic relations . the approach was evaluated on the first sentences of wikipedia pages describing people .

on convergence property of implicit self-paced objective
self-paced learning ( spl ) is a new methodology that simulates the learning principle of humans/animals to start learning easier aspects of a learning task , and then gradually take more complex examples into training . this new-coming learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks . recently , it has been proved that the spl regime has a close relationship to a implicit self-paced objective function . while this implicit objective could provide helpful interpretations to the effectiveness , especially the robustness , insights under the spl paradigms , there are still no theoretical results strictly proved to verify such relationship . to this issue , in this paper , we provide some convergence results on this implicit objective of spl . specifically , we prove that the learning process of spl always converges to critical points of this implicit objective under some mild conditions . this result verifies the intrinsic relationship between spl and this implicit objective , and makes the previous robustness analysis on spl complete and theoretically rational .

complete axiomatizations for reasoning about knowledge and time
sound and complete axiomatizations are provided for a number of different logics involving modalities for knowledge and time . these logics arise from different choices for various parameters . all the logics considered involve the discrete time linear temporal logic operators ` next ' and ` until ' and an operator for the knowledge of each of a number of agents . both the single agent and multiple agent cases are studied : in some instances of the latter there is also an operator for the common knowledge of the group of all agents . four different semantic properties of agents are considered : whether they have a unique initial state , whether they operate synchronously , whether they have perfect recall , and whether they learn . the property of no learning is essentially dual to perfect recall . not all settings of these parameters lead to recursively axiomatizable logics , but sound and complete axiomatizations are presented for all the ones that do .

offline specialisation in prolog using a hand-written compiler generator
the so called `` cogen approach '' to program specialisation , writing a compiler generator instead of a specialiser , has been used with considerable success in partial evaluation of both functional and imperative languages . this paper demonstrates that the cogen approach is also applicable to the specialisation of logic programs ( also called partial deduction ) and leads to effective specialisers . moreover , using good binding-time annotations , the speed-ups of the specialised programs are comparable to the speed-ups obtained with online specialisers . the paper first develops a generic approach to offline partial deduction and then a specific offline partial deduction method , leading to the offline system lix for pure logic programs . while this is a usable specialiser by itself , it is used to develop the cogen system logen . given a program , a specification of what inputs will be static , and an annotation specifying which calls should be unfolded , logen generates a specialised specialiser for the program at hand . running this specialiser with particular values for the static inputs results in the specialised program . while this requires two steps instead of one , the efficiency of the specialisation process is improved in situations where the same program is specialised multiple times . the paper also presents and evaluates an automatic binding-time analysis that is able to derive the annotations . while the derived annotations are still suboptimal compared to hand-crafted ones , they enable non-expert users to use the logen system in a fully automated way . finally , logen is extended so as to directly support a large part of prolog 's declarative and non-declarative features and so as to be able to perform so called mixline specialisations .

a pareto front-based multiobjective path planning algorithm
path planning is one of the most vital elements of mobile robotics . with a priori knowledge of the environment , global path planning provides a collision-free route through the workspace . the global path plan can be calculated with a variety of informed search algorithms , most notably the a* search method , guaranteed to deliver a complete and optimal solution that minimizes the path cost . path planning optimization typically looks to minimize the distance traversed from start to goal , yet many mobile robot applications call for additional path planning objectives , presenting a multiobjective optimization ( moo ) problem . past studies have applied genetic algorithms to moo path planning problems , but these may have the disadvantages of computational complexity and suboptimal solutions . alternatively , the algorithm in this paper approaches moo path planning with the use of pareto fronts , or finding non-dominated solutions . the algorithm presented incorporates pareto optimality into every step of a* search , thus it is named a*-po . results of simulations show a*-po outperformed several variations of the standard a* algorithm for moo path planning . a planetary exploration rover case study was added to demonstrate the viability of a*-po in a real-world application .

probabilistic inference in influence diagrams
this paper is about reducing influence diagram ( id ) evaluation into bayesian network ( bn ) inference problems . such reduction is interesting because it enables one to readily use one 's favorite bn inference algorithm to efficiently evaluate ids . two such reduction methods have been proposed previously ( cooper 1988 , shachter and peot 1992 ) . this paper proposes a new method . the bn inference problems induced by the mew method are much easier to solve than those induced by the two previous methods .

building fast bayesian computing machines out of intentionally stochastic , digital parts
the brain interprets ambiguous sensory information faster and more reliably than modern computers , using neurons that are slower and less reliable than logic gates . but bayesian inference , which underpins many computational models of perception and cognition , appears computationally challenging even given modern transistor speeds and energy budgets . the computational principles and structures needed to narrow this gap are unknown . here we show how to build fast bayesian computing machines using intentionally stochastic , digital parts , narrowing this efficiency gap by multiple orders of magnitude . we find that by connecting stochastic digital components according to simple mathematical rules , one can build massively parallel , low precision circuits that solve bayesian inference problems and are compatible with the poisson firing statistics of cortical neurons . we evaluate circuits for depth and motion perception , perceptual learning and causal reasoning , each performing inference over 10,000+ latent variables in real time - a 1,000x speed advantage over commodity microprocessors . these results suggest a new role for randomness in the engineering and reverse-engineering of intelligent computation .

how much chemistry does a deep neural network need to know to make accurate predictions ?
in the last few years , we have seen the rise of deep learning applications in a broad range of chemistry research problems . recently , we reported on the development of chemception , a deep convolutional neural network ( cnn ) architecture for general-purpose small molecule property prediction . in this work , we investigate the effects of systematically removing and adding basic chemical information to the image channels of the 2d images used to train chemception . by augmenting images with only 3 additional basic chemical information , we demonstrate that chemception now outperforms contemporary deep learning models trained on more sophisticated chemical representations ( molecular fingerprints ) for the prediction of toxicity , activity , and solvation free energy , as well as physics-based free energy simulation methods . thus , our work demonstrates that a firm grasp of first-principles chemical knowledge is not a pre-requisite for deep learning models to accurately predict chemical properties . lastly , by altering the chemical information content in the images , and examining the resulting performance of chemception , we also identify two different learning patterns in predicting toxicity/activity as compared to solvation free energy , and these patterns suggest that chemception is learning about its tasks in the manner that is consistent with established knowledge .

composition of credal sets via polyhedral geometry
recently introduced composition operator for credal sets is an analogy of such operators in probability , possibility , evidence and valuation-based systems theories . it was designed to construct multidimensional models ( in the framework of credal sets ) from a system of low- dimensional credal sets . in this paper we study its potential from the computational point of view utilizing methods of polyhedral geometry .

deep reinforcement learning from self-play in imperfect-information games
many real-world applications can be described as large-scale games of imperfect information . to deal with these challenging domains , prior work has focused on computing nash equilibria in a handcrafted abstraction of the domain . in this paper we introduce the first scalable end-to-end approach to learning approximate nash equilibria without prior domain knowledge . our method combines fictitious self-play with deep reinforcement learning . when applied to leduc poker , neural fictitious self-play ( nfsp ) approached a nash equilibrium , whereas common reinforcement learning methods diverged . in limit texas holdem , a poker game of real-world scale , nfsp learnt a strategy that approached the performance of state-of-the-art , superhuman algorithms based on significant domain expertise .

resource planning for rescue operations
after an earthquake , disaster sites pose a multitude of health and safety concerns . a rescue operation of people trapped in the ruins after an earthquake disaster requires a series of intelligent behavior , including planning . for a successful rescue operation , given a limited number of available actions and regulations , the role of planning in rescue operations is crucial . fortunately , recent developments in automated planning by artificial intelligence community can help different organization in this crucial task . due to the number of rules and regulations , we believe that a rule based system for planning can be helpful for this specific planning problem . in this research work , we use logic rules to represent rescue and related regular regulations , together with a logic based planner to solve this complicated problem . although this research is still in the prototyping and modeling stage , it clearly shows that rule based languages can be a good infrastructure for this computational task . the results of this research can be used by different organizations , such as iranian red crescent society and international institute of seismology and earthquake engineering ( iisee ) .

generalized prediction intervals for arbitrary distributed high-dimensional data
this paper generalizes the traditional statistical concept of prediction intervals for arbitrary probability density functions in high-dimensional feature spaces by introducing significance level distributions , which provides interval-independent probabilities for continuous random variables . the advantage of the transformation of a probability density function into a significance level distribution is that it enables one-class classification or outlier detection in a direct manner .

fuzzy knowledge representation , learning and optimization with bayesian analysis in fuzzy semantic networks
this paper presents a method of optimization , based on both bayesian analysis technical and gallois lattice , of a fuzzy semantic networks . the technical system we use learn by interpreting an unknown word using the links created between this new word and known words . the main link is provided by the context of the query . when novice 's query is confused with an unknown verb ( goal ) applied to a known noun denoting either an object in the ideal user 's network or an object in the user 's network , the system infer that this new verb corresponds to one of the known goal . with the learning of new words in natural language as the interpretation , which was produced in agreement with the user , the system improves its representation scheme at each experiment with a new user and , in addition , takes advantage of previous discussions with users . the semantic net of user objects thus obtained by these kinds of learning is not always optimal because some relationships between couple of user objects can be generalized and others suppressed according to values of forces that characterize them . indeed , to simplify the obtained net , we propose to proceed to an inductive bayesian analysis , on the net obtained from gallois lattice . the objective of this analysis can be seen as an operation of filtering of the obtained descriptive graph .

h-approximation : history-based approximation of possible world semantics as asp
we propose an approximation of the possible worlds semantics ( pws ) for action planning . a corresponding planning system is implemented by a transformation of the action specification to an answer-set program . a novelty is support for postdiction wrt . ( a ) the plan existence problem in our framework can be solved in np , as compared to $ \sigma_2^p $ for non-approximated pws of baral ( 2000 ) ; and ( b ) the planner generates optimal plans wrt . a minimal number of actions in $ \delta_2^p $ . we demo the planning system with standard problems , and illustrate its integration in a larger software framework for robot control in a smart home .

evidential force aggregation
in this paper we develop an evidential force aggregation method intended for classification of evidential intelligence into recognized force structures . we assume that the intelligence has already been partitioned into clusters and use the classification method individually in each cluster . the classification is based on a measure of fitness between template and fused intelligence that makes it possible to handle intelligence reports with multiple nonspecific and uncertain propositions . with this measure we can aggregate on a level-by-level basis , starting from general intelligence to achieve a complete force structure with recognized units on all hierarchical levels .

handling uncertainty in a system for text-symbol context analysis
in pattern analysis , information regarding an object can often be drawn from its surroundings . this paper presents a method for handling uncertainty when using context of symbols and texts for analyzing technical drawings . the method is based on dempster-shafer theory and possibility theory .

intransitivity and vagueness
there are many examples in the literature that suggest that indistinguishability is intransitive , despite the fact that the indistinguishability relation is typically taken to be an equivalence relation ( and thus transitive ) . it is shown that if the uncertainty perception and the question of when an agent reports that two things are indistinguishable are both carefully modeled , the problems disappear , and indistinguishability can indeed be taken to be an equivalence relation . moreover , this model also suggests a logic of vagueness that seems to solve many of the problems related to vagueness discussed in the philosophical literature . in particular , it is shown here how the logic can handle the sorites paradox .

mining multi-level frequent itemsets under constraints
mining association rules is a task of data mining , which extracts knowledge in the form of significant implication relation of useful items ( objects ) from a database . mining multilevel association rules uses concept hierarchies , also called taxonomies and defined as relations of type 'is-a ' between objects , to extract rules that items belong to different levels of abstraction . these rules are more useful , more refined and more interpretable by the user . several algorithms have been proposed in the literature to discover the multilevel association rules . in this article , we are interested in the problem of discovering multi-level frequent itemsets under constraints , involving the user in the research process . we proposed a technique for modeling and interpretation of constraints in a context of use of concept hierarchies . three approaches for discovering multi-level frequent itemsets under constraints were proposed and discussed : basic approach , `` test and generate '' approach and pruning based approach .

algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning
deep convolutional neural networks ( dcnns ) have been used to achieve state-of-the-art performance on many computer vision tasks ( e.g. , object recognition , object detection , semantic segmentation ) thanks to a large repository of annotated image data . large labeled datasets for other sensor modalities , e.g. , multispectral imagery ( msi ) , are not available due to the large cost and manpower required . in this paper , we adapt state-of-the-art dcnn frameworks in computer vision for semantic segmentation for msi imagery . to overcome label scarcity for msi data , we substitute real msi for generated synthetic msi in order to initialize a dcnn framework . we evaluate our network initialization scheme on the new rit-18 dataset that we present in this paper . this dataset contains very-high resolution msi collected by an unmanned aircraft system . the models initialized with synthetic imagery were less prone to over-fitting and provide a state-of-the-art baseline for future work .

evaluating morphological computation in muscle and dc-motor driven models of human hopping
in the context of embodied artificial intelligence , morphological computation refers to processes which are conducted by the body ( and environment ) that otherwise would have to be performed by the brain . exploiting environmental and morphological properties is an important feature of embodied systems . the main reason is that it allows to significantly reduce the controller complexity . an important aspect of morphological computation is that it can not be assigned to an embodied system per se , but that it is , as we show , behavior- and state-dependent . in this work , we evaluate two different measures of morphological computation that can be applied in robotic systems and in computer simulations of biological movement . as an example , these measures were evaluated on muscle and dc-motor driven hopping models . we show that a state-dependent analysis of the hopping behaviors provides additional insights that can not be gained from the averaged measures alone . this work includes algorithms and computer code for the measures .

development of a cargo screening process simulator : a first approach
the efficiency of current cargo screening processes at sea and air ports is largely unknown as few benchmarks exists against which they could be measured . some manufacturers provide benchmarks for individual sensors but we found no benchmarks that take a holistic view of the overall screening procedures and no benchmarks that take operator variability into account . just adding up resources and manpower used is not an effective way for assessing systems where human decision-making and operator compliance to rules play a vital role . our aim is to develop a decision support tool ( cargo-screening system simulator ) that will map the right technology and manpower to the right commodity-threat combination in order to maximise detection rates . in this paper we present our ideas for developing such a system and highlight the research challenges we have identified . then we introduce our first case study and report on the progress we have made so far .

glass-box program synthesis : a machine learning approach
recently proposed models which learn to write computer programs from data use either input/output examples or rich execution traces . instead , we argue that a novel alternative is to use a glass-box loss function , given as a program itself that can be directly inspected . glass-box optimization covers a wide range of problems , from computing the greatest common divisor of two integers , to learning-to-learn problems . in this paper , we present an intelligent search system which learns , given the partial program and the glass-box problem , the probabilities over the space of programs . we empirically demonstrate that our informed search procedure leads to significant improvements compared to brute-force program search , both in terms of accuracy and time . for our experiments we use rich context free grammars inspired by number theory , text processing , and algebra . our results show that ( i ) performing 4 rounds of our framework typically solves about 70 % of the target problems , ( ii ) our framework can improve itself even in domain agnostic scenarios , and ( iii ) it can solve problems that would be otherwise too slow to solve with brute-force search .

solving factored mdps with hybrid state and action variables
efficient representations and solutions for large decision problems with continuous and discrete variables are among the most important challenges faced by the designers of automated decision support systems . in this paper , we describe a novel hybrid factored markov decision process ( mdp ) model that allows for a compact representation of these problems , and a new hybrid approximate linear programming ( halp ) framework that permits their efficient solutions . the central idea of halp is to approximate the optimal value function by a linear combination of basis functions and optimize its weights by linear programming . we analyze both theoretical and computational aspects of this approach , and demonstrate its scale-up potential on several hybrid optimization problems .

performing bayesian risk aggregation using discrete approximation algorithms with graph factorization
risk aggregation is a popular method used to estimate the sum of a collection of financial assets or events , where each asset or event is modelled as a random variable . applications , in the financial services industry , include insurance , operational risk , stress testing , and sensitivity analysis , but the problem is widely encountered in many other application domains . this thesis has contributed two algorithms to perform bayesian risk aggregation when model exhibit hybrid dependency and high dimensional inter-dependency . the first algorithm operates on a subset of the general problem , with an emphasis on convolution problems , in the presence of continuous and discrete variables ( so called hybrid models ) and the second algorithm offer a universal method for general purpose inference over much wider classes of bayesian network models .

beyond word embeddings : learning entity and concept representations from large scale knowledge bases
text representation using neural word embeddings has proven efficacy in many nlp applications . recently , a lot of research interest goes beyond word embeddings by adapting the traditional word embedding models to learn vectors of multiword expressions ( concepts/entities ) . however , current methods are limited to textual knowledge bases only ( e.g. , wikipedia ) . in this paper , we propose a novel approach for learning concept vectors from two large scale knowledge bases ( wikipedia , and probase ) . we adapt the skip-gram model to seamlessly learn from the knowledge in wikipedia text and probase concept graph . we evaluate our concept embedding models intrinsically on two tasks : 1 ) analogical reasoning where we achieve a state-of-the-art performance of 91 % on semantic analogies , 2 ) concept categorization where we achieve a state-of-the-art performance on two benchmark datasets achieving categorization accuracy of 100 % on one and 98 % on the other . additionally , we present a case study to extrinsically evaluate our model on unsupervised argument type identification for neural semantic parsing . we demonstrate the competitive accuracy of our unsupervised method and its ability to better generalize to out of vocabulary entity mentions compared to the tedious and error prone methods which depend on gazetteers and regular expressions .

flow of activity in the ouroboros model
the ouroboros model is a new conceptual proposal for an algorithmic structure for efficient data processing in living beings as well as for artificial agents . its central feature is a general repetitive loop where one iteration cycle sets the stage for the next . sensory input activates data structures ( schemata ) with similar constituents encountered before , thus expectations are kindled . this corresponds to the highlighting of empty slots in the selected schema , and these expectations are compared with the actually encountered input . depending on the outcome of this consumption analysis different next steps like search for further data or a reset , i.e . a new attempt employing another schema , are triggered . monitoring of the whole process , and in particular of the flow of activation directed by the consumption analysis , yields valuable feedback for the optimum allocation of attention and resources including the selective establishment of useful new memory entries .

data centroid based multi-level fuzzy min-max neural network
recently , a multi-level fuzzy min max neural network ( mlf ) was proposed , which improves the classification accuracy by handling an overlapped region ( area of confusion ) with the help of a tree structure . in this brief , an extension of mlf is proposed which defines a new boundary region , where the previously proposed methods mark decisions with less confidence and hence misclassification is more frequent . a methodology to classify patterns more accurately is presented . our work enhances the testing procedure by means of data centroids . we exhibit an illustrative example , clearly highlighting the advantage of our approach . results on standard datasets are also presented to evidentially prove a consistent improvement in the classification rate .

ranking pages by topology and popularity within web sites
we compare two link analysis ranking methods of web pages in a site . the first , called site rank , is an adaptation of pagerank to the granularity of a web site and the second , called popularity rank , is based on the frequencies of user clicks on the outlinks in a page that are captured by navigation sessions of users through the web site . we ran experiments on artificially created web sites of different sizes and on two real data sets , employing the relative entropy to compare the distributions of the two ranking methods . for the real data sets we also employ a nonparametric measure , called spearman 's footrule , which we use to compare the top-ten web pages ranked by the two methods . our main result is that the distributions of the popularity rank and site rank are surprisingly close to each other , implying that the topology of a web site is very instrumental in guiding users through the site . thus , in practice , the site rank provides a reasonable first order approximation of the aggregate behaviour of users within a web site given by the popularity rank .

modeling relational data with graph convolutional networks
knowledge graphs enable a wide variety of applications , including question answering and information retrieval . despite the great effort invested in their creation and maintenance , even the largest ( e.g. , yago , dbpedia or wikidata ) remain incomplete . we introduce relational graph convolutional networks ( r-gcns ) and apply them to two standard knowledge base completion tasks : link prediction ( recovery of missing facts , i.e . subject-predicate-object triples ) and entity classification ( recovery of missing entity attributes ) . r-gcns are related to a recent class of neural networks operating on graphs , and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases . we demonstrate the effectiveness of r-gcns as a stand-alone model for entity classification . we further show that factorization models for link prediction such as distmult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph , demonstrating a large improvement of 29.8 % on fb15k-237 over a decoder-only baseline .

online spatial concept and lexical acquisition with simultaneous localization and mapping
in this paper , we propose an online learning algorithm based on a rao-blackwellized particle filter for spatial concept acquisition and mapping . we have proposed a nonparametric bayesian spatial concept acquisition model ( spcoa ) . we propose a novel method ( spcoslam ) integrating spcoa and fastslam in the theoretical framework of the bayesian generative model . the proposed method can simultaneously learn place categories and lexicons while incrementally generating an environmental map . furthermore , the proposed method has scene image features and a language model added to spcoa . in the experiments , we tested online learning of spatial concepts and environmental maps in a novel environment of which the robot did not have a map . then , we evaluated the results of online learning of spatial concepts and lexical acquisition . the experimental results demonstrated that the robot was able to more accurately learn the relationships between words and the place in the environmental map incrementally by using the proposed method .

ontoloki : an automatic , instance-based method for the evaluation of biological ontologies on the semantic web
the delineation of logical definitions for each class in an ontology and the consistent application of these definitions to the assignment of instances to classes are important criteria for ontology evaluation . if ontologies are specified with property-based restrictions on class membership , then such consistency can be checked automatically . if no such logical restrictions are applied , as is the case with many biological ontologies , there are currently no automated methods for measuring the semantic consistency of instance assignment on an ontology-wide scale , nor for inferring the patterns of properties that might define a particular class . we constructed a program that takes as its input an owl/rdf knowledge base containing an ontology , instances associated with each of the classes in the ontology , and properties of those instances . for each class , it outputs : 1 ) a rule for determining class membership based on the properties of the instances and 2 ) a quantitative score for the class that reflects the ability of the identified rule to correctly predict class membership for the instances in the knowledge base . we evaluated this program using both artificial knowledge bases of known quality and real , widely used ontologies . the results indicate that the suggested method can be used to conduct objective , automatic , data-driven evaluations of biological ontologies without formal class definitions in regards to the property-based consistency of instance-assignment . this inductive method complements existing , purely deductive approaches to automatic consistency checking , offering not just the potential to help in the ontology engineering process but also in the knowledge discovery process .

a truth serum for large-scale evaluations
a major challenge in obtaining large-scale evaluations , e.g. , product or service reviews on online platforms , labeling images , grading in online courses , etc. , is that of eliciting honest responses from agents in the absence of verifiability . we propose a new reward mechanism with strong incentive properties applicable in a wide variety of such settings . this mechanism has a simple and intuitive output agreement structure : an agent gets a reward only if her response for an evaluation matches that of her peer . but instead of the reward being the same across different answers , it is inversely proportional to a popularity index of each answer . this index is a second order population statistic that captures how frequently two agents performing the same evaluation agree on the particular answer . rare agreements thus earn a higher reward than agreements that are relatively more common . in the regime where there are a large number of evaluation tasks , we show that truthful behavior is a strict bayes-nash equilibrium of the game induced by the mechanism . further , we show that the truthful equilibrium is approximately optimal in terms of expected payoffs to the agents across all symmetric equilibria , where the approximation error vanishes in the number of evaluation tasks . moreover , under a mild condition on strategy space , we show that any symmetric equilibrium that gives a higher expected payoff than the truthful equilibrium must be close to being fully informative if the number of evaluations is large . these last two results are driven by a new notion of an agreement measure that is shown to be monotonic in information loss . this notion and its properties are of independent interest .

toward a research agenda in adversarial reasoning : computational approaches to anticipating the opponent 's intent and actions
this paper defines adversarial reasoning as computational approaches to inferring and anticipating an enemy 's perceptions , intents and actions . it argues that adversarial reasoning transcends the boundaries of game theory and must also leverage such disciplines as cognitive modeling , control theory , ai planning and others . to illustrate the challenges of applying adversarial reasoning to real-world problems , the paper explores the lessons learned in the cadet - a battle planning system that focuses on brigade-level ground operations and involves adversarial reasoning . from this example of current capabilities , the paper proceeds to describe raid - a darpa program that aims to build capabilities in adversarial reasoning , and how such capabilities would address practical requirements in defense and other application areas .

log-linear rnns : towards recurrent neural networks with flexible prior knowledge
we introduce ll-rnns ( log-linear rnns ) , an extension of recurrent neural networks that replaces the softmax output layer by a log-linear output layer , of which the softmax is a special case . this conceptually simple move has two main advantages . first , it allows the learner to combat training data sparsity by allowing it to model words ( or more generally , output symbols ) as complex combinations of attributes without requiring that each combination is directly observed in the training data ( as the softmax does ) . second , it permits the inclusion of flexible prior knowledge in the form of a priori specified modular features , where the neural network component learns to dynamically control the weights of a log-linear distribution exploiting these features . we conduct experiments in the domain of language modelling of french , that exploit morphological prior knowledge and show an important decrease in perplexity relative to a baseline rnn . we provide other motivating iillustrations , and finally argue that the log-linear and the neural-network components contribute complementary strengths to the ll-rnn : the ll aspect allows the model to incorporate rich prior knowledge , while the nn aspect , according to the `` representation learning '' paradigm , allows the model to discover novel combination of characteristics .

entanglement zoo i : foundational and structural aspects
we put forward a general classification for a structural description of the entanglement present in compound entities experimentally violating bell 's inequalities , making use of a new entanglement scheme that we developed recently . our scheme , although different from the traditional one , is completely compatible with standard quantum theory , and enables quantum modeling in complex hilbert space for different types of situations . namely , situations where entangled states and product measurements appear ( 'customary quantum modeling ' ) , and situations where states and measurements and evolutions between measurements are entangled ( 'nonlocal box modeling ' , 'nonlocal non-marginal box modeling ' ) . the role played by tsirelson 's bound and marginal distribution law is emphasized . specific quantum models are worked out in detail in complex hilbert space within this new entanglement scheme .

on integrating fuzzy knowledge using a novel evolutionary algorithm
fuzzy systems may be considered as knowledge-based systems that incorporates human knowledge into their knowledge base through fuzzy rules and fuzzy membership functions . the intent of this study is to present a fuzzy knowledge integration framework using a novel evolutionary strategy ( nes ) , which can simultaneously integrate multiple fuzzy rule sets and their membership function sets . the proposed approach consists of two phases : fuzzy knowledge encoding and fuzzy knowledge integration . four application domains , the hepatitis diagnosis , the sugarcane breeding prediction , iris plants classification , and tic-tac-toe endgame were used to show the performance ofthe proposed knowledge approach . results show that the fuzzy knowledge base derived using our approach performs better than genetic algorithm based approach .

general belief measures
probability measures by themselves , are known to be inappropriate for modeling the dynamics of plain belief and their excessively strong measurability constraints make them unsuitable for some representational tasks , e.g . in the context of firstorder knowledge . in this paper , we are therefore going to look for possible alternatives and extensions . we begin by delimiting the general area of interest , proposing a minimal list of assumptions to be satisfied by any reasonable quasi-probabilistic valuation concept . within this framework , we investigate two particularly interesting kinds of quasi-measures which are not or much less affected by the traditional problems . * ranking measures , which generalize spohn-type and possibility measures . * cumulative measures , which combine the probabilistic and the ranking philosophy , allowing thereby a fine-grained account of static and dynamic belief .

multi-target particle filtering for the probability hypothesis density
when tracking a large number of targets , it is often computationally expensive to represent the full joint distribution over target states . in cases where the targets move independently , each target can instead be tracked with a separate filter . however , this leads to a model-data association problem . another approach to solve the problem with computational complexity is to track only the first moment of the joint distribution , the probability hypothesis density ( phd ) . the integral of this distribution over any area s is the expected number of targets within s. since no record of object identity is kept , the model-data association problem is avoided . the contribution of this paper is a particle filter implementation of the phd filter mentioned above . this phd particle filter is applied to tracking of multiple vehicles in terrain , a non-linear tracking problem . experiments show that the filter can track a changing number of vehicles robustly , achieving near-real-time performance .

mtd ( f ) , a minimax algorithm faster than negascout
mtd ( f ) is a new minimax search algorithm , simpler and more efficient than previous algorithms . in tests with a number of tournament game playing programs for chess , checkers and othello it performed better , on average , than negascout/pvs ( the alphabeta variant used in practically all good chess , checkers , and othello programs ) . one of the strongest chess programs of the moment , mit 's parallel chess program cilkchess uses mtd ( f ) as its search algorithm , replacing negascout , which was used in starsocrates , the previous version of the program .

learning of behavior trees for autonomous agents
definition of an accurate system model for automated planner ( ap ) is often impractical , especially for real-world problems . conversely , off-the-shelf planners fail to scale up and are domain dependent . these drawbacks are inherited from conventional transition systems such as finite state machines ( fsms ) that describes the action-plan execution generated by the ap . on the other hand , behavior trees ( bts ) represent a valid alternative to fsms presenting many advantages in terms of modularity , reactiveness , scalability and domain-independence . in this paper , we propose a model-free ap framework using genetic programming ( gp ) to derive an optimal bt for an autonomous agent to achieve a given goal in unknown ( but fully observable ) environments . we illustrate the proposed framework using experiments conducted with an open source benchmark mario ai for automated generation of bts that can play the game character mario to complete a certain level at various levels of difficulty to include enemies and obstacles .

modeling of item-difficulty for ontology-based mcqs
multiple choice questions ( mcqs ) that can be generated from a domain ontology can significantly reduce human effort & time required for authoring & administering assessments in an e-learning environment . even though here are various methods for generating mcqs from ontologies , methods for determining the difficulty-levels of such mcqs are less explored . in this paper , we study various aspects and factors that are involved in determining the difficulty-score of an mcq , and propose an ontology-based model for the prediction . this model characterizes the difficulty values associated with the stem and choice set of the mcqs , and describes a measure which combines both the scores . further more , the notion of assigning difficultly-scores based on the skill level of the test taker is utilized for predicating difficulty-score of a stem . we studied the effectiveness of the predicted difficulty-scores with the help of a psychometric model from the item response theory , by involving real-students and domain experts . our results show that , the predicated difficulty-levels of the mcqs are having high correlation with their actual difficulty-levels .

on the workings of genetic algorithms : the genoclique fixing hypothesis
we recently reported that the simple genetic algorithm ( sga ) is capable of performing a remarkable form of sublinear computation which has a straightforward connection with the general problem of interacting attributes in data-mining . in this paper we explain how the sga can leverage this computational proficiency to perform efficient adaptation on a broad class of fitness functions . based on the relative ease with which a practical fitness function might belong to this broad class , we submit a new hypothesis about the workings of genetic algorithms . we explain why our hypothesis is superior to the building block hypothesis , and , by way of empirical validation , we present the results of an experiment in which the use of a simple mechanism called clamping dramatically improved the performance of an sga with uniform crossover on large , randomly generated instances of the max 3-sat problem .

a parallel corpus of python functions and documentation strings for automated code documentation and code generation
automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest . progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions , which tend to be small and constrained to specific domains . in this work we introduce a large and diverse parallel corpus of a hundred thousands python functions with their documentation strings ( `` docstrings '' ) generated by scraping open source repositories on github . we describe baseline results for the code documentation and code generation tasks obtained by neural machine translation . we also experiment with data augmentation techniques to further increase the amount of training data . we release our datasets and processing scripts in order to stimulate research in these areas .

a belief-function based decision support system
in this paper , we present a decision support system based on belief functions and the pignistic transformation . the system is an integration of an evidential system for belief function propagation and a valuation-based system for bayesian decision analysis . the two subsystems are connected through the pignistic transformation . the system takes as inputs the user 's `` gut feelings '' about a situation and suggests what , if any , are to be tested and in what order , and it does so with a user friendly interface .

2planning for contingencies : a decision-based approach
a fundamental assumption made by classical ai planners is that there is no uncertainty in the world : the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable . these planners can not therefore construct contingency plans , i.e. , plans in which different actions are performed in different circumstances . in this paper we discuss some issues that arise in the representation and construction of contingency plans and describe cassandra , a partial-order contingency planner . cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow . the decision-steps in a plan result in subgoals to acquire knowledge , which are planned for in the same way as any other subgoals . cassandra thus distinguishes the process of gathering information from the process of making decisions . the explicit representation of decisions in cassandra allows a coherent approach to the problems of contingent planning , and provides a solid base for extensions such as the use of different decision-making procedures .

exploiting functional dependence in bayesian network inference
we propose an efficient method for bayesian network inference in models with functional dependence . we generalize the multiplicative factorization method originally designed by takikawa and d ambrosio ( 1999 ) for models with independence of causal influence.using a hidden variable , we transform a probability potential into a product of two - dimensional potentials.the multiplicative factorization yields more efficient inference . for example , in junction tree propagation it helps to avoid large cliques . in order to keep potentials small , the number of states of the hidden variable should be minimized.we transform this problem into a combinatorial problem of minimal base in a particular space.we present an example of a computerized adaptive test , in which the factorization method is significantly more efficient than previous inference methods .

general video game ai : a multi-track framework for evaluating agents , games and content generation algorithms
general video game playing ( gvgp ) aims at designing an agent that is capable of playing multiple video games with no human intervention . in 2014 , the general video game ai ( gvgai ) competition framework was created and released with the purpose of providing researchers a common open-source and easy to use platform for testing their ai methods with potentially infinity of games created using video game description language ( vgdl ) . the framework has been expanded into several tracks during the last few years to meet the demand of different research directions . the agents are required to either play multiples unknown games with or without access to game simulations , or to design new game levels or rules . this survey paper presents the vgdl , the gvgai framework , existing tracks , and reviews the wide use of gvgai framework in research , education and competitions five years after its birth . a future plan of framework improvements is also described .

language asp { f } with arithmetic expressions and consistency-restoring rules
in this paper we continue the work on our extension of answer set programming by non-herbrand functions and add to the language support for arithmetic expressions and various inequality relations over non-herbrand functions , as well as consistency-restoring rules from cr-prolog . we demonstrate the use of this latest version of the language in the representation of important kinds of knowledge .

synergy of all-purpose static solver and temporal reasoning tools in dynamic integrated expert systems
the paper discusses scientific and technological problems of dynamic integrated expert systems development . extensions of problem-oriented methodology for dynamic integrated expert systems development are considered . attention is paid to the temporal knowledge representation and processing .

block-wise map inference for determinantal point processes with application to change-point detection
existing map inference algorithms for determinantal point processes ( dpps ) need to calculate determinants or conduct eigenvalue decomposition generally at the scale of the full kernel , which presents a great challenge for real-world applications . in this paper , we introduce a class of dpps , called bwdpps , that are characterized by an almost block diagonal kernel matrix and thus can allow efficient block-wise map inference . furthermore , bwdpps are successfully applied to address the difficulty of selecting change-points in the problem of change-point detection ( cpd ) , which results in a new bwdpp-based cpd method , named bwdppcpd . in bwdppcpd , a preliminary set of change-point candidates is first created based on existing well-studied metrics . then , these change-point candidates are treated as dpp items , and dpp-based subset selection is conducted to give the final estimate of the change-points that favours both quality and diversity . the effectiveness of bwdppcpd is demonstrated through extensive experiments on five real-world datasets .

a benchmark environment motivated by industrial control problems
in the research area of reinforcement learning ( rl ) , frequently novel and promising methods are developed and introduced to the rl community . however , although many researchers are keen to apply their methods on real-world problems , implementing such methods in real industry environments often is a frustrating and tedious process . generally , academic research groups have only limited access to real industrial data and applications . for this reason , new methods are usually developed , evaluated and compared by using artificial software benchmarks . on one hand , these benchmarks are designed to provide interpretable rl training scenarios and detailed insight into the learning process of the method on hand . on the other hand , they usually do not share much similarity with industrial real-world applications . for this reason we used our industry experience to design a benchmark which bridges the gap between freely available , documented , and motivated artificial benchmarks and properties of real industrial problems . the resulting industrial benchmark ( ib ) has been made publicly available to the rl community by publishing its java and python code , including an openai gym wrapper , on github . in this paper we motivate and describe in detail the ib 's dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems .

approximation complexity of maximum a posteriori inference in sum-product networks
we discuss the computational complexity of approximating maximum a posteriori inference in sum-product networks . we first show np-hardness in trees of height two by a reduction from maximum independent set ; this implies non-approximability within a sublinear factor . we show that this is a tight bound , as we can find an approximation within a linear factor in networks of height two . we then show that , in trees of height three , it is np-hard to approximate the problem within a factor $ 2^ { f ( n ) } $ for any sublinear function $ f $ of the size of the input $ n $ . again , this bound is tight , as we prove that the usual max-product algorithm finds ( in any network ) approximations within factor $ 2^ { c \cdot n } $ for some constant $ c < 1 $ . last , we present a simple algorithm , and show that it provably produces solutions at least as good as , and potentially much better than , the max-product algorithm . we empirically analyze the proposed algorithm against max-product using synthetic and realistic networks .

decentralized control of partially observable markov decision processes using belief space macro-actions
the focus of this paper is on solving multi-robot planning problems in continuous spaces with partial observability . decentralized partially observable markov decision processes ( dec-pomdps ) are general models for multi-robot coordination problems , but representing and solving dec-pomdps is often intractable for large problems . to allow for a high-level representation that is natural for multi-robot problems and scalable to large discrete and continuous problems , this paper extends the dec-pomdp model to the decentralized partially observable semi-markov decision process ( dec-posmdp ) . the dec-posmdp formulation allows asynchronous decision-making by the robots , which is crucial in multi-robot domains . we also present an algorithm for solving this dec-posmdp which is much more scalable than previous methods since it can incorporate closed-loop belief space macro-actions in planning . these macro-actions are automatically constructed to produce robust solutions . the proposed method 's performance is evaluated on a complex multi-robot package delivery problem under uncertainty , showing that our approach can naturally represent multi-robot problems and provide high-quality solutions for large-scale problems .

separate training for conditional random fields using co-occurrence rate factorization
the standard training method of conditional random fields ( crfs ) is very slow for large-scale applications . as an alternative , piecewise training divides the full graph into pieces , trains them independently , and combines the learned weights at test time . in this paper , we present \emph { separate } training for undirected models based on the novel co-occurrence rate factorization ( cr-f ) . separate training is a local training method . in contrast to memms , separate training is unaffected by the label bias problem . experiments show that separate training ( i ) is unaffected by the label bias problem ; ( ii ) reduces the training time from weeks to seconds ; and ( iii ) obtains competitive results to the standard and piecewise training on linear-chain crfs .

physics-guided neural networks ( pgnn ) : an application in lake temperature modeling
this paper introduces a novel framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery . this framework , termed as physics-guided neural network ( pgnn ) , leverages the output of physics-based model simulations along with observational features to generate predictions using a neural network architecture . further , this paper presents a novel framework for using physics-based loss functions in the learning objective of neural networks , to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set . we illustrate the effectiveness of pgnn for the problem of lake temperature modeling , where physical relationships between the temperature , density , and depth of water are used to design a physics-based loss function . by using scientific knowledge to guide the construction and learning of neural networks , we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results .

relaxation-based revision operators in description logics
as ontologies and description logics ( dls ) reach out to a broader audience , several reasoning services are developed in this context . belief revision is one of them , of prime importance when knowledge is prone to change and inconsistency . in this paper we address both the generalization of the well-known agm postulates , and the definition of concrete and well-founded revision operators in different dl families . we introduce a model-theoretic version of the agm postulates with a general definition of inconsistency , hence enlarging their scope to a wide family of non-classical logics , in particular negation-free dl families . we propose a general framework for defining revision operators based on the notion of relaxation , introduced recently for defining dissimilarity measures between dl concepts . a revision operator in this framework amounts to relax the set of models of the old belief until it reaches the sets of models of the new piece of knowledge . we demonstrate that such a relaxation-based revision operator defines a faithful assignment and satisfies the generalized agm postulates . another important contribution concerns the definition of several concrete relaxation operators suited to the syntax of some dls ( alc and its fragments el and elu ) .

deeppath : a reinforcement learning method for knowledge graph reasoning
we study the problem of learning to reason in large scale knowledge graphs ( kgs ) . more specifically , we describe a novel reinforcement learning framework for learning multi-hop relational paths : we use a policy-based agent with continuous states based on knowledge graph embeddings , which reasons in a kg vector space by sampling the most promising relation to extend its path . in contrast to prior work , our approach includes a reward function that takes the accuracy , diversity , and efficiency into consideration . experimentally , we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on freebase and never-ending language learning datasets .

generating explanations for biomedical queries
we introduce novel mathematical models and algorithms to generate ( shortest or k different ) explanations for biomedical queries , using answer set programming . we implement these algorithms and integrate them in bioquery-asp . we illustrate the usefulness of these methods with some complex biomedical queries related to drug discovery , over the biomedical knowledge resources pharmgkb , drugbank , biogrid , ctd , sider , disease ontology and orphadata . to appear in theory and practice of logic programming ( tplp ) .

an integrated classification model for financial data mining
nowadays , financial data analysis is becoming increasingly important in the business market . as companies collect more and more data from daily operations , they expect to extract useful knowledge from existing collected data to help make reasonable decisions for new customer requests , e.g . user credit category , churn analysis , real estate analysis , etc . financial institutes have applied different data mining techniques to enhance their business performance . however , simple ap-proach of these techniques could raise a performance issue . besides , there are very few general models for both understanding and forecasting different finan-cial fields . we present in this paper a new classification model for analyzing fi-nancial data . we also evaluate this model with different real-world data to show its performance .

the dynamic controllability of conditional stns with uncertainty
recent attempts to automate business processes and medical-treatment processes have uncovered the need for a formal framework that can accommodate not only temporal constraints , but also observations and actions with uncontrollable durations . to meet this need , this paper defines a conditional simple temporal network with uncertainty ( cstnu ) that combines the simple temporal constraints from a simple temporal network ( stn ) with the conditional nodes from a conditional simple temporal problem ( cstp ) and the contingent links from a simple temporal network with uncertainty ( stnu ) . a notion of dynamic controllability for a cstnu is defined that generalizes the dynamic consistency of a ctp and the dynamic controllability of an stnu . the paper also presents some sound constraint-propagation rules for dynamic controllability that are expected to form the backbone of a dynamic-controllability-checking algorithm for cstnus .

the shape of a benedictine monastery : the saintgall ontology
we present an owl 2 ontology representing the saint gall plan , one of the most ancient documents arrived intact to us , which describes the ideal model of a benedictine monastic complex that inspired the design of many european monasteries .

finite ltl synthesis is exptime-complete
ltl synthesis -- the construction of a function to satisfy a logical specification formulated in linear temporal logic -- is a 2exptime-complete problem with relevant applications in controller synthesis and a myriad of artificial intelligence applications . in this research note we consider de giacomo and vardi 's variant of the synthesis problem for ltl formulas interpreted over finite rather than infinite traces . rather surprisingly , given the existing claims on complexity , we establish that ltl synthesis is exptime-complete for the finite interpretation , and not 2exptime-complete as previously reported . our result coincides nicely with the planning perspective where non-deterministic planning with full observability is exptime-complete and partial observability increases the complexity to 2exptime-complete ; a recent related result for ltl synthesis shows that in the finite case with partial observability , the problem is 2exptime-complete .

feedback message passing for inference in gaussian graphical models
while loopy belief propagation ( lbp ) performs reasonably well for inference in some gaussian graphical models with cycles , its performance is unsatisfactory for many others . in particular for some models lbp does not converge , and in general when it does converge , the computed variances are incorrect ( except for cycle-free graphs for which belief propagation ( bp ) is non-iterative and exact ) . in this paper we propose { \em feedback message passing } ( fmp ) , a message-passing algorithm that makes use of a special set of vertices ( called a { \em feedback vertex set } or { \em fvs } ) whose removal results in a cycle-free graph . in fmp , standard bp is employed several times on the cycle-free subgraph excluding the fvs while a special message-passing scheme is used for the nodes in the fvs . the computational complexity of exact inference is $ o ( k^2n ) $ , where $ k $ is the number of feedback nodes , and $ n $ is the total number of nodes . when the size of the fvs is very large , fmp is intractable . hence we propose { \em approximate fmp } , where a pseudo-fvs is used instead of an fvs , and where inference in the non-cycle-free graph obtained by removing the pseudo-fvs is carried out approximately using lbp . we show that , when approximate fmp converges , it yields exact means and variances on the pseudo-fvs and exact means throughout the remainder of the graph . we also provide theoretical results on the convergence and accuracy of approximate fmp . in particular , we prove error bounds on variance computation . based on these theoretical results , we design efficient algorithms to select a pseudo-fvs of bounded size . the choice of the pseudo-fvs allows us to explicitly trade off between efficiency and accuracy . experimental results show that using a pseudo-fvs of size no larger than $ \log ( n ) $ , this procedure converges much more often , more quickly , and provides more accurate results than lbp on the entire graph .

causal inference by stochastic complexity
the algorithmic markov condition states that the most likely causal direction between two random variables x and y can be identified as that direction with the lowest kolmogorov complexity . due to the halting problem , however , this notion is not computable . we hence propose to do causal inference by stochastic complexity . that is , we propose to approximate kolmogorov complexity via the minimum description length ( mdl ) principle , using a score that is mini-max optimal with regard to the model class under consideration . this means that even in an adversarial setting , such as when the true distribution is not in this class , we still obtain the optimal encoding for the data relative to the class . we instantiate this framework , which we call cisc , for pairs of univariate discrete variables , using the class of multinomial distributions . experiments show that cisc is highly accurate on synthetic , benchmark , as well as real-world data , outperforming the state of the art by a margin , and scales extremely well with regard to sample and domain sizes .

alan turing and the `` hard '' and `` easy '' problem of cognition : doing and feeling
the `` easy '' problem of cognitive science is explaining how and why we can do what we can do . the `` hard '' problem is explaining how and why we feel . turing 's methodology for cognitive science ( the turing test ) is based on doing : design a model that can do anything a human can do , indistinguishably from a human , to a human , and you have explained cognition . searle has shown that the successful model can not be solely computational . sensory-motor robotic capacities are necessary to ground some , at least , of the model 's words , in what the robot can do with the things in the world that the words are about . but even grounding is not enough to guarantee that -- nor to explain how and why -- the model feels ( if it does ) . that problem is much harder to solve ( and perhaps insoluble ) .

are elephants bigger than butterflies ? reasoning about sizes of objects
human vision greatly benefits from the information about sizes of objects . the role of size in several visual reasoning tasks has been thoroughly explored in human perception and cognition . however , the impact of the information about sizes of objects is yet to be determined in ai . we postulate that this is mainly attributed to the lack of a comprehensive repository of size information . in this paper , we introduce a method to automatically infer object sizes , leveraging visual and textual information from web . by maximizing the joint likelihood of textual and visual observations , our method learns reliable relative size estimates , with no explicit human supervision . we introduce the relative size dataset and show that our method outperforms competitive textual and visual baselines in reasoning about size comparisons .

a context-theoretic framework for compositionality in distributional semantics
techniques in which words are represented as vectors have proved useful in many applications in computational linguistics , however there is currently no general semantic formalism for representing meaning in terms of vectors . we present a framework for natural language semantics in which words , phrases and sentences are all represented as vectors , based on a theoretical analysis which assumes that meaning is determined by context . in the theoretical analysis , we define a corpus model as a mathematical abstraction of a text corpus . the meaning of a string of words is assumed to be a vector representing the contexts in which it occurs in the corpus model . based on this assumption , we can show that the vector representations of words can be considered as elements of an algebra over a field . we note that in applications of vector spaces to representing meanings of words there is an underlying lattice structure ; we interpret the partial ordering of the lattice as describing entailment between meanings . we also define the context-theoretic probability of a string , and , based on this and the lattice structure , a degree of entailment between strings . we relate the framework to existing methods of composing vector-based representations of meaning , and show that our approach generalises many of these , including vector addition , component-wise multiplication , and the tensor product .

parametrizing filters of a cnn with a gan
it is commonly agreed that the use of relevant invariances as a good statistical bias is important in machine-learning . however , most approaches that explicitly incorporate invariances into a model architecture only make use of very simple transformations , such as translations and rotations . hence , there is a need for methods to model and extract richer transformations that capture much higher-level invariances . to that end , we introduce a tool allowing to parametrize the set of filters of a trained convolutional neural network with the latent space of a generative adversarial network . we then show that the method can capture highly non-linear invariances of the data by visualizing their effect in the data space .

memetic artificial bee colony algorithm for large-scale global optimization
memetic computation ( mc ) has emerged recently as a new paradigm of efficient algorithms for solving the hardest optimization problems . on the other hand , artificial bees colony ( abc ) algorithms demonstrate good performances when solving continuous and combinatorial optimization problems . this study tries to use these technologies under the same roof . as a result , a memetic abc ( mabc ) algorithm has been developed that is hybridized with two local search heuristics : the nelder-mead algorithm ( nma ) and the random walk with direction exploitation ( rwde ) . the former is attended more towards exploration , while the latter more towards exploitation of the search space . the stochastic adaptation rule was employed in order to control the balancing between exploration and exploitation . this mabc algorithm was applied to a special suite on large scale continuous global optimization at the 2012 ieee congress on evolutionary computation . the obtained results the mabc are comparable with the results of decc-g , decc-g* , and mlcc .

non-iterative label propagation on optimal leading forest
graph based semi-supervised learning ( gssl ) has intuitive representation and can be improved by exploiting the matrix calculation . however , it has to perform iterative optimization to achieve a preset objective , which usually leads to low efficiency . another inconvenience lying in gssl is that when new data come , the graph construction and the optimization have to be conducted all over again . we propose a sound assumption , arguing that : the neighboring data points are not in peer-to-peer relation , but in a partial-ordered relation induced by the local density and distance between the data ; and the label of a center can be regarded as the contribution of its followers . starting from the assumption , we develop a highly efficient non-iterative label propagation algorithm based on a novel data structure named as optimal leading forest ( lapoleaf ) . the major weaknesses of the traditional gssl are addressed by this study . we further scale lapoleaf to accommodate big data by utilizing block distance matrix technique , parallel computing , and locality-sensitive hashing ( lsh ) . experiments on large datasets have shown the promising results of the proposed methods .

on context-dependent clustering of bandits
we investigate a novel cluster-of-bandit algorithm cab for collaborative recommendation tasks that implements the underlying feedback sharing mechanism by estimating the neighborhood of users in a context-dependent manner . cab makes sharp departures from the state of the art by incorporating collaborative effects into inference as well as learning processes in a manner that seamlessly interleaving explore-exploit tradeoffs and collaborative steps . we prove regret bounds under various assumptions on the data , which exhibit a crisp dependence on the expected number of clusters over the users , a natural measure of the statistical difficulty of the learning task . experiments on production and real-world datasets show that cab offers significantly increased prediction performance against a representative pool of state-of-the-art methods .

enhancing qpns for trade-off resolution
qualitative probabilistic networks have been introduced as qualitative abstractions of bayesian belief networks . one of the major drawbacks of these qualitative networks is their coarse level of detail , which may lead to unresolved trade-offs during inference . we present an enhanced formalism for qualitative networks with a finer level of detail . an enhanced qualitative probabilistic network differs from a regular qualitative network in that it distinguishes between strong and weak influences . enhanced qualitative probabilistic networks are purely qualitative in nature , as regular qualitative networks are , yet allow for efficiently resolving trade-offs during inference .

transfer learning for music classification and regression tasks
in this paper , we present a transfer learning approach for music classification and regression tasks . we propose to use a pre-trained convnet feature , a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network . we show how this convnet feature can serve as general-purpose music representation . in the experiments , a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks . the convnet feature outperforms the baseline mfcc feature in all the considered tasks and several previous approaches that are aggregating mfccs as well as low- and high-level music features .

critical remarks on single link search in learning belief networks
in learning belief networks , the single link lookahead search is widely adopted to reduce the search space . we show that there exists a class of probabilistic domain models which displays a special pattern of dependency . we analyze the behavior of several learning algorithms using different scoring metrics such as the entropy , conditional independence , minimal description length and bayesian metrics . we demonstrate that single link lookahead search procedures ( employed in these algorithms ) can not learn these models correctly . thus , when the underlying domain model actually belongs to this class , the use of a single link search procedure will result in learning of an incorrect model . this may lead to inference errors when the model is used . our analysis suggests that if the prior knowledge about a domain does not rule out the possible existence of these models , a multi-link lookahead search or other heuristics should be used for the learning process .

active diagnosis via auc maximization : an efficient approach for multiple fault identification in large scale , noisy networks
the problem of active diagnosis arises in several applications such as disease diagnosis , and fault diagnosis in computer networks , where the goal is to rapidly identify the binary states of a set of objects ( e.g. , faulty or working ) by sequentially selecting , and observing , ( noisy ) responses to binary valued queries . current algorithms in this area rely on loopy belief propagation for active query selection . these algorithms have an exponential time complexity , making them slow and even intractable in large networks . we propose a rank-based greedy algorithm that sequentially chooses queries such that the area under the roc curve of the rank-based output is maximized . the auc criterion allows us to make a simplifying assumption that significantly reduces the complexity of active query selection ( from exponential to near quadratic ) , with little or no compromise on the performance quality .

centric selection : a way to tune the exploration/exploitation trade-off
in this paper , we study the exploration / exploitation trade-off in cellular genetic algorithms . we define a new selection scheme , the centric selection , which is tunable and allows controlling the selective pressure with a single parameter . the equilibrium model is used to study the influence of the centric selection on the selective pressure and a new model which takes into account problem dependent statistics and selective pressure in order to deal with the exploration / exploitation trade-off is proposed : the punctuated equilibria model . performances on the quadratic assignment problem and nk-landscapes put in evidence an optimal exploration / exploitation trade-off on both of the classes of problems . the punctuated equilibria model is used to explain these results .

temporal description logic for ontology-based data access ( extended version )
our aim is to investigate ontology-based data access over temporal data with validity time and ontologies capable of temporal conceptual modelling . to this end , we design a temporal description logic , tql , that extends the standard ontology language owl 2 ql , provides basic means for temporal conceptual modelling and ensures first-order rewritability of conjunctive queries for suitably defined data instances with validity time .

overcoming catastrophic forgetting with hard attention to the task
catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks . this problem remains a hurdle for artificial intelligence systems with sequential learning capabilities . in this paper , we propose a task-based hard attention mechanism that preserves previous tasks ' information without affecting the current task 's learning . a hard attention mask is learned concurrently to every task , through stochastic gradient descent , and previous masks are exploited to condition such learning . we show that the proposed mechanism is effective for reducing catastrophic forgetting , cutting current rates by 45 to 80 % . we also show that it is robust to different hyperparameter choices , and that it offers a number of monitoring capabilities . the approach features the possibility to control both the stability and compactness of the learned knowledge , which we believe makes it also attractive for online learning or network compression applications .

pspace reasoning for graded modal logics
we present a pspace algorithm that decides satisfiability of the graded modal logic gr ( k_r ) -- -a natural extension of propositional modal logic k_r by counting expressions -- -which plays an important role in the area of knowledge representation . the algorithm employs a tableaux approach and is the first known algorithm which meets the lower bound for the complexity of the problem . thus , we exactly fix the complexity of the problem and refute an exptime-hardness conjecture . we extend the results to the logic gr ( k_ ( r \cap i ) ) , which augments gr ( k_r ) with inverse relations and intersection of accessibility relations . this establishes a kind of `` theoretical benchmark '' that all algorithmic approaches can be measured against .

lifted variable elimination for probabilistic logic programming
lifted inference has been proposed for various probabilistic logical frameworks in order to compute the probability of queries in a time that depends on the size of the domains of the random variables rather than the number of instances . even if various authors have underlined its importance for probabilistic logic programming ( plp ) , lifted inference has been applied up to now only to relational languages outside of logic programming . in this paper we adapt generalized counting first order variable elimination ( gc-fove ) to the problem of computing the probability of queries to probabilistic logic programs under the distribution semantics . in particular , we extend the prolog factor language ( pfl ) to include two new types of factors that are needed for representing problog programs . these factors take into account the existing causal independence relationships among random variables and are managed by the extension to variable elimination proposed by zhang and poole for dealing with convergent variables and heterogeneous factors . two new operators are added to gc-fove for treating heterogeneous factors . the resulting algorithm , called lp $ ^2 $ for lifted probabilistic logic programming , has been implemented by modifying the pfl implementation of gc-fove and tested on three benchmarks for lifted inference . a comparison with pita and problog2 shows the potential of the approach .

a constructive semantic characterization of aggregates in asp
this technical note describes a monotone and continuous fixpoint operator to compute the answer sets of programs with aggregates . the fixpoint operator relies on the notion of aggregate solution . under certain conditions , this operator behaves identically to the three-valued immediate consequence operator $ \phi^ { aggr } _p $ for aggregate programs , independently proposed pelov et al . this operator allows us to closely tie the computational complexity of the answer set checking and answer sets existence problems to the cost of checking a solution of the aggregates in the program . finally , we relate the semantics described by the operator to other proposals for logic programming with aggregates . to appear in theory and practice of logic programming ( tplp ) .

query-based attention cnn for text similarity map
in this paper , we introduce query-based attention cnn ( qacnn ) for text similarity map , an end-to-end neural network for question answering . this network is composed of compare mechanism , two-staged cnn architecture with attention mechanism , and a prediction layer . first , the compare mechanism compares between the given passage , query , and multiple answer choices to build similarity maps . then , the two-staged cnn architecture extracts features through word-level and sentence-level . at the same time , attention mechanism helps cnn focus more on the important part of the passage based on the query information . finally , the prediction layer find out the most possible answer choice . we conduct this model on the movieqa dataset using plot synopses only , and achieve 79.99 % accuracy which is the state of the art on the dataset .

time and the prisoner 's dilemma
this paper examines the integration of computational complexity into game theoretic models . the example focused on is the prisoner 's dilemma , repeated for a finite length of time . we show that a minimal bound on the players ' computational ability is sufficient to enable cooperative behavior . in addition , a variant of the repeated prisoner 's dilemma game is suggested , in which players have the choice of opting out . this modification enriches the game and suggests dominance of cooperative strategies . competitive analysis is suggested as a tool for investigating sub-optimal ( but computationally tractable ) strategies and game theoretic models in general . using competitive analysis , it is shown that for bounded players , a sub-optimal strategy might be the optimal choice , given resource limitations .

item2vec : neural item embedding for collaborative filtering
many collaborative filtering ( cf ) algorithms are item-based in the sense that they analyze item-item relations in order to produce item similarities . recently , several works in the field of natural language processing ( nlp ) suggested to learn a latent representation of words using neural embedding algorithms . among them , the skip-gram with negative sampling ( sgns ) , also known as word2vec , was shown to provide state-of-the-art results on various linguistics tasks . in this paper , we show that item-based cf can be cast in the same framework of neural word embedding . inspired by sgns , we describe a method we name item2vec for item-based cf that produces embedding for items in a latent space . the method is capable of inferring item-item relations even when user information is not available . we present experimental results that demonstrate the effectiveness of the item2vec method and show it is competitive with svd .

now that i have a good theory of uncertainty , what else do i need ?
rather than discussing the isolated merits of a nominative theory of uncertainty , this paper focuses on a class of problems , referred to as dynamic classification problem ( dcp ) , which requires the integration of many theories , including a prescriptive theory of uncertainty . we start by analyzing the dynamic classification problem and by defining its induced requirements on a supporting ( plausible ) reasoning system . we provide a summary of the underlying theory ( based on the semantics of many-valed logics ) and illustrate the constraints imposed upon it to ensure the modularity and computational performance required by the applications . we describe the technologies used for knowledge engineering ( such as object-based simulator to exercise requirements , and development tools to build the knowledge base and functionally validate it ) . we emphasize the difference between development environment and run-time system , describe the rule cross-compiler , and the real-time inference engine with meta-reasoning capabilities . finally , we illustrate how our proposed technology satisfies the pop 's requirements and analyze some of the lessons reamed from its applications to situation assessment problems for pilot 's associate and submarine commander associate .

proceedings of the the first workshop on verification and validation of cyber-physical systems
the first international workshop on verification and validation of cyber-physical systems ( v2cps-16 ) was held in conjunction with the 12th international conference on integration of formal methods ( ifm 2016 ) in reykjavik , iceland . the purpose of v2cps-16 was to bring together researchers and experts of the fields of formal verification and cyber-physical systems ( cps ) to cover the theme of this workshop , namely a wide spectrum of verification and validation methods including ( but not limited to ) control , simulation , formal methods , etc . a cps is an integration of networked computational and physical processes with meaningful inter-effects ; the former monitors , controls , and affects the latter , while the latter also impacts the former . cpss have applications in a wide-range of systems spanning robotics , transportation , communication , infrastructure , energy , and manufacturing . many safety-critical systems such as chemical processes , medical devices , aircraft flight control , and automotive systems , are indeed cps . the advanced capabilities of cps require complex software and synthesis algorithms , which are hard to verify . in fact , many problems in this area are undecidable . thus , a major step is to find particular abstractions of such systems which might be algorithmically verifiable regarding specific properties of such systems , describing the partial/overall behaviors of cpss .

towards a systematic account of different semantics for logic programs
in [ hitzler and wendt 2002 , 2005 ] , a new methodology has been proposed which allows to derive uniform characterizations of different declarative semantics for logic programs with negation . one result from this work is that the well-founded semantics can formally be understood as a stratified version of the fitting ( or kripke-kleene ) semantics . the constructions leading to this result , however , show a certain asymmetry which is not readily understood . we will study this situation here with the result that we will obtain a coherent picture of relations between different semantics for normal logic programs .

proceedings of the twelfth conference on uncertainty in artificial intelligence ( 1996 )
this is the proceedings of the twelfth conference on uncertainty in artificial intelligence , which was held in portland , or , august 1-4 , 1996

chr ( prism ) -based probabilistic logic learning
prism is an extension of prolog with probabilistic predicates and built-in support for expectation-maximization learning . constraint handling rules ( chr ) is a high-level programming language based on multi-headed multiset rewrite rules . in this paper , we introduce a new probabilistic logic formalism , called chrism , based on a combination of chr and prism . it can be used for high-level rapid prototyping of complex statistical models by means of `` chance rules '' . the underlying prism system can then be used for several probabilistic inference tasks , including probability computation and parameter learning . we define the chrism language in terms of syntax and operational semantics , and illustrate it with examples . we define the notion of ambiguous programs and define a distribution semantics for unambiguous programs . next , we describe an implementation of chrism , based on chr ( prism ) . we discuss the relation between chrism and other probabilistic logic programming languages , in particular pchr . finally we identify potential application domains .

how ( not ) to train your generative model : scheduled sampling , likelihood , adversary ?
modern applications and progress in deep learning research have created renewed interest for generative models of text and of images . however , even today it is unclear what objective functions one should use to train and evaluate these models . in this paper we present two contributions . firstly , we present a critique of scheduled sampling , a state-of-the-art training method that contributed to the winning entry to the mscoco image captioning benchmark in 2015. here we show that despite this impressive empirical performance , the objective function underlying scheduled sampling is improper and leads to an inconsistent learning algorithm . secondly , we revisit the problems that scheduled sampling was meant to address , and present an alternative interpretation . we argue that maximum likelihood is an inappropriate training objective when the end-goal is to generate natural-looking samples . we go on to derive an ideal objective function to use in this situation instead . we introduce a generalisation of adversarial training , and show how such method can interpolate between maximum likelihood training and our ideal training objective . to our knowledge this is the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality .

analysis of supervised and semi-supervised growcut applied to segmentation of masses in mammography images
breast cancer is already one of the most common form of cancer worldwide . mammography image analysis is still the most effective diagnostic method to promote the early detection of breast cancer . accurately segmenting tumors in digital mammography images is important to improve diagnosis capabilities of health specialists and avoid misdiagnosis . in this work , we evaluate the feasibility of applying growcut to segment regions of tumor and we propose two growcut semi-supervised versions . all the analysis was performed by evaluating the application of segmentation techniques to a set of images obtained from the mini-mias mammography image database . growcut segmentation was compared to region growing , active contours , random walks and graph cut techniques . experiments showed that growcut , when compared to the other techniques , was able to acquire better results for the metrics analyzed . moreover , the proposed semi-supervised versions of growcut was proved to have a clinically satisfactory quality of segmentation .

a literature survey of benchmark functions for global optimization problems
test functions are important to validate and compare the performance of optimization algorithms . there have been many test or benchmark functions reported in the literature ; however , there is no standard list or set of benchmark functions . ideally , test functions should have diverse properties so that can be truly useful to test new algorithms in an unbiased way . for this purpose , we have reviewed and compiled a rich set of 175 benchmark functions for unconstrained optimization problems with diverse properties in terms of modality , separability , and valley landscape . this is by far the most complete set of functions so far in the literature , and tt can be expected this complete set of functions can be used for validation of new optimization in the future .

stable independance and complexity of representation
the representation of independence relations generally builds upon the well-known semigraphoid axioms of independence . recently , a representation has been proposed that captures a set of dominant statements of an independence relation from which any other statement can be generated by means of the axioms ; the cardinality of this set is taken to indicate the complexity of the relation . building upon the idea of dominance , we introduce the concept of stability to provide for a more compact representation of independence . we give an associated algorithm for establishing such a representation.we show that , with our concept of stability , many independence relations are found to be of lower complexity than with existing representations .

a knowledge mining model for ranking institutions using rough computing with ordering rules and formal concept analysis
emergences of computers and information technological revolution made tremendous changes in the real world and provides a different dimension for the intelligent data analysis . well formed fact , the information at right time and at right place deploy a better knowledge.however , the challenge arises when larger volume of inconsistent data is given for decision making and knowledge extraction . to handle such imprecise data certain mathematical tools of greater importance has developed by researches in recent past namely fuzzy set , intuitionistic fuzzy set , rough set , formal concept analysis and ordering rules . it is also observed that many information system contains numerical attribute values and therefore they are almost similar instead of exact similar . to handle such type of information system , in this paper we use two processes such as pre process and post process . in pre process we use rough set on intuitionistic fuzzy approximation space with ordering rules for finding the knowledge whereas in post process we use formal concept analysis to explore better knowledge and vital factors affecting decisions .

exploiting sparsity to build efficient kernel based collaborative filtering for top-n item recommendation
the increasing availability of implicit feedback datasets has raised the interest in developing effective collaborative filtering techniques able to deal asymmetrically with unambiguous positive feedback and ambiguous negative feedback . in this paper , we propose a principled kernel-based collaborative filtering method for top-n item recommendation with implicit feedback . we present an efficient implementation using the linear kernel , and we show how to generalize it to kernels of the dot product family preserving the efficiency . we also investigate on the elements which influence the sparsity of a standard cosine kernel . this analysis shows that the sparsity of the kernel strongly depends on the properties of the dataset , in particular on the long tail distribution . we compare our method with state-of-the-art algorithms achieving good results both in terms of efficiency and effectiveness .

users constraints in itemset mining
discovering significant itemsets is one of the fundamental problems in data mining . it has recently been shown that constraint programming is a flexible way to tackle data mining tasks . with a constraint programming approach , we can easily express and efficiently answer queries with users constraints on items . however , in many practical cases it is possible that queries also express users constraints on the dataset itself . for instance , asking for a particular itemset in a particular part of the dataset . this paper presents a general constraint programming model able to handle any kind of query on the items or the dataset for itemset mining .

predicate logic as a modeling language : modeling and solving some machine learning and data mining problems with idp3
this paper provides a gentle introduction to problem solving with the idp3 system . the core of idp3 is a finite model generator that supports first order logic enriched with types , inductive definitions , aggregates and partial functions . it offers its users a modeling language that is a slight extension of predicate logic and allows them to solve a wide range of search problems . apart from a small introductory example , applications are selected from problems that arose within machine learning and data mining research . these research areas have recently shown a strong interest in declarative modeling and constraint solving as opposed to algorithmic approaches . the paper illustrates that the idp3 system can be a valuable tool for researchers with such an interest . the first problem is in the domain of stemmatology , a domain of philology concerned with the relationship between surviving variant versions of text . the second problem is about a somewhat related problem within biology where phylogenetic trees are used to represent the evolution of species . the third and final problem concerns the classical problem of learning a minimal automaton consistent with a given set of strings . for this last problem , we show that the performance of our solution comes very close to that of a state-of-the art solution . for each of these applications , we analyze the problem , illustrate the development of a logic-based model and explore how alternatives can affect the performance .

combining two and three-way embeddings models for link prediction in knowledge bases
this paper tackles the problem of endogenous link prediction for knowledge base completion . knowledge bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships . previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns , which unfortunately usually end up overfitting on rare relationships , or in approaches that trade capacity for simplicity in order to fairly model all relationships , frequent or not . in this paper , we propose tatec a happy medium obtained by complementing a high-capacity model with a simpler one , both pre-trained separately and then combined . we present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature .

learning in riemannian orbifolds
learning in riemannian orbifolds is motivated by existing machine learning algorithms that directly operate on finite combinatorial structures such as point patterns , trees , and graphs . these methods , however , lack statistical justification . this contribution derives consistency results for learning problems in structured domains and thereby generalizes learning in vector spaces and manifolds .

patterns of social influence in a network of situated cognitive agents
this paper presents the results of computational experiments on the effects of social influence on individual and systemic behavior of situated cognitive agents in a product-consumer environment . paired experiments were performed with identical initial conditions to compare social agents with non- social agents . experiment results show that social agents are more productive in consuming available products , both in terms of aggregate unit consumption and aggregate utility . but this comes at a cost of individual average utility per unit consumed . in effect , social interaction achieved higher productivity by 'lowering the standards ' of individual consumers . while still at an early stage of development , such an agent-based model laboratory is shown to be an effective research tool to investigate rich collective behavior in the context of demanding cognitive tasks .

decompositions of grammar constraints
a wide range of constraints can be compactly specified using automata or formal languages . in a sequence of recent papers , we have shown that an effective means to reason with such specifications is to decompose them into primitive constraints . we can then , for instance , use state of the art sat solvers and profit from their advanced features like fast unit propagation , clause learning , and conflict-based search heuristics . this approach holds promise for solving combinatorial problems in scheduling , rostering , and configuration , as well as problems in more diverse areas like bioinformatics , software testing and natural language processing . in addition , decomposition may be an effective method to propagate other global constraints .

efficient architecture search by network transformation
techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results . however , their success is based on vast computational resources ( e.g . hundreds of gpus ) , making them difficult to be widely used . a noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space , which is highly inefficient . in this paper , we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights . we employ a reinforcement learning agent as the meta-controller , whose action is to grow the network depth or layer width with function-preserving transformations . as such , the previously validated networks can be reused for further exploration , thus saves a large amount of computational cost . we apply our method to explore the architecture space of the plain convolutional neural networks ( no skip-connections , branching etc . ) on image benchmark datasets ( cifar-10 , svhn ) with restricted computational resources ( 5 gpus ) . our method can design highly competitive networks that outperform existing networks using the same design scheme . on cifar-10 , our model without skip-connections achieves 4.23\ % test error rate , exceeding a vast majority of modern architectures and approaching densenet . furthermore , by applying our method to explore the densenet architecture space , we are able to achieve more accurate networks with fewer parameters .

credal model averaging for classification : representing prior ignorance and expert opinions
bayesian model averaging ( bma ) is the state of the art approach for overcoming model uncertainty . yet , especially on small data sets , the results yielded by bma might be sensitive to the prior over the models . credal model averaging ( cma ) addresses this problem by substituting the single prior over the models by a set of priors ( credal set ) . such approach solves the problem of how to choose the prior over the models and automates sensitivity analysis . we discuss various cma algorithms for building an ensemble of logistic regressors characterized by different sets of covariates . we show how cma can be appropriately tuned to the case in which one is prior-ignorant and to the case in which instead domain knowledge is available . cma detects prior-dependent instances , namely instances in which a different class is more probable depending on the prior over the models . on such instances cma suspends the judgment , returning multiple classes . we thoroughly compare different bma and cma variants on a real case study , predicting presence of alpine marmot burrows in an alpine valley . we find that bma is almost a random guesser on the instances recognized as prior-dependent by cma .

lloyd-topor completion and general stable models
we investigate the relationship between the generalization of program completion defined in 1984 by lloyd and topor and the generalization of the stable model semantics introduced recently by ferraris et al . the main theorem can be used to characterize , in some cases , the general stable models of a logic program by a first-order formula . the proof uses truszczynski 's stable model semantics of infinitary propositional formulas .

a deep reinforcement learning chatbot
we present milabot : a deep reinforcement learning chatbot developed by the montreal institute for learning algorithms ( mila ) for the amazon alexa prize competition . milabot is capable of conversing with humans on popular small talk topics through both speech and text . the system consists of an ensemble of natural language generation and retrieval models , including template-based models , bag-of-words models , sequence-to-sequence neural network and latent variable neural network models . by applying reinforcement learning to crowdsourced data and real-world user interactions , the system has been trained to select an appropriate response from the models in its ensemble . the system has been evaluated through a/b testing with real-world users , where it performed significantly better than many competing systems . due to its machine learning architecture , the system is likely to improve with additional data .

artificial agents and speculative bubbles
pertaining to agent-based computational economics ( ace ) , this work presents two models for the rise and downfall of speculative bubbles through an exchange price fixing based on double auction mechanisms . the first model is based on a finite time horizon context , where the expected dividends decrease along time . the second model follows the { \em greater fool } hypothesis ; the agent behaviour depends on the comparison of the estimated risk with the greater fool 's . simulations shed some light on the influent parameters and the necessary conditions for the apparition of speculative bubbles in an asset market within the considered framework .

formal model of uncertainty for possibilistic rules
given a universe of discourse x-a domain of possible outcomes-an experiment may consist of selecting one of its elements , subject to the operation of chance , or of observing the elements , subject to imprecision . a priori uncertainty about the actual result of the experiment may be quantified , representing either the likelihood of the choice of : r_x or the degree to which any such x would be suitable as a description of the outcome . the former case corresponds to a probability distribution , while the latter gives a possibility assignment on x. the study of such assignments and their properties falls within the purview of possibility theory [ dp88 , y80 , z783 . it , like probability theory , assigns values between 0 and 1 to express likelihoods of outcomes . here , however , the similarity ends . possibility theory uses the maximum and minimum functions to combine uncertainties , whereas probability theory uses the plus and times operations . this leads to very dissimilar theories in terms of analytical framework , even though they share several semantic concepts . one of the shared concepts consists of expressing quantitatively the uncertainty associated with a given distribution . in probability theory its value corresponds to the gain of information that would result from conducting an experiment and ascertaining an actual result . this gain of information can equally well be viewed as a decrease in uncertainty about the outcome of an experiment . in this case the standard measure of information , and thus uncertainty , is shannon entropy [ ad75 , g77 ] . it enjoys several advantages-it is characterized uniquely by a few , very natural properties , and it can be conveniently used in decision processes . this application is based on the principle of maximum entropy ; it has become a popular method of relating decisions to uncertainty . this paper demonstrates that an equally integrated theory can be built on the foundation of possibility theory . we first show how to define measures of in formation and uncertainty for possibility assignments . next we construct an information-based metric on the space of all possibility distributions defined on a given domain . it allows us to capture the notion of proximity in information content among the distributions . lastly , we show that all the above constructions can be carried out for continuous distributions-possibility assignments on arbitrary measurable domains . we consider this step very significant-finite domains of discourse are but approximations of the real-life infinite domains . if possibility theory is to represent real world situations , it must handle continuous distributions both directly and through finite approximations . in the last section we discuss a principle of maximum uncertainty for possibility distributions . we show how such a principle could be formalized as an inference rule . we also suggest it could be derived as a consequence of simple assumptions about combining information . we would like to mention that possibility assignments can be viewed as fuzzy sets and that every fuzzy set gives rise to an assignment of possibilities . this correspondence has far reaching consequences in logic and in control theory . our treatment here is independent of any special interpretation ; in particular we speak of possibility distributions and possibility measures , defining them as measurable mappings into the interval [ 0 , 1 ] . our presentation is intended as a self-contained , albeit terse summary . topics discussed were selected with care , to demonstrate both the completeness and a certain elegance of the theory . proofs are not included ; we only offer illustrative examples .

recommandation mobile , sensible au contexte de contenus évolutifs : contextuel-e-greedy
we introduce in this paper an algorithm named contextuel-e-greedy that tackles the dynamicity of the user 's content . it is based on dynamic exploration/exploitation tradeoff and can adaptively balance the two aspects by deciding which situation is most relevant for exploration or exploitation . the experimental results demonstrate that our algorithm outperforms surveyed algorithms .

modeling object oriented constraint programs in z
object oriented constraint programs ( oocps ) emerge as a leading evolution of constraint programming and artificial intelligence , first applied to a range of industrial applications called configuration problems . the rich variety of technical approaches to solving configuration problems ( clp ( fd ) , cc ( fd ) , dcsp , terminological systems , constraint programs with set variables ... ) is a source of difficulty . no universally accepted formal language exists for communicating about oocps , which makes the comparison of systems difficult . we present here a z based specification of oocps which avoids the falltrap of hidden object semantics . the object system is part of the specification , and captures all of the most advanced notions from the object oriented modeling standard uml . the paper illustrates these issues and the conciseness and precision of z by the specification of a working oocp that solves an historical ai problem : parsing a context free grammar . being written in z , an oocp specification also supports formal proofs . the whole builds the foundation of an adaptative and evolving framework for communicating about constrained object models and programs .

combination of upper and lower probabilities
in this paper , we consider several types of information and methods of combination associated with incomplete probabilistic systems . we discriminate between 'a priori ' and evidential information . the former one is a description of the whole population , the latest is a restriction based on observations for a particular case . then , we propose different combination methods for each one of them . we also consider conditioning as the heterogeneous combination of 'a priori ' and evidential information . the evidential information is represented as a convex set of likelihood functions . these will have an associated possibility distribution with behavior according to classical possibility theory .

a resolution prover for coalition logic
we present a prototype tool for automated reasoning for coalition logic , a non-normal modal logic that can be used for reasoning about cooperative agency . the theorem prover clprover is based on recent work on a resolution-based calculus for coalition logic that operates on coalition problems , a normal form for coalition logic . we provide an overview of coalition problems and of the resolution-based calculus for coalition logic . we then give details of the implementation of clprover and present the results for a comparison with an existing tableau-based solver .

beyond temporal pooling : recurrence and temporal convolutions for gesture recognition in video
recent studies have demonstrated the power of recurrent neural networks for machine translation , image captioning and speech recognition . for the task of capturing temporal structure in video , however , there still remain numerous open research questions . current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video . we demonstrate that this method is not sufficient for gesture recognition , where temporal information is more discriminative compared to general video classification tasks . we explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence . our main contributions are twofold ; first , we show that recurrence is crucial for this task ; second , we show that adding temporal convolutions leads to significant improvements . we evaluate the different approaches on the montalbano gesture recognition dataset , where we achieve state-of-the-art results .

truncating the loop series expansion for belief propagation
recently , m. chertkov and v.y . chernyak derived an exact expression for the partition sum ( normalization constant ) corresponding to a graphical model , which is an expansion around the belief propagation solution . by adding correction terms to the bp free energy , one for each `` generalized loop '' in the factor graph , the exact partition sum is obtained . however , the usually enormous number of generalized loops generally prohibits summation over all correction terms . in this article we introduce truncated loop series bp ( tlsbp ) , a particular way of truncating the loop series of m. chertkov and v.y . chernyak by considering generalized loops as compositions of simple loops . we analyze the performance of tlsbp in different scenarios , including the ising model , regular random graphs and on promedas , a large probabilistic medical diagnostic system . we show that tlsbp often improves upon the accuracy of the bp solution , at the expense of increased computation time . we also show that the performance of tlsbp strongly depends on the degree of interaction between the variables . for weak interactions , truncating the series leads to significant improvements , whereas for strong interactions it can be ineffective , even if a high number of terms is considered .

a theory of goal-oriented mdps with dead ends
stochastic shortest path ( ssp ) mdps is a problem class widely studied in ai , especially in probabilistic planning . they describe a wide range of scenarios but make the restrictive assumption that the goal is reachable from any state , i.e. , that dead-end states do not exist . because of this , ssps are unable to model various scenarios that may have catastrophic events ( e.g. , an airplane possibly crashing if it flies into a storm ) . even though mdp algorithms have been used for solving problems with dead ends , a principled theory of ssp extensions that would allow dead ends , including theoretically sound algorithms for solving such mdps , has been lacking . in this paper , we propose three new mdp classes that admit dead ends under increasingly weaker assumptions . we present value iteration-based as well as the more efficient heuristic search algorithms for optimally solving each class , and explore theoretical relationships between these classes . we also conduct a preliminary empirical study comparing the performance of our algorithms on different mdp classes , especially on scenarios with unavoidable dead ends .

document embedding with paragraph vectors
paragraph vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts . in their work , the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis . that proof of concept , while encouraging , was rather narrow . here we consider tasks other than sentiment analysis , provide a more thorough comparison of paragraph vectors to other document modelling algorithms such as latent dirichlet allocation , and evaluate performance of the method as we vary the dimensionality of the learned representation . we benchmarked the models on two document similarity data sets , one from wikipedia , one from arxiv . we observe that the paragraph vector method performs significantly better than other methods , and propose a simple improvement to enhance embedding quality . somewhat surprisingly , we also show that much like word embeddings , vector operations on paragraph vectors can perform useful semantic results .

conversation as action under uncertainty
conversations abound with uncetainties of various kinds . treating conversation as inference and decision making under uncertainty , we propose a task independent , multimodal architecture for supporting robust continuous spoken dialog called quartet . we introduce four interdependent levels of analysis , and describe representations , inference procedures , and decision strategies for managing uncertainties within and between the levels . we highlight the approach by reviewing interactions between a user and two spoken dialog systems developed using the quartet architecture : prsenter , a prototype system for navigating microsoft powerpoint presentations , and the bayesian receptionist , a prototype system for dealing with tasks typically handled by front desk receptionists at the microsoft corporate campus .

deep reinforcement learning for conversational ai
deep reinforcement learning is revolutionizing the artificial intelligence field . currently , it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world . it is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games . in this paper , key concepts of deep reinforcement learning including reward function , differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed . key challenges related to the implementation of reinforcement learning in conversational ai domain are identified as well as discussed in detail . various conversational models which are based on deep reinforcement learning ( as well as deep learning ) are also discussed . in summary , this paper discusses key aspects of deep reinforcement learning which are crucial for designing an efficient conversational ai .

optimistic concurrency control for distributed unsupervised learning
research on distributed machine learning algorithms has focused primarily on one of two extremes - algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints . we consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked . we view this `` optimistic concurrency control '' paradigm as particularly appropriate for large-scale machine learning algorithms , particularly in the unsupervised setting . we demonstrate our approach in three problem areas : clustering , feature learning and online facility location . we evaluate our methods via large-scale experiments in a cluster computing environment .

loopy belief propogation and gibbs measures
we address the question of convergence in the loopy belief propagation ( lbp ) algorithm . specifically , we relate convergence of lbp to the existence of a weak limit for a sequence of gibbs measures defined on the lbp s associated computation tree.using tools from the theory of gibbs measures we develop easily testable sufficient conditions for convergence.the failure of convergence of lbp implies the existence of multiple phases for the associated gibbs specification.these results give new insight into the mechanics of the algorithm .

learning conversational systems that interleave task and non-task content
task-oriented dialog systems have been applied in various tasks , such as automated personal assistants , customer service providers and tutors . these systems work well when users have clear and explicit intentions that are well-aligned to the systems ' capabilities . however , they fail if users intentions are not explicit . to address this shortcoming , we propose a framework to interleave non-task content ( i.e . everyday social conversation ) into task conversations . when the task content fails , the system can still keep the user engaged with the non-task content . we trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency , so that the system can have smooth transitions between task and non-task content . to test the effectiveness of the proposed framework , we developed a movie promotion dialog system . experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system .

an alternative softmax operator for reinforcement learning
a softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average . in sequential decision making , softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one 's weight behind a single maximum utility decision . the boltzmann softmax operator is the most commonly used softmax operator in this setting , but we show that this operator is prone to misbehavior . in this work , we study a differentiable softmax operator that , among other properties , is a non-expansion ensuring a convergent behavior in learning and planning . we introduce a variant of sarsa algorithm that , by utilizing the new operator , computes a boltzmann policy with a state-dependent temperature parameter . we show that the algorithm is convergent and that it performs favorably in practice .

handling pddl3.0 state trajectory constraints with temporal landmarks
temporal landmarks have been proved to be a helpful mechanism to deal with temporal planning problems , specifically to improve planners performance and handle problems with deadline constraints . in this paper , we show the strength of using temporal landmarks to handle the state trajectory constraints of pddl3.0 . we analyze the formalism of templm , a temporal planner particularly aimed at solving planning problems with deadlines , and we present a detailed study that exploits the underlying temporal landmark-based mechanism of templm for representing and reasoning with trajectory constraints .

bypassing captcha by machine a proof for passing the turing test
for the last ten years , captchas have been widely used by websites to prevent their data being automatically updated by machines . by supposedly allowing only humans to do so , captchas take advantage of the reverse turing test ( tt ) , knowing that humans are more intelligent than machines . generally , captchas have defeated machines , but things are changing rapidly as technology improves . hence , advanced research into optical character recognition ( ocr ) is overtaking attempts to strengthen captchas against machine-based attacks . this paper investigates the immunity of captcha , which was built on the failure of the tt . we show that some captchas are easily broken using a simple ocr machine built for the purpose of this study . by reviewing other techniques , we show that even more difficult captchas can be broken using advanced ocr machines . current advances in ocr should enable machines to pass the tt in the image recognition domain , which is exactly where machines are seeking to overcome captchas . we enhance traditional captchas by employing not only characters , but also natural language and multiple objects within the same captcha . the proposed captchas might be able to hold out against machines , at least until the advent of a machine that passes the tt completely .

towards information based spatiotemporal patterns as a foundation for agent representation in dynamical systems
we present some arguments why existing methods for representing agents fall short in applications crucial to artificial life . using a thought experiment involving a fictitious dynamical systems model of the biosphere we argue that the metabolism , motility , and the concept of counterfactual variation should be compatible with any agent representation in dynamical systems . we then propose an information-theoretic notion of \emph { integrated spatiotemporal patterns } which we believe can serve as the basic building block of an agent definition . we argue that these patterns are capable of solving the problems mentioned before . we also test this in some preliminary experiments .

the lovelace 2.0 test of artificial creativity and intelligence
observing that the creation of certain types of artistic artifacts necessitate intelligence , we present the lovelace 2.0 test of creativity as an alternative to the turing test as a means of determining whether an agent is intelligent . the lovelace 2.0 test builds off prior tests of creativity and additionally provides a means of directly comparing the relative intelligence of different agents .

survey of reasoning using neural networks
reason and inference require process as well as memory skills by humans . neural networks are able to process tasks like image recognition ( better than humans ) but in memory aspects are still limited ( by attention mechanism , size ) . recurrent neural network ( rnn ) and it 's modified version lstm are able to solve small memory contexts , but as context becomes larger than a threshold , it is difficult to use them . the solution is to use large external memory . still , it poses many challenges like , how to train neural networks for discrete memory representation , how to describe long term dependencies in sequential data etc . most prominent neural architectures for such tasks are memory networks : inference components combined with long term memory and neural turing machines : neural networks using external memory resources . also , additional techniques like attention mechanism , end to end gradient descent on discrete memory representation are needed to support these solutions . preliminary results of above neural architectures on simple algorithms ( sorting , copying ) and question answering ( based on story , dialogs ) application are comparable with the state of the art . in this paper , i explain these architectures ( in general ) , the additional techniques used and the results of their application .

relevant knowledge first - reinforcement learning and forgetting in knowledge based configuration
in order to solve complex configuration tasks in technical domains , various knowledge based methods have been developed . however their applicability is often unsuccessful due to their low efficiency . one of the reasons for this is that ( parts of the ) problems have to be solved again and again , instead of being `` learnt '' from preceding processes . however , learning processes bring with them the problem of conservatism , for in technical domains innovation is a deciding factor in competition . on the other hand a certain amount of conservatism is often desired since uncontrolled innovation as a rule is also detrimental . this paper proposes the heuristic rkf ( relevant knowledge first ) for making decisions in configuration processes based on the so-called relevance of objects in a knowledge base . the underlying relevance-function has two components , one based on reinforcement learning and the other based on forgetting ( fading ) . relevance of an object increases with its successful use and decreases with age when it is not used . rkf has been developed to speed up the configuration process and to improve the quality of the solutions relative to the reward value that is given by users .

learning semantic script knowledge with event embeddings
induction of common sense knowledge about prototypical sequences of events has recently received much attention . instead of inducing this knowledge in the form of graphs , as in much of the previous work , in our method , distributed representations of event realizations are computed based on distributed representations of predicates and their arguments , and then these representations are used to predict prototypical event orderings . the parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts . we show that this approach results in a substantial boost in ordering performance with respect to previous methods .

goal-driven query answering for existential rules with equality
inspired by the magic sets for datalog , we present a novel goal-driven approach for answering queries over terminating existential rules with equality ( aka tgds and egds ) . our technique improves the performance of query answering by pruning the consequences that are not relevant for the query . this is challenging in our setting because equalities can potentially affect all predicates in a dataset . we address this problem by combining the existing singularization technique with two new ingredients : an algorithm for identifying the rules relevant to a query and a new magic sets algorithm . we show empirically that our technique can significantly improve the performance of query answering , and that it can mean the difference between answering a query in a few seconds or not being able to process the query at all .

ai programmer : autonomously creating software programs using genetic algorithms
in this paper , we present the first-of-its-kind machine learning ( ml ) system , called ai programmer , that can automatically generate full software programs requiring only minimal human guidance . at its core , ai programmer uses genetic algorithms ( ga ) coupled with a tightly constrained programming language that minimizes the overhead of its ml search space . part of ai programmer 's novelty stems from ( i ) its unique system design , including an embedded , hand-crafted interpreter for efficiency and security and ( ii ) its augmentation of gas to include instruction-gene randomization bindings and programming language-specific genome construction and elimination techniques . we provide a detailed examination of ai programmer 's system design , several examples detailing how the system works , and experimental data demonstrating its software generation capabilities and performance using only mainstream cpus .

datalog rewritability of disjunctive datalog programs and its applications to ontology reasoning
we study the problem of rewriting a disjunctive datalog program into plain datalog . we show that a disjunctive program is rewritable if and only if it is equivalent to a linear disjunctive program , thus providing a novel characterisation of datalog rewritability . motivated by this result , we propose weakly linear disjunctive datalog -- -a novel rule-based kr language that extends both datalog and linear disjunctive datalog and for which reasoning is tractable in data complexity . we then explore applications of weakly linear programs to ontology reasoning and propose a tractable extension of owl 2 rl with disjunctive axioms . our empirical results suggest that many non-horn ontologies can be reduced to weakly linear programs and that query answering over such ontologies using a datalog engine is feasible in practice .

consistency management of normal logic program by top-down abductive proof procedure
this paper presents a method of computing a revision of a function-free normal logic program . if an added rule is inconsistent with a program , that is , if it leads to a situation such that no stable model exists for a new program , then deletion and addition of rules are performed to avoid inconsistency . we specify a revision by translating a normal logic program into an abductive logic program with abducibles to represent deletion and addition of rules . to compute such deletion and addition , we propose an adaptation of our top-down abductive proof procedure to compute a relevant abducibles to an added rule . we compute a minimally revised program , by choosing a minimal set of abducibles among all the sets of abducibles computed by a top-down proof procedure .

fourier policy gradients
we propose a new way of deriving policy gradient updates for reinforcement learning . our technique , based on fourier analysis , recasts integrals that arise with expected policy gradients as convolutions and turns them into multiplications . the obtained analytical solutions allow us to capture the low variance benefits of epg in a broad range of settings . for the critic , we treat trigonometric and radial basis functions , two function families with the universal approximation property . the choice of policy can be almost arbitrary , including mixtures or hybrid continuous-discrete probability distributions . moreover , we derive a general family of sample-based estimators for stochastic policy gradients , which unifies existing results on sample-based approximation . we believe that this technique has the potential to shape the next generation of policy gradient approaches , powered by analytical results .

simple regret optimization in online planning for markov decision processes
we consider online planning in markov decision processes ( mdps ) . in online planning , the agent focuses on its current state only , deliberates about the set of possible policies from that state onwards and , when interrupted , uses the outcome of that exploratory deliberation to choose what action to perform next . the performance of algorithms for online planning is assessed in terms of simple regret , which is the agent 's expected performance loss when the chosen action , rather than an optimal one , is followed . to date , state-of-the-art algorithms for online planning in general mdps are either best effort , or guarantee only polynomial-rate reduction of simple regret over time . here we introduce a new monte-carlo tree search algorithm , brue , that guarantees exponential-rate reduction of simple regret and error probability . this algorithm is based on a simple yet non-standard state-space sampling scheme , mcts2e , in which different parts of each sample are dedicated to different exploratory objectives . our empirical evaluation shows that brue not only provides superior performance guarantees , but is also very effective in practice and favorably compares to state-of-the-art . we then extend brue with a variant of `` learning by forgetting . '' the resulting set of algorithms , brue ( alpha ) , generalizes brue , improves the exponential factor in the upper bound on its reduction rate , and exhibits even more attractive empirical performance .

exploiting social annotation for automatic resource discovery
information integration applications , such as mediators or mashups , that require access to information resources currently rely on users manually discovering and integrating them in the application . manual resource discovery is a slow process , requiring the user to sift through results obtained via keyword-based search . although search methods have advanced to include evidence from document contents , its metadata and the contents and link structure of the referring pages , they still do not adequately cover information sources -- often called `` the hidden web '' -- that dynamically generate documents in response to a query . the recently popular social bookmarking sites , which allow users to annotate and share metadata about various information sources , provide rich evidence for resource discovery . in this paper , we describe a probabilistic model of the user annotation process in a social bookmarking system del.icio.us . we then use the model to automatically find resources relevant to a particular information domain . our experimental results on data obtained from \emph { del.icio.us } show this approach as a promising method for helping automate the resource discovery task .

fuzzy constraints linear discriminant analysis
in this paper we introduce a fuzzy constraint linear discriminant analysis ( fc-lda ) . the fc-lda tries to minimize misclassification error based on modified perceptron criterion that benefits handling the uncertainty near the decision boundary by means of a fuzzy linear programming approach with fuzzy resources . the method proposed has low computational complexity because of its linear characteristics and the ability to deal with noisy data with different degrees of tolerance . obtained results verify the success of the algorithm when dealing with different problems . comparing fc-lda and lda shows superiority in classification task .

tractable set constraints
many fundamental problems in artificial intelligence , knowledge representation , and verification involve reasoning about sets and relations between sets and can be modeled as set constraint satisfaction problems ( set csps ) . such problems are frequently intractable , but there are several important set csps that are known to be polynomial-time tractable . we introduce a large class of set csps that can be solved in quadratic time . our class , which we call ei , contains all previously known tractable set csps , but also some new ones that are of crucial importance for example in description logics . the class of ei set constraints has an elegant universal-algebraic characterization , which we use to show that every set constraint language that properly contains all ei set constraints already has a finite sublanguage with an np-hard constraint satisfaction problem .

measuring catastrophic forgetting in neural networks
deep neural networks are used in many state-of-the-art systems for machine perception . once a network is trained to do a specific task , e.g. , bird classification , it can not easily be trained to do new tasks , e.g. , incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition . when new tasks are added , typical deep neural networks are prone to catastrophically forgetting previous tasks . networks that are capable of assimilating new information incrementally , much like how humans form new memories over time , will be more efficient than re-training the model from scratch each time a new task needs to be learned . there have been multiple attempts to develop schemes that mitigate catastrophic forgetting , but these methods have not been directly compared , the tests used to evaluate them vary considerably , and these methods have only been evaluated on small-scale problems ( e.g. , mnist ) . in this paper , we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks : regularization , ensembling , rehearsal , dual-memory , and sparse-coding . our experiments on real-world images and sounds show that the mechanism ( s ) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used , but they all demonstrate that the catastrophic forgetting problem has yet to be solved .

vicious circle principle and logic programs with aggregates
the paper presents a knowledge representation language $ \mathcal { a } log $ which extends asp with aggregates . the goal is to have a language based on simple syntax and clear intuitive and mathematical semantics . we give some properties of $ \mathcal { a } log $ , an algorithm for computing its answer sets , and comparison with other approaches .

characterization of an inconsistency ranking for pairwise comparison matrices
pairwise comparisons between alternatives are a well-known method for measuring preferences of a decision-maker . since these often do not exhibit consistency , a number of inconsistency indices has been introduced in order to measure the deviation from this ideal case . we axiomatically characterize the inconsistency ranking induced by the koczkodaj inconsistency index : six independent properties are presented such that they determine a unique linear order on the set of all pairwise comparison matrices .

on sparse discretization for graphical games
this short paper concerns discretization schemes for representing and computing approximate nash equilibria , with emphasis on graphical games , but briefly touching on normal-form and poly-matrix games . the main technical contribution is a representation theorem that informally states that to account for every exact nash equilibrium using a nearby approximate nash equilibrium on a grid over mixed strategies , a uniform discretization size linear on the inverse of the approximation quality and natural game-representation parameters suffices . for graphical games , under natural conditions , the discretization is logarithmic in the game-representation size , a substantial improvement over the linear dependency previously required . the paper has five other objectives : ( 1 ) given the venue , to highlight the important , but often ignored , role that work on constraint networks in ai has in simplifying the derivation and analysis of algorithms for computing approximate nash equilibria ; ( 2 ) to summarize the state-of-the-art on computing approximate nash equilibria , with emphasis on relevance to graphical games ; ( 3 ) to help clarify the distinction between sparse-discretization and sparse-support techniques ; ( 4 ) to illustrate and advocate for the deliberate mathematical simplicity of the formal proof of the representation theorem ; and ( 5 ) to list and discuss important open problems , emphasizing graphical-game generalizations , which the ai community is most suitable to solve .

mixtures of gaussians and minimum relative entropy techniques for modeling continuous uncertainties
problems of probabilistic inference and decision making under uncertainty commonly involve continuous random variables . often these are discretized to a few points , to simplify assessments and computations . an alternative approximation is to fit analytically tractable continuous probability distributions . this approach has potential simplicity and accuracy advantages , especially if variables can be transformed first . this paper shows how a minimum relative entropy criterion can drive both transformation and fitting , illustrating with a power and logarithm family of transformations and mixtures of gaussian ( normal ) distributions , which allow use of efficient influence diagram methods . the fitting procedure in this case is the well-known em algorithm . the selection of the number of components in a fitted mixture distribution is automated with an objective that trades off accuracy and computational cost .

adaptive neural compilation
this paper proposes an adaptive neural-compilation framework to address the problem of efficient program learning . traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics . in contrast , our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution . our approach is inspired by the recent works on differentiable representations of programs . we show that it is possible to compile programs written in a low-level language to a differentiable representation . we also show how programs in this representation can be optimised to make them efficient on a target distribution of inputs . experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate .

learning hard alignments with variational inference
there has recently been significant interest in hard attention models for tasks such as object recognition , visual captioning and speech recognition . hard attention can offer benefits over soft attention such as decreased computational cost , but training hard attention models can be difficult because of the discrete latent variables they introduce . previous work used reinforce and q-learning to approach these issues , but those methods can provide high-variance gradient estimates and be slow to train . in this paper , we tackle the problem of learning hard attention for a sequential task using variational inference methods , specifically the recently introduced vimco and nvil . furthermore , we propose a novel baseline that adapts vimco to this setting . we demonstrate our method on a phoneme recognition task in clean and noisy environments and show that our method outperforms reinforce , with the difference being greater for a more complicated task .

the imprecisions of precision measures in process mining
in process mining , precision measures are used to quantify how much a process model overapproximates the behavior seen in an event log . although several measures have been proposed throughout the years , no research has been done to validate whether these measures achieve the intended aim of quantifying over-approximation in a consistent way for all models and logs . this paper fills this gap by postulating a number of axioms for quantifying precision consistently for any log and any model . further , we show through counter-examples that none of the existing measures consistently quantifies precision .

speeding up the convergence of value iteration in partially observable markov decision processes
partially observable markov decision processes ( pomdps ) have recently become popular among many ai researchers because they serve as a natural model for planning under uncertainty . value iteration is a well-known algorithm for finding optimal policies for pomdps . it typically takes a large number of iterations to converge . this paper proposes a method for accelerating the convergence of value iteration . the method has been evaluated on an array of benchmark problems and was found to be very effective : it enabled value iteration to converge after only a few iterations on all the test problems .

philosophy in the face of artificial intelligence
in this article , i discuss how the ai community views concerns about the emergence of superintelligent ai and related philosophical issues .

an investigation into mathematical programming for finite horizon decentralized pomdps
decentralized planning in uncertain environments is a complex task generally dealt with by using a decision-theoretic approach , mainly through the framework of decentralized partially observable markov decision processes ( dec-pomdps ) . although dec-pomdps are a general and powerful modeling tool , solving them is a task with an overwhelming complexity that can be doubly exponential . in this paper , we study an alternate formulation of dec-pomdps relying on a sequence-form representation of policies . from this formulation , we show how to derive mixed integer linear programming ( milp ) problems that , once solved , give exact optimal solutions to the dec-pomdps . we show that these milps can be derived either by using some combinatorial characteristics of the optimal solutions of the dec-pomdps or by using concepts borrowed from game theory . through an experimental validation on classical test problems from the dec-pomdp literature , we compare our approach to existing algorithms . results show that mathematical programming outperforms dynamic programming but is less efficient than forward search , except for some particular problems . the main contributions of this work are the use of mathematical programming for dec-pomdps and a better understanding of dec-pomdps and of their solutions . besides , we argue that our alternate representation of dec-pomdps could be helpful for designing novel algorithms looking for approximate solutions to dec-pomdps .

makespan optimal solving of cooperative path-finding via reductions to propositional satisfiability
the problem of makespan optimal solving of cooperative path finding ( cpf ) is addressed in this paper . the task in cpf is to relocate a group of agents in a non-colliding way so that each agent eventually reaches its goal location from the given initial location . the abstraction adopted in this work assumes that agents are discrete items moving in an undirected graph by traversing edges . makespan optimal solving of cpf means to generate solutions that are as short as possi-ble in terms of the total number of time steps required for the execution of the solution . we show that reducing cpf to propositional satisfiability ( sat ) represents a viable option for obtaining makespan optimal solutions . several encodings of cpf into propositional formulae are suggested and experimentally evaluated . the evaluation indicates that sat based cpf solving outperforms other makespan optimal methods significantly in highly constrained situations ( environments that are densely occupied by agents ) .

prosocial learning agents solve generalized stag hunts better than selfish ones
deep reinforcement learning has become an important paradigm for constructing agents that can enter complex multi-agent situations and improve their policies through experience . one commonly used technique is reactive training - applying standard rl methods while treating other agents as a part of the learner 's environment . it is known that in general-sum games reactive training can lead groups of agents to converge to inefficient outcomes . we focus on one such class of environments : stag hunt games . here agents either choose a risky cooperative policy ( which leads to high payoffs if both choose it but low payoffs to an agent who attempts it alone ) or a safe one ( which leads to a safe payoff no matter what ) . we ask how we can change the learning rule of a single agent to improve its outcomes in stag hunts that include other reactive learners . we extend existing work on reward-shaping in multi-agent reinforcement learning and show that that making a single agent prosocial , that is , making them care about the rewards of their partners can increase the probability that groups converge to good outcomes . thus , even if we control a single agent in a group making that agent prosocial can increase our agent 's long-run payoff . we show experimentally that this result carries over to a variety of more complex environments with stag hunt-like dynamics including ones where agents must learn from raw input pixels .

a constraint-directed local search approach to nurse rostering problems
in this paper , we investigate the hybridization of constraint programming and local search techniques within a large neighbourhood search scheme for solving highly constrained nurse rostering problems . as identified by the research , a crucial part of the large neighbourhood search is the selection of the fragment ( neighbourhood , i.e . the set of variables ) , to be relaxed and re-optimized iteratively . the success of the large neighbourhood search depends on the adequacy of this identified neighbourhood with regard to the problematic part of the solution assignment and the choice of the neighbourhood size . we investigate three strategies to choose the fragment of different sizes within the large neighbourhood search scheme . the first two strategies are tailored concerning the problem properties . the third strategy is more general , using the information of the cost from the soft constraint violations and their propagation as the indicator to choose the variables added into the fragment . the three strategies are analyzed and compared upon a benchmark nurse rostering problem . promising results demonstrate the possibility of future work in the hybrid approach .

question answering : from partitions to prolog
we implement groenendijk and stokhof 's partition semantics of questions in a simple question answering algorithm . the algorithm is sound , complete , and based on tableau theorem proving . the algorithm relies on a syntactic characterization of answerhood : any answer to a question is equivalent to some formula built up only from instances of the question . we prove this characterization by translating the logic of interrogation to classical predicate logic and applying craig 's interpolation theorem .

generalization in deep learning
with a direct analysis of neural networks , this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning . unlike previous bound-based theory , our main theory is quantitatively as tight as possible for every dataset individually , while producing qualitative insights competitively . our results give insight into why and how deep learning can generalize well , despite its large capacity , complexity , possible algorithmic instability , nonrobustness , and sharp minima , answering to an open question in the literature . we also discuss limitations of our results and propose additional open problems .

dynasp2.5 : dynamic programming on tree decompositions in action
a vibrant theoretical research area are efficient exact parameterized algorithms . very recent solving competitions such as the pace challenge show that there is also increasing practical interest in the parameterized algorithms community . an important research question is whether dedicated parameterized exact algorithms exhibit certain practical relevance and one can even beat well-established problem solvers . we consider the logic-based declarative modeling language and problem solving framework answer set programming ( asp ) . state-of-the-art asp solvers rely considerably on sat-based algorithms . an asp solver ( dynasp2 ) , which is based on a classical dynamic programming on tree decompositions , has been published very recently . unfortunately , dynasp2 can outperform modern asp solvers on programs of small treewidth only if the question of interest is to count the number of solutions . in this paper , we describe underlying concepts of our new implementation ( dynasp2.5 ) that shows competitive behavior to state-of-the-art asp solvers even for finding just one solution when solving problems as the steiner tree problem that have been modeled in asp on graphs with low treewidth . our implementation is based on a novel approach that we call multi-pass dynamic programming ( m-dpsinc ) .

framework for learning agents in quantum environments
in this paper we provide a broad framework for describing learning agents in general quantum environments . we analyze the types of classically specified environments which allow for quantum enhancements in learning , by contrasting environments to quantum oracles . we show that whether or not quantum improvements are at all possible depends on the internal structure of the quantum environment . if the environments are constructed and the internal structure is appropriately chosen , or if the agent has limited capacities to influence the internal states of the environment , we show that improvements in learning times are possible in a broad range of scenarios . such scenarios we call luck-favoring settings . the case of constructed environments is particularly relevant for the class of model-based learning agents , where our results imply a near-generic improvement .

reading comprehension using entity-based memory network
this paper introduces a novel neural network model for question answering , the \emph { entity-based memory network } . it enhances neural networks ' ability of representing and calculating information over a long period by keeping records of entities contained in text . the core component is a memory pool which comprises entities ' states . these entities ' states are continuously updated according to the input text . questions with regard to the input text are used to search the memory pool for related entities and answers are further predicted based on the states of retrieved entities . compared with previous memory network models , the proposed model is capable of handling fine-grained information and more sophisticated relations based on entities . we formulated several different tasks as question answering problems and tested the proposed model . experiments reported satisfying results .

ask , attend and answer : exploring question-guided spatial attention for visual question answering
we address the problem of visual question answering ( vqa ) , which requires joint image and language understanding to answer a question about a given photograph . recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem , but have failed to model spatial inference . to remedy this , we propose a model we call the spatial memory network and apply it to the vqa task . memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory . our spatial memory network stores neuron activations from different spatial regions of the image in its memory , and uses the question to choose relevant regions for computing the answer , a process of which constitutes a single `` hop '' in the network . we propose a novel spatial attention architecture that aligns words with image patches in the first hop , and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop . to better understand the inference process learned by the network , we design synthetic questions that specifically require spatial inference and visualize the attention weights . we evaluate our model on two published visual question answering datasets , daquar [ 1 ] and vqa [ 2 ] , and obtain improved results compared to a strong deep baseline model ( ibowimg ) which concatenates image and question features to predict the answer [ 3 ] .

how essential are unstructured clinical narratives and information fusion to clinical trial recruitment ?
electronic health records capture patient information using structured controlled vocabularies and unstructured narrative text . while structured data typically encodes lab values , encounters and medication lists , unstructured data captures the physician 's interpretation of the patient 's condition , prognosis , and response to therapeutic intervention . in this paper , we demonstrate that information extraction from unstructured clinical narratives is essential to most clinical applications . we perform an empirical study to validate the argument and show that structured data alone is insufficient in resolving eligibility criteria for recruiting patients onto clinical trials for chronic lymphocytic leukemia ( cll ) and prostate cancer . unstructured data is essential to solving 59 % of the cll trial criteria and 77 % of the prostate cancer trial criteria . more specifically , for resolving eligibility criteria with temporal constraints , we show the need for temporal reasoning and information integration with medical events within and across unstructured clinical narratives and structured data .

ambiguous language and differences in beliefs
standard models of multi-agent modal logic do not capture the fact that information is often ambiguous , and may be interpreted in different ways by different agents . we propose a framework that can model this , and consider different semantics that capture different assumptions about the agents ' beliefs regarding whether or not there is ambiguity . we consider the impact of ambiguity on a seminal result in economics : aumann 's result saying that agents with a common prior can not agree to disagree . this result is known not to hold if agents do not have a common prior ; we show that it also does not hold in the presence of ambiguity . we then consider the tradeoff between assuming a common interpretation ( i.e. , no ambiguity ) and a common prior ( i.e. , shared initial beliefs ) .

how not to evaluate your dialogue system : an empirical study of unsupervised evaluation metrics for dialogue response generation
we investigate evaluation metrics for dialogue response generation systems where supervised labels , such as task completion , are not available . recent works in response generation have adopted metrics from machine translation to compare a model 's generated response to a single target response . we show that these metrics correlate very weakly with human judgements in the non-technical twitter domain , and not at all in the technical ubuntu domain . we provide quantitative and qualitative results highlighting specific weaknesses in existing metrics , and provide recommendations for future development of better automatic evaluation metrics for dialogue systems .

bounds on sample size for policy evaluation in markov environments
reinforcement learning means finding the optimal course of action in markovian environments without knowledge of the environment 's dynamics . stochastic optimization algorithms used in the field rely on estimates of the value of a policy . typically , the value of a policy is estimated from results of simulating that very policy in the environment . this approach requires a large amount of simulation as different points in the policy space are considered . in this paper , we develop value estimators that utilize data gathered when using one policy to estimate the value of using another policy , resulting in much more data-efficient algorithms . we consider the question of accumulating a sufficient experience and give pac-style bounds .

a cyber science based ontology for artificial general intelligence containment
the development of artificial general intelligence is considered by many to be inevitable . what such intelligence does after becoming aware is not so certain . to that end , research suggests that the likelihood of artificial general intelligence becoming hostile to humans is significant enough to warrant inquiry into methods to limit such potential . thus , containment of artificial general intelligence is a timely and meaningful research topic . while there is limited research exploring possible containment strategies , such work is bounded by the underlying field the strategies draw upon . accordingly , we set out to construct an ontology to describe necessary elements in any future containment technology . using existing academic literature , we developed a single domain ontology containing five levels , 32 codes , and 32 associated descriptors . further , we constructed ontology diagrams to demonstrate intended relationships . we then identified humans , agi , and the cyber world as novel agent objects necessary for future containment activities . collectively , the work addresses three critical gaps : ( a ) identifying and arranging fundamental constructs ; ( b ) situating agi containment within cyber science ; and ( c ) developing scientific rigor within the field .

discrete route/trajectory decision making problems
the paper focuses on composite multistage decision making problems which are targeted to design a route/trajectory from an initial decision situation ( origin ) to goal ( destination ) decision situation ( s ) . automobile routing problem is considered as a basic physical metaphor . the problems are based on a discrete ( combinatorial ) operations/states design/solving space ( e.g. , digraph ) . the described types of discrete decision making problems can be considered as intelligent design of a route ( trajectory , strategy ) and can be used in many domains : ( a ) education ( planning of student educational trajectory ) , ( b ) medicine ( medical treatment ) , ( c ) economics ( trajectory of start-up development ) . several types of the route decision making problems are described : ( i ) basic route decision making , ( ii ) multi-goal route decision making , ( iii ) multi-route decision making , ( iv ) multi-route decision making with route/trajectory change ( s ) , ( v ) composite multi-route decision making ( solution is a composition of several routes/trajectories at several corresponding domains ) , and ( vi ) composite multi-route decision making with coordinated routes/trajectories . in addition , problems of modeling and building the design spaces are considered . numerical examples illustrate the suggested approach . three applications are considered : educational trajectory ( orienteering problem ) , plan of start-up company ( modular three-stage design ) , and plan of medical treatment ( planning over digraph with two-component vertices ) .

intrinsically motivated multimodal structure learning
we present a long-term intrinsically motivated structure learning method for modeling transition dynamics during controlled interactions between a robot and semi-permanent structures in the world . in particular , we discuss how partially-observable state is represented using distributions over a markovian state and build models of objects that predict how state distributions change in response to interactions with such objects . these structures serve as the basis for a number of possible future tasks defined as markov decision processes ( mdps ) . the approach is an example of a structure learning technique applied to a multimodal affordance representation that yields a population of forward models for use in planning . we evaluate the approach using experiments on a bimanual mobile manipulator ( ubot-6 ) that show the performance of model acquisition as the number of transition actions increases .

dating texts without explicit temporal cues
this paper tackles temporal resolution of documents , such as determining when a document is about or when it was written , based only on its text . we apply techniques from information retrieval that predict dates via language models over a discretized timeline . unlike most previous works , we rely { \it solely } on temporal cues implicit in the text . we consider both document-likelihood and divergence based techniques and several smoothing methods for both of them . our best model predicts the mid-point of individuals ' lives with a median of 22 and mean error of 36 years for wikipedia biographies from 3800 b.c . to the present day . we also show that this approach works well when training on such biographies and predicting dates both for non-biographical wikipedia pages about specific years ( 500 b.c . to 2010 a.d. ) and for publication dates of short stories ( 1798 to 2008 ) . together , our work shows that , even in absence of temporal extraction resources , it is possible to achieve remarkable temporal locality across a diverse set of texts .

history based coalition formation in hedonic context using trust
in this paper we address the problem of coalition formation in hedonic context . our modelling tries to be as realistic as possible . in previous models , once an agent joins a coalition it would not be able to leave the coalition and join the new one ; in this research we made it possible to leave a coalition but put some restrictions to control the behavior of agents . leaving or staying of an agent in a coalition will affect on the trust of the other agents included in this coalition . agents will use the trust values in computing the expected utility of coalitions . three different risk behaviors are introduced for agents that want to initiate a coalition . using these risk behaviors , some simulations are made and results are analyzed .

an associative memory for the on-line recognition and prediction of temporal sequences
this paper presents the design of an associative memory with feedback that is capable of on-line temporal sequence learning . a framework for on-line sequence learning has been proposed , and different sequence learning models have been analysed according to this framework . the network model is an associative memory with a separate store for the sequence context of a symbol . a sparse distributed memory is used to gain scalability . the context store combines the functionality of a neural layer with a shift register . the sensitivity of the machine to the sequence context is controllable , resulting in different characteristic behaviours . the model can store and predict on-line sequences of various types and length . numerical simulations on the model have been carried out to determine its properties .

hypertree decompositions and tractable queries
several important decision problems on conjunctive queries ( cqs ) are np-complete in general but become tractable , and actually highly parallelizable , if restricted to acyclic or nearly acyclic queries . examples are the evaluation of boolean cqs and query containment . these problems were shown tractable for conjunctive queries of bounded treewidth and of bounded degree of cyclicity . the so far most general concept of nearly acyclic queries was the notion of queries of bounded query-width introduced by chekuri and rajaraman ( 1997 ) . while cqs of bounded query width are tractable , it remained unclear whether such queries are efficiently recognizable . chekuri and rajaraman stated as an open problem whether for each constant k it can be determined in polynomial time if a query has query width less than or equal to k. we give a negative answer by proving this problem np-complete ( specifically , for k=4 ) . in order to circumvent this difficulty , we introduce the new concept of hypertree decomposition of a query and the corresponding notion of hypertree width . we prove : ( a ) for each k , the class of queries with query width bounded by k is properly contained in the class of queries whose hypertree width is bounded by k ; ( b ) unlike query width , constant hypertree-width is efficiently recognizable ; ( c ) boolean queries of constant hypertree width can be efficiently evaluated .

a parallel and efficient algorithm for learning to match
many tasks in data mining and related fields can be formalized as matching between objects in two heterogeneous domains , including collaborative filtering , link prediction , image tagging , and web search . machine learning techniques , referred to as learning-to-match in this paper , have been successfully applied to the problems . among them , a class of state-of-the-art methods , named feature-based matrix factorization , formalize the task as an extension to matrix factorization by incorporating auxiliary features into the model . unfortunately , making those algorithms scale to real world problems is challenging , and simple parallelization strategies fail due to the complex cross talking patterns between sub-tasks . in this paper , we tackle this challenge with a novel parallel and efficient algorithm for feature-based matrix factorization . our algorithm , based on coordinate descent , can easily handle hundreds of millions of instances and features on a single machine . the key recipe of this algorithm is an iterative relaxation of the objective to facilitate parallel updates of parameters , with guaranteed convergence on minimizing the original objective function . experimental results demonstrate that the proposed method is effective on a wide range of matching problems , with efficiency significantly improved upon the baselines while accuracy retained unchanged .

bayesian modeling of a human mmorpg player
this paper describes an application of bayesian programming to the control of an autonomous avatar in a multiplayer role-playing game ( the example is based on world of warcraft ) . we model a particular task , which consists of choosing what to do and to select which target in a situation where allies and foes are present . we explain the model in bayesian programming and show how we could learn the conditional probabilities from data gathered during human-played sessions .

resumevis : a visual analytics system to discover semantic information in semi-structured resume data
massive public resume data emerging on the www indicates individual-related characteristics in terms of profile and career experiences . resume analysis ( ra ) provides opportunities for many applications , such as talent seeking and evaluation . existing ra studies based on statistical analyzing have primarily focused on talent recruitment by identifying explicit attributes . however , they failed to discover the implicit semantic information , i.e. , individual career progress patterns and social-relations , which are vital to comprehensive understanding of career development . besides , how to visualize them for better human cognition is also challenging . to tackle these issues , we propose a visual analytics system resumevis to mine and visualize resume data . firstly , a text-mining based approach is presented to extract semantic information . then , a set of visualizations are devised to represent the semantic information in multiple perspectives . by interactive exploration on resumevis performed by domain experts , the following tasks can be accomplished : to trace individual career evolving trajectory ; to mine latent social-relations among individuals ; and to hold the full picture of massive resumes ' collective mobility . case studies with over 2500 online officer resumes demonstrate the effectiveness of our system . we provide a demonstration video .

leveraging unstructured data to detect emerging reliability issues
unstructured data refers to information that does not have a predefined data model or is not organized in a pre-defined manner . loosely speaking , unstructured data refers to text data that is generated by humans . in after-sales service businesses , there are two main sources of unstructured data : customer complaints , which generally describe symptoms , and technician comments , which outline diagnostics and treatment information . a legitimate customer complaint can eventually be tracked to a failure or a claim . however , there is a delay between the time of a customer complaint and the time of a failure or a claim . a proactive strategy aimed at analyzing customer complaints for symptoms can help service providers detect reliability problems in advance and initiate corrective actions such as recalls . this paper introduces essential text mining concepts in the context of reliability analysis and a method to detect emerging reliability issues . the application of the method is illustrated using a case study .

toward real-time decentralized reinforcement learning using finite support basis functions
this paper addresses the design and implementation of complex reinforcement learning ( rl ) behaviors where multi-dimensional action spaces are involved , as well as the need to execute the behaviors in real-time using robotic platforms with limited computational resources and training times . for this purpose , we propose the use of decentralized rl , in combination with finite support basis functions as alternatives to gaussian rbf , in order to alleviate the effects of the curse of dimensionality on the action and state spaces respectively , and to reduce the computation time . as testbed , a rl based controller for the in-walk kick in nao robots , a challenging and critical problem for soccer robotics , is used . the reported experiments show empirically that our solution saves up to 99.94 % of execution time and 98.82 % of memory consumption during execution , without diminishing performance compared to classical approaches .

graph2vec : learning distributed representations of graphs
recent works on representation learning for graph structured data predominantly focus on learning distributed representations of graph substructures such as nodes and subgraphs . however , many graph analytics tasks such as graph classification and clustering require representing entire graphs as fixed length feature vectors . while the aforementioned approaches are naturally unequipped to learn such representations , graph kernels remain as the most effective way of obtaining them . however , these graph kernels use handcrafted features ( e.g. , shortest paths , graphlets , etc . ) and hence are hampered by problems such as poor generalization . to address this limitation , in this work , we propose a neural embedding framework named graph2vec to learn data-driven distributed representations of arbitrary sized graphs . graph2vec 's embeddings are learnt in an unsupervised manner and are task agnostic . hence , they could be used for any downstream task such as graph classification , clustering and even seeding supervised representation learning approaches . our experiments on several benchmark and large real-world datasets show that graph2vec achieves significant improvements in classification and clustering accuracies over substructure representation learning approaches and are competitive with state-of-the-art graph kernels .

modeling belief in dynamic systems , part i : foundations
belief change is a fundamental problem in ai : agents constantly have to update their beliefs to accommodate new observations . in recent years , there has been much work on axiomatic characterizations of belief change . we claim that a better understanding of belief change can be gained from examining appropriate semantic models . in this paper we propose a general framework in which to model belief change . we begin by defining belief in terms of knowledge and plausibility : an agent believes p if he knows that p is more plausible than its negation . we then consider some properties defining the interaction between knowledge and plausibility , and show how these properties affect the properties of belief . in particular , we show that by assuming two of the most natural properties , belief becomes a kd45 operator . finally , we add time to the picture . this gives us a framework in which we can talk about knowledge , plausibility ( and hence belief ) , and time , which extends the framework of halpern and fagin for modeling knowledge in multi-agent systems . we then examine the problem of `` minimal change '' . this notion can be captured by using prior plausibilities , an analogue to prior probabilities , which can be updated by `` conditioning '' . we show by example that conditioning on a plausibility measure can capture many scenarios of interest . in a companion paper , we show how the two best-studied scenarios of belief change , belief revisionand belief update , fit into our framework .

embedding data within knowledge spaces
the promise of e-science will only be realized when data is discoverable , accessible , and comprehensible within distributed teams , across disciplines , and over the long-term -- without reliance on out-of-band ( non-digital ) means . we have developed the open-source tupelo semantic content management framework and are employing it to manage a wide range of e-science entities ( including data , documents , workflows , people , and projects ) and a broad range of metadata ( including provenance , social networks , geospatial relationships , temporal relations , and domain descriptions ) . tupelo couples the use of global identifiers and resource description framework ( rdf ) statements with an aggregatable content repository model to provide a unified space for securely managing distributed heterogeneous content and relationships .

the world as evolving information
this paper discusses the benefits of describing the world as information , especially in the study of the evolution of life and cognition . traditional studies encounter problems because it is difficult to describe life and cognition in terms of matter and energy , since their laws are valid only at the physical scale . however , if matter and energy , as well as life and cognition , are described in terms of information , evolution can be described consistently as information becoming more complex . the paper presents eight tentative laws of information , valid at multiple scales , which are generalizations of darwinian , cybernetic , thermodynamic , psychological , philosophical , and complexity principles . these are further used to discuss the notions of life , cognition and their evolution .

generating hard satisfiable formulas by hiding solutions deceptively
to test incomplete search algorithms for constraint satisfaction problems such as 3-sat , we need a source of hard , but satisfiable , benchmark instances . a simple way to do this is to choose a random truth assignment a , and then choose clauses randomly from among those satisfied by a. however , this method tends to produce easy problems , since the majority of literals point toward the `` hidden '' assignment a. last year , achlioptas , jia and moore proposed a problem generator that cancels this effect by hiding both a and its complement . while the resulting formulas appear to be just as hard for dpll algorithms as random 3-sat formulas with no hidden assignment , they can be solved by walksat in only polynomial time . here we propose a new method to cancel the attraction to a , by choosing a clause with t > 0 literals satisfied by a with probability proportional to q^t for some q < 1. by varying q , we can generate formulas whose variables have no bias , i.e. , which are equally likely to be true or false ; we can even cause the formula to `` deceptively '' point away from a. we present theoretical and experimental results suggesting that these formulas are exponentially hard both for dpll algorithms and for incomplete algorithms such as walksat .

applying policy iteration for training recurrent neural networks
recurrent neural networks are often used for learning time-series data . based on a few assumptions we model this learning task as a minimization problem of a nonlinear least-squares cost function . the special structure of the cost function allows us to build a connection to reinforcement learning . we exploit this connection and derive a convergent , policy iteration-based algorithm . furthermore , we argue that rnn training can be fit naturally into the reinforcement learning framework .

comparison of the bayesian and randomised decision tree ensembles within an uncertainty envelope technique
multiple classifier systems ( mcss ) allow evaluation of the uncertainty of classification outcomes that is of crucial importance for safety critical applications . the uncertainty of classification is determined by a trade-off between the amount of data available for training , the classifier diversity and the required performance . the interpretability of mcss can also give useful information for experts responsible for making reliable classifications . for this reason decision trees ( dts ) seem to be attractive classification models for experts . the required diversity of mcss exploiting such classification models can be achieved by using two techniques , the bayesian model averaging and the randomised dt ensemble . both techniques have revealed promising results when applied to real-world problems . in this paper we experimentally compare the classification uncertainty of the bayesian model averaging with a restarting strategy and the randomised dt ensemble on a synthetic dataset and some domain problems commonly used in the machine learning community . to make the bayesian dt averaging feasible , we use a markov chain monte carlo technique . the classification uncertainty is evaluated within an uncertainty envelope technique dealing with the class posterior distribution and a given confidence probability . exploring a full posterior distribution , this technique produces realistic estimates which can be easily interpreted in statistical terms . in our experiments we found out that the bayesian dts are superior to the randomised dt ensembles within the uncertainty envelope technique .

learning to cooperate via policy search
cooperative games are those in which both agents share the same payoff structure . value-based reinforcement-learning algorithms , such as variants of q-learning , have been applied to learning cooperative games , but they only apply when the game state is completely observable to both agents . policy search methods are a reasonable alternative to value-based methods for partially observable environments . in this paper , we provide a gradient-based distributed policy-search method for cooperative games and compare the notion of local optimum to that of nash equilibrium . we demonstrate the effectiveness of this method experimentally in a small , partially observable simulated soccer domain .

explaining adaptation in genetic algorithms with uniform crossover : the hyperclimbing hypothesis
the hyperclimbing hypothesis is a hypothetical explanation for adaptation in genetic algorithms with uniform crossover ( ugas ) . hyperclimbing is an intuitive , general-purpose , non-local search heuristic applicable to discrete product spaces with rugged or stochastic cost functions . the strength of this heuristic lie in its insusceptibility to local optima when the cost function is deterministic , and its tolerance for noise when the cost function is stochastic . hyperclimbing works by decimating a search space , i.e . by iteratively fixing the values of small numbers of variables . the hyperclimbing hypothesis holds that ugas work by implementing efficient hyperclimbing . proof of concept for this hypothesis comes from the use of a novel analytic technique involving the exploitation of algorithmic symmetry . we have also obtained experimental results that show that a simple tweak inspired by the hyperclimbing hypothesis dramatically improves the performance of a uga on large , random instances of max-3sat and the sherrington kirkpatrick spin glasses problem .

robust subspace outlier detection in high dimensional space
rare data in a large-scale database are called outliers that reveal significant information in the real world . the subspace-based outlier detection is regarded as a feasible approach in very high dimensional space . however , the outliers found in subspaces are only part of the true outliers in high dimensional space , indeed . the outliers hidden in normal-clustered points are sometimes neglected in the projected dimensional subspace . in this paper , we propose a robust subspace method for detecting such inner outliers in a given dataset , which uses two dimensional-projections : detecting outliers in subspaces with local density ratio in the first projected dimensions ; finding outliers by comparing neighbor 's positions in the second projected dimensions . each point 's weight is calculated by summing up all related values got in the two steps projected dimensions , and then the points scoring the largest weight values are taken as outliers . by taking a series of experiments with the number of dimensions from 10 to 10000 , the results show that our proposed method achieves high precision in the case of extremely high dimensional space , and works well in low dimensional space .

cognitive science in the era of artificial intelligence : a roadmap for reverse-engineering the infant language-learner
during their first years of life , infants learn the language ( s ) of their environment at an amazing speed despite large cross cultural variations in amount and complexity of the available language input . understanding this simple fact still escapes current cognitive and linguistic theories . recently , spectacular progress in the engineering science , notably , machine learning and wearable technology , offer the promise of revolutionizing the study of cognitive development . machine learning offers powerful learning algorithms that can achieve human-like performance on many linguistic tasks . wearable sensors can capture vast amounts of data , which enable the reconstruction of the sensory experience of infants in their natural environment . the project of 'reverse engineering ' language development , i.e. , of building an effective system that mimics infant 's achievements appears therefore to be within reach . here , we analyze the conditions under which such a project can contribute to our scientific understanding of early language development . we argue that instead of defining a sub-problem or simplifying the data , computational models should address the full complexity of the learning situation , and take as input the raw sensory signals available to infants . this implies that ( 1 ) accessible but privacy-preserving repositories of home data be setup and widely shared , and ( 2 ) models be evaluated at different linguistic levels through a benchmark of psycholinguist tests that can be passed by machines and humans alike , ( 3 ) linguistically and psychologically plausible learning architectures be scaled up to real data using probabilistic/optimization principles from machine learning . we discuss the feasibility of this approach and present preliminary results .

exploration of the scalability of locfaults approach for error localization with while-loops programs
a model checker can produce a trace of counterexample , for an erroneous program , which is often long and difficult to understand . in general , the part about the loops is the largest among the instructions in this trace . this makes the location of errors in loops critical , to analyze errors in the overall program . in this paper , we explore the scala-bility capabilities of locfaults , our error localization approach exploiting paths of cfg ( control flow graph ) from a counterexample to calculate the mcds ( minimal correction deviations ) , and mcss ( minimal correction subsets ) from each found mcd . we present the times of our approach on programs with while-loops unfolded b times , and a number of deviated conditions ranging from 0 to n. our preliminary results show that the times of our approach , constraint-based and flow-driven , are better compared to bugassist which is based on sat and transforms the entire program to a boolean formula , and further the information provided by locfaults is more expressive for the user .

the shape of art history in the eyes of the machine
how does the machine classify styles in art ? and how does it relate to art historians ' methods for analyzing style ? several studies have shown the ability of the machine to learn and predict style categories , such as renaissance , baroque , impressionism , etc. , from images of paintings . this implies that the machine can learn an internal representation encoding discriminative features through its visual analysis . however , such a representation is not necessarily interpretable . we conducted a comprehensive study of several of the state-of-the-art convolutional neural networks applied to the task of style classification on 77k images of paintings , and analyzed the learned representation through correlation analysis with concepts derived from art history . surprisingly , the networks could place the works of art in a smooth temporal arrangement mainly based on learning style labels , without any a priori knowledge of time of creation , the historical time and context of styles , or relations between styles . the learned representations showed that there are few underlying factors that explain the visual variations of style in art . some of these factors were found to correlate with style patterns suggested by heinrich w\ '' olfflin ( 1846-1945 ) . the learned representations also consistently highlighted certain artists as the extreme distinctive representative of their styles , which quantitatively confirms art historian observations .

artificial neoteny in evolutionary image segmentation
neoteny , also spelled paedomorphosis , can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life . in some species , all morphological development is retarded ; the organism is juvenilized but sexually mature . such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it . in terms of evolutionary theory , the process of paedomorphosis suggests that larval stages and developmental phases of existing organisms may give rise , under certain circumstances , to wholly new organisms . although the present work does not pretend to model or simulate the biological details of such a concept in any way , these ideas were incorporated by a rather simple abstract computational strategy , in order to allow ( if possible ) for faster convergence into simple non-memetic genetic algorithms , i.e . without using local improvement procedures ( e.g . via baldwin or lamarckian learning ) . as a case-study , the genetic algorithm was used for colour image segmentation purposes by using k-mean unsupervised clustering methods , namely for guiding the evolutionary algorithm in his search for finding the optimal or sub-optimal data partition . average results suggest that the use of neotonic strategies by employing juvenile genotypes into the later generations and the use of linear-dynamic mutation rates instead of constant , can increase fitness values by 58 % comparing to classical genetic algorithms , independently from the starting population characteristics on the search space . keywords : genetic algorithms , artificial neoteny , dynamic mutation rates , faster convergence , colour image segmentation , classification , clustering .

knowledge management for enterprises ( wissensmanagement fuer unternehmen )
although knowledge is one of the most valuable resource of enterprises and an important production and competition factor , this intellectual potential is often used ( or maintained ) only inadequate by the enterprises . therefore , in a globalised and growing market the optimal usage of existing knowledge represents a key factor for enterprises of the future . here , knowledge management systems should engage facilitating . because geographically far distributed establishments cause , however , a distributed system , this paper should uncover the spectrum connected with it and present a possible basic approach which is based on ontologies and modern , platform independent technologies . last but not least this attempt , as well as general questions of the knowledge management , are discussed .

learning bayesian networks with incomplete data by augmentation
we present new algorithms for learning bayesian networks from data with missing values using a data augmentation approach . an exact bayesian network learning algorithm is obtained by recasting the problem into a standard bayesian network learning problem without missing data . to the best of our knowledge , this is the first exact algorithm for this problem . as expected , the exact algorithm does not scale to large domains . we build on the exact method to create an approximate algorithm using a hill-climbing technique . this algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available . we perform a wide range of experiments to demonstrate the benefits of learning bayesian networks with such new approach .

characteristic matrix of covering and its application to boolean matrix decomposition and axiomatization
covering is an important type of data structure while covering-based rough sets provide an efficient and systematic theory to deal with covering data . in this paper , we use boolean matrices to represent and axiomatize three types of covering approximation operators . first , we define two types of characteristic matrices of a covering which are essentially square boolean ones , and their properties are studied . through the characteristic matrices , three important types of covering approximation operators are concisely equivalently represented . second , matrix representations of covering approximation operators are used in boolean matrix decomposition . we provide a sufficient and necessary condition for a square boolean matrix to decompose into the boolean product of another one and its transpose . and we develop an algorithm for this boolean matrix decomposition . finally , based on the above results , these three types of covering approximation operators are axiomatized using boolean matrices . in a word , this work borrows extensively from boolean matrices and present a new view to study covering-based rough sets .

prediction with expert advice in games with unbounded one-step gains
the games of prediction with expert advice are considered in this paper . we present some modification of kalai and vempala algorithm of following the perturbed leader for the case of unrestrictedly large one-step gains . we show that in general case the cumulative gain of any probabilistic prediction algorithm can be much worse than the gain of some expert of the pool . nevertheless , we give the lower bound for this cumulative gain in general case and construct a universal algorithm which has the optimal performance ; we also prove that in case when one-step gains of experts of the pool have `` limited deviations '' the performance of our algorithm is close to the performance of the best expert .

deepfacelift : interpretable personalized models for automatic estimation of self-reported pain
previous research on automatic pain estimation from facial expressions has focused primarily on `` one-size-fits-all '' metrics ( such as pspi ) . in this work , we focus on directly estimating each individual 's self-reported visual-analog scale ( vas ) pain metric , as this is considered the gold standard for pain measurement . the vas pain score is highly subjective and context-dependent , and its range can vary significantly among different persons . to tackle these issues , we propose a novel two-stage personalized model , named deepfacelift , for automatic estimation of vas . this model is based on ( 1 ) neural network and ( 2 ) gaussian process regression models , and is used to personalize the estimation of self-reported pain via a set of hand-crafted personal features and multi-task learning . we show on the benchmark dataset for pain analysis ( the unbc-mcmaster shoulder pain expression archive ) that the proposed personalized model largely outperforms the traditional , unpersonalized models : the intra-class correlation improves from a baseline performance of 19\ % to a personalized performance of 35\ % while also providing confidence in the model\textquotesingle s estimates -- in contrast to existing models for the target task . additionally , deepfacelift automatically discovers the pain-relevant facial regions for each person , allowing for an easy interpretation of the pain-related facial cues .

evolution of voronoi based fuzzy recurrent controllers
a fuzzy controller is usually designed by formulating the knowledge of a human expert into a set of linguistic variables and fuzzy rules . among the most successful methods to automate the fuzzy controllers development process are evolutionary algorithms . in this work , we propose the recurrent fuzzy voronoi ( rfv ) model , a representation for recurrent fuzzy systems . it is an extension of the fv model proposed by kavka and schoenauer that extends the application domain to include temporal problems . the fv model is a representation for fuzzy controllers based on voronoi diagrams that can represent fuzzy systems with synergistic rules , fulfilling the $ \epsilon $ -completeness property and providing a simple way to introduce a priory knowledge . in the proposed representation , the temporal relations are embedded by including internal units that provide feedback by connecting outputs to inputs . these internal units act as memory elements . in the rfv model , the semantic of the internal units can be specified together with the a priori rules . the geometric interpretation of the rules allows the use of geometric variational operators during the evolution . the representation and the algorithms are validated in two problems in the area of system identification and evolutionary robotics .

strong backdoors to bounded treewidth sat
there are various approaches to exploiting `` hidden structure '' in instances of hard combinatorial problems to allow faster algorithms than for general unstructured or random instances . for sat and its counting version # sat , hidden structure has been exploited in terms of decomposability and strong backdoor sets . decomposability can be considered in terms of the treewidth of a graph that is associated with the given cnf formula , for instance by considering clauses and variables as vertices of the graph , and making a variable adjacent with all the clauses it appears in . on the other hand , a strong backdoor set of a cnf formula is a set of variables such that each possible partial assignment to this set moves the formula into a fixed class for which ( # ) sat can be solved in polynomial time . in this paper we combine the two above approaches . in particular , we study the algorithmic question of finding a small strong backdoor set into the class w_t of cnf formulas whose associated graphs have treewidth at most t. the main results are positive : ( 1 ) there is a cubic-time algorithm that , given a cnf formula f and two constants k , t\ge 0 , either finds a strong w_t-backdoor set of size at most 2^k , or concludes that f has no strong w_t-backdoor set of size at most k. ( 2 ) there is a cubic-time algorithm that , given a cnf formula f , computes the number of satisfying assignments of f or concludes that sb_t ( f ) > k , for any pair of constants k , t\ge 0. here , sb_t ( f ) denotes the size of a smallest strong w_t-backdoor set of f. the significance of our results lies in the fact that they allow us to exploit algorithmically a hidden structure in formulas that is not accessible by any one of the two approaches ( decomposability , backdoors ) alone . already a backdoor size 1 on top of treewidth 1 ( i.e. , sb_1 ( f ) =1 ) entails formulas of arbitrarily large treewidth and arbitrarily large cycle cutsets .

large margin nearest neighbor embedding for knowledge representation
traditional way of storing facts in triplets ( { \it head\_entity , relation , tail\_entity } ) , abbreviated as ( { \it h , r , t } ) , makes the knowledge intuitively displayed and easily acquired by mankind , but hardly computed or even reasoned by ai machines . inspired by the success in applying { \it distributed representations } to ai-related fields , recent studies expect to represent each entity and relation with a unique low-dimensional embedding , which is different from the symbolic and atomic framework of displaying knowledge in triplets . in this way , the knowledge computing and reasoning can be essentially facilitated by means of a simple { \it vector calculation } , i.e . $ { \bf h } + { \bf r } \approx { \bf t } $ . we thus contribute an effective model to learn better embeddings satisfying the formula by pulling the positive tail entities $ { \bf t^ { + } } $ to get together and close to { \bf h } + { \bf r } ( { \it nearest neighbor } ) , and simultaneously pushing the negatives $ { \bf t^ { - } } $ away from the positives $ { \bf t^ { + } } $ via keeping a { \it large margin } . we also design a corresponding learning algorithm to efficiently find the optimal solution based on { \it stochastic gradient descent } in iterative fashion . quantitative experiments illustrate that our approach can achieve the state-of-the-art performance , compared with several latest methods on some benchmark datasets for two classical applications , i.e . { \it link prediction } and { \it triplet classification } . moreover , we analyze the parameter complexities among all the evaluated models , and analytical results indicate that our model needs fewer computational resources on outperforming the other methods .

principal manifolds and nonlinear dimension reduction via local tangent space alignment
nonlinear manifold learning from unorganized data points is a very challenging unsupervised learning and data visualization problem with a great variety of applications . in this paper we present a new algorithm for manifold learning and nonlinear dimension reduction . based on a set of unorganized data points sampled with noise from the manifold , we represent the local geometry of the manifold using tangent spaces learned by fitting an affine subspace in a neighborhood of each data point . those tangent spaces are aligned to give the internal global coordinates of the data points with respect to the underlying manifold by way of a partial eigendecomposition of the neighborhood connection matrix . we present a careful error analysis of our algorithm and show that the reconstruction errors are of second-order accuracy . we illustrate our algorithm using curves and surfaces both in 2d/3d and higher dimensional euclidean spaces , and 64-by-64 pixel face images with various pose and lighting conditions . we also address several theoretical and algorithmic issues for further research and improvements .

comparative study of financial time series prediction by artificial neural network with gradient descent learning
financial forecasting is an example of a signal processing problem which is challenging due to small sample sizes , high noise , non-stationarity , and non-linearity , but fast forecasting of stock market price is very important for strategic business planning.present study is aimed to develop a comparative predictive model with feedforward multilayer artificial neural network & recurrent time delay neural network for the financial timeseries prediction.this study is developed with the help of historical stockprice dataset made available by googlefinance.to develop this prediction model backpropagation method with gradient descent learning has been implemented.finally the neural net , learned with said algorithm is found to be skillful predictor for non-stationary noisy financial timeseries .

tweeting ai : perceptions of lay vs expert twitterati
with the recent advancements in artificial intelligence ( ai ) , various organizations and individuals are debating about the progress of ai as a blessing or a curse for the future of the society . this paper conducts an investigation on how the public perceives the progress of ai by utilizing the data shared on twitter . specifically , this paper performs a comparative analysis on the understanding of users belonging to two categories -- general ai-tweeters ( ait ) and expert ai-tweeters ( eait ) who share posts about ai on twitter . our analysis revealed that users from both the categories express distinct emotions and interests towards ai . users from both the categories regard ai as positive and are optimistic about the progress of ai but the experts are more negative than the general ai-tweeters . expert ai-tweeters share relatively large percentage of tweets about their personal news compared to technical aspects of ai . however , the effects of automation on the future are of primary concern to ait than to eait . when the expert category is sub-categorized , the emotion analysis revealed that students and industry professionals have more insights in their tweets about ai than academicians .

learning efficient convolutional networks through network slimming
the deployment of deep convolutional neural networks ( cnns ) in many real world applications is largely hindered by their high computational cost . in this paper , we propose a novel learning scheme for cnns to simultaneously 1 ) reduce the model size ; 2 ) decrease the run-time memory footprint ; and 3 ) lower the number of computing operations , without compromising accuracy . this is achieved by enforcing channel-level sparsity in the network in a simple but effective way . different from many existing approaches , the proposed method directly applies to modern cnn architectures , introduces minimum overhead to the training process , and requires no special software/hardware accelerators for the resulting models . we call our approach network slimming , which takes wide and large networks as input models , but during training insignificant channels are automatically identified and pruned afterwards , yielding thin and compact models with comparable accuracy . we empirically demonstrate the effectiveness of our approach with several state-of-the-art cnn models , including vggnet , resnet and densenet , on various image classification datasets . for vggnet , a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations .

learning graphical models from a distributed stream
a current challenge for data management systems is to support the construction and maintenance of machine learning models over data that is large , multi-dimensional , and evolving . while systems that could support these tasks are emerging , the need to scale to distributed , streaming data requires new models and algorithms . in this setting , as well as computational scalability and model accuracy , we also need to minimize the amount of communication between distributed processors , which is the chief component of latency . we study bayesian networks , the workhorse of graphical models , and present a communication-efficient method for continuously learning and maintaining a bayesian network model over data that is arriving as a distributed stream partitioned across multiple processors . we show a strategy for maintaining model parameters that leads to an exponential reduction in communication when compared with baseline approaches to maintain the exact mle ( maximum likelihood estimation ) . meanwhile , our strategy provides similar prediction errors for the target distribution and for classification tasks .

social networks and social information filtering on digg
the new social media sites -- blogs , wikis , flickr and digg , among others -- underscore the transformation of the web to a participatory medium in which users are actively creating , evaluating and distributing information . digg is a social news aggregator which allows users to submit links to , vote on and discuss news stories . each day digg selects a handful of stories to feature on its front page . rather than rely on the opinion of a few editors , digg aggregates opinions of thousands of its users to decide which stories to promote to the front page . digg users can designate other users as `` friends '' and easily track friends ' activities : what new stories they submitted , commented on or read . the friends interface acts as a \emph { social filtering } system , recommending to user stories his or her friends liked or found interesting . by tracking the votes received by newly submitted stories over time , we showed that social filtering is an effective information filtering approach . specifically , we showed that ( a ) users tend to like stories submitted by friends and ( b ) users tend to like stories their friends read and liked . as a byproduct of social filtering , social networks also play a role in promoting stories to digg 's front page , potentially leading to `` tyranny of the minority '' situation where a disproportionate number of front page stories comes from the same small group of interconnected users . despite this , social filtering is a promising new technology that can be used to personalize and tailor information to individual users : for example , through personal front pages .

combining ontologies with correspondences and link relations : the e-shiq representation framework
combining knowledge and beliefs of autonomous peers in distributed settings , is a ma- jor challenge . in this paper we consider peers that combine ontologies and reason jointly with their coupled knowledge . ontologies are within the shiq fragment of description logics . although there are several representation frameworks for modular description log- ics , each one makes crucial assumptions concerning the subjectivity of peers ' knowledge , the relation between the domains over which ontologies are interpreted , the expressivity of the constructors used for combining knowledge , and the way peers share their knowledge . however in settings where autonomous peers can evolve and extend their knowledge and beliefs independently from others , these assumptions may not hold . in this article , we moti- vate the need for a representation framework that allows peers to combine their knowledge in various ways , maintaining the subjectivity of their own knowledge and beliefs , and that reason collaboratively , constructing a tableau that is distributed among them , jointly . the paper presents the proposed e-shiq representation framework , the implementation of the e-shiq distributed tableau reasoner , and discusses the efficiency of this reasoner .

learning what to share between loosely related tasks
multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones . similarly , deep neural networks can profit from related tasks by sharing parameters with other networks . however , humans do not consciously decide to transfer knowledge between tasks . in natural language processing ( nlp ) , it is hard to predict if sharing will lead to improvements , particularly if tasks are only loosely related . to overcome this , we introduce sluice networks , a general framework for multi-task learning where trainable parameters control the amount of sharing . our framework generalizes previous proposals in enabling sharing of all combinations of subspaces , layers , and skip connections . we perform experiments on three task pairs , and across seven different domains , using data from ontonotes 5.0 , and achieve up to 15 % average error reductions over common approaches to multi-task learning . we show that a ) label entropy is predictive of gains in sluice networks , confirming findings for hard parameter sharing and b ) while sluice networks easily fit noise , they are robust across domains in practice .

semantic web today : from oil rigs to panama papers
the next leap on the internet has already started as semantic web . at its core , semantic web transforms the document oriented web to a data oriented web enriched with semantics embedded as metadata . this change in perspective towards the web offers numerous benefits for vast amount of data intensive industries that are bound to the web and its related applications . the industries are diverse as they range from oil & gas exploration to the investigative journalism , and everything in between . this paper discusses eight different industries which currently reap the benefits of semantic web . the paper also offers a future outlook into semantic web applications and discusses the areas in which semantic web would play a key role in the future .

object manipulation learning by imitation
we aim to enable robot to learn object manipulation by imitation . given external observations of demonstrations on object manipulations , we believe that two underlying problems to address in learning by imitation is 1 ) segment a given demonstration into skills that can be individually learned and reused , and 2 ) formulate the correct rl ( reinforcement learning ) problem that only considers the relevant aspects of each skill so that the policy for each skill can be effectively learned . previous works made certain progress in this direction , but none has taken private information into account . the public information is the information that is available in the external observations of demonstration , and the private information is the information that are only available to the agent that executes the actions , such as tactile sensations . our contribution is that we provide a method for the robot to automatically segment the demonstration of object manipulations into multiple skills , and formulate the correct rl problem for each skill , and automatically decide whether the private information is an important aspect of each skill based on interaction with the world . our experiment shows that our robot learns to pick up a block , and stack it onto another block by imitating an observed demonstration . the evaluation is based on 1 ) whether the demonstration is reasonably segmented , 2 ) whether the correct rl problems are formulated , 3 ) and whether a good policy is learned .

libtissue - implementing innate immunity
in a previous paper the authors argued the case for incorporating ideas from innate immunity into articficial immune systems ( aiss ) and presented an outline for a conceptual framework for such systems . a number of key general properties observed in the biological innate and adaptive immune systems were hughlighted , and how such properties might be instantiated in artificial systems was discussed in detail . the next logical step is to take these ideas and build a software system with which aiss with these properties can be implemented and experimentally evaluated . this paper reports on the results of that step - the libtissue system .

basic reasoning with tensor product representations
in this paper we present the initial development of a general theory for mapping inference in predicate logic to computation over tensor product representations ( tprs ; smolensky ( 1990 ) , smolensky & legendre ( 2006 ) ) . after an initial brief synopsis of tprs ( section 0 ) , we begin with particular examples of inference with tprs in the 'babi ' question-answering task of weston et al . ( 2015 ) ( section 1 ) . we then present a simplification of the general analysis that suffices for the babi task ( section 2 ) . finally , we lay out the general treatment of inference over tprs ( section 3 ) . we also show the simplification in section 2 derives the inference methods described in lee et al . ( 2016 ) ; this shows how the simple methods of lee et al . ( 2016 ) can be formally extended to more general reasoning tasks .

modeling concept combinations in a quantum-theoretic framework
we present modeling for conceptual combinations which uses the mathematical formalism of quantum theory . our model faithfully describes a large amount of experimental data collected by different scholars on concept conjunctions and disjunctions . furthermore , our approach sheds a new light on long standing drawbacks connected with vagueness , or fuzziness , of concepts , and puts forward a completely novel possible solution to the 'combination problem ' in concept theory . additionally , we introduce an explanation for the occurrence of quantum structures in the mechanisms and dynamics of concepts and , more generally , in cognitive and decision processes , according to which human thought is a well structured superposition of a 'logical thought ' and a 'conceptual thought ' , and the latter usually prevails over the former , at variance with some widespread beliefs

proceedings of the seventeenth conference on uncertainty in artificial intelligence ( 2001 )
this is the proceedings of the seventeenth conference on uncertainty in artificial intelligence , which was held in seattle , wa , august 2-5 2001

integration of knowledge to support automatic object reconstruction from images and 3d data
object reconstruction is an important task in many fields of application as it allows to generate digital representations of our physical world used as base for analysis , planning , construction , visualization or other aims . a reconstruction itself normally is based on reliable data ( images , 3d point clouds for example ) expressing the object in his complete extent . this data then has to be compiled and analyzed in order to extract all necessary geometrical elements , which represent the object and form a digital copy of it . traditional strategies are largely based on manual interaction and interpretation , because with increasing complexity of objects human understanding is inevitable to achieve acceptable and reliable results . but human interaction is time consuming and expensive , why many researches has already been invested to use algorithmic support , what allows to speed up the process and to reduce manual work load . presently most of such supporting algorithms are data-driven and concentate on specific features of the objects , being accessible to numerical models . by means of these models , which normally will represent geometrical ( flatness , roughness , for example ) or physical features ( color , texture ) , the data is classified and analyzed . this is successful for objects with low complexity , but gets to its limits with increasing complexness of objects . then purely numerical strategies are not able to sufficiently model the reality . therefore , the intention of our approach is to take human cognitive strategy as an example , and to simulate extraction processes based on available human defined knowledge for the objects of interest . such processes will introduce a semantic structure for the objects and guide the algorithms used to detect and recognize objects , which will yield a higher effectiveness . hence , our research proposes an approach using knowledge to guide the algorithms in 3d point cloud and image processing .

random forest models of the retention constants in the thin layer chromatography
in the current study we examine an application of the machine learning methods to model the retention constants in the thin layer chromatography ( tlc ) . this problem can be described with hundreds or even thousands of descriptors relevant to various molecular properties , most of them redundant and not relevant for the retention constant prediction . hence we employed feature selection to significantly reduce the number of attributes . additionally we have tested application of the bagging procedure to the feature selection . the random forest regression models were built using selected variables . the resulting models have better correlation with the experimental data than the reference models obtained with linear regression . the cross-validation confirms robustness of the models .

de-identification of patient notes with recurrent neural networks
objective : patient notes in electronic health records ( ehrs ) may contain critical information for medical investigations . however , the vast majority of medical investigators can only access de-identified notes , in order to protect the confidentiality of patients . in the united states , the health insurance portability and accountability act ( hipaa ) defines 18 types of protected health information ( phi ) that needs to be removed to de-identify patient notes . manual de-identification is impractical given the size of ehr databases , the limited number of researchers with access to the non-de-identified notes , and the frequent mistakes of human annotators . a reliable automated de-identification system would consequently be of high value . materials and methods : we introduce the first de-identification system based on artificial neural networks ( anns ) , which requires no handcrafted features or rules , unlike existing systems . we compare the performance of the system with state-of-the-art systems on two datasets : the i2b2 2014 de-identification challenge dataset , which is the largest publicly available de-identification dataset , and the mimic de-identification dataset , which we assembled and is twice as large as the i2b2 2014 dataset . results : our ann model outperforms the state-of-the-art systems . it yields an f1-score of 97.85 on the i2b2 2014 dataset , with a recall 97.38 and a precision of 97.32 , and an f1-score of 99.23 on the mimic de-identification dataset , with a recall 99.25 and a precision of 99.06. conclusion : our findings support the use of anns for de-identification of patient notes , as they show better performance than previously published systems while requiring no feature engineering .

how to avoid ethically relevant machine consciousness
this paper discusses the root cause of systems perceiving the self experience and how to exploit adaptive and learning features without introducing ethically problematic system properties .

sample complexity of learning mahalanobis distance metrics
metric learning seeks a transformation of the feature space that enhances prediction quality for the given task at hand . in this work we provide pac-style sample complexity rates for supervised metric learning . we give matching lower- and upper-bounds showing that the sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution . however , by leveraging the structure of the data distribution , we show that one can achieve rates that are fine-tuned to a specific notion of intrinsic complexity for a given dataset . our analysis reveals that augmenting the metric learning optimization criterion with a simple norm-based regularization can help adapt to a dataset 's intrinsic complexity , yielding better generalization . experiments on benchmark datasets validate our analysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise .

formal analysis of htm spatial pooler performance under predefined operation conditions
this paper introduces mathematical formalism for spatial ( sp ) of hierarchical temporal memory ( htm ) with a spacial consideration for its hardware implementation . performance of htm network and its ability to learn and adjust to a problem at hand is governed by a large set of parameters . most of parameters are codependent which makes creating efficient htm-based solutions challenging . it requires profound knowledge of the settings and their impact on the performance of system . consequently , this paper introduced a set of formulas which are to facilitate the design process by enhancing tedious trial-and-error method with a tool for choosing initial parameters which enable quick learning convergence . this is especially important in hardware implementations which are constrained by the limited resources of a platform . the authors focused especially on a formalism of spatial pooler and derive at the formulas for quality and convergence of the model . this may be considered as recipes for designing efficient htm models for given input patterns .

boosting applied to word sense disambiguation
in this paper schapire and singer 's adaboost.mh boosting algorithm is applied to the word sense disambiguation ( wsd ) problem . initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses naive bayes and exemplar-based approaches , which represent state-of-the-art accuracy on supervised wsd . in order to make boosting practical for a real learning domain of thousands of words , several ways of accelerating the algorithm by reducing the feature space are studied . the best variant , which we call lazyboosting , is tested on the largest sense-tagged corpus available containing 192,800 examples of the 191 most frequent and ambiguous english words . again , boosting compares favourably to the other benchmark algorithms .

extreme learning machine with local connections
this paper is concerned with the sparsification of the input-hidden weights of elm ( extreme learning machine ) . for ordinary feedforward neural networks , the sparsification is usually done by introducing certain regularization technique into the learning process of the network . but this strategy can not be applied for elm , since the input-hidden weights of elm are supposed to be randomly chosen rather than to be learned . to this end , we propose a modified elm , called elm-lc ( elm with local connections ) , which is designed for the sparsification of the input-hidden weights as follows : the hidden nodes and the input nodes are divided respectively into several corresponding groups , and an input node group is fully connected with its corresponding hidden node group , but is not connected with any other hidden node group . as in the usual elm , the hidden-input weights are randomly given , and the hidden-output weights are obtained through a least square learning . in the numerical simulations on some benchmark problems , the new elm-cl behaves better than the traditional elm .

algorithm portfolio design : theory vs. practice
stochastic algorithms are among the best for solving computationally hard search and reasoning problems . the runtime of such procedures is characterized by a random variable . different algorithms give rise to different probability distributions . one can take advantage of such differences by combining several algorithms into a portfolio , and running them in parallel or interleaving them on a single processor . we provide a detailed evaluation of the portfolio approach on distributions of hard combinatorial search problems . we show under what conditions the protfolio approach can have a dramatic computational advantage over the best traditional methods .

modeling of mixed decision making process
decision making whenever and wherever it is happened is key to organizations success . in order to make correct decision , individuals , teams and organizations need both knowledge management ( to manage content ) and collaboration ( to manage group processes ) to make that more effective and efficient . in this paper , we explain the knowledge management and collaboration convergence . then , we propose a formal description of mixed and multimodal decision making ( mdm ) process where decision may be made by three possible modes : individual , collective or hybrid . finally , we explicit the mdm process based on uml-g profile .

discovery radiomics via stochasticnet sequencers for cancer detection
radiomics has proven to be a powerful prognostic tool for cancer detection , and has previously been applied in lung , breast , prostate , and head-and-neck cancer studies with great success . however , these radiomics-driven methods rely on pre-defined , hand-crafted radiomic feature sets that can limit their ability to characterize unique cancer traits . in this study , we introduce a novel discovery radiomics framework where we directly discover custom radiomic features from the wealth of available medical imaging data . in particular , we leverage novel stochasticnet radiomic sequencers for extracting custom radiomic features tailored for characterizing unique cancer tissue phenotype . using stochasticnet radiomic sequencers discovered using a wealth of lung ct data , we perform binary classification on 42,340 lung lesions obtained from the ct scans of 93 patients in the lidc-idri dataset . preliminary results show significant improvement over previous state-of-the-art methods , indicating the potential of the proposed discovery radiomics framework for improving cancer screening and diagnosis .

temporal pattern mining from evolving networks
recently , evolving networks are becoming a suitable form to model many real-world complex systems , due to their peculiarities to represent the systems and their constituting entities , the interactions between the entities and the time-variability of their structure and properties . designing computational models able to analyze evolving networks becomes relevant in many applications . the goal of this research project is to evaluate the possible contribution of temporal pattern mining techniques in the analysis of evolving networks . in particular , we aim at exploiting available snapshots for the recognition of valuable and potentially useful knowledge about the temporal dynamics exhibited by the network over the time , without making any prior assumption about the underlying evolutionary schema . pattern-based approaches of temporal pattern mining can be exploited to detect and characterize changes exhibited by a network over the time , starting from observed snapshots .

human perception of performance
humans are routinely asked to evaluate the performance of other individuals , separating success from failure and affecting outcomes from science to education and sports . yet , in many contexts , the metrics driving the human evaluation process remain unclear . here we analyse a massive dataset capturing players ' evaluations by human judges to explore human perception of performance in soccer , the world 's most popular sport . we use machine learning to design an artificial judge which accurately reproduces human evaluation , allowing us to demonstrate how human observers are biased towards diverse contextual features . by investigating the structure of the artificial judge , we uncover the aspects of the players ' behavior which attract the attention of human judges , demonstrating that human evaluation is based on a noticeability heuristic where only feature values far from the norm are considered to rate an individual 's performance .

non-characterizability of belief revision : an application of finite model theory
a formal framework is given for the characterizability of a class of belief revision operators , defined using minimization over a class of partial preorders , by postulates . it is shown that for partial orders characterizability implies a definability property of the class of partial orders in monadic second-order logic . based on a non-definability result for a class of partial orders , an example is given of a non-characterizable class of revision operators . this appears to be the first non-characterizability result in belief revision .

bridging the gap between value and policy based reinforcement learning
we establish a new connection between value and policy based reinforcement learning ( rl ) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization . specifically , we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence , regardless of provenance . from this observation , we develop a new rl algorithm , path consistency learning ( pcl ) , that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces . we examine the behavior of pcl in different scenarios and show that pcl can be interpreted as generalizing both actor-critic and q-learning algorithms . we subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values , eliminating the need for a separate critic . the experimental evaluation demonstrates that pcl significantly outperforms strong actor-critic and q-learning baselines across several benchmarks .

image colour segmentation by genetic algorithms
segmentation of a colour image composed of different kinds of texture regions can be a hard problem , namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields . in this work , a method is described for evolving adaptive procedures for these problems . in many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be mapped into topological domains . we formulate the segmentation problem upon such images as an optimisation problem and adopt evolutionary strategy of genetic algorithms for the clustering of small regions in colour feature space . the present approach uses k-means unsupervised clustering methods into genetic algorithms , namely for guiding this last evolutionary algorithm in his search for finding the optimal or sub-optimal data partition , task that as we know , requires a non-trivial search because of its intrinsic np-complete nature . to solve this task , the appropriate genetic coding is also discussed , since this is a key aspect in the implementation . our purpose is to demonstrate the efficiency of genetic algorithms to automatic and unsupervised texture segmentation . some examples in colour maps , ornamental stones and in human skin mark segmentation are presented and overall results discussed . keywords : genetic algorithms , colour image segmentation , classification , clustering .

automatic knowledge base evolution by learning instances
knowledge base is the way to store structured and unstructured data throughout the web . since the size of the web is increasing rapidly , there are huge needs to structure the knowledge in a fully automated way . however fully-automated knowledge-base evolution on the semantic web is a major challenges , although there are many ontology evolution techniques available . therefore learning ontology automatically can contribute to the semantic web society significantly . in this paper , we propose full-automated ontology learning algorithm to generate refined knowledge base from incomplete knowledge base and rdf-triples . our algorithm is data-driven approach which is based on the property of each instance . ontology class is being elaborated by generalizing frequent property of its instances . by using that developed class information , each instance can find its most relatively matching class . by repeating these two steps , we achieve fully-automated ontology evolution from incomplete basic knowledge base .

an improved gauss-newtons method based back-propagation algorithm for fast convergence
the present work deals with an improved back-propagation algorithm based on gauss-newton numerical optimization method for fast convergence . the steepest descent method is used for the back-propagation . the algorithm is tested using various datasets and compared with the steepest descent back-propagation algorithm . in the system , optimization is carried out using multilayer neural network . the efficacy of the proposed method is observed during the training period as it converges quickly for the dataset used in test . the requirement of memory for computing the steps of algorithm is also analyzed .

weighted sets of probabilities and minimax weighted expected regret : new approaches for representing uncertainty and making decisions
we consider a setting where an agent 's uncertainty is represented by a set of probability measures , rather than a single measure . measure-by-measure updating of such a set of measures upon acquiring new information is well-known to suffer from problems ; agents are not always able to learn appropriately . to deal with these problems , we propose using weighted sets of probabilities : a representation where each measure is associated with a weight , which denotes its significance . we describe a natural approach to updating in such a situation and a natural approach to determining the weights . we then show how this representation can be used in decision-making , by modifying a standard approach to decision making -- minimizing expected regret -- to obtain minimax weighted expected regret ( mwer ) . we provide an axiomatization that characterizes preferences induced by mwer both in the static and dynamic case .

extended breadth-first search algorithm
the task of artificial intelligence is to provide representation techniques for describing problems , as well as search algorithms that can be used to answer our questions . a widespread and elaborated model is state-space representation , which , however , has some shortcomings . classical search algorithms are not applicable in practice when the state space contains even only a few tens of thousands of states . we can give remedy to this problem by defining some kind of heuristic knowledge . in case of classical state-space representation , heuristic must be defined so that it qualifies an arbitrary state based on its `` goodness , '' which is obviously not trivial . in our paper , we introduce an algorithm that gives us the ability to handle huge state spaces and to use a heuristic concept which is easier to embed into search algorithms .

detecting danger : applying a novel immunological concept to intrusion detection systems
in recent years computer systems have become increasingly complex and consequently the challenge of protecting these systems has become increasingly difficult . various techniques have been implemented to counteract the misuse of computer systems in the form of firewalls , anti-virus software and intrusion detection systems . the complexity of networks and dynamic nature of computer systems leaves current methods with significant room for improvement . computer scientists have recently drawn inspiration from mechanisms found in biological systems and , in the context of computer security , have focused on the human immune system ( his ) . the human immune system provides a high level of protection from constant attacks . by examining the precise mechanisms of the human immune system , it is hoped the paradigm will improve the performance of real intrusion detection systems . this paper presents an introduction to recent developments in the field of immunology . it discusses the incorporation of a novel immunological paradigm , danger theory , and how this concept is inspiring artificial immune systems ( ais ) . applications within the context of computer security are outlined drawing direct reference to the underlying principles of danger theory and finally , the current state of intrusion detection systems is discussed and improvements suggested .

generating retinal flow maps from structural optical coherence tomography with artificial intelligence
despite significant advances in artificial intelligence ( ai ) for computer vision , its application in medical imaging has been limited by the burden and limits of expert-generated labels . we used images from optical coherence tomography angiography ( octa ) , a relatively new imaging modality that measures perfusion of the retinal vasculature , to train an ai algorithm to generate vasculature maps from standard structural optical coherence tomography ( oct ) images of the same retinae , both exceeding the ability and bypassing the need for expert labeling . deep learning was able to infer perfusion of microvasculature from structural oct images with similar fidelity to octa and significantly better than expert clinicians ( p < 0.00001 ) . octa suffers from need of specialized hardware , laborious acquisition protocols , and motion artifacts ; whereas our model works directly from standard oct which are ubiquitous and quick to obtain , and allows unlocking of large volumes of previously collected standard oct data both in existing clinical trials and clinical practice . this finding demonstrates a novel application of ai to medical imaging , whereby subtle regularities between different modalities are used to image the same body part and ai is used to generate detailed and accurate inferences of tissue function from structure imaging .

hypothesis management in situation-specific network construction
this paper considers the problem of knowledge-based model construction in the presence of uncertainty about the association of domain entities to random variables . multi-entity bayesian networks ( mebns ) are defined as a representation for knowledge in domains characterized by uncertainty in the number of relevant entities , their interrelationships , and their association with observables . an mebn implicitly specifies a probability distribution in terms of a hierarchically structured collection of bayesian network fragments that together encode a joint probability distribution over arbitrarily many interrelated hypotheses . although a finite query-complete model can always be constructed , association uncertainty typically makes exact model construction and evaluation intractable . the objective of hypothesis management is to balance tractability against accuracy . we describe an application to the problem of using intelligence reports to infer the organization and activities of groups of military vehicles . our approach is compared to related work in the tracking and fusion literature .

what is an optimal diagnosis ?
within diagnostic reasoning there have been a number of proposed definitions of a diagnosis , and thus of the most likely diagnosis , including most probable posterior hypothesis , most probable interpretation , most probable covering hypothesis , etc . most of these approaches assume that the most likely diagnosis must be computed , and that a definition of what should be computed can be made a priori , independent of what the diagnosis is used for . we argue that the diagnostic problem , as currently posed , is incomplete : it does not consider how the diagnosis is to be used , or the utility associated with the treatment of the abnormalities . in this paper we analyze several well-known definitions of diagnosis , showing that the different definitions of the most likely diagnosis have different qualitative meanings , even given the same input data . we argue that the most appropriate definition of ( optimal ) diagnosis needs to take into account the utility of outcomes and what the diagnosis is used for .

generating plans that predict themselves
collaboration requires coordination , and we coordinate by anticipating our teammates ' future actions and adapting to their plan . in some cases , our teammates ' actions early on can give us a clear idea of what the remainder of their plan is , i.e . what action sequence we should expect . in others , they might leave us less confident , or even lead us to the wrong conclusion . our goal is for robot actions to fall in the first category : we want to enable robots to select their actions in such a way that human collaborators can easily use them to correctly anticipate what will follow . while previous work has focused on finding initial plans that convey a set goal , here we focus on finding two portions of a plan such that the initial portion conveys the final one . we introduce $ t $ -\acty { } : a measure that quantifies the accuracy and confidence with which human observers can predict the remaining robot plan from the overall task goal and the observed initial $ t $ actions in the plan . we contribute a method for generating $ t $ -predictable plans : we search for a full plan that accomplishes the task , but in which the first $ t $ actions make it as easy as possible to infer the remaining ones . the result is often different from the most efficient plan , in which the initial actions might leave a lot of ambiguity as to how the task will be completed . through an online experiment and an in-person user study with physical robots , we find that our approach outperforms a traditional efficiency-based planner in objective and subjective collaboration metrics .

redundant sudoku rules
the rules of sudoku are often specified using twenty seven \texttt { all\_different } constraints , referred to as the { \em big } \mrules . using graphical proofs and exploratory logic programming , the following main and new result is obtained : many subsets of six of these big \mrules are redundant ( i.e. , they are entailed by the remaining twenty one \mrules ) , and six is maximal ( i.e. , removing more than six \mrules is not possible while maintaining equivalence ) . the corresponding result for binary inequality constraints , referred to as the { \em small } \mrules , is stated as a conjecture .

granger-causal attentive mixtures of experts
several methods have recently been proposed to detect salient input features for outputs of neural networks . those methods offer a qualitative glimpse at feature importance , but they fall short of providing quantifiable attributions that can be compared across decisions and measures of the expected quality of their explanations . to address these shortcomings , we present an attentive mixture of experts ( ame ) that couples attentive gating with a granger-causal objective to jointly produce accurate predictions as well as measures of feature importance . we demonstrate the utility of ames by determining factors driving demand for medical prescriptions , comparing predictive features for parkinson 's disease and pinpointing discriminatory genes across cancer types .

decision-theoretic coordination and control for active multi-camera surveillance in uncertain , partially observable environments
a central problem of surveillance is to monitor multiple targets moving in a large-scale , obstacle-ridden environment with occlusions . this paper presents a novel principled partially observable markov decision process-based approach to coordinating and controlling a network of active cameras for tracking and observing multiple mobile targets at high resolution in such surveillance environments . our proposed approach is capable of ( a ) maintaining a belief over the targets ' states ( i.e. , locations , directions , and velocities ) to track them , even when they may not be observed directly by the cameras at all times , ( b ) coordinating the cameras ' actions to simultaneously improve the belief over the targets ' states and maximize the expected number of targets observed with a guaranteed resolution , and ( c ) exploiting the inherent structure of our surveillance problem to improve its scalability ( i.e. , linear time ) in the number of targets to be observed . quantitative comparisons with state-of-the-art multi-camera coordination and control techniques show that our approach can achieve higher surveillance quality in real time . the practical feasibility of our approach is also demonstrated using real axis 214 ptz cameras

interpretable and pedagogical examples
teachers intentionally pick the most informative examples to show their students . however , if the teacher and student are neural networks , the examples that the teacher network learns to give , although effective at teaching the student , are typically uninterpretable . we show that training the student and teacher iteratively , rather than jointly , can produce interpretable teaching strategies . we evaluate interpretability by ( 1 ) measuring the similarity of the teacher 's emergent strategies to intuitive strategies in each domain and ( 2 ) conducting human experiments to evaluate how effective the teacher 's strategies are at teaching humans . we show that the teacher network learns to select or generate interpretable , pedagogical examples to teach rule-based , probabilistic , boolean , and hierarchical concepts .

reinforcement-based simultaneous algorithm and its hyperparameters selection
many algorithms for data analysis exist , especially for classification problems . to solve a data analysis problem , a proper algorithm should be chosen , and also its hyperparameters should be selected . in this paper , we present a new method for the simultaneous selection of an algorithm and its hyperparameters . in order to do so , we reduced this problem to the multi-armed bandit problem . we consider an algorithm as an arm and algorithm hyperparameters search during a fixed time as the corresponding arm play . we also suggest a problem-specific reward function . we performed the experiments on 10 real datasets and compare the suggested method with the existing one implemented in auto-weka . the results show that our method is significantly better in most of the cases and never worse than the auto-weka .

on concise encodings of preferred extensions
much work on argument systems has focussed on preferred extensions which define the maximal collectively defensible subsets . identification and enumeration of these subsets is ( under the usual assumptions ) computationally demanding . we consider approaches to deciding if a subset s is a preferred extension which query a representations encoding all such extensions , so that the computational effort is invested once only ( for the initial enumeration ) rather than for each separate query .

diagnosability of fuzzy discrete event systems
in order to more effectively cope with the real-world problems of vagueness , { \it fuzzy discrete event systems } ( fdess ) were proposed recently , and the supervisory control theory of fdess was developed . in view of the importance of failure diagnosis , in this paper , we present an approach of the failure diagnosis in the framework of fdess . more specifically : ( 1 ) we formalize the definition of diagnosability for fdess , in which the observable set and failure set of events are { \it fuzzy } , that is , each event has certain degree to be observable and unobservable , and , also , each event may possess different possibility of failure occurring . ( 2 ) through the construction of observability-based diagnosers of fdess , we investigate its some basic properties . in particular , we present a necessary and sufficient condition for diagnosability of fdess . ( 3 ) some examples serving to illuminate the applications of the diagnosability of fdess are described . to conclude , some related issues are raised for further consideration .

a fast pc algorithm for high dimensional causal discovery with multi-core pcs
discovering causal relationships from observational data is a crucial problem and it has applications in many research areas . the pc algorithm is the state-of-the-art constraint based method for causal discovery . however , runtime of the pc algorithm , in the worst-case , is exponential to the number of nodes ( variables ) , and thus it is inefficient when being applied to high dimensional data , e.g . gene expression datasets . on another note , the advancement of computer hardware in the last decade has resulted in the widespread availability of multi-core personal computers . there is a significant motivation for designing a parallelised pc algorithm that is suitable for personal computers and does not require end users ' parallel computing knowledge beyond their competency in using the pc algorithm . in this paper , we develop parallel-pc , a fast and memory efficient pc algorithm using the parallel computing technique . we apply our method to a range of synthetic and real-world high dimensional datasets . experimental results on a dataset from the dream 5 challenge show that the original pc algorithm could not produce any results after running more than 24 hours ; meanwhile , our parallel-pc algorithm managed to finish within around 12 hours with a 4-core cpu computer , and less than 6 hours with a 8-core cpu computer . furthermore , we integrate parallel-pc into a causal inference method for inferring mirna-mrna regulatory relationships . the experimental results show that parallel-pc helps improve both the efficiency and accuracy of the causal inference algorithm .

improving naive bayes for regression with optimised artificial surrogate data
can we evolve better training data for machine learning algorithms ? to investigate this question we use population-based optimisation algorithms to generate artificial surrogate training data for naive bayes for regression . we demonstrate that the generalisation performance of naive bayes for regression models is enhanced by training them on the artificial data as opposed to the real data . these results are important for two reasons . firstly , naive bayes models are simple and interpretable but frequently underperform compared to more complex `` black box '' models , and therefore new methods of enhancing accuracy are called for . secondly , the idea of using the real training data indirectly in the construction of the artificial training data , as opposed to directly for model training , is a novel twist on the usual machine learning paradigm .

learning continuous user representations through hybrid filtering with doc2vec
players in the online ad ecosystem are struggling to acquire the user data required for precise targeting . audience look-alike modeling has the potential to alleviate this issue , but models ' performance strongly depends on quantity and quality of available data . in order to maximize the predictive performance of our look-alike modeling algorithms , we propose two novel hybrid filtering techniques that utilize the recent neural probabilistic language model algorithm doc2vec . we apply these methods to data from a large mobile ad exchange and additional app metadata acquired from the apple app store and google play store . first , we model mobile app users through their app usage histories and app descriptions ( user2vec ) . second , we introduce context awareness to that model by incorporating additional user and app-related metadata in model training ( context2vec ) . our findings are threefold : ( 1 ) the quality of recommendations provided by user2vec is notably higher than current state-of-the-art techniques . ( 2 ) user representations generated through hybrid filtering using doc2vec prove to be highly valuable features in supervised machine learning models for look-alike modeling . this represents the first application of hybrid filtering user models using neural probabilistic language models , specifically doc2vec , in look-alike modeling . ( 3 ) incorporating context metadata in the doc2vec model training process to introduce context awareness has positive effects on performance and is superior to directly including the data as features in the downstream supervised models .

optimistic simulated exploration as an incentive for real exploration
many reinforcement learning exploration techniques are overly optimistic and try to explore every state . such exploration is impossible in environments with the unlimited number of states . i propose to use simulated exploration with an optimistic model to discover promising paths for real exploration . this reduces the needs for the real exploration .

on-line condition monitoring using computational intelligence
this paper presents bushing condition monitoring frameworks that use multi-layer perceptrons ( mlp ) , radial basis functions ( rbf ) and support vector machines ( svm ) classifiers . the first level of the framework determines if the bushing is faulty or not while the second level determines the type of fault . the diagnostic gases in the bushings are analyzed using the dissolve gas analysis . mlp gives superior performance in terms of accuracy and training time than svm and rbf . in addition , an on-line bushing condition monitoring approach , which is able to adapt to newly acquired data are introduced . this approach is able to accommodate new classes that are introduced by incoming data and is implemented using an incremental learning algorithm that uses mlp . the testing results improved from 67.5 % to 95.8 % as new data were introduced and the testing results improved from 60 % to 95.3 % as new conditions were introduced . on average the confidence value of the framework on its decision was 0.92 .

ray rllib : a composable and scalable reinforcement learning library
reinforcement learning ( rl ) algorithms involve the deep nesting of distinct components , where each component typically exhibits opportunities for distributed computation . current rl libraries offer parallelism at the level of the entire program , coupling all the components together and making existing implementations difficult to extend , combine , and reuse . we argue for building composable rl components by encapsulating parallelism and resource requirements within individual components , which can be achieved by building on top of a flexible task-based programming model . we demonstrate this principle by building ray rllib on top of ray and show that we can implement a wide range of state-of-the-art algorithms by composing and reusing a handful of standard components . this composability does not come at the cost of performance -- - in our experiments , rllib matches or exceeds the performance of highly optimized reference implementations . ray rllib is available as part of ray at https : //github.com/ray-project/ray/ .

as cool as a cucumber : towards a corpus of contemporary similes in serbian
similes are natural language expressions used to compare unlikely things , where the comparison is not taken literally . they are often used in everyday communication and are an important part of cultural heritage . having an up-to-date corpus of similes is challenging , as they are constantly coined and/or adapted to the contemporary times . in this paper we present a methodology for semi-automated collection of similes from the world wide web using text mining techniques . we expanded an existing corpus of traditional similes ( containing 333 similes ) by collecting 446 additional expressions . we , also , explore how crowdsourcing can be used to extract and curate new similes .

probabilistic reasoning with answer sets
this paper develops a declarative language , p-log , that combines logical and probabilistic arguments in its reasoning . answer set prolog is used as the logical foundation , while causal bayes nets serve as a probabilistic foundation . we give several non-trivial examples and illustrate the use of p-log for knowledge representation and updating of knowledge . we argue that our approach to updates is more appealing than existing approaches . we give sufficiency conditions for the coherency of p-log programs and show that bayes nets can be easily mapped to coherent p-log programs .

robust task clustering for deep many-task learning
we investigate task clustering for deep-learning based multi-task and few-shot learning in a many-task setting . we propose a new method to measure task similarities with cross-task transfer performance matrix for the deep learning scenario . although this matrix provides us critical information regarding similarity between tasks , its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely . additionally , the uncertain task-pairs , i.e. , the ones with extremely asymmetric transfer scores , may collectively mislead clustering algorithms to output an inaccurate task-partition . to overcome these limitations , we propose a novel task-clustering algorithm by using the matrix completion technique . the proposed algorithm constructs a partially-observed similarity matrix based on the certainty of cluster membership of the task-pairs . we then use a matrix completion algorithm to complete the similarity matrix . our theoretical analysis shows that under mild constraints , the proposed algorithm will perfectly recover the underlying `` true '' similarity matrix with a high probability . our results show that the new task clustering method can discover task clusters for training flexible and superior neural network models in a multi-task learning setup for sentiment classification and dialog intent classification tasks . our task clustering approach also extends metric-based few-shot learning methods to adapt multiple metrics , which demonstrates empirical advantages when the tasks are diverse .

normalized information distance
the normalized information distance is a universal distance measure for objects of all kinds . it is based on kolmogorov complexity and thus uncomputable , but there are ways to utilize it . first , compression algorithms can be used to approximate the kolmogorov complexity if the objects have a string representation . second , for names and abstract concepts , page count statistics from the world wide web can be used . these practical realizations of the normalized information distance can then be applied to machine learning tasks , expecially clustering , to perform feature-free and parameter-free data mining . this chapter discusses the theoretical foundations of the normalized information distance and both practical realizations . it presents numerous examples of successful real-world applications based on these distance measures , ranging from bioinformatics to music clustering to machine translation .

threshold choice methods : the missing link
many performance metrics have been introduced for the evaluation of classification performance , with different origins and niches of application : accuracy , macro-accuracy , area under the roc curve , the roc convex hull , the absolute error , and the brier score ( with its decomposition into refinement and calibration ) . one way of understanding the relation among some of these metrics is the use of variable operating conditions ( either in the form of misclassification costs or class proportions ) . thus , a metric may correspond to some expected loss over a range of operating conditions . one dimension for the analysis has been precisely the distribution we take for this range of operating conditions , leading to some important connections in the area of proper scoring rules . however , we show that there is another dimension which has not received attention in the analysis of performance metrics . this new dimension is given by the decision rule , which is typically implemented as a threshold choice method when using scoring models . in this paper , we explore many old and new threshold choice methods : fixed , score-uniform , score-driven , rate-driven and optimal , among others . by calculating the loss of these methods for a uniform range of operating conditions we get the 0-1 loss , the absolute error , the brier score ( mean squared error ) , the auc and the refinement loss respectively . this provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation , namely : take a model , apply several threshold choice methods consistent with the information which is ( and will be ) available about the operating condition , and compare their expected losses . in order to assist in this procedure we also derive several connections between the aforementioned performance metrics , and we highlight the role of calibration in choosing the threshold choice method .

use of rapid probabilistic argumentation for ranking on large complex networks
we introduce a family of novel ranking algorithms called erank which run in linear/near linear time and build on explicitly modeling a network as uncertain evidence . the model uses probabilistic argumentation systems ( pas ) which are a combination of probability theory and propositional logic , and also a special case of dempster-shafer theory of evidence . erank rapidly generates approximate results for the np-complete problem involved enabling the use of the technique in large networks . we use a previously introduced pas model for citation networks generalizing it for all networks . we propose a statistical test to be used for comparing the performances of different ranking algorithms based on a clustering validity test . our experimentation using this test on a real-world network shows erank to have the best performance in comparison to well-known algorithms including pagerank , closeness , and betweenness .

wsat ( cc ) - a fast local-search asp solver
we describe wsat ( cc ) , a local-search solver for computing models of theories in the language of propositional logic extended by cardinality atoms . wsat ( cc ) is a processing back-end for the logic ps+ , a recently proposed formalism for answer-set programming .

security , privacy and safety evaluation of dynamic and static fleets of drones
inter-connected objects , either via public or private networks are the near future of modern societies . such inter-connected objects are referred to as internet-of-things ( iot ) and/or cyber-physical systems ( cps ) . one example of such a system is based on unmanned aerial vehicles ( uavs ) . the fleet of such vehicles are prophesied to take on multiple roles involving mundane to high-sensitive , such as , prompt pizza or shopping deliveries to your homes to battlefield deployment for reconnaissance and combat missions . drones , as we refer to uavs in this paper , either can operate individually ( solo missions ) or part of a fleet ( group missions ) , with and without constant connection with the base station . the base station acts as the command centre to manage the activities of the drones . however , an independent , localised and effective fleet control is required , potentially based on swarm intelligence , for the reasons : 1 ) increase in the number of drone fleets , 2 ) number of drones in a fleet might be multiple of tens , 3 ) time-criticality in making decisions by such fleets in the wild , 4 ) potential communication congestions/lag , and 5 ) in some cases working in challenging terrains that hinders or mandates-limited communication with control centre ( i.e. , operations spanning long period of times or military usage of such fleets in enemy territory ) . this self-ware , mission-focused and independent fleet of drones that potential utilises swarm intelligence for a ) air-traffic and/or flight control management , b ) obstacle avoidance , c ) self-preservation while maintaining the mission criteria , d ) collaboration with other fleets in the wild ( autonomously ) and e ) assuring the security , privacy and safety of physical ( drones itself ) and virtual ( data , software ) assets . in this paper , we investigate the challenges faced by fleet of drones and propose a potential course of action on how to overcome them .

e-res : a system for reasoning about actions , events and observations
e-res is a system that implements the language e , a logic for reasoning about narratives of action occurrences and observations . e 's semantics is model-theoretic , but this implementation is based on a sound and complete reformulation of e in terms of argumentation , and uses general computational techniques of argumentation frameworks . the system derives sceptical non-monotonic consequences of a given reformulated theory which exactly correspond to consequences entailed by e 's model-theory . the computation relies on a complimentary ability of the system to derive credulous non-monotonic consequences together with a set of supporting assumptions which is sufficient for the ( credulous ) conclusion to hold . e-res allows theories to contain general action laws , statements about action occurrences , observations and statements of ramifications ( or universal laws ) . it is able to derive consequences both forward and backward in time . this paper gives a short overview of the theoretical basis of e-res and illustrates its use on a variety of examples . currently , e-res is being extended so that the system can be used for planning .

a second order primal-dual method for nonsmooth convex composite optimization
we develop a second order primal-dual method for optimization problems in which the objective function is given by the sum of a strongly convex twice differentiable term and a possibly nondifferentiable convex regularizer . after introducing an auxiliary variable , we utilize the proximal operator of the nonsmooth regularizer to transform the associated augmented lagrangian into a function that is once , but not twice , continuously differentiable . the saddle point of this function corresponds to the solution of the original optimization problem . we employ a generalization of the hessian to define second order updates on this function and prove global exponential stability of the corresponding differential inclusion . furthermore , we develop a globally convergent customized algorithm that utilizes the primal-dual augmented lagrangian as a merit function . we show that the search direction can be computed efficiently and prove quadratic/superlinear asymptotic convergence . we use the $ \ell_1 $ -regularized least squares problem and the problem of designing a distributed controller for a spatially-invariant system to demonstrate the merits and the effectiveness of our method .

automated cloud provisioning on aws using deep reinforcement learning
as the use of cloud computing continues to rise , controlling cost becomes increasingly important . yet there is evidence that 30\ % - 45\ % of cloud spend is wasted . existing tools for cloud provisioning typically rely on highly trained human experts to specify what to monitor , thresholds for triggering action , and actions . in this paper we explore the use of reinforcement learning ( rl ) to acquire policies to balance performance and spend , allowing humans to specify what they want as opposed to how to do it , minimizing the need for cloud expertise . empirical results with tabular , deep , and dueling double deep q-learning with the cloudsim simulator show the utility of rl and the relative merits of the approaches . we also demonstrate effective policy transfer learning from an extremely simple simulator to cloudsim , with the next step being transfer from cloudsim to an amazon web services physical environment .

nonuniform dynamic discretization in hybrid networks
we consider probabilistic inference in general hybrid networks , which include continuous and discrete variables in an arbitrary topology . we reexamine the question of variable discretization in a hybrid network aiming at minimizing the information loss induced by the discretization . we show that a nonuniform partition across all variables as opposed to uniform partition of each variable separately reduces the size of the data structures needed to represent a continuous function . we also provide a simple but efficient procedure for nonuniform partition . to represent a nonuniform discretization in the computer memory , we introduce a new data structure , which we call a binary split partition ( bsp ) tree . we show that bsp trees can be an exponential factor smaller than the data structures in the standard uniform discretization in multiple dimensions and show how the bsp trees can be used in the standard join tree algorithm . we show that the accuracy of the inference process can be significantly improved by adjusting discretization with evidence . we construct an iterative anytime algorithm that gradually improves the quality of the discretization and the accuracy of the answer on a query . we provide empirical evidence that the algorithm converges .

narrative based postdictive reasoning for cognitive robotics
making sense of incomplete and conflicting narrative knowledge in the presence of abnormalities , unobservable processes , and other real world considerations is a challenge and crucial requirement for cognitive robotics systems . an added challenge , even when suitably specialised action languages and reasoning systems exist , is practical integration and application within large-scale robot control frameworks . in the backdrop of an autonomous wheelchair robot control task , we report on application-driven work to realise postdiction triggered abnormality detection and re-planning for real-time robot control : ( a ) narrative-based knowledge about the environment is obtained via a larger smart environment framework ; and ( b ) abnormalities are postdicted from stable-models of an answer-set program corresponding to the robot 's epistemic model . the overall reasoning is performed in the context of an approximate epistemic action theory based planner implemented via a translation to answer-set programming .

how should a robot assess risk ? towards an axiomatic theory of risk in robotics
endowing robots with the capability of assessing risk and making risk-aware decisions is widely considered a key step toward ensuring safety for robots operating under uncertainty . but , how should a robot quantify risk ? a natural and common approach is to consider the framework whereby costs are assigned to stochastic outcomes - an assignment captured by a cost random variable . quantifying risk then corresponds to evaluating a risk metric , i.e. , a mapping from the cost random variable to a real number . yet , the question of what constitutes a `` good '' risk metric has received little attention within the robotics community . the goal of this paper is to explore and partially address this question by advocating axioms that risk metrics in robotics applications should satisfy in order to be employed as rational assessments of risk . we discuss general representation theorems that precisely characterize the class of metrics that satisfy these axioms ( referred to as distortion risk metrics ) , and provide instantiations that can be used in applications . we further discuss pitfalls of commonly used risk metrics in robotics , and discuss additional properties that one must consider in sequential decision making tasks . our hope is that the ideas presented here will lead to a foundational framework for quantifying risk ( and hence safety ) in robotics applications .

making sensitivity analysis computationally efficient
to investigate the robustness of the output probabilities of a bayesian network , a sensitivity analysis can be performed . a one-way sensitivity analysis establishes , for each of the probability parameters of a network , a function expressing a posterior marginal probability of interest in terms of the parameter . current methods for computing the coefficients in such a function rely on a large number of network evaluations . in this paper , we present a method that requires just a single outward propagation in a junction tree for establishing the coefficients in the functions for all possible parameters ; in addition , an inward propagation is required for processing evidence . conversely , the method requires a single outward propagation for computing the coefficients in the functions expressing all possible posterior marginals in terms of a single parameter . we extend these results to an n-way sensitivity analysis in which sets of parameters are studied .

two steps feature selection and neural network classification for the trec-8 routing
for the trec-8 routing , one specific filter is built for each topic . each filter is a classifier trained to recognize the documents that are relevant to the topic . when presented with a document , each classifier estimates the probability for the document to be relevant to the topic for which it has been trained . since the procedure for building a filter is topic-independent , the system is fully automatic . by making use of a sample of documents that have previously been evaluated as relevant or not relevant to a particular topic , a term selection is performed , and a neural network is trained . each document is represented by a vector of frequencies of a list of selected terms . this list depends on the topic to be filtered ; it is constructed in two steps . the first step defines the characteristic words used in the relevant documents of the corpus ; the second one chooses , among the previous list , the most discriminant ones . the length of the vector is optimized automatically for each topic . at the end of the term selection , a vector of typically 25 words is defined for the topic , so that each document which has to be processed is represented by a vector of term frequencies . this vector is subsequently input to a classifier that is trained from the same sample . after training , the classifier estimates for each document of a test set its probability of being relevant ; for submission to trec , the top 1000 documents are ranked in order of decreasing relevance .

a compilation target for probabilistic programming languages
forward inference techniques such as sequential monte carlo and particle markov chain monte carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes , forking , mutexes , and shared memory . exploiting this we have defined , developed , and tested a probabilistic programming language intermediate representation language we call probabilistic c , which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient , scalable , portable probabilistic programming compilation target . this opens up a new hardware and systems research path for optimizing probabilistic programming systems .

operations on soft sets revisited
soft sets , as a mathematical tool for dealing with uncertainty , have recently gained considerable attention , including some successful applications in information processing , decision , demand analysis , and forecasting . to construct new soft sets from given soft sets , some operations on soft sets have been proposed . unfortunately , such operations can not keep all classical set-theoretic laws true for soft sets . in this paper , we redefine the intersection , complement , and difference of soft sets and investigate the algebraic properties of these operations along with a known union operation . we find that the new operation system on soft sets inherits all basic properties of operations on classical sets , which justifies our definitions .

multiple instance learning : a survey of problem characteristics and applications
multiple instance learning ( mil ) is a form of weakly supervised learning where training instances are arranged in sets , called bags , and a label is provided for the entire bag . this formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data . consequently , it has been used in diverse application fields such as computer vision and document classification . however , learning from bags raises important challenges that are unique to mil . this paper provides a comprehensive survey of the characteristics which define and differentiate the types of mil problems . until now , these problem characteristics have not been formally identified and described . as a result , the variations in performance of mil algorithms from one data set to another are difficult to explain . in this paper , mil problem characteristics are grouped into four broad categories : the composition of the bags , the types of data distribution , the ambiguity of instance labels , and the task to be performed . methods specialized to address each category are reviewed . then , the extent to which these characteristics manifest themselves in key mil application areas are described . finally , experiments are conducted to compare the performance of 16 state-of-the-art mil methods on selected problem characteristics . this paper provides insight on how the problem characteristics affect mil algorithms , recommendations for future benchmarking and promising avenues for research .

introspective perception : learning to predict failures in vision systems
as robots aspire for long-term autonomous operations in complex dynamic environments , the ability to reliably take mission-critical decisions in ambiguous situations becomes critical . this motivates the need to build systems that have situational awareness to assess how qualified they are at that moment to make a decision . we call this self-evaluating capability as introspection . in this paper , we take a small step in this direction and propose a generic framework for introspective behavior in perception systems . our goal is to learn a model to reliably predict failures in a given system , with respect to a task , directly from input sensor data . we present this in the context of vision-based autonomous mav flight in outdoor natural environments , and show that it effectively handles uncertain situations .

evolving tsp heuristics using multi expression programming
multi expression programming ( mep ) is an evolutionary technique that may be used for solving computationally difficult problems . mep uses a linear solution representation . each mep individual is a string encoding complex expressions ( computer programs ) . a mep individual may encode multiple solutions of the current problem . in this paper mep is used for evolving a traveling salesman problem ( tsp ) heuristic for graphs satisfying triangle inequality . evolved mep heuristic is compared with nearest neighbor heuristic ( nn ) and minimum spanning tree heuristic ( mst ) on some difficult problems in tsplib . for most of the considered problems the evolved mep heuristic outperforms nn and mst . the obtained algorithm was tested against some problems in tsplib . the results emphasizes that evolved mep heuristic is a powerful tool for solving difficult tsp instances .

applying chatbots to the internet of things : opportunities and architectural elements
internet of things ( iot ) is emerging as a significant technology in shaping the future by connecting physical devices or things with internet . it also presents various opportunities for intersection of other technological trends which can allow it to become even more intelligent and efficient . in this paper we focus our attention on the integration of intelligent conversational software agents or chatbots with iot . literature surveys have looked into various applications , features , underlying technologies and known challenges of iot . on the other hand , chatbots are being adopted in greater numbers due to major strides in development of platforms and frameworks . the novelty of this paper lies in the specific integration of chatbots in the iot scenario . we analyzed the shortcomings of existing iot systems and put forward ways to tackle them by incorporating chatbots . a general architecture is proposed for implementing such a system , as well as platforms and frameworks , both commercial and open source , which allow for implementation of such systems . identification of the newer challenges and possible future directions with this new integration , have also been addressed .

pso-mismo modeling strategy for multi-step-ahead time series prediction
multi-step-ahead time series prediction is one of the most challenging research topics in the field of time series modeling and prediction , and is continually under research . recently , the multiple-input several multiple-outputs ( mismo ) modeling strategy has been proposed as a promising alternative for multi-step-ahead time series prediction , exhibiting advantages compared with the two currently dominating strategies , the iterated and the direct strategies . built on the established mismo strategy , this study proposes a particle swarm optimization ( pso ) -based mismo modeling strategy , which is capable of determining the number of sub-models in a self-adaptive mode , with varying prediction horizons . rather than deriving crisp divides with equal-size s prediction horizons from the established mismo , the proposed pso-mismo strategy , implemented with neural networks , employs a heuristic to create flexible divides with varying sizes of prediction horizons and to generate corresponding sub-models , providing considerable flexibility in model construction , which has been validated with simulated and real datasets .

using potential influence diagrams for probabilistic inference and decision making
the potential influence diagram is a generalization of the standard `` conditional '' influence diagram , a directed network representation for probabilistic inference and decision analysis [ ndilikilikesha , 1991 ] . it allows efficient inference calculations corresponding exactly to those on undirected graphs . in this paper , we explore the relationship between potential and conditional influence diagrams and provide insight into the properties of the potential influence diagram . in particular , we show how to convert a potential influence diagram into a conditional influence diagram , and how to view the potential influence diagram operations in terms of the conditional influence diagram .

controlled hierarchical filtering : model of neocortical sensory processing
a model of sensory information processing is presented . the model assumes that learning of internal ( hidden ) generative models , which can predict the future and evaluate the precision of that prediction , is of central importance for information extraction . furthermore , the model makes a bridge to goal-oriented systems and builds upon the structural similarity between the architecture of a robust controller and that of the hippocampal entorhinal loop . this generative control architecture is mapped to the neocortex and to the hippocampal entorhinal loop . implicit memory phenomena ; priming and prototype learning are emerging features of the model . mathematical theorems ensure stability and attractive learning properties of the architecture . connections to reinforcement learning are also established : both the control network , and the network with a hidden model converge to ( near ) optimal policy under suitable conditions . falsifying predictions , including the role of the feedback connections between neocortical areas are made .

probabilistic logic programming under inheritance with overriding
we present probabilistic logic programming under inheritance with overriding . this approach is based on new notions of entailment for reasoning with conditional constraints , which are obtained from the classical notion of logical entailment by adding the principle of inheritance with overriding . this is done by using recent approaches to probabilistic default reasoning with conditional constraints . we analyze the semantic properties of the new entailment relations . we also present algorithms for probabilistic logic programming under inheritance with overriding , and program transformations for an increased efficiency .

fault tolerant boolean satisfiability
a delta-model is a satisfying assignment of a boolean formula for which any small alteration , such as a single bit flip , can be repaired by flips to some small number of other bits , yielding a new satisfying assignment . these satisfying assignments represent robust solutions to optimization problems ( e.g. , scheduling ) where it is possible to recover from unforeseen events ( e.g. , a resource becoming unavailable ) . the concept of delta-models was introduced by ginsberg , parkes and roy ( aaai 1998 ) , where it was proved that finding delta-models for general boolean formulas is np-complete . in this paper , we extend that result by studying the complexity of finding delta-models for classes of boolean formulas which are known to have polynomial time satisfiability solvers . in particular , we examine 2-sat , horn-sat , affine-sat , dual-horn-sat , 0-valid and 1-valid sat . we see a wide variation in the complexity of finding delta-models , e.g. , while 2-sat and affine-sat have polynomial time tests for delta-models , testing whether a horn-sat formula has one is np-complete .

perceptual context in cognitive hierarchies
cognition does not only depend on bottom-up sensor feature abstraction , but also relies on contextual information being passed top-down . context is higher level information that helps to predict belief states at lower levels . the main contribution of this paper is to provide a formalisation of perceptual context and its integration into a new process model for cognitive hierarchies . several simple instantiations of a cognitive hierarchy are used to illustrate the role of context . notably , we demonstrate the use context in a novel approach to visually track the pose of rigid objects with just a 2d camera .

deep episodic memory : encoding , recalling , and predicting episodic experiences for robot action execution
we present a novel deep neural network architecture for representing robot experiences in an episodic-like memory which facilitates encoding , recalling , and predicting action experiences . our proposed unsupervised deep episodic memory model 1 ) encodes observed actions in a latent vector space and , based on this latent encoding , 2 ) infers most similar episodes previously experienced , 3 ) reconstructs original episodes , and 4 ) predicts future frames in an end-to-end fashion . results show that conceptually similar actions are mapped into the same region of the latent vector space . based on these results , we introduce an action matching and retrieval mechanism , benchmark its performance on two large-scale action datasets , 20bn-something-something and activitynet and evaluate its generalization capability in a real-world scenario on a humanoid robot .

a game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems
the ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination . we conceptualise this problem formally using a game-theoretic model , called the stochastic bayesian game , in which the behaviour of a player is determined by its private information , or type . based on this model , we derive a solution , called harsanyi-bellman ad hoc coordination ( hba ) , which utilises the concept of bayesian nash equilibrium in a planning procedure to find optimal actions in the sense of bellman optimal control . we evaluate hba in a multiagent logistics domain called level-based foraging , showing that it achieves higher flexibility and efficiency than several alternative algorithms . we also report on a human-machine experiment at a public science exhibition in which the human participants played repeated prisoner 's dilemma and rock-paper-scissors against hba and alternative algorithms , showing that hba achieves equal efficiency and a significantly higher welfare and winning rate .

bayesian network structure learning with integer programming : polytopes , facets , and complexity
the challenging task of learning structures of probabilistic graphical models is an important problem within modern ai research . recent years have witnessed several major algorithmic advances in structure learning for bayesian networks -- -arguably the most central class of graphical models -- -especially in what is known as the score-based setting . a successful generic approach to optimal bayesian network structure learning ( bnsl ) , based on integer programming ( ip ) , is implemented in the gobnilp system . despite the recent algorithmic advances , current understanding of foundational aspects underlying the ip based approach to bnsl is still somewhat lacking . understanding fundamental aspects of cutting planes and the related separation problem ( is important not only from a purely theoretical perspective , but also since it holds out the promise of further improving the efficiency of state-of-the-art approaches to solving bnsl exactly . in this paper , we make several theoretical contributions towards these goals : ( i ) we study the computational complexity of the separation problem , proving that the problem is np-hard ; ( ii ) we formalise and analyse the relationship between three key polytopes underlying the ip-based approach to bnsl ; ( iii ) we study the facets of the three polytopes both from the theoretical and practical perspective , providing , via exhaustive computation , a complete enumeration of facets for low-dimensional family-variable polytopes ; and , furthermore , ( iv ) we establish a tight connection of the bnsl problem to the acyclic subgraph problem .

structure-based causes and explanations in the independent choice logic
this paper is directed towards combining pearl 's structural-model approach to causal reasoning with high-level formalisms for reasoning about actions . more precisely , we present a combination of pearl 's structural-model approach with poole 's independent choice logic . we show how probabilistic theories in the independent choice logic can be mapped to probabilistic causal models . this mapping provides the independent choice logic with appealing concepts of causality and explanation from the structural-model approach . we illustrate this along halpern and pearl 's sophisticated notions of actual cause , explanation , and partial explanation . this mapping also adds first-order modeling capabilities and explicit actions to the structural-model approach .

vehicle routing problem with vector profits ( vrpvp ) with max-min criterion
this paper introduces a new routing problem referred to as the vehicle routing problem with vector profits . given a network composed of nodes ( depot/sites ) and arcs connecting the nodes , the problem determines routes that depart from the depot , visit sites to collect profits , and return to the depot . there are multiple stakeholders interested in the mission and each site is associated with a vector whose k-th element represents the profit value for the k-th stakeholder . the objective of the problem is to maximize the profit sum for the least satisfied stakeholder , i.e. , the stakeholder with the smallest total profit value . an approach based on the linear programming relaxation and column-generation to solve this max-min type routing problem was developed . two cases studies - the planetary surface exploration and the rome tour cases - were presented to demonstrate the effectiveness of the proposed problem formulation and solution methodology .

deriving a stationary dynamic bayesian network from a logic program with recursive loops
recursive loops in a logic program present a challenging problem to the plp framework . on the one hand , they loop forever so that the plp backward-chaining inferences would never stop . on the other hand , they generate cyclic influences , which are disallowed in bayesian networks . therefore , in existing plp approaches logic programs with recursive loops are considered to be problematic and thus are excluded . in this paper , we propose an approach that makes use of recursive loops to build a stationary dynamic bayesian network . our work stems from an observation that recursive loops in a logic program imply a time sequence and thus can be used to model a stationary dynamic bayesian network without using explicit time parameters . we introduce a bayesian knowledge base with logic clauses of the form $ a \leftarrow a_1 , ... , a_l , true , context , types $ , which naturally represents the knowledge that the $ a_i $ s have direct influences on $ a $ in the context $ context $ under the type constraints $ types $ . we then use the well-founded model of a logic program to define the direct influence relation and apply slg-resolution to compute the space of random variables together with their parental connections . we introduce a novel notion of influence clauses , based on which a declarative semantics for a bayesian knowledge base is established and algorithms for building a two-slice dynamic bayesian network from a logic program are developed .

can active learning experience be transferred ?
active learning is an important machine learning problem in reducing the human labeling effort . current active learning strategies are designed from human knowledge , and are applied on each dataset in an immutable manner . in other words , experience about the usefulness of strategies can not be updated and transferred to improve active learning on other datasets . this paper initiates a pioneering study on whether active learning experience can be transferred . we first propose a novel active learning model that linearly aggregates existing strategies . the linear weights can then be used to represent the active learning experience . we equip the model with the popular linear upper- confidence-bound ( linucb ) algorithm for contextual bandit to update the weights . finally , we extend our model to transfer the experience across datasets with the technique of biased regularization . empirical studies demonstrate that the learned experience not only is competitive with existing strategies on most single datasets , but also can be transferred across datasets to improve the performance on future learning tasks .

debugging machine learning tasks
unlike traditional programs ( such as operating systems or word processors ) which have large amounts of code , machine learning tasks use programs with relatively small amounts of code ( written in machine learning libraries ) , but voluminous amounts of data . just like developers of traditional programs debug errors in their code , developers of machine learning tasks debug and fix errors in their data . however , algorithms and tools for debugging and fixing errors in data are less common , when compared to their counterparts for detecting and fixing errors in code . in this paper , we consider classification tasks where errors in training data lead to misclassifications in test points , and propose an automated method to find the root causes of such misclassifications . our root cause analysis is based on pearl 's theory of causation , and uses pearl 's ps ( probability of sufficiency ) as a scoring metric . our implementation , psi , encodes the computation of ps as a probabilistic program , and uses recent work on probabilistic programs and transformations on probabilistic programs ( along with gray-box models of machine learning algorithms ) to efficiently compute ps . psi is able to identify root causes of data errors in interesting data sets .

dealing with metonymic readings of named entities
the aim of this paper is to propose a method for tagging named entities ( ne ) , using natural language processing techniques . beyond their literal meaning , named entities are frequently subject to metonymy . we show the limits of current ne type hierarchies and detail a new proposal aiming at dynamically capturing the semantics of entities in context . this model can analyze complex linguistic phenomena like metonymy , which are known to be difficult for natural language processing but crucial for most applications . we present an implementation and some test using the french ester corpus and give significant results .

the role of artificial intelligence technologies in crisis response
crisis response poses many of the most difficult information technology in crisis management . it requires information and communication-intensive efforts , utilized for reducing uncertainty , calculating and comparing costs and benefits , and managing resources in a fashion beyond those regularly available to handle routine problems . in this paper , we explore the benefits of artificial intelligence technologies in crisis response . this paper discusses the role of artificial intelligence technologies ; namely , robotics , ontology and semantic web , and multi-agent systems in crisis response .

weighted positive binary decision diagrams for exact probabilistic inference
recent work on weighted model counting has been very successfully applied to the problem of probabilistic inference in bayesian networks . the probability distribution is encoded into a boolean normal form and compiled to a target language , in order to represent local structure expressed among conditional probabilities more efficiently . we show that further improvements are possible , by exploiting the knowledge that is lost during the encoding phase and incorporating it into a compiler inspired by satisfiability modulo theories . constraints among variables are used as a background theory , which allows us to optimize the shannon decomposition . we propose a new language , called weighted positive binary decision diagrams , that reduces the cost of probabilistic inference by using this decomposition variant to induce an arithmetic circuit of reduced size .

iterated variable neighborhood search for the resource constrained multi-mode multi-project scheduling problem
the resource constrained multi-mode multi-project scheduling problem ( rcmmmpsp ) is a notoriously difficult combinatorial optimization problem . for a given set of activities , feasible execution mode assignments and execution starting times must be found such that some optimization function , e.g . the makespan , is optimized . when determining an optimal ( or at least feasible ) assignment of decision variable values , a set of side constraints , such as resource availabilities , precedence constraints , etc. , has to be respected . in 2013 , the mista 2013 challenge stipulated research in the rcmmmpsp . it 's goal was the solution of a given set of instances under running time restrictions . we have contributed to this challenge with the here presented approach .

a state vector algebra for algorithmic implementation of second-order logic
we present a mathematical framework for mapping second-order logic relations onto a simple state vector algebra . using this algebra , basic theorems of set theory can be proven in an algorithmic way , hence by an expert system . we illustrate the use of the algebra with simple examples and show that , in principle , all theorems of basic set theory can be recovered in an elementary way . the developed technique can be used for an automated theorem proving in the 1st and 2nd order logic .

a comment on argumentation
we use the theory of defaults and their meaning of [ gs16 ] to develop ( the outline of a ) new theory of argumentation .

dual decomposition from the perspective of relax , compensate and then recover
relax , compensate and then recover ( rcr ) is a paradigm for approximate inference in probabilistic graphical models that has previously provided theoretical and practical insights on iterative belief propagation and some of its generalizations . in this paper , we characterize the technique of dual decomposition in the terms of rcr , viewing it as a specific way to compensate for relaxed equivalence constraints . among other insights gathered from this perspective , we propose novel heuristics for recovering relaxed equivalence constraints with the goal of incrementally tightening dual decomposition approximations , all the way to reaching exact solutions . we also show empirically that recovering equivalence constraints can sometimes tighten the corresponding approximation ( and obtaining exact results ) , without increasing much the complexity of inference .

causal falling rule lists
a causal falling rule list ( cfrl ) is a sequence of if-then rules that specifies heterogeneous treatment effects , where ( i ) the order of rules determines the treatment effect subgroup a subject belongs to , and ( ii ) the treatment effect decreases monotonically down the list . a given cfrl parameterizes a hierarchical bayesian regression model in which the treatment effects are incorporated as parameters , and assumed constant within model-specific subgroups . we formulate the search for the cfrl best supported by the data as a bayesian model selection problem , where we perform a search over the space of cfrl models , and approximate the evidence for a given cfrl model using standard variational techniques . we apply cfrl to a census wage dataset to identify subgroups of differing wage inequalities between men and women .

$ q $ - and $ a $ -learning methods for estimating optimal dynamic treatment regimes
in clinical practice , physicians make a series of treatment decisions over the course of a patient 's disease based on his/her baseline and evolving characteristics . a dynamic treatment regime is a set of sequential decision rules that operationalizes this process . each rule corresponds to a decision point and dictates the next treatment action based on the accrued information . using existing data , a key goal is estimating the optimal regime , that , if followed by the patient population , would yield the most favorable outcome on average . q- and a-learning are two main approaches for this purpose . we provide a detailed account of these methods , study their performance , and illustrate them using data from a depression study .

automating the dispute resolution in task dependency network
when perturbation or unexpected events do occur , agents need protocols for repairing or reforming the supply chain . unfortunate contingency could increase too much the cost of performance , while breaching the current contract may be more efficient . in our framework the principles of contract law are applied to set penalties : expectation damages , opportunity cost , reliance damages , and party design remedies , and they are introduced in the task dependency model

efficient decision-making by volume-conserving physical object
we demonstrate that any physical object , as long as its volume is conserved when coupled with suitable operations , provides a sophisticated decision-making capability . we consider the problem of finding , as accurately and quickly as possible , the most profitable option from a set of options that gives stochastic rewards . these decisions are made as dictated by a physical object , which is moved in a manner similar to the fluctuations of a rigid body in a tug-of-war game . our analytical calculations validate statistical reasons why our method exhibits higher efficiency than conventional algorithms .

discovering semantic spatial and spatio-temporal outliers from moving object trajectories
several algorithms have been proposed for discovering patterns from trajectories of moving objects , but only a few have concentrated on outlier detection . existing approaches , in general , discover spatial outliers , and do not provide any further analysis of the patterns . in this paper we introduce semantic spatial and spatio-temporal outliers and propose a new algorithm for trajectory outlier detection . semantic outliers are computed between regions of interest , where objects have similar movement intention , and there exist standard paths which connect the regions . we show with experiments on real data that the method finds semantic outliers from trajectory data that are not discovered by similar approaches .

le terme et le concept : fondements d'une ontoterminologie
most definitions of ontology , viewed as a `` specification of a conceptualization '' , agree on the fact that if an ontology can take different forms , it necessarily includes a vocabulary of terms and some specification of their meaning in relation to the domain 's conceptualization . and as domain knowledge is mainly conveyed through scientific and technical texts , we can hope to extract some useful information from them for building ontology . but is it as simple as this ? in this article we shall see that the lexical structure , i.e . the network of words linked by linguistic relationships , does not necessarily match the domain conceptualization . we have to bear in mind that writing documents is the concern of textual linguistics , of which one of the principles is the incompleteness of text , whereas building ontology - viewed as task-independent knowledge - is concerned with conceptualization based on formal and not natural languages . nevertheless , the famous sapir and whorf hypothesis , concerning the interdependence of thought and language , is also applicable to formal languages . this means that the way an ontology is built and a concept is defined depends directly on the formal language which is used ; and the results will not be the same . the introduction of the notion of ontoterminology allows to take into account epistemological principles for formal ontology building .

new s-norm and t-norm operators for active learning method
active learning method ( alm ) is a soft computing method used for modeling and control based on fuzzy logic . all operators defined for fuzzy sets must serve as either fuzzy s-norm or fuzzy t-norm . despite being a powerful modeling method , alm does not possess operators which serve as s-norms and t-norms which deprive it of a profound analytical expression/form . this paper introduces two new operators based on morphology which satisfy the following conditions : first , they serve as fuzzy s-norm and t-norm . second , they satisfy demorgans law , so they complement each other perfectly . these operators are investigated via three viewpoints : mathematics , geometry and fuzzy logic .

discovering underlying plans based on distributed representations of actions
plan recognition aims to discover target plans ( i.e. , sequences of actions ) behind observed actions , with history plan libraries or domain models in hand . previous approaches either discover plans by maximally `` matching '' observed actions to plan libraries , assuming target plans are from plan libraries , or infer plans by executing domain models to best explain the observed actions , assuming complete domain models are available . in real world applications , however , target plans are often not from plan libraries and complete domain models are often not available , since building complete sets of plans and complete domain models are often difficult or expensive . in this paper we view plan libraries as corpora and learn vector representations of actions using the corpora ; we then discover target plans based on the vector representations . our approach is capable of discovering underlying plans that are not from plan libraries , without requiring domain models provided . we empirically demonstrate the effectiveness of our approach by comparing its performance to traditional plan recognition approaches in three planning domains .

solving the goddard problem by an influence diagram
influence diagrams are a decision-theoretic extension of probabilistic graphical models . in this paper we show how they can be used to solve the goddard problem . we present results of numerical experiments with this problem and compare the solutions provided by influence diagrams with the optimal solution .

joint structured models for extraction from overlapping sources
we consider the problem of jointly training structured models for extraction from sources whose instances enjoy partial overlap . this has important applications like user-driven ad-hoc information extraction on the web . such applications present new challenges in terms of the number of sources and their arbitrary pattern of overlap not seen by earlier collective training schemes applied on two sources . we present an agreement-based learning framework and alternatives within it to trade-off tractability , robustness to noise , and extent of agreement . we provide a principled scheme to discover low-noise agreement sets in unlabeled data across the sources . through extensive experiments over 58 real datasets , we establish that our method of additively rewarding agreement over maximal segments of text provides the best trade-offs , and also scores over alternatives such as collective inference , staged training , and multi-view learning .

the metrics matter ! on the incompatibility of different flavors of replanning
when autonomous agents are executing in the real world , the state of the world as well as the objectives of the agent may change from the agent 's original model . in such cases , the agent 's planning process must modify the plan under execution to make it amenable to the new conditions , and to resume execution . this brings up the replanning problem , and the various techniques that have been proposed to solve it . in all , three main techniques -- based on three different metrics -- have been proposed in prior automated planning work . an open question is whether these metrics are interchangeable ; answering this requires a normalized comparison of the various replanning quality metrics . in this paper , we show that it is possible to support such a comparison by compiling all the respective techniques into a single substrate . using this novel compilation , we demonstrate that these different metrics are not interchangeable , and that they are not good surrogates for each other . thus we focus attention on the incompatibility of the various replanning flavors with each other , founded in the differences between the metrics that they respectively seek to optimize .

reinforcement learning with deep energy-based policies
we propose a method for learning expressive energy-based policies for continuous states and actions , which has been feasible only in tabular domains before . we apply our method to learning maximum entropy policies , resulting into a new algorithm , called soft q-learning , that expresses the optimal policy via a boltzmann distribution . we use the recently proposed amortized stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution . the benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks , which we confirm in simulated experiments with swimming and walking robots . we also draw a connection to actor-critic methods , which can be viewed performing approximate inference on the corresponding energy-based model .

reasoning for moving blocks problem : formal representation and implementation
the combined approach of the qualitative reasoning and probabilistic functions for the knowledge representation is proposed . the method aims at represent uncertain , qualitative knowledge that is essential for the moving blocks task 's execution . the attempt to formalize the commonsense knowledge is performed with the situation calculus language for reasoning and robot 's beliefs representation . the method is implemented in the prolog programming language and tested for a specific simulated scenario . in most cases the implementation enables us to solve a given task , i.e. , move blocks to desired positions . the example of robot 's reasoning and main parts of the implemented program 's code are presented .

object-oriented neural programming ( oonp ) for document understanding
we propose object-oriented neural programming ( oonp ) , a framework for semantically parsing documents in specific domains . basically , oonp reads a document and parses it into a predesigned object-oriented data structure ( referred to as ontology in this paper ) that reflects the domain-specific semantics of the document . an oonp parser models semantic parsing as a decision process : a neural net-based reader sequentially goes through the document , and during the process it builds and updates an intermediate ontology to summarize its partial understanding of the text it covers . oonp supports a rich family of operations ( both symbolic and differentiable ) for composing the ontology , and a big variety of forms ( both symbolic and differentiable ) for representing the state and the document . an oonp parser can be trained with supervision of different forms and strength , including supervised learning ( sl ) , reinforcement learning ( rl ) and hybrid of the two . our experiments on both synthetic and real-world document parsing tasks have shown that oonp can learn to handle fairly complicated ontology with training data of modest sizes .

parallel strategies selection
we consider the problem of selecting the best variable-value strategy for solving a given problem in constraint programming . we show that the recent embarrassingly parallel search method ( eps ) can be used for this purpose . eps proposes to solve a problem by decomposing it in a lot of subproblems and to give them on-demand to workers which run in parallel . our method uses a part of these subproblems as a simple sample as defined in statistics for comparing some strategies in order to select the most promising one that will be used for solving the remaining subproblems . for each subproblem of the sample , the parallelism helps us to control the running time of the strategies because it gives us the possibility to introduce timeouts by stopping a strategy when it requires more than twice the time of the best one . thus , we can deal with the great disparity in solving times for the strategies . the selections we made are based on the wilcoxon signed rank tests because no assumption has to be made on the distribution of the solving times and because these tests can deal with the censored data that we obtain after introducing timeouts . the experiments we performed on a set of classical benchmarks for satisfaction and optimization problems show that our method obtain good performance by selecting almost all the time the best variable-value strategy and by almost never choosing a variable-value strategy which is dramatically slower than the best one . our method also outperforms the portfolio approach consisting in running some strategies in parallel and is competitive with the multi armed bandit framework .

grouplingam : linear non-gaussian acyclic models for sets of variables
finding the structure of a graphical model has been received much attention in many fields . recently , it is reported that the non-gaussianity of data enables us to identify the structure of a directed acyclic graph without any prior knowledge on the structure . in this paper , we propose a novel non-gaussianity based algorithm for more general type of models ; chain graphs . the algorithm finds an ordering of the disjoint subsets of variables by iteratively evaluating the independence between the variable subset and the residuals when the remaining variables are regressed on those . however , its computational cost grows exponentially according to the number of variables . therefore , we further discuss an efficient approximate approach for applying the algorithm to large sized graphs . we illustrate the algorithm with artificial and real-world datasets .

evaluating defaults
we seek to find normative criteria of adequacy for nonmonotonic logic similar to the criterion of validity for deductive logic . rather than stipulating that the conclusion of an inference be true in all models in which the premises are true , we require that the conclusion of a nonmonotonic inference be true in `` almost all '' models of a certain sort in which the premises are true . this `` certain sort '' specification picks out the models that are relevant to the inference , taking into account factors such as specificity and vagueness , and previous inferences . the frequencies characterizing the relevant models reflect known frequencies in our actual world . the criteria of adequacy for a default inference can be extended by thresholding to criteria of adequacy for an extension . we show that this avoids the implausibilities that might otherwise result from the chaining of default inferences . the model proportions , when construed in terms of frequencies , provide a verifiable grounding of default rules , and can become the basis for generating default rules from statistics .

emotion analysis of songs based on lyrical and audio features
in this paper , a method is proposed to detect the emotion of a song based on its lyrical and audio features . lyrical features are generated by segmentation of lyrics during the process of data extraction . anew and wordnet knowledge is then incorporated to compute valence and arousal values . in addition to this , linguistic association rules are applied to ensure that the issue of ambiguity is properly addressed . audio features are used to supplement the lyrical ones and include attributes like energy , tempo , and danceability . these features are extracted from the echo nest , a widely used music intelligence platform . construction of training and test sets is done on the basis of social tags extracted from the last.fm website . the classification is done by applying feature weighting and stepwise threshold reduction on the k-nearest neighbors algorithm to provide fuzziness in the classification .

using genetic algorithms to optimise rough set partition sizes for hiv data analysis
in this paper , we present a method to optimise rough set partition sizes , to which rule extraction is performed on hiv data . the genetic algorithm optimisation technique is used to determine the partition sizes of a rough set in order to maximise the rough sets prediction accuracy . the proposed method is tested on a set of demographic properties of individuals obtained from the south african antenatal survey . six demographic variables were used in the analysis , these variables are ; race , age of mother , education , gravidity , parity , and age of father , with the outcome or decision being either hiv positive or negative . rough set theory is chosen based on the fact that it is easy to interpret the extracted rules . the prediction accuracy of equal width bin partitioning is 57.7 % while the accuracy achieved after optimising the partitions is 72.8 % . several other methods have been used to analyse the hiv data and their results are stated and compared to that of rough set theory ( rst ) .

an argumentation system for reasoning with conflict-minimal paraconsistent alc
the semantic web is an open and distributed environment in which it is hard to guarantee consistency of knowledge and information . under the standard two-valued semantics everything is entailed if knowledge and information is inconsistent . the semantics of the paraconsistent logic lp offers a solution . however , if the available knowledge and information is consistent , the set of conclusions entailed under the three-valued semantics of the paraconsistent logic lp is smaller than the set of conclusions entailed under the two-valued semantics . preferring conflict-minimal three-valued interpretations eliminates this difference . preferring conflict-minimal interpretations introduces non-monotonicity . to handle the non-monotonicity , this paper proposes an assumption-based argumentation system . assumptions needed to close branches of a semantic tableaux form the arguments . stable extensions of the set of derived arguments correspond to conflict minimal interpretations and conclusions entailed by all conflict-minimal interpretations are supported by arguments in all stable extensions .

obfuscated gradients give a false sense of security : circumventing defenses to adversarial examples
we identify obfuscated gradients , a kind of gradient masking , as a phenomenon that leads to a false sense of security in defenses against adversarial examples . while defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks , we find defenses relying on this effect can be circumvented . for each of the three types of obfuscated gradients we discover , we describe characteristic behaviors of defenses exhibiting this effect and develop attack techniques to overcome it . in a case study , examining non-certified white-box-secure defenses at iclr 2018 , we find obfuscated gradients are a common occurrence , with 7 of 8 defenses relying on obfuscated gradients . our new attacks successfully circumvent 6 completely and 1 partially .

general scaled support vector machines
support vector machines ( svms ) are popular tools for data mining tasks such as classification , regression , and density estimation . however , original svm ( c-svm ) only considers local information of data points on or over the margin . therefore , c-svm loses robustness . to solve this problem , one approach is to translate ( i.e. , to move without rotation or change of shape ) the hyperplane according to the distribution of the entire data . but existing work can only be applied for 1-d case . in this paper , we propose a simple and efficient method called general scaled svm ( gs-svm ) to extend the existing approach to multi-dimensional case . our method translates the hyperplane according to the distribution of data projected on the normal vector of the hyperplane . compared with c-svm , gs-svm has better performance on several data sets .

learning 6-dof grasping interaction with deep geometry-aware 3d representations
this paper focuses on the problem of learning 6-dof grasping with a parallel jaw gripper in simulation . we propose the notion of a geometry-aware representation in grasping based on the assumption that knowledge of 3d geometry is at the heart of interaction . our key idea is constraining and regularizing grasping interaction learning through 3d geometry prediction . specifically , we formulate the learning of deep geometry-aware grasping model in two steps : first , we learn to build mental geometry-aware representation by reconstructing the scene ( i.e. , 3d occupancy grid ) from rgbd input via generative 3d shape modeling . second , we learn to predict grasping outcome with its internal geometry-aware representation . the learned outcome prediction model is used to sequentially propose grasping solutions via analysis-by-synthesis optimization . our contributions are fourfold : ( 1 ) to best of our knowledge , we are presenting for the first time a method to learn a 6-dof grasping net from rgbd input ; ( 2 ) we build a grasping dataset from demonstrations in virtual reality with rich sensory and interaction annotations . this dataset includes 101 everyday objects spread across 7 categories , additionally , we propose a data augmentation strategy for effective learning ; ( 3 ) we demonstrate that the learned geometry-aware representation leads to about 10 percent relative performance improvement over the baseline cnn on grasping objects from our dataset . ( 4 ) we further demonstrate that the model generalizes to novel viewpoints and object instances .

synthesizing imperative programs from examples guided by static analysis
we present a novel algorithm that synthesizes imperative programs for introductory programming courses . given a set of input-output examples and a partial program , our algorithm generates a complete program that is consistent with every example . our key idea is to combine enumerative program synthesis and static analysis , which aggressively prunes out a large search space while guaranteeing to find , if any , a correct solution . we have implemented our algorithm in a tool , called simpl , and evaluated it on 30 problems used in introductory programming courses . the results show that simpl is able to solve the benchmark problems in 6.6 seconds on average .

a deep learning approach for joint video frame and reward prediction in atari games
reinforcement learning is concerned with identifying reward-maximizing behaviour policies in environments that are initially unknown . state-of-the-art reinforcement learning approaches , such as deep q-networks , are model-free and learn to act effectively across a wide range of environments such as atari games , but require huge amounts of data . model-based techniques are more data-efficient , but need to acquire explicit knowledge about the environment . in this paper , we take a step towards using model-based techniques in environments with a high-dimensional visual state space by demonstrating that it is possible to learn system dynamics and the reward structure jointly . our contribution is to extend a recently developed deep neural network for video frame prediction in atari games to enable reward prediction as well . to this end , we phrase a joint optimization problem for minimizing both video frame and reward reconstruction loss , and adapt network parameters accordingly . empirical evaluations on five atari games demonstrate accurate cumulative reward prediction of up to 200 frames . we consider these results as opening up important directions for model-based reinforcement learning in complex , initially unknown environments .

inference in multiply sectioned bayesian networks with extended shafer-shenoy and lazy propagation
as bayesian networks are applied to larger and more complex problem domains , search for flexible modeling and more efficient inference methods is an ongoing effort . multiply sectioned bayesian networks ( msbns ) extend the hugin inference for bayesian networks into a coherent framework for flexible modeling and distributed inference.lazy propagation extends the shafer-shenoy and hugin inference methods with reduced space complexity . we apply the shafer-shenoy and lazy propagation to inference in msbns . the combination of the msbn framework and lazy propagation provides a better framework for modeling and inference in very large domains . it retains the modeling flexibility of msbns and reduces the runtime space complexity , allowing exact inference in much larger domains given the same computational resources .

surprising properties of dropout in deep networks
we analyze dropout in deep networks with rectified linear units and the quadratic loss . our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay . for example , on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs . this provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights . we also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear , and that dropout is insensitive to various re-scalings of the input features , outputs , and network weights . this last insensitivity implies that there are no isolated local minima of the dropout training criterion . our work uncovers new properties of dropout , extends our understanding of why dropout succeeds , and lays the foundation for further progress .

geometric algebra model of distributed representations
formalism based on ga is an alternative to distributed representation models developed so far -- - smolensky 's tensor product , holographic reduced representations ( hrr ) and binary spatter code ( bsc ) . convolutions are replaced by geometric products , interpretable in terms of geometry which seems to be the most natural language for visualization of higher concepts . this paper recalls the main ideas behind the ga model and investigates recognition test results using both inner product and a clipped version of matrix representation . the influence of accidental blade equality on recognition is also studied . finally , the efficiency of the ga model is compared to that of previously developed models .

two-dimensional arma modeling for breast cancer detection and classification
we propose a new model-based computer-aided diagnosis ( cad ) system for tumor detection and classification ( cancerous v.s . benign ) in breast images . specifically , we show that ( x-ray , ultrasound and mri ) images can be accurately modeled by two-dimensional autoregressive-moving average ( arma ) random fields . we derive a two-stage yule-walker least-squares estimates of the model parameters , which are subsequently used as the basis for statistical inference and biophysical interpretation of the breast image . we use a k-means classifier to segment the breast image into three regions : healthy tissue , benign tumor , and cancerous tumor . our simulation results on ultrasound breast images illustrate the power of the proposed approach .

asynchronous stochastic gradient mcmc with elastic coupling
we consider parallel asynchronous markov chain monte carlo ( mcmc ) sampling for problems where we can leverage ( stochastic ) gradients to define continuous dynamics which explore the target distribution . we outline a solution strategy for this setting based on stochastic gradient hamiltonian monte carlo sampling ( sghmc ) which we alter to include an elastic coupling term that ties together multiple mcmc instances . the proposed strategy turns inherently sequential hmc algorithms into asynchronous parallel versions . first experiments empirically show that the resulting parallel sampler significantly speeds up exploration of the target distribution , when compared to standard sghmc , and is less prone to the harmful effects of stale gradients than a naive parallelization approach .

constructive preference elicitation by setwise max-margin learning
in this paper we propose an approach to preference elicitation that is suitable to large configuration spaces beyond the reach of existing state-of-the-art approaches . our setwise max-margin method can be viewed as a generalization of max-margin learning to sets , and can produce a set of `` diverse '' items that can be used to ask informative queries to the user . moreover , the approach can encourage sparsity in the parameter space , in order to favor the assessment of utility towards combinations of weights that concentrate on just few features . we present a mixed integer linear programming formulation and show how our approach compares favourably with bayesian preference elicitation alternatives and easily scales to realistic datasets .

evaluation the efficiency of artificial bee colony and the firefly algorithm in solving the continuous optimization problem
now the meta-heuristic algorithms have been used vastly in solving the problem of continuous optimization . in this paper the artificial bee colony ( abc ) algorithm and the firefly algorithm ( fa ) are valuated . and for presenting the efficiency of the algorithms and also for more analysis of them , the continuous optimization problems which are of the type of the problems of vast limit of answer and the close optimized points are tested . so , in this paper the efficiency of the abc algorithm and fa are presented for solving the continuous optimization problems and also the said algorithms are studied from the accuracy in reaching the optimized solution and the resulting time and the reliability of the optimized answer points of view .

representing context-sensitive knowledge in a network formalism : a preliminary report
automated decision making is often complicated by the complexity of the knowledge involved . much of this complexity arises from the context sensitive variations of the underlying phenomena . we propose a framework for representing descriptive , context-sensitive knowledge . our approach attempts to integrate categorical and uncertain knowledge in a network formalism . this paper outlines the basic representation constructs , examines their expressiveness and efficiency , and discusses the potential applications of the framework .

efficient local search limitation strategy for single machine total weighted tardiness scheduling with sequence-dependent setup times
this paper concerns the single machine total weighted tardiness scheduling with sequence-dependent setup times , usually referred as $ 1|s_ { ij } |\sum w_jt_j $ . in this $ \mathcal { np } $ -hard problem , each job has an associated processing time , due date and a weight . for each pair of jobs $ i $ and $ j $ , there may be a setup time before starting to process $ j $ in case this job is scheduled immediately after $ i $ . the objective is to determine a schedule that minimizes the total weighted tardiness , where the tardiness of a job is equal to its completion time minus its due date , in case the job is completely processed only after its due date , and is equal to zero otherwise . due to its complexity , this problem is most commonly solved by heuristics . the aim of this work is to develop a simple yet effective limitation strategy that speeds up the local search procedure without a significant loss in the solution quality . such strategy consists of a filtering mechanism that prevents unpromising moves to be evaluated . the proposed strategy has been embedded in a local search based metaheuristic from the literature and tested in classical benchmark instances . computational experiments revealed that the limitation strategy enabled the metaheuristic to be extremely competitive when compared to other algorithms from the literature , since it allowed the use of a large number of neighborhood structures without a significant increase in the cpu time and , consequently , high quality solutions could be achieved in a matter of seconds . in addition , we analyzed the effectiveness of the proposed strategy in two other well-known metaheuristics . further experiments were also carried out on benchmark instances of problem $ 1|s_ { ij } |\sum t_j $ .

the structure of narrative : the case of film scripts
we analyze the style and structure of story narrative using the case of film scripts . the practical importance of this is noted , especially the need to have support tools for television movie writing . we use the casablanca film script , and scripts from six episodes of csi ( crime scene investigation ) . for analysis of style and structure , we quantify various central perspectives discussed in mckee 's book , `` story : substance , structure , style , and the principles of screenwriting '' . film scripts offer a useful point of departure for exploration of the analysis of more general narratives . our methodology , using correspondence analysis , and hierarchical clustering , is innovative in a range of areas that we discuss . in particular this work is groundbreaking in taking the qualitative analysis of mckee and grounding this analysis in a quantitative and algorithmic framework .

reasons and means to model preferences as incomplete
literature involving preferences of artificial agents or human beings often assume their preferences can be represented using a complete transitive binary relation . much has been written however on different models of preferences . we review some of the reasons that have been put forward to justify more complex modeling , and review some of the techniques that have been proposed to obtain models of such preferences .

systems of natural-language-facilitated human-robot cooperation : a review
natural-language-facilitated human-robot cooperation ( nlc ) , in which natural language ( nl ) is used to share knowledge between a human and a robot for conducting intuitive human-robot cooperation ( hrc ) , is continuously developing in the recent decade . currently , nlc is used in several robotic domains such as manufacturing , daily assistance and health caregiving . it is necessary to summarize current nlc-based robotic systems and discuss the future developing trends , providing helpful information for future nlc research . in this review , we first analyzed the driving forces behind the nlc research . regarding to a robot s cognition level during the cooperation , the nlc implementations then were categorized into four types { nl-based control , nl-based robot training , nl-based task execution , nl-based social companion } for comparison and discussion . last based on our perspective and comprehensive paper review , the future research trends were discussed .

deep value networks learn to evaluate and iteratively refine structured outputs
we approach structured output prediction by optimizing a deep value network ( dvn ) to precisely estimate the task loss on different output configurations for a given input . once the model is trained , we perform inference by gradient descent on the continuous relaxations of the output variables to find outputs with promising scores from the value network . when applied to image segmentation , the value network takes an image and a segmentation mask as inputs and predicts a scalar estimating the intersection over union between the input and ground truth masks . for multi-label classification , the dvn 's objective is to correctly predict the f1 score for any potential label configuration . the dvn framework achieves the state-of-the-art results on multi-label prediction and image segmentation benchmarks .

cas-cnn : a deep convolutional neural network for image compression artifact suppression
lossy image compression algorithms are pervasively used to reduce the size of images transmitted over the web and recorded on data storage media . however , we pay for their high compression rate with visual artifacts degrading the user experience . deep convolutional neural networks have become a widespread tool to address high-level computer vision tasks very successfully . recently , they have found their way into the areas of low-level computer vision and image processing to solve regression problems mostly with relatively shallow networks . we present a novel 12-layer deep convolutional network for image compression artifact suppression with hierarchical skip connections and a multi-scale loss function . we achieve a boost of up to 1.79 db in psnr over ordinary jpeg and an improvement of up to 0.36 db over the best previous convnet result . we show that a network trained for a specific quality factor ( qf ) is resilient to the qf used to compress the input image - a single network trained for qf 60 provides a psnr gain of more than 1.5 db over the wide qf range from 40 to 76 .

constructing lower probabilities
an elaboration of dempster 's method of constructing belief functions suggests a broadly applicable strategy for constructing lower probabilities under a variety of evidentiary constraints .

ecmdd : evidential c-medoids clustering with multiple prototypes
in this work , a new prototype-based clustering method named evidential c-medoids ( ecmdd ) , which belongs to the family of medoid-based clustering for proximity data , is proposed as an extension of fuzzy c-medoids ( fcmdd ) on the theoretical framework of belief functions . in the application of fcmdd and original ecmdd , a single medoid ( prototype ) , which is supposed to belong to the object set , is utilized to represent one class . for the sake of clarity , this kind of ecmdd using a single medoid is denoted by secmdd . in real clustering applications , using only one pattern to capture or interpret a class may not adequately model different types of group structure and hence limits the clustering performance . in order to address this problem , a variation of ecmdd using multiple weighted medoids , denoted by wecmdd , is presented . unlike secmdd , in wecmdd objects in each cluster carry various weights describing their degree of representativeness for that class . this mechanism enables each class to be represented by more than one object . experimental results in synthetic and real data sets clearly demonstrate the superiority of secmdd and wecmdd . moreover , the clustering results by wecmdd can provide richer information for the inner structure of the detected classes with the help of prototype weights .

ice : enabling non-experts to build models interactively for large-scale lopsided problems
quick interaction between a human teacher and a learning machine presents numerous benefits and challenges when working with web-scale data . the human teacher guides the machine towards accomplishing the task of interest . the learning machine leverages big data to find examples that maximize the training value of its interaction with the teacher . when the teacher is restricted to labeling examples selected by the machine , this problem is an instance of active learning . when the teacher can provide additional information to the machine ( e.g. , suggestions on what examples or predictive features should be used ) as the learning task progresses , then the problem becomes one of interactive learning . to accommodate the two-way communication channel needed for efficient interactive learning , the teacher and the machine need an environment that supports an interaction language . the machine can access , process , and summarize more examples than the teacher can see in a lifetime . based on the machine 's output , the teacher can revise the definition of the task or make it more precise . both the teacher and the machine continuously learn and benefit from the interaction . we have built a platform to ( 1 ) produce valuable and deployable models and ( 2 ) support research on both the machine learning and user interface challenges of the interactive learning problem . the platform relies on a dedicated , low-latency , distributed , in-memory architecture that allows us to construct web-scale learning machines with quick interaction speed . the purpose of this paper is to describe this architecture and demonstrate how it supports our research efforts . preliminary results are presented as illustrations of the architecture but are not the primary focus of the paper .

assessment of effective parameters on dilution using approximate reasoning methods in longwall mining method , iran coal mines
approximately more than 90 % of all coal production in iranian underground mines is derived directly longwall mining method . out of seam dilution is one of the essential problems in these mines . therefore the dilution can impose the additional cost of mining and milling . as a result , recognition of the effective parameters on the dilution has a remarkable role in industry . in this way , this paper has analyzed the influence of 13 parameters ( attributed variables ) versus the decision attribute ( dilution value ) , so that using two approximate reasoning methods , namely rough set theory ( rst ) and self organizing neuro- fuzzy inference system ( sonfis ) the best rules on our collected data sets has been extracted . the other benefit of later methods is to predict new unknown cases . so , the reduced sets ( reducts ) by rst have been obtained . therefore the emerged results by utilizing mentioned methods shows that the high sensitive variables are thickness of layer , length of stope , rate of advance , number of miners , type of advancing .

propagation of belief functions : a distributed approach
in this paper , we describe a scheme for propagating belief functions in certain kinds of trees using only local computations . this scheme generalizes the computational scheme proposed by shafer and logan1 for diagnostic trees of the type studied by gordon and shortliffe , and the slightly more general scheme given by shafer for hierarchical evidence . it also generalizes the scheme proposed by pearl for bayesian causal trees ( see shenoy and shafer ) . pearl 's causal trees and gordon and shortliffe 's diagnostic trees are both ways of breaking the evidence that bears on a large problem down into smaller items of evidence that bear on smaller parts of the problem so that these smaller problems can be dealt with one at a time . this localization of effort is often essential in order to make the process of probability judgment feasible , both for the person who is making probability judgments and for the machine that is combining them . the basic structure for our scheme is a type of tree that generalizes both pearl 's and gordon and shortliffe 's trees . trees of this general type permit localized computation in pearl 's sense . they are based on qualitative judgments of conditional independence . we believe that the scheme we describe here will prove useful in expert systems . it is now clear that the successful propagation of probabilities or certainty factors in expert systems requires much more structure than can be provided in a pure production-system framework . bayesian schemes , on the other hand , often make unrealistic demands for structure . the propagation of belief functions in trees and more general networks stands on a middle ground where some sensible and useful things can be done . we would like to emphasize that the basic idea of local computation for propagating probabilities is due to judea pearl . it is a very innovative idea ; we do not believe that it can be found in the bayesian literature prior to pearl 's work . we see our contribution as extending the usefulness of pearl 's idea by generalizing it from bayesian probabilities to belief functions . in the next section , we give a brief introduction to belief functions . the notions of qualitative independence for partitions and a qualitative markov tree are introduced in section iii . finally , in section iv , we describe a scheme for propagating belief functions in qualitative markov trees .

improved branch-and-bound for low autocorrelation binary sequences
the low autocorrelation binary sequence problem has applications in telecommunications , is of theoretical interest to physicists , and has inspired many optimisation researchers . metaheuristics for the problem have progressed greatly in recent years but complete search has not progressed since a branch-and-bound method of 1996. in this paper we find four ways of improving branch-and-bound , leading to a tighter relaxation , faster convergence to optimality , and better empirical scalability .

creating a new ontology : a modular approach
creating a new ontology : a modular approach

neural and synaptic array transceiver : a brain-inspired computing framework for embedded learning
embedded , continual learning for autonomous and adaptive behavior is a key application of neuromorphic hardware designed to mimic the dynamics and architecture of biological neural networks . however , neuromorphic implementations of embedded learning at large scales that are both flexible and efficient have been hindered by a lack of a suitable algorithmic framework . as a result , most neuromorphic hardware are trained off-line on large clusters of dedicated processors or gpus and transferred post hoc to the device . we address this by introducing the neural and synaptic array transceiver ( nsat ) , a neuromorphic computational framework facilitating flexible and efficient embedded learning . nsat supports event-driven supervised , unsupervised and reinforcement learning algorithms including deep learning . we demonstrate the nsat in a wide range of tasks , including the simulation of mihalas-niebur neuron , dynamic neural fields , event-driven random back-propagation for event-based deep learning , event-based contrastive divergence for unsupervised learning , and voltage-based learning rules for sequence learning . we anticipate that this contribution will establish the foundation for a new generation of devices enabling adaptive mobile systems , wearable devices , and robots with data-driven autonomy .

efficient attack graph analysis through approximate inference
attack graphs provide compact representations of the attack paths that an attacker can follow to compromise network resources by analysing network vulnerabilities and topology . these representations are a powerful tool for security risk assessment . bayesian inference on attack graphs enables the estimation of the risk of compromise to the system 's components given their vulnerabilities and interconnections , and accounts for multi-step attacks spreading through the system . whilst static analysis considers the risk posture at rest , dynamic analysis also accounts for evidence of compromise , e.g . from siem software or forensic investigation . however , in this context , exact bayesian inference techniques do not scale well . in this paper we show how loopy belief propagation - an approximate inference technique - can be applied to attack graphs , and that it scales linearly in the number of nodes for both static and dynamic analysis , making such analyses viable for larger networks . we experiment with different topologies and network clustering on synthetic bayesian attack graphs with thousands of nodes to show that the algorithm 's accuracy is acceptable and converge to a stable solution . we compare sequential and parallel versions of loopy belief propagation with exact inference techniques for both static and dynamic analysis , showing the advantages of approximate inference techniques to scale to larger attack graphs .

computing reference classes
for any system with limited statistical knowledge , the combination of evidence and the interpretation of sampling information require the determination of the right reference class ( or of an adequate one ) . the present note ( 1 ) discusses the use of reference classes in evidential reasoning , and ( 2 ) discusses implementations of kyburg 's rules for reference classes . this paper contributes the first frank discussion of how much of kyburg 's system is needed to be powerful , how much can be computed effectively , and how much is philosophical fat .

software agents : completing patterns and constructing user interfaces
to support the goal of allowing users to record and retrieve information , this paper describes an interactive note-taking system for pen-based computers with two distinctive features . first , it actively predicts what the user is going to write . second , it automatically constructs a custom , button-box user interface on request . the system is an example of a learning-apprentice software- agent . a machine learning component characterizes the syntax and semantics of the user 's information . a performance system uses this learned information to generate completion strings and construct a user interface . description of online appendix : people like to record information . doing this on paper is initially efficient , but lacks flexibility . recording information on a computer is less efficient but more powerful . in our new note taking softwre , the user records information directly on a computer . behind the interface , an agent acts for the user . to help , it provides defaults and constructs a custom user interface . the demonstration is a quicktime movie of the note taking agent in action . the file is a binhexed self-extracting archive . macintosh utilities for binhex are available from mac.archive.umich.edu . quicktime is available from ftp.apple.com in the dts/mac/sys.soft/quicktime .

redundancy , deduction schemes , and minimum-size bases for association rules
association rules are among the most widely employed data analysis methods in the field of data mining . an association rule is a form of partial implication between two sets of binary variables . in the most common approach , association rules are parameterized by a lower bound on their confidence , which is the empirical conditional probability of their consequent given the antecedent , and/or by some other parameter bounds such as `` support '' or deviation from independence . we study here notions of redundancy among association rules from a fundamental perspective . we see each transaction in a dataset as an interpretation ( or model ) in the propositional logic sense , and consider existing notions of redundancy , that is , of logical entailment , among association rules , of the form `` any dataset in which this first rule holds must obey also that second rule , therefore the second is redundant '' . we discuss several existing alternative definitions of redundancy between association rules and provide new characterizations and relationships among them . we show that the main alternatives we discuss correspond actually to just two variants , which differ in the treatment of full-confidence implications . for each of these two notions of redundancy , we provide a sound and complete deduction calculus , and we show how to construct complete bases ( that is , axiomatizations ) of absolutely minimum size in terms of the number of rules . we explore finally an approach to redundancy with respect to several association rules , and fully characterize its simplest case of two partial premises .

truncating temporal differences : on the efficient implementation of td ( lambda ) for reinforcement learning
temporal difference ( td ) methods constitute a class of methods for learning predictions in multi-step prediction problems , parameterized by a recency factor lambda . currently the most important application of these methods is to temporal credit assignment in reinforcement learning . well known reinforcement learning algorithms , such as ahc or q-learning , may be viewed as instances of td learning . this paper examines the issues of the efficient and general implementation of td ( lambda ) for arbitrary lambda , for use with reinforcement learning algorithms optimizing the discounted sum of rewards . the traditional approach , based on eligibility traces , is argued to suffer from both inefficiency and lack of generality . the ttd ( truncated temporal differences ) procedure is proposed as an alternative , that indeed only approximates td ( lambda ) , but requires very little computation per action and can be used with arbitrary function representation methods . the idea from which it is derived is fairly simple and not new , but probably unexplored so far . encouraging experimental results are presented , suggesting that using lambda & gt 0 with the ttd procedure allows one to obtain a significant learning speedup at essentially the same cost as usual td ( 0 ) learning .

levels of integration between low-level reasoning and task planning
we provide a systematic analysis of levels of integration between discrete high-level reasoning and continuous low-level reasoning to address hybrid planning problems in robotics . we identify four distinct strategies for such an integration : ( i ) low-level checks are done for all possible cases in advance and then this information is used during plan generation , ( ii ) low-level checks are done exactly when they are needed during the search for a plan , ( iii ) first all plans are computed and then infeasible ones are filtered , and ( iv ) by means of replanning , after finding a plan , low-level checks identify whether it is infeasible or not ; if it is infeasible , a new plan is computed considering the results of previous low- level checks . we perform experiments on hybrid planning problems in robotic manipulation and legged locomotion domains considering these four methods of integration , as well as some of their combinations . we analyze the usefulness of levels of integration in these domains , both from the point of view of computational efficiency ( in time and space ) and from the point of view of plan quality relative to its feasibility . we discuss advantages and disadvantages of each strategy in the light of experimental results and provide some guidelines on choosing proper strategies for a given domain .

classification of approaches and challenges of frequent subgraphs mining in biological networks
understanding the structure and dynamics of biological networks is one of the important challenges in system biology . in addition , increasing amount of experimental data in biological networks necessitate the use of efficient methods to analyze these huge amounts of data . such methods require to recognize common patterns to analyze data . as biological networks can be modeled by graphs , the problem of common patterns recognition is equivalent with frequent sub graph mining in a set of graphs . in this paper , at first the challenges of frequent subgrpahs mining in biological networks are introduced and the existing approaches are classified for each challenge . then the algorithms are analyzed on the basis of the type of the approach they apply for each of the challenges .

sentence pair scoring : towards unified framework for text comprehension
we review the task of sentence pair scoring , popular in the literature in various forms - viewed as answer sentence selection , semantic text scoring , next utterance ranking , recognizing textual entailment , paraphrasing or e.g . a component of memory networks . we argue that all such tasks are similar from the model perspective and propose new baselines by comparing the performance of common ir metrics and popular convolutional , recurrent and attention-based neural models across many sentence pair scoring tasks and datasets . we discuss the problem of evaluating randomized models , propose a statistically grounded methodology , and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks . we introduce a unified open source software framework with easily pluggable models and tasks , which enables us to experiment with multi-task reusability of trained sentence model . we set a new state-of-art in performance on the ubuntu dialogue dataset .

explicit probabilistic models for databases and networks
recent work in data mining and related areas has highlighted the importance of the statistical assessment of data mining results . crucial to this endeavour is the choice of a non-trivial null model for the data , to which the found patterns can be contrasted . the most influential null models proposed so far are defined in terms of invariants of the null distribution . such null models can be used by computation intensive randomization approaches in estimating the statistical significance of data mining results . here , we introduce a methodology to construct non-trivial probabilistic models based on the maximum entropy ( maxent ) principle . we show how maxent models allow for the natural incorporation of prior information . furthermore , they satisfy a number of desirable properties of previously introduced randomization approaches . lastly , they also have the benefit that they can be represented explicitly . we argue that our approach can be used for a variety of data types . however , for concreteness , we have chosen to demonstrate it in particular for databases and networks .

inference with idempotent valuations
valuation based systems verifying an idempotent property are studied . a partial order is defined between the valuations giving them a lattice structure . then , two different strategies are introduced to represent valuations : as infimum of the most informative valuations or as supremum of the least informative ones . it is studied how to carry out computations with both representations in an efficient way . the particular cases of finite sets and convex polytopes are considered .

interactive debugging of knowledge bases
many ai applications rely on knowledge about a relevant real-world domain that is encoded by means of some logical knowledge base ( kb ) . the most essential benefit of logical kbs is the opportunity to perform automatic reasoning to derive implicit knowledge or to answer complex queries about the modeled domain . the feasibility of meaningful reasoning requires kbs to meet some minimal quality criteria such as logical consistency . without adequate tool assistance , the task of resolving violated quality criteria in kbs can be extremely tough even for domain experts , especially when the problematic kb includes a large number of logical formulas or comprises complicated logical formalisms . published non-interactive debugging systems often can not localize all possible faults ( incompleteness ) , suggest the deletion or modification of unnecessarily large parts of the kb ( non-minimality ) , return incorrect solutions which lead to a repaired kb not satisfying the imposed quality requirements ( unsoundness ) or suffer from poor scalability due to the inherent complexity of the kb debugging problem . even if a system is complete and sound and considers only minimal solutions , there are generally exponentially many solution candidates to select one from . however , any two repaired kbs obtained from these candidates differ in their semantics in terms of entailments and non-entailments . selection of just any of these repaired kbs might result in unexpected entailments , the loss of desired entailments or unwanted changes to the kb . this work proposes complete , sound and optimal methods for the interactive debugging of kbs that suggest the one ( minimally invasive ) error correction of the faulty kb that yields a repaired kb with exactly the intended semantics . users , e.g . domain experts , are involved in the debugging process by answering automatically generated queries about the intended domain .

deep learning evaluation using deep linguistic processing
we discuss problems with the standard approaches to evaluation for tasks like visual question answering , and argue that artificial data can be used to address these as a complement to current practice . we demonstrate that with the help of existing 'deep ' linguistic processing technology we are able to create challenging abstract datasets , which enable us to investigate the language understanding abilities of multimodal deep learning models in detail .

an odds ratio based inference engine
expert systems applications that involve uncertain inference can be represented by a multidimensional contingency table . these tables offer a general approach to inferring with uncertain evidence , because they can embody any form of association between any number of pieces of evidence and conclusions . ( simpler models may be required , however , if the number of pieces of evidence bearing on a conclusion is large . ) this paper presents a method of using these tables to make uncertain inferences without assumptions of conditional independence among pieces of evidence or heuristic combining rules . as evidence is accumulated , new joint probabilities are calculated so as to maintain any dependencies among the pieces of evidence that are found in the contingency table . the new conditional probability of the conclusion is then calculated directly from these new joint probabilities and the conditional probabilities in the contingency table .

query significance in databases via randomizations
many sorts of structured data are commonly stored in a multi-relational format of interrelated tables . under this relational model , exploratory data analysis can be done by using relational queries . as an example , in the internet movie database ( imdb ) a query can be used to check whether the average rank of action movies is higher than the average rank of drama movies . we consider the problem of assessing whether the results returned by such a query are statistically significant or just a random artifact of the structure in the data . our approach is based on randomizing the tables occurring in the queries and repeating the original query on the randomized tables . it turns out that there is no unique way of randomizing in multi-relational data . we propose several randomization techniques , study their properties , and show how to find out which queries or hypotheses about our data result in statistically significant information . we give results on real and generated data and show how the significance of some queries vary between different randomizations .

deep learning for ontology reasoning
in this work , we present a novel approach to ontology reasoning that is based on deep learning rather than logic-based formal reasoning . to this end , we introduce a new model for statistical relational learning that is built upon deep recursive neural networks , and give experimental evidence that it can easily compete with , or even outperform , existing logic-based reasoners on the task of ontology reasoning . more precisely , we compared our implemented system with one of the best logic-based ontology reasoners at present , rdfox , on a number of large standard benchmark datasets , and found that our system attained high reasoning quality , while being up to two orders of magnitude faster .

a principled analysis of merging operations in possibilistic logic
possibilistic logic offers a qualitative framework for representing pieces of information associated with levels of uncertainty of priority . the fusion of multiple sources information is discussed in this setting . different classes of merging operators are considered including conjunctive , disjunctive , reinforcement , adaptive and averaging operators . then we propose to analyse these classes in terms of postulates . this is done by first extending the postulate for merging classical bases to the case where priorites are avaialbe .

pfax : predictable feature analysis to perform control
predictable feature analysis ( pfa ) ( richthofer , wiskott , icmla 2015 ) is an algorithm that performs dimensionality reduction on high dimensional input signal . it extracts those subsignals that are most predictable according to a certain prediction model . we refer to these extracted signals as predictable features . in this work we extend the notion of pfa to take supplementary information into account for improving its predictions . such information can be a multidimensional signal like the main input to pfa , but is regarded external . that means it wo n't participate in the feature extraction - no features get extracted or composed of it . features will be exclusively extracted from the main input such that they are most predictable based on themselves and the supplementary information . we refer to this enhanced pfa as pfax ( pfa extended ) . even more important than improving prediction quality is to observe the effect of supplementary information on feature selection . pfax transparently provides insight how the supplementary information adds to prediction quality and whether it is valuable at all . finally we show how to invert that relation and can generate the supplementary information such that it would yield a certain desired outcome of the main signal . we apply this to a setting inspired by reinforcement learning and let the algorithm learn how to control an agent in an environment . with this method it is feasible to locally optimize the agent 's state , i.e . reach a certain goal that is near enough . we are preparing a follow-up paper that extends this method such that also global optimization is feasible .

colin : planning with continuous linear numeric change
in this paper we describe colin , a forward-chaining heuristic search planner , capable of reasoning with continuous linear numeric change , in addition to the full temporal semantics of pddl . through this work we make two advances to the state-of-the-art in terms of expressive reasoning capabilities of planners : the handling of continuous linear change , and the handling of duration-dependent effects in combination with duration inequalities , both of which require tightly coupled temporal and numeric reasoning during planning . colin combines ff-style forward chaining search , with the use of a linear program ( lp ) to check the consistency of the interacting temporal and numeric constraints at each state . the lp is used to compute bounds on the values of variables in each state , reducing the range of actions that need to be considered for application . in addition , we develop an extension of the temporal relaxed planning graph heuristic of crikey3 , to support reasoning directly with continuous change . we extend the range of task variables considered to be suitable candidates for specifying the gradient of the continuous numeric change effected by an action . finally , we explore the potential for employing mixed integer programming as a tool for optimising the timestamps of the actions in the plan , once a solution has been found . to support this , we further contribute a selection of extended benchmark domains that include continuous numeric effects . we present results for colin that demonstrate its scalability on a range of benchmarks , and compare to existing state-of-the-art planners .

learning efficient disambiguation
this dissertation analyses the computational properties of current performance-models of natural language parsing , in particular data oriented parsing ( dop ) , points out some of their major shortcomings and suggests suitable solutions . it provides proofs that various problems of probabilistic disambiguation are np-complete under instances of these performance-models , and it argues that none of these models accounts for attractive efficiency properties of human language processing in limited domains , e.g . that frequent inputs are usually processed faster than infrequent ones . the central hypothesis of this dissertation is that these shortcomings can be eliminated by specializing the performance-models to the limited domains . the dissertation addresses `` grammar and model specialization '' and presents a new framework , the ambiguity-reduction specialization ( ars ) framework , that formulates the necessary and sufficient conditions for successful specialization . the framework is instantiated into specialization algorithms and applied to specializing dop . novelties of these learning algorithms are 1 ) they limit the hypotheses-space to include only `` safe '' models , 2 ) are expressed as constrained optimization formulae that minimize the entropy of the training tree-bank given the specialized grammar , under the constraint that the size of the specialized model does not exceed a predefined maximum , and 3 ) they enable integrating the specialized model with the original one in a complementary manner . the dissertation provides experiments with initial implementations and compares the resulting specialized dop ( sdop ) models to the original dop models with encouraging results .

comparative study of data mining query languages
since formulation of inductive database ( idb ) problem , several data mining ( dm ) languages have been proposed , confirming that kdd process could be supported via inductive queries ( iq ) answering . this paper reviews the existing dm languages . we are presenting important primitives of the dm language and classifying our languages according to primitives ' satisfaction . in addition , we presented languages ' syntaxes and tried to apply each one to a database sample to test a set of kdd operations . this study allows us to highlight languages capabilities and limits , which is very useful for future work and perspectives .

function space analysis of deep learning representation layers
in this paper we propose a function space approach to representation learning and the analysis of the representation layers in deep learning architectures . we show how to compute a weak-type besov smoothness index that quantifies the geometry of the clustering in the feature space . this approach was already applied successfully to improve the performance of machine learning algorithms such as the random forest and tree-based gradient boosting . our experiments demonstrate that in well-known and well-performing trained networks , the besov smoothness of the training set , measured in the corresponding hidden layer feature map representation , increases from layer to layer . we also contribute to the understanding of generalization by showing how the besov smoothness of the representations , decreases as we add more mis-labeling to the training data . we hope this approach will contribute to the de-mystification of some aspects of deep learning .

a total uncertainty measure for d numbers based on belief intervals
as a generalization of dempster-shafer theory , the theory of d numbers is a new theoretical framework for uncertainty reasoning . measuring the uncertainty of knowledge or information represented by d numbers is an unsolved issue in that theory . in this paper , inspired by distance based uncertainty measures for dempster-shafer theory , a total uncertainty measure for a d number is proposed based on its belief intervals . the proposed total uncertainty measure can simultaneously capture the discord , and non-specificity , and non-exclusiveness involved in d numbers . and some basic properties of this total uncertainty measure , including range , monotonicity , generalized set consistency , are also presented .

a methodology for learning players ' styles from game records
we describe a preliminary investigation into learning a chess player 's style from game records . the method is based on attempting to learn features of a player 's individual evaluation function using the method of temporal differences , with the aid of a conventional chess engine architecture . some encouraging results were obtained in learning the styles of two recent chess world champions , and we report on our attempt to use the learnt styles to discriminate between the players from game records by trying to detect who was playing white and who was playing black . we also discuss some limitations of our approach and propose possible directions for future research . the method we have presented may also be applicable to other strategic games , and may even be generalisable to other domains where sequences of agents ' actions are recorded .

monaural audio speaker separation with source contrastive estimation
we propose an algorithm to separate simultaneously speaking persons from each other , the `` cocktail party problem '' , using a single microphone . our approach involves a deep recurrent neural networks regression to a vector space that is descriptive of independent speakers . such a vector space can embed empirically determined speaker characteristics and is optimized by distinguishing between speaker masks . we call this technique source-contrastive estimation . the methodology is inspired by negative sampling , which has seen success in natural language processing , where an embedding is learned by correlating and de-correlating a given input vector with output weights . although the matrix determined by the output weights is dependent on a set of known speakers , we only use the input vectors during inference . doing so will ensure that source separation is explicitly speaker-independent . our approach is similar to recent deep neural network clustering and permutation-invariant training research ; we use weighted spectral features and masks to augment individual speaker frequencies while filtering out other speakers . we avoid , however , the severe computational burden of other approaches with our technique . furthermore , by training a vector space rather than combinations of different speakers or differences thereof , we avoid the so-called permutation problem during training . our algorithm offers an intuitive , computationally efficient response to the cocktail party problem , and most importantly boasts better empirical performance than other current techniques .

`` conditional inter-causally independent '' node distributions , a property of `` noisy-or '' models
this paper examines the interdependence generated between two parent nodes with a common instantiated child node , such as two hypotheses sharing common evidence . the relation so generated has been termed `` intercausal . '' it is shown by construction that inter-causal independence is possible for binary distributions at one state of evidence . for such `` cici '' distributions , the two measures of inter-causal effect , `` multiplicative synergy '' and `` additive synergy '' are equal . the well known `` noisy-or '' model is an example of such a distribution . this introduces novel semantics for the noisy-or , as a model of the degree of conflict among competing hypotheses of a common observation .

etymo : a new discovery engine for ai research
we present etymo ( https : //etymo.io ) , a discovery engine to facilitate artificial intelligence ( ai ) research and development . it aims to help readers navigate a large number of ai-related papers published every week by using a novel form of search that finds relevant papers and displays related papers in a graphical interface . etymo constructs and maintains an adaptive similarity-based network of research papers as an all-purpose knowledge graph for ranking , recommendation , and visualisation . the network is constantly evolving and can learn from user feedback to adjust itself .

language models for image captioning : the quirks and what works
two recent approaches have achieved state-of-the-art results in image captioning . the first uses a pipelined process where a set of candidate words is generated by a convolutional neural network ( cnn ) trained on images , and then a maximum entropy ( me ) language model is used to arrange these words into a coherent sentence . the second uses the penultimate activation layer of the cnn as input to a recurrent neural network ( rnn ) that then generates the caption sequence . in this paper , we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art cnn as input . we examine issues in the different approaches , including linguistic irregularities , caption repetition , and data set overlap . by combining key aspects of the me and rnn methods , we achieve a new record performance over previously published results on the benchmark coco dataset . however , the gains we see in bleu do not translate to human judgments .

graphical probabilistic routing model for obs networks with realistic traffic scenario
burst contention is a well-known challenging problem in optical burst switching ( obs ) networks . contention resolution approaches are always reactive and attempt to minimize the blr based on local information available at the core node . on the other hand , a proactive approach that avoids burst losses before they occur is desirable . to reduce the probability of burst contention , a more robust routing algorithm than the shortest path is needed . this paper proposes a new routing mechanism for jet-based obs networks , called graphical probabilistic routing model ( gprm ) that selects less utilized links , on a hop-by-hop basis by using a bayesian network . we assume no wavelength conversion and no buffering to be available at the core nodes of the obs network . we simulate the proposed approach under dynamic load to demonstrate that it reduces the burst loss ratio ( blr ) compared to static approaches by using network simulator 2 ( ns-2 ) on nsfnet network topology and with realistic traffic matrix . simulation results clearly show that the proposed approach outperforms static approaches in terms of blr .

comparing system dynamics and agent-based simulation for tumour growth and its interactions with effector cells
there is little research concerning comparisons and combination of system dynamics simulation ( sds ) and agent based simulation ( abs ) . abs is a paradigm used in many levels of abstraction , including those levels covered by sds . we believe that the establishment of frameworks for the choice between these two simulation approaches would contribute to the simulation research . hence , our work aims for the establishment of directions for the choice between sds and abs approaches for immune system-related problems . previously , we compared the use of abs and sds for modelling agents ' behaviour in an environment with nomovement or interactions between these agents . we concluded that for these types of agents it is preferable to use sds , as it takes up less computational resources and produces the same results as those obtained by the abs model . in order to move this research forward , our next research question is : if we introduce interactions between these agents will sds still be the most appropriate paradigm to be used ? to answer this question for immune system simulation problems , we will use , as case studies , models involving interactions between tumour cells and immune effector cells . experiments show that there are cases where sds and abs can not be used interchangeably , and therefore , their comparison is not straightforward .

a formal framework of virtual organisations as agent societies
we propose a formal framework that supports a model of agent-based virtual organisations ( vos ) for service grids and provides an associated operational model for the creation of vos . the framework is intended to be used for describing different service grid applications based on multiple agents and , as a result , it abstracts away from any realisation choices of the service grid application , the agents involved to support the applications and their interactions . within the proposed framework vos are seen as emerging from societies of agents , where agents are abstractly characterised by goals and roles they can play within vos . in turn , vos are abstractly characterised by the agents participating in them with specific roles , as well as the workflow of services and corresponding contracts suitable for achieving the goals of the participating agents . we illustrate the proposed framework with an earth observation scenario .

spectral ranking using seriation
we describe a seriation algorithm for ranking a set of items given pairwise comparisons between these items . intuitively , the algorithm assigns similar rankings to items that compare similarly with all others . it does so by constructing a similarity matrix from pairwise comparisons , using seriation methods to reorder this matrix and construct a ranking . we first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order . we then show that ranking reconstruction is still exact when some pairwise comparisons are corrupted or missing , and that seriation based spectral ranking is more robust to noise than classical scoring methods . finally , we bound the ranking error when only a random subset of the comparions are observed . an additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems . experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods .

symbolic computing with incremental mindmaps to manage and mine data streams - some applications
in our understanding , a mind-map is an adaptive engine that basically works incrementally on the fundament of existing transactional streams . generally , mind-maps consist of symbolic cells that are connected with each other and that become either stronger or weaker depending on the transactional stream . based on the underlying biologic principle , these symbolic cells and their connections as well may adaptively survive or die , forming different cell agglomerates of arbitrary size . in this work , we intend to prove mind-maps ' eligibility following diverse application scenarios , for example being an underlying management system to represent normal and abnormal traffic behaviour in computer networks , supporting the detection of the user behaviour within search engines , or being a hidden communication layer for natural language interaction .

representing and combining partially specified cpts
this paper extends previous work with network fragments and situation-specific network construction . we formally define the asymmetry network , an alternative representation for a conditional probability table . we also present an object-oriented representation for partially specified asymmetry networks . we show that the representation is parsimonious . we define an algebra for the elements of the representation that allows us to 'factor ' any cpt and to soundly combine the partially specified asymmetry networks .

query complexity of tournament solutions
a directed graph where there is exactly one edge between every pair of vertices is called a { \em tournament } . finding the `` best '' set of vertices of a tournament is a well studied problem in social choice theory . a { \em tournament solution } takes a tournament as input and outputs a subset of vertices of the input tournament . however , in many applications , for example , choosing the best set of drugs from a given set of drugs , the edges of the tournament are given only implicitly and knowing the orientation of an edge is costly . in such scenarios , we would like to know the best set of vertices ( according to some tournament solution ) by `` querying '' as few edges as possible . we , in this paper , precisely study this problem for commonly used tournament solutions : given an oracle access to the edges of a tournament t , find $ f ( t ) $ by querying as few edges as possible , for a tournament solution f. we first show that the set of condorcet non-losers in a tournament can be found by querying $ 2n-\lfloor \log n \rfloor -2 $ edges only and this is tight in the sense that every algorithm for finding the set of condorcet non-losers needs to query at least $ 2n-\lfloor \log n \rfloor -2 $ edges in the worst case , where $ n $ is the number of vertices in the input tournament . we then move on to study other popular tournament solutions and show that any algorithm for finding the copeland set , the slater set , the markov set , the bipartisan set , the uncovered set , the banks set , and the top cycle must query $ \omega ( n^2 ) $ edges in the worst case . on the positive side , we are able to circumvent our strong query complexity lower bound results by proving that , if the size of the top cycle of the input tournament is at most $ k $ , then we can find all the tournament solutions mentioned above by querying $ o ( nk + \frac { n\log n } { \log ( 1-\frac { 1 } { k } ) } ) $ edges only .

utility-based control for computer vision
several key issues arise in implementing computer vision recognition of world objects in terms of bayesian networks . computational efficiency is a driving force . perceptual networks are very deep , typically fifteen levels of structure . images are wide , e.g. , an unspecified-number of edges may appear anywhere in an image 512 x 512 pixels or larger . for efficiency , we dynamically instantiate hypotheses of observed objects . the network is not fixed , but is created incrementally at runtime . generation of hypotheses of world objects and indexing of models for recognition are important , but they are not considered here [ 4,11 ] . this work is aimed at near-term implementation with parallel computation in a radar surveillance system , adries [ 5 , 15 ] , and a system for industrial part recognition , successor [ 2 ] . for many applications , vision must be faster to be practical and so efficiently controlling the machine vision process is critical . perceptual operators may scan megapixels and may require minutes of computation time . it is necessary to avoid unnecessary sensor actions and computation . parallel computation is available at several levels of processor capability . the potential for parallel , distributed computation for high-level vision means distributing non-homogeneous computations . this paper addresses the problem of task control in machine vision systems based on bayesian probability models . we separate control and inference to extend the previous work [ 3 ] to maximize utility instead of probability . maximizing utility allows adopting perceptual strategies for efficient information gathering with sensors and analysis of sensor data . results of controlling machine vision via utility to recognize military situations are presented in this paper . future work extends this to industrial part recognition for successor .

$ \ell_1 $ regularized gradient temporal-difference learning
in this paper , we study the temporal difference ( td ) learning with linear value function approximation . it is well known that most td learning algorithms are unstable with linear function approximation and off-policy learning . recent development of gradient td ( gtd ) algorithms has addressed this problem successfully . however , the success of gtd algorithms requires a set of well chosen features , which are not always available . when the number of features is huge , the gtd algorithms might face the problem of overfitting and being computationally expensive . to cope with this difficulty , regularization techniques , in particular $ \ell_1 $ regularization , have attracted significant attentions in developing td learning algorithms . the present work combines the gtd algorithms with $ \ell_1 $ regularization . we propose a family of $ \ell_1 $ regularized gtd algorithms , which employ the well known soft thresholding operator . we investigate convergence properties of the proposed algorithms , and depict their performance with several numerical experiments .

conjunctive query answering for the description logic shiq
conjunctive queries play an important role as an expressive query language for description logics ( dls ) . although modern dls usually provide for transitive roles , conjunctive query answering over dl knowledge bases is only poorly understood if transitive roles are admitted in the query . in this paper , we consider unions of conjunctive queries over knowledge bases formulated in the prominent dl shiq and allow transitive roles in both the query and the knowledge base . we show decidability of query answering in this setting and establish two tight complexity bounds : regarding combined complexity , we prove that there is a deterministic algorithm for query answering that needs time single exponential in the size of the kb and double exponential in the size of the query , which is optimal . regarding data complexity , we prove containment in co-np .

learning robocup-keepaway with kernels
we apply kernel-based methods to solve the difficult reinforcement learning problem of 3vs2 keepaway in robocup simulated soccer . key challenges in keepaway are the high-dimensionality of the state space ( rendering conventional discretization-based function approximation like tilecoding infeasible ) , the stochasticity due to noise and multiple learning agents needing to cooperate ( meaning that the exact dynamics of the environment are unknown ) and real-time learning ( meaning that an efficient online implementation is required ) . we employ the general framework of approximate policy iteration with least-squares-based policy evaluation . as underlying function approximator we consider the family of regularization networks with subset of regressors approximation . the core of our proposed solution is an efficient recursive implementation with automatic supervised selection of relevant basis functions . simulation results indicate that the behavior learned through our approach clearly outperforms the best results obtained earlier with tilecoding by stone et al . ( 2005 ) .

constraint solvers for user interface layout
constraints have played an important role in the construction of guis , where they are mainly used to define the layout of the widgets . resizing behavior is very important in guis because areas have domain specific parameters such as form the resizing of windows . if linear objective function is used and window is resized then error is not distributed equally . to distribute the error equally , a quadratic objective function is introduced . different algorithms are widely used for solving linear constraints and quadratic problems in a variety of different scientific areas . the linear relxation , kaczmarz , direct and linear programming methods are common methods for solving linear constraints for gui layout . the interior point and active set methods are most commonly used techniques to solve quadratic programming problems . current constraint solvers designed for gui layout do not use interior point methods for solving a quadratic objective function subject to linear equality and inequality constraints . in this paper , performance aspects and the convergence speed of interior point and active set methods are compared along with one most commonly used linear programming method when they are implemented for graphical user interface layout . the performance and convergence of the proposed algorithms are evaluated empirically using randomly generated ui layout specifications of various sizes . the results show that the interior point algorithms perform significantly better than the simplex method and qoca-solver , which uses the active set method implementation for solving quadratic optimization .

reasoning within fuzzy description logics
description logics ( dls ) are suitable , well-known , logics for managing structured knowledge . they allow reasoning about individuals and well defined concepts , i.e. , set of individuals with common properties . the experience in using dls in applications has shown that in many cases we would like to extend their capabilities . in particular , their use in the context of multimedia information retrieval ( mir ) leads to the convincement that such dls should allow the treatment of the inherent imprecision in multimedia object content representation and retrieval . in this paper we will present a fuzzy extension of alc , combining zadeh 's fuzzy logic with a classical dl . in particular , concepts becomes fuzzy and , thus , reasoning about imprecise concepts is supported . we will define its syntax , its semantics , describe its properties and present a constraint propagation calculus for reasoning in it .

modelling reactive and proactive behaviour in simulation
this research investigated the simulation model behaviour of a traditional and combined discrete event as well as agent based simulation models when modelling human reactive and proactive behaviour in human centric complex systems . a departmental store was chosen as human centric complex case study where the operation system of a fitting room in womenswear department was investigated . we have looked at ways to determine the efficiency of new management policies for the fitting room operation through simulating the reactive and proactive behaviour of staff towards customers . once development of the simulation models and their verification had been done , we carried out a validation experiment in the form of a sensitivity analysis . subsequently , we executed a statistical analysis where the mixed reactive and proactive behaviour experimental results were compared with some reactive experimental results from previously published works . generally , this case study discovered that simple proactive individual behaviour could be modelled in both simulation models . in addition , we found the traditional discrete event model performed similar in the simulation model output compared to the combined discrete event and agent based simulation when modelling similar human behaviour .

self-organizing traffic lights : a realistic simulation
we have previously shown in an abstract simulation ( gershenson , 2005 ) that self-organizing traffic lights can improve greatly traffic flow for any density . in this paper , we extend these results to a realistic setting , implementing self-organizing traffic lights in an advanced traffic simulator using real data from a brussels avenue . on average , for different traffic densities , travel waiting times are reduced by 50 % compared to the current green wave method .

the price of anarchy in auctions
this survey outlines a general and modular theory for proving approximation guarantees for equilibria of auctions in complex settings . this theory complements traditional economic techniques , which generally focus on exact and optimal solutions and are accordingly limited to relatively stylized settings . we highlight three user-friendly analytical tools : smoothness-type inequalities , which immediately yield approximation guarantees for many auction formats of interest in the special case of complete information and deterministic strategies ; extension theorems , which extend such guarantees to randomized strategies , no-regret learning outcomes , and incomplete-information settings ; and composition theorems , which extend such guarantees from simpler to more complex auctions . combining these tools yields tight worst-case approximation guarantees for the equilibria of many widely-used auction formats .

dd-eba : an algorithm for determining the number of neighbors in cost estimation by analogy using distance distributions
case based reasoning and particularly estimation by analogy , has been used in a number of problem-solving areas , such as cost estimation . conventional methods , despite the lack of a sound criterion for choosing nearest projects , were based on estimation using a fixed and predetermined number of neighbors from the entire set of historical instances . this approach puts boundaries to the estimation ability of such algorithms , for they do not take into consideration that every project under estimation is unique and requires different handling . the notion of distributions of distances together with a distance metric for distributions help us to adapt the proposed method ( we call it dd-eba ) each time to a specific case that is to be estimated without loosing in prediction power or computational cost . the results of this paper show that the proposed technique achieves the above idea in a very efficient way .

modelling and simulating retail management practices : a first approach
multi-agent systems offer a new and exciting way of understanding the world of work . we apply agent-based modeling and simulation to investigate a set of problems in a retail context . specifically , we are working to understand the relationship between people management practices on the shop-floor and retail performance . despite the fact we are working within a relatively novel and complex domain , it is clear that using an agent-based approach offers great potential for improving organizational capabilities in the future . our multi-disciplinary research team has worked closely with one of the uk 's top ten retailers to collect data and build an understanding of shop-floor operations and the key actors in a department ( customers , staff , and managers ) . based on this case study we have built and tested our first version of a retail branch agent-based simulation model where we have focused on how we can simulate the effects of people management practices on customer satisfaction and sales . in our experiments we have looked at employee development and cashier empowerment as two examples of shop floor management practices . in this paper we describe the underlying conceptual ideas and the features of our simulation model . we present a selection of experiments we have conducted in order to validate our simulation model and to show its potential for answering `` what-if '' questions in a retail context . we also introduce a novel performance measure which we have created to quantify customers ' satisfaction with service , based on their individual shopping experiences .

an influence-based fast preceding questionnaire model for elderly assessments
to improve the efficiency of elderly assessments , an influence-based fast preceding questionnaire model ( fpqm ) is proposed . compared with traditional assessments , the fpqm optimizes questionnaires by reordering their attributes . the values of low-ranking attributes can be predicted by the values of the high-ranking attributes . therefore , the number of attributes can be reduced without redesigning the questionnaires . a new function for calculating the influence of the attributes is proposed based on probability theory . reordering and reducing algorithms are given based on the attributes ' influences . the model is verified through a practical application . the practice in an elderly-care company shows that the fpqm can reduce the number of attributes by 90.56 % with a prediction accuracy of 98.39 % . compared with other methods , such as the expert knowledge , rough set and c4.5 methods , the fpqm achieves the best performance . in addition , the fpqm can also be applied to other questionnaires .

gradual tuning : a better way of fine tuning the parameters of a deep neural network
in this paper we present an alternative strategy for fine-tuning the parameters of a network . we named the technique gradual tuning . once trained on a first task , the network is fine-tuned on a second task by modifying a progressively larger set of the network 's parameters . we test gradual tuning on different transfer learning tasks , using networks of different sizes trained with different regularization techniques . the result shows that compared to the usual fine tuning , our approach significantly reduces catastrophic forgetting of the initial task , while still retaining comparable if not better performance on the new task .

new ideas for brain modelling
this paper describes some biologically-inspired processes that could be used to build the sort of networks that we associate with the human brain . new to this paper , a 'refined ' neuron will be proposed . this is a group of neurons that by joining together can produce a more analogue system , but with the same level of control and reliability that a binary neuron would have . with this new structure , it will be possible to think of an essentially binary system in terms of a more variable set of values . the paper also shows how recent research associated with the new model , can be combined with established theories , to produce a more complete picture . the propositions are largely in line with conventional thinking , but possibly with one or two more radical suggestions . an earlier cognitive model can be filled in with more specific details , based on the new research results , where the components appear to fit together almost seamlessly . the intention of the research has been to describe plausible 'mechanical ' processes that can produce the appropriate brain structures and mechanisms , but that could be used without the magical 'intelligence ' part that is still not fully understood . there are also some important updates from an earlier version of this paper .

continual reinforcement learning with complex synapses
unlike humans , who are capable of continual learning over their lifetimes , artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting , whereby new learning can lead to abrupt erasure of previously acquired knowledge . whereas in a neural network the parameters are typically modelled as scalar values , an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales . in this paper , we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity ( benna & fusi , 2016 ) , catastrophic forgetting can be mitigated at multiple timescales . in particular , we find that as well as enabling continual learning across sequential training of two simple tasks , it can also be used to overcome within-task forgetting by reducing the need for an experience replay database .

bisimulation-based approximate lifted inference
there has been a great deal of recent interest in methods for performing lifted inference ; however , most of this work assumes that the first-order model is given as input to the system . here , we describe lifted inference algorithms that determine symmetries and automatically lift the probabilistic model to speedup inference . in particular , we describe approximate lifted inference techniques that allow the user to trade off inference accuracy for computational efficiency by using a handful of tunable parameters , while keeping the error bounded . our algorithms are closely related to the graph-theoretic concept of bisimulation . we report experiments on both synthetic and real data to show that in the presence of symmetries , run-times for inference can be improved significantly , with approximate lifted inference providing orders of magnitude speedup over ground inference .

emotional intensity analysis in bipolar subjects
the massive availability of digital repositories of human thought opens radical novel way of studying the human mind . natural language processing tools and computational models have evolved such that many mental conditions are predicted by analysing speech . transcription of interviews and discourses are analyzed using syntactic , grammatical or sentiment analysis to infer the mental state . here we set to investigate if classification of bipolar and control subjects is possible . we develop the emotion intensity index based on the dictionary of affect , and find that subjects categories are distinguishable . using classical classification techniques we get more than 75\ % of labeling performance . these results sumed to previous studies show that current automated speech analysis is capable of identifying altered mental states towards a quantitative psychiatry .

probabilistic theorem proving
many representation schemes combining first-order logic and probability have been proposed in recent years . progress in unifying logical and probabilistic inference has been slower . existing methods are mainly variants of lifted variable elimination and belief propagation , neither of which take logical structure into account . we propose the first method that has the full power of both graphical model inference and first-order theorem proving ( in finite domains with herbrand interpretations ) . we first define probabilistic theorem proving , their generalization , as the problem of computing the probability of a logical formula given the probabilities or weights of a set of formulas . we then show how this can be reduced to the problem of lifted weighted model counting , and develop an efficient algorithm for the latter . we prove the correctness of this algorithm , investigate its properties , and show how it generalizes previous approaches . experiments show that it greatly outperforms lifted variable elimination when logical structure is present . finally , we propose an algorithm for approximate probabilistic theorem proving , and show that it can greatly outperform lifted belief propagation .

knowledge on treelike spaces
this paper presents a bimodal logic for reasoning about knowledge during knowledge acquisition . one of the modalities represents ( effort during ) non-deterministic time and the other represents knowledge . the semantics of this logic are tree-like spaces which are a generalization of semantics used for modeling branching time and historical necessity . a finite system of axiom schemes is shown to be canonically complete for the formentioned spaces . a characterization of the satisfaction relation implies the small model property and decidability for this system .

a generic global constraint based on mdds
the paper suggests the use of multi-valued decision diagrams ( mdds ) as the supporting data structure for a generic global constraint . we give an algorithm for maintaining generalized arc consistency ( gac ) on this constraint that amortizes the cost of the gac computation over a root-to-terminal path in the search tree . the technique used is an extension of the gac algorithm for the regular language constraint on finite length input . our approach adds support for skipped variables , maintains the reduced property of the mdd dynamically and provides domain entailment detection . finally we also show how to adapt the approach to constraint types that are closely related to mdds , such as aomdds and case dags .

approximating the backbone in the weighted maximum satisfiability problem
the weighted maximum satisfiability problem ( weighted max-sat ) is a np-hard problem with numerous applications arising in artificial intelligence . as an efficient tool for heuristic design , the backbone has been applied to heuristics design for many np-hard problems . in this paper , we investigated the computational complexity for retrieving the backbone in weighted max-sat and developed a new algorithm for solving this problem . we showed that it is intractable to retrieve the full backbone under the assumption that . moreover , it is intractable to retrieve a fixed fraction of the backbone as well . and then we presented a backbone guided local search ( bgls ) with walksat operator for weighted max-sat . bgls consists of two phases : the first phase samples the backbone information from local optima and the backbone phase conducts local search under the guideline of backbone . extensive experimental results on the benchmark showed that bgls outperforms the existing heuristics in both solution quality and runtime .

online event recognition from moving vessel trajectories
we present a system for online monitoring of maritime activity over streaming positions from numerous vessels sailing at sea . it employs an online tracking module for detecting important changes in the evolving trajectory of each vessel across time , and thus can incrementally retain concise , yet reliable summaries of its recent movement . in addition , thanks to its complex event recognition module , this system can also offer instant notification to marine authorities regarding emergency situations , such as risk of collisions , suspicious moves in protected zones , or package picking at open sea . not only did our extensive tests validate the performance , efficiency , and robustness of the system against scalable volumes of real-world and synthetically enlarged datasets , but its deployment against online feeds from vessels has also confirmed its capabilities for effective , real-time maritime surveillance .

the os* algorithm : a joint approach to exact optimization and sampling
most current sampling algorithms for high-dimensional distributions are based on mcmc techniques and are approximate in the sense that they are valid only asymptotically . rejection sampling , on the other hand , produces valid samples , but is unrealistically slow in high-dimension spaces . the os* algorithm that we propose is a unified approach to exact optimization and sampling , based on incremental refinements of a functional upper bound , which combines ideas of adaptive rejection sampling and of a* optimization search . we show that the choice of the refinement can be done in a way that ensures tractability in high-dimension spaces , and we present first experiments in two different settings : inference in high-order hmms and in large discrete graphical models .

evaluating real-time anomaly detection algorithms - the numenta anomaly benchmark
much of the world 's data is streaming , time-series data , where anomalies give significant information in critical situations ; examples abound in domains such as finance , it , security , medical , and energy . yet detecting anomalies in streaming data is a difficult task , requiring detectors to process data in real-time , not batches , and learn while simultaneously making predictions . there are no benchmarks to adequately test and score the efficacy of real-time anomaly detectors . here we propose the numenta anomaly benchmark ( nab ) , which attempts to provide a controlled and repeatable environment of open-source tools to test and measure anomaly detection algorithms on streaming data . the perfect detector would detect all anomalies as soon as possible , trigger no false alarms , work with real-world time-series data across a variety of domains , and automatically adapt to changing statistics . rewarding these characteristics is formalized in nab , using a scoring algorithm designed for streaming data . nab evaluates detectors on a benchmark dataset with labeled , real-world time-series data . we present these components , and give results and analyses for several open source , commercially-used algorithms . the goal for nab is to provide a standard , open source framework with which the research community can compare and evaluate different algorithms for detecting anomalies in streaming data .

efficient test selection in active diagnosis via entropy approximation
we consider the problem of diagnosing faults in a system represented by a bayesian network , where diagnosis corresponds to recovering the most likely state of unobserved nodes given the outcomes of tests ( observed nodes ) . finding an optimal subset of tests in this setting is intractable in general . we show that it is difficult even to compute the next most-informative test using greedy test selection , as it involves several entropy terms whose exact computation is intractable . we propose an approximate approach that utilizes the loopy belief propagation infrastructure to simultaneously compute approximations of marginal and conditional entropies on multiple subsets of nodes . we apply our method to fault diagnosis in computer networks , and show the algorithm to be very effective on realistic internet-like topologies . we also provide theoretical justification for the greedy test selection approach , along with some performance guarantees .

an intelligent location management approaches in gsm mobile network
location management refers to the problem of updating and searching the current location of mobile nodes in a wireless network . to make it efficient , the sum of update costs of location database must be minimized . previous work relying on fixed location databases is unable to fully exploit the knowledge of user mobility patterns in the system so as to achieve this minimization . the study presents an intelligent location management approach which has interacts between intelligent information system and knowledge-base technologies , so we can dynamically change the user patterns and reduce the transition between the vlr and hlr . the study provides algorithms are ability to handle location registration and call delivery

perception , attention , and resources : a decision-theoretic approach to graphics rendering
we describe work to control graphics rendering under limited computational resources by taking a decision-theoretic perspective on perceptual costs and computational savings of approximations . the work extends earlier work on the control of rendering by introducing methods and models for computing the expected cost associated with degradations of scene components . the expected cost is computed by considering the perceptual cost of degradations and a probability distribution over the attentional focus of viewers . we review the critical literature describing findings on visual search and attention , discuss the implications of the findings , and introduce models of expected perceptual cost . finally , we discuss policies that harness information about the expected cost of scene components .

limitations and alternatives for the evaluation of large-scale link prediction
link prediction , the problem of identifying missing links among a set of inter-related data entities , is a popular field of research due to its application to graph-like domains . producing consistent evaluations of the performance of the many link prediction algorithms being proposed can be challenging due to variable graph properties , such as size and density . in this paper we first discuss traditional data mining solutions which are applicable to link prediction evaluation , arguing about their capacity for producing faithful and useful evaluations . we also introduce an innovative modification to a traditional evaluation methodology with the goal of adapting it to the problem of evaluating link prediction algorithms when applied to large graphs , by tackling the problem of class imbalance . we empirically evaluate the proposed methodology and , building on these findings , make a case for its importance on the evaluation of large-scale graph processing .

a symbolic algebra for the computation of expected utilities in multiplicative influence diagrams
influence diagrams provide a compact graphical representation of decision problems . several algorithms for the quick computation of their associated expected utilities are available in the literature . however , often they rely on a full quantification of both probabilistic uncertainties and utility values . for problems where all random variables and decision spaces are finite and discrete , here we develop a symbolic way to calculate the expected utilities of influence diagrams that does not require a full numerical representation . within this approach expected utilities correspond to families of polynomials . after characterizing their polynomial structure , we develop an efficient symbolic algorithm for the propagation of expected utilities through the diagram and provide an implementation of this algorithm using a computer algebra system . we then characterize many of the standard manipulations of influence diagrams as transformations of polynomials . we also generalize the decision analytic framework of these diagrams by defining asymmetries as operations over the expected utility polynomials .

conditional probability tree estimation analysis and algorithms
we consider the problem of estimating the conditional probability of a label in time $ o ( \log n ) $ , where $ n $ is the number of possible labels . we analyze a natural reduction of this problem to a set of binary regression problems organized in a tree structure , proving a regret bound that scales with the depth of the tree . motivated by this analysis , we propose the first online algorithm which provably constructs a logarithmic depth tree on the set of labels to solve this problem . we test the algorithm empirically , showing that it works succesfully on a dataset with roughly $ 10^6 $ labels .

managing overstaying electric vehicles in park-and-charge facilities
with the increase in adoption of electric vehicles ( evs ) , proper utilization of the charging infrastructure is an emerging challenge for service providers . overstaying of an ev after a charging event is a key contributor to low utilization . since overstaying is easily detectable by monitoring the power drawn from the charger , managing this problem primarily involves designing an appropriate `` penalty '' during the overstaying period . higher penalties do discourage overstaying ; however , due to uncertainty in parking duration , less people would find such penalties acceptable , leading to decreased utilization ( and revenue ) . to analyze this central trade-off , we develop a novel framework that integrates models for realistic user behavior into queueing dynamics to locate the optimal penalty from the points of view of utilization and revenue , for different values of the external charging demand . next , when the model parameters are unknown , we show how an online learning algorithm , such as ucb , can be adapted to learn the optimal penalty . our experimental validation , based on charging data from london , shows that an appropriate penalty can increase both utilization and revenue while significantly reducing overstaying .

video face editing using temporal-spatial-smooth warping
editing faces in videos is a popular yet challenging aspect of computer vision and graphics , which encompasses several applications including facial attractiveness enhancement , makeup transfer , face replacement , and expression manipulation . simply applying image-based warping algorithms to video-based face editing produces temporal incoherence in the synthesized videos because it is impossible to consistently localize facial features in two frames representing two different faces in two different videos ( or even two consecutive frames representing the same face in one video ) . therefore , high performance face editing usually requires significant manual manipulation . in this paper we propose a novel temporal-spatial-smooth warping ( tssw ) algorithm to effectively exploit the temporal information in two consecutive frames , as well as the spatial smoothness within each frame . tssw precisely estimates two control lattices in the horizontal and vertical directions respectively from the corresponding control lattices in the previous frame , by minimizing a novel energy function that unifies a data-driven term , a smoothness term , and feature point constraints . corresponding warping surfaces then precisely map source frames to the target frames . experimental testing on facial attractiveness enhancement , makeup transfer , face replacement , and expression manipulation demonstrates that the proposed approaches can effectively preserve spatial smoothness and temporal coherence in editing facial geometry , skin detail , identity , and expression , which outperform the existing face editing methods . in particular , tssw is robust to subtly inaccurate localization of feature points and is a vast improvement over image-based warping methods .

