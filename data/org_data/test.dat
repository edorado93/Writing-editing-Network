embodied artificial intelligence through distributed adaptive control : an integrated framework
in this paper , we argue that the future of artificial intelligence research resides in two keywords : integration and embodiment . we support this claim by analyzing the recent advances of the field . regarding integration , we note that the most impactful recent contributions have been made possible through the integration of recent machine learning methods ( based in particular on deep learning and recurrent neural networks ) with more traditional ones ( e.g . monte-carlo tree search , goal babbling exploration or addressable memory systems ) . regarding embodiment , we note that the traditional benchmark tasks ( e.g . visual classification or board games ) are becoming obsolete as state-of-the-art learning algorithms approach or even surpass human performance in most of them , having recently encouraged the development of first-person 3d game platforms embedding realistic physics . building upon this analysis , we first propose an embodied cognitive architecture integrating heterogenous sub-fields of artificial intelligence into a unified framework . we demonstrate the utility of our approach by showing how major contributions of the field can be expressed within the proposed framework . we then claim that benchmarking environments need to reproduce ecologically-valid conditions for bootstrapping the acquisition of increasingly complex cognitive skills through the concept of a cognitive arms race between embodied agents .

wavelet scattering on the pitch spiral
we present a new representation of harmonic sounds that linearizes the dynamics of pitch and spectral envelope , while remaining stable to deformations in the time-frequency plane . it is an instance of the scattering transform , a generic operator which cascades wavelet convolutions and modulus nonlinearities . it is derived from the pitch spiral , in that convolutions are successively performed in time , log-frequency , and octave index . we give a closed-form approximation of spiral scattering coefficients for a nonstationary generalization of the harmonic source-filter model .

modelling protagonist goals and desires in first-person narrative
many genres of natural language text are narratively structured , a testament to our predilection for organizing our experiences as narratives . there is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes . however , to date , there has been limited work on computational models for this problem . we introduce a new dataset , desiredb , which includes gold-standard labels for identifying statements of desire , textual evidence for desire fulfillment , and annotations for whether the stated desire is fulfilled given the evidence in the narrative context . we report experiments on tracking desire fulfillment using different methods , and show that lstm skip-thought model achieves f-measure of 0.7 on our corpus .

effect of incomplete meta-dataset on average ranking method
one of the simplest metalearning methods is the average ranking method . this method uses metadata in the form of test results of a given set of algorithms on given set of datasets and calculates an average rank for each algorithm . the ranks are used to construct the average ranking . we investigate the problem of how the process of generating the average ranking is affected by incomplete metadata including fewer test results . this issue is relevant , because if we could show that incomplete metadata does not affect the final results much , we could explore it in future design . we could simply conduct fewer tests and save thus computation time . in this paper we describe an upgraded average ranking method that is capable of dealing with incomplete metadata . our results show that the proposed method is relatively robust to omission in test results in the meta datasets .

a matrix approach for weighted argumentation frameworks : a preliminary report
the assignment of weights to attacks in a classical argumentation framework allows to compute semantics by taking into account the different importance of each argument . we represent a weighted argumentation framework by a non-binary matrix , and we characterize the basic extensions ( such as w-admissible , w- stable , w-complete ) by analysing sub-blocks of this matrix . also , we show how to reduce the matrix into another one of smaller size , that is equivalent to the original one for the determination of extensions . furthermore , we provide two algorithms that allow to build incrementally w-grounded and w-preferred extensions starting from a w-admissible extension .

measuring visual complexity of cluster-based visualizations
handling visual complexity is a challenging problem in visualization owing to the subjectiveness of its definition and the difficulty in devising generalizable quantitative metrics . in this paper we address this challenge by measuring the visual complexity of two common forms of cluster-based visualizations : scatter plots and parallel coordinatess . we conceptualize visual complexity as a form of visual uncertainty , which is a measure of the degree of difficulty for humans to interpret a visual representation correctly . we propose an algorithm for estimating visual complexity for the aforementioned visualizations using allen 's interval algebra . we first establish a set of primitive 2-cluster cases in scatter plots and another set for parallel coordinatess based on symmetric isomorphism . we confirm that both are the minimal sets and verify the correctness of their members computationally . we score the uncertainty of each primitive case based on its topological properties , including the existence of overlapping regions , splitting regions and meeting points or edges . we compare a few optional scoring schemes against a set of subjective scores by humans , and identify the one that is the most consistent with the subjective scores . finally , we extend the 2-cluster measure to k-cluster measure as a general purpose estimator of visual complexity for these two forms of cluster-based visualization .

point-based pomdp algorithms : improved analysis and implementation
existing complexity bounds for point-based pomdp value iteration algorithms focus either on the curse of dimensionality or the curse of history . we derive a new bound that relies on both and uses the concept of discounted reachability ; our conclusions may help guide future algorithm design . we also discuss recent improvements to our ( point-based ) heuristic search value iteration algorithm . our new implementation calculates tighter initial bounds , avoids solving linear programs , and makes more effective use of sparsity .

crowd labeling : a survey
recently , there has been a burst in the number of research projects on human computation via crowdsourcing . multiple choice ( or labeling ) questions could be referred to as a common type of problem which is solved by this approach . as an application , crowd labeling is applied to find true labels for large machine learning datasets . since crowds are not necessarily experts , the labels they provide are rather noisy and erroneous . this challenge is usually resolved by collecting multiple labels for each sample , and then aggregating them to estimate the true label . although the mechanism leads to high-quality labels , it is not actually cost-effective . as a result , efforts are currently made to maximize the accuracy in estimating true labels , while fixing the number of acquired labels . this paper surveys methods to aggregate redundant crowd labels in order to estimate unknown true labels . it presents a unified statistical latent model where the differences among popular methods in the field correspond to different choices for the parameters of the model . afterwards , algorithms to make inference on these models will be surveyed . moreover , adaptive methods which iteratively collect labels based on the previously collected labels and estimated models will be discussed . in addition , this paper compares the distinguished methods , and provides guidelines for future work required to address the current open issues .

identifying unknown unknowns in the open world : representations and policies for guided exploration
predictive models deployed in the real world may assign incorrect labels to instances with high confidence . such errors or unknown unknowns are rooted in model incompleteness , and typically arise because of the mismatch between training data and the cases encountered at test time . as the models are blind to such errors , input from an oracle is needed to identify these failures . in this paper , we formulate and address the problem of informed discovery of unknown unknowns of any given predictive model where unknown unknowns occur due to systematic biases in the training data . we propose a model-agnostic methodology which uses feedback from an oracle to both identify unknown unknowns and to intelligently guide the discovery . we employ a two-phase approach which first organizes the data into multiple partitions based on the feature similarity of instances and the confidence scores assigned by the predictive model , and then utilizes an explore-exploit strategy for discovering unknown unknowns across these partitions . we demonstrate the efficacy of our framework by varying the underlying causes of unknown unknowns across various applications . to the best of our knowledge , this paper presents the first algorithmic approach to the problem of discovering unknown unknowns of predictive models .

combining semantic wikis and controlled natural language
we demonstrate acewiki that is a semantic wiki using the controlled natural language attempto controlled english ( ace ) . the goal is to enable easy creation and modification of ontologies through the web . texts in ace can automatically be translated into first-order logic and other languages , for example owl . previous evaluation showed that ordinary people are able to use acewiki without being instructed .

wordnet2vec : corpora agnostic word vectorization method
a complex nature of big data resources demands new methods for structuring especially for textual content . wordnet is a good knowledge source for comprehensive abstraction of natural language as its good implementations exist for many languages . since wordnet embeds natural language in the form of a complex network , a transformation mechanism wordnet2vec is proposed in the paper . it creates vectors for each word from wordnet . these vectors encapsulate general position - role of a given word towards all other words in the natural language . any list or set of such vectors contains knowledge about the context of its component within the whole language . such word representation can be easily applied to many analytic tasks like classification or clustering . the usefulness of the wordnet2vec method was demonstrated in sentiment analysis , i.e . classification with transfer learning for the real amazon opinion textual dataset .

lex-partitioning : a new option for bdd search
for the exploration of large state spaces , symbolic search using binary decision diagrams ( bdds ) can save huge amounts of memory and computation time . state sets are represented and modified by accessing and manipulating their characteristic functions . bdd partitioning is used to compute the image as the disjunction of smaller subimages . in this paper , we propose a novel bdd partitioning option . the partitioning is lexicographical in the binary representation of the states contained in the set that is represented by a bdd and uniform with respect to the number of states represented . the motivation of controlling the state set sizes in the partitioning is to eventually bridge the gap between explicit and symbolic search . let n be the size of the binary state vector . we propose an o ( n ) ranking and unranking scheme that supports negated edges and operates on top of precomputed satcount values . for the uniform split of a bdd , we then use unranking to provide paths along which we partition the bdds . in a shared bdd representation the efforts are o ( n ) . the algorithms are fully integrated in the cudd library and evaluated in strongly solving general game playing benchmarks .

towards a simulation-based programming paradigm for ai applications
we present initial ideas for a programming paradigm based on simulation that is targeted towards applications of artificial intelligence ( ai ) . the approach aims at integrating techniques from different areas of ai and is based on the idea that simulated entities may freely exchange data and behavioural patterns . we define basic notions of a simulation-based programming paradigm and show how it can be used for implementing ai applications .

semantic entity retrieval toolkit
unsupervised learning of low-dimensional , semantic representations of words and entities has recently gained attention . in this paper we describe the semantic entity retrieval toolkit ( sert ) that provides implementations of our previously published entity representation models . the toolkit provides a unified interface to different representation learning algorithms , fine-grained parsing configuration and can be used transparently with gpus . in addition , users can easily modify existing models or implement their own models in the framework . after model training , sert can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms , such as clustering or recommendation .

mdps with unawareness
markov decision processes ( mdps ) are widely used for modeling decision-making problems in robotics , automated control , and economics . traditional mdps assume that the decision maker ( dm ) knows all states and actions . however , this may not be true in many situations of interest . we define a new framework , mdps with unawareness ( mdpus ) to deal with the possibilities that a dm may not be aware of all possible actions . we provide a complete characterization of when a dm can learn to play near-optimally in an mdpu , and give an algorithm that learns to play near-optimally when it is possible to do so , as efficiently as possible . in particular , we characterize when a near-optimal solution can be found in polynomial time .

simplification and integration in computing and cognition : the sp theory and the multiple alignment concept
the main purpose of this article is to describe potential benefits and applications of the sp theory , a unique attempt to simplify and integrate ideas across artificial intelligence , mainstream computing and human cognition , with information compression as a unifying theme . the theory , including a concept of multiple alignment , combines conceptual simplicity with descriptive and explanatory power in several areas including representation of knowledge , natural language processing , pattern recognition , several kinds of reasoning , the storage and retrieval of information , planning and problem solving , unsupervised learning , information compression , and human perception and cognition . in the sp machine -- an expression of the sp theory which is currently realised in the form of computer models -- there is potential for an overall simplification of computing systems , including software . as a theory with a broad base of support , the sp theory promises useful insights in many areas and the integration of structures and functions , both within a given area and amongst different areas . there are potential benefits in natural language processing ( with potential for the understanding and translation of natural languages ) , the need for a versatile intelligence in autonomous robots , computer vision , intelligent databases , maintaining multiple versions of documents or web pages , software engineering , criminal investigations , the management of big data and gaining benefits from it , the semantic web , medical diagnosis , the detection of computer viruses , the economical transmission of data , and data fusion . further development of these ideas would be facilitated by the creation of a high-parallel , web-based , open-source version of the sp machine , with a good user interface . this would provide a means for researchers to explore what can be done with the system and to refine it .

entropy production rate as a criterion for inconsistency in decision theory
evaluating pairwise comparisons breaks down complex decision problems into tractable ones . pairwise comparison matrices ( pcms ) are regularly used to solve multiple-criteria decision-making ( mcdm ) problems using saaty 's analytic hierarchy process ( ahp ) framework . there are two significant drawbacks of using pcms . first , humans evaluate pcm in an inconsistent manner . second , pcms of large problems often have missing entries . we address these two issues by first establishing a novel connection between pcms and time-irreversible markov processes . specifically , we show that every pcm induces a family of dissipative maximum path entropy random walks ( merw ) over the set of alternatives . we show that only ` consistent ' pcms correspond to detailed balanced merws . we identify the non-equilibrium entropy production in the induced merws as a metric of inconsistency of the underlying pcms . notably , the entropy production satisfies all of the recently laid out criteria for reasonable consistency indices . we also propose an approach to use incompletely filled pcms in ahp . potential future avenues are discussed as well .

probability theory without bayes ' rule
within the kolmogorov theory of probability , bayes ' rule allows one to perform statistical inference by relating conditional probabilities to unconditional probabilities . as we show here , however , there is a continuous set of alternative inference rules that yield the same results , and that may have computational or practical advantages for certain problems . we formulate generalized axioms for probability theory , according to which the reverse conditional probability distribution p ( b|a ) is not specified by the forward conditional probability distribution p ( a|b ) and the marginals p ( a ) and p ( b ) . thus , in order to perform statistical inference , one must specify an additional `` inference axiom , '' which relates p ( b|a ) to p ( a|b ) , p ( a ) , and p ( b ) . we show that when bayes ' rule is chosen as the inference axiom , the axioms are equivalent to the classical kolmogorov axioms . we then derive consistency conditions on the inference axiom , and thereby characterize the set of all possible rules for inference . the set of `` first-order '' inference axioms , defined as the set of axioms in which p ( b|a ) depends on the first power of p ( a|b ) , is found to be a 1-simplex , with bayes ' rule at one of the extreme points . the other extreme point , the `` inversion rule , '' is studied in depth .

properties of aba+ for non-monotonic reasoning
we investigate properties of aba+ , a formalism that extends the well studied structured argumentation formalism assumption-based argumentation ( aba ) with a preference handling mechanism . in particular , we establish desirable properties that aba+ semantics exhibit . these pave way to the satisfaction by aba+ of some ( arguably ) desirable principles of preference handling in argumentation and nonmonotonic reasoning , as well as non-monotonic inference properties of aba+ under various semantics .

representing bayesian networks within probabilistic horn abduction
this paper presents a simple framework for horn clause abduction , with probabilities associated with hypotheses . it is shown how this representation can represent any probabilistic knowledge representable in a bayesian belief network . the main contributions are in finding a relationship between logical and probabilistic notions of evidential reasoning . this can be used as a basis for a new way to implement bayesian networks that allows for approximations to the value of the posterior probabilities , and also points to a way that bayesian networks can be extended beyond a propositional language .

structured factored inference : a framework for automated reasoning in probabilistic programming languages
reasoning on large and complex real-world models is a computationally difficult task , yet one that is required for effective use of many ai applications . a plethora of inference algorithms have been developed that work well on specific models or only on parts of general models . consequently , a system that can intelligently apply these inference algorithms to different parts of a model for fast reasoning is highly desirable . we introduce a new framework called structured factored inference ( sfi ) that provides the foundation for such a system . using models encoded in a probabilistic programming language , sfi provides a sound means to decompose a model into sub-models , apply an inference algorithm to each sub-model , and combine the resulting information to answer a query . our results show that sfi is nearly as accurate as exact inference yet retains the benefits of approximate inference methods .

a probabilistic approach for learning folksonomies from structured data
learning structured representations has emerged as an important problem in many domains , including document and web data mining , bioinformatics , and image analysis . one approach to learning complex structures is to integrate many smaller , incomplete and noisy structure fragments . in this work , we present an unsupervised probabilistic approach that extends affinity propagation to combine the small ontological fragments into a collection of integrated , consistent , and larger folksonomies . this is a challenging task because the method must aggregate similar structures while avoiding structural inconsistencies and handling noise . we validate the approach on a real-world social media dataset , comprised of shallow personal hierarchies specified by many individual users , collected from the photosharing website flickr . our empirical results show that our proposed approach is able to construct deeper and denser structures , compared to an approach using only the standard affinity propagation algorithm . additionally , the approach yields better overall integration quality than a state-of-the-art approach based on incremental relational clustering .

is this a joke ? detecting humor in spanish tweets
while humor has been historically studied from a psychological , cognitive and linguistic standpoint , its study from a computational perspective is an area yet to be explored in computational linguistics . there exist some previous works , but a characterization of humor that allows its automatic recognition and generation is far from being specified . in this work we build a crowdsourced corpus of labeled tweets , annotated according to its humor value , letting the annotators subjectively decide which are humorous . a humor classifier for spanish tweets is assembled based on supervised learning , reaching a precision of 84 % and a recall of 69 % .

sound , complete and minimal ucq-rewriting for existential rules
we address the issue of ontology-based data access , with ontologies represented in the framework of existential rules , also known as datalog+/- . a well-known approach involves rewriting the query using ontological knowledge . we focus here on the basic rewriting technique which consists of rewriting the initial query into a union of conjunctive queries . first , we study a generic breadth-first rewriting algorithm , which takes as input any rewriting operator , and define properties of rewriting operators that ensure the correctness of the algorithm . then , we focus on piece-unifiers , which provide a rewriting operator with the desired properties . finally , we propose an implementation of this framework and report some experiments .

msc : a dataset for macro-management in starcraft ii
macro-management is an important problem in starcraft , which has been studied for a long time . various datasets together with assorted methods have been proposed in the last few years . but these datasets have some defects for boosting the academic and industrial research : 1 ) there 're neither standard preprocessing , parsing and feature extraction procedures nor predefined training , validation and test set in some datasets . 2 ) some datasets are only specified for certain tasks in macro-management . 3 ) some datasets are either too small or do n't have enough labeled data for modern machine learning algorithms such as deep neural networks . so most previous methods are trained with various features , evaluated on different test sets from the same or different datasets , making it difficult to be compared directly . to boost the research of macro-management in starcraft , we release a new dataset msc based on the platform sc2le . msc consists of well-designed feature vectors , pre-defined high-level actions and final result of each match . we also split msc into training , validation and test set for the convenience of evaluation and comparison . besides the dataset , we propose a baseline model and present initial baseline results for global state evaluation and build order prediction , which are two of the key tasks in macro-management . various downstream tasks and analyses of the dataset are also described for the sake of research on macro-management in starcraft ii . homepage : https : //github.com/wuhuikai/msc .

verifying properties of binarized deep neural networks
understanding properties of deep neural networks is an important challenge in deep learning . in this paper , we take a step in this direction by proposing a rigorous way of verifying properties of a popular class of neural networks , binarized neural networks , using the well-developed means of boolean satisfiability . our main contribution is a construction that creates a representation of a binarized neural network as a boolean formula . our encoding is the first exact boolean representation of a deep neural network . using this encoding , we leverage the power of modern sat solvers along with a proposed counterexample-guided search procedure to verify various properties of these networks . a particular focus will be on the critical property of robustness to adversarial perturbations . for this property , our experimental results demonstrate that our approach scales to medium-size deep neural networks used in image classification tasks . to the best of our knowledge , this is the first work on verifying properties of deep neural networks using an exact boolean encoding of the network .

gatekeeping algorithms with human ethical bias : the ethics of algorithms in archives , libraries and society
in the age of algorithms , i focus on the question of how to ensure algorithms that will take over many of our familiar archival and library tasks , will behave according to human ethical norms that have evolved over many years . i start by characterizing physical archives in the context of related institutions such as libraries and museums . in this setting i analyze how ethical principles , in particular about access to information , have been formalized and communicated in the form of ethical codes , or : codes of conducts . after that i describe two main developments : digitalization , in which physical aspects of the world are turned into digital data , and algorithmization , in which intelligent computer programs turn this data into predictions and decisions . both affect interactions that were once physical but now digital . in this new setting i survey and analyze the ethical aspects of algorithms and how they shape a vision on the future of archivists and librarians , in the form of algorithmic documentalists , or : codementalists . finally i outline a general research strategy , called intermeedium , to obtain algorithms that obey are human ethical values encoded in code of ethics .

switching portfolios
a constant rebalanced portfolio is an asset allocation algorithm which keeps the same distribution of wealth among a set of assets along a period of time . recently , there has been work on on-line portfolio selection algorithms which are competitive with the best constant rebalanced portfolio determined in hindsight . by their nature , these algorithms employ the assumption that high returns can be achieved using a fixed asset allocation strategy . however , stock markets are far from being stationary and in many cases the wealth achieved by a constant rebalanced portfolio is much smaller than the wealth achieved by an ad-hoc investment strategy that adapts to changes in the market . in this paper we present an efficient bayesian portfolio selection algorithm that is able to track a changing market . we also describe a simple extension of the algorithm for the case of a general transaction cost , including the transactions cost models recently investigated by blum and kalai . we provide a simple analysis of the competitiveness of the algorithm and check its performance on real stock data from the new york stock exchange accumulated during a 22-year period .

learning representations by stochastic meta-gradient descent in neural networks
representations are fundamental to artificial intelligence . the performance of a learning system depends on the type of representation used for representing the data . typically , these representations are hand-engineered using domain knowledge . more recently , the trend is to learn these representations through stochastic gradient descent in multi-layer neural networks , which is called backprop . learning the representations directly from the incoming data stream reduces the human labour involved in designing a learning system . more importantly , this allows in scaling of a learning system for difficult tasks . in this paper , we introduce a new incremental learning algorithm called crossprop , which learns incoming weights of hidden units based on the meta-gradient descent approach , that was previously introduced by sutton ( 1992 ) and schraudolph ( 1999 ) for learning step-sizes . the final update equation introduces an additional memory parameter for each of these weights and generalizes the backprop update equation . from our experiments , we show that crossprop learns and reuses its feature representation while tackling new and unseen tasks whereas backprop relearns a new feature representation .

proceedings of the seventh conference on uncertainty in artificial intelligence ( 1991 )
this is the proceedings of the seventh conference on uncertainty in artificial intelligence , which was held in los angeles , ca , july 13-15 , 1991

solving multistage influence diagrams using branch-and-bound search
a branch-and-bound approach to solving influ- ence diagrams has been previously proposed in the literature , but appears to have never been implemented and evaluated - apparently due to the difficulties of computing effective bounds for the branch-and-bound search . in this paper , we describe how to efficiently compute effective bounds , and we develop a practical implementa- tion of depth-first branch-and-bound search for influence diagram evaluation that outperforms existing methods for solving influence diagrams with multiple stages .

object category understanding via eye fixations on freehand sketches
the study of eye gaze fixations on photographic images is an active research area . in contrast , the image subcategory of freehand sketches has not received as much attention for such studies . in this paper , we analyze the results of a free-viewing gaze fixation study conducted on 3904 freehand sketches distributed across 160 object categories . our analysis shows that fixation sequences exhibit marked consistency within a sketch , across sketches of a category and even across suitably grouped sets of categories . this multi-level consistency is remarkable given the variability in depiction and extreme image content sparsity that characterizes hand-drawn object sketches . in our paper , we show that the multi-level consistency in the fixation data can be exploited to ( a ) predict a test sketch 's category given only its fixation sequence and ( b ) build a computational model which predicts part-labels underlying fixations on objects . we hope that our findings motivate the community to deem sketch-like representations worthy of gaze-based studies vis-a-vis photographic images .

combining symmetry breaking and global constraints
we propose a new family of constraints which combine together lexicographical ordering constraints for symmetry breaking with other common global constraints . we give a general purpose propagator for this family of constraints , and show how to improve its complexity by exploiting properties of the included global constraints .

unconstrained influence diagrams
we extend the language of influence diagrams to cope with decision scenarios where the order of decisions and observations is not determined . as the ordering of decisions is dependent on the evidence , a step-strategy of such a scenario is a sequence of dependent choices of the next action . a strategy is a step-strategy together with selection functions for decision actions . the structure of a step-strategy can be represented as a dag with nodes labeled with action variables . we introduce the concept of gs-dag : a dag incorporating an optimal step-strategy for any instantiation . we give a method for constructing gs-dags , and we show how to use a gs-dag for determining an optimal strategy . finally we discuss how analysis of relevant past can be used to reduce the size of the gs-dag .

identifying interaction sites in `` recalcitrant '' proteins : predicted protein and rna binding sites in rev proteins of hiv-1 and eiav agree with experimental data
protein-protein and protein nucleic acid interactions are vitally important for a wide range of biological processes , including regulation of gene expression , protein synthesis , and replication and assembly of many viruses . we have developed machine learning approaches for predicting which amino acids of a protein participate in its interactions with other proteins and/or nucleic acids , using only the protein sequence as input . in this paper , we describe an application of classifiers trained on datasets of well-characterized protein-protein and protein-rna complexes for which experimental structures are available . we apply these classifiers to the problem of predicting protein and rna binding sites in the sequence of a clinically important protein for which the structure is not known : the regulatory protein rev , essential for the replication of hiv-1 and other lentiviruses . we compare our predictions with published biochemical , genetic and partial structural information for hiv-1 and eiav rev and with our own published experimental mapping of rna binding sites in eiav rev . the predicted and experimentally determined binding sites are in very good agreement . the ability to predict reliably the residues of a protein that directly contribute to specific binding events - without the requirement for structural information regarding either the protein or complexes in which it participates - can potentially generate new disease intervention strategies .

classification driven dynamic image enhancement
convolutional neural networks rely on image texture and structure to serve as discriminative features to classify the image content . image enhancement techniques can be used as preprocessing steps to help improve the overall image quality and in turn improve the overall effectiveness of a cnn . existing image enhancement methods , however , are designed to improve the perceptual quality of an image for a human observer . in this paper , we are interested in learning cnns that can emulate image enhancement and restoration , but with the overall goal to improve image classification and not necessarily human perception . to this end , we present a unified cnn architecture that uses a range of enhancement filters that can enhance image-specific details via end-to-end dynamic filter learning . we demonstrate the effectiveness of this strategy on four challenging benchmark datasets for fine-grained , object , scene and texture classification : cub-200-2011 , pascal-voc2007 , mit-indoor , and dtd . experiments using our proposed enhancement shows promising results on all the datasets . in addition , our approach is capable of improving the performance of all generic cnn architectures .

detecting unseen falls from wearable devices using channel-wise ensemble of autoencoders
a fall is an abnormal activity that occurs rarely , so it is hard to collect real data for falls . it is , therefore , difficult to use supervised learning methods to automatically detect falls . another challenge in using machine learning methods to automatically detect falls is the choice of engineered features . in this paper , we propose to use an ensemble of autoencoders to extract features from different channels of wearable sensor data trained only on normal activities . we show that the traditional approach of choosing a threshold as the maximum of the reconstruction error on the training normal data is not the right way to identify unseen falls . we propose two methods for automatic tightening of reconstruction error from only the normal activities for better identification of unseen falls . we present our results on two activity recognition datasets and show the efficacy of our proposed method against traditional autoencoder models and two standard one-class classification methods .

the role of time in the creation of knowledge
this paper i assume that in humans the creation of knowledge depends on a discrete time , or stage , sequential decision-making process subjected to a stochastic , information transmitting environment . for each time-stage , this environment randomly transmits shannon type information-packets to the decision-maker , who examines each of them for relevancy and then determines his optimal choices . using this set of relevant information-packets , the decision-maker adapts , over time , to the stochastic nature of his environment , and optimizes the subjective expected rate-of-growth of knowledge . the decision-maker 's optimal actions , lead to a decision function that involves , over time , his view of the subjective entropy of the environmental process and other important parameters at each time-stage of the process . using this model of human behavior , one could create psychometric experiments using computer simulation and real decision-makers , to play programmed games to measure the resulting human performance .

the optimal reward baseline for gradient-based reinforcement learning
there exist a number of reinforcement learning algorithms which learnby climbing the gradient of expected reward . their long-runconvergence has been proved , even in partially observableenvironments with non-deterministic actions , and without the need fora system model . however , the variance of the gradient estimator hasbeen found to be a significant practical problem . recent approacheshave discounted future rewards , introducing a bias-variance trade-offinto the gradient estimate . we incorporate a reward baseline into thelearning system , and show that it affects variance without introducingfurther bias . in particular , as we approach the zero-bias , high-variance parameterization , the optimal ( or variance minimizing ) constant reward baseline is equal to the long-term average expectedreward . modified policy-gradient algorithms are presented , and anumber of experiments demonstrate their improvement over previous work .

solving weighted constraint satisfaction problems with memetic/exact hybrid algorithms
a weighted constraint satisfaction problem ( wcsp ) is a constraint satisfaction problem in which preferences among solutions can be expressed . bucket elimination is a complete technique commonly used to solve this kind of constraint satisfaction problem . when the memory required to apply bucket elimination is too high , a heuristic method based on it ( denominated mini-buckets ) can be used to calculate bounds for the optimal solution . nevertheless , the curse of dimensionality makes these techniques impractical on large scale problems . in response to this situation , we present a memetic algorithm for wcsps in which bucket elimination is used as a mechanism for recombining solutions , providing the best possible child from the parental set . subsequently , a multi-level model in which this exact/metaheuristic hybrid is further hybridized with branch-and-bound techniques and mini-buckets is studied . as a case study , we have applied these algorithms to the resolution of the maximum density still life problem , a hard constraint optimization problem based on conways game of life . the resulting algorithm consistently finds optimal patterns for up to date solved instances in less time than current approaches . moreover , it is shown that this proposal provides new best known solutions for very large instances .

are minds computable ?
this essay explores the limits of turing machines concerning the modeling of minds and suggests alternatives to go beyond those limits .

toward idealized decision theory
this paper motivates the study of decision theory as necessary for aligning smarter-than-human artificial systems with human interests . we discuss the shortcomings of two standard formulations of decision theory , and demonstrate that they can not be used to describe an idealized decision procedure suitable for approximation by artificial systems . we then explore the notions of policy selection and logical counterfactuals , two recent insights into decision theory that point the way toward promising paths for future research .

the sp theory of intelligence as a foundation for the development of a general , human-level thinking machine
this paper summarises how the `` sp theory of intelligence '' and its realisation in the `` sp computer model '' simplifies and integrates concepts across artificial intelligence and related areas , and thus provides a promising foundation for the development of a general , human-level thinking machine , in accordance with the main goal of research in artificial general intelligence . the key to this simplification and integration is the powerful concept of `` multiple alignment '' , borrowed and adapted from bioinformatics . this concept has the potential to be the `` double helix '' of intelligence , with as much significance for human-level intelligence as has dna for biological sciences . strengths of the sp system include : versatility in the representation of diverse kinds of knowledge ; versatility in aspects of intelligence ( including : strengths in unsupervised learning ; the processing of natural language ; pattern recognition at multiple levels of abstraction that is robust in the face of errors in data ; several kinds of reasoning ( including : one-step ` deductive ' reasoning ; chains of reasoning ; abductive reasoning ; reasoning with probabilistic networks and trees ; reasoning with 'rules ' ; nonmonotonic reasoning and reasoning with default values ; bayesian reasoning with 'explaining away ' ; and more ) ; planning ; problem solving ; and more ) ; seamless integration of diverse kinds of knowledge and diverse aspects of intelligence in any combination ; and potential for application in several areas ( including : helping to solve nine problems with big data ; helping to develop human-level intelligence in autonomous robots ; serving as a database with intelligence and with versatility in the representation and integration of several forms of knowledge ; serving as a vehicle for medical knowledge and as an aid to medical diagnosis ; and several more ) .

the temporal logic of causal structures
computational analysis of time-course data with an underlying causal structure is needed in a variety of domains , including neural spike trains , stock price movements , and gene expression levels . however , it can be challenging to determine from just the numerical time course data alone what is coordinating the visible processes , to separate the underlying prima facie causes into genuine and spurious causes and to do so with a feasible computational complexity . for this purpose , we have been developing a novel algorithm based on a framework that combines notions of causality in philosophy with algorithmic approaches built on model checking and statistical techniques for multiple hypotheses testing . the causal relationships are described in terms of temporal logic formulae , reframing the inference problem in terms of model checking . the logic used , pctl , allows description of both the time between cause and effect and the probability of this relationship being observed . we show that equipped with these causal formulae with their associated probabilities we may compute the average impact a cause makes to its effect and then discover statistically significant causes through the concepts of multiple hypothesis testing ( treating each causal relationship as a hypothesis ) , and false discovery control . by exploring a well-chosen family of potentially all significant hypotheses with reasonably minimal description length , it is possible to tame the algorithm 's computational complexity while exploring the nearly complete search-space of all prima facie causes . we have tested these ideas in a number of domains and illustrate them here with two examples .

variational algorithms for marginal map
the marginal maximum a posteriori probability ( map ) estimation problem , which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized , is an important inference problem in many models , such as those with hidden variables or uncertain parameters . unfortunately , marginal map can be np-hard even on trees , and has attracted less attention in the literature compared to the joint map ( maximization ) and marginalization problems . we derive a general dual representation for marginal map that naturally integrates the marginalization and maximization operations into a joint variational optimization problem , making it possible to easily extend most or all variational-based algorithms to marginal map . in particular , we derive a set of `` mixed-product '' message passing algorithms for marginal map , whose form is a hybrid of max-product , sum-product and a novel `` argmax-product '' message updates . we also derive a class of convergent algorithms based on proximal point methods , including one that transforms the marginal map problem into a sequence of standard marginalization problems . theoretically , we provide guarantees under which our algorithms give globally or locally optimal solutions , and provide novel upper bounds on the optimal objectives . empirically , we demonstrate that our algorithms significantly outperform the existing approaches , including a state-of-the-art algorithm based on local search methods .

generating graphical chain by mutual matching of bayesian network and extracted rules of bayesian network using genetic algorithm
with the technology development , the need of analyze and extraction of useful information is increasing . bayesian networks contain knowledge from data and experts that could be used for decision making processes but they are not easily understandable thus the rule extraction methods have been used but they have high computation costs . to overcome this problem we extract rules from bayesian network using genetic algorithm . then we generate the graphical chain by mutually matching the extracted rules and bayesian network . this graphical chain could shows the sequence of events that lead to the target which could help the decision making process . the experimental results on small networks show that the proposed method has comparable results with brute force method which has a significantly higher computation cost .

an efficient high-quality hierarchical clustering algorithm for automatic inference of software architecture from the source code of a software system
it is a high-quality algorithm for hierarchical clustering of large software source code . this effectively allows to break the complexity of tens of millions lines of source code , so that a human software engineer can comprehend a software system at high level by means of looking at its architectural diagram that is reconstructed automatically from the source code of the software system . the architectural diagram shows a tree of subsystems having oop classes in its leaves ( in the other words , a nested software decomposition ) . the tool reconstructs the missing ( inconsistent/incomplete/inexistent ) architectural documentation for a software system from its source code . this facilitates software maintenance : change requests can be performed substantially faster . simply speaking , this unique tool allows to lift the comprehensible grain of object-oriented software systems from oop class-level to subsystem-level . it is estimated that a commercial tool , developed on the basis of this work , will reduce software maintenance expenses 10 times on the current needs , and will allow to implement next-generation software systems which are currently too complex to be within the range of human comprehension , therefore ca n't yet be designed or implemented . implemented prototype in open source : http : //sourceforge.net/p/insoar/code-0/1/tree/

human action recognition system using good features and multilayer perceptron network
human action recognition involves the characterization of human actions through the automated analysis of video data and is integral in the development of smart computer vision systems . however , several challenges like dynamic backgrounds , camera stabilization , complex actions , occlusions etc . make action recognition in a real time and robust fashion difficult . several complex approaches exist but are computationally intensive . this paper presents a novel approach of using a combination of good features along with iterative optical flow algorithm to compute feature vectors which are classified using a multilayer perceptron ( mlp ) network . the use of multiple features for motion descriptors enhances the quality of tracking . resilient backpropagation algorithm is used for training the feedforward neural network reducing the learning time . the overall system accuracy is improved by optimizing the various parameters of the multilayer perceptron network .

a notation for markov decision processes
this paper specifies a notation for markov decision processes .

automatic induction of bellman-error features for probabilistic planning
domain-specific features are important in representing problem structure throughout machine learning and decision-theoretic planning . in planning , once state features are provided , domain-independent algorithms such as approximate value iteration can learn weighted combinations of those features that often perform well as heuristic estimates of state value ( e.g. , distance to the goal ) . successful applications in real-world domains often require features crafted by human experts . here , we propose automatic processes for learning useful domain-specific feature sets with little or no human intervention . our methods select and add features that describe state-space regions of high inconsistency in the bellman equation ( statewise bellman error ) during approximate value iteration . our method can be applied using any real-valued-feature hypothesis space and corresponding learning method for selecting features from training sets of state-value pairs . we evaluate the method with hypothesis spaces defined by both relational and propositional feature languages , using nine probabilistic planning domains . we show that approximate value iteration using a relational feature space performs at the state-of-the-art in domain-independent stochastic relational planning . our method provides the first domain-independent approach that plays tetris successfully ( without human-engineered features ) .

coincidences and the encounter problem : a formal account
individuals have an intuitive perception of what makes a good coincidence . though the sensitivity to coincidences has often been presented as resulting from an erroneous assessment of probability , it appears to be a genuine competence , based on non-trivial computations . the model presented here suggests that coincidences occur when subjects perceive complexity drops . co-occurring events are , together , simpler than if considered separately . this model leads to a possible redefinition of subjective probability .

solving highly constrained search problems with quantum computers
a previously developed quantum search algorithm for solving 1-sat problems in a single step is generalized to apply to a range of highly constrained k-sat problems . we identify a bound on the number of clauses in satisfiability problems for which the generalized algorithm can find a solution in a constant number of steps as the number of variables increases . this performance contrasts with the linear growth in the number of steps required by the best classical algorithms , and the exponential number required by classical and quantum methods that ignore the problem structure . in some cases , the algorithm can also guarantee that insoluble problems in fact have no solutions , unlike previously proposed quantum search algorithms .

using machine learning to make constraint solver implementation decisions
programs to solve so-called constraint problems are complex pieces of software which require many design decisions to be made more or less arbitrarily by the implementer . these decisions affect the performance of the finished solver significantly . once a design decision has been made , it can not easily be reversed , although a different decision may be more appropriate for a particular problem . we investigate using machine learning to make these decisions automatically depending on the problem to solve with the alldifferent constraint as an example . our system is capable of making non-trivial , multi-level decisions that improve over always making a default choice .

bayesian optimization with censored response data
bayesian optimization ( bo ) aims to minimize a given blackbox function using a model that is updated whenever new evidence about the function becomes available . here , we address the problem of bo under partially right-censored response data , where in some evaluations we only obtain a lower bound on the function value . the ability to handle such response data allows us to adaptively censor costly function evaluations in minimization problems where the cost of a function evaluation corresponds to the function value . one important application giving rise to such censored data is the runtime-minimizing variant of the algorithm configuration problem : finding settings of a given parametric algorithm that minimize the runtime required for solving problem instances from a given distribution . we demonstrate that terminating slow algorithm runs prematurely and handling the resulting right-censored observations can substantially improve the state of the art in model-based algorithm configuration .

expected similarity estimation for large-scale batch and streaming anomaly detection
we present a novel algorithm for anomaly detection on very large datasets and data streams . the method , named expected similarity estimation ( expose ) , is kernel-based and able to efficiently compute the similarity between new data points and the distribution of regular data . the estimator is formulated as an inner product with a reproducing kernel hilbert space embedding and makes no assumption about the type or shape of the underlying data distribution . we show that offline ( batch ) learning with expose can be done in linear time and online ( incremental ) learning takes constant time per instance and model update . furthermore , expose can make predictions in constant time , while it requires only constant memory . in addition , we propose different methodologies for concept drift adaptation on evolving data streams . on several real datasets we demonstrate that our approach can compete with state of the art algorithms for anomaly detection while being an order of magnitude faster than most other approaches .

factorization , inference and parameter learning in discrete amp chain graphs
we address some computational issues that may hinder the use of amp chain graphs in practice . specifically , we show how a discrete probability distribution that satisfies all the independencies represented by an amp chain graph factorizes according to it . we show how this factorization makes it possible to perform inference and parameter learning efficiently , by adapting existing algorithms for markov and bayesian networks . finally , we turn our attention to another issue that may hinder the use of amp cgs , namely the lack of an intuitive interpretation of their edges . we provide one such interpretation .

many hard examples in exact phase transitions with application to generating hard satisfiable instances
this paper first analyzes the resolution complexity of two random csp models ( i.e . model rb/rd ) for which we can establish the existence of phase transitions and identify the threshold points exactly . by encoding csps into cnf formulas , it is proved that almost all instances of model rb/rd have no tree-like resolution proofs of less than exponential size . thus , we not only introduce new families of cnf formulas hard for resolution , which is a central task of proof-complexity theory , but also propose models with both many hard instances and exact phase transitions . then , the implications of such models are addressed . it is shown both theoretically and experimentally that an application of model rb/rd might be in the generation of hard satisfiable instances , which is not only of practical importance but also related to some open problems in cryptography such as generating one-way functions . subsequently , a further theoretical support for the generation method is shown by establishing exponential lower bounds on the complexity of solving random satisfiable and forced satisfiable instances of rb/rd near the threshold . finally , conclusions are presented , as well as a detailed comparison of model rb/rd with the hamiltonian cycle problem and random 3-sat , which , respectively , exhibit three different kinds of phase transition behavior in np-complete problems .

the all relevant feature selection using random forest
in this paper we examine the application of the random forest classifier for the all relevant feature selection problem . to this end we first examine two recently proposed all relevant feature selection algorithms , both being a random forest wrappers , on a series of synthetic data sets with varying size . we show that reasonable accuracy of predictions can be achieved and that heuristic algorithms that were designed to handle the all relevant problem , have performance that is close to that of the reference ideal algorithm . then , we apply one of the algorithms to four families of semi-synthetic data sets to assess how the properties of particular data set influence results of feature selection . finally we test the procedure using a well-known gene expression data set . the relevance of nearly all previously established important genes was confirmed , moreover the relevance of several new ones is discovered .

cauchy annealing schedule : an annealing schedule for boltzmann selection scheme in evolutionary algorithms
boltzmann selection is an important selection mechanism in evolutionary algorithms as it has theoretical properties which help in theoretical analysis . however , boltzmann selection is not used in practice because a good annealing schedule for the ` inverse temperature ' parameter is lacking . in this paper we propose a cauchy annealing schedule for boltzmann selection scheme based on a hypothesis that selection-strength should increase as evolutionary process goes on and distance between two selection strengths should decrease for the process to converge . to formalize these aspects , we develop formalism for selection mechanisms using fitness distributions and give an appropriate measure for selection-strength . in this paper , we prove an important result , by which we derive an annealing schedule called cauchy annealing schedule . we demonstrate the novelty of proposed annealing schedule using simulations in the framework of genetic algorithms .

representing conversations for scalable overhearing
open distributed multi-agent systems are gaining interest in the academic community and in industry . in such open settings , agents are often coordinated using standardized agent conversation protocols . the representation of such protocols ( for analysis , validation , monitoring , etc ) is an important aspect of multi-agent applications . recently , petri nets have been shown to be an interesting approach to such representation , and radically different approaches using petri nets have been proposed . however , their relative strengths and weaknesses have not been examined . moreover , their scalability and suitability for different tasks have not been addressed . this paper addresses both these challenges . first , we analyze existing petri net representations in terms of their scalability and appropriateness for overhearing , an important task in monitoring open multi-agent systems . then , building on the insights gained , we introduce a novel representation using colored petri nets that explicitly represent legal joint conversation states and messages . this representation approach offers significant improvements in scalability and is particularly suitable for overhearing . furthermore , we show that this new representation offers a comprehensive coverage of all conversation features of fipa conversation standards . we also present a procedure for transforming auml conversation protocol diagrams ( a standard human-readable representation ) , to our colored petri net representation .

neural task programming : learning to generalize across hierarchical tasks
in this work , we propose a novel robot learning framework called neural task programming ( ntp ) , which bridges the idea of few-shot learning from demonstration and neural program induction . ntp takes as input a task specification ( e.g. , video demonstration of a task ) and recursively decomposes it into finer sub-task specifications . these specifications are fed to a hierarchical neural program , where bottom-level programs are callable subroutines that interact with the environment . we validate our method in three robot manipulation tasks . ntp achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures . the experimental results show that ntp learns to generalize well to- wards unseen tasks with increasing lengths , variable topologies , and changing objectives .

policy search : any local optimum enjoys a global performance guarantee
local policy search is a popular reinforcement learning approach for handling large state spaces . formally , it searches locally in a paramet erized policy space in order to maximize the associated value function averaged over some predefined distribution . it is probably commonly b elieved that the best one can hope in general from such an approach is to get a local optimum of this criterion . in this article , we show th e following surprising result : \emph { any } ( approximate ) \emph { local optimum } enjoys a \emph { global performance guarantee } . we compare this g uarantee with the one that is satisfied by direct policy iteration , an approximate dynamic programming algorithm that does some form of poli cy search : if the approximation error of local policy search may generally be bigger ( because local search requires to consider a space of s tochastic policies ) , we argue that the concentrability coefficient that appears in the performance bound is much nicer . finally , we discuss several practical and theoretical consequences of our analysis .

combining voting rules together
we propose a simple method for combining together voting rules that performs a run-off between the different winners of each voting rule . we prove that this combinator has several good properties . for instance , even if just one of the base voting rules has a desirable property like condorcet consistency , the combination inherits this property . in addition , we prove that combining voting rules together in this way can make finding a manipulation more computationally difficult . finally , we study the impact of this combinator on approximation methods that find close to optimal manipulations .

tree-based iterated local search for markov random fields with applications in image analysis
the \emph { maximum a posteriori } ( map ) assignment for general structure markov random fields ( mrfs ) is computationally intractable . in this paper , we exploit tree-based methods to efficiently address this problem . our novel method , named tree-based iterated local search ( t-ils ) takes advantage of the tractability of tree-structures embedded within mrfs to derive strong local search in an ils framework . the method efficiently explores exponentially large neighborhood and does so with limited memory without any requirement on the cost functions . we evaluate the t-ils in a simulation of ising model and two real-world problems in computer vision : stereo matching , image denoising . experimental results demonstrate that our methods are competitive against state-of-the-art rivals with a significant computational gain .

anomaly detection with the voronoi diagram evolutionary algorithm
this paper presents the voronoi diagram-based evolutionary algorithm ( voreal ) . voreal partitions input space in abnormal/normal subsets using voronoi diagrams . diagrams are evolved using a multi-objective bio-inspired approach in order to conjointly optimize classification metrics while also being able to represent areas of the data space that are not present in the training dataset . as part of the paper voreal is experimentally validated and contrasted with similar approaches .

proactive decision support using automated planning
proactive decision support ( pds ) helps in improving the decision making experience of human decision makers in human-in-the-loop planning environments . here both the quality of the decisions and the ease of making them are enhanced . in this regard , we propose a pds framework , named radar , based on the research in automated planning in ai , that aids the human decision maker with her plan to achieve her goals by providing alerts on : whether such a plan can succeed at all , whether there exist any resource constraints that may foil her plan , etc . this is achieved by generating and analyzing the landmarks that must be accomplished by any successful plan on the way to achieving the goals . note that , this approach also supports naturalistic decision making which is being acknowledged as a necessary element in proactive decision support , since it only aids the human decision maker through suggestions and alerts rather than enforcing fixed plans or decisions . we demonstrate the utility of the proposed framework through search-and-rescue examples in a fire-fighting domain .

on ultrametric algorithmic information
how best to quantify the information of an object , whether natural or artifact , is a problem of wide interest . a related problem is the computability of an object . we present practical examples of a new way to address this problem . by giving an appropriate representation to our objects , based on a hierarchical coding of information , we exemplify how it is remarkably easy to compute complex objects . our algorithmic complexity is related to the length of the class of objects , rather than to the length of the object .

towards a virtual assistant that can be taught new tasks in any domain by its end-users
the challenge stated in the title can be divided into two main problems . the first problem is to reliably mimic the way that users interact with user interfaces . the second problem is to build an instructible agent , i.e . one that can be taught to execute tasks expressed as previously unseen natural language commands . this paper proposes a solution to the second problem , a system we call helpa . end-users can teach helpa arbitrary new tasks whose level of complexity is similar to the tasks available from today 's most popular virtual assistants . teaching helpa does not involve any programming . instead , users teach helpa by providing just one example of a command paired with a demonstration of how to execute that command . helpa does not rely on any pre-existing domain-specific knowledge . it is therefore completely domain-independent . our usability study showed that end-users can teach helpa many new tasks in less than a minute each , often much less .

towards a benchmark of natural language arguments
the connections among natural language processing and argumentation theory are becoming stronger in the latest years , with a growing amount of works going in this direction , in different scenarios and applying heterogeneous techniques . in this paper , we present two datasets we built to cope with the combination of the textual entailment framework and bipolar abstract argumentation . in our approach , such datasets are used to automatically identify through a textual entailment system the relations among the arguments ( i.e. , attack , support ) , and then the resulting bipolar argumentation graphs are analyzed to compute the accepted arguments .

representing the insincere : strategically robust proportional representation
proportional representation ( pr ) is a fundamental principle of many democracies world-wide which employ pr-based voting rules to elect their representatives . the normative properties of these voting rules however , are often only understood in the context of sincere voting . in this paper we consider pr in the presence of strategic voters . we construct a voting rule such that for every preference profile there exists at least one costly voting equilibrium satisfying pr with respect to voters ' private and unrevealed preferences - such a voting rule is said to be strategically robust . in contrast , a commonly applied voting rule is shown not be strategically robust . furthermore , we prove a limit on ` how strategically robust ' a pr-based voting rule can be ; we show that there is no pr-based voting rule which ensures that every equilibrium satisfies pr . collectively , our results highlight the possibility and limit of achieving pr in the presence of strategic voters and a positive role for mechanisms , such as pre-election polls , which coordinate voter behaviour towards equilibria which satisfy pr .

generalized fmd detection for spectrum sensing under low signal-to-noise ratio
spectrum sensing is a fundamental problem in cognitive radio . we propose a function of covariance matrix based detection algorithm for spectrum sensing in cognitive radio network . monotonically increasing property of function of matrix involving trace operation is utilized as the cornerstone for this algorithm . the advantage of proposed algorithm is it works under extremely low signal-to-noise ratio , like lower than -30 db with limited sample data . theoretical analysis of threshold setting for the algorithm is discussed . a performance comparison between the proposed algorithm and other state-of-the-art methods is provided , by the simulation on captured digital television ( dtv ) signal .

saliency-based sequential image attention with multiset prediction
humans process visual scenes selectively and sequentially using attention . central to models of human visual attention is the saliency map . we propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions . the architecture is motivated by human visual attention , and is used for multi-label image classification on a novel multiset task , demonstrating that it achieves high precision and recall while localizing objects with its attention . unlike conventional multi-label image classification models , the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label .

quantified multimodal logics in simple type theory
we present a straightforward embedding of quantified multimodal logic in simple type theory and prove its soundness and completeness . modal operators are replaced by quantification over a type of possible worlds . we present simple experiments , using existing higher-order theorem provers , to demonstrate that the embedding allows automated proofs of statements in these logics , as well as meta properties of them .

probabilistic models for computerized adaptive testing : experiments
this paper follows previous research we have already performed in the area of bayesian networks models for cat . we present models using item response theory ( irt - standard cat method ) , bayesian networks , and neural networks . we conducted simulated cat tests on empirical data . results of these tests are presented for each model separately and compared .

proceedings of the tenth conference on uncertainty in artificial intelligence ( 1994 )
this is the proceedings of the tenth conference on uncertainty in artificial intelligence , which was held in seattle , wa , july 29-31 , 1994

syntax , parsing and production of natural language in a framework of information compression by multiple alignment , unification and search
this article introduces the idea that `` information compression by multiple alignment , unification and search '' ( icmaus ) provides a framework within which natural language syntax may be represented in a simple format and the parsing and production of natural language may be performed in a transparent manner . the icmaus concepts are embodied in a software model , sp61 . the organisation and operation of the model are described and a simple example is presented showing how the model can achieve parsing of natural language . notwithstanding the apparent paradox of 'decompression by compression ' , the icmaus framework , without any modification , can produce a sentence by decoding a compressed code for the sentence . this is illustrated with output from the sp61 model . the article includes four other examples - one of the parsing of a sentence in french and three from the domain of english auxiliary verbs . these examples show how the icmaus framework and the sp61 model can accommodate 'context sensitive ' features of syntax in a relatively simple and direct manner .

complexity of inference in graphical models
it is well-known that inference in graphical models is hard in the worst case , but tractable for models with bounded treewidth . we ask whether treewidth is the only structural criterion of the underlying graph that enables tractable inference . in other words , is there some class of structures with unbounded treewidth in which inference is tractable ? subject to a combinatorial hypothesis due to robertson et al . ( 1994 ) , we show that low treewidth is indeed the only structural restriction that can ensure tractability . thus , even for the `` best case '' graph structure , there is no inference algorithm with complexity polynomial in the treewidth .

quick and energy-efficient bayesian computing of binocular disparity using stochastic digital signals
reconstruction of the tridimensional geometry of a visual scene using the binocular disparity information is an important issue in computer vision and mobile robotics , which can be formulated as a bayesian inference problem . however , computation of the full disparity distribution with an advanced bayesian model is usually an intractable problem , and proves computationally challenging even with a simple model . in this paper , we show how probabilistic hardware using distributed memory and alternate representation of data as stochastic bitstreams can solve that problem with high performance and energy efficiency . we put forward a way to express discrete probability distributions using stochastic data representations and perform bayesian fusion using those representations , and show how that approach can be applied to diparity computation . we evaluate the system using a simulated stochastic implementation and discuss possible hardware implementations of such architectures and their potential for sensorimotor processing and robotics .

approximations from anywhere and general rough sets
not all approximations arise from information systems . the problem of fitting approximations , subjected to some rules ( and related data ) , to information systems in a rough scheme of things is known as the \emph { inverse problem } . the inverse problem is more general than the duality ( or abstract representation ) problems and was introduced by the present author in her earlier papers . from the practical perspective , a few ( as opposed to one ) theoretical frameworks may be suitable for formulating the problem itself . \emph { granular operator spaces } have been recently introduced and investigated by the present author in her recent work in the context of antichain based and dialectical semantics for general rough sets . the nature of the inverse problem is examined from number-theoretic and combinatorial perspectives in a higher order variant of granular operator spaces and some necessary conditions are proved . the results and the novel approach would be useful in a number of unsupervised and semi supervised learning contexts and algorithms .

a computer program for simulating time travel and a possible 'solution ' for the grandfather paradox
while the possibility of time travel in physics is still debated , the explosive growth of virtual-reality simulations opens up new possibilities to rigorously explore such time travel and its consequences in the digital domain . here we provide a computational model of time travel and a computer program that allows exploring digital time travel . in order to explain our method we formalize a simplified version of the famous grandfather paradox , show how the system can allow the participant to go back in time , try to kill their ancestors before they were born , and experience the consequences . the system has even come up with scenarios that can be considered consistent `` solutions '' of the grandfather paradox . we discuss the conditions for digital time travel , which indicate that it has a large number of practical applications .

spontaneous symmetry breaking in neural networks
we propose a framework to understand the unprecedented performance and robustness of deep neural networks using field theory . correlations between the weights within the same layer can be described by symmetries in that layer , and networks generalize better if such symmetries are broken to reduce the redundancies of the weights . using a two parameter field theory , we find that the network can break such symmetries itself towards the end of training in a process commonly known in physics as spontaneous symmetry breaking . this corresponds to a network generalizing itself without any user input layers to break the symmetry , but by communication with adjacent layers . in the layer decoupling limit applicable to residual networks ( he et al. , 2015 ) , we show that the remnant symmetries that survive the non-linear layers are spontaneously broken . the lagrangian for the non-linear and weight layers together has striking similarities with the one in quantum field theory of a scalar . using results from quantum field theory we show that our framework is able to explain many experimentally observed phenomena , such as training on random labels with zero error ( zhang et al. , 2017 ) , the information bottleneck , the phase transition out of it and gradient variance explosion ( shwartz-ziv & tishby , 2017 ) , shattered gradients ( balduzzi et al. , 2017 ) , and many more .

on the performance of network parallel training in artificial neural networks
artificial neural networks ( anns ) have received increasing attention in recent years with applications that span a wide range of disciplines including vital domains such as medicine , network security and autonomous transportation . however , neural network architectures are becoming increasingly complex and with an increasing need to obtain real-time results from such models , it has become pivotal to use parallelization as a mechanism for speeding up network training and deployment . in this work we propose an implementation of network parallel training through cannon 's algorithm for matrix multiplication . we show that increasing the number of processes speeds up training until the point where process communication costs become prohibitive ; this point varies by network complexity . we also show through empirical efficiency calculations that the speedup obtained is superlinear .

the road to quantum artificial intelligence
this paper overviews the basic principles and recent advances in the emerging field of quantum computation ( qc ) , highlighting its potential application to artificial intelligence ( ai ) . the paper provides a very brief introduction to basic qc issues like quantum registers , quantum gates and quantum algorithms and then it presents references , ideas and research guidelines on how qc can be used to deal with some basic ai problems , such as search and pattern matching , as soon as quantum computers become widely available .

developing an ontology for the access to the contents of an archival fonds : the case of the catasto gregoriano
the research was proposed to exploit and extend the relational and contextual nature of the information assets of the catasto gregoriano , kept at the archivio di stato in rome . developed within the modeus project ( making open data effectively usable ) , this study originates from the following key ideas of modeus : to require open data to be expressed in terms of an ontology , and to include such an ontology as a documentation of the data themselves . thus , open data are naturally linked by means of the ontology , which meets the requirements of the linked open data vision .

inferring the underlying structure of information cascades
in social networks , information and influence diffuse among users as cascades . while the importance of studying cascades has been recognized in various applications , it is difficult to observe the complete structure of cascades in practice . moreover , much less is known on how to infer cascades based on partial observations . in this paper we study the cascade inference problem following the independent cascade model , and provide a full treatment from complexity to algorithms : ( a ) we propose the idea of consistent trees as the inferred structures for cascades ; these trees connect source nodes and observed nodes with paths satisfying the constraints from the observed temporal information . ( b ) we introduce metrics to measure the likelihood of consistent trees as inferred cascades , as well as several optimization problems for finding them . ( c ) we show that the decision problems for consistent trees are in general np-complete , and that the optimization problems are hard to approximate . ( d ) we provide approximation algorithms with performance guarantees on the quality of the inferred cascades , as well as heuristics . we experimentally verify the efficiency and effectiveness of our inference algorithms , using real and synthetic data .

mathematical knowledge representation : semantic models and formalisms
the paper provides a survey of semantic methods for solution of fundamental tasks in mathematical knowledge management . ontological models and formalisms are discussed . we propose an ontology of mathematical knowledge , covering a wide range of fields of mathematics . we demonstrate applications of this representation in mathematical formula search , and learning .

properties of sparse distributed representations and their application to hierarchical temporal memory
empirical evidence demonstrates that every region of the neocortex represents information using sparse activity patterns . this paper examines sparse distributed representations ( sdrs ) , the primary information representation strategy in hierarchical temporal memory ( htm ) systems and the neocortex . we derive a number of properties that are core to scaling , robustness , and generalization . we use the theory to provide practical guidelines and illustrate the power of sdrs as the basis of htm . our goal is to help create a unified mathematical and practical framework for sdrs as it relates to cortical function .

obtaining reliable feedback for sanctioning reputation mechanisms
reputation mechanisms offer an effective alternative to verification authorities for building trust in electronic markets with moral hazard . future clients guide their business decisions by considering the feedback from past transactions ; if truthfully exposed , cheating behavior is sanctioned and thus becomes irrational . it therefore becomes important to ensure that rational clients have the right incentives to report honestly . as an alternative to side-payment schemes that explicitly reward truthful reports , we show that honesty can emerge as a rational behavior when clients have a repeated presence in the market . to this end we describe a mechanism that supports an equilibrium where truthful feedback is obtained . then we characterize the set of pareto-optimal equilibria of the mechanism , and derive an upper bound on the percentage of false reports that can be recorded by the mechanism . an important role in the existence of this bound is played by the fact that rational clients can establish a reputation for reporting honestly .

onto2vec : joint vector-based representation of biological entities and their ontology-based annotations
we propose the onto2vec method , an approach to learn feature vectors for biological entities based on their annotations to biomedical ontologies . our method can be applied to a wide range of bioinformatics research problems such as similarity-based prediction of interactions between proteins , classification of interaction types using supervised learning , or clustering .

on the comparison of plans : proposition of an instability measure for dynamic machine scheduling
on the basis of an analysis of previous research , we present a generalized approach for measuring the difference of plans with an exemplary application to machine scheduling . our work is motivated by the need for such measures , which are used in dynamic scheduling and planning situations . in this context , quantitative approaches are needed for the assessment of the robustness and stability of schedules . obviously , any ` robustness ' or ` stability ' of plans has to be defined w. r. t. the particular situation and the requirements of the human decision maker . besides the proposition of an instability measure , we therefore discuss possibilities of obtaining meaningful information from the decision maker for the implementation of the introduced approach .

bounded rational decision-making in changing environments
a perfectly rational decision-maker chooses the best action with the highest utility gain from a set of possible actions . the optimality principles that describe such decision processes do not take into account the computational costs of finding the optimal action . bounded rational decision-making addresses this problem by specifically trading off information-processing costs and expected utility . interestingly , a similar trade-off between energy and entropy arises when describing changes in thermodynamic systems . this similarity has been recently used to describe bounded rational agents . crucially , this framework assumes that the environment does not change while the decision-maker is computing the optimal policy . when this requirement is not fulfilled , the decision-maker will suffer inefficiencies in utility , that arise because the current policy is optimal for an environment in the past . here we borrow concepts from non-equilibrium thermodynamics to quantify these inefficiencies and illustrate with simulations its relationship with computational resources .

an approximation of the universal intelligence measure
the universal intelligence measure is a recently proposed formal definition of intelligence . it is mathematically specified , extremely general , and captures the essence of many informal definitions of intelligence . it is based on hutter 's universal artificial intelligence theory , an extension of ray solomonoff 's pioneering work on universal induction . since the universal intelligence measure is only asymptotically computable , building a practical intelligence test from it is not straightforward . this paper studies the practical issues involved in developing a real-world uim-based performance metric . based on our investigation , we develop a prototype implementation which we use to evaluate a number of different artificial agents .

semantic matchmaking as non-monotonic reasoning : a description logic approach
matchmaking arises when supply and demand meet in an electronic marketplace , or when agents search for a web service to perform some task , or even when recruiting agencies match curricula and job profiles . in such open environments , the objective of a matchmaking process is to discover best available offers to a given request . we address the problem of matchmaking from a knowledge representation perspective , with a formalization based on description logics . we devise concept abduction and concept contraction as non-monotonic inferences in description logics suitable for modeling matchmaking in a logical framework , and prove some related complexity results . we also present reasonable algorithms for semantic matchmaking based on the devised inferences , and prove that they obey to some commonsense properties . finally , we report on the implementation of the proposed matchmaking framework , which has been used both as a mediator in e-marketplaces and for semantic web services discovery .

statistical modeling in continuous speech recognition ( csr ) ( invited talk )
automatic continuous speech recognition ( csr ) is sufficiently mature that a variety of real world applications are now possible including large vocabulary transcription and interactive spoken dialogues . this paper reviews the evolution of the statistical modelling techniques which underlie current-day systems , specifically hidden markov models ( hmms ) and n-grams . starting from a description of the speech signal and its parameterisation , the various modelling assumptions and their consequences are discussed . it then describes various techniques by which the effects of these assumptions can be mitigated . despite the progress that has been made , the limitations of current modelling techniques are still evident . the paper therefore concludes with a brief review of some of the more fundamental modelling work now in progress .

measuring the similarity of sentential arguments in dialog
when people converse about social or political topics , similar arguments are often paraphrased by different speakers , across many different conversations . debate websites produce curated summaries of arguments on such topics ; these summaries typically consist of lists of sentences that represent frequently paraphrased propositions , or labels capturing the essence of one particular aspect of an argument , e.g . morality or second amendment . we call these frequently paraphrased propositions argument facets . like these curated sites , our goal is to induce and identify argument facets across multiple conversations , and produce summaries . however , we aim to do this automatically . we frame the problem as consisting of two steps : we first extract sentences that express an argument from raw social media dialogs , and then rank the extracted arguments in terms of their similarity to one another . sets of similar arguments are used to represent argument facets . we show here that we can predict argument facet similarity with a correlation averaging 0.63 compared to a human topline averaging 0.68 over three debate topics , easily beating several reasonable baselines .

approximation and heuristic algorithms for probabilistic physical search on general graphs
we consider an agent seeking to obtain an item , potentially available at different locations in a physical environment . the traveling costs between locations are known in advance , but there is only probabilistic knowledge regarding the possible prices of the item at any given location . given such a setting , the problem is to find a plan that maximizes the probability of acquiring the good while minimizing both travel and purchase costs . sample applications include agents in search-and-rescue or exploration missions , e.g. , a rover on mars seeking to mine a specific mineral . these probabilistic physical search problems have been previously studied , but we present the first approximation and heuristic algorithms for solving such problems on general graphs . we establish an interesting connection between these problems and classical graph-search problems , which led us to provide the approximation algorithms and hardness of approximation results for our settings . we further suggest several heuristics for practical use , and demonstrate their effectiveness with simulation on real graph structure and synthetic graphs .

minimal proof search for modal logic k model checking
most modal logics such as s5 , ltl , or atl are extensions of modal logic k. while the model checking problems for ltl and to a lesser extent atl have been very active research areas for the past decades , the model checking problem for the more basic multi-agent modal logic k ( mmlk ) has important applications as a formal framework for perfect information multi-player games on its own . we present minimal proof search ( mps ) , an effort number based algorithm solving the model checking problem for mmlk . we prove two important properties for mps beyond its correctness . the ( dis ) proof exhibited by mps is of minimal cost for a general definition of cost , and mps is an optimal algorithm for finding ( dis ) proofs of minimal cost . optimality means that any comparable algorithm either needs to explore a bigger or equal state space than mps , or is not guaranteed to find a ( dis ) proof of minimal cost on every input . as such , our work relates to a* and ao* in heuristic search , to proof number search and dfpn+ in two-player games , and to counterexample minimization in software model checking .

a joint model for question answering and question generation
we propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents . the proposed model uses a sequence-to-sequence framework that encodes the document and generates a question ( answer ) given an answer ( question ) . significant improvement in model performance is observed empirically on the squad corpus , confirming our hypothesis that the model benefits from jointly learning to perform both tasks . we believe the joint model 's novelty offers a new perspective on machine comprehension beyond architectural engineering , and serves as a first step towards autonomous information seeking .

a multiagent urban traffic simulation . part ii : dealing with the extraordinary
in probabilistic risk management , risk is characterized by two quantities : the magnitude ( or severity ) of the adverse consequences that can potentially result from the given activity or action , and by the likelihood of occurrence of the given adverse consequences . but a risk seldom exists in isolation : chain of consequences must be examined , as the outcome of one risk can increase the likelihood of other risks . systemic theory must complement classic prm . indeed these chains are composed of many different elements , all of which may have a critical importance at many different levels . furthermore , when urban catastrophes are envisioned , space and time constraints are key determinants of the workings and dynamics of these chains of catastrophes : models must include a correct spatial topology of the studied risk . finally , literature insists on the importance small events can have on the risk on a greater scale : urban risks management models belong to self-organized criticality theory . we chose multiagent systems to incorporate this property in our model : the behavior of an agent can transform the dynamics of important groups of them .

comments on `` a new combination of evidence based on compromise '' by k. yamada
comments on `` a new combination of evidence based on compromise '' by k. yamada

learning discrete bayesian networks from continuous data
real data often contains a mixture of discrete and continuous variables , but many bayesian network structure learning and inference algorithms assume all random variables are discrete . continuous variables are often discretized , but the choice of discretization policy has significant impact on the accuracy , speed , and interpretability of the resulting models . this paper introduces a principled bayesian discretization method for continuous variables in bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques . empirical demonstrations show that the proposed method is superior to the state of the art . in addition , this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn bayesian network structures .

temporal logic programs with variables
in this note we consider the problem of introducing variables in temporal logic programs under the formalism of `` temporal equilibrium logic '' ( tel ) , an extension of answer set programming ( asp ) for dealing with linear-time modal operators . to this aim , we provide a definition of a first-order version of tel that shares the syntax of first-order linear-time temporal logic ( ltl ) but has a different semantics , selecting some ltl models we call `` temporal stable models '' . then , we consider a subclass of theories ( called `` splittable temporal logic programs '' ) that are close to usual logic programs but allowing a restricted use of temporal operators . in this setting , we provide a syntactic definition of `` safe variables '' that suffices to show the property of `` domain independence '' -- that is , addition of arbitrary elements in the universe does not vary the set of temporal stable models . finally , we present a method for computing the derivable facts by constructing a non-temporal logic program with variables that is fed to a standard asp grounder . the information provided by the grounder is then used to generate a subset of ground temporal rules which is equivalent to ( and generally smaller than ) the full program instantiation .

incorporating copying mechanism in sequence-to-sequence learning
we address an important problem in sequence-to-sequence ( seq2seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence . a similar phenomenon is observable in human language communication . for example , humans tend to repeat entity names or even long phrases in conversation . the challenge with regard to copying in seq2seq is that new machinery is needed to decide when to perform the operation . in this paper , we incorporate copying into neural network-based seq2seq learning and propose a new model called copynet with encoder-decoder structure . copynet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence . our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of copynet . for example , copynet can outperform regular rnn-based model with remarkable margins on text summarization tasks .

back to the basics : bayesian extensions of irt outperform neural networks for proficiency estimation
estimating student proficiency is an important task for computer based learning systems . we compare a family of irt-based proficiency estimation methods to deep knowledge tracing ( dkt ) , a recently proposed recurrent neural network model with promising initial results . we evaluate how well each model predicts a student 's future response given previous responses using two publicly available and one proprietary data set . we find that irt-based methods consistently matched or outperformed dkt across all data sets at the finest level of content granularity that was tractable for them to be trained on . a hierarchical extension of irt that captured item grouping structure performed best overall . when data sets included non-trivial autocorrelations in student response patterns , a temporal extension of irt improved performance over standard irt while the rnn-based method did not . we conclude that irt-based models provide a simpler , better-performing alternative to existing rnn-based models of student interaction data while also affording more interpretability and guarantees due to their formulation as bayesian probabilistic models .

beem : bucket elimination with external memory
a major limitation of exact inference algorithms for probabilistic graphical models is their extensive memory usage , which often puts real-world problems out of their reach . in this paper we show how we can extend inference algorithms , particularly bucket elimination , a special case of cluster ( join ) tree decomposition , to utilize disk memory . we provide the underlying ideas and show promising empirical results of exactly solving large problems not solvable before .

qualitative decision theory with sugeno integrals
this paper presents an axiomatic framework for qualitative decision under uncertainty in a finite setting . the corresponding utility is expressed by a sup-min expression , called sugeno ( or fuzzy ) integral . technically speaking , sugeno integral is a median , which is indeed a qualitative counterpart to the averaging operation underlying expected utility . the axiomatic justification of sugeno integral-based utility is expressed in terms of preference between acts as in savage decision theory . pessimistic and optimistic qualitative utilities , based on necessity and possibility measures , previously introduced by two of the authors , can be retrieved in this setting by adding appropriate axioms .

the relationship between and/or search and variable elimination
in this paper we compare search and inference in graphical models through the new framework of and/or search . specifically , we compare variable elimination ( ve ) and memoryintensive and/or search ( ao ) and place algorithms such as graph-based backjumping and no-good and good learning , as well as recursive conditioning [ 7 ] and value elimination [ 2 ] within the and/or search framework .

breaking symmetries in graph search with canonizing sets
there are many complex combinatorial problems which involve searching for an undirected graph satisfying given constraints . such problems are often highly challenging because of the large number of isomorphic representations of their solutions . this paper introduces effective and compact , complete symmetry breaking constraints for small graph search . enumerating with these symmetry breaks generates all and only non-isomorphic solutions . for small search problems , with up to $ 10 $ vertices , we compute instance independent symmetry breaking constraints . for small search problems with a larger number of vertices we demonstrate the computation of instance dependent constraints which are complete . we illustrate the application of complete symmetry breaking constraints to extend two known sequences from the oeis related to graph enumeration . we also demonstrate the application of a generalization of our approach to fully-interchangeable matrix search problems .

the factored frontier algorithm for approximate inference in dbns
the factored frontier ( ff ) algorithm is a simple approximate inferencealgorithm for dynamic bayesian networks ( dbns ) . it is very similar tothe fully factorized version of the boyen-koller ( bk ) algorithm , butinstead of doing an exact update at every step followed bymarginalisation ( projection ) , it always works with factoreddistributions . hence it can be applied to models for which the exactupdate step is intractable . we show that ff is equivalent to ( oneiteration of ) loopy belief propagation ( lbp ) on the original dbn , andthat bk is equivalent ( to one iteration of ) lbp on a dbn where wecluster some of the nodes . we then show empirically that byiterating , lbp can improve on the accuracy of both ff and bk . wecompare these algorithms on two real-world dbns : the first is a modelof a water treatment plant , and the second is a coupled hmm , used tomodel freeway traffic .

back analysis based on som-rst system
this paper describes application of information granulation theory , on the back analysis of jeffrey mine southeast wall quebec . in this manner , using a combining of self organizing map ( som ) and rough set theory ( rst ) , crisp and rough granules are obtained . balancing of crisp granules and sub rough granules is rendered in close-open iteration . combining of hard and soft computing , namely finite difference method ( fdm ) and computational intelligence and taking in to account missing information are two main benefits of the proposed method . as a practical example , reverse analysis on the failure of the southeast wall jeffrey mine is accomplished .

the complexity of manipulating $ k $ -approval elections
an important problem in computational social choice theory is the complexity of undesirable behavior among agents , such as control , manipulation , and bribery in election systems . these kinds of voting strategies are often tempting at the individual level but disastrous for the agents as a whole . creating election systems where the determination of such strategies is difficult is thus an important goal . an interesting set of elections is that of scoring protocols . previous work in this area has demonstrated the complexity of misuse in cases involving a fixed number of candidates , and of specific election systems on unbounded number of candidates such as borda . in contrast , we take the first step in generalizing the results of computational complexity of election misuse to cases of infinitely many scoring protocols on an unbounded number of candidates . interesting families of systems include $ k $ -approval and $ k $ -veto elections , in which voters distinguish $ k $ candidates from the candidate set . our main result is to partition the problems of these families based on their complexity . we do so by showing they are polynomial-time computable , np-hard , or polynomial-time equivalent to another problem of interest . we also demonstrate a surprising connection between manipulation in election systems and some graph theory problems .

exception-based knowledge updates
existing methods for dealing with knowledge updates differ greatly depending on the underlying knowledge representation formalism . when classical logic is used , updates are typically performed by manipulating the knowledge base on the model-theoretic level . on the opposite side of the spectrum stand the semantics for updating answer-set programs that need to rely on rule syntax . yet , a unifying perspective that could embrace both these branches of research is of great importance as it enables a deeper understanding of all involved methods and principles and creates room for their cross-fertilisation , ripening and further development . this paper bridges the seemingly irreconcilable approaches to updates . it introduces a novel monotonic characterisation of rules , dubbed re-models , and shows it to be a more suitable semantic foundation for rule updates than se-models . then it proposes a generic scheme for specifying semantic rule update operators , based on the idea of viewing a program as the set of sets of re-models of its rules ; updates are performed by introducing additional interpretations - exceptions - to the sets of re-models of rules in the original program . the introduced scheme is used to define rule update operators that are closely related to both classical update principles and traditional approaches to rules updates , and serve as a basis for a solution to the long-standing problem of state condensing , showing how they can be equivalently defined as binary operators on some class of logic programs . finally , the essence of these ideas is extracted to define an abstract framework for exception-based update operators , viewing a knowledge base as the set of sets of models of its elements , which can capture a wide range of both model- and formula-based classical update operators , and thus serves as the first firm formal ground connecting classical and rule updates .

learning an integrated distance metric for comparing structure of complex networks
graph comparison plays a major role in many network applications . we often need a similarity metric for comparing networks according to their structural properties . various network features - such as degree distribution and clustering coefficient - provide measurements for comparing networks from different points of view , but a global and integrated distance metric is still missing . in this paper , we employ distance metric learning algorithms in order to construct an integrated distance metric for comparing structural properties of complex networks . according to natural witnesses of network similarities ( such as network categories ) the distance metric is learned by the means of a dataset of some labeled real networks . for evaluating our proposed method which is called netdistance , we applied it as the distance metric in k-nearest-neighbors classification . empirical results show that netdistance outperforms previous methods , at least 20 percent , with respect to precision .

gaussian process kernels for pattern discovery and extrapolation
gaussian processes are rich distributions over functions , which provide a bayesian nonparametric approach to smoothing and interpolation . we introduce simple closed form kernels that can be used with gaussian processes to discover patterns and enable extrapolation . these kernels are derived by modelling a spectral density -- the fourier transform of a kernel -- with a gaussian mixture . the proposed kernels support a broad class of stationary covariances , but gaussian process inference remains simple and analytic . we demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples , as well as atmospheric co2 trends and airline passenger data . we also show that we can reconstruct standard covariances within our framework .

generative adversarial network for abstractive text summarization
in this paper , we propose an adversarial process for abstractive text summarization , in which we simultaneously train a generative model g and a discriminative model d. in particular , we build the generator g as an agent of reinforcement learning , which takes the raw text as input and predicts the abstractive summarization . we also build a discriminator which attempts to distinguish the generated summary from the ground truth summary . extensive experiments demonstrate that our model achieves competitive rouge scores with the state-of-the-art methods on cnn/daily mail dataset . qualitatively , we show that our model is able to generate more abstractive , readable and diverse summaries .

discovering cyclic causal models by independent components analysis
we generalize shimizu et al 's ( 2006 ) ica-based approach for discovering linear non-gaussian acyclic ( lingam ) structural equation models ( sems ) from causally sufficient , continuous-valued observational data . by relaxing the assumption that the generating sem 's graph is acyclic , we solve the more general problem of linear non-gaussian ( ling ) sem discovery . ling discovery algorithms output the distribution equivalence class of sems which , in the large sample limit , represents the population distribution . we apply a ling discovery algorithm to simulated data . finally , we give sufficient conditions under which only one of the sems in the output class is 'stable ' .

group event detection with a varying number of group members for video surveillance
this paper presents a novel approach for automatic recognition of group activities for video surveillance applications . we propose to use a group representative to handle the recognition with a varying number of group members , and use an asynchronous hidden markov model ( ahmm ) to model the relationship between people . furthermore , we propose a group activity detection algorithm which can handle both symmetric and asymmetric group activities , and demonstrate that this approach enables the detection of hierarchical interactions between people . experimental results show the effectiveness of our approach .

detecção de comunidades em redes complexas para identificar gargalos e desperdício de recursos em sistemas de ônibus
we propose here a methodology to help to understand the shortcomings of public transportation in a city via the mining of complex networks representing the supply and demand of public transport . we show how to build these networks based upon data on smart card use in buses via the application of algorithms that estimate an od and reconstruct the complete itinerary of the passengers . the overlapping of the two networks sheds light in potential overload and waste in the offer of resources that can be mitigated with strategies for balancing supply and demand .

imitation learning with a value-based prior
the goal of imitation learning is for an apprentice to learn how to behave in a stochastic environment by observing a mentor demonstrating the correct behavior . accurate prior knowledge about the correct behavior can reduce the need for demonstrations from the mentor . we present a novel approach to encoding prior knowledge about the correct behavior , where we assume that this prior knowledge takes the form of a markov decision process ( mdp ) that is used by the apprentice as a rough and imperfect model of the mentor 's behavior . specifically , taking a bayesian approach , we treat the value of a policy in this modeling mdp as the log prior probability of the policy . in other words , we assume a priori that the mentor 's behavior is likely to be a high value policy in the modeling mdp , though quite possibly different from the optimal policy . we describe an efficient algorithm that , given a modeling mdp and a set of demonstrations by a mentor , provably converges to a stationary point of the log posterior of the mentor 's policy , where the posterior is computed with respect to the `` value based '' prior . we also present empirical evidence that this prior does in fact speed learning of the mentor 's policy , and is an improvement in our experiments over similar previous methods .

curiosity-driven reinforcement learning with homeostatic regulation
we propose a curiosity reward based on information theory principles and consistent with the animal instinct to maintain certain critical parameters within a bounded range . our experimental validation shows the added value of the additional homeostatic drive to enhance the overall information gain of a reinforcement learning agent interacting with a complex environment using continuous actions . our method builds upon two ideas : i ) to take advantage of a new bellman-like equation of information gain and ii ) to simplify the computation of the local rewards by avoiding the approximation of complex distributions over continuous states and actions .

automated reasoning using possibilistic logic : semantics , belief revision and variable certainty weights
in this paper an approach to automated deduction under uncertainty , based on possibilistic logic , is proposed ; for that purpose we deal with clauses weighted by a degree which is a lower bound of a necessity or a possibility measure , according to the nature of the uncertainty . two resolution rules are used for coping with the different situations , and the refutation method can be generalized . besides the lower bounds are allowed to be functions of variables involved in the clause , which gives hypothetical reasoning capabilities . the relation between our approach and the idea of minimizing abnormality is briefly discussed . in case where only lower bounds of necessity measures are involved , a semantics is proposed , in which the completeness of the extended resolution principle is proved . moreover deduction from a partially inconsistent knowledge base can be managed in this approach and displays some form of non-monotonicity .

a note on the pac bayesian theorem
we prove general exponential moment inequalities for averages of [ 0,1 ] -valued iid random variables and use them to tighten the pac bayesian theorem . the logarithmic dependence on the sample count in the enumerator of the pac bayesian bound is halved .

the possibilities and limitations of private prediction markets
we consider the design of private prediction markets , financial markets designed to elicit predictions about uncertain events without revealing too much information about market participants ' actions or beliefs . our goal is to design market mechanisms in which participants ' trades or wagers influence the market 's behavior in a way that leads to accurate predictions , yet no single participant has too much influence over what others are able to observe . we study the possibilities and limitations of such mechanisms using tools from differential privacy . we begin by designing a private one-shot wagering mechanism in which bettors specify a belief about the likelihood of a future event and a corresponding monetary wager . wagers are redistributed among bettors in a way that more highly rewards those with accurate predictions . we provide a class of wagering mechanisms that are guaranteed to satisfy truthfulness , budget balance in expectation , and other desirable properties while additionally guaranteeing epsilon-joint differential privacy in the bettors ' reported beliefs , and analyze the trade-off between the achievable level of privacy and the sensitivity of a bettor 's payment to her own report . we then ask whether it is possible to obtain privacy in dynamic prediction markets , focusing our attention on the popular cost-function framework in which securities with payments linked to future events are bought and sold by an automated market maker . we show that under general conditions , it is impossible for such a market maker to simultaneously achieve bounded worst-case loss and epsilon-differential privacy without allowing the privacy guarantee to degrade extremely quickly as the number of trades grows , making such markets impractical in settings in which privacy is valued . we conclude by suggesting several avenues for potentially circumventing this lower bound .

transmission of distress in a bank credit network
the european sovereign debt crisis has impaired many european banks . the distress on the european banks may transmit worldwide , and result in a large-scale knock-on default of financial institutions . this study presents a computer simulation model to analyze the risk of insolvency of banks and defaults in a bank credit network . simulation experiments reproduce the knock-on default , and quantify the impact which is imposed on the number of bank defaults by heterogeneity of the bank credit network , the equity capital ratio of banks , and the capital surcharge on big banks .

an intelligent extension of variable neighbourhood search for labelling graph problems
in this paper we describe an extension of the variable neighbourhood search ( vns ) which integrates the basic vns with other complementary approaches from machine learning , statistics and experimental algorithmic , in order to produce high-quality performance and to completely automate the resulting optimization strategy . the resulting intelligent vns has been successfully applied to a couple of optimization problems where the solution space consists of the subsets of a finite reference set . these problems are the labelled spanning tree and forest problems that are formulated on an undirected labelled graph ; a graph where each edge has a label in a finite set of labels l. the problems consist on selecting the subset of labels such that the subgraph generated by these labels has an optimal spanning tree or forest , respectively . these problems have several applications in the real-world , where one aims to ensure connectivity by means of homogeneous connections .

bachelor 's thesis on generative probabilistic programming ( in russian language , june 2014 )
this bachelor 's thesis , written in russian , is devoted to a relatively new direction in the field of machine learning and artificial intelligence , namely probabilistic programming . the thesis gives a brief overview to the already existing probabilistic programming languages : church , venture , and anglican . it also describes the results of the first experiments on the automatic induction of probabilistic programs . the thesis was submitted , in june 2014 , in partial fulfilment of the requirements for the degree of bachelor of science in mathematics in the department of mathematics and computer science , siberian federal university , krasnoyarsk , russia . the work , which is described in this thesis , has been performing in 2012-2014 in the massachusetts institute of technology and in the university of oxford by the colleagues of the author and by himself .

formal concept analysis for knowledge discovery from biological data
due to rapid advancement in high-throughput techniques , such as microarrays and next generation sequencing technologies , biological data are increasing exponentially . the current challenge in computational biology and bioinformatics research is how to analyze these huge raw biological data to extract biologically meaningful knowledge . this review paper presents the applications of formal concept analysis for the analysis and knowledge discovery from biological data , including gene expression discretization , gene co-expression mining , gene expression clustering , finding genes in gene regulatory networks , enzyme/protein classifications , binding site classifications , and so on . it also presents a list of fca-based software tools applied in biological domain and covers the challenges faced so far .

on axiomatization of probabilistic conditional independencies
this paper studies the connection between probabilistic conditional independence in uncertain reasoning and data dependency in relational databases . as a demonstration of the usefulness of this preliminary investigation , an alternate proof is presented for refuting the conjecture suggested by pearl and paz that probabilistic conditional independencies have a complete axiomatization .

contextually guided semantic labeling and search for 3d point clouds
rgb-d cameras , which give an rgb image to- gether with depths , are becoming increasingly popular for robotic perception . in this paper , we address the task of detecting commonly found objects in the 3d point cloud of indoor scenes obtained from such cameras . our method uses a graphical model that captures various features and contextual relations , including the local visual appearance and shape cues , object co-occurence relationships and geometric relationships . with a large number of object classes and relations , the model 's parsimony becomes important and we address that by using multiple types of edge potentials . we train the model using a maximum-margin learning approach . in our experiments over a total of 52 3d scenes of homes and offices ( composed from about 550 views ) , we get a performance of 84.06 % and 73.38 % in labeling office and home scenes respectively for 17 object classes each . we also present a method for a robot to search for an object using the learned model and the contextual information available from the current labelings of the scene . we applied this algorithm successfully on a mobile robot for the task of finding 12 object classes in 10 different offices and achieved a precision of 97.56 % with 78.43 % recall .

the advantage of evidential attributes in social networks
nowadays , there are many approaches designed for the task of detecting communities in social networks . among them , some methods only consider the topological graph structure , while others take use of both the graph structure and the node attributes . in real-world networks , there are many uncertain and noisy attributes in the graph . in this paper , we will present how we detect communities in graphs with uncertain attributes in the first step . the numerical , probabilistic as well as evidential attributes are generated according to the graph structure . in the second step , some noise will be added to the attributes . we perform experiments on graphs with different types of attributes and compare the detection results in terms of the normalized mutual information ( nmi ) values . the experimental results show that the clustering with evidential attributes gives better results comparing to those with probabilistic and numerical attributes . this illustrates the advantages of evidential attributes .

structured sparsity via alternating direction methods
we consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm which incorporates prior knowledge of the group structure of the features . such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term . in this paper , we focus on two commonly adopted sparsity-inducing regularization terms , the overlapping group lasso penalty $ l_1/l_2 $ -norm and the $ l_1/l_\infty $ -norm . we propose a unified framework based on the augmented lagrangian method , under which problems with both types of regularization and their variants can be efficiently solved . as the core building-block of this framework , we develop new algorithms using an alternating partial-linearization/splitting technique , and we prove that the accelerated versions of these algorithms require $ o ( \frac { 1 } { \sqrt { \epsilon } } ) $ iterations to obtain an $ \epsilon $ -optimal solution . to demonstrate the efficiency and relevance of our algorithms , we test them on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms .

multi-attention recurrent network for human communication comprehension
human face-to-face communication is a complex multimodal signal . we use words ( language modality ) , gestures ( vision modality ) and changes in tone ( acoustic modality ) to convey our intentions . humans easily process and understand face-to-face communication , however , comprehending this form of communication remains a significant challenge for artificial intelligence ( ai ) . ai must understand each modality and the interactions between them that shape human communication . in this paper , we present a novel neural architecture for understanding human communication called the multi-attention recurrent network ( marn ) . the main strength of our model comes from discovering interactions between modalities through time using a neural component called the multi-attention block ( mab ) and storing them in the hybrid memory of a recurrent component called the long-short term hybrid memory ( lsthm ) . we perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis , speaker trait recognition and emotion recognition . marn shows state-of-the-art performance on all the datasets .

computer assisted composition with recurrent neural networks
sequence modeling with neural networks has lead to powerful models of symbolic music data . we address the problem of exploiting these models to reach creative musical goals , by combining with human input . to this end we generalise previous work , which sampled markovian sequence models under the constraint that the sequence belong to the language of a given finite state machine provided by the human . we consider more expressive non-markov models , thereby requiring approximate sampling which we provide in the form of an efficient sequential monte carlo method . in addition we provide and compare with a beam search strategy for conditional probability maximisation . our algorithms are capable of convincingly re-harmonising famous musical works . to demonstrate this we provide visualisations , quantitative experiments , a human listening test and audio examples . we find both the sampling and optimisation procedures to be effective , yet complementary in character . for the case of highly permissive constraint sets , we find that sampling is to be preferred due to the overly regular nature of the optimisation based results . the generality of our algorithms permits countless other creative applications .

end-to-end tracking and semantic segmentation using recurrent neural networks
in this work we present a novel end-to-end framework for tracking and classifying a robot 's surroundings in complex , dynamic and only partially observable real-world environments . the approach deploys a recurrent neural network to filter an input stream of raw laser measurements in order to directly infer object locations , along with their identity in both visible and occluded areas . to achieve this we first train the network using unsupervised deep tracking , a recently proposed theoretical framework for end-to-end space occupancy prediction . we show that by learning to track on a large amount of unsupervised data , the network creates a rich internal representation of its environment which we in turn exploit through the principle of inductive transfer of knowledge to perform the task of it 's semantic classification . as a result , we show that only a small amount of labelled data suffices to steer the network towards mastering this additional task . furthermore we propose a novel recurrent neural network architecture specifically tailored to tracking and semantic classification in real-world robotics applications . we demonstrate the tracking and classification performance of the method on real-world data collected at a busy road junction . our evaluation shows that the proposed end-to-end framework compares favourably to a state-of-the-art , model-free tracking solution and that it outperforms a conventional one-shot training scheme for semantic classification .

using thought-provoking children 's questions to drive artificial intelligence research
we propose to use thought-provoking children 's questions ( tpcqs ) , namely highlights brainplay questions , as a new method to drive artificial intelligence research and to evaluate the capabilities of general-purpose ai systems . these questions are designed to stimulate thought and learning in children , and they can be used to do the same thing in ai systems , while demonstrating the system 's reasoning capabilities to the evaluator . we introduce the tpcq task , which which takes a tpcq question as input and produces as output ( 1 ) answers to the question and ( 2 ) learned generalizations . we discuss how brainplay questions stimulate learning . we analyze 244 brainplay questions , and we report statistics on question type , question class , answer cardinality , answer class , types of knowledge needed , and types of reasoning needed . we find that brainplay questions span many aspects of intelligence . because the answers to brainplay questions and the generalizations learned from them are often highly open-ended , we suggest using human judges for evaluation .

jeffrey 's rule of conditioning generalized to belief functions
jeffrey 's rule of conditioning has been proposed in order to revise a probability measure by another probability function . we generalize it within the framework of the models based on belief functions . we show that several forms of jeffrey 's conditionings can be defined that correspond to the geometrical rule of conditioning and to dempster 's rule of conditioning , respectively .

on the semantic relationship between probabilistic soft logic and markov logic
markov logic networks ( mln ) and probabilistic soft logic ( psl ) are widely applied formalisms in statistical relational learning , an emerging area in artificial intelligence that is concerned with combining logical and statistical ai . despite their resemblance , the relationship has not been formally stated . in this paper , we describe the precise semantic relationship between them from a logical perspective . this is facilitated by first extending fuzzy logic to allow weights , which can be also viewed as a generalization of psl , and then relate that generalization to mln . we observe that the relationship between psl and mln is analogous to the known relationship between fuzzy logic and boolean logic , and furthermore the weight scheme of psl is essentially a generalization of the weight scheme of mln for the many-valued setting .

towards reducing the multidimensionality of olap cubes using the evolutionary algorithms and factor analysis methods
data warehouses are structures with large amount of data collected from heterogeneous sources to be used in a decision support system . data warehouses analysis identifies hidden patterns initially unexpected which analysis requires great memory and computation cost . data reduction methods were proposed to make this analysis easier . in this paper , we present a hybrid approach based on genetic algorithms ( ga ) as evolutionary algorithms and the multiple correspondence analysis ( mca ) as analysis factor methods to conduct this reduction . our approach identifies reduced subset of dimensions from the initial subset p where p ' < p where it is proposed to find the profile fact that is the closest to reference . gas identify the possible subsets and the khi formula of the acm evaluates the quality of each subset . the study is based on a distance measurement between the reference and n facts profile extracted from the warehouses .

the pet-fish problem on the world-wide web
we identify the presence of pet-fish problem situations and the corresponding guppy effect of concept theory on the world-wide web . for this purpose , we introduce absolute weights for words expressing concepts and relative weights between words expressing concepts , and the notion of 'meaning bound ' between two words expressing concepts , making explicit use of the conceptual structure of the world-wide web . the pet-fish problem occurs whenever there are exemplars - in the case of pet and fish these can be guppy or goldfish - for which the meaning bound with respect to the conjunction is stronger than the meaning bounds with respect to the individual concepts .

synthesizing the preferred inputs for neurons in neural networks via deep generator networks
deep neural networks ( dnns ) have demonstrated state-of-the-art results on many pattern recognition tasks , especially vision classification problems . understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve dnns . one path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect . one such method is called activation maximization ( am ) , which synthesizes an input ( e.g . an image ) that highly activates a neuron . here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful , learned prior : a deep generator network ( dgn ) . the algorithm ( 1 ) generates qualitatively state-of-the-art synthetic images that look almost real , ( 2 ) reveals the features learned by each neuron in an interpretable way , ( 3 ) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned , and ( 4 ) can be considered as a high-quality generative method ( in this case , by generating novel , creative , interesting , recognizable images ) .

testing quantum models of conjunction fallacy on the world wide web
the 'conjunction fallacy ' has been extensively debated by scholars in cognitive science and , in recent times , the discussion has been enriched by the proposal of modeling the fallacy using the quantum formalism . two major quantum approaches have been put forward : the first assumes that respondents use a two-step sequential reasoning and that the fallacy results from the presence of 'question order effects ' ; the second assumes that respondents evaluate the cognitive situation as a whole and that the fallacy results from the 'emergence of new meanings ' , as an 'effect of overextension ' in the conceptual conjunction . thus , the question arises as to determine whether and to what extent conjunction fallacies would result from 'order effects ' or , instead , from 'emergence effects ' . to help clarify this situation , we propose to use the world wide web as an 'information space ' that can be interrogated both in a sequential and non-sequential way , to test these two quantum approaches . we find that 'emergence effects ' , and not 'order effects ' , should be considered the main cognitive mechanism producing the observed conjunction fallacies .

predictive coding-based deep dynamic neural network for visuomotor learning
this study presents a dynamic neural network model based on the predictive coding framework for perceiving and predicting the dynamic visuo-proprioceptive patterns . in our previous study [ 1 ] , we have shown that the deep dynamic neural network model was able to coordinate visual perception and action generation in a seamless manner . in the current study , we extended the previous model under the predictive coding framework to endow the model with a capability of perceiving and predicting dynamic visuo-proprioceptive patterns as well as a capability of inferring intention behind the perceived visuomotor information through minimizing prediction error . a set of synthetic experiments were conducted in which a robot learned to imitate the gestures of another robot in a simulation environment . the experimental results showed that with given intention states , the model was able to mentally simulate the possible incoming dynamic visuo-proprioceptive patterns in a top-down process without the inputs from the external environment . moreover , the results highlighted the role of minimizing prediction error in inferring underlying intention of the perceived visuo-proprioceptive patterns , supporting the predictive coding account of the mirror neuron systems . the results also revealed that minimizing prediction error in one modality induced the recall of the corresponding representation of another modality acquired during the consolidative learning of raw-level visuo-proprioceptive patterns .

arriving on time : estimating travel time distributions on large-scale road networks
most optimal routing problems focus on minimizing travel time or distance traveled . oftentimes , a more useful objective is to maximize the probability of on-time arrival , which requires statistical distributions of travel times , rather than just mean values . we propose a method to estimate travel time distributions on large-scale road networks , using probe vehicle data collected from gps . we present a framework that works with large input of data , and scales linearly with the size of the network . leveraging the planar topology of the graph , the method computes efficiently the time correlations between neighboring streets . first , raw probe vehicle traces are compressed into pairs of travel times and number of stops for each traversed road segment using a ` stop-and-go ' algorithm developed for this work . the compressed data is then used as input for training a path travel time model , which couples a markov model along with a gaussian markov random field . finally , scalable inference algorithms are developed for obtaining path travel time distributions from the composite mm-gmrf model . we illustrate the accuracy and scalability of our model on a 505,000 road link network spanning the san francisco bay area .

cdvae : co-embedding deep variational auto encoder for conditional variational generation
problems such as predicting a new shading field ( y ) for an image ( x ) are ambiguous : many very distinct solutions are good . representing this ambiguity requires building a conditional model p ( y|x ) of the prediction , conditioned on the image . such a model is difficult to train , because we do not usually have training data containing many different shadings for the same image . as a result , we need different training examples to share data to produce good models . this presents a danger we call `` code space collapse '' - the training procedure produces a model that has a very good loss score , but which represents the conditional distribution poorly . we demonstrate an improved method for building conditional models by exploiting a metric constraint on training data that prevents code space collapse . we demonstrate our model on two example tasks using real data : image saturation adjustment , image relighting . we describe quantitative metrics to evaluate ambiguous generation results . our results quantitatively and qualitatively outperform different strong baselines .

end-to-end task-completion neural dialogue systems
one of the major drawbacks of modularized task-completion dialogue systems is that each module is trained individually , which presents several challenges . for example , downstream modules are affected by earlier modules , and the performance of the entire system is not robust to the accumulated errors . this paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues . our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks . the reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system . our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation , but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module .

large-margin learning of submodular summarization methods
in this paper , we present a supervised learning approach to training submodular scoring functions for extractive multi-document summarization . by taking a structured predicition approach , we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure . the learning method applies to all submodular summarization methods , and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets . compared to state-of-the-art functions that were tuned manually , our method significantly improves performance and enables high-fidelity models with numbers of parameters well beyond what could reasonbly be tuned by hand .

belief updating and learning in semi-qualitative probabilistic networks
this paper explores semi-qualitative probabilistic networks ( sqpns ) that combine numeric and qualitative information . we first show that exact inferences with sqpns are nppp-complete . we then show that existing qualitative relations in sqpns ( plus probabilistic logic and imprecise assessments ) can be dealt effectively through multilinear programming . we then discuss learning : we consider a maximum likelihood method that generates point estimates given a sqpn and empirical data , and we describe a bayesian-minded method that employs the imprecise dirichlet model to generate set-valued estimates .

fuzzy logic and probability
in this paper we deal with a new approach to probabilistic reasoning in a logical framework . nearly almost all logics of probability that have been proposed in the literature are based on classical two-valued logic . after making clear the differences between fuzzy logic and probability theory , here we propose a { em fuzzy } logic of probability for which completeness results ( in a probabilistic sense ) are provided . the main idea behind this approach is that probability values of crisp propositions can be understood as truth-values of some suitable fuzzy propositions associated to the crisp ones . moreover , suggestions and examples of how to extend the formalism to cope with conditional probabilities and with other uncertainty formalisms are also provided .

intelligent encoding and economical communication in the visual stream
the theory of computational complexity is used to underpin a recent model of neocortical sensory processing . we argue that encoding into reconstruction networks is appealing for communicating agents using hebbian learning and working on hard combinatorial problems , which are easy to verify . computational definition of the concept of intelligence is provided . simulations illustrate the idea .

feature selection in conditional random fields for map matching of gps trajectories
map matching of the gps trajectory serves the purpose of recovering the original route on a road network from a sequence of noisy gps observations . it is a fundamental technique to many location based services . however , map matching of a low sampling rate on urban road network is still a challenging task . in this paper , the characteristics of conditional random fields with regard to inducing many contextual features and feature selection are explored for the map matching of the gps trajectories at a low sampling rate . experiments on a taxi trajectory dataset show that our method may achieve competitive results along with the success of reducing model complexity for computation-limited applications .

an argumentation-based framework to address the attribution problem in cyber-warfare
attributing a cyber-operation through the use of multiple pieces of technical evidence ( i.e. , malware reverse-engineering and source tracking ) and conventional intelligence sources ( i.e. , human or signals intelligence ) is a difficult problem not only due to the effort required to obtain evidence , but the ease with which an adversary can plant false evidence . in this paper , we introduce a formal reasoning system called the inca ( intelligent cyber attribution ) framework that is designed to aid an analyst in the attribution of a cyber-operation even when the available information is conflicting and/or uncertain . our approach combines argumentation-based reasoning , logic programming , and probabilistic models to not only attribute an operation but also explain to the analyst why the system reaches its conclusions .

feature representation for online signature verification
biometrics systems have been used in a wide range of applications and have improved people authentication . signature verification is one of the most common biometric methods with techniques that employ various specifications of a signature . recently , deep learning has achieved great success in many fields , such as image , sounds and text processing . in this paper , deep learning method has been used for feature extraction and feature selection .

efficient exploration through bayesian deep q-networks
we propose bayesian deep q-network ( bdqn ) , a practical thompson sampling based reinforcement learning ( rl ) algorithm . thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive . we address this limitation by introducing uncertainty only at the output layer of the network through a bayesian linear regression ( blr ) model . this layer can be trained with fast closed-form updates and its samples can be drawn efficiently through the gaussian distribution . we apply our method to a wide range of atari games in arcade learning environments . since bdqn carries out more efficient exploration , it is able to reach higher rewards substantially faster than a key baseline , the double deep q network ( ddqn ) .

robustly leveraging prior knowledge in text classification
prior knowledge has been shown very useful to address many natural language processing tasks . many approaches have been proposed to formalise a variety of knowledge , however , whether the proposed approach is robust or sensitive to the knowledge supplied to the model has rarely been discussed . in this paper , we propose three regularization terms on top of generalized expectation criteria , and conduct extensive experiments to justify the robustness of the proposed methods . experimental results demonstrate that our proposed methods obtain remarkable improvements and are much more robust than baselines .

video question answering via attribute-augmented attention network learning
video question answering is a challenging problem in visual information retrieval , which provides the answer to the referenced video content according to the question . however , the existing visual question answering approaches mainly tackle the problem of static image question , which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents . in this paper , we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism . we propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering . we then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance . we construct a large-scale video question answering dataset . we conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method .

enabling semantic analysis of user browsing patterns in the web of data
a useful step towards better interpretation and analysis of the usage patterns is to formalize the semantics of the resources that users are accessing in the web . we focus on this problem and present an approach for the semantic formalization of usage logs , which lays the basis for eective techniques of querying expressive usage patterns . we also present a query answering approach , which is useful to nd in the logs expressive patterns of usage behavior via formulation of semantic and temporal-based constraints . we have processed over 30 thousand user browsing sessions extracted from usage logs of dbpedia and semantic web dog food . all these events are formalized semantically using respective domain ontologies and rdf representations of the web resources being accessed . we show the eectiveness of our approach through experimental results , providing in this way an exploratory analysis of the way users browse theweb of data .

cognitive discriminative mappings for rapid learning
humans can learn concepts or recognize items from just a handful of examples , while machines require many more samples to perform the same task . in this paper , we build a computational model to investigate the possibility of this kind of rapid learning . the proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory . we present a simple and intuitive technique called cognitive discriminative mappings ( cdm ) to explore the cognitive problem . first , cdm separates and clusters the data instances retrieved from long-term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm . cdm then maps each sensory data instance to be as close as possible to the median point of the data group with the same class . the experimental results demonstrate that the cdm approach is effective for learning the discriminative features of supervised classifications with few training sensory input instances .

comparing bayesian network classifiers
in this paper , we empirically evaluate algorithms for learning four types of bayesian network ( bn ) classifiers - naive-bayes , tree augmented naive-bayes , bn augmented naive-bayes and general bns , where the latter two are learned using two variants of a conditional-independence ( ci ) based bn-learning algorithm . experimental results show the obtained classifiers , learned using the ci based algorithms , are competitive with ( or superior to ) the best known classifiers , based on both bayesian networks and other formalisms ; and that the computational time for learning and using these classifiers is relatively small . moreover , these results also suggest a way to learn yet more effective classifiers ; we demonstrate empirically that this new algorithm does work as expected . collectively , these results argue that bn classifiers deserve more attention in machine learning and data mining communities .

nominals , inverses , counting , and conjunctive queries or : why infinity is your friend !
description logics are knowledge representation formalisms that provide , for example , the logical underpinning of the w3c owl standards . conjunctive queries , the standard query language in databases , have recently gained significant attention as an expressive formalism for querying description logic knowledge bases . several different techniques for deciding conjunctive query entailment are available for a wide range of dls . nevertheless , the combination of nominals , inverse roles , and number restrictions in owl 1 and owl 2 dl causes unsolvable problems for the techniques hitherto available . we tackle this problem and present a decidability result for entailment of unions of conjunctive queries in the dl alchoiqb that contains all three problematic constructors simultaneously . provided that queries contain only simple roles , our result also shows decidability of entailment of ( unions of ) conjunctive queries in the logic that underpins owl 1 dl and we believe that the presented results will pave the way for further progress towards conjunctive query entailment decision procedures for the description logics underlying the owl standards .

multi-task gloh feature selection for human age estimation
in this paper , we propose a novel age estimation method based on gloh feature descriptor and multi-task learning ( mtl ) . the gloh feature descriptor , one of the state-of-the-art feature descriptor , is used to capture the age-related local and spatial information of face image . as the exacted gloh features are often redundant , mtl is designed to select the most informative feature bins for age estimation problem , while the corresponding weights are determined by ridge regression . this approach largely reduces the dimensions of feature , which can not only improve performance but also decrease the computational burden . experiments on the public available fg-net database show that the proposed method can achieve comparable performance over previous approaches while using much fewer features .

elastic solver : balancing solution time and energy consumption
combinatorial decision problems arise in many different domains such as scheduling , routing , packing , bioinformatics , and many more . despite recent advances in developing scalable solvers , there are still many problems which are often very hard to solve . typically the most advanced solvers include elements which are stochastic in nature . if a same instance is solved many times using different seeds then depending on the inherent characteristics of a problem instance and the solver , one can observe a highly-variant distribution of times spanning multiple orders of magnitude . therefore , to solve a problem instance efficiently it is often useful to solve the same instance in parallel with different seeds . with the proliferation of cloud computing , it is natural to think about an elastic solver which can scale up by launching searches in parallel on thousands of machines ( or cores ) . however , this could result in consuming a lot of energy . moreover , not every instance would require thousands of machines . the challenge is to resolve the tradeoff between solution time and energy consumption optimally for a given problem instance . we analyse the impact of the number of machines ( or cores ) on not only solution time but also on energy consumption . we highlight that although solution time always drops as the number of machines increases , the relation between the number of machines and energy consumption is more complicated . in many cases , the optimal energy consumption may be achieved by a middle ground , we analyse this relationship in detail . the tradeoff between solution time and energy consumption is studied further , showing that the energy consumption of a solver can be reduced drastically if we increase the solution time marginally . we also develop a prediction model , demonstrating that such insights can be exploited to achieve faster solutions times in a more energy efficient manor .

steepest ascent hill climbing for a mathematical problem
the paper proposes artificial intelligence technique called hill climbing to find numerical solutions of diophantine equations . such equations are important as they have many applications in fields like public key cryptography , integer factorization , algebraic curves , projective curves and data dependency in super computers . importantly , it has been proved that there is no general method to find solutions of such equations . this paper is an attempt to find numerical solutions of diophantine equations using steepest ascent version of hill climbing . the method , which uses tree representation to depict possible solutions of diophantine equations , adopts a novel methodology to generate successors . the heuristic function used help to make the process of finding solution as a minimization process . the work illustrates the effectiveness of the proposed methodology using a class of diophantine equations given by a1 . x1 p1 + a2 . x2 p2 + ... ... + an . xn pn = n where ai and n are integers . the experimental results validate that the procedure proposed is successful in finding solutions of diophantine equations with sufficiently large powers and large number of variables .

source-sensitive belief change
the agm model is the most remarkable framework for modeling belief revision . however , it is not perfect in all aspects . paraconsistent belief revision , multi-agent belief revision and non-prioritized belief revision are three different extensions to agm to address three important criticisms applied to it . in this article , we propose a framework based on agm that takes a position in each of these categories . also , we discuss some features of our framework and study the satisfiability of agm postulates in this new context .

bayesian modeling with gaussian processes using the gpstuff toolbox
gaussian processes ( gp ) are powerful tools for probabilistic modeling purposes . they can be used to define prior distributions over latent functions in hierarchical bayesian models . the prior over functions is defined implicitly by the mean and covariance function , which determine the smoothness and variability of the function . the inference can then be conducted directly in the function space by evaluating or approximating the posterior process . despite their attractive theoretical properties gps provide practical challenges in their implementation . gpstuff is a versatile collection of computational tools for gp models compatible with linux and windows matlab and octave . it includes , among others , various inference methods , sparse approximations and tools for model assessment . in this work , we review these tools and demonstrate the use of gpstuff in several models .

search for choquet-optimal paths under uncertainty
choquet expected utility ( ceu ) is one of the most sophisticated decision criteria used in decision theory under uncertainty . it provides a generalisation of expected utility enhancing both descriptive and prescriptive possibilities . in this paper , we investigate the use of ceu for path-planning under uncertainty with a special focus on robust solutions . we first recall the main features of the ceu model and introduce some examples showing its descriptive potential . then we focus on the search for choquet-optimal paths in multivalued implicit graphs where costs depend on different scenarios . after discussing complexity issues , we propose two different heuristic search algorithms to solve the problem . finally , numerical experiments are reported , showing the practical efficiency of the proposed algorithms .

teknowbase : towards construction of a knowledge-base of technical concepts
in this paper , we describe the construction of teknowbase , a knowledge-base of technical concepts in computer science . our main information sources are technical websites such as webopedia and techtarget as well as wikipedia and online textbooks . we divide the knowledge-base construction problem into two parts -- the acquisition of entities and the extraction of relationships among these entities . our knowledge-base consists of approximately 100,000 triples . we conducted an evaluation on a sample of triples and report an accuracy of a little over 90\ % . we additionally conducted classification experiments on stackoverflow data with features from teknowbase and achieved improved classification accuracy .

a novice looks at emotional cognition
modeling emotional-cognition is in a nascent stage and therefore wide-open for new ideas and discussions . in this paper the author looks at the modeling problem by bringing in ideas from axiomatic mathematics , information theory , computer science , molecular biology , non-linear dynamical systems and quantum computing and explains how ideas from these disciplines may have applications in modeling emotional-cognition .

real-time retrieval for case-based reasoning in interactive multiagent-based simulations
the aim of this paper is to present the principles and results about case-based reasoning adapted to real- time interactive simulations , more precisely concerning retrieval mechanisms . the article begins by introducing the constraints involved in interactive multiagent-based simulations . the second section pre- sents a framework stemming from case-based reasoning by autonomous agents . each agent uses a case base of local situations and , from this base , it can choose an action in order to interact with other auton- omous agents or users ' avatars . we illustrate this framework with an example dedicated to the study of dynamic situations in football . we then go on to address the difficulties of conducting such simulations in real-time and propose a model for case and for case base . using generic agents and adequate case base structure associated with a dedicated recall algorithm , we improve retrieval performance under time pressure compared to classic cbr techniques . we present some results relating to the performance of this solution . the article concludes by outlining future development of our project .

paving the roadway for safety of automated vehicles : an empirical study on testing challenges
the technology in the area of automated vehicles is gaining speed and promises many advantages . however , with the recent introduction of conditionally automated driving , we have also seen accidents . test protocols for both , conditionally automated ( e.g. , on highways ) and automated vehicles do not exist yet and leave researchers and practitioners with different challenges . for instance , current test procedures do not suffice for fully automated vehicles , which are supposed to be completely in charge for the driving task and have no driver as a back up . this paper presents current challenges of testing the functionality and safety of automated vehicles derived from conducting focus groups and interviews with 26 participants from five countries having a background related to testing automotive safety-related topics.we provide an overview of the state-of-practice of testing active safety features as well as challenges that needs to be addressed in the future to ensure safety for automated vehicles . the major challenges identified through the interviews and focus groups , enriched by literature on this topic are related to 1 ) virtual testing and simulation , 2 ) safety , reliability , and quality , 3 ) sensors and sensor models , 4 ) required scenario complexity and amount of test cases , and 5 ) handover of responsibility between the driver and the vehicle .

an anytime algorithm for task and motion mdps
integrated task and motion planning has emerged as a challenging problem in sequential decision making , where a robot needs to compute high-level strategy and low-level motion plans for solving complex tasks . while high-level strategies require decision making over longer time-horizons and scales , their feasibility depends on low-level constraints based upon the geometries and continuous dynamics of the environment . the hybrid nature of this problem makes it difficult to scale ; most existing approaches focus on deterministic , fully observable scenarios . we present a new approach where the high-level decision problem occurs in a stochastic setting and can be modeled as a markov decision process . in contrast to prior efforts , we show that complete mdp policies , or contingent behaviors , can be computed effectively in an anytime fashion . our algorithm continuously improves the quality of the solution and is guaranteed to be probabilistically complete . we evaluate the performance of our approach on a challenging , realistic test problem : autonomous aircraft inspection . our results show that we can effectively compute consistent task and motion policies for the most likely execution-time outcomes using only a fraction of the computation required to develop the complete task and motion policy .

optimizing causal orderings for generating dags from data
an algorithm for generating the structure of a directed acyclic graph from data using the notion of causal input lists is presented . the algorithm manipulates the ordering of the variables with operations which very much resemble arc reversal . operations are only applied if the dag after the operation represents at least the independencies represented by the dag before the operation until no more arcs can be removed from the dag . the resulting dag is a minimal l-map .

tensorflow agents : efficient batched reinforcement learning in tensorflow
we introduce tensorflow agents , an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in tensorflow . we simulate multiple environments in parallel , and group them to perform the neural network computation on a batch rather than individual observations . this allows the tensorflow execution engine to parallelize computation , without the need for manual synchronization . environments are stepped in separate python processes to progress them in parallel without interference of the global interpreter lock . as part of this project , we introduce batchppo , an efficient implementation of the proximal policy optimization algorithm . by open sourcing tensorflow agents , we hope to provide a flexible starting point for future projects that accelerates future research in the field .

introduction to the 26th international conference on logic programming special issue
this is the preface to the 26th international conference on logic programming special issue

real-time top-k predictive query processing over event streams
this paper addresses the problem of predicting the k events that are most likely to occur next , over historical real-time event streams . existing approaches to causal prediction queries have a number of limitations . first , they exhaustively search over an acyclic causal network to find the most likely k effect events ; however , data from real event streams frequently reflect cyclic causality . second , they contain conservative assumptions intended to exclude all possible non-causal links in the causal network ; it leads to the omission of many less-frequent but important causal links . we overcome these limitations by proposing a novel event precedence model and a run-time causal inference mechanism . the event precedence model constructs a first order absorbing markov chain incrementally over event streams , where an edge between two events signifies a temporal precedence relationship between them , which is a necessary condition for causality . then , the run-time causal inference mechanism learns causal relationships dynamically during query processing . this is done by removing some of the temporal precedence relationships that do not exhibit causality in the presence of other events in the event precedence model . this paper presents two query processing algorithms -- one performs exhaustive search on the model and the other performs a more efficient reduced search with early termination . experiments using two real datasets ( cascading blackouts in power systems and web page views ) verify the effectiveness of the probabilistic top-k prediction queries and the efficiency of the algorithms . specifically , the reduced search algorithm reduced runtime , relative to exhaustive search , by 25-80 % ( depending on the application ) with only a small reduction in accuracy .

an interpretable knowledge transfer model for knowledge base completion
knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness . we propose a novel embedding model , \emph { itransf } , to perform knowledge base completion . equipped with a sparse attention mechanism , itransf discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts . moreover , the learned associations between relations and concepts , which are represented by sparse attention vectors , can be interpreted easily . we evaluate itransf on two benchmark datasets -- -wn18 and fb15k for knowledge base completion and obtains improvements on both the mean rank and hits @ 10 metrics , over all baselines that do not use additional information .

toward controlled generation of text
generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain . this paper aims at generating plausible natural language sentences , whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics . we propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures . with differentiable approximation to discrete text samples , explicit constraints on independent attribute controls , and efficient collaborative learning of generator and discriminators , our model learns highly interpretable representations from even only word annotations , and produces realistic sentences with desired attributes . quantitative evaluation validates the accuracy of sentence and attribute generation .

incorporating causal prior knowledge as path-constraints in bayesian networks and maximal ancestral graphs
we consider the incorporation of causal knowledge about the presence or absence of ( possibly indirect ) causal relations into a causal model . such causal relations correspond to directed paths in a causal model . this type of knowledge naturally arises from experimental data , among others . specifically , we consider the formalisms of causal bayesian networks and maximal ancestral graphs and their markov equivalence classes : partially directed acyclic graphs and partially oriented ancestral graphs . we introduce sound and complete procedures which are able to incorporate causal prior knowledge in such models . in simulated experiments , we show that often considering even a few causal facts leads to a significant number of new inferences . in a case study , we also show how to use real experimental data to infer causal knowledge and incorporate it into a real biological causal network . the code is available at mensxmachina.org .

field geology with a wearable computer : 1st results of the cyborg astrobiologist system
we present results from the first geological field tests of the ` cyborg astrobiologist ' , which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist . the cyborg astrobiologist platform has thus far been used for testing and development of these algorithms and systems : robotic acquisition of quasi-mosaics of images , real-time image segmentation , and real-time determination of interesting points in the image mosaics . this work is more of a test of the whole system , rather than of any one part of the system . however , beyond the concept of the system itself , the uncommon map ( despite its simplicity ) is the main innovative part of the system . the uncommon map helps to determine interest-points in a context-free manner . overall , the hardware and software systems function reliably , and the computer-vision algorithms are adequate for the first field tests . in addition to the proof-of-concept aspect of these field tests , the main result of these field tests is the enumeration of those issues that we can improve in the future , including : dealing with structural shadow and microtexture , and also , controlling the camera 's zoom lens in an intelligent manner . nonetheless , despite these and other technical inadequacies , this cyborg astrobiologist system , consisting of a camera-equipped wearable-computer and its computer-vision algorithms , has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery , and then gathering more information about these interest points in an automated manner . we use these capabilities for autonomous guidance towards geological points-of-interest .

want a good answer ? ask a good question first !
community question answering ( cqa ) websites have become valuable repositories which host a massive volume of human knowledge . to maximize the utility of such knowledge , it is essential to evaluate the quality of an existing question or answer , especially soon after it is posted on the cqa website . in this paper , we study the problem of inferring the quality of questions and answers through a case study of a software cqa ( stack overflow ) . our key finding is that the quality of an answer is strongly positively correlated with that of its question . armed with this observation , we propose a family of algorithms to jointly predict the quality of questions and answers , for both quantifying numerical quality scores and differentiating the high-quality questions/answers from those of low quality . we conduct extensive experimental evaluations to demonstrate the effectiveness and efficiency of our methods .

generating graphoids from generalised conditional probability
we take a general approach to uncertainty on product spaces , and give sufficient conditions for the independence structures of uncertainty measures to satisfy graphoid properties . since these conditions are arguably more intuitive than some of the graphoid properties , they can be viewed as explanations why probability and certain other formalisms generate graphoids . the conditions include a sufficient condition for the intersection property which can still apply even if there is a strong logical relations hip between the variables . we indicate how these results can be used to produce theories of qualitative conditional probability which are semi-graphoids and graphoids .

a deep architecture for semantic matching with multiple positional sentence representations
matching natural language sentences is central for many applications such as information retrieval and question answering . existing deep models rely on a single sentence representation or multiple granularity representations for matching . however , such methods can not well capture the contextualized local information in the matching process . to tackle this problem , we present a new deep architecture to match two sentences with multiple positional sentence representations . specifically , each positional sentence representation is a sentence representation at this position , generated by a bidirectional long short term memory ( bi-lstm ) . the matching score is finally produced by aggregating interactions between these different positional sentence representations , through $ k $ -max pooling and a multi-layer perceptron . our model has several advantages : ( 1 ) by using bi-lstm , rich context of the whole sentence is leveraged to capture the contextualized local information in each positional sentence representation ; ( 2 ) by matching with multiple positional sentence representations , it is flexible to aggregate different important contextualized local information in a sentence to support the matching ; ( 3 ) experiments on different tasks such as question answering and sentence completion demonstrate the superiority of our model .

ganc : greedy agglomerative normalized cut
this paper describes a graph clustering algorithm that aims to minimize the normalized cut criterion and has a model order selection procedure . the performance of the proposed algorithm is comparable to spectral approaches in terms of minimizing normalized cut . however , unlike spectral approaches , the proposed algorithm scales to graphs with millions of nodes and edges . the algorithm consists of three components that are processed sequentially : a greedy agglomerative hierarchical clustering procedure , model order selection , and a local refinement . for a graph of n nodes and o ( n ) edges , the computational complexity of the algorithm is o ( n log^2 n ) , a major improvement over the o ( n^3 ) complexity of spectral methods . experiments are performed on real and synthetic networks to demonstrate the scalability of the proposed approach , the effectiveness of the model order selection procedure , and the performance of the proposed algorithm in terms of minimizing the normalized cut metric .

learning to repeat : fine grained action repetition for deep reinforcement learning
reinforcement learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it . traditionally , such algorithms make decisions , i.e. , select actions to execute , at every single time step of the agent-environment interactions . in this paper , we propose a novel framework , fine grained action repetition ( figar ) , which enables the agent to decide the action as well as the time scale of repeating it . figar can be used for improving any deep reinforcement learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space . we empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains : asynchronous advantage actor critic in the atari 2600 domain , trust region policy optimization in mujoco domain and deep deterministic policy gradients in the torcs car racing domain .

hybrid code networks : practical and efficient end-to-end dialog control with supervised and reinforcement learning
end-to-end learning of recurrent neural networks ( rnns ) is an attractive solution for dialog systems ; however , current techniques are data-intensive and require thousands of dialogs to learn simple behaviors . we introduce hybrid code networks ( hcns ) , which combine an rnn with domain-specific knowledge encoded as software and system action templates . compared to existing end-to-end approaches , hcns considerably reduce the amount of training data required , while retaining the key benefit of inferring a latent representation of dialog state . in addition , hcns can be optimized with supervised learning , reinforcement learning , or a mixture of both . hcns attain state-of-the-art performance on the babi dialog dataset , and outperform two commercially deployed customer-facing dialog systems .

multi-instance learning by treating instances as non-i.i.d . samples
multi-instance learning attempts to learn from a training set consisting of labeled bags each containing many unlabeled instances . previous studies typically treat the instances in the bags as independently and identically distributed . however , the instances in a bag are rarely independent , and therefore a better performance can be expected if the instances are treated in an non-i.i.d . way that exploits the relations among instances . in this paper , we propose a simple yet effective multi-instance learning method , which regards each bag as a graph and uses a specific kernel to distinguish the graphs by considering the features of the nodes as well as the features of the edges that convey some relations among instances . the effectiveness of the proposed method is validated by experiments .

evolutionary multimodal optimization : a short survey
real world problems always have different multiple solutions . for instance , optical engineers need to tune the recording parameters to get as many optimal solutions as possible for multiple trials in the varied-line-spacing holographic grating design problem . unfortunately , most traditional optimization techniques focus on solving for a single optimal solution . they need to be applied several times ; yet all solutions are not guaranteed to be found . thus the multimodal optimization problem was proposed . in that problem , we are interested in not only a single optimal point , but also the others . with strong parallel search capability , evolutionary algorithms are shown to be particularly effective in solving this type of problem . in particular , the evolutionary algorithms for multimodal optimization usually not only locate multiple optima in a single run , but also preserve their population diversity throughout a run , resulting in their global optimization ability on multimodal functions . in addition , the techniques for multimodal optimization are borrowed as diversity maintenance techniques to other problems . in this chapter , we describe and review the state-of-the-arts evolutionary algorithms for multimodal optimization in terms of methodology , benchmarking , and application .

distributed reinforcement learning via gossip
we consider the classical td ( 0 ) algorithm implemented on a network of agents wherein the agents also incorporate the updates received from neighboring agents using a gossip-like mechanism . the combined scheme is shown to converge for both discounted and average cost problems .

a spatiotemporal context definition for service adaptation prediction in a pervasive computing environment
pervasive systems refers to context-aware systems that can sense their context , and adapt their behavior accordingly to provide adaptable services . proactive adaptation of such systems allows changing the service and the context based on prediction . however , the definition of the context is still vague and not suitable to prediction . in this paper we discuss and classify previous definitions of context . then , we present a new definition which allows pervasive systems to understand and predict their contexts . we analyze the essential lines that fall within the context definition , and propose some scenarios to make it clear our approach .

mining massive hierarchical data using a scalable probabilistic graphical model
probabilistic graphical models ( pgm ) are very useful in the fields of machine learning and data mining . the crucial limitation of those models , however , is the scalability . the bayesian network , which is one of the most common pgms used in machine learning and data mining , demonstrates this limitation when the training data consists of random variables , each of them has a large set of possible values . in the big data era , one would expect new extensions to the existing pgms to handle the massive amount of data produced these days by computers , sensors and other electronic devices . with hierarchical data - data that is arranged in a treelike structure with several levels - one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels . when modeling this kind of hierarchical data across large data sets , bayesian networks become infeasible for representing the probability distributions . in this paper we introduce an extension to bayesian networks to handle massive sets of hierarchical data in a reasonable amount of time and space . the proposed model achieves perfect precision of 1.0 and high recall of 0.93 when it is used as multi-label classifier for the annotation of mass spectrometry data . on another data set of 1.5 billion search logs provided by careerbuilder.com the model was able to predict latent semantic relationships between search keywords with accuracy up to 0.80 .

reliable decision support using counterfactual models
decision-makers are faced with the challenge of estimating what is likely to happen when they take an action . for instance , if i choose not to treat this patient , are they likely to die ? practitioners commonly use supervised learning algorithms to fit predictive models that help decision-makers reason about likely future outcomes , but we show that this approach is unreliable , and sometimes even dangerous . the key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data , which causes the model to capture relationships that do not generalize . we propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning . to support decision-making in temporal settings , we introduce the counterfactual gaussian process ( cgp ) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions . we demonstrate the benefits of the cgp on two important decision-support tasks : risk prediction and `` what if ? '' reasoning for individualized treatment planning .

conditional plausibility measures and bayesian networks
a general notion of algebraic conditional plausibility measures is defined . probability measures , ranking functions , possibility measures , and ( under the appropriate definitions ) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures . it is shown that algebraic conditional plausibility measures can be represented using bayesian networks .

the path inference filter : model-based low-latency map matching of probe vehicle data
we consider the problem of reconstructing vehicle trajectories from sparse sequences of gps points , for which the sampling interval is between 10 seconds and 2 minutes . we introduce a new class of algorithms , called altogether path inference filter ( pif ) , that maps gps data in real time , for a variety of trade-offs and scenarios , and with a high throughput . numerous prior approaches in map-matching can be shown to be special cases of the path inference filter presented in this article . we present an efficient procedure for automatically training the filter on new data , with or without ground truth observations . the framework is evaluated on a large san francisco taxi dataset and is shown to improve upon the current state of the art . this filter also provides insights about driving patterns of drivers . the path inference filter has been deployed at an industrial scale inside the mobile millennium traffic information system , and is used to map fleets of data in san francisco , sacramento , stockholm and porto .

nag : network for adversary generation
adversarial perturbations can pose a serious threat for deploying machine learning systems . recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images . existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations . however , for a given classifier , they generate one perturbation at a time , which is a single instance from the manifold of adversarial perturbations . also , in order to build robust models , it is essential to explore the manifold of adversarial perturbations . in this paper , we propose for the first time , a generative approach to model the distribution of adversarial perturbations . the architecture of the proposed model is inspired from that of gans and is trained using fooling and diversity objectives . our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations . our experimental evaluation demonstrates that perturbations crafted by our model ( i ) achieve state-of-the-art fooling rates , ( ii ) exhibit wide variety and ( iii ) deliver excellent cross model generalizability . our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations .

a new belief markov chain model and its application in inventory prediction
markov chain model is widely applied in many fields , especially the field of prediction . the classical discrete-time markov chain ( dtmc ) is a widely used method for prediction . however , the classical dtmc model has some limitation when the system is complex with uncertain information or state space is not discrete . to address it , a new belief markov chain model is proposed by combining dempster-shafer evidence theory with markov chain . in our model , the uncertain data is allowed to be handle in the form of interval number and the basic probability assignment ( bpa ) is generated based on the distance between interval numbers . the new belief markov chain model overcomes the shortcomings of classical markov chain and has an efficient ability in dealing with uncertain information . moreover , an example of inventory prediction and the comparison between our model and classical dtmc model can show the effectiveness and rationality of our proposed model .

datum-wise classification : a sequential approach to sparsity
we propose a novel classification technique whose aim is to select an appropriate representation for each datapoint , in contrast to the usual approach of selecting a representation encompassing the whole dataset . this datum-wise representation is found by using a sparsity inducing empirical risk , which is a relaxation of the standard l 0 regularized risk . the classification problem is modeled as a sequential decision process that sequentially chooses , for each datapoint , which features to use before classifying . datum-wise classification extends naturally to multi-class tasks , and we describe a specific case where our inference has equivalent complexity to a traditional linear classifier , while still using a variable number of features . we compare our classifier to classical l 1 regularized linear models ( l 1-svm and lars ) on a set of common binary and multi-class datasets and show that for an equal average number of features used we can get improved performance using our method .

stackinsights : cognitive learning for hybrid cloud readiness
hybrid cloud is an integrated cloud computing environment utilizing a mix of public cloud , private cloud , and on-premise traditional it infrastructures . workload awareness , defined as a detailed full range understanding of each individual workload , is essential in implementing the hybrid cloud . while it is critical to perform an accurate analysis to determine which workloads are appropriate for on-premise deployment versus which workloads can be migrated to a cloud off-premise , the assessment is mainly performed by rule or policy based approaches . in this paper , we introduce stackinsights , a novel cognitive system to automatically analyze and predict the cloud readiness of workloads for an enterprise . our system harnesses the critical metrics across the entire stack : 1 ) infrastructure metrics , 2 ) data relevance metrics , and 3 ) application taxonomy , to identify workloads that have characteristics of a ) low sensitivity with respect to business security , criticality and compliance , and b ) low response time requirements and access patterns . since the capture of the data relevance metrics involves an intrusive and in-depth scanning of the content of storage objects , a machine learning model is applied to perform the business relevance classification by learning from the meta level metrics harnessed across stack . in contrast to traditional methods , stackinsights significantly reduces the total time for hybrid cloud readiness assessment by orders of magnitude .

detecting anomalous process behaviour using second generation artificial immune systems
artificial immune systems have been successfully applied to a number of problem domains including fault tolerance and data mining , but have been shown to scale poorly when applied to computer intrusion detec- tion despite the fact that the biological immune system is a very effective anomaly detector . this may be because ais algorithms have previously been based on the adaptive immune system and biologically-naive mod- els . this paper focuses on describing and testing a more complex and biologically-authentic ais model , inspired by the interactions between the innate and adaptive immune systems . its performance on a realistic process anomaly detection problem is shown to be better than standard ais methods ( negative-selection ) , policy-based anomaly detection methods ( systrace ) , and an alternative innate ais approach ( the dca ) . in addition , it is shown that runtime information can be used in combination with system call information to enhance detection capability .

an algebraic dexter-based hypertext reference model
we present the first formal algebraic specification of a hypertext reference model . it is based on the well-known dexter hypertext reference model and includes modifications with respect to the development of hypertext since the www came up . our hypertext model was developed as a product model with the aim to automatically support the design process and is extended to a model of hypertext-systems in order to be able to describe the state transitions in this process . while the specification should be easy to read for non-experts in algebraic specification , it guarantees a unique understanding and enables a close connection to logic-based development and verification .

providing self-aware systems with reflexivity
we propose a new type of self-aware systems inspired by ideas from higher-order theories of consciousness . first , we discussed the crucial distinction between introspection and reflexion . then , we focus on computational reflexion as a mechanism by which a computer program can inspect its own code at every stage of the computation . finally , we provide a formal definition and a proof-of-concept implementation of computational reflexion , viewed as an enriched form of program interpretation and a way to dynamically `` augment '' a computational process .

ctbnctoolkit : continuous time bayesian network classifier toolkit
continuous time bayesian network classifiers are designed for temporal classification of multivariate streaming data when time duration of events matters and the class does not change over time . this paper introduces the ctbnctoolkit : an open source java toolkit which provides a stand-alone application for temporal classification and a library for continuous time bayesian network classifiers . ctbnctoolkit implements the inference algorithm , the parameter learning algorithm , and the structural learning algorithm for continuous time bayesian network classifiers . the structural learning algorithm is based on scoring functions : the marginal log-likelihood score and the conditional log-likelihood score are provided . ctbnctoolkit provides also an implementation of the expectation maximization algorithm for clustering purpose . the paper introduces continuous time bayesian network classifiers . how to use the ctbntoolkit from the command line is described in a specific section . tutorial examples are included to facilitate users to understand how the toolkit must be used . a section dedicate to the java library is proposed to help further code extensions .

deep learning for medical image segmentation
this report provides an overview of the current state of the art deep learning architectures and optimisation techniques , and uses the adni hippocampus mri dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation , which is important in the diagnosis of alzheimer 's disease . we found that a slightly unconventional `` stacked 2d '' approach provides much better classification performance than simple 2d patches without requiring significantly more computational power . we also examined the popular `` tri-planar '' approach used in some recently published studies , and found that it provides much better results than the 2d approaches , but also with a moderate increase in computational power requirement . finally , we evaluated a full 3d convolutional architecture , and found that it provides marginally better results than the tri-planar approach , but at the cost of a very significant increase in computational power requirement .

extended mixture of mlp experts by hybrid of conjugate gradient method and modified cuckoo search
this paper investigates a new method for improving the learning algorithm of mixture of experts ( me ) model using a hybrid of modified cuckoo search ( mcs ) and conjugate gradient ( cg ) as a second order optimization technique . the cg technique is combined with back-propagation ( bp ) algorithm to yield a much more efficient learning algorithm for me structure . in addition , the experts and gating networks in enhanced model are replaced by cg based multi-layer perceptrons ( mlps ) to provide faster and more accurate learning . the cg is considerably depends on initial weights of connections of artificial neural network ( ann ) , so , a metaheuristic algorithm , the so-called modified cuckoo search is applied in order to select the optimal weights . the performance of proposed method is compared with gradient decent based me ( gdme ) and conjugate gradient based me ( cgme ) in classification and regression problems . the experimental results show that hybrid msc and cg based me ( mcs-cgme ) has faster convergence and better performance in utilized benchmark data sets .

causal rule sets for identifying subgroups with enhanced treatment effect
we introduce a novel generative model for interpretable subgroup analysis for causal inference applications , causal rule sets ( crs ) . a crs model uses a small set of short rules to capture a subgroup where the average treatment effect is elevated compared to the entire population . we present a bayesian framework for learning a causal rule set . the bayesian framework consists of a prior that favors simpler models and a bayesian logistic regression that characterizes the relation between outcomes , attributes and subgroup membership . we find maximum a posteriori models using discrete monte carlo steps in the joint solution space of rules sets and parameters . we provide theoretically grounded heuristics and bounding strategies to improve search efficiency . experiments show that the search algorithm can efficiently recover a true underlying subgroup and crs shows consistently competitive performance compared to other state-of-the-art baseline methods .

deep predictive models in interactive music
automatic music generation is a compelling task where much recent progress has been made with deep learning models . in this paper , we ask how these models can be integrated into interactive music systems ; how can they encourage or enhance the music making of human users ? musical performance requires prediction to operate instruments , and perform in groups . we argue that predictive models could help interactive systems to understand their temporal context , and ensemble behaviour . deep learning can allow data-driven models with a long memory of past states . we advocate for predictive musical interaction , where a predictive model is embedded in a musical interface , assisting users by predicting unknown states of musical processes . we propose a framework for incorporating such predictive models into the sensing , processing , and result architecture that is often used in musical interface design . we show that our framework accommodates deep generative models , as well as models for predicting gestural states , or other high-level musical information . we motivate the framework with two examples from our recent work , as well as systems from the literature , and suggest musical use-cases where prediction is a necessary component .

scalable exact parent sets identification in bayesian networks learning with apache spark
in machine learning , the parent set identification problem is to find a set of random variables that best explain selected variable given the data and some predefined scoring function . this problem is a critical component to structure learning of bayesian networks and markov blankets discovery , and thus has many practical applications , ranging from fraud detection to clinical decision support . in this paper , we introduce a new distributed memory approach to the exact parent sets assignment problem . to achieve scalability , we derive theoretical bounds to constraint the search space when mdl scoring function is used , and we reorganize the underlying dynamic programming such that the computational density is increased and fine-grain synchronization is eliminated . we then design efficient realization of our approach in the apache spark platform . through experimental results , we demonstrate that the method maintains strong scalability on a 500-core standalone spark cluster , and it can be used to efficiently process data sets with 70 variables , far beyond the reach of the currently available solutions .

a system for probabilistic linking of thesauri and classification systems
this paper presents a system which creates and visualizes probabilistic semantic links between concepts in a thesaurus and classes in a classification system . for creating the links , we build on the polylingual labeled topic model ( pll-tm ) . pll-tm identifies probable thesaurus descriptors for each class in the classification system by using information from the natural language text of documents , their assigned thesaurus descriptors and their designated classes . the links are then presented to users of the system in an interactive visualization , providing them with an automatically generated overview of the relations between the thesaurus and the classification system .

inferring disease and gene set associations with rank coherence in networks
a computational challenge to validate the candidate disease genes identified in a high-throughput genomic study is to elucidate the associations between the set of candidate genes and disease phenotypes . the conventional gene set enrichment analysis often fails to reveal associations between disease phenotypes and the gene sets with a short list of poorly annotated genes , because the existing annotations of disease causative genes are incomplete . we propose a network-based computational approach called rcnet to discover the associations between gene sets and disease phenotypes . assuming coherent associations between the genes ranked by their relevance to the query gene set , and the disease phenotypes ranked by their relevance to the hidden target disease phenotypes of the query gene set , we formulate a learning framework maximizing the rank coherence with respect to the known disease phenotype-gene associations . an efficient algorithm coupling ridge regression with label propagation , and two variants are introduced to find the optimal solution of the framework . we evaluated the rcnet algorithms and existing baseline methods with both leave-one-out cross-validation and a task of predicting recently discovered disease-gene associations in omim . the experiments demonstrated that the rcnet algorithms achieved the best overall rankings compared to the baselines . to further validate the reproducibility of the performance , we applied the algorithms to identify the target diseases of novel candidate disease genes obtained from recent studies of gwas , dna copy number variation analysis , and gene expression profiling . the algorithms ranked the target disease of the candidate genes at the top of the rank list in many cases across all the three case studies . the rcnet algorithms are available as a webtool for disease and gene set association analysis at http : //compbio.cs.umn.edu/dgsa_rcnet .

complexity of shift bribery in committee elections
we study the ( parameterized ) complexity of shift bribery for multiwinner voting rules . we focus on sntv , bloc , k-borda , and chamberlin-courant , as well as on approximate variants of chamberlin-courant , since the original rule is np-hard to compute . we show that shift bribery tends to be significantly harder in the multiwinner setting than in the single-winner one by showing settings where shift bribery is easy in the single-winner cases , but is hard ( and hard to approximate ) in the multiwinner ones . moreover , we show that the non-monotonicity of those rules which are based on approximation algorithms for the chamberlin-courant rule sometimes affects the complexity of shift bribery .

causal models have no complete axiomatic characterization
markov networks and bayesian networks are effective graphic representations of the dependencies embedded in probabilistic models . it is well known that independencies captured by markov networks ( called graph-isomorphs ) have a finite axiomatic characterization . this paper , however , shows that independencies captured by bayesian networks ( called causal models ) have no axiomatization by using even countably many horn or disjunctive clauses . this is because a sub-independency model of a causal model may be not causal , while graph-isomorphs are closed under sub-models .

when ignorance is bliss
it is commonly-accepted wisdom that more information is better , and that information should never be ignored . here we argue , using both a bayesian and a non-bayesian analysis , that in some situations you are better off ignoring information if your uncertainty is represented by a set of probability measures . these include situations in which the information is relevant for the prediction task at hand . in the non-bayesian analysis , we show how ignoring information avoids dilation , the phenomenon that additional pieces of information sometimes lead to an increase in uncertainty . in the bayesian analysis , we show that for small sample sizes and certain prediction tasks , the bayesian posterior based on a noninformative prior yields worse predictions than simply ignoring the given information .

modeling variations of first-order horn abduction in answer set programming
we study abduction in first order horn logic theories where all atoms can be abduced and we are looking for preferred solutions with respect to three objective functions : cardinality minimality , coherence , and weighted abduction . we represent this reasoning problem in answer set programming ( asp ) , in order to obtain a flexible framework for experimenting with global constraints and objective functions , and to test the boundaries of what is possible with asp . realizing this problem in asp is challenging as it requires value invention and equivalence between certain constants , because the unique names assumption does not hold in general . to permit reasoning in cyclic theories , we formally describe fine-grained variations of limiting skolemization . we identify term equivalence as a main instantiation bottleneck , and improve the efficiency of our approach with on-demand constraints that were used to eliminate the same bottleneck in state-of-the-art solvers . we evaluate our approach experimentally on the accel benchmark for plan recognition in natural language understanding . our encodings are publicly available , modular , and our approach is more efficient than state-of-the-art solvers on the accel benchmark .

bootstrapping ternary relation extractors
binary relation extraction methods have been widely studied in recent years . however , few methods have been developed for higher n-ary relation extraction . one limiting factor is the effort required to generate training data . for binary relations , one only has to provide a few dozen pairs of entities per relation , as training data . for ternary relations ( n=3 ) , each training instance is a triplet of entities , placing a greater cognitive load on people . for example , many people know that google acquired youtube but not the dollar amount or the date of the acquisition and many people know that hillary clinton is married to bill clinton by not the location or date of their wedding . this makes higher n-nary training data generation a time consuming exercise in searching the web . we present a resource for training ternary relation extractors . this was generated using a minimally supervised yet effective approach . we present statistics on the size and the quality of the dataset .

low-complexity stochastic generalized belief propagation
the generalized belief propagation ( gbp ) , introduced by yedidia et al. , is an extension of the belief propagation ( bp ) algorithm , which is widely used in different problems involved in calculating exact or approximate marginals of probability distributions . in many problems , it has been observed that the accuracy of gbp considerably outperforms that of bp . however , because in general the computational complexity of gbp is higher than bp , its application is limited in practice . in this paper , we introduce a stochastic version of gbp called stochastic generalized belief propagation ( sgbp ) that can be considered as an extension to the stochastic bp ( sbp ) algorithm introduced by noorshams et al . they have shown that sbp reduces the complexity per iteration of bp by an order of magnitude in alphabet size . in contrast to sbp , sgbp can reduce the computation complexity if certain topological conditions are met by the region graph associated to a graphical model . however , this reduction can be larger than only one order of magnitude in alphabet size . in this paper , we characterize these conditions and the amount of computation gain that we can obtain by using sgbp . finally , using similar proof techniques employed by noorshams et al. , for general graphical models satisfy contraction conditions , we prove the asymptotic convergence of sgbp to the unique gbp fixed point , as well as providing non-asymptotic upper bounds on the mean square error and on the high probability error .

non-linear label ranking for large-scale prediction of long-term user interests
we consider the problem of personalization of online services from the viewpoint of ad targeting , where we seek to find the best ad categories to be shown to each user , resulting in improved user experience and increased advertisers ' revenue . we propose to address this problem as a task of ranking the ad categories depending on a user 's preference , and introduce a novel label ranking approach capable of efficiently learning non-linear , highly accurate models in large-scale settings . experiments on a real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-k retrieval performance , strongly suggesting the benefit of using the proposed model on large-scale ranking problems .

inductive logic boosting
recent years have seen a surge of interest in probabilistic logic programming ( plp ) and statistical relational learning ( srl ) models that combine logic with probabilities . structure learning of these systems is an intersection area of inductive logic programming ( ilp ) and statistical learning ( sl ) . however , ilp can not deal with probabilities , sl can not model relational hypothesis . the biggest challenge of integrating these two machine learning frameworks is how to estimate the probability of a logic clause only from the observation of grounded logic atoms . many current methods models a joint probability by representing clause as graphical model and literals as vertices in it . this model is still too complicate and only can be approximate by pseudo-likelihood . we propose inductive logic boosting framework to transform the relational dataset into a feature-based dataset , induces logic rules by boosting problog rule trees and relaxes the independence constraint of pseudo-likelihood . experimental evaluation on benchmark datasets demonstrates that the auc-pr and auc-roc value of ilp learned rules are higher than current state-of-the-art srl methods .

the case for automatic database administration using deep reinforcement learning
like any large software system , a full-fledged dbms offers an overwhelming amount of configuration knobs . these range from static initialisation parameters like buffer sizes , degree of concurrency , or level of replication to complex runtime decisions like creating a secondary index on a particular column or reorganising the physical layout of the store . to simplify the configuration , industry grade dbmss are usually shipped with various advisory tools , that provide recommendations for given workloads and machines . however , reality shows that the actual configuration , tuning , and maintenance is usually still done by a human administrator , relying on intuition and experience . recent work on deep reinforcement learning has shown very promising results in solving problems , that require such a sense of intuition . for instance , it has been applied very successfully in learning how to play complicated games with enormous search spaces . motivated by these achievements , in this work we explore how deep reinforcement learning can be used to administer a dbms . first , we will describe how deep reinforcement learning can be used to automatically tune an arbitrary software system like a dbms by defining a problem environment . second , we showcase our concept of nodba at the concrete example of index selection and evaluate how well it recommends indexes for given workloads .

probabilistically safe policy transfer
although learning-based methods have great potential for robotics , one concern is that a robot that updates its parameters might cause large amounts of damage before it learns the optimal policy . we formalize the idea of safe learning in a probabilistic sense by defining an optimization problem : we desire to maximize the expected return while keeping the expected damage below a given safety limit . we study this optimization for the case of a robot manipulator with safety-based torque limits . we would like to ensure that the damage constraint is maintained at every step of the optimization and not just at convergence . to achieve this aim , we introduce a novel method which predicts how modifying the torque limit , as well as how updating the policy parameters , might affect the robot 's safety . we show through a number of experiments that our approach allows the robot to improve its performance while ensuring that the expected damage constraint is not violated during the learning process .

agreement maintenance based on schema and ontology change in p2p environment
this paper is concern about developing a semantic agreement maintenance method based on semantic distance by calculating the change of local schema or ontology . this approach is important in dynamic and autonomous environment , in which the current approach assumed that agreement or mapping in static environment . the contribution of this research is to develop a framework based on semantic agreement maintenance approach for p2p environment . this framework based on two level hybrid p2p model architecture , which consist of two peer type : ( 1 ) super peer that use to register and manage the other peers , and ( 2 ) simple peer , as a simple peer , it exports and shares its contents with others . this research develop a model to maintain the semantic agreement in p2p environment , so the current approach which does not have the mechanism to know the change , since it assumed that ontology and local schema are in the static condition , and it is different in dynamic condition . the main issues are how to calculate the change of local schema or common ontology and the calculation result is used to determine which algorithm in maintaining the agreement . the experiment on the job matching domain in indonesia have been done to show how far the performance of the approach . from the experiment , the main result are ( i ) the more change so the f-measure value tend to be decreased , ( ii ) there is no significant different in f-measure value for various modification type ( add , delete , rename ) , and ( iii ) the correct choice of algorithm would improve the f-measure value .

the acrv picking benchmark ( apb ) : a robotic shelf picking benchmark to foster reproducible research
robotic challenges like the amazon picking challenge ( apc ) or the darpa challenges are an established and important way to drive scientific progress . they make research comparable on a well-defined benchmark with equal test conditions for all participants . however , such challenge events occur only occasionally , are limited to a small number of contestants , and the test conditions are very difficult to replicate after the main event . we present a new physical benchmark challenge for robotic picking : the acrv picking benchmark ( apb ) . designed to be reproducible , it consists of a set of 42 common objects , a widely available shelf , and exact guidelines for object arrangement using stencils . a well-defined evaluation protocol enables the comparison of \emph { complete } robotic systems -- including perception and manipulation -- instead of sub-systems only . our paper also describes and reports results achieved by an open baseline system based on a baxter robot .

using contexts and constraints for improved geotagging of human trafficking webpages
extracting geographical tags from webpages is a well-motivated application in many domains . in illicit domains with unusual language models , like human trafficking , extracting geotags with both high precision and recall is a challenging problem . in this paper , we describe a geotag extraction framework in which context , constraints and the openly available geonames knowledge base work in tandem in an integer linear programming ( ilp ) model to achieve good performance . in preliminary empirical investigations , the framework improves precision by 28.57 % and f-measure by 36.9 % on a difficult human trafficking geotagging task compared to a machine learning-based baseline . the method is already being integrated into an existing knowledge base construction system widely used by us law enforcement agencies to combat human trafficking .

structure based extended resolution for constraint programming
nogood learning is a powerful approach to reducing search in constraint programming ( cp ) solvers . the current state of the art , called lazy clause generation ( lcg ) , uses resolution to derive nogoods expressing the reasons for each search failure . such nogoods can prune other parts of the search tree , producing exponential speedups on a wide variety of problems . nogood learning solvers can be seen as resolution proof systems . the stronger the proof system , the faster it can solve a cp problem . it has recently been shown that the proof system used in lcg is at least as strong as general resolution . however , stronger proof systems such as \emph { extended resolution } exist . extended resolution allows for literals expressing arbitrary logical concepts over existing variables to be introduced and can allow exponentially smaller proofs than general resolution . the primary problem in using extended resolution is to figure out exactly which literals are useful to introduce . in this paper , we show that we can use the structural information contained in a cp model in order to introduce useful literals , and that this can translate into significant speedups on a range of problems .

separating the real from the synthetic : minutiae histograms as fingerprints of fingerprints
in this study we show that by the current state-of-the-art synthetically generated fingerprints can easily be discriminated from real fingerprints . we propose a method based on second order extended minutiae histograms ( mhs ) which can distinguish between real and synthetic prints with very high accuracy . mhs provide a fixed-length feature vector for a fingerprint which are invariant under rotation and translation . this 'test of realness ' can be applied to synthetic fingerprints produced by any method . in this work , tests are conducted on the 12 publicly available databases of fvc2000 , fvc2002 and fvc2004 which are well established benchmarks for evaluating the performance of fingerprint recognition algorithms ; 3 of these 12 databases consist of artificial fingerprints generated by the sfinge software . additionally , we evaluate the discriminative performance on a database of synthetic fingerprints generated by the software of bicz versus real fingerprint images . we conclude with suggestions for the improvement of synthetic fingerprint generation .

towards statistical reasoning in description logics over finite domains ( full version )
we present a probabilistic extension of the description logic $ \mathcal { alc } $ for reasoning about statistical knowledge . we consider conditional statements over proportions of the domain and are interested in the probabilistic-logical consequences of these proportions . after introducing some general reasoning problems and analyzing their properties , we present first algorithms and complexity results for reasoning in some fragments of statistical $ \mathcal { alc } $ .

a review of methodologies for natural-language-facilitated human-robot cooperation
natural-language-facilitated human-robot cooperation ( nlc ) refers to using natural language ( nl ) to facilitate interactive information sharing and task executions with a common goal constraint between robots and humans . recently , nlc research has received increasing attention . typical nlc scenarios include robotic daily assistance , robotic health caregiving , intelligent manufacturing , autonomous navigation , and robot social accompany . however , a thorough review , that can reveal latest methodologies to use nl to facilitate human-robot cooperation , is missing . in this review , a comprehensive summary about methodologies for nlc is presented . nlc research includes three main research focuses : nl instruction understanding , nl-based execution plan generation , and knowledge-world mapping . in-depth analyses on theoretical methods , applications , and model advantages and disadvantages are made . based on our paper review and perspective , potential research directions of nlc are summarized .

the complex event recognition group
the complex event recognition ( cer ) group is a research team , affiliated with the national centre of scientific research `` demokritos '' in greece . the cer group works towards advanced and efficient methods for the recognition of complex events in a multitude of large , heterogeneous and interdependent data streams . its research covers multiple aspects of complex event recognition , from efficient detection of patterns on event streams to handling uncertainty and noise in streams , and machine learning techniques for inferring interesting patterns . lately , it has expanded to methods for forecasting the occurrence of events . it was founded in 2009 and currently hosts 3 senior researchers , 5 phd students and works regularly with under-graduate students .

crowdsourcing with unsure option
one of the fundamental problems in crowdsourcing is the trade-off between the number of the workers needed for high-accuracy aggregation and the budget to pay . for saving budget , it is important to ensure high quality of the crowd-sourced labels , hence the total cost on label collection will be reduced . since the self-confidence of the workers often has a close relationship with their abilities , a possible way for quality control is to request the workers to return the labels only when they feel confident , by means of providing unsure option to them . on the other hand , allowing workers to choose unsure option also leads to the potential danger of budget waste . in this work , we propose the analysis towards understanding when providing the unsure option indeed leads to significant cost reduction , as well as how the confidence threshold is set . we also propose an online mechanism , which is alternative for threshold selection when the estimation of the crowd ability distribution is difficult .

towards the design of prospect-theory based human decision rules for hypothesis testing
detection rules have traditionally been designed for rational agents that minimize the bayes risk ( average decision cost ) . with the advent of crowd-sensing systems , there is a need to redesign binary hypothesis testing rules for behavioral agents , whose cognitive behavior is not captured by traditional utility functions such as bayes risk . in this paper , we adopt prospect theory based models for decision makers . we consider special agent models namely optimists and pessimists in this paper , and derive optimal detection rules under different scenarios . using an illustrative example , we also show how the decision rule of a human agent deviates from the bayesian decision rule under various behavioral models , considered in this paper .

a disembodied developmental robotic agent called samu bátfai
the agent program , called samu , is an experiment to build a disembodied devrob ( developmental robotics ) chatter bot that can talk in a natural language like humans do . one of the main design feature is that samu can be interacted with using only a character terminal . this is important not only for practical aspects of turing test or loebner prize , but also for the study of basic principles of developmental robotics . our purpose is to create a rapid prototype of q-learning with neural network approximators for samu . we sketch out the early stages of the development process of this prototype , where samu 's task is to predict the next sentence of tales or conversations . the basic objective of this paper is to reach the same results using reinforcement learning with general function approximators that can be achieved by using the classical q lookup table on small input samples . the paper is closed by an experiment that shows a significant improvement in samu 's learning when using lzw tree to narrow the number of possible q-actions .

xml matchers : approaches and challenges
schema matching , i.e . the process of discovering semantic correspondences between concepts adopted in different data source schemas , has been a key topic in database and artificial intelligence research areas for many years . in the past , it was largely investigated especially for classical database models ( e.g. , e/r schemas , relational databases , etc. ) . however , in the latest years , the widespread adoption of xml in the most disparate application fields pushed a growing number of researchers to design xml-specific schema matching approaches , called xml matchers , aiming at finding semantic matchings between concepts defined in dtds and xsds . xml matchers do not just take well-known techniques originally designed for other data models and apply them on dtds/xsds , but they exploit specific xml features ( e.g. , the hierarchical structure of a dtd/xsd ) to improve the performance of the schema matching process . the design of xml matchers is currently a well-established research area . the main goal of this paper is to provide a detailed description and classification of xml matchers . we first describe to what extent the specificities of dtds/xsds impact on the schema matching task . then we introduce a template , called xml matcher template , that describes the main components of an xml matcher , their role and behavior . we illustrate how each of these components has been implemented in some popular xml matchers . we consider our xml matcher template as the baseline for objectively comparing approaches that , at first glance , might appear as unrelated . the introduction of this template can be useful in the design of future xml matchers . finally , we analyze commercial tools implementing xml matchers and introduce two challenging issues strictly related to this topic , namely xml source clustering and uncertainty management in xml matchers .

game theory models for communication between agents : a review
in the real world , agents or entities are in a continuous state of interactions . these inter- actions lead to various types of complexity dynamics . one key difficulty in the study of complex agent interactions is the difficulty of modeling agent communication on the basis of rewards . game theory offers a perspective of analysis and modeling these interactions . previously , while a large amount of literature is available on game theory , most of it is from specific domains and does not cater for the concepts from an agent- based perspective . here in this paper , we present a comprehensive multidisciplinary state-of-the-art review and taxonomy of game theory models of complex interactions between agents .

feudal networks for hierarchical reinforcement learning
we introduce feudal networks ( funs ) : a novel architecture for hierarchical reinforcement learning . our approach is inspired by the feudal reinforcement learning proposal of dayan and hinton , and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time . our framework employs a manager module and a worker module . the manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the worker . the worker generates primitive actions at every tick of the environment . the decoupled structure of fun conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the manager . these properties allow fun to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation . we demonstrate the performance of our proposed system on a range of tasks from the atari suite and also from a 3d deepmind lab environment .

friend inspector : a serious game to enhance privacy awareness in social networks
currently , many users of social network sites are insufficiently aware of who can see their shared personal items . nonetheless , most approaches focus on enhancing privacy in social networks through improved privacy settings , neglecting the fact that privacy awareness is a prerequisite for privacy control . social network users first need to know about privacy issues before being able to make adjustments . in this paper , we introduce friend inspector , a serious game that allows its users to playfully increase their privacy awareness on facebook . since its launch , friend inspector has attracted a significant number of visitors , emphasising the need for better tools to understand privacy settings on social networks .

poincaré embeddings for learning hierarchical representations
representation learning has become an invaluable approach for learning from symbolic data such as text and graphs . however , while complex symbolic datasets often exhibit a latent hierarchical structure , state-of-the-art methods typically learn embeddings in euclidean vector spaces , which do not account for this property . for this purpose , we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional poincar\'e ball . due to the underlying hyperbolic geometry , this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity . we introduce an efficient algorithm to learn the embeddings based on riemannian optimization and show experimentally that poincar\'e embeddings outperform euclidean embeddings significantly on data with latent hierarchies , both in terms of representation capacity and in terms of generalization ability .

a survey on optical character recognition system
optical character recognition ( ocr ) has been a topic of interest for many years . it is defined as the process of digitizing a document image into its constituent characters . despite decades of intense research , developing ocr with capabilities comparable to that of human still remains an open challenge . due to this challenging nature , researchers from industry and academic circles have directed their attentions towards optical character recognition . over the last few years , the number of academic laboratories and companies involved in research on character recognition has increased dramatically . this research aims at summarizing the research so far done in the field of ocr . it provides an overview of different aspects of ocr and discusses corresponding proposals aimed at resolving issues of ocr .

governing governance : a formal framework for analysing institutional design and enactment governance
this dissertation is motivated by the need , in today 's globalist world , for a precise way to enable governments , organisations and other regulatory bodies to evaluate the constraints they place on themselves and others . an organisation 's modus operandi is enacting and fulfilling contracts between itself and its participants . yet , organisational contracts should respect external laws , such as those setting out data privacy rights and liberties . contracts can only be enacted by following contract law processes , which often require bilateral agreement and consideration . governments need to legislate whilst understanding today 's context of national and international governance hierarchy where law makers shun isolationism and seek to influence one another . governments should avoid punishment by respecting constraints from international treaties and human rights charters . governments can only enact legislation by following their own , pre-existing , law making procedures . in other words , institutions , such as laws and contracts are designed and enacted under constraints .

representation requirements for supporting decision model formulation
this paper outlines a methodology for analyzing the representational support for knowledge-based decision-modeling in a broad domain . a relevant set of inference patterns and knowledge types are identified . by comparing the analysis results to existing representations , some insights are gained into a design approach for integrating categorical and uncertain knowledge in a context sensitive manner .

the rationale behind the concept of goal
the paper proposes a fresh look at the concept of goal and advances that motivational attitudes like desire , goal and intention are just facets of the broader notion of ( acceptable ) outcome . we propose to encode the preferences of an agent as sequences of `` alternative acceptable outcomes '' . we then study how the agent 's beliefs and norms can be used to filter the mental attitudes out of the sequences of alternative acceptable outcomes . finally , we formalise such intuitions in a novel modal defeasible logic and we prove that the resulting formalisation is computationally feasible .

ai in arbitrary world
in order to build ai we have to create a program which copes well in an arbitrary world . in this paper we will restrict our attention on one concrete world , which represents the game tick-tack-toe . this world is a very simple one but it is sufficiently complicated for our task because most people can not manage with it . the main difficulty in this world is that the player can not see the entire internal state of the world so he has to build a model in order to understand the world . the model which we will offer will consist of final automata and first order formulas .

efficient markov network structure discovery using independence tests
we present two algorithms for learning the structure of a markov network from data : gsmn* and gsimn . both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests . until very recently , algorithms for structure learning were based on maximum likelihood estimation , which has been proved to be np-hard for markov networks due to the difficulty of estimating the parameters of the network , needed for the computation of the data likelihood . the independence-based approach does not require the computation of the likelihood , and thus both gsmn* and gsimn can compute the structure efficiently ( as shown in our experiments ) . gsmn* is an adaptation of the grow-shrink algorithm of margaritis and thrun for learning the structure of bayesian networks . gsimn extends gsmn* by additionally exploiting pearls well-known properties of the conditional independence relation to infer novel independences from known ones , thus avoiding the performance of statistical tests to estimate them . to accomplish this efficiently gsimn uses the triangle theorem , also introduced in this work , which is a simplified version of the set of markov axioms . experimental comparisons on artificial and real-world data sets show gsimn can yield significant savings with respect to gsmn* , while generating a markov network with comparable or in some cases improved quality . we also compare gsimn to a forward-chaining implementation , called gsimn-fch , that produces all possible conditional independences resulting from repeatedly applying pearls theorems on the known conditional independence tests . the results of this comparison show that gsimn , by the sole use of the triangle theorem , is nearly optimal in terms of the set of independences tests that it infers .

icon challenge on algorithm selection
we present the results of the icon challenge on algorithm selection .

structured region graphs : morphing ep into gbp
gbp and ep are two successful algorithms for approximate probabilistic inference , which are based on different approximation strategies . an open problem in both algorithms has been how to choose an appropriate approximation structure . we introduce 'structured region graphs ' , a formalism which marries these two strategies , reveals a deep connection between them , and suggests how to choose good approximation structures . in this formalism , each region has an internal structure which defines an exponential family , whose sufficient statistics must be matched by the parent region . reduction operators on these structures allow conversion between ep and gbp free energies . thus it is revealed that all ep approximations on discrete variables are special cases of gbp , and conversely that some wellknown gbp approximations , such as overlapping squares , are special cases of ep . furthermore , region graphs derived from ep have a number of good structural properties , including maxent-normality and overall counting number of one . the result is a convenient framework for producing high-quality approximations with a user-adjustable level of complexity

imagination-augmented agents for deep reinforcement learning
we introduce imagination-augmented agents ( i2as ) , a novel architecture for deep reinforcement learning combining model-free and model-based aspects . in contrast to most existing model-based reinforcement learning and planning methods , which prescribe how a model should be used to arrive at a policy , i2as learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways , by using the predictions as additional context in deep policy networks . i2as show improved data efficiency , performance , and robustness to model misspecification compared to several baselines .

an online universal classifier for binary , multi-class and multi-label classification
classification involves the learning of the mapping function that associates input samples to corresponding target label . there are two major categories of classification problems : single-label classification and multi-label classification . traditional binary and multi-class classifications are sub-categories of single-label classification . several classifiers are developed for binary , multi-class and multi-label classification problems , but there are no classifiers available in the literature capable of performing all three types of classification . in this paper , a novel online universal classifier capable of performing all the three types of classification is proposed . being a high speed online classifier , the proposed technique can be applied to streaming data applications . the performance of the developed classifier is evaluated using datasets from binary , multi-class and multi-label problems . the results obtained are compared with state-of-the-art techniques from each of the classification types .

novel exploration techniques ( nets ) for malaria policy interventions
the task of decision-making under uncertainty is daunting , especially for problems which have significant complexity . healthcare policy makers across the globe are facing problems under challenging constraints , with limited tools to help them make data driven decisions . in this work we frame the process of finding an optimal malaria policy as a stochastic multi-armed bandit problem , and implement three agent based strategies to explore the policy space . we apply a gaussian process regression to the findings of each agent , both for comparison and to account for stochastic results from simulating the spread of malaria in a fixed population . the generated policy spaces are compared with published results to give a direct reference with human expert decisions for the same simulated population . our novel approach provides a powerful resource for policy makers , and a platform which can be readily extended to capture future more nuanced policy spaces .

exact and heuristic methods for the assembly line worker assignment and balancing problem
in traditional assembly lines , it is reasonable to assume that task execution times are the same for each worker . however , in sheltered work centres for disabled this assumption is not valid : some workers may execute some tasks considerably slower or even be incapable of executing them . worker heterogeneity leads to a problem called the assembly line worker assignment and balancing problem ( alwabp ) . for a fixed number of workers the problem is to maximize the production rate of an assembly line by assigning workers to stations and tasks to workers , while satisfying precedence constraints between the tasks . this paper introduces new heuristic and exact methods to solve this problem . we present a new mip model , propose a novel heuristic algorithm based on beam search , as well as a task-oriented branch-and-bound procedure which uses new reduction rules and lower bounds for solving the problem . extensive computational tests on a large set of instances show that these methods are effective and improve over existing ones .

cost-based intuitionist probabilities on spaces of graphs , hypergraphs and theorems
a novel partial order is defined on the space of digraphs or hypergraphs , based on assessing the cost of producing a graph via a sequence of elementary transformations . leveraging work by knuth and skilling on the foundations of inference , and the structure of heyting algebras on graph space , this partial order is used to construct an intuitionistic probability measure that applies to either digraphs or hypergraphs . as logical inference steps can be represented as transformations on hypergraphs representing logical statements , this also yields an intuitionistic probability measure on spaces of theorems . the central result is also extended to yield intuitionistic probabilities based on more general weighted rule systems defined over bicartesian closed categories .

inferring fine-grained details on user activities and home location from social media : detecting drinking-while-tweeting patterns in communities
nearly all previous work on geo-locating latent states and activities from social media confounds general discussions about activities , self-reports of users participating in those activities at times in the past or future , and self-reports made at the immediate time and place the activity occurs . activities , such as alcohol consumption , may occur at different places and types of places , and it is important not only to detect the local regions where these activities occur , but also to analyze the degree of participation in them by local residents . in this paper , we develop new machine learning based methods for fine-grained localization of activities and home locations from twitter data . we apply these methods to discover and compare alcohol consumption patterns in a large urban area , new york city , and a more suburban and rural area , monroe county . we find positive correlations between the rate of alcohol consumption reported among a community 's twitter users and the density of alcohol outlets , demonstrating that the degree of correlation varies significantly between urban and suburban areas . while our experiments are focused on alcohol use , our methods for locating homes and distinguishing temporally-specific self-reports are applicable to a broad range of behaviors and latent states .

representing scholarly claims in internet digital libraries : a knowledge modelling approach
this paper is concerned with tracking and interpreting scholarly documents in distributed research communities . we argue that current approaches to document description , and current technological infrastructures particularly over the world wide web , provide poor support for these tasks . we describe the design of a digital library server which will enable authors to submit a summary of the contributions they claim their documents makes , and its relations to the literature . we describe a knowledge-based web environment to support the emergence of such a community-constructed semantic hypertext , and the services it could provide to assist the interpretation of an idea or document in the context of its literature . the discussion considers in detail how the approach addresses usability issues associated with knowledge structuring environments .

synthesis of shared control protocols with provable safety and performance guarantees
we formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications . more specifically , we introduce a modeling formalism in which both a human and an autonomy protocol can issue commands to a robot towards performing a certain task . these commands are blended into a joint input to the robot . the autonomy protocol is synthesized using an abstraction of possible human commands accounting for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand . the synthesis is designed to ensure that the resulting robot behavior satisfies given safety and performance specifications , e.g. , in temporal logic . our solution is based on nonlinear programming and we address the inherent scalability issue by presenting alternative methods . we assess the feasibility and the scalability of the approach by an experimental evaluation .

graph clustering bandits for recommendation
we investigate an efficient context-dependent clustering technique for recommender systems based on exploration-exploitation strategies through multi-armed bandits over multiple users . our algorithm dynamically groups users based on their observed behavioral similarity during a sequence of logged activities . in doing so , the algorithm reacts to the currently served user by shaping clusters around him/her but , at the same time , it explores the generation of clusters over users which are not currently engaged . we motivate the effectiveness of this clustering policy , and provide an extensive empirical analysis on real-world datasets , showing scalability and improved prediction performance over state-of-the-art methods for sequential clustering of users in multi-armed bandit scenarios .

a decision-based view of causality
most traditional models of uncertainty have focused on the associational relationship among variables as captured by conditional dependence . in order to successfully manage intelligent systems for decision making , however , we must be able to predict the effects of actions . in this paper , we attempt to unite two branches of research that address such predictions : causal modeling and decision analysis . first , we provide a definition of causal dependence in decision-analytic terms , which we derive from consequences of causal dependence cited in the literature . using this definition , we show how causal dependence can be represented within an influence diagram . in particular , we identify two inadequacies of an ordinary influence diagram as a representation for cause . we introduce a special class of influence diagrams , called causal influence diagrams , which corrects one of these problems , and identify situations where the other inadequacy can be eliminated . in addition , we describe the relationships between howard canonical form and existing graphical representations of cause .

a logical study of partial entailment
we introduce a novel logical notion -- partial entailment -- to propositional logic . in contrast with classical entailment , that a formula p partially entails another formula q with respect to a background formula set \gamma intuitively means that under the circumstance of \gamma , if p is true then some `` part '' of q will also be true . we distinguish three different kinds of partial entailments and formalize them by using an extended notion of prime implicant . we study their semantic properties , which show that , surprisingly , partial entailments fail for many simple inference rules . then , we study the related computational properties , which indicate that partial entailments are relatively difficult to be computed . finally , we consider a potential application of partial entailments in reasoning about rational agents .

pmog : the projected mixture of gaussians model with application to blind source separation
we extend the mixtures of gaussians ( mog ) model to the projected mixture of gaussians ( pmog ) model . in the pmog model , we assume that q dimensional input data points z_i are projected by a q dimensional vector w into 1-d variables u_i . the projected variables u_i are assumed to follow a 1-d mog model . in the pmog model , we maximize the likelihood of observing u_i to find both the model parameters for the 1-d mog as well as the projection vector w. first , we derive an em algorithm for estimating the pmog model . next , we show how the pmog model can be applied to the problem of blind source separation ( bss ) . in contrast to conventional bss where an objective function based on an approximation to differential entropy is minimized , pmog based bss simply minimizes the differential entropy of projected sources by fitting a flexible mog model in the projected 1-d space while simultaneously optimizing the projection vector w. the advantage of pmog over conventional bss algorithms is the more flexible fitting of non-gaussian source densities without assuming near-gaussianity ( as in conventional bss ) and still retaining computational feasibility .

fine-grained word sense disambiguation based on parallel corpora , word alignment , word clustering and aligned wordnets
the paper presents a method for word sense disambiguation based on parallel corpora . the method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus . the wordnets are aligned to the princeton wordnet , according to the principles established by eurowordnet . the evaluation of the wsd system , implementing the method described herein showed very encouraging results . the same system used in a validation mode , can be used to check and spot alignment errors in multilingually aligned wordnets as balkanet and eurowordnet .

a dynamic phase selection strategy for satisfiability solvers
the phase selection is an important of a sat solver based on conflict-driven dpll . this paper presents a new phase selection strategy , in which the weight of each literal is defined as the sum of its implied-literals static weights . the implied literals of each literal is computed dynamically during the search . therefore , it is call a dynamic phase selection strategy . in general , computing dynamically a weight is time-consuming . hence , so far no sat solver applies successfully a dynamic phase selection . since the implied literal of our strategy conforms to that of the search process , the usual two watched-literals scheme can be applied here . thus , the cost of our dynamic phase selection is very low . to improve glucose 2.0 which won a gold medal for application category at sat 2011 competition , we build five phase selection schemes using the dynamic phase selection policy . on application instances of sat 2011 , glucose improved by the dynamic phase selection is significantly better than the original glucose . we conduct also experiments on lingeling , using the dynamic phase selection policy , and build two phase selection schemes . experimental results show that the improved lingeling is better than the original lingeling .

context-aware sentiment word identification : sentiword2vec
traditional sentiment analysis often uses sentiment dictionary to extract sentiment information in text and classify documents . however , emerging informal words and phrases in user generated content call for analysis aware to the context . usually , they have special meanings in a particular context . because of its great performance in representing inter-word relation , we use sentiment word vectors to identify the special words . based on the distributed language model word2vec , in this paper we represent a novel method about sentiment representation of word under particular context , to be detailed , to identify the words with abnormal sentiment polarity in long answers . result shows the improved model shows better performance in representing the words with special meaning , while keep doing well in representing special idiomatic pattern . finally , we will discuss the meaning of vectors representing in the field of sentiment , which may be different from general object-based conditions .

larger is better : the effect of learning rates enjoyed by stochastic optimization with progressive variance reduction
in this paper , we propose a simple variant of the original stochastic variance reduction gradient ( svrg ) , where hereafter we refer to as the variance reduced stochastic gradient descent ( vr-sgd ) . different from the choices of the snapshot point and starting point in svrg and its proximal variant , prox-svrg , the two vectors of each epoch in vr-sgd are set to the average and last iterate of the previous epoch , respectively . this setting allows us to use much larger learning rates or step sizes than svrg , e.g. , 3/ ( 7l ) for vr-sgd vs 1/ ( 10l ) for svrg , and also makes our convergence analysis more challenging . in fact , a larger learning rate enjoyed by vr-sgd means that the variance of its stochastic gradient estimator asymptotically approaches zero more rapidly . unlike common stochastic methods such as svrg and proximal stochastic methods such as prox-svrg , we design two different update rules for smooth and non-smooth objective functions , respectively . in other words , vr-sgd can tackle non-smooth and/or non-strongly convex problems directly without using any reduction techniques such as quadratic regularizers . moreover , we analyze the convergence properties of vr-sgd for strongly convex problems , which show that vr-sgd attains a linear convergence rate . we also provide the convergence guarantees of vr-sgd for non-strongly convex problems . experimental results show that the performance of vr-sgd is significantly better than its counterparts , svrg and prox-svrg , and it is also much better than the best known stochastic method , katyusha .

decomposable theories
we present in this paper a general algorithm for solving first-order formulas in particular theories called `` decomposable theories '' . first of all , using special quantifiers , we give a formal characterization of decomposable theories and show some of their properties . then , we present a general algorithm for solving first-order formulas in any decomposable theory `` t '' . the algorithm is given in the form of five rewriting rules . it transforms a first-order formula `` p '' , which can possibly contain free variables , into a conjunction `` q '' of solved formulas easily transformable into a boolean combination of existentially quantified conjunctions of atomic formulas . in particular , if `` p '' has no free variables then `` q '' is either the formula `` true '' or `` false '' . the correctness of our algorithm proves the completeness of the decomposable theories . finally , we show that the theory `` tr '' of finite or infinite trees is a decomposable theory and give some benchmarks realized by an implementation of our algorithm , solving formulas on two-partner games in `` tr '' with more than 160 nested alternated quantifiers .

theory-guided data science : a new paradigm for scientific discovery from data
data science models , although successful in a number of commercial domains , have had limited applicability in scientific problems involving complex physical phenomena . theory-guided data science ( tgds ) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery . the overarching vision of tgds is to introduce scientific consistency as an essential component for learning generalizable models . further , by producing scientifically interpretable models , tgds aims to advance our scientific understanding by discovering novel domain insights . indeed , the paradigm of tgds has started to gain prominence in a number of scientific disciplines such as turbulence modeling , material discovery , quantum chemistry , bio-medical science , bio-marker discovery , climate science , and hydrology . in this paper , we formally conceptualize the paradigm of tgds and present a taxonomy of research themes in tgds . we describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines . we also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science .

on the number of samples needed to learn the correct structure of a bayesian network
bayesian networks ( bns ) are useful tools giving a natural and compact representation of joint probability distributions . in many applications one needs to learn a bayesian network ( bn ) from data . in this context , it is important to understand the number of samples needed in order to guarantee a successful learning . previous work have studied bns sample complexity , yet it mainly focused on the requirement that the learned distribution will be close to the original distribution which generated the data . in this work , we study a different aspect of the learning , namely the number of samples needed in order to learn the correct structure of the network . we give both asymptotic results , valid in the large sample limit , and experimental results , demonstrating the learning behavior for feasible sample sizes . we show that structure learning is a more difficult task , compared to approximating the correct distribution , in the sense that it requires a much larger number of samples , regardless of the computational power available for the learner .

on local regret
online learning aims to perform nearly as well as the best hypothesis in hindsight . for some hypothesis classes , though , even finding the best hypothesis offline is challenging . in such offline cases , local search techniques are often employed and only local optimality guaranteed . for online decision-making with such hypothesis classes , we introduce local regret , a generalization of regret that aims to perform nearly as well as only nearby hypotheses . we then present a general algorithm to minimize local regret with arbitrary locality graphs . we also show how the graph structure can be exploited to drastically speed learning . these algorithms are then demonstrated on a diverse set of online problems : online disjunct learning , online max-sat , and online decision tree learning .

decision principles to justify carnap 's updating method and to suggest corrections of probability judgments ( invited talks )
this paper uses decision-theoretic principles to obtain new insights into the assessment and updating of probabilities . first , a new foundation of bayesianism is given . it does not require infinite atomless uncertainties as did savage s classical result , and can therefore be applied to any finite bayesian network.it neither requires linear utility as did de finetti s classical result , and r ntherefore allows for the empirically and normatively desirable risk r naversion.finally , by identifying and fixing utility in an elementary r nmanner , our result can readily be applied to identify methods of r nprobability updating.thus , a decision - theoretic foundation is given r nto the computationally efficient method of inductive reasoning r ndeveloped by rudolf carnap.finally , recent empirical findings on r nprobability assessments are discussed.it leads to suggestions for r ncorrecting biases in probability assessments , and for an alternative r nto the dempster - shafer belief functions that avoids the reduction to r ndegeneracy after multiple updatings.r n

falling rule lists
falling rule lists are classification models consisting of an ordered list of if-then rules , where ( i ) the order of rules determines which example should be classified by each rule , and ( ii ) the estimated probability of success decreases monotonically down the list . these kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first . we provide a bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods .

near-optimal adversarial policy switching for decentralized asynchronous multi-agent systems
a key challenge in multi-robot and multi-agent systems is generating solutions that are robust to other self-interested or even adversarial parties who actively try to prevent the agents from achieving their goals . the practicality of existing works addressing this challenge is limited to only small-scale synchronous decision-making scenarios or a single agent planning its best response against a single adversary with fixed , procedurally characterized strategies . in contrast this paper considers a more realistic class of problems where a team of asynchronous agents with limited observation and communication capabilities need to compete against multiple strategic adversaries with changing strategies . this problem necessitates agents that can coordinate to detect changes in adversary strategies and plan the best response accordingly . our approach first optimizes a set of stratagems that represent these best responses . these optimized stratagems are then integrated into a unified policy that can detect and respond when the adversaries change their strategies . the near-optimality of the proposed framework is established theoretically as well as demonstrated empirically in simulation and hardware .

learning to play guess who ? and inventing a grounded language as a consequence
acquiring your first language is an incredible feat and not easily duplicated . learning to communicate using nothing but a few pictureless books , a corpus , would likely be impossible even for humans . nevertheless , this is the dominating approach in most natural language processing today . as an alternative , we propose the use of situated interactions between agents as a driving force for communication , and the framework of deep recurrent q-networks for evolving a shared language grounded in the provided environment . we task the agents with interactive image search in the form of the game guess who ? . the images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication . our experiments show that the agents learn not only to encode physical concepts in their words , i.e . grounding , but also that the agents learn to hold a multi-step dialogue remembering the state of the dialogue from step to step .

security games with information leakage : modeling and computation
most models of stackelberg security games assume that the attacker only knows the defender 's mixed strategy , but is not able to observe ( even partially ) the instantiated pure strategy . such partial observation of the deployed pure strategy -- an issue we refer to as information leakage -- is a significant concern in practical applications . while previous research on patrolling games has considered the attacker 's real-time surveillance , our settings , therefore models and techniques , are fundamentally different . more specifically , after describing the information leakage model , we start with an lp formulation to compute the defender 's optimal strategy in the presence of leakage . perhaps surprisingly , we show that a key subproblem to solve this lp ( more precisely , the defender oracle ) is np-hard even for the simplest of security game models . we then approach the problem from three possible directions : efficient algorithms for restricted cases , approximation algorithms , and heuristic algorithms for sampling that improves upon the status quo . our experiments confirm the necessity of handling information leakage and the advantage of our algorithms .

ui-net : interactive artificial neural networks for iterative image segmentation based on a user model
for complex segmentation tasks , fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects . especially in cases where only few data sets need to be processed for a highly accurate result , semi-automatic segmentation techniques exhibit a clear benefit for the user . one area of application is medical image processing during an intervention for a single patient . we propose a learning-based cooperative segmentation approach which includes the computing entity as well as the user into the task . our system builds upon a state-of-the-art fully convolutional artificial neural network ( fcn ) as well as an active user model for training . during the segmentation process , a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the fcn system to achieve an interactive and precise segmentation result . the segmentation quality of interactive fcns is evaluated . iterative fcn approaches can yield superior results compared to networks without the user input channel component , due to a consistent improvement in segmentation quality after each interaction .

deepstack : expert-level artificial intelligence in no-limit poker
artificial intelligence has seen several breakthroughs in recent years , with games often serving as milestones . a common feature of these games is that players have perfect information . poker is the quintessential game of imperfect information , and a longstanding challenge problem in artificial intelligence . we introduce deepstack , an algorithm for imperfect information settings . it combines recursive reasoning to handle information asymmetry , decomposition to focus computation on the relevant decision , and a form of intuition that is automatically learned from self-play using deep learning . in a study involving 44,000 hands of poker , deepstack defeated with statistical significance professional poker players in heads-up no-limit texas hold'em . the approach is theoretically sound and is shown to produce more difficult to exploit strategies than prior approaches .

clustering belief functions based on attracting and conflicting metalevel evidence
in this paper we develop a method for clustering belief functions based on attracting and conflicting metalevel evidence . such clustering is done when the belief functions concern multiple events , and all belief functions are mixed up . the clustering process is used as the means for separating the belief functions into subsets that should be handled independently . while the conflicting metalevel evidence is generated internally from pairwise conflicts of all belief functions , the attracting metalevel evidence is assumed given by some external source .

probabilistic semantic web mining using artificial neural analysis
most of the web user 's requirements are search or navigation time and getting correctly matched result . these constrains can be satisfied with some additional modules attached to the existing search engines and web servers . this paper proposes that powerful architecture for search engines with the title of probabilistic semantic web mining named from the methods used . with the increase of larger and larger collection of various data resources on the world wide web ( www ) , web mining has become one of the most important requirements for the web users . web servers will store various formats of data including text , image , audio , video etc. , but servers can not identify the contents of the data . these search techniques can be improved by adding some special techniques including semantic web mining and probabilistic analysis to get more accurate results . semantic web mining technique can provide meaningful search of data resources by eliminating useless information with mining process . in this technique web servers will maintain meta information of each and every data resources available in that particular web server . this will help the search engine to retrieve information that is relevant to user given input string . this paper proposing the idea of combing these two techniques semantic web mining and probabilistic analysis for efficient and accurate search results of web mining . spf can be calculated by considering both semantic accuracy and syntactic accuracy of data with the input string . this will be the deciding factor for producing results .

informal concepts in machines
this paper constructively proves the existence of an effective procedure generating a computable ( total ) function that is not contained in any given effectively enumerable set of such functions . the proof implies the existence of machines that process informal concepts such as computable ( total ) functions beyond the limits of any given turing machine or formal system , that is , these machines can , in a certain sense , `` compute '' function values beyond these limits . we call these machines creative . we argue that any `` intelligent '' machine should be capable of processing informal concepts such as computable ( total ) functions , that is , it should be creative . finally , we introduce hypotheses on creative machines which were developed on the basis of theoretical investigations and experiments with computer programs . the hypotheses say that machine intelligence is the execution of a self-developing procedure starting from any universal programming language and any input .

engineering a conformant probabilistic planner
we present a partial-order , conformant , probabilistic planner , probapop which competed in the blind track of the probabilistic planning competition in ipc-4 . we explain how we adapt distance based heuristics for use with probabilistic domains . probapop also incorporates heuristics based on probability of success . we explain the successes and difficulties encountered during the design and implementation of probapop .

the role of commutativity in constraint propagation algorithms
constraint propagation algorithms form an important part of most of the constraint programming systems . we provide here a simple , yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way . in this framework we proceed in two steps . first , we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting . then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms . in particular , using the notions commutativity and semi-commutativity , we show that the { \tt ac-3 } , { \tt pc-2 } , { \tt dac } and { \tt dpc } algorithms for achieving ( directional ) arc consistency and ( directional ) path consistency are instances of a single generic algorithm . the work reported here extends and simplifies that of apt \citeyear { apt99b } .

a behavioral distance for fuzzy-transition systems
in contrast to the existing approaches to bisimulation for fuzzy systems , we introduce a behavioral distance to measure the behavioral similarity of states in a nondeterministic fuzzy-transition system . this behavioral distance is defined as the greatest fixed point of a suitable monotonic function and provides a quantitative analogue of bisimilarity . the behavioral distance has the important property that two states are at zero distance if and only if they are bisimilar . moreover , for any given threshold , we find that states with behavioral distances bounded by the threshold are equivalent . in addition , we show that two system combinators -- -parallel composition and product -- -are non-expansive with respect to our behavioral distance , which makes compositional verification possible .

rooting opinions in the minds : a cognitive model and a formal account of opinions and their dynamics
the study of opinions , their formation and change , is one of the defining topics addressed by social psychology , but in recent years other disciplines , like computer science and complexity , have tried to deal with this issue . despite the flourishing of different models and theories in both fields , several key questions still remain unanswered . the understanding of how opinions change and the way they are affected by social influence are challenging issues requiring a thorough analysis of opinion per se but also of the way in which they travel between agents ' minds and are modulated by these exchanges . to account for the two-faceted nature of opinions , which are mental entities undergoing complex social processes , we outline a preliminary model in which a cognitive theory of opinions is put forward and it is paired with a formal description of them and of their spreading among minds . furthermore , investigating social influence also implies the necessity to account for the way in which people change their minds , as a consequence of interacting with other people , and the need to explain the higher or lower persistence of such changes .

e-qraq : a multi-turn reasoning dataset and simulator with explanations
in this paper we present a new dataset and user simulator e-qraq ( explainable query , reason , and answer question ) which tests an agent 's ability to read an ambiguous text ; ask questions until it can answer a challenge question ; and explain the reasoning behind its questions and answer . the user simulator provides the agent with a short , ambiguous story and a challenge question about the story . the story is ambiguous because some of the entities have been replaced by variables . at each turn the agent may ask for the value of a variable or try to answer the challenge question . in response the user simulator provides a natural language explanation of why the agent 's query or answer was useful in narrowing down the set of possible answers , or not . to demonstrate one potential application of the e-qraq dataset , we train a new neural architecture based on end-to-end memory networks to successfully generate both predictions and partial explanations of its current understanding of the problem . we observe a strong correlation between the quality of the prediction and explanation .

lift-based bidding in ad selection
real-time bidding ( rtb ) has become one of the largest online advertising markets in the world . today the bid price per ad impression is typically decided by the expected value of how it can lead to a desired action event ( e.g. , registering an account or placing a purchase order ) to the advertiser . however , this industry standard approach to decide the bid price does not consider the actual effect of the ad shown to the user , which should be measured based on the performance lift among users who have been or have not been exposed to a certain treatment of ads . in this paper , we propose a new bidding strategy and prove that if the bid price is decided based on the performance lift rather than absolute performance value , advertisers can actually gain more action events . we describe the modeling methodology to predict the performance lift and demonstrate the actual performance gain through blind a/b test with real ad campaigns in an industry-leading demand-side platform ( dsp ) . we also discuss the relationship between attribution models and bidding strategies . we prove that , to move the dsps to bid based on performance lift , they should be rewarded according to the relative performance lift they contribute .

the neural network pushdown automaton : model , stack and learning simulations
in order for neural networks to learn complex languages or grammars , they must have sufficient computational power or resources to recognize or generate such languages . though many approaches have been discussed , one ob- vious approach to enhancing the processing power of a recurrent neural network is to couple it with an external stack memory - in effect creating a neural network pushdown automata ( nnpda ) . this paper discusses in detail this nnpda - its construction , how it can be trained and how useful symbolic information can be extracted from the trained network . in order to couple the external stack to the neural network , an optimization method is developed which uses an error function that connects the learning of the state automaton of the neural network to the learning of the operation of the external stack . to minimize the error function using gradient descent learning , an analog stack is designed such that the action and storage of information in the stack are continuous . one interpretation of a continuous stack is the probabilistic storage of and action on data . after training on sample strings of an unknown source grammar , a quantization procedure extracts from the analog stack and neural network a discrete pushdown automata ( pda ) . simulations show that in learning deterministic context-free grammars - the balanced parenthesis language , 1*n0*n , and the deterministic palindrome - the extracted pda is correct in the sense that it can correctly recognize unseen strings of arbitrary length . in addition , the extracted pdas can be shown to be identical or equivalent to the pdas of the source grammars which were used to generate the training strings .

short portfolio training for csp solving
many different approaches for solving constraint satisfaction problems ( csps ) and related constraint optimization problems ( cops ) exist . however , there is no single solver ( nor approach ) that performs well on all classes of problems and many portfolio approaches for selecting a suitable solver based on simple syntactic features of the input csp instance have been developed . in this paper we first present a simple portfolio method for csp based on k-nearest neighbors method . then , we propose a new way of using portfolio systems -- - training them shortly in the exploitation time , specifically for the set of instances to be solved and using them on that set . thorough evaluation has been performed and has shown that the approach yields good results . we evaluated several machine learning techniques for our portfolio . due to its simplicity and efficiency , the selected k-nearest neighbors method is especially suited for our short training approach and it also yields the best results among the tested methods . we also confirm that our approach yields good results on sat domain .

computational methods for probabilistic inference of sector congestion in air traffic management
this article addresses the issue of computing the expected cost functions from a probabilistic model of the air traffic flow and capacity management . the clenshaw-curtis quadrature is compared to monte-carlo algorithms defined specifically for this problem . by tailoring the algorithms to this model , we reduce the computational burden in order to simulate real instances . the study shows that the monte-carlo algorithm is more sensible to the amount of uncertainty in the system , but has the advantage to return a result with the associated accuracy on demand . the performances for both approaches are comparable for the computation of the expected cost of delay and the expected cost of congestion . finally , this study shows some evidences that the simulation of the proposed probabilistic model is tractable for realistic instances .

the loss surface and expressivity of deep convolutional neural networks
we analyze the expressiveness and loss surface of practical deep convolutional neural networks ( cnns ) with shared weights and max pooling layers . we show that such cnns produce linearly independent features at a `` wide '' layer which has more neurons than the number of training samples . this condition holds e.g . for the vgg network . furthermore , we provide for such wide cnns necessary and sufficient conditions for global minima with zero training error . for the case where the wide layer is followed by a fully connected layer , we show that almost every critical point of the empirical loss is a global minimum with zero training error . our analysis suggests that both depth and width are very important in deep learning . while depth brings more representational power and allows the network to learn high level features , width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well-behaved loss surface with potentially no bad local minima .

advances in learning bayesian networks of bounded treewidth
this work presents novel algorithms for learning bayesian network structures with bounded treewidth . both exact and approximate methods are developed . the exact method combines mixed-integer linear programming formulations for structure learning and treewidth computation . the approximate method consists in uniformly sampling $ k $ -trees ( maximal graphs of treewidth $ k $ ) , and subsequently selecting , exactly or approximately , the best structure whose moral graph is a subgraph of that $ k $ -tree . some properties of these methods are discussed and proven . the approaches are empirically compared to each other and to a state-of-the-art method for learning bounded treewidth structures on a collection of public data sets with up to 100 variables . the experiments show that our exact algorithm outperforms the state of the art , and that the approximate approach is fairly accurate .

understanding grounded language learning agents
neural network-based systems can now learn to locate the referents of words and phrases in images , answer questions about visual scenes , and even execute symbolic instructions as first-person actors in partially-observable worlds . to achieve this so-called grounded language learning , models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words . while it is notable that models with no meaningful prior knowledge overcome these learning obstacles , ai researchers and practitioners currently lack a clear understanding of exactly how they do so . here we address this question as a way of achieving a clearer general understanding of grounded language learning , both to inform future research and to improve confidence in model predictions . for maximum control and generality , we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3d world . we apply experimental paradigms from developmental psychology to this agent , exploring the conditions under which established human biases and learning effects emerge . we further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects .

citizen science : contributions to astronomy research
the contributions of everyday individuals to significant research has grown dramatically beyond the early days of classical birdwatching and endeavors of amateurs of the 19th century . now people who are casually interested in science can participate directly in research covering diverse scientific fields . regarding astronomy , volunteers , either as individuals or as networks of people , are involved in a variety of types of studies . citizen science is intuitive , engaging , yet necessarily robust in its adoption of sci-entific principles and methods . herein , we discuss citizen science , focusing on fully participatory projects such as zooniverse ( by several of the au-thors cl , as , lf , sb ) , with mention of other programs . in particular , we make the case that citizen science ( cs ) can be an important aspect of the scientific data analysis pipelines provided to scientists by observatories .

combining existential rules and transitivity : next steps
we consider existential rules ( aka datalog+ ) as a formalism for specifying ontologies . in recent years , many classes of existential rules have been exhibited for which conjunctive query ( cq ) entailment is decidable . however , most of these classes can not express transitivity of binary relations , a frequently used modelling construct . in this paper , we address the issue of whether transitivity can be safely combined with decidable classes of existential rules . first , we prove that transitivity is incompatible with one of the simplest decidable classes , namely agrd ( acyclic graph of rule dependencies ) , which clarifies the landscape of ` finite expansion sets ' of rules . second , we show that transitivity can be safely added to linear rules ( a subclass of guarded rules , which generalizes the description logic dl-lite-r ) in the case of atomic cqs , and also for general cqs if we place a minor syntactic restriction on the rule set . this is shown by means of a novel query rewriting algorithm that is specially tailored to handle transitivity rules . third , for the identified decidable cases , we pinpoint the combined and data complexities of query entailment .

a base camp for scaling ai
modern statistical machine learning ( sml ) methods share a major limitation with the early approaches to ai : there is no scalable way to adapt them to new domains . human learning solves this in part by leveraging a rich , shared , updateable world model . such scalability requires modularity : updating part of the world model should not impact unrelated parts . we have argued that such modularity will require both `` correctability '' ( so that errors can be corrected without introducing new errors ) and `` interpretability '' ( so that we can understand what components need correcting ) . to achieve this , one could attempt to adapt state of the art sml systems to be interpretable and correctable ; or one could see how far the simplest possible interpretable , correctable learning methods can take us , and try to control the limitations of sml methods by applying them only where needed . here we focus on the latter approach and we investigate two main ideas : `` teacher assisted learning '' , which leverages crowd sourcing to learn language ; and `` factored dialog learning '' , which factors the process of application development into roles where the language competencies needed are isolated , enabling non-experts to quickly create new applications . we test these ideas in an `` automated personal assistant '' ( apa ) setting , with two scenarios : that of detecting user intent from a user-apa dialog ; and that of creating a class of event reminder applications , where a non-expert `` teacher '' can then create specific apps . for the intent detection task , we use a dataset of a thousand labeled utterances from user dialogs with cortana , and we show that our approach matches state of the art sml methods , but in addition provides full transparency : the whole ( editable ) model can be summarized on one human-readable page . for the reminder app task , we ran small user studies to verify the efficacy of the approach .

fusion of eeg and musical features in continuous music-emotion recognition
emotion estimation in music listening is confronting challenges to capture the emotion variation of listeners . recent years have witnessed attempts to exploit multimodality fusing information from musical contents and physiological signals captured from listeners to improve the performance of emotion recognition . in this paper , we present a study of fusion of signals of electroencephalogram ( eeg ) , a tool to capture brainwaves at a high-temporal resolution , and musical features at decision level in recognizing the time-varying binary classes of arousal and valence . our empirical results showed that the fusion could outperform the performance of emotion recognition using only eeg modality that was suffered from inter-subject variability , and this suggested the promise of multimodal fusion in improving the accuracy of music-emotion recognition .

toward experiential utility elicitation for interface customization
user preferences for automated assistance often vary widely , depending on the situation , and quality or presentation of help . developing effectivemodels to learn individual preferences online requires domain models that associate observations of user behavior with their utility functions , which in turn can be constructed using utility elicitation techniques . however , most elicitation methods ask for users ' predicted utilities based on hypothetical scenarios rather than more realistic experienced utilities . this is especially true in interface customization , where users are asked to assess novel interface designs . we propose experiential utility elicitation methods for customization and compare these to predictivemethods . as experienced utilities have been argued to better reflect true preferences in behavioral decision making , the purpose here is to investigate accurate and efficient procedures that are suitable for software domains . unlike conventional elicitation , our results indicate that an experiential approach helps people understand stochastic outcomes , as well as better appreciate the sequential utility of intelligent assistance .

a combinatorial optimisation approach to designing dual-parented long-reach passive optical networks
we present an application focused on the design of resilient long-reach passive optical networks . we specifically consider dual-parented networks whereby each customer must be connected to two metro sites via local exchange sites . an important property of such a placement is resilience to single metro node failure . the objective of the application is to determine the optimal position of a set of metro nodes such that the total optical fibre length is minimized . we prove that this problem is np-complete . we present two alternative combinatorial optimisation approaches to finding an optimal metro node placement using : a mixed integer linear programming ( mip ) formulation of the problem ; and , a hybrid approach that uses clustering as a preprocessing step . we consider a detailed case-study based on a network for ireland . the hybrid approach scales well and finds solutions that are close to optimal , with a runtime that is two orders-of-magnitude better than the mip model .

of the people : voting is more effective with representative candidates
in light of the classic impossibility results of arrow and gibbard and satterthwaite regarding voting with ordinal rules , there has been recent interest in characterizing how well common voting rules approximate the social optimum . in order to quantify the quality of approximation , it is natural to consider the candidates and voters as embedded within a common metric space , and to ask how much further the chosen candidate is from the population as compared to the socially optimal one . we use this metric preference model to explore a fundamental and timely question : does the social welfare of a population improve when candidates are representative of the population ? if so , then by how much , and how does the answer depend on the complexity of the metric space ? we restrict attention to the most fundamental and common social choice setting : a population of voters , two independently drawn candidates , and a majority rule election . when candidates are not representative of the population , it is known that the candidate selected by the majority rule can be thrice as far from the population as the socially optimal one . we examine how this ratio improves when candidates are drawn independently from the population of voters . our results are two-fold : when the metric is a line , the ratio improves from $ 3 $ to $ 4-2\sqrt { 2 } $ , roughly $ 1.1716 $ ; this bound is tight . when the metric is arbitrary , we show a lower bound of $ 1.5 $ and a constant upper bound strictly better than $ 2 $ on the approximation ratio of the majority rule . the positive result depends in part on the assumption that candidates are independent and identically distributed . however , we show that independence alone is not enough to achieve the upper bound : even when candidates are drawn independently , if the population of candidates can be different from the voters , then an upper bound of $ 2 $ on the approximation is tight .

ontology and formal semantics - integration overdue
in this note we suggest that difficulties encountered in natural language semantics are , for the most part , due to the use of mere symbol manipulation systems that are devoid of any content . in such systems , where there is hardly any link with our common-sense view of the world , and it is quite difficult to envision how one can formally account for the considerable amount of content that is often implicit , but almost never explicitly stated in our everyday discourse . the solution , in our opinion , is a compositional semantics grounded in an ontology that reflects our commonsense view of the world and the way we talk about it in ordinary language . in the compositional logic we envision there are ontological ( or first-intension ) concepts , and logical ( or second-intension ) concepts , and where the ontological concepts include not only davidsonian events , but other abstract objects as well ( e.g. , states , processes , properties , activities , attributes , etc . ) it will be demonstrated here that in such a framework , a number of challenges in the semantics of natural language ( e.g. , metonymy , intensionality , metaphor , etc . ) can be properly and uniformly addressed .

on the possibility of learning in reactive environments with arbitrary dependence
we address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions , i.e . environments more general than ( po ) mdps . the task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments . we find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class . we analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields , such as markov decision processes and mixing conditions .

computing as compression : the sp theory of intelligence
this paper provides an overview of the sp theory of intelligence and its central idea that artificial intelligence , mainstream computing , and much of human perception and cognition , may be understood as information compression . the background and origins of the sp theory are described , and the main elements of the theory , including the key concept of multiple alignment , borrowed from bioinformatics but with important differences . associated with the sp theory is the idea that redundancy in information may be understood as repetition of patterns , that compression of information may be achieved via the matching and unification ( merging ) of patterns , and that computing and information compression are both fundamentally probabilistic . it appears that the sp system is turing-equivalent in the sense that anything that may be computed with a turing machine may , in principle , also be computed with an sp machine . one of the main strengths of the sp theory and the multiple alignment concept is in modelling concepts and phenomena in artificial intelligence . within that area , the sp theory provides a simple but versatile means of representing different kinds of knowledge , it can model both the parsing and production of natural language , with potential for the understanding and translation of natural languages , it has strengths in pattern recognition , with potential in computer vision , it can model several kinds of reasoning , and it has capabilities in planning , problem solving , and unsupervised learning . the paper includes two examples showing how alternative parsings of an ambiguous sentence may be modelled as multiple alignments , and another example showing how the concept of multiple alignment may be applied in medical diagnosis .

superposition for fixed domains
superposition is an established decision procedure for a variety of first-order logic theories represented by sets of clauses . a satisfiable theory , saturated by superposition , implicitly defines a minimal term-generated model for the theory . proving universal properties with respect to a saturated theory directly leads to a modification of the minimal model 's term-generated domain , as new skolem functions are introduced . for many applications , this is not desired . therefore , we propose the first superposition calculus that can explicitly represent existentially quantified variables and can thus compute with respect to a given domain . this calculus is sound and refutationally complete in the limit for a first-order fixed domain semantics . for saturated horn theories and classes of positive formulas , we can even employ the calculus to prove properties of the minimal model itself , going beyond the scope of known superposition-based approaches .

a spectrum of applications of automated reasoning
the likelihood of an automated reasoning program being of substantial assistance for a wide spectrum of applications rests with the nature of the options and parameters it offers on which to base needed strategies and methodologies . this article focuses on such a spectrum , featuring w. mccune 's program otter , discussing widely varied successes in answering open questions , and touching on some of the strategies and methodologies that played a key role . the applications include finding a first proof , discovering single axioms , locating improved axiom systems , and simplifying existing proofs . the last application is directly pertinent to the recently found ( by r. thiele ) hilbert 's twenty-fourth problem -- which is extremely amenable to attack with the appropriate automated reasoning program -- a problem concerned with proof simplification . the methodologies include those for seeking shorter proofs and for finding proofs that avoid unwanted lemmas or classes of term , a specific option for seeking proofs with smaller equational or formula complexity , and a different option to address the variable richness of a proof . the type of proof one obtains with the use of otter is hilbert-style axiomatic , including details that permit one sometimes to gain new insights . we include questions still open and challenges that merit consideration .

improving semantic embedding consistency by metric learning for zero-shot classification
this paper addresses the task of zero-shot image classification . the key contribution of the proposed approach is to control the semantic embedding of images -- one of the main ingredients of zero-shot learning -- by formulating it as a metric learning problem . the optimized empirical criterion associates two types of sub-task constraints : metric discriminating capacity and accurate attribute prediction . this results in a novel expression of zero-shot learning not requiring the notion of class in the training phase : only pairs of image/attributes , augmented with a consistency indicator , are given as ground truth . at test time , the learned model can predict the consistency of a test image with a given set of attributes , allowing flexible ways to produce recognition inferences . despite its simplicity , the proposed approach gives state-of-the-art results on four challenging datasets used for zero-shot recognition evaluation .

towards the evolution of novel vertical-axis wind turbines
renewable and sustainable energy is one of the most important challenges currently facing mankind . wind has made an increasing contribution to the world 's energy supply mix , but still remains a long way from reaching its full potential . in this paper , we investigate the use of artificial evolution to design vertical-axis wind turbine prototypes that are physically instantiated and evaluated under approximated wind tunnel conditions . an artificial neural network is used as a surrogate model to assist learning and found to reduce the number of fabrications required to reach a higher aerodynamic efficiency , resulting in an important cost reduction . unlike in other approaches , such as computational fluid dynamics simulations , no mathematical formulations are used and no model assumptions are made .

geometric implications of the naive bayes assumption
a naive ( or idiot ) bayes network is a network with a single hypothesis node and several observations that are conditionally independent given the hypothesis . we recently surveyed a number of members of the uai community and discovered a general lack of understanding of the implications of the naive bayes assumption on the kinds of problems that can be solved by these networks . it has long been recognized [ minsky 61 ] that if observations are binary , the decision surfaces in these networks are hyperplanes . we extend this result ( hyperplane separability ) to naive bayes networks with m-ary observations . in addition , we illustrate the effect of observation-observation dependencies on decision surfaces . finally , we discuss the implications of these results on knowledge acquisition and research in learning .

probabilistic constraint satisfaction with non-gaussian noise
we have previously reported a bayesian algorithm for determining the coordinates of points in three-dimensional space from uncertain constraints . this method is useful in the determination of biological molecular structure . it is limited , however , by the requirement that the uncertainty in the constraints be normally distributed . in this paper , we present an extension of the original algorithm that allows constraint uncertainty to be represented as a mixture of gaussians , and thereby allows arbitrary constraint distributions . we illustrate the performance of this algorithm on a problem drawn from the domain of molecular structure determination , in which a multicomponent constraint representation produces a much more accurate solution than the old single component mechanism . the new mechanism uses mixture distributions to decompose the problem into a set of independent problems with unimodal constraint uncertainty . the results of the unimodal subproblems are periodically recombined using bayes ' law , to avoid combinatorial explosion . the new algorithm is particularly suited for parallel implementation .

equilibrium points of an and-or tree : under constraints on probability
we study a probability distribution d on the truth assignments to a uniform binary and-or tree . liu and tanaka [ 2007 , inform . process . lett . ] showed the following : if d achieves the equilibrium among independent distributions ( id ) then d is an independent identical distribution ( iid ) . we show a stronger form of the above result . given a real number r such that 0 < r < 1 , we consider a constraint that the probability of the root node having the value 0 is r. our main result is the following : when we restrict ourselves to ids satisfying this constraint , the above result of liu and tanaka still holds . the proof employs clever tricks of induction . in particular , we show two fundamental relationships between expected cost and probability in an iid on an or-and tree : ( 1 ) the ratio of the cost to the probability ( of the root having the value 0 ) is a decreasing function of the probability x of the leaf . ( 2 ) the ratio of derivative of the cost to the derivative of the probability is a decreasing function of x , too .

strategic dialogue management via deep reinforcement learning
artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped . this paper describes a successful application of deep reinforcement learning ( drl ) for training intelligent agents with strategic conversational skills , in a situated dialogue setting . previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques , the latter using tabular representations or learning with linear function approximation . in this study , we apply drl with a high-dimensional state space to the strategic board game of settlers of catan -- -where players can offer resources in exchange for others and they can also reply to offers made by other players . our experimental results report that the drl-based learnt policies significantly outperformed several baselines including random , rule-based , and supervised-based behaviours . the drl-based policy has a 53 % win rate versus 3 automated players ( ` bots ' ) , whereas a supervised player trained on a dialogue corpus in this setting achieved only 27 % , versus the same 3 bots . this result supports the claim that drl is a promising framework for training dialogue systems , and strategic agents with negotiation abilities .

toward a general , scaleable framework for bayesian teaching with applications to topic models
machines , not humans , are the world 's dominant knowledge accumulators but humans remain the dominant decision makers . interpreting and disseminating the knowledge accumulated by machines requires expertise , time , and is prone to failure . the problem of how best to convey accumulated knowledge from computers to humans is a critical bottleneck in the broader application of machine learning . we propose an approach based on human teaching where the problem is formalized as selecting a small subset of the data that will , with high probability , lead the human user to the correct inference . this approach , though successful for modeling human learning in simple laboratory experiments , has failed to achieve broader relevance due to challenges in formulating general and scalable algorithms . we propose general-purpose teaching via pseudo-marginal sampling and demonstrate the algorithm by teaching topic models . simulation results show our sampling-based approach : effectively approximates the probability where ground-truth is possible via enumeration , results in data that are markedly different from those expected by random sampling , and speeds learning especially for small amounts of data . application to movie synopsis data illustrates differences between teaching and random sampling for teaching distributions and specific topics , and demonstrates gains in scalability and applicability to real-world problems .

a geometry of information , i : nerves , posets and differential forms
the main theme of this workshop ( dagstuhl seminar 04351 ) is ` spatial representation : continuous vs. discrete ' . spatial representation has two contrasting but interacting aspects ( i ) representation of spaces ' and ( ii ) representation by spaces . in this paper , we will examine two aspects that are common to both interpretations of the theme , namely nerve constructions and refinement . representations change , data changes , spaces change . we will examine the possibility of a ` differential geometry ' of spatial representations of both types , and in the sequel give an algebra of differential forms that has the potential to handle the dynamical aspect of such a geometry . we will discuss briefly a conjectured class of spaces , generalising the cantor set which would seem ideal as a test-bed for the set of tools we are developing .

a unified approach to interpreting model predictions
understanding why a model makes a certain prediction can be as crucial as the prediction 's accuracy in many applications . however , the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret , such as ensemble or deep learning models , creating a tension between accuracy and interpretability . in response , various methods have recently been proposed to help users interpret the predictions of complex models , but it is often unclear how these methods are related and when one method is preferable over another . to address this problem , we present a unified framework for interpreting predictions , shap ( shapley additive explanations ) . shap assigns each feature an importance value for a particular prediction . its novel components include : ( 1 ) the identification of a new class of additive feature importance measures , and ( 2 ) theoretical results showing there is a unique solution in this class with a set of desirable properties . the new class unifies six existing methods , notable because several recent methods in the class lack the proposed desirable properties . based on insights from this unification , we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches .

a fuzzy relation-based extension of reggia 's relational model for diagnosis handling uncertain and incomplete information
relational models for diagnosis are based on a direct description of the association between disorders and manifestations . this type of model has been specially used and developed by reggia and his co-workers in the late eighties as a basic starting point for approaching diagnosis problems . the paper proposes a new relational model which includes reggia 's model as a particular case and which allows for a more expressive representation of the observations and of the manifestations associated with disorders . the model distinguishes , i ) between manifestations which are certainly absent and those which are not ( yet ) observed , and ii ) between manifestations which can not be caused by a given disorder and manifestations for which we do not know if they can or can not be caused by this disorder . this new model , which can handle uncertainty in a non-probabilistic way , is based on possibility theory and so-called twofold fuzzy sets , previously introduced by the authors .

automated coin recognition system using ann
coins are integral part of our day to day life . we use coins everywhere like grocery store , banks , buses , trains etc . so it becomes a basic need that coins can be sorted and counted automatically . for this it is necessary that coins can be recognized automatically . in this paper we have developed an ann ( artificial neural network ) based automated coin recognition system for the recognition of indian coins of denomination rs . 1 , 2 , 5 and 10 with rotation invariance . we have taken images from both sides of coin . so this system is capable of recognizing coins from both sides . features are extracted from images using techniques of hough transformation , pattern averaging etc . then , the extracted features are passed as input to a trained neural network . 97.74 % recognition rate has been achieved during the experiments i.e . only 2.26 % miss recognition , which is quite encouraging .

the traits of the personable
information personalization is fertile ground for application of ai techniques . in this article i relate personalization to the ability to capture partial information in an information-seeking interaction . the specific focus is on personalizing interactions at web sites . using ideas from partial evaluation and explanation-based generalization , i present a modeling methodology for reasoning about personalization . this approach helps identify seven tiers of ` personable traits ' in web sites .

making early predictions of the accuracy of machine learning applications
the accuracy of machine learning systems is a widely studied research topic . established techniques such as cross-validation predict the accuracy on unseen data of the classifier produced by applying a given learning method to a given training data set . however , they do not predict whether incurring the cost of obtaining more data and undergoing further training will lead to higher accuracy . in this paper we investigate techniques for making such early predictions . we note that when a machine learning algorithm is presented with a training set the classifier produced , and hence its error , will depend on the characteristics of the algorithm , on training set 's size , and also on its specific composition . in particular we hypothesise that if a number of classifiers are produced , and their observed error is decomposed into bias and variance terms , then although these components may behave differently , their behaviour may be predictable . we test our hypothesis by building models that , given a measurement taken from the classifier created from a limited number of samples , predict the values that would be measured from the classifier produced when the full data set is presented . we create separate models for bias , variance and total error . our models are built from the results of applying ten different machine learning algorithms to a range of data sets , and tested with `` unseen '' algorithms and datasets . we analyse the results for various numbers of initial training samples , and total dataset sizes . results show that our predictions are very highly correlated with the values observed after undertaking the extra training . finally we consider the more complex case where an ensemble of heterogeneous classifiers is trained , and show how we can accurately estimate an upper bound on the accuracy achievable after further training .

scaling up heuristic planning with relational decision trees
current evaluation functions for heuristic planning are expensive to compute . in numerous planning problems these functions provide good guidance to the solution , so they are worth the expense . however , when evaluation functions are misguiding or when planning problems are large enough , lots of node evaluations must be computed , which severely limits the scalability of heuristic planners . in this paper , we present a novel solution for reducing node evaluations in heuristic planning based on machine learning . particularly , we define the task of learning search control for heuristic planning as a relational classification task , and we use an off-the-shelf relational classification tool to address this learning task . our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain . these planning contexts are defined by the set of helpful actions of the current state , the goals remaining to be achieved , and the static predicates of the planning task . this paper shows two methods for guiding the search of a heuristic planner with the learned classifiers . the first one consists of using the resulting classifier as an action policy . the second one consists of applying the classifier to generate lookahead states within a best first search algorithm . experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners .

asp for minimal entailment in a rational extension of sroel
in this paper we exploit answer set programming ( asp ) for reasoning in a rational extension sroel-r-t of the low complexity description logic sroel , which underlies the owl el ontology language . in the extended language , a typicality operator t is allowed to define concepts t ( c ) ( typical c 's ) under a rational semantics . it has been proven that instance checking under rational entailment has a polynomial complexity . to strengthen rational entailment , in this paper we consider a minimal model semantics . we show that , for arbitrary sroel-r-t knowledge bases , instance checking under minimal entailment is \pi^p_2-complete . relying on a small model result , where models correspond to answer sets of a suitable asp encoding , we exploit answer set preferences ( and , in particular , the asprin framework ) for reasoning under minimal entailment . the paper is under consideration for acceptance in theory and practice of logic programming .

feature and variable selection in classification
the amount of information in the form of features and variables avail- able to machine learning algorithms is ever increasing . this can lead to classifiers that are prone to overfitting in high dimensions , high di- mensional models do not lend themselves to interpretable results , and the cpu and memory resources necessary to run on high-dimensional datasets severly limit the applications of the approaches . variable and feature selection aim to remedy this by finding a subset of features that in some way captures the information provided best . in this paper we present the general methodology and highlight some specific approaches .

lstmvis : a tool for visual analysis of hidden state dynamics in recurrent neural networks
recurrent neural networks , and in particular long short-term memory ( lstm ) networks , are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input . researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise . in this work , we present lstmvis , a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics . the tool allows users to select a hypothesis input range to focus on local state changes , to match these states changes to similar patterns in a large data set , and to align these results with structural annotations from their domain . we show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting , phrase structure , and chord progressions , and demonstrate how the tool can be used to isolate patterns for further statistical analysis . we characterize the domain , the different stakeholders , and their goals and tasks .

towards cooperation in sequential prisoner 's dilemmas : a deep multiagent reinforcement learning approach
the iterated prisoner 's dilemma has guided research on social dilemmas for decades . however , it distinguishes between only two atomic actions : cooperate and defect . in real-world prisoner 's dilemmas , these choices are temporally extended and different strategies may correspond to sequences of actions , reflecting grades of cooperation . we introduce a sequential prisoner 's dilemma ( spd ) game to better capture the aforementioned characteristics . in this work , we propose a deep multiagent reinforcement learning approach that investigates the evolution of mutual cooperation in spd games . our approach consists of two phases . the first phase is offline : it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network . the second phase is online : an agent adaptively selects its policy based on the detected degree of opponent cooperation . the effectiveness of our approach is demonstrated in two representative spd 2d games : the apple-pear game and the fruit gathering game . experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents .

related family-based attribute reduction of covering information systems when varying attribute sets
in practical situations , there are many dynamic covering information systems with variations of attributes , but there are few studies on related family-based attribute reduction of dynamic covering information systems . in this paper , we first investigate updated mechanisms of constructing attribute reducts for consistent and inconsistent covering information systems when varying attribute sets by using related families . then we employ examples to illustrate how to compute attribute reducts of dynamic covering information systems with variations of attribute sets . finally , the experimental results illustrates that the related family-based methods are effective to perform attribute reduction of dynamic covering information systems when attribute sets are varying with time .

restricted predicates for hypothetical datalog
hypothetical datalog is based on an intuitionistic semantics rather than on a classical logic semantics , and embedded implications are allowed in rule bodies . while the usual implication ( i.e. , the neck of a horn clause ) stands for inferring facts , an embedded implication plays the role of assuming its premise for deriving its consequence . a former work introduced both a formal framework and a goal-oriented tabled implementation , allowing negation in rule bodies . while in that work positive assumptions for both facts and rules can occur in the premise , negative assumptions are not allowed . in this work , we cover this subject by introducing a new concept : a restricted predicate , which allows negative assumptions by pruning the usual semantics of a predicate . this new setting has been implemented in the deductive system des .

abduction , asp and open logic programs
open logic programs and open entailment have been recently proposed as an abstract framework for the verification of incomplete specifications based upon normal logic programs and the stable model semantics . there are obvious analogies between open predicates and abducible predicates . however , despite superficial similarities , there are features of open programs that have no immediate counterpart in the framework of abduction and viceversa . similarly , open programs can not be immediately simulated with answer set programming ( asp ) . in this paper we start a thorough investigation of the relationships between open inference , abduction and asp . we shall prove that open programs generalize the other two frameworks . the generalized framework suggests interesting extensions of abduction under the generalized stable model semantics . in some cases , we will be able to reduce open inference to abduction and asp , thereby estimating its computational complexity . at the same time , the aforementioned reduction opens the way to new applications of abduction and asp .

extensible knowledge representation : the case of description reasoners
this paper offers an approach to extensible knowledge representation and reasoning for a family of formalisms known as description logics . the approach is based on the notion of adding new concept constructors , and includes a heuristic methodology for specifying the desired extensions , as well as a modularized software architecture that supports implementing extensions . the architecture detailed here falls in the normalize-compared paradigm , and supports both intentional reasoning ( subsumption ) involving concepts , and extensional reasoning involving individuals after incremental updates to the knowledge base . the resulting approach can be used to extend the reasoner with specialized notions that are motivated by specific problems or application areas , such as reasoning about dates , plans , etc . in addition , it provides an opportunity to implement constructors that are not currently yet sufficiently well understood theoretically , but are needed in practice . also , for constructors that are provably hard to reason with ( e.g. , ones whose presence would lead to undecidability ) , it allows the implementation of incomplete reasoners where the incompleteness is tailored to be acceptable for the application at hand .

measuring the directional distance between fuzzy sets
the measure of distance between two fuzzy sets is a fundamental tool within fuzzy set theory . however , current distance measures within the literature do not account for the direction of change between fuzzy sets ; a useful concept in a variety of applications , such as computing with words . in this paper , we highlight this utility and introduce a distance measure which takes the direction between sets into account . we provide details of its application for normal and non-normal , as well as convex and non-convex fuzzy sets . we demonstrate the new distance measure using real data from the movielens dataset and establish the benefits of measuring the direction between fuzzy sets .

planning in pomdps using multiplicity automata
planning and learning in partially observable mdps ( pomdps ) are among the most challenging tasks in both the ai and operation research communities . although solutions to these problems are intractable in general , there might be special cases , such as structured pomdps , which can be solved efficiently . a natural and possibly efficient way to represent a pomdp is through the predictive state representation ( psr ) - a representation which recently has been receiving increasing attention . in this work , we relate pomdps to multiplicity automata- showing that pomdps can be represented by multiplicity automata with no increase in the representation size . furthermore , we show that the size of the multiplicity automaton is equal to the rank of the predictive state representation . therefore , we relate both the predictive state representation and pomdps to the well-founded multiplicity automata literature . based on the multiplicity automata representation , we provide a planning algorithm which is exponential only in the multiplicity automata rank rather than the number of states of the pomdp . as a result , whenever the predictive state representation is logarithmic in the standard pomdp representation , our planning algorithm is efficient .

a brief survey of text mining : classification , clustering and extraction techniques
the amount of text that is generated every day is increasing dramatically . this tremendous volume of mostly unstructured text can not be simply processed and perceived by computers . therefore , efficient and effective techniques and algorithms are required to discover useful patterns . text mining is the task of extracting meaningful information from text , which has gained significant attentions in recent years . in this paper , we describe several of the most fundamental text mining tasks and techniques including text pre-processing , classification and clustering . additionally , we briefly explain text mining in biomedical and health care domains .

possibilistic logic bases and possibilistic graphs
possibilistic logic bases and possibilistic graphs are two different frameworks of interest for representing knowledge . the former stratifies the pieces of knowledge ( expressed by logical formulas ) according to their level of certainty , while the latter exhibits relationships between variables . the two types of representations are semantically equivalent when they lead to the same possibility distribution ( which rank-orders the possible interpretations ) . a possibility distribution can be decomposed using a chain rule which may be based on two different kinds of conditioning which exist in possibility theory ( one based on product in a numerical setting , one based on minimum operation in a qualitative setting ) . these two types of conditioning induce two kinds of possibilistic graphs . in both cases , a translation of these graphs into possibilistic bases is provided . the converse translation from a possibilistic knowledge base into a min-based graph is also described .

towards a common implementation of reinforcement learning for multiple robotic tasks
mobile robots are increasingly being employed for performing complex tasks in dynamic environments . reinforcement learning ( rl ) methods are recognized to be promising for specifying such tasks in a relatively simple manner . however , the strong dependency between the learning method and the task to learn is a well-known problem that restricts practical implementations of rl in robotics , often requiring major modifications of parameters and adding other techniques for each particular task . in this paper we present a practical core implementation of rl which enables the learning process for multiple robotic tasks with minimal per-task tuning or none . based on value iteration methods , this implementation includes a novel approach for action selection , called q-biased softmax regression ( qbiassr ) , which avoids poor performance of the learning process when the robot reaches new unexplored states . our approach takes advantage of the structure of the state space by attending the physical variables involved ( e.g. , distances to obstacles , x , y , { \theta } pose , etc . ) , thus experienced sets of states may favor the decision-making process of unexplored or rarely-explored states . this improvement has a relevant role in reducing the tuning of the algorithm for particular tasks . experiments with real and simulated robots , performed with the software framework also introduced here , show that our implementation is effectively able to learn different robotic tasks without tuning the learning method . results also suggest that the combination of true online sarsa ( { \lambda } ) with qbiassr can outperform the existing rl core algorithms in low-dimensional robotic tasks .

improving scalability of inductive logic programming via pruning and best-effort optimisation
inductive logic programming ( ilp ) combines rule-based and statistical artificial intelligence methods , by learning a hypothesis comprising a set of rules given background knowledge and constraints for the search space . we focus on extending the xhail algorithm for ilp which is based on answer set programming and we evaluate our extensions using the natural language processing application of sentence chunking . with respect to processing natural language , ilp can cater for the constant change in how we use language on a daily basis . at the same time , ilp does not require huge amounts of training examples such as other statistical methods and produces interpretable results , that means a set of rules , which can be analysed and tweaked if necessary . as contributions we extend xhail with ( i ) a pruning mechanism within the hypothesis generalisation algorithm which enables learning from larger datasets , ( ii ) a better usage of modern solver technology using recently developed optimisation methods , and ( iii ) a time budget that permits the usage of suboptimal results . we evaluate these improvements on the task of sentence chunking using three datasets from a recent semeval competition . results show that our improvements allow for learning on bigger datasets with results that are of similar quality to state-of-the-art systems on the same task . moreover , we compare the hypotheses obtained on datasets to gain insights on the structure of each dataset .

an investigation report on auction mechanism design
auctions are markets with strict regulations governing the information available to traders in the market and the possible actions they can take . since well designed auctions achieve desirable economic outcomes , they have been widely used in solving real-world optimization problems , and in structuring stock or futures exchanges . auctions also provide a very valuable testing-ground for economic theory , and they play an important role in computer-based control systems . auction mechanism design aims to manipulate the rules of an auction in order to achieve specific goals . economists traditionally use mathematical methods , mainly game theory , to analyze auctions and design new auction forms . however , due to the high complexity of auctions , the mathematical models are typically simplified to obtain results , and this makes it difficult to apply results derived from such models to market environments in the real world . as a result , researchers are turning to empirical approaches . this report aims to survey the theoretical and empirical approaches to designing auction mechanisms and trading strategies with more weights on empirical ones , and build the foundation for further research in the field .

learning to navigate in complex environments
learning to navigate in complex environments with dynamic elements is an important milestone in developing ai agents . in this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs . in particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks . this approach can learn to navigate from raw sensory input in complicated 3d mazes , approaching human-level performance even under conditions where the goal location changes frequently . we provide detailed analysis of the agent behaviour , its ability to localise , and its network activity dynamics , showing that the agent implicitly learns key navigation abilities .

certainty closure : reliable constraint reasoning with incomplete or erroneous data
constraint programming ( cp ) has proved an effective paradigm to model and solve difficult combinatorial satisfaction and optimisation problems from disparate domains . many such problems arising from the commercial world are permeated by data uncertainty . existing cp approaches that accommodate uncertainty are less suited to uncertainty arising due to incomplete and erroneous data , because they do not build reliable models and solutions guaranteed to address the user 's genuine problem as she perceives it . other fields such as reliable computation offer combinations of models and associated methods to handle these types of uncertain data , but lack an expressive framework characterising the resolution methodology independently of the model . we present a unifying framework that extends the cp formalism in both model and solutions , to tackle ill-defined combinatorial problems with incomplete or erroneous data . the certainty closure framework brings together modelling and solving methodologies from different fields into the cp paradigm to provide reliable and efficient approches for uncertain constraint problems . we demonstrate the applicability of the framework on a case study in network diagnosis . we define resolution forms that give generic templates , and their associated operational semantics , to derive practical solution methods for reliable solutions .

towards well-specified semi-supervised model-based classifiers via structural adaptation
semi-supervised learning plays an important role in large-scale machine learning . properly using additional unlabeled data ( largely available nowadays ) often can improve the machine learning accuracy . however , if the machine learning model is misspecified for the underlying true data distribution , the model performance could be seriously jeopardized . this issue is known as model misspecification . to address this issue , we focus on generative models and propose a criterion to detect the onset of model misspecification by measuring the performance difference between models obtained using supervised and semi-supervised learning . then , we propose to automatically modify the generative models during model training to achieve an unbiased generative model . rigorous experiments were carried out to evaluate the proposed method using two image classification data sets pascal voc'07 and mir flickr . our proposed method has been demonstrated to outperform a number of state-of-the-art semi-supervised learning approaches for the classification task .

predictive state temporal difference learning
we propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification . in practical applications , reinforcement learning ( rl ) is complicated by the fact that state is either high-dimensional or partially observable . therefore , rl methods are designed to work with features of state rather than state itself , and the success or failure of learning is often determined by the suitability of the selected features . by comparison , subspace identification ( ssid ) methods are designed to select a feature set which preserves as much information as possible about state . in this paper we connect the two approaches , looking at the problem of reinforcement learning with a large set of features , each of which may only be marginally useful for value function approximation . we introduce a new algorithm for this situation , called predictive state temporal difference ( pstd ) learning . as in ssid for predictive state representations , pstd finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information . as in rl , pstd then uses a bellman recursion to estimate a value function . we discuss the connection between pstd and prior approaches in rl and ssid . we prove that pstd is statistically consistent , perform several experiments that illustrate its properties , and demonstrate its potential on a difficult optimal stopping problem .

characterizing optimal hierarchical policy inference on graphs via non-equilibrium thermodynamics
hierarchies are of fundamental interest in both stochastic optimal control and biological control due to their facilitation of a range of desirable computational traits in a control algorithm and the possibility that they may form a core principle of sensorimotor and cognitive control systems . however , a theoretically justified construction of state-space hierarchies over all spatial resolutions and their evolution through a policy inference process remains elusive . here , a formalism for deriving such normative representations of discrete markov decision processes is introduced in the context of graphs . the resulting hierarchies correspond to a hierarchical policy inference algorithm approximating a discrete gradient flow between state-space trajectory densities generated by the prior and optimal policies .

on the scope of the universal-algebraic approach to constraint satisfaction
the universal-algebraic approach has proved a powerful tool in the study of the complexity of csps . this approach has previously been applied to the study of csps with finite or ( infinite ) omega-categorical templates , and relies on two facts . the first is that in finite or omega-categorical structures a , a relation is primitive positive definable if and only if it is preserved by the polymorphisms of a. the second is that every finite or omega-categorical structure is homomorphically equivalent to a core structure . in this paper , we present generalizations of these facts to infinite structures that are not necessarily omega-categorical . ( this abstract has been severely curtailed by the space constraints of arxiv -- please read the full abstract in the article . ) finally , we present applications of our general results to the description and analysis of the complexity of csps . in particular , we give general hardness criteria based on the absence of polymorphisms that depend on more than one argument , and we present a polymorphism-based description of those csps that are first-order definable ( and therefore can be solved in polynomial time ) .

explicit approximations of the gaussian kernel
we investigate training and using gaussian kernel svms by approximating the kernel with an explicit finite- dimensional polynomial feature representation based on the taylor expansion of the exponential . although not as efficient as the recently-proposed random fourier features [ rahimi and recht , 2007 ] in terms of the number of features , we show how this polynomial representation can provide a better approximation in terms of the computational cost involved . this makes our `` taylor features '' especially attractive for use on very large data sets , in conjunction with online or stochastic training .

generating machine-executable plans from end-user 's natural-language instructions
it is critical for advanced manufacturing machines to autonomously execute a task by following an end-user 's natural language ( nl ) instructions . however , nl instructions are usually ambiguous and abstract so that the machines may misunderstand and incorrectly execute the task . to address this nl-based human-machine communication problem and enable the machines to appropriately execute tasks by following the end-user 's nl instructions , we developed a machine-executable-plan-generation ( exeplan ) method . the exeplan method conducts task-centered semantic analysis to extract task-related information from ambiguous nl instructions . in addition , the method specifies machine execution parameters to generate a machine-executable plan by interpreting abstract nl instructions . to evaluate the exeplan method , an industrial robot baxter was instructed by nl to perform three types of industrial tasks { 'drill a hole ' , 'clean a spot ' , 'install a screw ' } . the experiment results proved that the exeplan method was effective in generating machine-executable plans from the end-user 's nl instructions . such a method has the promise to endow a machine with the ability of nl-instructed task execution .

indian regional movie dataset for recommender systems
indian regional movie dataset is the first database of regional indian movies , users and their ratings . it consists of movies belonging to 18 different indian regional languages and metadata of users with varying demographics . through this dataset , the diversity of indian regional cinema and its huge viewership is captured . we analyze the dataset that contains roughly 10k ratings of 919 users and 2,851 movies using some supervised and unsupervised collaborative filtering techniques like probabilistic matrix factorization , matrix completion , blind compressed sensing etc . the dataset consists of metadata information of users like age , occupation , home state and known languages . it also consists of metadata of movies like genre , language , release year and cast . india has a wide base of viewers which is evident by the large number of movies released every year and the huge box-office revenue . this dataset can be used for designing recommendation systems for indian users and regional movies , which do not , yet , exist . the dataset can be downloaded from \href { https : //goo.gl/emtpv6 } { https : //goo.gl/emtpv6 } .

vanquishing the xcb question : the methodology discovery of the last shortest single axiom for the equivalential calculus
with the inclusion of an effective methodology , this article answers in detail a question that , for a quarter of a century , remained open despite intense study by various researchers . is the formula xcb = e ( x , e ( e ( e ( x , y ) , e ( z , y ) ) , z ) ) a single axiom for the classical equivalential calculus when the rules of inference consist of detachment ( modus ponens ) and substitution ? where the function e represents equivalence , this calculus can be axiomatized quite naturally with the formulas e ( x , x ) , e ( e ( x , y ) , e ( y , x ) ) , and e ( e ( x , y ) , e ( e ( y , z ) , e ( x , z ) ) ) , which correspond to reflexivity , symmetry , and transitivity , respectively . ( we note that e ( x , x ) is dependent on the other two axioms . ) heretofore , thirteen shortest single axioms for classical equivalence of length eleven had been discovered , and xcb was the only remaining formula of that length whose status was undetermined . to show that xcb is indeed such a single axiom , we focus on the rule of condensed detachment , a rule that captures detachment together with an appropriately general , but restricted , form of substitution . the proof we present in this paper consists of twenty-five applications of condensed detachment , completing with the deduction of transitivity followed by a deduction of symmetry . we also discuss some factors that may explain in part why xcb resisted relinquishing its treasure for so long . our approach relied on diverse strategies applied by the automated reasoning program otter . thus ends the search for shortest single axioms for the equivalential calculus .

low-rank matrix factorization with attributes
we develop a new collaborative filtering ( cf ) method that combines both previously known users ' preferences , i.e . standard cf , as well as product/user attributes , i.e . classical function approximation , to predict a given user 's interest in a particular product . our method is a generalized low rank matrix completion problem , where we learn a function whose inputs are pairs of vectors -- the standard low rank matrix completion problem being a special case where the inputs to the function are the row and column indices of the matrix . we solve this generalized matrix completion problem using tensor product kernels for which we also formally generalize standard kernel properties . benchmark experiments on movie ratings show the advantages of our generalized matrix completion method over the standard matrix completion one with no information about movies or people , as well as over standard multi-task or single task learning methods .

similarity measures on preference structures , part ii : utility functions
in previous work cite { ha98 : towards } we presented a case-based approach to eliciting and reasoning with preferences . a key issue in this approach is the definition of similarity between user preferences . we introduced the probabilistic distance as a measure of similarity on user preferences , and provided an algorithm to compute the distance between two partially specified { em value } functions . this is for the case of decision making under { em certainty } . in this paper we address the more challenging issue of computing the probabilistic distance in the case of decision making under { em uncertainty } . we provide an algorithm to compute the probabilistic distance between two partially specified { em utility } functions . we demonstrate the use of this algorithm with a medical data set of partially specified patient preferences , where none of the other existing distancemeasures appear definable . using this data set , we also demonstrate that the case-based approach to preference elicitation isapplicable in domains with uncertainty . finally , we provide a comprehensive analytical comparison of the probabilistic distance with some existing distance measures on preferences .

verifying termination of general logic programs with concrete queries
we introduce a method of verifying termination of logic programs with respect to concrete queries ( instead of abstract query patterns ) . a necessary and sufficient condition is established and an algorithm for automatic verification is developed . in contrast to existing query pattern-based approaches , our method has the following features : ( 1 ) it applies to all general logic programs with non-floundering queries . ( 2 ) it is very easy to automate because it does not need to search for a level mapping or a model , nor does it need to compute an interargument relation based on additional mode or type information . ( 3 ) it bridges termination analysis with loop checking , the two problems that have been studied separately in the past despite their close technical relation with each other .

theory of semi-instantiation in abstract argumentation
we study instantiated abstract argumentation frames of the form $ ( s , r , i ) $ , where $ ( s , r ) $ is an abstract argumentation frame and where the arguments $ x $ of $ s $ are instantiated by $ i ( x ) $ as well formed formulas of a well known logic , for example as boolean formulas or as predicate logic formulas or as modal logic formulas . we use the method of conceptual analysis to derive the properties of our proposed system . we seek to define the notion of complete extensions for such systems and provide algorithms for finding such extensions . we further develop a theory of instantiation in the abstract , using the framework of boolean attack formations and of conjunctive and disjunctive attacks . we discuss applications and compare critically with the existing related literature .

beyond owl 2 ql in obda : rewritings and approximations ( extended version )
ontology-based data access ( obda ) is a novel paradigm facilitating access to relational data , realized by linking data sources to an ontology by means of declarative mappings . dl-lite_r , which is the logic underpinning the w3c ontology language owl 2 ql and the current language of choice for obda , has been designed with the goal of delegating query answering to the underlying database engine , and thus is restricted in expressive power . e.g. , it does not allow one to express disjunctive information , and any form of recursion on the data . the aim of this paper is to overcome these limitations of dl-lite_r , and extend obda to more expressive ontology languages , while still leveraging the underlying relational technology for query answering . we achieve this by relying on two well-known mechanisms , namely conservative rewriting and approximation , but significantly extend their practical impact by bringing into the picture the mapping , an essential component of obda . specifically , we develop techniques to rewrite obda specifications with an expressive ontology to `` equivalent '' ones with a dl-lite_r ontology , if possible , and to approximate them otherwise . we do so by exploiting the high expressive power of the mapping layer to capture part of the domain semantics of rich ontology languages . we have implemented our techniques in the prototype system ontoprox , making use of the state-of-the-art obda system ontop and the query answering system clipper , and we have shown their feasibility and effectiveness with experiments on synthetic and real-world data .

advantages and limitations of using successor features for transfer in reinforcement learning
one question central to reinforcement learning is how to learn a feature representation that supports algorithm scaling and re-use of learned information from different tasks . successor features approach this problem by learning a feature representation that satisfies a temporal constraint . we present an implementation of an approach that decouples the feature representation from the reward function , making it suitable for transferring knowledge between domains . we then assess the advantages and limitations of using successor features for transfer .

a rule-based short query intent identification system
using sms ( short message system ) , cell phones can be used to query for information about various topics . in an sms based search system , one of the key problems is to identify a domain ( broad topic ) associated with the user query ; so that a more comprehensive search can be carried out by the domain specific search engine . in this paper we use a rule based approach , to identify the domain , called short query intent identification system ( sqiis ) . we construct two different rule-bases using different strategies to suit query intent identification . we evaluate the two rule-bases experimentally .

normalization and the representation of nonmonotonic knowledge in the theory of evidence
we discuss the dempster-shafer theory of evidence . we introduce a concept of monotonicity which is related to the diminution of the range between belief and plausibility . we show that the accumulation of knowledge in this framework exhibits a nonmonotonic property . we show how the belief structure can be used to represent typical or commonsense knowledge .

incremental learning of event definitions with inductive logic programming
event recognition systems rely on properly engineered knowledge bases of event definitions to infer occurrences of events in time . the manual development of such knowledge is a tedious and error-prone task , thus event-based applications may benefit from automated knowledge construction techniques , such as inductive logic programming ( ilp ) , which combines machine learning with the declarative and formal semantics of first-order logic . however , learning temporal logical formalisms , which are typically utilized by logic-based event recognition systems is a challenging task , which most ilp systems can not fully undertake . in addition , event-based data is usually massive and collected at different times and under various circumstances . ideally , systems that learn from temporal data should be able to operate in an incremental mode , that is , revise prior constructed knowledge in the face of new evidence . most ilp systems are batch learners , in the sense that in order to account for new evidence they have no alternative but to forget past knowledge and learn from scratch . given the increased inherent complexity of ilp and the volumes of real-life temporal data , this results to algorithms that scale poorly . in this work we present an incremental method for learning and revising event-based knowledge , in the form of event calculus programs . the proposed algorithm relies on abductive-inductive learning and comprises a scalable clause refinement methodology , based on a compressive summarization of clause coverage in a stream of examples . we present an empirical evaluation of our approach on real and synthetic data from activity recognition and city transport applications .

learning to predict combinatorial structures
the major challenge in designing a discriminative learning algorithm for predicting structured data is to address the computational issues arising from the exponential size of the output space . existing algorithms make different assumptions to ensure efficient , polynomial time estimation of model parameters . for several combinatorial structures , including cycles , partially ordered sets , permutations and other graph classes , these assumptions do not hold . in this thesis , we address the problem of designing learning algorithms for predicting combinatorial structures by introducing two new assumptions : ( i ) the first assumption is that a particular counting problem can be solved efficiently . the consequence is a generalisation of the classical ridge regression for structured prediction . ( ii ) the second assumption is that a particular sampling problem can be solved efficiently . the consequence is a new technique for designing and analysing probabilistic structured prediction models . these results can be applied to solve several complex learning problems including but not limited to multi-label classification , multi-category hierarchical classification , and label ranking .

the ubuntu dialogue corpus : a large dataset for research in unstructured multi-turn dialogue systems
this paper introduces the ubuntu dialogue corpus , a dataset containing almost 1 million multi-turn dialogues , with a total of over 7 million utterances and 100 million words . this provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data . the dataset has both the multi-turn property of conversations in the dialog state tracking challenge datasets , and the unstructured nature of interactions from microblog services such as twitter . we also describe two neural learning architectures suitable for analyzing this dataset , and provide benchmark performance on the task of selecting the best next response .

generalizing boolean satisfiability i : background and survey of existing work
this is the first of three planned papers describing zap , a satisfiability engine that substantially generalizes existing tools while retaining the performance characteristics of modern high-performance solvers . the fundamental idea underlying zap is that many problems passed to such engines contain rich internal structure that is obscured by the boolean representation used ; our goal is to define a representation in which this structure is apparent and can easily be exploited to improve computational performance . this paper is a survey of the work underlying zap , and discusses previous attempts to improve the performance of the davis-putnam-logemann-loveland algorithm by exploiting the structure of the problem being solved . we examine existing ideas including extensions of the boolean language to allow cardinality constraints , pseudo-boolean representations , symmetry , and a limited form of quantification . while this paper is intended as a survey , our research results are contained in the two subsequent articles , with the theoretical structure of zap described in the second paper in this series , and zap 's implementation described in the third .

can we learn to beat the best stock
a novel algorithm for actively trading stocks is presented . while traditional expert advice and `` universal '' algorithms ( as well as standard technical trading heuristics ) attempt to predict winners or trends , our approach relies on predictable statistical relations between all pairs of stocks in the market . our empirical results on historical markets provide strong evidence that this type of technical trading can `` beat the market '' and moreover , can beat the best stock in the market . in doing so we utilize a new idea for smoothing critical parameters in the context of expert learning .

reflective oracles : a foundation for classical game theory
classical game theory treats players as special -- -a description of a game contains a full , explicit enumeration of all players -- -even though in the real world , `` players '' are no more fundamentally special than rocks or clouds . it is n't trivial to find a decision-theoretic foundation for game theory in which an agent 's coplayers are a non-distinguished part of the agent 's environment . attempts to model both players and the environment as turing machines , for example , fail for standard diagonalization reasons . in this paper , we introduce a `` reflective '' type of oracle , which is able to answer questions about the outputs of oracle machines with access to the same oracle . these oracles avoid diagonalization by answering some queries randomly . we show that machines with access to a reflective oracle can be used to define rational agents using causal decision theory . these agents model their environment as a probabilistic oracle machine , which may contain other agents as a non-distinguished part . we show that if such agents interact , they will play a nash equilibrium , with the randomization in mixed strategies coming from the randomization in the oracle 's answers . this can be seen as providing a foundation for classical game theory in which players are n't special .

inferring knowledge from a large semantic network
in this paper , we present a rich semantic network based on a differential analysis . we then detail implemented measures that take into account common and differential features between words . in a last section , we describe some industrial applications .

learning dtw global constraint for time series classification
1-nearest neighbor with the dynamic time warping ( dtw ) distance is one of the most effective classifiers on time series domain . since the global constraint has been introduced in speech community , many global constraint models have been proposed including sakoe-chiba ( s-c ) band , itakura parallelogram , and ratanamahatana-keogh ( r-k ) band . the r-k band is a general global constraint model that can represent any global constraints with arbitrary shape and size effectively . however , we need a good learning algorithm to discover the most suitable set of r-k bands , and the current r-k band learning algorithm still suffers from an 'overfitting ' phenomenon . in this paper , we propose two new learning algorithms , i.e. , band boundary extraction algorithm and iterative learning algorithm . the band boundary extraction is calculated from the bound of all possible warping paths in each class , and the iterative learning is adjusted from the original r-k band learning . we also use a silhouette index , a well-known clustering validation technique , as a heuristic function , and the lower bound function , lb_keogh , to enhance the prediction speed . twenty datasets , from the workshop and challenge on time series classification , held in conjunction of the sigkdd 2007 , are used to evaluate our approach .

introducing memory and association mechanism into a biologically inspired visual model
a famous biologically inspired hierarchical model firstly proposed by riesenhuber and poggio has been successfully applied to multiple visual recognition tasks . the model is able to achieve a set of position- and scale-tolerant recognition , which is a central problem in pattern recognition . in this paper , based on some other biological experimental results , we introduce the memory and association mechanisms into the above biologically inspired model . the main motivations of the work are ( a ) to mimic the active memory and association mechanism and add the 'top down ' adjustment to the above biologically inspired hierarchical model and ( b ) to build up an algorithm which can save the space and keep a good recognition performance . the new model is also applied to object recognition processes . the primary experimental results show that our method is efficient with much less memory requirement .

multi-pointer co-attention networks for recommendation
many recent state-of-the-art recommender systems such as d-att , transnet and deepconn exploit reviews for representation learning . this paper proposes a new neural architecture for recommendation with reviews . our model operates on a multi-hierarchical paradigm and is based on the intuition that not all reviews are created equal , i.e. , only a select few are important . the importance , however , should be dynamically inferred depending on the current target . to this end , we propose a review-by-review pointer-based learning scheme that extracts important reviews , subsequently matching them in a word-by-word fashion . this enables not only the most informative reviews to be utilized for prediction but also a deeper word-level interaction . our pointer-based method operates with a novel gumbel-softmax based pointer mechanism that enables the incorporation of discrete vectors within differentiable neural architectures . our pointer mechanism is co-attentive in nature , learning pointers which are co-dependent on user-item relationships . finally , we propose a multi-pointer learning scheme that learns to combine multiple views of interactions between user and item . overall , we demonstrate the effectiveness of our proposed model via extensive experiments on \textbf { 24 } benchmark datasets from amazon and yelp . empirical results show that our approach significantly outperforms existing state-of-the-art , with up to 19 % and 71 % relative improvement when compared to transnet and deepconn respectively . we study the behavior of our multi-pointer learning mechanism , shedding light on evidence aggregation patterns in review-based recommender systems .

investigating human priors for playing video games
what makes humans so good at solving seemingly complex video games ? unlike computers , humans bring in a great deal of prior knowledge about the world , enabling efficient decision making . this paper investigates the role of human priors for solving video games . given a sample game , we conduct a series of ablation studies to quantify the importance of various priors on human performance . we do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors . we find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game , e.g . from 2 minutes to over 20 minutes . furthermore , our results indicate that general priors , such as the importance of objects and visual consistency , are critical for efficient game-play . videos and the game manipulations are available at https : //rach0012.github.io/humanrl_website/

a multi-stage probabilistic algorithm for dynamic path-planning
probabilistic sampling methods have become very popular to solve single-shot path planning problems . rapidly-exploring random trees ( rrts ) in particular have been shown to be efficient in solving high dimensional problems . even though several rrt variants have been proposed for dynamic replanning , these methods only perform well in environments with infrequent changes . this paper addresses the dynamic path planning problem by combining simple techniques in a multi-stage probabilistic algorithm . this algorithm uses rrts for initial planning and informed local search for navigation . we show that this combination of simple techniques provides better responses to highly dynamic environments than the rrt extensions .

tensor completion algorithms in big data analytics
tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors . due to the multidimensional character of tensors in describing complex datasets , tensor completion algorithms and their applications have received wide attention and achievement in data mining , computer vision , signal processing , and neuroscience , etc . in this survey , we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety , large volume , and high velocity . towards a better comprehension and comparison of vast existing advances , we summarize and categorize them into four groups including general tensor completion algorithms , tensor completion with auxiliary information ( variety ) , scalable tensor completion algorithms ( volume ) and dynamic tensor completion algorithms ( velocity ) . besides , we introduce their applications on real-world data-driven problems and present an open-source package covering several widely used tensor decomposition and completion algorithms . our goal is to summarize these popular methods and introduce them to researchers for promoting the research process in this field and give an available repository for practitioners . in the end , we also discuss some challenges and promising research directions in this community for future explorations .

iterative learning of answer set programs from context dependent examples
in recent years , several frameworks and systems have been proposed that extend inductive logic programming ( ilp ) to the answer set programming ( asp ) paradigm . in ilp , examples must all be explained by a hypothesis together with a given background knowledge . in existing systems , the background knowledge is the same for all examples ; however , examples may be context-dependent . this means that some examples should be explained in the context of some information , whereas others should be explained in different contexts . in this paper , we capture this notion and present a context-dependent extension of the learning from ordered answer sets framework . in this extension , contexts can be used to further structure the background knowledge . we then propose a new iterative algorithm , ilasp2i , which exploits this feature to scale up the existing ilasp2 system to learning tasks with large numbers of examples . we demonstrate the gain in scalability by applying both algorithms to various learning tasks . our results show that , compared to ilasp2 , the newly proposed ilasp2i system can be two orders of magnitude faster and use two orders of magnitude less memory , whilst preserving the same average accuracy . this paper is under consideration for acceptance in tplp .

traversing knowledge graph in vector space without symbolic space guidance
recent studies on knowledge base completion , the task of recovering missing facts based on observed facts , demonstrate the importance of learning embeddings from multi-step relations . due to the size of knowledge bases , previous works manually design relation paths of observed triplets in symbolic space ( e.g . random walk ) to learn multi-step relations during training . however , these approaches suffer some limitations as most paths are not informative , and it is prohibitively expensive to consider all possible paths . to address the limitations , we propose learning to traverse in vector space directly without the need of symbolic space guidance . to remember the connections between related observed triplets and be able to adaptively change relation paths in vector space , we propose implicit reasonets ( irns ) , that is composed of a global memory and a controller module to learn multi-step relation paths in vector space and infer missing facts jointly without any human-designed procedure . without using any axillary information , our proposed model achieves state-of-the-art results on popular knowledge base completion benchmarks .

the dialog state tracking challenge with bayesian approach
generative model has been one of the most common approaches for solving the dialog state tracking problem with the capabilities to model the dialog hypotheses in an explicit manner . the most important task in such bayesian networks models is constructing the most reliable user models by learning and reflecting the training data into the probability distribution of user actions conditional on networks states . this paper provides an overall picture of the learning process in a bayesian framework with an emphasize on the state-of-the-art theoretical analyses of the expectation maximization learning algorithm .

on compiling dnnfs without determinism
state-of-the-art knowledge compilers generate deterministic subsets of dnnf , which have been recently shown to be exponentially less succinct than dnnf . in this paper , we propose a new method to compile dnnfs without enforcing determinism necessarily . our approach is based on compiling deterministic dnnfs with the addition of auxiliary variables to the input formula . these variables are then existentially quantified from the deterministic structure in linear time , which would lead to a dnnf that is equivalent to the input formula and not necessarily deterministic . on the theoretical side , we show that the new method could generate exponentially smaller dnnfs than deterministic ones , even by adding a single auxiliary variable . further , we show that various existing techniques that introduce auxiliary variables to the input formulas can be employed in our framework . on the practical side , we empirically demonstrate that our new method can significantly advance dnnf compilation on certain benchmarks .

certified connection tableaux proofs for hol light and tptp
in the recent years , the metis prover based on ordered paramodulation and model elimination has replaced the earlier built-in methods for general-purpose proof automation in hol4 and isabelle/hol . in the annual casc competition , the leancop system based on connection tableaux has however performed better than metis . in this paper we show how the leancop 's core algorithm can be implemented inside hollight . leancop 's flagship feature , namely its minimalistic core , results in a very simple proof system . this plays a crucial role in extending the meson proof reconstruction mechanism to connection tableaux proofs , providing an implementation of leancop that certifies its proofs . we discuss the differences between our direct implementation using an explicit prolog stack , to the continuation passing implementation of meson present in hollight and compare their performance on all core hollight goals . the resulting prover can be also used as a general purpose tptp prover . we compare its performance against the resolution based metis on tptp and other interesting datasets .

on validating boolean optimizers
boolean optimization finds a wide range of application domains , that motivated a number of different organizations of boolean optimizers since the mid 90s . some of the most successful approaches are based on iterative calls to an np oracle , using either linear search , binary search or the identification of unsatisfiable sub-formulas . the increasing use of boolean optimizers in practical settings raises the question of confidence in computed results . for example , the issue of confidence is paramount in safety critical settings . one way of increasing the confidence of the results computed by boolean optimizers is to develop techniques for validating the results . recent work studied the validation of boolean optimizers based on branch-and-bound search . this paper complements existing work , and develops methods for validating boolean optimizers that are based on iterative calls to an np oracle . this entails implementing solutions for validating both satisfiable and unsatisfiable answers from the np oracle . the work described in this paper can be applied to a wide range of boolean optimizers , that find application in pseudo-boolean optimization and in maximum satisfiability . preliminary experimental results indicate that the impact of the proposed method in overall performance is negligible .

multiple-path selection for new highway alignments using discrete algorithms
this paper addresses the problem of finding multiple near-optimal , spatially-dissimilar paths that can be considered as alternatives in the decision making process , for finding optimal corridors in which to construct a new road . we further consider combinations of techniques for reducing the costs associated with the computation and increasing the accuracy of the cost formulation . numerical results for five algorithms to solve the dissimilar multipath problem show that a `` bidirectional approach '' yields the fastest running times and the most robust algorithm . further modifications of the algorithms to reduce the running time were tested and it is shown that running time can be reduced by an average of 56 percent without compromising the quality of the results .

fixed-parameter complexity of semantics for logic programs
a decision problem is called parameterized if its input is a pair of strings . one of these strings is referred to as a parameter . the problem : given a propositional logic program p and a non-negative integer k , decide whether p has a stable model of size no more than k , is an example of a parameterized decision problem with k serving as a parameter . parameterized problems that are np-complete often become solvable in polynomial time if the parameter is fixed . the problem to decide whether a program p has a stable model of size no more than k , where k is fixed and not a part of input , can be solved in time o ( mn^k ) , where m is the size of p and n is the number of atoms in p. thus , this problem is in the class p. however , algorithms with the running time given by a polynomial of order k are not satisfactory even for relatively small values of k. the key question then is whether significantly better algorithms ( with the degree of the polynomial not dependent on k ) exist . to tackle it , we use the framework of fixed-parameter complexity . we establish the fixed-parameter complexity for several parameterized decision problems involving models , supported models and stable models of logic programs . we also establish the fixed-parameter complexity for variants of these problems resulting from restricting attention to horn programs and to purely negative programs . most of the problems considered in the paper have high fixed-parameter complexity . thus , it is unlikely that fixing bounds on models ( supported models , stable models ) will lead to fast algorithms to decide the existence of such models .

optimizing selective search in chess
in this paper we introduce a novel method for automatically tuning the search parameters of a chess program using genetic algorithms . our results show that a large set of parameter values can be learned automatically , such that the resulting performance is comparable with that of manually tuned parameters of top tournament-playing chess programs .

end-to-end differentiable proving
we introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols . these neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in prolog . specifically , we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel , thereby combining symbolic reasoning with learning subsymbolic vector representations . by using gradient descent , the resulting neural network can be trained to infer facts from a given incomplete knowledge base . it learns to ( i ) place representations of similar symbols in close proximity in a vector space , ( ii ) make use of such similarities to prove queries , ( iii ) induce logical rules , and ( iv ) use provided and induced logical rules for multi-hop reasoning . we demonstrate that this architecture outperforms complex , a state-of-the-art neural link prediction model , on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules .

pruning isomorphic structural sub-problems in configuration
configuring consists in simulating the realization of a complex product from a catalog of component parts , using known relations between types , and picking values for object attributes . this highly combinatorial problem in the field of constraint programming has been addressed with a variety of approaches since the foundation system r1 ( mcdermott82 ) . an inherent difficulty in solving configuration problems is the existence of many isomorphisms among interpretations . we describe a formalism independent approach to improve the detection of isomorphisms by configurators , which does not require to adapt the problem model . to achieve this , we exploit the properties of a characteristic subset of configuration problems , called the structural sub-problem , which canonical solutions can be produced or tested at a limited cost . in this paper we present an algorithm for testing the canonicity of configurations , that can be added as a symmetry breaking constraint to any configurator . the cost and efficiency of this canonicity test are given .

algorithms for multi-armed bandit problems
although many algorithms for the multi-armed bandit problem are well-understood theoretically , empirical confirmation of their effectiveness is generally scarce . this paper presents a thorough empirical study of the most popular multi-armed bandit algorithms . three important observations can be made from our results . firstly , simple heuristics such as epsilon-greedy and boltzmann exploration outperform theoretically sound algorithms on most settings by a significant margin . secondly , the performance of most algorithms varies dramatically with the parameters of the bandit problem . our study identifies for each algorithm the settings where it performs well , and the settings where it performs poorly . thirdly , the algorithms ' performance relative each to other is affected only by the number of bandit arms and the variance of the rewards . this finding may guide the design of subsequent empirical evaluations . in the second part of the paper , we turn our attention to an important area of application of bandit algorithms : clinical trials . although the design of clinical trials has been one of the principal practical problems motivating research on multi-armed bandits , bandit algorithms have never been evaluated as potential treatment allocation strategies . using data from a real study , we simulate the outcome that a 2001-2002 clinical trial would have had if bandit algorithms had been used to allocate patients to treatments . we find that an adaptive trial would have successfully treated at least 50 % more patients , while significantly reducing the number of adverse effects and increasing patient retention . at the end of the trial , the best treatment could have still been identified with a high level of statistical confidence . our findings demonstrate that bandit algorithms are attractive alternatives to current adaptive treatment allocation strategies .

dilated fcn for multi-agent 2d/3d medical image registration
2d/3d image registration to align a 3d volume and 2d x-ray images is a challenging problem due to its ill-posed nature and various artifacts presented in 2d x-ray images . in this paper , we propose a multi-agent system with an auto attention mechanism for robust and efficient 2d/3d image registration . specifically , an individual agent is trained with dilated fully convolutional network ( fcn ) to perform registration in a markov decision process ( mdp ) by observing a local region , and the final action is then taken based on the proposals from multiple agents and weighted by their corresponding confidence levels . the contributions of this paper are threefold . first , we formulate 2d/3d registration as a mdp with observations , actions , and rewards properly defined with respect to x-ray imaging systems . second , to handle various artifacts in 2d x-ray images , multiple local agents are employed efficiently via fcn-based structures , and an auto attention mechanism is proposed to favor the proposals from regions with more reliable visual cues . third , a dilated fcn-based training mechanism is proposed to significantly reduce the degree of freedom in the simulation of registration environment , and drastically improve training efficiency by an order of magnitude compared to standard cnn-based training method . we demonstrate that the proposed method achieves high robustness on both spine cone beam computed tomography data with a low signal-to-noise ratio and data from minimally invasive spine surgery where severe image artifacts and occlusions are presented due to metal screws and guide wires , outperforming other state-of-the-art methods ( single agent-based and optimization-based ) by a large margin .

collective robot reinforcement learning with distributed asynchronous guided policy search
in principle , reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world . however , training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot . fortunately , it is possible for multiple robots to share their experience with one another , and thereby , learn a policy collectively . in this work , we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging , real-world manipulation tasks . we propose a distributed and asynchronous version of guided policy search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots . we show that it achieves better generalization , utilization , and training times than the single robot alternative .

larex - a semi-automatic open-source tool for layout analysis and region extraction on early printed books
a semi-automatic open-source tool for layout analysis on early printed books is presented . larex uses a rule based connected components approach which is very fast , easily comprehensible for the user and allows an intuitive manual correction if necessary . the pagexml format is used to support integration into existing ocr workflows . evaluations showed that larex provides an efficient and flexible way to segment pages of early printed books .

scalable semantic matching of queries to ads in sponsored search advertising
sponsored search represents a major source of revenue for web search engines . this popular advertising model brings a unique possibility for advertisers to target users ' immediate intent communicated through a search query , usually by displaying their ads alongside organic search results for queries deemed relevant to their products or services . however , due to a large number of unique queries it is challenging for advertisers to identify all such relevant queries . for this reason search engines often provide a service of advanced matching , which automatically finds additional relevant queries for advertisers to bid on . we present a novel advanced matching approach based on the idea of semantic embeddings of queries and ads . the embeddings were learned using a large data set of user search sessions , consisting of search queries , clicked ads and search links , while utilizing contextual information such as dwell time and skipped ads . to address the large-scale nature of our problem , both in terms of data and vocabulary size , we propose a novel distributed algorithm for training of the embeddings . finally , we present an approach for overcoming a cold-start problem associated with new ads and queries . we report results of editorial evaluation and online tests on actual search traffic . the results show that our approach significantly outperforms baselines in terms of relevance , coverage , and incremental revenue . lastly , we open-source learned query embeddings to be used by researchers in computational advertising and related fields .

a popperian falsification of ai - lighthill 's argument defended
the area of computation called artificial intelligence ( ai ) is falsified by describing a previous 1972 falsification of ai by british applied mathematician james lighthill . it is explained how lighthill 's arguments continue to apply to current ai . it is argued that ai should use the popperian scientific method in which it is the duty of every scientist to attempt to falsify theories and if theories are falsified to replace or modify them . the paper describes the popperian method in detail and discusses paul nurse 's application of the method to cell biology that also involves questions of mechanism and behavior . arguments used by lighthill in his original 1972 report that falsifed ai are discussed . the lighthill arguments are then shown to apply to current ai . the argument uses recent scholarship to explain lighthill 's assumptions and to show how the arguments based on those assumptions continue to falsify modern ai . an iimportant focus of the argument involves hilbert 's philosophical programme that defined knowledge and truth as provable formal sentences . current ai takes the hilbert programme as dogma beyond criticism while lighthill as a mid 20th century applied mathematician had abandoned it . the paper uses recent scholarship to explain john von neumann 's criticism of ai that i claim was assumed by lighthill . the paper discusses computer chess programs to show lighthill 's combinatorial explosion still applies to ai but not humans . an argument showing that turing machines ( tm ) are not the correct description of computation is given . the paper concludes by advocating studying computation as peter naur 's dataology .

increasing the action gap : new operators for reinforcement learning
this paper introduces new optimality-preserving operators on q-functions . we first describe an operator for tabular representations , the consistent bellman operator , which incorporates a notion of local policy consistency . we show that this local consistency leads to an increase in the action gap at each state ; increasing this gap , we argue , mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies . this operator can also be applied to discretized continuous space and time problems , and we provide empirical results evidencing superior performance in this context . extending the idea of a locally consistent operator , we then derive sufficient conditions for an operator to preserve optimality , leading to a family of operators which includes our consistent bellman operator . as corollaries we provide a proof of optimality for baird 's advantage learning algorithm and derive other gap-increasing operators with interesting properties . we conclude with an empirical study on 60 atari 2600 games illustrating the strong potential of these new operators .

induction of first-order decision lists : results on learning the past tense of english verbs
this paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists , defined as ordered lists of clauses each ending in a cut . the method , called foidl , is based on foil ( quinlan , 1990 ) but employs intensional background knowledge and avoids the need for explicit negative examples . it is particularly useful for problems that involve rules with specific exceptions , such as learning the past-tense of english verbs , a task widely studied in the context of the symbolic/connectionist debate . foidl is able to learn concise , accurate programs for this problem from significantly fewer examples than previous methods ( both connectionist and symbolic ) .

improving circuit miniaturization and its efficiency using rough set theory
high-speed , accuracy , meticulousness and quick response are notion of the vital necessities for modern digital world . an efficient electronic circuit unswervingly affects the maneuver of the whole system . different tools are required to unravel different types of engineering tribulations . improving the efficiency , accuracy and low power consumption in an electronic circuit is always been a bottle neck problem . so the need of circuit miniaturization is always there . it saves a lot of time and power that is wasted in switching of gates , the wiring-crises is reduced , cross-sectional area of chip is reduced , the number of transistors that can implemented in chip is multiplied many folds . therefore to trounce with this problem we have proposed an artificial intelligence ( ai ) based approach that make use of rough set theory for its implementation . theory of rough set has been proposed by z pawlak in the year 1982. rough set theory is a new mathematical tool which deals with uncertainty and vagueness . decisions can be generated using rough set theory by reducing the unwanted and superfluous data . we have condensed the number of gates without upsetting the productivity of the given circuit . this paper proposes an approach with the help of rough set theory which basically lessens the number of gates in the circuit , based on decision rules .

detecting botnets through log correlation
botnets , which consist of thousands of compromised machines , can cause significant threats to other systems by launching distributed denial of service ( ssos ) attacks , keylogging , and backdoors . in response to these threats , new effective techniques are needed to detect the presence of botnets . in this paper , we have used an interception technique to monitor windows application programming interface ( api ) functions calls made by communication applications and store these calls with their arguments in log files . our algorithm detects botnets based on monitoring abnormal activity by correlating the changes in log file sizes from different hosts .

incremental map generation by low cost robots based on possibility/necessity grids
in this paper we present some results obtained with a troupe of low-cost robots designed to cooperatively explore and adquire the map of unknown structured orthogonal environments . in order to improve the covering of the explored zone , the robots show different behaviours and cooperate by transferring each other the perceived environment when they meet . the returning robots deliver to a host computer their partial maps and the host incrementally generates the map of the environment by means of apossibility/ necessity grid .

exploiting sentence and context representations in deep neural models for spoken language understanding
this paper presents a deep learning architecture for the semantic decoder component of a statistical spoken dialogue system . in a slot-filling dialogue , the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the automatic speech recognition . most current models for spoken language understanding assume ( i ) word-aligned semantic annotations as in sequence taggers and ( ii ) delexicalisation , or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation ( e.g. , morphology , synonyms , paraphrasing ) . in this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation . the proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation . results are presented for the publicly available dstc2 corpus and an in-car corpus which is similar to dstc2 but has a significantly higher word error rate ( wer ) .

recommender system based on algorithm of bicluster analysis recbi
in this paper we propose two new algorithms based on biclustering analysis , which can be used at the basis of a recommender system for educational orientation of russian school graduates . the first algorithm was designed to help students make a choice between different university faculties when some of their preferences are known . the second algorithm was developed for the special situation when nothing is known about their preferences . the final version of this recommender system will be used by higher school of economics .

influence-optimistic local values for multiagent planning -- - extended version
recent years have seen the development of methods for multiagent planning under uncertainty that scale to tens or even hundreds of agents . however , most of these methods either make restrictive assumptions on the problem domain , or provide approximate solutions without any guarantees on quality . methods in the former category typically build on heuristic search using upper bounds on the value function . unfortunately , no techniques exist to compute such upper bounds for problems with non-factored value functions . to allow for meaningful benchmarking through measurable quality guarantees on a very general class of problems , this paper introduces a family of influence-optimistic upper bounds for factored decentralized partially observable markov decision processes ( dec-pomdps ) that do not have factored value functions . intuitively , we derive bounds on very large multiagent planning problems by subdividing them in sub-problems , and at each of these sub-problems making optimistic assumptions with respect to the influence that will be exerted by the rest of the system . we numerically compare the different upper bounds and demonstrate how we can achieve a non-trivial guarantee that a heuristic solution for problems with hundreds of agents is close to optimal . furthermore , we provide evidence that the upper bounds may improve the effectiveness of heuristic influence search , and discuss further potential applications to multiagent planning .

adaptive submodularity : theory and applications in active learning and stochastic optimization
solving stochastic optimization problems under partial observability , where one needs to adaptively make decisions with uncertain outcomes , is a fundamental but notoriously difficult challenge . in this paper , we introduce the concept of adaptive submodularity , generalizing submodular set functions to adaptive policies . we prove that if a problem satisfies this property , a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy . in addition to providing performance guarantees for both stochastic maximization and coverage , adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations . we illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse applications including sensor placement , viral marketing and active learning . proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases , improve approximation guarantees and handle natural generalizations .

algorithms for finding dispensable variables
this short note reviews briefly three algorithms for finding the set of dispensable variables of a boolean formula . the presentation is light on proofs and heavy on intuitions .

conditional independence and markov properties in possibility theory
conditional independence and markov properties are powerful tools allowing expression of multidimensional probability distributions by means of low-dimensional ones . as multidimensional possibilistic models have been studied for several years , the demand for analogous tools in possibility theory seems to be quite natural . this paper is intended to be a promotion of de cooman 's measure-theoretic approcah to possibility theory , as this approach allows us to find analogies to many important results obtained in probabilistic framework . first , we recall semi-graphoid properties of conditional possibilistic independence , parameterized by a continuous t-norm , and find sufficient conditions for a class of archimedean t-norms to have the graphoid property . then we introduce markov properties and factorization of possibility distrubtions ( again parameterized by a continuous t-norm ) and find the relationships between them . these results are accompanied by a number of conterexamples , which show that the assumptions of specific theorems are substantial .

homomorphic parameter compression for distributed deep learning training
distributed training of deep neural networks has received significant research interest , and its major approaches include implementations on multiple gpus and clusters . parallelization can dramatically improve the efficiency of training deep and complicated models with large-scale data . a fundamental barrier against the speedup of dnn training , however , is the trade-off between computation and communication time . in other words , increasing the number of worker nodes decreases the time consumed in computation while simultaneously increasing communication overhead under constrained network bandwidth , especially in commodity hardware environments . to alleviate this trade-off , we suggest the idea of homomorphic parameter compression , which compresses parameters with the least expense and trains the dnn with the compressed representation . although the specific method is yet to be discovered , we demonstrate that there is a high probability that the homomorphism can reduce the communication overhead , thanks to little compression and decompression times . we also provide theoretical speedup of homomorphic compression .

introduction : cognitive issues in natural language processing
this special issue is dedicated to get a better picture of the relationships between computational linguistics and cognitive science . it specifically raises two questions : `` what is the potential contribution of computational language modeling to cognitive science ? '' and conversely : `` what is the influence of cognitive science in contemporary computational linguistics ? ''

on kernelization of supervised mahalanobis distance learners
this paper focuses on the problem of kernelizing an existing supervised mahalanobis distance learner . the following features are included in the paper . firstly , three popular learners , namely , `` neighborhood component analysis '' , `` large margin nearest neighbors '' and `` discriminant neighborhood embedding '' , which do not have kernel versions are kernelized in order to improve their classification performances . secondly , an alternative kernelization framework called `` kpca trick '' is presented . implementing a learner in the new framework gains several advantages over the standard framework , e.g . no mathematical formulas and no reprogramming are required for a kernel implementation , the framework avoids troublesome problems such as singularity , etc . thirdly , while the truths of representer theorems are just assumptions in previous papers related to ours , here , representer theorems are formally proven . the proofs validate both the kernel trick and the kpca trick in the context of mahalanobis distance learning . fourthly , unlike previous works which always apply brute force methods to select a kernel , we investigate two approaches which can be efficiently adopted to construct an appropriate kernel for a given dataset . finally , numerical results on various real-world datasets are presented .

procedural content generation via machine learning ( pcgml )
this survey explores procedural content generation via machine learning ( pcgml ) , defined as the generation of game content using machine learning models trained on existing content . as the importance of pcg for game development increases , researchers explore new avenues for generating high-quality content with or without human involvement ; this paper addresses the relatively new paradigm of using machine learning ( in contrast with search-based , solver-based , and constructive methods ) . we focus on what is most often considered functional game content such as platformer levels , game maps , interactive fiction stories , and cards in collectible card games , as opposed to cosmetic content such as sprites and sound effects . in addition to using pcg for autonomous generation , co-creativity , mixed-initiative design , and compression , pcgml is suited for repair , critique , and content analysis because of its focus on modeling existing content . we discuss various data sources and representations that affect the resulting generated content . multiple pcgml methods are covered , including neural networks , long short-term memory ( lstm ) networks , autoencoders , and deep convolutional networks ; markov models , $ n $ -grams , and multi-dimensional markov chains ; clustering ; and matrix factorization . finally , we discuss open problems in the application of pcgml , including learning from small datasets , lack of training data , multi-layered learning , style-transfer , parameter tuning , and pcg as a game mechanic .

reasoning about noisy sensors and effectors in the situation calculus
agents interacting with an incompletely known world need to be able to reason about the effects of their actions , and to gain further information about that world they need to use sensors of some sort . unfortunately , both the effects of actions and the information returned from sensors are subject to error . to cope with such uncertainties , the agent can maintain probabilistic beliefs about the state of the world . with probabilistic beliefs the agent will be able to quantify the likelihood of the various outcomes of its actions and is better able to utilize the information gathered from its error-prone actions and sensors . in this paper , we present a model in which we can reason about an agent 's probabilistic degrees of belief and the manner in which these beliefs change as various actions are executed . we build on a general logical theory of action developed by reiter and others , formalized in the situation calculus . we propose a simple axiomatization that captures an agent 's state of belief and the manner in which these beliefs change when actions are executed . our model displays a number of intuitively reasonable properties .

estimating causal direction and confounding of two discrete variables
we propose a method to classify the causal relationship between two discrete variables given only the joint distribution of the variables , acknowledging that the method is subject to an inherent baseline error . we assume that the causal system is acyclicity , but we do allow for hidden common causes . our algorithm presupposes that the probability distributions $ p ( c ) $ of a cause $ c $ is independent from the probability distribution $ p ( e\mid c ) $ of the cause-effect mechanism . while our classifier is trained with a bayesian assumption of flat hyperpriors , we do not make this assumption about our test data . this work connects to recent developments on the identifiability of causal models over continuous variables under the assumption of `` independent mechanisms '' . carefully-commented python notebooks that reproduce all our experiments are available online at http : //vision.caltech.edu/~kchalupk/code.html .

local subspace-based outlier detection using global neighbourhoods
outlier detection in high-dimensional data is a challenging yet important task , as it has applications in , e.g. , fraud detection and quality control . state-of-the-art density-based algorithms perform well because they 1 ) take the local neighbourhoods of data points into account and 2 ) consider feature subspaces . in highly complex and high-dimensional data , however , existing methods are likely to overlook important outliers because they do not explicitly take into account that the data is often a mixture distribution of multiple components . we therefore introduce gloss , an algorithm that performs local subspace outlier detection using global neighbourhoods . experiments on synthetic data demonstrate that gloss more accurately detects local outliers in mixed data than its competitors . moreover , experiments on real-world data show that our approach identifies relevant outliers overlooked by existing methods , confirming that one should keep an eye on the global perspective even when doing local outlier detection .

multi-agent projective simulation : a starting point
we develop a two-defender ( alice and bob ) invasion game using the method of projective simulation as an embodied model for artificial intelligence . we hope that it will be the first step towards the effect of perception on different actions in a given game . as a given perception of a given situation , the agent , say alice , encounters some attack symbols coming from the right attacker where she can learn to prevent . however , some of these percepts are invisible for her . instead , she perceives some other signs that are related to her partner 's ( bob ) task . we elaborate an example in which an agent perceives an equal portion of percepts from both attackers . alice can choose to concentrate on her job , though she loses some attacks . alternatively , she can have some sort of cooperation with bob to get and give help . it follows that the maximum blocking efficiency in concentration is just the minimum blocking efficiency in cooperation . furthermore , alice would have a choice to select two different forgetting factors for blocking attacks and for helping task . therefore , she can choose between herself and the other . consequently , selfishness is discerned as an only nash equilibrium in this game . it is a pure strategy and pareto optimal and containing shapley value in this superadditive coalition . finally , we propose another perception for the same situation that can be tracked in the future regarding the present study .

business process deviance mining : review and evaluation
business process deviance refers to the phenomenon whereby a subset of the executions of a business process deviate , in a negative or positive way , with respect to its expected or desirable outcomes . deviant executions of a business process include those that violate compliance rules , or executions that undershoot or exceed performance targets . deviance mining is concerned with uncovering the reasons for deviant executions by analyzing business process event logs . this article provides a systematic review and comparative evaluation of deviance mining approaches based on a family of data mining techniques known as sequence classification . using real-life logs from multiple domains , we evaluate a range of feature types and classification methods in terms of their ability to accurately discriminate between normal and deviant executions of a process . we also analyze the interestingness of the rule sets extracted using different methods . we observe that feature sets extracted using pattern mining techniques only slightly outperform simpler feature sets based on counts of individual activity occurrences in a trace .

strategyproof peer selection using randomization , partitioning , and apportionment
peer review , evaluation , and selection is a fundamental aspect of modern science . funding bodies the world over employ experts to review and select the best proposals of those submitted for funding . the problem of peer selection , however , is much more general : a professional society may want to give a subset of its members awards based on the opinions of all members ; an instructor for a mooc or online course may want to crowdsource grading ; or a marketing company may select ideas from group brainstorming sessions based on peer evaluation . we make three fundamental contributions to the study of procedures or mechanisms for peer selection , a specific type of group decision-making problem , studied in computer science , economics , and political science . first , we propose a novel mechanism that is strategyproof , i.e. , agents can not benefit by reporting insincere valuations . second , we demonstrate the effectiveness of our mechanism by a comprehensive simulation-based comparison with a suite of mechanisms found in the literature . finally , our mechanism employs a randomized rounding technique that is of independent interest , as it solves the apportionment problem that arises in various settings where discrete resources such as parliamentary representation slots need to be divided proportionally .

on consistency of optimal pricing algorithms in repeated posted-price auctions with strategic buyer
we study revenue optimization learning algorithms for repeated posted-price auctions where a seller interacts with a single strategic buyer that holds a fixed private valuation for a good and seeks to maximize his cumulative discounted surplus . for this setting , first , we propose a novel algorithm that never decreases offered prices and has a tight strategic regret bound in $ \theta ( \log\log t ) $ under some mild assumptions on the buyer surplus discounting . this result closes the open research question on the existence of a no-regret horizon-independent weakly consistent pricing . the proposed algorithm is inspired by our observation that a double decrease of offered prices in a weakly consistent algorithm is enough to cause a linear regret . this motivates us to construct a novel transformation that maps a right-consistent algorithm to a weakly consistent one that never decreases offered prices . second , we outperform the previously known strategic regret upper bound of the algorithm prrfes , where the improvement is achieved by means of a finer constant factor $ c $ of the principal term $ c\log\log t $ in this upper bound . finally , we generalize results on strategic regret previously known for geometric discounting of the buyer 's surplus to discounting of other types , namely : the optimality of the pricing prrfes to the case of geometrically concave decreasing discounting ; and linear lower bound on the strategic regret of a wide range of horizon-independent weakly consistent algorithms to the case of arbitrary discounts .

neural enquirer : learning to query tables with natural language
we proposed neural enquirer as a neural network architecture to execute a natural language ( nl ) query on a knowledge-base ( kb ) for answers . basically , neural enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables . unlike similar efforts in end-to-end training of semantic parsers , neural enquirer is fully `` neuralized '' : it not only gives distributional representation of the query and the knowledge-base , but also realizes the execution of compositional queries as a series of differentiable operations , with intermediate results ( consisting of annotations of the tables at different levels ) saved on multiple layers of memory . neural enquirer can be trained with gradient descent , with which not only the parameters of the controlling components and semantic parsing component , but also the embeddings of the tables and query words can be learned from scratch . the training can be done in an end-to-end fashion , but it can take stronger guidance , e.g. , the step-by-step supervision for complicated queries , and benefit from it . neural enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world . our experiments show that neural enquirer can learn to execute fairly complicated nl queries on tables with rich structures .

faith in the algorithm , part 2 : computational eudaemonics
eudaemonics is the study of the nature , causes , and conditions of human well-being . according to the ethical theory of eudaemonia , reaping satisfaction and fulfillment from life is not only a desirable end , but a moral responsibility . however , in modern society , many individuals struggle to meet this responsibility . computational mechanisms could better enable individuals to achieve eudaemonia by yielding practical real-world systems that embody algorithms that promote human flourishing . this article presents eudaemonic systems as the evolutionary goal of the present day recommender system .

the reachability of computer programs
would it be possible to explain the emergence of new computational ideas using the computation itself ? would it be feasible to describe the discovery process of new algorithmic solutions using only mathematics ? this study is the first effort to analyze the nature of such inquiry from the viewpoint of effort to find a new algorithmic solution to a given problem . we define program reachability as a probability function whose argument is a form of the energetic cost ( algorithmic entropy ) of the problem .

online tool condition monitoring based on parsimonious ensemble+
accurate diagnosis of tool wear in metal turning process remains an open challenge for both scientists and industrial practitioners because of inhomogeneities in workpiece material , nonstationary machining settings to suit production requirements , and nonlinear relations between measured variables and tool wear . common methodologies for tool condition monitoring still rely on batch approaches which can not cope with a fast sampling rate of metal cutting process . furthermore they require a retraining process to be completed from scratch when dealing with a new set of machining parameters . this paper presents an online tool condition monitoring approach based on parsimonious ensemble+ , pensemble+ . the unique feature of pensemble+ lies in its highly flexible principle where both ensemble structure and base-classifier structure can automatically grow and shrink on the fly based on the characteristics of data streams . moreover , the online feature selection scenario is integrated to actively sample relevant input attributes . the paper presents advancement of a newly developed ensemble learning algorithm , pensemble+ , where online active learning scenario is incorporated to reduce operator labelling effort . the ensemble merging scenario is proposed which allows reduction of ensemble complexity while retaining its diversity . experimental studies utilising real-world manufacturing data streams and comparisons with well known algorithms were carried out . furthermore , the efficacy of pensemble was examined using benchmark concept drift data streams . it has been found that pensemble+ incurs low structural complexity and results in a significant reduction of operator labelling effort .

co-clustering of fuzzy lagged data
the paper focuses on mining patterns that are characterized by a fuzzy lagged relationship between the data objects forming them . such a regulatory mechanism is quite common in real life settings . it appears in a variety of fields : finance , gene expression , neuroscience , crowds and collective movements are but a limited list of examples . mining such patterns not only helps in understanding the relationship between objects in the domain , but assists in forecasting their future behavior . for most interesting variants of this problem , finding an optimal fuzzy lagged co-cluster is an np-complete problem . we thus present a polynomial-time monte-carlo approximation algorithm for mining fuzzy lagged co-clusters . we prove that for any data matrix , the algorithm mines a fuzzy lagged co-cluster with fixed probability , which encompasses the optimal fuzzy lagged co-cluster by a maximum 2 ratio columns overhead and completely no rows overhead . moreover , the algorithm handles noise , anti-correlations , missing values and overlapping patterns . the algorithm was extensively evaluated using both artificial and real datasets . the results not only corroborate the ability of the algorithm to efficiently mine relevant and accurate fuzzy lagged co-clusters , but also illustrate the importance of including the fuzziness in the lagged-pattern model .

test-driven development of ontologies ( extended version )
emerging ontology authoring methods to add knowledge to an ontology focus on ameliorating the validation bottleneck . the verification of the newly added axiom is still one of trying and seeing what the reasoner says , because a systematic testbed for ontology authoring is missing . we sought to address this by introducing the approach of test-driven development for ontology authoring . we specify 36 generic tests , as tbox queries and tbox axioms tested through individuals , and structure their inner workings in an ` open box'-way , which cover the owl 2 dl language features . this is implemented as a protege plugin so that one can perform a tdd test as a black box test . we evaluated the two test approaches on their performance . the tbox queries were faster , and that effect is more pronounced the larger the ontology is . we provide a general sequence of a tdd process for ontology engineering as a foundation for a tdd methodology .

the sp theory of intelligence : distinctive features and advantages
this paper highlights distinctive features of the `` sp theory of intelligence '' and its apparent advantages compared with some ai-related alternatives . distinctive features and advantages are : simplification and integration of observations and concepts ; simplification and integration of structures and processes in computing systems ; the theory is itself a theory of computing ; it can be the basis for new architectures for computers ; information compression via the matching and unification of patterns and , more specifically , via multiple alignment , is fundamental ; transparency in the representation and processing of knowledge ; the discovery of 'natural ' structures via information compression ( donsvic ) ; interpretations of mathematics ; interpretations in human perception and cognition ; and realisation of abstract concepts in terms of neurons and their inter-connections ( `` sp-neural '' ) . these things relate to ai-related alternatives : minimum length encoding and related concepts ; deep learning in neural networks ; unified theories of cognition and related research ; universal search ; bayesian networks and more ; pattern recognition and vision ; the analysis , production , and translation of natural language ; unsupervised learning of natural language ; exact and inexact forms of reasoning ; representation and processing of diverse forms of knowledge ; ibm 's watson ; software engineering ; solving problems associated with big data , and in the development of intelligence in autonomous robots . in conclusion , the sp system can provide a firm foundation for the long-term development of ai , with many potential benefits and applications . it may also deliver useful results on relatively short timescales . a high-parallel , open-source version of the sp machine , derived from the sp computer model , would be a means for researchers everywhere to explore what can be done with the system , and to create new versions of it .

earl : joint entity and relation linking for question answering over knowledge graphs
in order to answer natural language questions over knowledge graphs , most processing pipelines involve entity and relation linking . traditionally , entity linking and relation linking has been performed either as dependent sequential tasks or independent parallel tasks . in this paper , we propose a framework called `` earl '' , which performs entity linking and relation linking as a joint single task . earl uses a graph connection based solution to the problem . we model the linking task as an instance of the generalised travelling salesman problem ( gtsp ) and use gtsp approximate algorithm solutions . we later develop earl which uses a pair-wise graph-distance based solution to the problem.the system determines the best semantic connection between all keywords of the question by referring to a knowledge graph . this is achieved by exploiting the `` connection density '' between entity candidates and relation candidates . the `` connection density '' based solution performs at par with the approximate gtsp solution.we have empirically evaluated the framework on a dataset with 5000 questions . our system surpasses state-of-the-art scores for entity linking task by reporting an accuracy of 0.65 to 0.40 from the next best entity linker .

reinforcement learning in rich-observation mdps using spectral methods
designing effective exploration-exploitation algorithms in markov decision processes ( mdps ) with large state-action spaces is the main challenge in reinforcement learning ( rl ) . in fact , the learning performance degrades with the number of states and actions in the mdp . however , mdps often exhibit a low-dimensional latent structure in practice , where a small hidden state is observable through a possibly large number of observations . in this paper , we study the setting of rich-observation markov decision processes ( \richmdp ) , where hidden states are mapped to observations through an injective mapping , so that an observation can be generated by only one hidden state . while this mapping is unknown a priori , we introduce a spectral decomposition method that consistently estimates how observations are clustered in the hidden states . the estimated clustering is then integrated into an optimistic algorithm for rl ( ucrl ) , which operates on the smaller clustered space . the resulting algorithm proceeds through phases and we show that its per-step regret ( i.e. , the difference in cumulative reward between the algorithm and the optimal policy ) decreases as more observations are clustered together and finally , matches the ( ideal ) performance of an rl algorithm running directly on the hidden mdp .

committee-based sample selection for probabilistic classifiers
in many real-world learning tasks , it is expensive to acquire a sufficient number of labeled examples for training . this paper investigates methods for reducing annotation cost by ` sample selection ' . in this approach , during training the learning program examines many unlabeled examples and selects for labeling only those that are most informative at each stage . this avoids redundantly labeling examples that contribute little new information . our work follows on previous research on query by committee , extending the committee-based paradigm to the context of probabilistic classification . we describe a family of empirical methods for committee-based sample selection in probabilistic classification models , which evaluate the informativeness of an example by measuring the degree of disagreement between several model variants . these variants ( the committee ) are drawn randomly from a probability distribution conditioned by the training set labeled so far . the method was applied to the real-world natural language processing task of stochastic part-of-speech tagging . we find that all variants of the method achieve a significant reduction in annotation cost , although their computational efficiency differs . in particular , the simplest variant , a two member committee with no parameters to tune , gives excellent results . we also show that sample selection yields a significant reduction in the size of the model used by the tagger .

the dl-lite family and relations
the recently introduced series of description logics under the common moniker dl-lite has attracted attention of the description logic and semantic web communities due to the low computational complexity of inference , on the one hand , and the ability to represent conceptual modeling formalisms , on the other . the main aim of this article is to carry out a thorough and systematic investigation of inference in extensions of the original dl-lite logics along five axes : by ( i ) adding the boolean connectives and ( ii ) number restrictions to concept constructs , ( iii ) allowing role hierarchies , ( iv ) allowing role disjointness , symmetry , asymmetry , reflexivity , irreflexivity and transitivity constraints , and ( v ) adopting or dropping the unique same assumption . we analyze the combined complexity of satisfiability for the resulting logics , as well as the data complexity of instance checking and answering positive existential queries . our approach is based on embedding dl-lite logics in suitable fragments of the one-variable first-order logic , which provides useful insights into their properties and , in particular , computational behavior .

a workflow for visual diagnostics of binary classifiers using instance-level explanations
human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions . to this end , we propose a visual analytics workflow to help data scientists and domain experts explore , diagnose , and understand the decisions made by a binary classifier . the approach leverages `` instance-level explanations '' , measures of local feature relevance that explain single instances , and uses them to build a set of visual representations that guide the users in their investigation . the workflow is based on three main visual representations and steps : one based on aggregate statistics to see how data distributes across correct / incorrect decisions ; one based on explanations to understand which features are used to make these decisions ; and one based on raw data , to derive insights on potential root causes for the observed patterns . the workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed . the case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes , thus experts can generate useful hypotheses on how a model can be improved .

merging locally correct knowledge bases : a preliminary report
belief integration methods are often aimed at deriving a single and consistent knowledge base that retains as much as possible of the knowledge bases to integrate . the rationale behind this approach is the minimal change principle : the result of the integration process should differ as less as possible from the knowledge bases to integrate . we show that this principle can be reformulated in terms of a more general model of belief revision , based on the assumption that inconsistency is due to the mistakes the knowledge bases contain . current belief revision strategies are based on a specific kind of mistakes , which however does not include all possible ones . some alternative possibilities are discussed .

an improved multimodal pso method based on electrostatic interaction using n- nearest-neighbor local search
in this paper , an improved multimodal optimization ( mmo ) algorithm , called lsepso , has been proposed . lsepso combined electrostatic particle swarm optimization ( epso ) algorithm and a local search method and then made some modification on them . it has been shown to improve global and local optima finding ability of the algorithm . this algorithm useda modified local search to improve particle 's personal best , which used n-nearest-neighbour instead of nearest-neighbour . then , by creating n new points among each particle and n nearest particles , it tried to find a point which could be the alternative of particle 's personal best . this method prevented particle 's attenuation and following a specific particle by its neighbours . the performed tests on a number of benchmark functions clearly demonstrated that the improved algorithm is able to solve mmo problems and outperform other tested algorithms in this article .

verification , validation and integrity of distributed and interchanged rule based policies and contracts in the semantic web
rule-based policy and contract systems have rarely been studied in terms of their software engineering properties . this is a serious omission , because in rule-based policy or contract representation languages rules are being used as a declarative programming language to formalize real-world decision logic and create is production systems upon . this paper adopts an se methodology from extreme programming , namely test driven development , and discusses how it can be adapted to verification , validation and integrity testing ( v & v & i ) of policy and contract specifications . since , the test-driven approach focuses on the behavioral aspects and the drawn conclusions instead of the structure of the rule base and the causes of faults , it is independent of the complexity of the rule language and the system under test and thus much easier to use and understand for the rule engineer and the user .

meta-unsupervised-learning : a supervised approach to unsupervised learning
we introduce a new paradigm to investigate unsupervised learning , reducing unsupervised learning to supervised learning . specifically , we mitigate the subjectivity in unsupervised decision-making by leveraging knowledge acquired from prior , possibly heterogeneous , supervised learning tasks . we demonstrate the versatility of our framework via comprehensive expositions and detailed experiments on several unsupervised problems such as ( a ) clustering , ( b ) outlier detection , and ( c ) similarity prediction under a common umbrella of meta-unsupervised-learning . we also provide rigorous pac-agnostic bounds to establish the theoretical foundations of our framework , and show that our framing of meta-clustering circumvents kleinberg 's impossibility theorem for clustering .

applications of algorithmic probability to the philosophy of mind
this paper presents formulae that can solve various seemingly hopeless philosophical conundrums . we discuss the simulation argument , teleportation , mind-uploading , the rationality of utilitarianism , and the ethics of exploiting artificial general intelligence . our approach arises from combining the essential ideas of formalisms such as algorithmic probability , the universal intelligence measure , space-time-embedded intelligence , and hutter 's observer localization . we argue that such universal models can yield the ultimate solutions , but a novel research direction would be required in order to find computationally efficient approximations thereof .

visually-aware fashion recommendation and design with generative image models
building effective recommender systems for domains like fashion is challenging due to the high level of subjectivity and the semantic complexity of the features involved ( i.e. , fashion styles ) . recent work has shown that approaches to ` visual ' recommendation ( e.g.~clothing , art , etc . ) can be made more accurate by incorporating visual signals directly into the recommendation objective , using ` off-the-shelf ' feature representations derived from deep networks . here , we seek to extend this contribution by showing that recommendation performance can be significantly improved by learning ` fashion aware ' image representations directly , i.e. , by training the image representation ( from the pixel level ) and the recommender system jointly ; this contribution is related to recent work using siamese cnns , though we are able to show improvements over state-of-the-art recommendation techniques such as bpr and variants that make use of pre-trained visual features . furthermore , we show that our model can be used \emph { generatively } , i.e. , given a user and a product category , we can generate new images ( i.e. , clothing items ) that are most consistent with their personal taste . this represents a first step towards building systems that go beyond recommending existing items from a product corpus , but which can be used to suggest styles and aid the design of new products .

convolutional factor graphs as probabilistic models
based on a recent development in the area of error control coding , we introduce the notion of convolutional factor graphs ( cfgs ) as a new class of probabilistic graphical models . in this context , the conventional factor graphs are referred to as multiplicative factor graphs ( mfgs ) . this paper shows that cfgs are natural models for probability functions when summation of independent latent random variables is involved . in particular , cfgs capture a large class of linear models , where the linearity is in the sense that the observed variables are obtained as a linear ransformation of the latent variables taking arbitrary distributions . we use gaussian models and independent factor models as examples to emonstrate the use of cfgs . the requirement of a linear transformation between latent variables ( with certain independence restriction ) and the bserved variables , to an extent , limits the modelling flexibility of cfgs . this structural restriction however provides a powerful analytic tool to the framework of cfgs ; that is , upon taking the fourier transform of the function represented by the cfg , the resulting function is represented by a fg with identical structure . this fourier transform duality allows inference problems on a cfg to be solved on the corresponding dual mfg .

theoretical foundations for abstraction-based probabilistic planning
modeling worlds and actions under uncertainty is one of the central problems in the framework of decision-theoretic planning . the representation must be general enough to capture real-world problems but at the same time it must provide a basis upon which theoretical results can be derived . the central notion in the framework we propose here is that of the affine-operator , which serves as a tool for constructing ( convex ) sets of probability distributions , and which can be considered as a generalization of belief functions and interval mass assignments . uncertainty in the state of the worlds is modeled with sets of probability distributions , represented by affine-trees while actions are defined as tree-manipulators . a small set of key properties of the affine-operator is presented , forming the basis for most existing operator-based definitions of probabilistic action projection and action abstraction . we derive and prove correct three projection rules , which vividly illustrate the precision-complexity tradeoff in plan projection . finally , we show how the three types of action abstraction identified by haddawy and doan are manifested in the present framework .

fruler : fuzzy rule learning through evolution for regression
in regression problems , the use of tsk fuzzy systems is widely extended due to the precision of the obtained models . moreover , the use of simple linear tsk models is a good choice in many real problems due to the easy understanding of the relationship between the output and input variables . in this paper we present fruler , a new genetic fuzzy system for automatically learning accurate and simple linguistic tsk fuzzy rule bases for regression problems . in order to reduce the complexity of the learned models while keeping a high accuracy , the algorithm consists of three stages : instance selection , multi-granularity fuzzy discretization of the input variables , and the evolutionary learning of the rule base that uses the elastic net regularization to obtain the consequents of the rules . each stage was validated using 28 real-world datasets and fruler was compared with three state of the art enetic fuzzy systems . experimental results show that fruler achieves the most accurate and simple models compared even with approximative approaches .

subjectivity , bayesianism , and causality
bayesian probability theory is one of the most successful frameworks to model reasoning under uncertainty . its defining property is the interpretation of probabilities as degrees of belief in propositions about the state of the world relative to an inquiring subject . this essay examines the notion of subjectivity by drawing parallels between lacanian theory and bayesian probability theory , and concludes that the latter must be enriched with causal interventions to model agency . the central contribution of this work is an abstract model of the subject that accommodates causal interventions in a measure-theoretic formalisation . this formalisation is obtained through a game-theoretic ansatz based on modelling the inside and outside of the subject as an extensive-form game with imperfect information between two players . finally , i illustrate the expressiveness of this model with an example of causal induction .

the schema editor of openiot for semantic sensor networks
ontologies provide conceptual abstractions over data , in domains such as the internet of things , in a way that sensor data can be harvested and interpreted by people and applications . the semantic sensor network ( ssn ) ontology is the de-facto standard for semantic representation of sensor observations and metadata , and it is used at the core of the open source platform for the internet of things , openiot . in this paper we present a schema editor that provides an intuitive web interface for defining new types of sensors , and concrete instances of them , using the ssn ontology as the core model . this editor is fully integrated with the openiot platform for generating virtual sensor descriptions and automating their semantic annotation and registration process .

tractable bayesian learning of tree belief networks
in this paper we present decomposable priors , a family of priors over structure and parameters of tree belief nets for which bayesian learning with complete observations is tractable , in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time . this follows from two main results : first , we show that factored distributions over spanning trees in a graph can be integrated in closed form . second , we examine priors over tree parameters and show that a set of assumptions similar to ( heckerman and al . 1995 ) constrain the tree parameter priors to be a compactly parameterized product of dirichlet distributions . beside allowing for exact bayesian learning , these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures .

simple epistemic planning : generalised gossiping
the gossip problem , in which information ( known as secrets ) must be shared among a certain number of agents using the minimum number of calls , is of interest in the conception of communication networks and protocols . we extend the gossip problem to arbitrary epistemic depths . for example , we may require not only that all agents know all secrets but also that all agents know that all agents know all secrets . we give optimal protocols for various versions of the generalised gossip problem , depending on the graph of communication links , in the case of two-way communications , one-way communications and parallel communication . we also study different variants which allow us to impose negative goals such as that certain agents must not know certain secrets . we show that in the presence of negative goals testing the existence of a successful protocol is np-complete whereas this is always polynomial-time in the case of purely positive goals .

dispute resolution using argumentation-based mediation
mediation is a process , in which both parties agree to resolve their dispute by negotiating over alternative solutions presented by a mediator . in order to construct such solutions , mediation brings more information and knowledge , and , if possible , resources to the negotiation table . the contribution of this paper is the automated mediation machinery which does that . it presents an argumentation-based mediation approach that extends the logic-based approach to argumentation-based negotiation involving bdi agents . the paper describes the mediation algorithm . for comparison it illustrates the method with a case study used in an earlier work . it demonstrates how the computational mediator can deal with realistic situations in which the negotiating agents would otherwise fail due to lack of knowledge and/or resources .

pre-processing in ai based prediction of qsars
machine learning , data mining and artificial intelligence ( ai ) based methods have been used to determine the relations between chemical structure and biological activity , called quantitative structure activity relationships ( qsars ) for the compounds . pre-processing of the dataset , which includes the mapping from a large number of molecular descriptors in the original high dimensional space to a small number of components in the lower dimensional space while retaining the features of the original data , is the first step in this process . a common practice is to use a mapping method for a dataset without prior analysis . this pre-analysis has been stressed in our work by applying it to two important classes of qsar prediction problems : drug design ( predicting anti-hiv-1 activity ) and predictive toxicology ( estimating hepatocarcinogenicity of chemicals ) . we apply one linear and two nonlinear mapping methods on each of the datasets . based on this analysis , we conclude the nature of the inherent relationships between the elements of each dataset , and hence , the mapping method best suited for it . we also show that proper preprocessing can help us in choosing the right feature extraction tool as well as give an insight about the type of classifier pertinent for the given problem .

a comparative study of meta-heuristic algorithms for solving quadratic assignment problem
quadratic assignment problem ( qap ) is an np-hard combinatorial optimization problem , therefore , solving the qap requires applying one or more of the meta-heuristic algorithms . this paper presents a comparative study between meta-heuristic algorithms : genetic algorithm , tabu search , and simulated annealing for solving a real-life ( qap ) and analyze their performance in terms of both runtime efficiency and solution quality . the results show that genetic algorithm has a better solution quality while tabu search has a faster execution time in comparison with other meta-heuristic algorithms for solving qap .

methods for integrating knowledge with the three-weight optimization algorithm for hybrid cognitive processing
in this paper we consider optimization as an approach for quickly and flexibly developing hybrid cognitive capabilities that are efficient , scalable , and can exploit knowledge to improve solution speed and quality . in this context , we focus on the three-weight algorithm , which aims to solve general optimization problems . we propose novel methods by which to integrate knowledge with this algorithm to improve expressiveness , efficiency , and scaling , and demonstrate these techniques on two example problems ( sudoku and circle packing ) .

mixed-membership stochastic block-models for transactional networks
transactional network data can be thought of as a list of one-to-many communications ( e.g. , email ) between nodes in a social network . most social network models convert this type of data into binary relations between pairs of nodes . we develop a latent mixed membership model capable of modeling richer forms of transactional network data , including relations between more than two nodes . the model can cluster nodes and predict transactions . the block-model nature of the model implies that groups can be characterized in very general ways . this flexible notion of group structure enables discovery of rich structure in transactional networks . estimation and inference are accomplished via a variational em algorithm . simulations indicate that the learning algorithm can recover the correct generative model . interesting structure is discovered in the enron email dataset and another dataset extracted from the reddit website . analysis of the reddit data is facilitated by a novel performance measure for comparing two soft clusterings . the new model is superior at discovering mixed membership in groups and in predicting transactions .

missing data estimation in high-dimensional datasets : a swarm intelligence-deep neural network approach
in this paper , we examine the problem of missing data in high-dimensional datasets by taking into consideration the missing completely at random and missing at random mechanisms , as well as thearbitrary missing pattern . additionally , this paper employs a methodology based on deep learning and swarm intelligence algorithms in order to provide reliable estimates for missing data . the deep learning technique is used to extract features from the input data via an unsupervised learning approach by modeling the data distribution based on the input . this deep learning technique is then used as part of the objective function for the swarm intelligence technique in order to estimate the missing data after a supervised fine-tuning phase by minimizing an error function based on the interrelationship and correlation between features in the dataset . the investigated methodology in this paper therefore has longer running times , however , the promising potential outcomes justify the trade-off . also , basic knowledge of statistics is presumed .

top-k query answering in datalog+/- ontologies under subjective reports ( technical report )
the use of preferences in query answering , both in traditional databases and in ontology-based data access , has recently received much attention , due to its many real-world applications . in this paper , we tackle the problem of top-k query answering in datalog+/- ontologies subject to the querying user 's preferences and a collection of ( subjective ) reports of other users . here , each report consists of scores for a list of features , its author 's preferences among the features , as well as other information . theses pieces of information of every report are then combined , along with the querying user 's preferences and his/her trust into each report , to rank the query results . we present two alternative such rankings , along with algorithms for top-k ( atomic ) query answering under these rankings . we also show that , under suitable assumptions , these algorithms run in polynomial time in the data complexity . we finally present more general reports , which are associated with sets of atoms rather than single atoms .

r2-d2 : color-inspired convolutional neural network ( cnn ) -based android malware detections
machine learning ( ml ) has found it particularly useful in malware detection . however , as the malware evolves very fast , the stability of the feature extracted from malware serves as a critical issue in malware detection . recent success of deep learning in image recognition , natural language processing , and machine translation indicate a potential solution for stabilizing the malware detection effectiveness . we present a color-inspired convolutional neural network-based android malware detection ( r2-d2 ) , which can detect malware without extracting pre-selected features ( e.g. , the control-flow of op-code , classes , methods of functions and the timing they are invoked etc . ) from android apps . in particular , we develop a color representation for translating android apps into rgb color code and transform them to a fixed-sized encoded image . after that , the encoded image is fed to convolutional neural network for automatic feature extraction and learning , reducing the expert 's intervention . we have collected over 1 million malware samples and 1 million benign samples according to the data provided by leopard mobile inc. from its core product security master ( which has 623 million monthly active users and 10k new malware samples per day ) . it is shown that r2-d2 can effectively detect the malware . furthermore , we keep our research results and release experiment material on http : //r2d2.twman.org if there is any update .

des : a challenge problem for nonmonotonic reasoning systems
the us data encryption standard , des for short , is put forward as an interesting benchmark problem for nonmonotonic reasoning systems because ( i ) it provides a set of test cases of industrial relevance which shares features of randomly generated problems and real-world problems , ( ii ) the representation of des using normal logic programs with the stable model semantics is simple and easy to understand , and ( iii ) this subclass of logic programs can be seen as an interesting special case for many other formalizations of nonmonotonic reasoning . in this paper we present two encodings of des as logic programs : a direct one out of the standard specifications and an optimized one extending the work of massacci and marraro . the computational properties of the encodings are studied by using them for des key search with the smodels system as the implementation of the stable model semantics . results indicate that the encodings and smodels are quite competitive : they outperform state-of-the-art sat-checkers working with an optimized encoding of des into sat and are comparable with a sat-checker that is customized and tuned for the optimized sat encoding .

beyond one-step-ahead forecasting : evaluation of alternative multi-step-ahead forecasting models for crude oil prices
an accurate prediction of crude oil prices over long future horizons is challenging and of great interest to governments , enterprises , and investors . this paper proposes a revised hybrid model built upon empirical mode decomposition ( emd ) based on the feed-forward neural network ( fnn ) modeling framework incorporating the slope-based method ( sbm ) , which is capable of capturing the complex dynamic of crude oil prices . three commonly used multi-step-ahead prediction strategies proposed in the literature , including iterated strategy , direct strategy , and mimo ( multiple-input multiple-output ) strategy , are examined and compared , and practical considerations for the selection of a prediction strategy for multi-step-ahead forecasting relating to crude oil prices are identified . the weekly data from the wti ( west texas intermediate ) crude oil spot price are used to compare the performance of the alternative models under the emd-sbm-fnn modeling framework with selected counterparts . the quantitative and comprehensive assessments are performed on the basis of prediction accuracy and computational cost . the results obtained in this study indicate that the proposed emd-sbm-fnn model using the mimo strategy is the best in terms of prediction accuracy with accredited computational load .

a decision-theoretic model for using scientific data
many artificial intelligence systems depend on the agent 's updating its beliefs about the world on the basis of experience . experiments constitute one type of experience , so scientific methodology offers a natural environment for examining the issues attendant to using this class of evidence . this paper presents a framework which structures the process of using scientific data from research reports for the purpose of making decisions , using decision analysis as the basis for the structure and using medical research as the general scientific domain . the structure extends the basic influence diagram for updating belief in an object domain parameter of interest by expanding the parameter into four parts : those of the patient , the population , the study sample , and the effective study sample . the structure uses biases to perform the transformation of one parameter into another , so that , for instance , selection biases , in concert with the population parameter , yield the study sample parameter . the influence diagram structure provides decision theoretic justification for practices of good clinical research such as randomized assignment and blindfolding of care providers . the model covers most research designs used in medicine : case-control studies , cohort studies , and controlled clinical trials , and provides an architecture to separate clearly between statistical knowledge and domain knowledge . the proposed general model can be the basis for clinical epidemiological advisory systems , when coupled with heuristic pruning of irrelevant biases ; of statistical workstations , when the computational machinery for calculation of posterior distributions is added ; and of meta-analytic reviews , when multiple studies may impact on a single population parameter .

tableau-based decision procedures for logics of strategic ability in multi-agent systems
we develop an incremental tableau-based decision procedures for the alternating-time temporal logic atl and some of its variants . while running within the theoretically established complexity upper bound , we claim that our tableau is practically more efficient in the average case than other decision procedures for atl known so far . besides , the ease of its adaptation to variants of atl demonstrates the flexibility of the proposed procedure .

interpretable deep convolutional neural networks via meta-learning
model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model 's outputs . the recent movement for `` algorithmic fairness '' also stipulates explainability , and therefore interpretability of learning models . and yet the most successful contemporary machine learning approaches , the deep neural networks , produce models that are highly non-interpretable . we attempt to address this challenge by proposing a technique called cnn-inte to interpret deep convolutional neural networks ( cnn ) via meta-learning . in this work , we interpret a specific hidden layer of the deep cnn model on the mnist image dataset . we use a clustering algorithm in a two-level structure to find the meta-level training data and random forest as base learning algorithms to generate the meta-level test data . the interpretation results are displayed visually via diagrams , which clearly indicates how a specific test instance is classified . our method achieves global interpretation for all the test instances without sacrificing the accuracy obtained by the original deep cnn model . this means our model is faithful to the deep cnn model , which leads to reliable interpretations .

advances in artificial intelligence : are you sure , we are on the right track ?
over the past decade , ai has made a remarkable progress . it is agreed that this is due to the recently revived deep learning technology . deep learning enables to process large amounts of data using simplified neuron networks that simulate the way in which the brain works . however , there is a different point of view , which posits that the brain is processing information , not data . this unresolved duality hampered ai progress for years . in this paper , i propose a notion of integrated information that hopefully will resolve the problem . i consider integrated information as a coupling between two separate entities - physical information ( that implies data processing ) and semantic information ( that provides physical information interpretation ) . in this regard , intelligence becomes a product of information processing . extending further this line of thinking , it can be said that information processing does not require more a human brain for its implementation . indeed , bacteria and amoebas exhibit intelligent behavior without any sign of a brain . that dramatically removes the need for ai systems to emulate the human brain complexity ! the paper tries to explore this shift in ai systems design philosophy .

transition systems for model generators - a unifying approach
a fundamental task for propositional logic is to compute models of propositional formulas . programs developed for this task are called satisfiability solvers . we show that transition systems introduced by nieuwenhuis , oliveras , and tinelli to model and analyze satisfiability solvers can be adapted for solvers developed for two other propositional formalisms : logic programming under the answer-set semantics , and the logic pc ( id ) . we show that in each case the task of computing models can be seen as `` satisfiability modulo answer-set programming , '' where the goal is to find a model of a theory that also is an answer set of a certain program . the unifying perspective we develop shows , in particular , that solvers clasp and minisatid are closely related despite being developed for different formalisms , one for answer-set programming and the latter for the logic pc ( id ) .

inter-session modeling for session-based recommendation
in recent years , research has been done on applying recurrent neural networks ( rnns ) as recommender systems . results have been promising , especially in the session-based setting where rnns have been shown to outperform state-of-the-art models . in many of these experiments , the rnn could potentially improve the recommendations by utilizing information about the user 's past sessions , in addition to its own interactions in the current session . a problem for session-based recommendation , is how to produce accurate recommendations at the start of a session , before the system has learned much about the user 's current interests . we propose a novel approach that extends a rnn recommender to be able to process the user 's recent sessions , in order to improve recommendations . this is done by using a second rnn to learn from recent sessions , and predict the user 's interest in the current session . by feeding this information to the original rnn , it is able to improve its recommendations . our experiments on two different datasets show that the proposed approach can significantly improve recommendations throughout the sessions , compared to a single rnn working only on the current session . the proposed model especially improves recommendations at the start of sessions , and is therefore able to deal with the cold start problem within sessions .

ask , and shall you receive ? : understanding desire fulfillment in natural language text
the ability to comprehend wishes or desires and their fulfillment is important to natural language understanding . this paper introduces the task of identifying if a desire expressed by a subject in a given short piece of text was fulfilled . we propose various unstructured and structured models that capture fulfillment cues such as the subject 's emotional state and actions . our experiments with two different datasets demonstrate the importance of understanding the narrative and discourse structure to address this task .

sentence based semantic similarity measure for blog-posts
blogs-online digital diary like application on web 2.0 has opened new and easy way to voice opinion , thoughts , and like-dislike of every internet user to the world . blogosphere has no doubt the largest user-generated content repository full of knowledge . the potential of this knowledge is still to be explored . knowledge discovery from this new genre is quite difficult and challenging as it is totally different from other popular genre of web-applications like world wide web ( www ) . blog-posts unlike web documents are small in size , thus lack in context and contain relaxed grammatical structures . hence , standard text similarity measure fails to provide good results . in this paper , specialized requirements for comparing a pair of blog-posts is thoroughly investigated . based on this we proposed a novel algorithm for sentence oriented semantic similarity measure of a pair of blog-posts . we applied this algorithm on a subset of political blogosphere of pakistan , to cluster the blogs on different issues of political realm and to identify the influential bloggers .

neural networks models for entity discovery and linking
this paper describes the ustc_nelslip systems submitted to the trilingual entity detection and linking ( edl ) track in 2016 tac knowledge base population ( kbp ) contests . we have built two systems for entity discovery and mention detection ( md ) : one uses the conditional rnnlm and the other one uses the attention-based encoder-decoder framework . the entity linking ( el ) system consists of two modules : a rule based candidate generation and a neural networks probability ranking model . moreover , some simple string matching rules are used for nil clustering . at the end , our best system has achieved an f1 score of 0.624 in the end-to-end typed mention ceaf plus metric .

agent-oriented approach for detecting and managing risks in emergency situations
this paper presents an agent-oriented approach to build a decision support system aimed at helping emergency managers to detect and to manage risks . we stress the flexibility and the adaptivity characteristics that are crucial to build a robust and efficient system , able to resolve complex problems . the system should be independent as much as possible from the subject of study . thereby , an original approach based on a mechanism of perception , representation , characterisation and assessment is proposed . the work described here is applied on the robocuprescue application . experimentations and results are provided .

segmentation of offline handwritten bengali script
character segmentation has long been one of the most critical areas of optical character recognition process . through this operation , an image of a sequence of characters , which may be connected in some cases , is decomposed into sub-images of individual alphabetic symbols . in this paper , segmentation of cursive handwritten script of world 's fourth popular language , bengali , is considered . unlike english script , bengali handwritten characters and its components often encircle the main character , making the conventional segmentation methodologies inapplicable . experimental results , using the proposed segmentation technique , on sample cursive handwritten data containing 218 ideal segmentation points show a success rate of 97.7 % . further feature-analysis on these segments may lead to actual recognition of handwritten cursive bengali script .

identifying diabetic patients with high risk of readmission
hospital readmissions are expensive and reflect the inadequacies in healthcare system . in the united states alone , treatment of readmitted diabetic patients exceeds 250 million dollars per year . early identification of patients facing a high risk of readmission can enable healthcare providers to to conduct additional investigations and possibly prevent future readmissions . this not only improves the quality of care but also reduces the medical expenses on readmission . machine learning methods have been leveraged on public health data to build a system for identifying diabetic patients facing a high risk of future readmission . number of inpatient visits , discharge disposition and admission type were identified as strong predictors of readmission . further , it was found that the number of laboratory tests and discharge disposition together predict whether the patient will be readmitted shortly after being discharged from the hospital ( i.e . < 30 days ) or after a longer period of time ( i.e . > 30 days ) . these insights can help healthcare providers to improve inpatient diabetic care . finally , the cost analysis suggests that \ $ 252.76 million can be saved across 98,053 diabetic patient encounters by incorporating the proposed cost sensitive analysis model .

how will the internet of things enable augmented personalized health ?
internet-of-things ( iot ) is profoundly redefining the way we create , consume , and share information . health aficionados and citizens are increasingly using iot technologies to track their sleep , food intake , activity , vital body signals , and other physiological observations . this is complemented by iot systems that continuously collect health-related data from the environment and inside the living quarters . together , these have created an opportunity for a new generation of healthcare solutions . however , interpreting data to understand an individual 's health is challenging . it is usually necessary to look at that individual 's clinical record and behavioral information , as well as social and environmental information affecting that individual . interpreting how well a patient is doing also requires looking at his adherence to respective health objectives , application of relevant clinical knowledge and the desired outcomes . we resort to the vision of augmented personalized healthcare ( aph ) to exploit the extensive variety of relevant data and medical knowledge using artificial intelligence ( ai ) techniques to extend and enhance human health to presents various stages of augmented health management strategies : self-monitoring , self-appraisal , self-management , intervention , and disease progress tracking and prediction . khealth technology , a specific incarnation of aph , and its application to asthma and other diseases are used to provide illustrations and discuss alternatives for technology-assisted health management . several prominent efforts involving iot and patient-generated health data ( pghd ) with respect converting multimodal data into actionable information ( big data to smart data ) are also identified . roles of three components in an evidence-based semantic perception approach- contextualization , abstraction , and personalization are discussed .

quantum decision theory as quantum theory of measurement
we present a general theory of quantum information processing devices , that can be applied to human decision makers , to atomic multimode registers , or to molecular high-spin registers . our quantum decision theory is a generalization of the quantum theory of measurement , endowed with an action ring , a prospect lattice and a probability operator measure . the algebra of probability operators plays the role of the algebra of local observables . because of the composite nature of prospects and of the entangling properties of the probability operators , quantum interference terms appear , which make actions noncommutative and the prospect probabilities non-additive . the theory provides the basis for explaining a variety of paradoxes typical of the application of classical utility theory to real human decision making . the principal advantage of our approach is that it is formulated as a self-consistent mathematical theory , which allows us to explain not just one effect but actually all known paradoxes in human decision making . being general , the approach can serve as a tool for characterizing quantum information processing by means of atomic , molecular , and condensed-matter systems .

asymmetric action abstractions for multi-unit control in adversarial real-time games
action abstractions restrict the number of legal actions available during search in multi-unit real-time adversarial games , thus allowing algorithms to focus their search on a set of promising actions . optimal strategies derived from un-abstracted spaces are guaranteed to be no worse than optimal strategies derived from action-abstracted spaces . in practice , however , due to real-time constraints and the state space size , one is only able to derive good strategies in un-abstracted spaces in small-scale games . in this paper we introduce search algorithms that use an action abstraction scheme we call asymmetric abstraction . asymmetric abstractions retain the un-abstracted spaces ' theoretical advantage over regularly abstracted spaces while still allowing the search algorithms to derive effective strategies , even in large-scale games . empirical results on combat scenarios that arise in a real-time strategy game show that our search algorithms are able to substantially outperform state-of-the-art approaches .

query answering over contextualized rdf/owl knowledge with forall-existential bridge rules : attaining decidability using acyclicity ( full version )
the recent outburst of context-dependent knowledge on the semantic web ( sw ) has led to the realization of the importance of the quads in the sw community . quads , which extend a standard rdf triple , by adding a new parameter of the ` context ' of an rdf triple , thus informs a reasoner to distinguish between the knowledge in various contexts . although this distinction separates the triples in an rdf graph into various contexts , and allows the reasoning to be decoupled across various contexts , bridge rules need to be provided for inter-operating the knowledge across these contexts . we call a set of quads together with the bridge rules , a quad-system . in this paper , we discuss the problem of query answering over quad-systems with expressive forall-existential bridge rules . it turns out the query answering over quad-systems is undecidable , in general . we derive a decidable class of quad-systems , namely context-acyclic quad-systems , for which query answering can be done using forward chaining . tight bounds for data and combined complexity of query entailment has been established for the derived class .

algorithms and limits for compact plan representations
compact representations of objects is a common concept in computer science . automated planning can be viewed as a case of this concept : a planning instance is a compact implicit representation of a graph and the problem is to find a path ( a plan ) in this graph . while the graphs themselves are represented compactly as planning instances , the paths are usually represented explicitly as sequences of actions . some cases are known where the plans always have compact representations , for example , using macros . we show that these results do not extend to the general case , by proving a number of bounds for compact representations of plans under various criteria , like efficient sequential or random access of actions . in addition to this , we show that our results have consequences for what can be gained from reformulating planning into some other problem . as a contrast to this we also prove a number of positive results , demonstrating restricted cases where plans do have useful compact representations , as well as proving that macro plans have favourable access properties . our results are finally discussed in relation to other relevant contexts .

using atl to define advanced and flexible constraint model transformations
transforming constraint models is an important task in re- cent constraint programming systems . user-understandable models are defined during the modeling phase but rewriting or tuning them is manda- tory to get solving-efficient models . we propose a new architecture al- lowing to define bridges between any ( modeling or solver ) languages and to implement model optimizations . this architecture follows a model- driven approach where the constraint modeling process is seen as a set of model transformations . among others , an interesting feature is the def- inition of transformations as concept-oriented rules , i.e . based on types of model elements where the types are organized into a hierarchy called a metamodel .

a framework for generalizing graph-based representation learning methods
random walks are at the heart of many existing deep learning algorithms for graph data . however , such algorithms have many limitations that arise from the use of random walks , e.g. , the features resulting from these methods are unable to transfer to new nodes and graphs as they are tied to node identity . in this work , we introduce the notion of attributed random walks which serves as a basis for generalizing existing methods such as deepwalk , node2vec , and many others that leverage random walks . our proposed framework enables these methods to be more widely applicable for both transductive and inductive learning as well as for use on graphs with attributes ( if available ) . this is achieved by learning functions that generalize to new nodes and graphs . we show that our proposed framework is effective with an average auc improvement of 16.1 % while requiring on average 853 times less space than existing methods on a variety of graphs from several domains .

ranking algorithms by performance
a common way of doing algorithm selection is to train a machine learning model and predict the best algorithm from a portfolio to solve a particular problem . while this method has been highly successful , choosing only a single algorithm has inherent limitations -- if the choice was bad , no remedial action can be taken and parallelism can not be exploited , to name but a few problems . in this paper , we investigate how to predict the ranking of the portfolio algorithms on a particular problem . this information can be used to choose the single best algorithm , but also to allocate resources to the algorithms according to their rank . we evaluate a range of approaches to predict the ranking of a set of algorithms on a problem . we furthermore introduce a framework for categorizing ranking predictions that allows to judge the expressiveness of the predictive output . our experimental evaluation demonstrates on a range of data sets from the literature that it is beneficial to consider the relationship between algorithms when predicting rankings . we furthermore show that relatively naive approaches deliver rankings of good quality already .

beyond opening up the black box : investigating the role of algorithmic systems in wikipedian organizational culture
scholars and practitioners across domains are increasingly concerned with algorithmic transparency and opacity , interrogating the values and assumptions embedded in automated , black-boxed systems , particularly in user-generated content platforms . i report from an ethnography of infrastructure in wikipedia to discuss an often understudied aspect of this topic : the local , contextual , learned expertise involved in participating in a highly automated social-technical environment . today , the organizational culture of wikipedia is deeply intertwined with various data-driven algorithmic systems , which wikipedians rely on to help manage and govern the `` anyone can edit '' encyclopedia at a massive scale . these bots , scripts , tools , plugins , and dashboards make wikipedia more efficient for those who know how to work with them , but like all organizational culture , newcomers must learn them if they want to fully participate . i illustrate how cultural and organizational expertise is enacted around algorithmic agents by discussing two autoethnographic vignettes , which relate my personal experience as a veteran in wikipedia . i present thick descriptions of how governance and gatekeeping practices are articulated through and in alignment with these automated infrastructures . over the past 15 years , wikipedian veterans and administrators have made specific decisions to support administrative and editorial workflows with automation in particular ways and not others . i use these cases of wikipedia 's bot-supported bureaucracy to discuss several issues in the fields of critical algorithms studies , critical data studies , and fairness , accountability , and transparency in machine learning -- most principally arguing that scholarship and practice must go beyond trying to `` open up the black box '' of such systems and also examine sociocultural processes like newcomer socialization .

a fuzzy ahp approach for supplier selection problem : a case study in a gear motor company
suuplier selection is one of the most important functions of a purchasing department . since by deciding the best supplier , companies can save material costs and increase competitive advantage.however this decision becomes compilcated in case of multiple suppliers , multiple conflicting criteria , and imprecise parameters . in addition the uncertainty and vagueness of the experts ' opinion is the prominent characteristic of the problem . therefore an extensively used multi criteria decision making tool fuzzy ahp can be utilized as an approach for supplier selection problem . this paper reveals the application of fuzzy ahp in a gear motor company determining the best supplier with respect to selected criteria . the contribution of this study is not only the application of the fuzzy ahp methodology for supplier selection problem , but also releasing a comprehensive literature review of multi criteria decision making problems . in addition by stating the steps of fuzzy ahp clearly and numerically , this study can be a guide of the methodology to be implemented to other multiple criteria decision making problems .

a logic for global and local announcements
in this paper we introduce { \em global and local announcement logic } ( glal ) , a dynamic epistemic logic with two distinct announcement operators -- $ [ \phi ] ^+_a $ and $ [ \phi ] ^-_a $ indexed to a subset $ a $ of the set $ ag $ of all agents -- for global and local announcements respectively . the boundary case $ [ \phi ] ^+_ { ag } $ corresponds to the public announcement of $ \phi $ , as known from the literature . unlike standard public announcements , which are { \em model transformers } , the global and local announcements are { \em pointed model transformers } . in particular , the update induced by the announcement may be different in different states of the model . therefore , the resulting computations are trees of models , rather than the typical sequences . a consequence of our semantics is that modally bisimilar states may be distinguished in our logic . then , we provide a stronger notion of bisimilarity and we show that it preserves modal equivalence in glal . additionally , we show that glal is strictly more expressive than public announcement logic with common knowledge . we prove a wide range of validities for glal involving the interaction between dynamics and knowledge , and show that the satisfiability problem for glal is decidable . we illustrate the formal machinery by means of detailed epistemic scenarios .

a reflection on the structure and process of the web of data
the web community has introduced a set of standards and technologies for representing , querying , and manipulating a globally distributed data structure known as the web of data . the proponents of the web of data envision much of the world 's data being interrelated and openly accessible to the general public . this vision is analogous in many ways to the web of documents of common knowledge , but instead of making documents and media openly accessible , the focus is on making data openly accessible . in providing data for public use , there has been a stimulated interest in a movement dubbed open data . open data is analogous in many ways to the open source movement . however , instead of focusing on software , open data is focused on the legal and licensing issues around publicly exposed data . together , various technological and legal tools are laying the groundwork for the future of global-scale data management on the web . as of today , in its early form , the web of data hosts a variety of data sets that include encyclopedic facts , drug and protein data , metadata on music , books and scholarly articles , social network representations , geospatial information , and many other types of information . the size and diversity of the web of data is a demonstration of the flexibility of the underlying standards and the overall feasibility of the project as a whole . the purpose of this article is to provide a review of the technological underpinnings of the web of data as well as some of the hurdles that need to be overcome if the web of data is to emerge as the defacto medium for data representation , distribution , and ultimately , processing .

fast k-nearest neighbour search via prioritized dci
most exact methods for k-nearest neighbour search suffer from the curse of dimensionality ; that is , their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality . dynamic continuous indexing ( dci ) offers a promising way of circumventing the curse and successfully reduces the dependence of query time on intrinsic dimensionality from exponential to sublinear . in this paper , we propose a variant of dci , which we call prioritized dci , and show a remarkable improvement in the dependence of query time on intrinsic dimensionality . in particular , a linear increase in intrinsic dimensionality , or equivalently , an exponential increase in the number of points near a query , can be mostly counteracted with just a linear increase in space . we also demonstrate empirically that prioritized dci significantly outperforms prior methods . in particular , relative to locality-sensitive hashing ( lsh ) , prioritized dci reduces the number of distance evaluations by a factor of 14 to 116 and the memory consumption by a factor of 21 .

a hidden absorbing semi-markov model for informatively censored temporal data : learning and inference
modeling continuous-time physiological processes that manifest a patient 's evolving clinical states is a key step in approaching many problems in healthcare . in this paper , we develop the hidden absorbing semi-markov model ( hasmm ) : a versatile probabilistic model that is capable of capturing the modern electronic health record ( ehr ) data . unlike exist- ing models , an hasmm accommodates irregularly sampled , temporally correlated , and informatively censored physiological data , and can describe non-stationary clinical state transitions . learning an hasmm from the ehr data is achieved via a novel forward- filtering backward-sampling monte-carlo em algorithm that exploits the knowledge of the end-point clinical outcomes ( informative censoring ) in the ehr data , and implements the e-step by sequentially sampling the patients ' clinical states in the reverse-time direction while conditioning on the future states . real-time inferences are drawn via a forward- filtering algorithm that operates on a virtually constructed discrete-time embedded markov chain that mirrors the patient 's continuous-time state trajectory . we demonstrate the di- agnostic and prognostic utility of the hasmm in a critical care prognosis setting using a real-world dataset for patients admitted to the ronald reagan ucla medical center .

discretization-free knowledge gradient methods for bayesian optimization
this paper studies bayesian ranking and selection ( r & s ) problems with correlated prior beliefs and continuous domains , i.e . bayesian optimization ( bo ) . knowledge gradient methods [ frazier et al. , 2008 , 2009 ] have been widely studied for discrete r & s problems , which sample the one-step bayes-optimal point . when used over continuous domains , previous work on the knowledge gradient [ scott et al. , 2011 , wu and frazier , 2016 , wu et al. , 2017 ] often rely on a discretized finite approximation . however , the discretization introduces error and scales poorly as the dimension of domain grows . in this paper , we develop a fast discretization-free knowledge gradient method for bayesian optimization . our method is not restricted to the fully sequential setting , but useful in all settings where knowledge gradient can be used over continuous domains . we show how our method can be generalized to handle ( i ) batch of points suggestion ( parallel knowledge gradient ) ; ( ii ) the setting where derivative information is available in the optimization process ( derivative-enabled knowledge gradient ) . in numerical experiments , we demonstrate that the discretization-free knowledge gradient method finds global optima significantly faster than previous bayesian optimization algorithms on both synthetic test functions and real-world applications , especially when function evaluations are noisy ; and derivative-enabled knowledge gradient can further improve the performances , even outperforming the gradient-based optimizer such as bfgs when derivative information is available .

visualizing and understanding atari agents
deep reinforcement learning ( deep rl ) agents have achieved remarkable success in a broad range of game-playing and continuous control tasks . while these agents are effective at maximizing rewards , it is often unclear what strategies they use to do so . in this paper , we take a step toward explaining deep rl agents through a case study in three atari 2600 environments . in particular , we focus on understanding agents in terms of their visual attentional patterns during decision making . to this end , we introduce a method for generating rich saliency maps and use it to explain 1 ) what strong agents attend to 2 ) whether agents are making decisions for the right or wrong reasons , and 3 ) how agents evolve during the learning phase . we also test our method on non-expert human subjects and find that it improves their ability to reason about these agents . our techniques are general and , though we focus on atari , our long-term objective is to produce tools that explain any deep rl policy .

a framework for decision-theoretic planning i : combining the situation calculus , conditional plans , probability and utility
this paper shows how we can combine logical representations of actions and decision theory in such a manner that seems natural for both . in particular we assume an axiomatization of the domain in terms of situation calculus , using what is essentially reiter 's solution to the frame problem , in terms of the completion of the axioms defining the state change . uncertainty is handled in terms of the independent choice logic , which allows for independent choices and a logic program that gives the consequences of the choices . as part of the consequences are a specification of the utility of ( final ) states . the robot adopts robot plans , similar to the golog programming language . within this logic , we can define the expected utility of a conditional plan , based on the axiomatization of the actions , the uncertainty and the utility . the ? planning ' problem is to find the plan with the highest expected utility . this is related to recent structured representations for pomdps ; here we use stochastic situation calculus rules to specify the state transition function and the reward/value function . finally we show that with stochastic frame axioms , actions representations in probabilistic strips are exponentially larger than using the representation proposed here .

decision theory with prospect interference and entanglement
we present a novel variant of decision making based on the mathematical theory of separable hilbert spaces . this mathematical structure captures the effect of superposition of composite prospects , including many incorporated intentions , which allows us to describe a variety of interesting fallacies and anomalies that have been reported to particularize the decision making of real human beings . the theory characterizes entangled decision making , non-commutativity of subsequent decisions , and intention interference . we demonstrate how the violation of the savage 's sure-thing principle , known as the disjunction effect , can be explained quantitatively as a result of the interference of intentions , when making decisions under uncertainty . the disjunction effects , observed in experiments , are accurately predicted using a theorem on interference alternation that we derive , which connects aversion-to-uncertainty to the appearance of negative interference terms suppressing the probability of actions . the conjunction fallacy is also explained by the presence of the interference terms . a series of experiments are analysed and shown to be in excellent agreement with a priori evaluation of interference effects . the conjunction fallacy is also shown to be a sufficient condition for the disjunction effect and novel experiments testing the combined interplay between the two effects are suggested .

capacity and trainability in recurrent neural networks
two potential bottlenecks on the expressiveness of recurrent neural networks ( rnns ) are their ability to store information about the task in their parameters , and to store information about the input history in their units . we show experimentally that all common rnn architectures achieve nearly the same per-task and per-unit capacity bounds with careful training , for a variety of tasks and stacking depths . they can store an amount of task information which is linear in the number of parameters , and is approximately 5 bits per parameter . they can additionally store approximately one real number from their input history per hidden unit . we further find that for several tasks it is the per-task parameter capacity bound that determines performance . these results suggest that many previous results comparing rnn architectures are driven primarily by differences in training effectiveness , rather than differences in capacity . supporting this observation , we compare training difficulty for several architectures , and show that vanilla rnns are far more difficult to train , yet have slightly higher capacity . finally , we propose two novel rnn architectures , one of which is easier to train than the lstm or gru for deeply stacked architectures .

modus ponens generating function in the class of ^-valuations of plausibility
we discuss the problem of construction of inference procedures which can manipulate with uncertainties measured in ordinal scales and fulfill to the property of strict monotonicity of conclusion . the class of a-valuations of plausibility is considered where operations based only on information about linear ordering of plausibility values are used . in this class the modus ponens generating function fulfiling to the property of strict monotonicity of conclusions is introduced .

impala : scalable distributed deep-rl with importance weighted actor-learner architectures
in this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters . a key challenge is to handle the increased amount of data and extended training time . we have developed a new distributed agent impala ( importance weighted actor-learner architecture ) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation . we achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called v-trace . we demonstrate the effectiveness of impala for multi-task reinforcement learning on dmlab-30 ( a set of 30 tasks from the deepmind lab environment ( beattie et al. , 2016 ) ) and atari-57 ( all available atari games in arcade learning environment ( bellemare et al. , 2013a ) ) . our results show that impala is able to achieve better performance than previous agents with less data , and crucially exhibits positive transfer between tasks as a result of its multi-task approach .

bisimulations for fuzzy automata
bisimulations have been widely used in many areas of computer science to model equivalence between various systems , and to reduce the number of states of these systems , whereas uniform fuzzy relations have recently been introduced as a means to model the fuzzy equivalence between elements of two possible different sets . here we use the conjunction of these two concepts as a powerful tool in the study of equivalence between fuzzy automata . we prove that a uniform fuzzy relation between fuzzy automata $ \cal a $ and $ \cal b $ is a forward bisimulation if and only if its kernel and co-kernel are forward bisimulation fuzzy equivalences on $ \cal a $ and $ \cal b $ and there is a special isomorphism between factor fuzzy automata with respect to these fuzzy equivalences . as a consequence we get that fuzzy automata $ \cal a $ and $ \cal b $ are ufb-equivalent , i.e. , there is a uniform forward bisimulation between them , if and only if there is a special isomorphism between the factor fuzzy automata of $ \cal a $ and $ \cal b $ with respect to their greatest forward bisimulation fuzzy equivalences . this result reduces the problem of testing ufb-equivalence to the problem of testing isomorphism of fuzzy automata , which is closely related to the well-known graph isomorphism problem . we prove some similar results for backward-forward bisimulations , and we point to fundamental differences . because of the duality with the studied concepts , backward and forward-backward bisimulations are not considered separately . finally , we give a comprehensive overview of various concepts on deterministic , nondeterministic , fuzzy , and weighted automata , which are related to bisimulations .

towards owl-based knowledge representation in petrology
this paper presents our work on development of owl-driven systems for formal representation and reasoning about terminological knowledge and facts in petrology . the long-term aim of our project is to provide solid foundations for a large-scale integration of various kinds of knowledge , including basic terms , rock classification algorithms , findings and reports . we describe three steps we have taken towards that goal here . first , we develop a semi-automated procedure for transforming a database of igneous rock samples to texts in a controlled natural language ( cnl ) , and then a collection of owl ontologies . second , we create an owl ontology of important petrology terms currently described in natural language thesauri . we describe a prototype of a tool for collecting definitions from domain experts . third , we present an approach to formalization of current industrial standards for classification of rock samples , which requires linear equations in owl 2. in conclusion , we discuss a range of opportunities arising from the use of semantic technologies in petrology and outline the future work in this area .

adapting heuristic mastermind strategies to evolutionary algorithms
the art of solving the mastermind puzzle was initiated by donald knuth and is already more than 30 years old ; despite that , it still receives much attention in operational research and computer games journals , not to mention the nature-inspired stochastic algorithm literature . in this paper we try to suggest a strategy that will allow nature-inspired algorithms to obtain results as good as those based on exhaustive search strategies ; in order to do that , we first review , compare and improve current approaches to solving the puzzle ; then we test one of these strategies with an estimation of distribution algorithm . finally , we try to find a strategy that falls short of being exhaustive , and is then amenable for inclusion in nature inspired algorithms ( such as evolutionary or particle swarm algorithms ) . this paper proves that by the incorporation of local entropy into the fitness function of the evolutionary algorithm it becomes a better player than a random one , and gives a rule of thumb on how to incorporate the best heuristic strategies to evolutionary algorithms without incurring in an excessive computational cost .

extremal problems in logic programming and stable model computation
we study the following problem : given a class of logic programs c , determine the maximum number of stable models of a program from c. we establish the maximum for the class of all logic programs with at most n clauses , and for the class of all logic programs of size at most n. we also characterize the programs for which the maxima are attained . we obtain similar results for the class of all disjunctive logic programs with at most n clauses , each of length at most m , and for the class of all disjunctive logic programs of size at most n. our results on logic programs have direct implication for the design of algorithms to compute stable models . several such algorithms , similar in spirit to the davis-putnam procedure , are described in the paper . our results imply that there is an algorithm that finds all stable models of a program with n clauses after considering the search space of size o ( 3^ { n/3 } ) in the worst case . our results also provide some insights into the question of representability of families of sets as families of stable models of logic programs .

joint causal inference from observational and experimental datasets
we introduce joint causal inference ( jci ) , a powerful formulation of causal discovery from multiple datasets that allows to jointly learn both the causal structure and targets of interventions from statistical independences in pooled data . compared with existing constraint-based approaches for causal discovery from multiple data sets , jci offers several advantages : it allows for several different types of interventions in a unified fashion , it can learn intervention targets , it systematically pools data across different datasets which improves the statistical power of independence tests , and most importantly , it improves on the accuracy and identifiability of the predicted causal relations . a technical complication that arises in jci is the occurrence of faithfulness violations due to deterministic relations . we propose a simple but effective strategy for dealing with this type of faithfulness violations . we implement it in acid , a determinism-tolerant extension of ancestral causal inference ( aci ) ( magliacane et al. , 2016 ) , a recently proposed logic-based causal discovery method that improves reliability of the output by exploiting redundant information in the data . we illustrate the benefits of jci with acid with an evaluation on a simulated dataset .

defeasible decisions : what the proposal is and is n't
in two recent papers , i have proposed a description of decision analysis that differs from the bayesian picture painted by savage , jeffrey and other classic authors . response to this view has been either overly enthusiastic or unduly pessimistic . in this paper i try to place the idea in its proper place , which must be somewhere in between . looking at decision analysis as defeasible reasoning produces a framework in which planning and decision theory can be integrated , but work on the details has barely begun . it also produces a framework in which the meta-decision regress can be stopped in a reasonable way , but it does not allow us to ignore meta-level decisions . the heuristics for producing arguments that i have presented are only supposed to be suggestive ; but they are not open to the egregious errors about which some have worried . and though the idea is familiar to those who have studied heuristic search , it is somewhat richer because the control of dialectic is more interesting than the deepening of search .

atpboost : learning premise selection in binary setting with atp feedback
atpboost is a system for solving sets of large-theory problems by interleaving atp runs with state-of-the-art machine learning of premise selection from the proofs . unlike many previous approaches that use multi-label setting , the learning is implemented as binary classification that estimates the pairwise-relevance of ( theorem , premise ) pairs . atpboost uses for this the xgboost gradient boosting algorithm , which is fast and has state-of-the-art performance on many tasks . learning in the binary setting however requires negative examples , which is nontrivial due to many alternative proofs . we discuss and implement several solutions in the context of the atp/ml feedback loop , and show that atpboost with such methods significantly outperforms the k-nearest neighbors multilabel classifier .

network model selection using task-focused minimum description length
networks are fundamental models for data used in practically every application domain . in most instances , several implicit or explicit choices about the network definition impact the translation of underlying data to a network representation , and the subsequent question ( s ) about the underlying system being represented . users of downstream network data may not even be aware of these choices or their impacts . we propose a task-focused network model selection methodology which addresses several key challenges . our approach constructs network models from underlying data and uses minimum description length ( mdl ) criteria for selection . our methodology measures efficiency , a general and comparable measure of the network 's performance of a local ( i.e . node-level ) predictive task of interest . selection on efficiency favors parsimonious ( e.g . sparse ) models to avoid overfitting and can be applied across arbitrary tasks and representations . we show stability , sensitivity , and significance testing in our methodology .

pbm : a new dataset for blog mining
text mining is becoming vital as web 2.0 offers collaborative content creation and sharing . now researchers have growing interest in text mining methods for discovering knowledge . text mining researchers come from variety of areas like : natural language processing , computational linguistic , machine learning , and statistics . a typical text mining application involves preprocessing of text , stemming and lemmatization , tagging and annotation , deriving knowledge patterns , evaluating and interpreting the results . there are numerous approaches for performing text mining tasks , like : clustering , categorization , sentimental analysis , and summarization . there is a growing need to standardize the evaluation of these tasks . one major component of establishing standardization is to provide standard datasets for these tasks . although there are various standard datasets available for traditional text mining tasks , but there are very few and expensive datasets for blog-mining task . blogs , a new genre in web 2.0 is a digital diary of web user , which has chronological entries and contains a lot of useful knowledge , thus offers a lot of challenges and opportunities for text mining . in this paper , we report a new indigenous dataset for pakistani political blogosphere . the paper describes the process of data collection , organization , and standardization . we have used this dataset for carrying out various text mining tasks for blogosphere , like : blog-search , political sentiments analysis and tracking , identification of influential blogger , and clustering of the blog-posts . we wish to offer this dataset free for others who aspire to pursue further in this domain .

learning to cluster in order to transfer across domains and tasks
this paper introduces a novel method to perform transfer learning across domains and tasks , formulating it as a problem of learning to cluster . the key insight is that , in addition to features , we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning . we begin by reducing categorical information to pairwise constraints , which only considers whether two instances belong to the same class or not . this similarity is category-agnostic and can be learned from data in the source domain using a similarity network . we then present two novel approaches for performing transfer learning using this similarity function . first , for unsupervised domain adaptation , we design a new loss function to regularize classification with a constrained clustering loss , hence learning a clustering network with the transferred similarity metric generating the training inputs . second , for cross-task learning ( i.e. , unsupervised clustering with unseen categories ) , we propose a framework to reconstruct and estimate the number of semantic clusters , again using the clustering network . since the similarity network is noisy , the key is to use a robust clustering algorithm , and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches . using this method , we first show state of the art results for the challenging cross-task problem , applied on omniglot and imagenet . our results show that we can reconstruct semantic clusters with high accuracy . we then evaluate the performance of cross-domain transfer using images from the office-31 and svhn-mnist tasks and present top accuracy on both datasets . our approach does n't explicitly deal with domain discrepancy . if we combine with a domain adaptation loss , it shows further improvement .

constraint-free natural image reconstruction from fmri signals based on convolutional neural network
in recent years , research on decoding brain activity based on functional magnetic resonance imaging ( fmri ) has made remarkable achievements . however , constraint-free natural image reconstruction from brain activity is still a challenge . the existing methods simplified the problem by using semantic prior information or just reconstructing simple images such as letters and digitals . without semantic prior information , we present a novel method to reconstruct nature images from fmri signals of human visual cortex based on the computation model of convolutional neural network ( cnn ) . firstly , we extracted the units output of viewed natural images in each layer of a pre-trained cnn as cnn features . secondly , we transformed image reconstruction from fmri signals into the problem of cnn feature visualizations by training a sparse linear regression to map from the fmri patterns to cnn features . by iteratively optimization to find the matched image , whose cnn unit features become most similar to those predicted from the brain activity , we finally achieved the promising results for the challenging constraint-free natural image reconstruction . as there was no use of semantic prior information of the stimuli when training decoding model , any category of images ( not constraint by the training set ) could be reconstructed theoretically . we found that the reconstructed images resembled the natural stimuli , especially in position and shape . the experimental results suggest that hierarchical visual features can effectively express the visual perception process of human brain .

learning to represent mechanics via long-term extrapolation and interpolation
while the basic laws of newtonian mechanics are well understood , explaining a physical scenario still requires manually modeling the problem with suitable equations and associated parameters . in order to adopt such models for artificial intelligence , researchers have handcrafted the relevant states , and then used neural networks to learn the state transitions using simulation runs as training data . unfortunately , such approaches can be unsuitable for modeling complex real-world scenarios , where manually authoring relevant state spaces tend to be challenging . in this work , we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data , and thus enable long-term physical extrapolation . we develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates . we evaluate our setup to extrapolate motion of a rolling ball on bowl of varying shape and orientation using only images as input , and report competitive results with approaches that assume access to internal physics models and parameters .

novel model-based heuristics for energy optimal motion planning of an autonomous vehicle using a*
predictive motion planning is the key to achieve energy-efficient driving , which is one of the main benefits of automated driving . researchers have been studying the planning of velocity trajectories , a simpler form of motion planning , for over a decade now and many different methods are available . dynamic programming has shown to be the most common choice due to its numerical background and ability to include nonlinear constraints and models . although planning of optimal trajectory is done in a systematic way , dynamic programming does n't use any knowledge about the considered problem to guide the exploration and therefore explores all possible trajectories . a* is an algorithm which enables using knowledge about the problem to guide the exploration to the most promising solutions first . knowledge has to be represented in a form of a heuristic function , which gives an optimistic estimate of cost for transitioning between two states , which is not a straightforward task . this paper presents a novel heuristics incorporating air drag and auxiliary power as well as operational costs of the vehicle , besides kinetic and potential energy and rolling resistance known in the literature . furthermore , optimal cruising velocity , which depends on vehicle aerodynamic properties and auxiliary power , is derived . results are compared for different variants of heuristic functions and dynamic programming as well .

combined task and motion planning as classical ai planning
planning in robotics is often split into task and motion planning . the high-level , symbolic task planner decides what needs to be done , while the motion planner checks feasibility and fills up geometric detail . it is known however that such a decomposition is not effective in general as the symbolic and geometrical components are not independent . in this work , we show that it is possible to compile task and motion planning problems into classical ai planning problems ; i.e. , planning problems over finite and discrete state spaces with a known initial state , deterministic actions , and goal states to be reached . the compilation is sound , meaning that classical plans are valid robot plans , and probabilistically complete , meaning that valid robot plans are classical plans when a sufficient number of configurations is sampled . in this approach , motion planners and collision checkers are used for the compilation , but not at planning time . the key elements that make the approach effective are 1 ) expressive classical ai planning languages for representing the compiled problems in compact form , that unlike pddl make use of functions and state constraints , and 2 ) general width-based search algorithms capable of finding plans over huge combinatorial spaces using weak heuristics only . empirical results are presented for a pr2 robot manipulating tens of objects , for which long plans are required .

efficient independence-based map approach for robust markov networks structure discovery
this work introduces the ib-score , a family of independence-based score functions for robust learning of markov networks independence structures . markov networks are a widely used graphical representation of probability distributions , with many applications in several fields of science . the main advantage of the ib-score is the possibility of computing it without the need of estimation of the numerical parameters , an np-hard problem , usually solved through an approximate , data-intensive , iterative optimization . we derive a formal expression for the ib-score from first principles , mainly maximum a posteriori and conditional independence properties , and exemplify several instantiations of it , resulting in two novel algorithms for structure learning : ibmap-hc and ibmap-ts . experimental results over both artificial and real world data show these algorithms achieve important error reductions in the learnt structures when compared with the state-of-the-art independence-based structure learning algorithm gsmn , achieving increments of more than 50 % in the amount of independencies they encode correctly , and in some cases , learning correctly over 90 % of the edges that gsmn learnt incorrectly . theoretical analysis shows ibmap-hc proceeds efficiently , achieving these improvements in a time polynomial to the number of random variables in the domain .

causes and explanations : a structural-model approach . part ii : explanations
we propose new definitions of ( causal ) explanation , using structural equations to model counterfactuals . the definition is based on the notion of actual cause , as defined and motivated in a companion paper . essentially , an explanation is a fact that is not known for certain but , if found to be true , would constitute an actual cause of the fact to be explained , regardless of the agent 's initial uncertainty . we show that the definition handles well a number of problematic examples from the literature .

transferring agent behaviors from videos via motion gans
a major bottleneck for developing general reinforcement learning agents is determining rewards that will yield desirable behaviors under various circumstances . we introduce a general mechanism for automatically specifying meaningful behaviors from raw pixels . in particular , we train a generative adversarial network to produce short sub-goals represented through motion templates . we demonstrate that this approach generates visually meaningful behaviors in unknown environments with novel agents and describe how these motions can be used to train reinforcement learning agents .

adjacency-faithfulness and conservative causal inference
most causal inference algorithms in the literature ( e.g. , pearl ( 2000 ) , spirtes et al . ( 2000 ) , heckerman et al . ( 1999 ) ) exploit an assumption usually referred to as the causal faithfulness or stability condition . in this paper , we highlight two components of the condition used in constraint-based algorithms , which we call `` adjacency-faithfulness '' and `` orientation-faithfulness '' . we point out that assuming adjacency-faithfulness is true , it is in principle possible to test the validity of orientation-faithfulness . based on this observation , we explore the consequence of making only the adjacency-faithfulness assumption . we show that the familiar pc algorithm has to be modified to be ( asymptotically ) correct under the weaker , adjacency-faithfulness assumption . roughly the modified algorithm , called conservative pc ( cpc ) , checks whether orientation-faithfulness holds in the orientation phase , and if not , avoids drawing certain causal conclusions the pc algorithm would draw . however , if the stronger , standard causal faithfulness condition actually obtains , the cpc algorithm is shown to output the same pattern as the pc algorithm does in the large sample limit . we also present a simulation study showing that the cpc algorithm runs almost as fast as the pc algorithm , and outputs significantly fewer false causal arrowheads than the pc algorithm does on realistic sample sizes . we end our paper by discussing how score-based algorithms such as ges perform when the adjacency-faithfulness but not the standard causal faithfulness condition holds , and how to extend our work to the fci algorithm , which allows for the possibility of latent variables .

feature base fusion for splicing forgery detection based on neuro fuzzy
most of researches on image forensics have been mainly focused on detection of artifacts introduced by a single processing tool . they lead in the development of many specialized algorithms looking for one or more particular footprints under specific settings . naturally , the performance of such algorithms are not perfect , and accordingly the provided output might be noisy , inaccurate and only partially correct . furthermore , a forged image in practical scenarios is often the result of utilizing several tools available by image-processing software systems . therefore , reliable tamper detection requires developing more poweful tools to deal with various tempering scenarios . fusion of forgery detection tools based on fuzzy inference system has been used before for addressing this problem . adjusting the membership functions and defining proper fuzzy rules for attaining to better results are time-consuming processes . this can be accounted as main disadvantage of fuzzy inference systems . in this paper , a neuro-fuzzy inference system for fusion of forgery detection tools is developed . the neural network characteristic of these systems provides appropriate tool for automatically adjusting the membership functions . moreover , initial fuzzy inference system is generated based on fuzzy clustering techniques . the proposed framework is implemented and validated on a benchmark image splicing data set in which three forgery detection tools are fused based on adaptive neuro-fuzzy inference system . the outcome of the proposed method reveals that applying neuro fuzzy inference systems could be a better approach for fusion of forgery detection tools .

two timescale stochastic approximation with controlled markov noise and off-policy temporal difference learning
we present for the first time an asymptotic convergence analysis of two time-scale stochastic approximation driven by ` controlled ' markov noise . in particular , both the faster and slower recursions have non-additive controlled markov noise components in addition to martingale difference noise . we analyze the asymptotic behavior of our framework by relating it to limiting differential inclusions in both time-scales that are defined in terms of the ergodic occupation measures associated with the controlled markov processes . finally , we present a solution to the off-policy convergence problem for temporal difference learning with linear function approximation , using our results .

truelabel + confusions : a spectrum of probabilistic models in analyzing multiple ratings
this paper revisits the problem of analyzing multiple ratings given by different judges . different from previous work that focuses on distilling the true labels from noisy crowdsourcing ratings , we emphasize gaining diagnostic insights into our in-house well-trained judges . we generalize the well-known dawidskene model ( dawid & skene , 1979 ) to a spectrum of probabilistic models under the same `` truelabel + confusion '' paradigm , and show that our proposed hierarchical bayesian model , called hybridconfusion , consistently outperforms dawidskene on both synthetic and real-world data sets .

a logic programming approach to knowledge-state planning : semantics and complexity
we propose a new declarative planning language , called k , which is based on principles and methods of logic programming . in this language , transitions between states of knowledge can be described , rather than transitions between completely described states of the world , which makes the language well-suited for planning under incomplete knowledge . furthermore , it enables the use of default principles in the planning process by supporting negation as failure . nonetheless , k also supports the representation of transitions between states of the world ( i.e. , states of complete knowledge ) as a special case , which shows that the language is very flexible . as we demonstrate on particular examples , the use of knowledge states may allow for a natural and compact problem representation . we then provide a thorough analysis of the computational complexity of k , and consider different planning problems , including standard planning and secure planning ( also known as conformant planning ) problems . we show that these problems have different complexities under various restrictions , ranging from np to nexptime in the propositional case . our results form the theoretical basis for the dlv^k system , which implements the language k on top of the dlv logic programming system .

neural slam : learning to explore with external memory
we present an approach for agents to learn representations of a global map from sensor data , to aid their exploration in new environments . to achieve this , we embed procedures mimicking that of traditional simultaneous localization and mapping ( slam ) into the soft attention based addressing of external memory architectures , in which the external memory acts as an internal representation of the environment . this structure encourages the evolution of slam-like behaviors inside a completely differentiable deep neural network . we show that this approach can help reinforcement learning agents to successfully explore new environments where long-term memory is essential . we validate our approach in both challenging grid-world environments and preliminary gazebo experiments . a video of our experiments can be found at : https : //goo.gl/g2vu5y .

intelligent automated diagnosis of client device bottlenecks in private clouds
we present an automated solution for rapid diagnosis of client device problems in private cloud environments : the intelligent automated client diagnostic ( iacd ) system . clients are diagnosed with the aid of transmission control protocol ( tcp ) packet traces , by ( i ) observation of anomalous artifacts occurring as a result of each fault and ( ii ) subsequent use of the inference capabilities of soft-margin support vector machine ( svm ) classifiers . the iacd system features a modular design and is extendible to new faults , with detection capability unaffected by the tcp variant used at the client . experimental evaluation of the iacd system in a controlled environment demonstrated an overall diagnostic accuracy of 98 % .

ksr : a semantic representation of knowledge graph within a novel unsupervised paradigm
knowledge representation is a long-history topic in ai , which is very important . a variety of models have been proposed for knowledge graph embedding , which projects symbolic entities and relations into continuous vector space . however , most related methods merely focus on the data-fitting of knowledge graph , and ignore the interpretable semantic expression . thus , traditional embedding methods are not friendly for applications that require semantic analysis , such as question answering and entity retrieval . to this end , this paper proposes a semantic representation method for knowledge graph \textbf { ( ksr ) } , which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple . since both aspects and categories are semantics-relevant , the collection of categories in each aspect is treated as the semantic representation of this triple . extensive experiments show that our model outperforms other state-of-the-art baselines substantially .

grainy numbers
grainy numbers are defined as tuples of bits . they form a lattice where the meet and the join operations are an addition and a multiplication . they may be substituted for the real numbers in the definition of fuzzy sets . the aim is to propose an alternative negation for the complement that we 'll call supplement .

accelerating reinforcement learning by composing solutions of automatically identified subtasks
this paper discusses a system that accelerates reinforcement learning by using transfer from related tasks . without such transfer , even if two tasks are very similar at some abstract level , an extensive re-learning effort is required . the system achieves much of its power by transferring parts of previously learned solutions rather than a single complete solution . the system exploits strong features in the multi-dimensional function produced by reinforcement learning in solving a particular task . these features are stable and easy to recognize early in the learning process . they generate a partitioning of the state space and thus the function . the partition is represented as a graph . this is used to index and compose functions stored in a case base to form a close approximation to the solution of the new task . experiments demonstrate that function composition often produces more than an order of magnitude increase in learning rate compared to a basic reinforcement learning algorithm .

most relevant explanation : properties , algorithms , and evaluations
most relevant explanation ( mre ) is a method for finding multivariate explanations for given evidence in bayesian networks [ 12 ] . this paper studies the theoretical properties of mre and develops an algorithm for finding multiple top mre solutions . our study shows that mre relies on an implicit soft relevance measure in automatically identifying the most relevant target variables and pruning less relevant variables from an explanation . the soft measure also enables mre to capture the intuitive phenomenon of explaining away encoded in bayesian networks . furthermore , our study shows that the solution space of mre has a special lattice structure which yields interesting dominance relations among the solutions . a k-mre algorithm based on these dominance relations is developed for generating a set of top solutions that are more representative . our empirical results show that mre methods are promising approaches for explanation in bayesian networks .

big data : how geo-information helped shape the future of data engineering
very large data sets are the common rule in automated mapping , gis , remote sensing , and what we can name geo-information . indeed , in 1983 landsat was already delivering gigabytes of data , and other sensors were in orbit or ready for launch , and a tantamount of cartographic data was being digitized . the retrospective paper revisits several issues that geo-information sciences had to face from the early stages on , including : structure ( to bring some structure to the data registered from a sampled signal , metadata ) ; processing ( huge amounts of data for big computers and fast algorithms ) ; uncertainty ( the kinds of errors , their quantification ) ; consistency ( when merging different sources of data is logically allowed , and meaningful ) ; ontologies ( clear and agreed shared definitions , if any kind of decision should be based upon them ) . all these issues are the background of internet queries , and the underlying technology has been shaped during those years when geo-information engineering emerged .

empirical probabilities in monadic deductive databases
we address the problem of supporting empirical probabilities in monadic logic databases . though the semantics of multivalued logic programs has been studied extensively , the treatment of probabilities as results of statistical findings has not been studied in logic programming/deductive databases . we develop a model-theoretic characterization of logic databases that facilitates such a treatment . we present an algorithm for checking consistency of such databases and prove its total correctness . we develop a sound and complete query processing procedure for handling queries to such databases .

cleannet : transfer learning for scalable image classifier training with label noise
in this paper , we study the problem of learning image classification models with label noise . existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is timeconsuming , whereas approaches not relying on human supervision are scalable but less effective . to reduce the amount of human supervision for label noise cleaning , we introduce cleannet , a joint neural embedding network , which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes . we further integrate cleannet and conventional convolutional neural network classifier into one framework for image classification learning . we demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets . experimental results show that cleannet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5 % compared to current weakly supervised methods . it also achieves 47 % of the performance gain of verifying all images with only 3.2 % images verified on an image classification task .

decision-theoretic rough sets-based three-way approximations of interval-valued fuzzy sets
in practical situations , interval-valued fuzzy sets are frequently encountered . in this paper , firstly , we present shadowed sets for interpreting and understanding interval fuzzy sets . we also provide an analytic solution to computing the pair of thresholds by searching for a balance of uncertainty in the framework of shadowed sets . secondly , we construct errors-based three-way approximations of interval-valued fuzzy sets . we also provide an alternative decision-theoretic formulation for calculating the pair of thresholds by transforming interval-valued loss functions into single-valued loss functions , in which the required thresholds are computed by minimizing decision costs . thirdly , we compute errors-based three-way approximations of interval-valued fuzzy sets by using interval-valued loss functions . finally , we employ several examples to illustrate that how to take an action for an object with interval-valued membership grade by using interval-valued loss functions .

ultimate intelligence part i : physical completeness and objectivity of induction
we propose that solomonoff induction is complete in the physical sense via several strong physical arguments . we also argue that solomonoff induction is fully applicable to quantum mechanics . we show how to choose an objective reference machine for universal induction by defining a physical message complexity and physical message probability , and argue that this choice dissolves some well-known objections to universal induction . we also introduce many more variants of physical message complexity based on energy and action , and discuss the ramifications of our proposals .

weighted parallel sgd for distributed unbalanced-workload training system
stochastic gradient descent ( sgd ) is a popular stochastic optimization method in machine learning . traditional parallel sgd algorithms , e.g. , simuparallel sgd , often require all nodes to have the same performance or to consume equal quantities of data . however , these requirements are difficult to satisfy when the parallel sgd algorithms run in a heterogeneous computing environment ; low-performance nodes will exert a negative influence on the final result . in this paper , we propose an algorithm called weighted parallel sgd ( wp-sgd ) . wp-sgd combines weighted model parameters from different nodes in the system to produce the final output . wp-sgd makes use of the reduction in standard deviation to compensate for the loss from the inconsistency in performance of nodes in the cluster , which means that wp-sgd does not require that all nodes consume equal quantities of data . we also analyze the theoretical feasibility of running two other parallel sgd algorithms combined with wp-sgd in a heterogeneous environment . the experimental results show that wp-sgd significantly outperforms the traditional parallel sgd algorithms on distributed training systems with an unbalanced workload .

contractibility for open global constraints
open forms of global constraints allow the addition of new variables to an argument during the execution of a constraint program . such forms are needed for difficult constraint programming problems where problem construction and problem solving are interleaved , and fit naturally within constraint logic programming . however , in general , filtering that is sound for a global constraint can be unsound when the constraint is open . this paper provides a simple characterization , called contractibility , of the constraints where filtering remains sound when the constraint is open . with this characterization we can easily determine whether a constraint has this property or not . in the latter case , we can use it to derive a contractible approximation to the constraint . we demonstrate this work on both hard and soft constraints . in the process , we formulate two general classes of soft constraints .

a high-performance analog max-sat solver and its application to ramsey numbers
we introduce a continuous-time analog solver for maxsat , a quintessential class of np-hard discrete optimization problems , where the task is to find a truth assignment for a set of boolean variables satisfying the maximum number of given logical constraints . we show that the scaling of an invariant of the solver 's dynamics , the escape rate , as function of the number of unsatisfied clauses can predict the global optimum value , often well before reaching the corresponding state . we demonstrate the performance of the solver on hard maxsat competition problems . we then consider the two-color ramsey number $ r ( m , m ) $ problem , translate it to sat , and apply our algorithm to the still unknown $ r ( 5,5 ) $ . we find edge colorings without monochromatic 5-cliques for complete graphs up to 42 vertices , while on 43 vertices we find colorings with only two monochromatic 5-cliques , the best coloring found so far , supporting the conjecture that $ r ( 5,5 ) = 43 $ .

genetic and memetic algorithm with diversity equilibrium based on greedy diversification
the lack of diversity in a genetic algorithm 's population may lead to a bad performance of the genetic operators since there is not an equilibrium between exploration and exploitation . in those cases , genetic algorithms present a fast and unsuitable convergence . in this paper we develop a novel hybrid genetic algorithm which attempts to obtain a balance between exploration and exploitation . it confronts the diversity problem using the named greedy diversification operator . furthermore , the proposed algorithm applies a competition between parent and children so as to exploit the high quality visited solutions . these operators are complemented by a simple selection mechanism designed to preserve and take advantage of the population diversity . additionally , we extend our proposal to the field of memetic algorithms , obtaining an improved model with outstanding results in practice . the experimental study shows the validity of the approach as well as how important is taking into account the exploration and exploitation concepts when designing an evolutionary algorithm .

enhanced quantum synchronization via quantum machine learning
we study the quantum synchronization between a pair of two-level systems inside two coupledcavities . using a digital-analog decomposition of the master equation that rules the system dynamics , we show that this approach leads to quantum synchronization between both two-level systems . moreover , we can identify in this digital-analog block decomposition the fundamental elements of a quantum machine learning protocol , in which the agent and the environment ( learning units ) interact through a mediating system , namely , the register . if we can additionally equip this algorithm with a classical feedback mechanism , which consists of projective measurements in the register , reinitialization of the register state and local conditional operations on the agent and register subspace , a powerful and flexible quantum machine learning protocol emerges . indeed , numerical simulations show that this protocol enhances the synchronization process , even when every subsystem experience different loss/decoherence mechanisms , and give us flexibility to choose the synchronization state . finally , we propose an implementation based on current technologies in superconducting circuits .

target tracking in the recommender space : toward a new recommender system based on kalman filtering
in this paper , we propose a new approach for recommender systems based on target tracking by kalman filtering . we assume that users and their seen resources are vectors in the multidimensional space of the categories of the resources . knowing this space , we propose an algorithm based on a kalman filter to track users and to predict the best prediction of their future position in the recommendation space .

proceedings of the fifteenth conference on uncertainty in artificial intelligence ( 1999 )
this is the proceedings of the fifteenth conference on uncertainty in artificial intelligence , which was held in stockholm sweden , july 30 - august 1 , 1999

image reconstruction from bag-of-visual-words
the objective of this work is to reconstruct an original image from bag-of-visual-words ( bovw ) . image reconstruction from features can be a means of identifying the characteristics of features . additionally , it enables us to generate novel images via features . although bovw is the de facto standard feature for image recognition and retrieval , successful image reconstruction from bovw has not been reported yet . what complicates this task is that bovw lacks the spatial information for including visual words . as described in this paper , to estimate an original arrangement , we propose an evaluation function that incorporates the naturalness of local adjacency and the global position , with a method to obtain related parameters using an external image database . to evaluate the performance of our method , we reconstruct images of objects of 101 kinds . additionally , we apply our method to analyze object classifiers and to generate novel images via bovw .

advice from the oracle : really intelligent information retrieval
what is `` intelligent '' information retrieval ? essentially this is asking what is intelligence , in this article i will attempt to show some of the aspects of human intelligence , as related to information retrieval . i will do this by the device of a semi-imaginary oracle . every observatory has an oracle , someone who is a distinguished scientist , has great administrative responsibilities , acts as mentor to a number of less senior people , and as trusted advisor to even the most accomplished scientists , and knows essentially everyone in the field . in an appendix i will present a brief summary of the statistical factor space method for text indexing and retrieval , and indicate how it will be used in the astrophysics data system abstract service . 2018 keywords : personal digital assistant ; supervised topic models

a motion planning strategy for the active vision-based mapping of ground-level structures
this paper presents a strategy to guide a mobile ground robot equipped with a camera or depth sensor , in order to autonomously map the visible part of a bounded three-dimensional structure . we describe motion planning algorithms that determine appropriate successive viewpoints and attempt to fill holes automatically in a point cloud produced by the sensing and perception layer . the emphasis is on accurately reconstructing a 3d model of a structure of moderate size rather than mapping large open environments , with applications for example in architecture , construction and inspection . the proposed algorithms do not require any initialization in the form of a mesh model or a bounding box , and the paths generated are well adapted to situations where the vision sensor is used simultaneously for mapping and for localizing the robot , in the absence of additional absolute positioning system . we analyze the coverage properties of our policy , and compare its performance to the classic frontier based exploration algorithm . we illustrate its efficacy for different structure sizes , levels of localization accuracy and range of the depth sensor , and validate our design on a real-world experiment .

the sat phase transition
phase transition is an important feature of sat problem . for random k-sat model , it is proved that as r ( ratio of clauses to variables ) increases , the structure of solutions will undergo a sudden change like satisfiability phase transition when r reaches a threshold point . this phenomenon shows that the satisfying truth assignments suddenly shift from being relatively different from each other to being very similar to each other .

sequential operators in computability logic
computability logic ( cl ) ( see http : //www.cis.upenn.edu/~giorgi/cl.html ) is a semantical platform and research program for redeveloping logic as a formal theory of computability , as opposed to the formal theory of truth which it has more traditionally been . formulas in cl stand for ( interactive ) computational problems , understood as games between a machine and its environment ; logical operators represent operations on such entities ; and `` truth '' is understood as existence of an effective solution , i.e. , of an algorithmic winning strategy . the formalism of cl is open-ended , and may undergo series of extensions as the study of the subject advances . the main groups of operators on which cl has been focused so far are the parallel , choice , branching , and blind operators . the present paper introduces a new important group of operators , called sequential . the latter come in the form of sequential conjunction and disjunction , sequential quantifiers , and sequential recurrences . as the name may suggest , the algorithmic intuitions associated with this group are those of sequential computations , as opposed to the intuitions of parallel computations associated with the parallel group of operations : playing a sequential combination of games means playing its components in a sequential fashion , one after one . the main technical result of the present paper is a sound and complete axiomatization of the propositional fragment of computability logic whose vocabulary , together with negation , includes all three -- parallel , choice and sequential -- sorts of conjunction and disjunction . an extension of this result to the first-order level is also outlined .

irregular-time bayesian networks
in many fields observations are performed irregularly along time , due to either measurement limitations or lack of a constant immanent rate . while discrete-time markov models ( as dynamic bayesian networks ) introduce either inefficient computation or an information loss to reasoning about such processes , continuous-time markov models assume either a discrete state space ( as continuous-time bayesian networks ) , or a flat continuous state space ( as stochastic differential equations ) . to address these problems , we present a new modeling class called irregular-time bayesian networks ( itbns ) , generalizing dynamic bayesian networks , allowing substantially more compact representations , and increasing the expressivity of the temporal dynamics . in addition , a globally optimal solution is guaranteed when learning temporal systems , provided that they are fully observed at the same irregularly spaced time-points , and a semiparametric subclass of itbns is introduced to allow further adaptation to the irregular nature of the available data .

a prototype for educational planning using course constraints to simulate student populations
distance learning universities usually afford their students the flexibility to advance their studies at their own pace . this can lead to a considerable fluctuation of student populations within a program 's courses , possibly affecting the academic viability of a program as well as the related required resources . providing a method that estimates this population could be of substantial help to university management and academic personnel . we describe how to use course precedence constraints to calculate alternative tuition paths and then use markov models to estimate future populations . in doing so , we identify key issues of a large scale potential deployment .

co-attending free-form regions and detections with multi-modal multiplicative feature embedding for visual question answering
recently , the visual question answering ( vqa ) task has gained increasing attention in artificial intelligence . existing vqa methods mainly adopt the visual attention mechanism to associate the input question with corresponding image regions for effective question answering . the free-form region based and the detection-based visual attention mechanisms are mostly investigated , with the former ones attending free-form image regions and the latter ones attending pre-specified detection-box regions . we argue that the two attention mechanisms are able to provide complementary information and should be effectively integrated to better solve the vqa problem . in this paper , we propose a novel deep neural network for vqa that integrates both attention mechanisms . our proposed framework effectively fuses features from free-form image regions , detection boxes , and question representations via a multi-modal multiplicative feature embedding scheme to jointly attend question-related free-form image regions and detection boxes for more accurate question answering . the proposed method is extensively evaluated on two publicly available datasets , coco-qa and vqa , and outperforms state-of-the-art approaches . source code is available at https : //github.com/lupantech/dual-mfa-vqa .

integrating topic models and latent factors for recommendation
the research of personalized recommendation techniques today has mostly parted into two mainstream directions , i.e. , the factorization-based approaches and topic models . practically , they aim to benefit from the numerical ratings and textual reviews , correspondingly , which compose two major information sources in various real-world systems . however , although the two approaches are supposed to be correlated for their same goal of accurate recommendation , there still lacks a clear theoretical understanding of how their objective functions can be mathematically bridged to leverage the numerical ratings and textual reviews collectively , and why such a bridge is intuitively reasonable to match up their learning procedures for the rating prediction and top-n recommendation tasks , respectively . in this work , we exposit with mathematical analysis that , the vector-level randomization functions to coordinate the optimization objectives of factorizational and topic models unfortunately do not exist at all , although they are usually pre-assumed and intuitively designed in the literature . fortunately , we also point out that one can avoid the seeking of such a randomization function by optimizing a joint factorizational topic ( jft ) model directly . we apply our jft model to restaurant recommendation , and study its performance in both normal and cross-city recommendation scenarios , where the latter is an extremely difficult task for its inherent cold-start nature . experimental results on real-world datasets verified the appealing performance of our approach against previous methods , on both rating prediction and top-n recommendation tasks .

optimal and approximate q-value functions for decentralized pomdps
decision-theoretic planning is a popular approach to sequential decision making problems , because it treats uncertainty in sensing and acting in a principled way . in single-agent frameworks like mdps and pomdps , planning can be carried out by resorting to q-value functions : an optimal q-value function q* is computed in a recursive manner by dynamic programming , and then an optimal policy is extracted from q* . in this paper we study whether similar q-value functions can be defined for decentralized pomdp models ( dec-pomdps ) , and how policies can be extracted from such value functions . we define two forms of the optimal q-value function for dec-pomdps : one that gives a normative description as the q-value function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation . this computation , however , is infeasible for all but the smallest problems . therefore , we analyze various approximate q-value functions that allow for efficient computation . we describe how they relate , and we prove that they all provide an upper bound to the optimal q-value function q* . finally , unifying some previous approaches for solving dec-pomdps , we describe a family of algorithms for extracting policies from such q-value functions , and perform an experimental evaluation on existing test problems , including a new firefighting benchmark problem .

global continuous optimization with error bound and fast convergence
this paper considers global optimization with a black-box unknown objective function that can be non-convex and non-differentiable . such a difficult optimization problem arises in many real-world applications , such as parameter tuning in machine learning , engineering design problem , and planning with a complex physics simulator . this paper proposes a new global optimization algorithm , called locally oriented global optimization ( logo ) , to aim for both fast convergence in practice and finite-time error bound in theory . the advantage and usage of the new algorithm are illustrated via theoretical analysis and an experiment conducted with 11 benchmark test functions . further , we modify the logo algorithm to specifically solve a planning problem via policy search with continuous state/action space and long time horizon while maintaining its finite-time error bound . we apply the proposed planning method to accident management of a nuclear power plant . the result of the application study demonstrates the practical utility of our method .

empirical study of artificial fish swarm algorithm
artificial fish swarm algorithm ( afsa ) is one of the swarm intelligence optimization algorithms that works based on population and stochastic search . in order to achieve acceptable result , there are many parameters needs to be adjusted in afsa . among these parameters , visual and step are very significant in view of the fact that artificial fish basically move based on these parameters . in standard afsa , these two parameters remain constant until the algorithm termination . large values of these parameters increase the capability of algorithm in global search , while small values improve the local search ability of the algorithm . in this paper , we empirically study the performance of the afsa and different approaches to balance between local and global exploration have been tested based on the adaptive modification of visual and step during algorithm execution . the proposed approaches have been evaluated based on the four well-known benchmark functions . experimental results show considerable positive impact on the performance of afsa .

artificial decision making under uncertainty in intelligent buildings
our hypothesis is that by equipping certain agents in a multi-agent system controlling an intelligent building with automated decision support , two important factors will be increased . the first is energy saving in the building . the second is customer value -- -how the people in the building experience the effects of the actions of the agents . we give evidence for the truth of this hypothesis through experimental findings related to tools for artificial decision making . a number of assumptions related to agent control , through monitoring and delegation of tasks to other kinds of agents , of rooms at a test site are relaxed . each assumption controls at least one uncertainty that complicates considerably the procedures for selecting actions part of each such agent . we show that in realistic decision situations , room-controlling agents can make bounded rational decisions even under dynamic real-time constraints . this result can be , and has been , generalized to other domains with even harsher time constraints .

factored filtering of continuous-time systems
we consider filtering for a continuous-time , or asynchronous , stochastic system where the full distribution over states is too large to be stored or calculated . we assume that the rate matrix of the system can be compactly represented and that the belief distribution is to be approximated as a product of marginals . the essential computation is the matrix exponential . we look at two different methods for its computation : ode integration and uniformization of the taylor expansion . for both we consider approximations in which only a factored belief state is maintained . for factored uniformization we demonstrate that the kl-divergence of the filtering is bounded . our experimental results confirm our factored uniformization performs better than previously suggested uniformization methods and the mean field algorithm .

probabilistic reasoning via deep learning : neural association models
in this paper , we propose a new deep learning approach , called neural association model ( nam ) , for probabilistic reasoning in artificial intelligence . we propose to use neural networks to model association between any two events in a domain . neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are to be associated . the actual meaning of the conditional probabilities varies between applications and depends on how the models are trained . in this work , as two case studies , we have investigated two nam structures , namely deep neural networks ( dnn ) and relation-modulated neural nets ( rmnn ) , on several probabilistic reasoning tasks in ai , including recognizing textual entailment , triple classification in multi-relational knowledge bases and commonsense reasoning . experimental results on several popular datasets derived from wordnet , freebase and conceptnet have all demonstrated that both dnns and rmnns perform equally well and they can significantly outperform the conventional methods available for these reasoning tasks . moreover , compared with dnns , rmnns are superior in knowledge transfer , where a pre-trained model can be quickly extended to an unseen relation after observing only a few training samples . to further prove the effectiveness of the proposed models , in this work , we have applied nams to solving challenging winograd schema ( ws ) problems . experiments conducted on a set of ws problems prove that the proposed models have the potential for commonsense reasoning .

the mind grows circuits
there is a vast supply of prior art that study models for mental processes . some studies in psychology and philosophy approach it from an inner perspective in terms of experiences and percepts . others such as neurobiology or connectionist-machines approach it externally by viewing the mind as complex circuit of neurons where each neuron is a primitive binary circuit . in this paper , we also model the mind as a place where a circuit grows , starting as a collection of primitive components at birth and then builds up incrementally in a bottom up fashion . a new node is formed by a simple composition of prior nodes when we undergo a repeated experience that can be described by that composition . unlike neural networks , however , these circuits take `` concepts '' or `` percepts '' as inputs and outputs . thus the growing circuits can be likened to a growing collection of lambda expressions that are built on top of one another in an attempt to compress the sensory input as a heuristic to bound its kolmogorov complexity .

incremental clustering and expansion for faster optimal planning in dec-pomdps
this article presents the state-of-the-art in optimal solution methods for decentralized partially observable markov decision processes ( dec-pomdps ) , which are general models for collaborative multiagent planning under uncertainty . building off the generalized multiagent a* ( gmaa* ) algorithm , which reduces the problem to a tree of one-shot collaborative bayesian games ( cbgs ) , we describe several advances that greatly expand the range of dec-pomdps that can be solved optimally . first , we introduce lossless incremental clustering of the cbgs solved by gmaa* , which achieves exponential speedups without sacrificing optimality . second , we introduce incremental expansion of nodes in the gmaa* search tree , which avoids the need to expand all children , the number of which is in the worst case doubly exponential in the nodes depth . this is particularly beneficial when little clustering is possible . in addition , we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger dec-pomdps . we provide theoretical guarantees that , when a suitable heuristic is used , both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent . finally , we present extensive empirical results demonstrating that gmaa*-ice , an algorithm that synthesizes these advances , can optimally solve dec-pomdps of unprecedented size .

the n-tuple bandit evolutionary algorithm for automatic game improvement
this paper describes a new evolutionary algorithm that is especially well suited to ai-assisted game design . the approach adopted in this paper is to use observations of ai agents playing the game to estimate the game 's quality . some of best agents for this purpose are general video game ai agents , since they can be deployed directly on a new game without game-specific tuning ; these agents tend to be based on stochastic algorithms which give robust but noisy results and tend to be expensive to run . this motivates the main contribution of the paper : the development of the novel n-tuple bandit evolutionary algorithm , where a model is used to estimate the fitness of unsampled points and a bandit approach is used to balance exploration and exploitation of the search space . initial results on optimising a space battle game variant suggest that the algorithm offers far more robust results than the random mutation hill climber and a biased mutation variant , which are themselves known to offer competitive performance across a range of problems . subjective observations are also given by human players on the nature of the evolved games , which indicate a preference towards games generated by the n-tuple algorithm .

a hybrid approach with multi-channel i-vectors and convolutional neural networks for acoustic scene classification
in acoustic scene classification ( asc ) two major approaches have been followed . while one utilizes engineered features such as mel-frequency-cepstral-coefficients ( mfccs ) , the other uses learned features that are the outcome of an optimization algorithm . i-vectors are the result of a modeling technique that usually takes engineered features as input . it has been shown that standard mfccs extracted from monaural audio signals lead to i-vectors that exhibit poor performance , especially on indoor acoustic scenes . at the same time , convolutional neural networks ( cnns ) are well known for their ability to learn features by optimizing their filters . they have been applied on asc and have shown promising results . in this paper , we first propose a novel multi-channel i-vector extraction and scoring scheme for asc , improving their performance on indoor and outdoor scenes . second , we propose a cnn architecture that achieves promising asc results . further , we show that i-vectors and cnns capture complementary information from acoustic scenes . finally , we propose a hybrid system for asc using multi-channel i-vectors and cnns by utilizing a score fusion technique . using our method , we participated in the asc task of the dcase-2016 challenge . our hybrid approach achieved 1 st rank among 49 submissions , substantially improving the previous state of the art .

a learning based optimal human robot collaboration with linear temporal logic constraints
this paper considers an optimal task allocation problem for human robot collaboration in human robot systems with persistent tasks . such human robot systems consist of human operators and intelligent robots collaborating with each other to accomplish complex tasks that can not be done by either part alone . the system objective is to maximize the probability of successfully executing persistent tasks that are formulated as linear temporal logic specifications and minimize the average cost between consecutive visits of a particular proposition . this paper proposes to model the human robot collaboration under a framework with the composition of multiple markov decision process ( mdp ) with possibly unknown transition probabilities , which characterizes how human cognitive states , such as human trust and fatigue , stochastically change with the robot performance . under the unknown mdp models , an algorithm is developed to learn the model and obtain an optimal task allocation policy that minimizes the expected average cost for each task cycle and maximizes the probability of satisfying linear temporal logic constraints . moreover , this paper shows that the difference between the optimal policy based on the learned model and that based on the underlying ground truth model can be bounded by arbitrarily small constant and large confidence level with sufficient samples . the case study of an assembly process demonstrates the effectiveness and benefits of our proposed learning based human robot collaboration .

improving efficiency in convolutional neural network with multilinear filters
the excellent performance of deep neural networks has enabled us to solve several automatization problems , opening an era of autonomous devices . however , current deep net architectures are heavy with millions of parameters and require billions of floating point operations . several works have been developed to compress a pre-trained deep network to reduce memory footprint and , possibly , computation . instead of compressing a pre-trained network , in this work , we propose a generic neural network layer structure employing multilinear projection as the primary feature extractor . the proposed architecture requires several times less memory as compared to the traditional convolutional neural networks ( cnn ) , while inherits the similar design principles of a cnn . in addition , the proposed architecture is equipped with two computation schemes that enable computation reduction or scalability . experimental results show the effectiveness of our compact projection that outperforms traditional cnn , while requiring far fewer parameters .

fast belief update using order-of-magnitude probabilities
we present an algorithm , called predict , for updating beliefs in causal networks quantified with order-of-magnitude probabilities . the algorithm takes advantage of both the structure and the quantification of the network and presents a polynomial asymptotic complexity . predict exhibits a conservative behavior in that it is always sound but not always complete . we provide sufficient conditions for completeness and present algorithms for testing these conditions and for computing a complete set of plausible values . we propose predict as an efficient method to estimate probabilistic values and illustrate its use in conjunction with two known algorithms for probabilistic inference . finally , we describe an application of predict to plan evaluation , present experimental results , and discuss issues regarding its use with conditional logics of belief , and in the characterization of irrelevance .

the rise and fall of the church-turing thesis
the essay consists of three parts . in the first part , it is explained how theory of algorithms and computations evaluates the contemporary situation with computers and global networks . in the second part , it is demonstrated what new perspectives this theory opens through its new direction that is called theory of super-recursive algorithms . these algorithms have much higher computing power than conventional algorithmic schemes . in the third part , we explicate how realization of what this theory suggests might influence life of people in future . it is demonstrated that now the theory is far ahead computing practice and practice has to catch up with the theory . we conclude with a comparison of different approaches to the development of information technology .

a stochastic model for case-based reasoning
case-bsed reasoning ( cbr ) is a recent theory for problem-solving and learning in computers and people.broadly construed it is the process of solving new problems based on the solution of similar past problems . in the present paper we introduce an absorbing markov chain on the main steps of the cbr process.in this way we succeed in obtaining the probabilities for the above process to be in a certain step at a certain phase of the solution of the corresponding problem , and a measure for the efficiency of a cbr system . examples are given to illustrate our results .

planning with trust for human-robot collaboration
trust is essential for human-robot collaboration and user adoption of autonomous systems , such as robot assistants . this paper introduces a computational model which integrates trust into robot decision-making . specifically , we learn from data a partially observable markov decision process ( pomdp ) with human trust as a latent variable . the trust-pomdp model provides a principled approach for the robot to ( i ) infer the trust of a human teammate through interaction , ( ii ) reason about the effect of its own actions on human behaviors , and ( iii ) choose actions that maximize team performance over the long term . we validated the model through human subject experiments on a table-clearing task in simulation ( 201 participants ) and with a real robot ( 20 participants ) . the results show that the trust-pomdp improves human-robot team performance in this task . they further suggest that maximizing trust in itself may not improve team performance .

fast generation of best interval patterns for nonmonotonic constraints
in pattern mining , the main challenge is the exponential explosion of the set of patterns . typically , to solve this problem , a constraint for pattern selection is introduced . one of the first constraints proposed in pattern mining is support ( frequency ) of a pattern in a dataset . frequency is an anti-monotonic function , i.e. , given an infrequent pattern , all its superpatterns are not frequent . however , many other constraints for pattern selection are not ( anti- ) monotonic , which makes it difficult to generate patterns satisfying these constraints . in this paper we introduce the notion of projection-antimonotonicity and $ \theta $ - $ \sigma\o\phi\iota\alpha $ algorithm that allows efficient generation of the best patterns for some nonmonotonic constraints . in this paper we consider stability and $ \delta $ -measure , which are nonmonotonic constraints , and apply them to interval tuple datasets . in the experiments , we compute best interval tuple patterns w.r.t . these measures and show the advantage of our approach over postfiltering approaches . keywords : pattern mining , nonmonotonic constraints , interval tuple data

differentiable learning of logical rules for knowledge base reasoning
we study the problem of learning probabilistic first-order logical rules for knowledge base reasoning . this learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space . we propose a framework , neural logic programming , that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model . this approach is inspired by a recently-developed differentiable logic called tensorlog , where inference tasks can be compiled into sequences of differentiable operations . we design a neural controller system that learns to compose these operations . empirically , our method outperforms prior work on multiple knowledge base benchmark datasets , including freebase and wikimovies .

complexity distribution of agent policies
we analyse the complexity of environments according to the policies that need to be used to achieve high performance . the performance results for a population of policies leads to a distribution that is examined in terms of policy complexity and analysed through several diagrams and indicators . the notion of environment response curve is also introduced , by inverting the performance results into an ability scale . we apply all these concepts , diagrams and indicators to a minimalistic environment class , agent-populated elementary cellular automata , showing how the difficulty , discriminating power and ranges ( previous to normalisation ) may vary for several environments .

robot localisation and 3d position estimation using a free-moving camera and cascaded convolutional neural networks
many works in collaborative robotics and human-robot interaction focuses on identifying and predicting human behaviour while considering the information about the robot itself as given . this can be the case when sensors and the robot are calibrated in relation to each other and often the reconfiguration of the system is not possible , or extra manual work is required . we present a deep learning based approach to remove the constraint of having the need for the robot and the vision sensor to be fixed and calibrated in relation to each other . the system learns the visual cues of the robot body and is able to localise it , as well as estimate the position of robot joints in 3d space by just using a 2d color image . the method uses a cascaded convolutional neural network , and we present the structure of the network , describe our own collected dataset , explain the network training and achieved results . a fully trained system shows promising results in providing an accurate mask of where the robot is located and a good estimate of its joints positions in 3d . the accuracy is not good enough for visual servoing applications yet , however , it can be sufficient for general safety and some collaborative tasks not requiring very high precision . the main benefit of our method is the possibility of the vision sensor to move freely . this allows it to be mounted on moving objects , for example , a body of the person or a mobile robot working in the same environment as the robots are operating in .

on the compressive power of deep rectifier networks for high resolution representation of class boundaries
this paper provides a theoretical justification of the superior classification performance of deep rectifier networks over shallow rectifier networks from the geometrical perspective of piecewise linear ( pwl ) classifier boundaries . we show that , for a given threshold on the approximation error , the required number of boundary facets to approximate a general smooth boundary grows exponentially with the dimension of the data , and thus the number of boundary facets , referred to as boundary resolution , of a pwl classifier is an important quality measure that can be used to estimate a lower bound on the classification errors . however , learning naively an exponentially large number of boundary facets requires the determination of an exponentially large number of parameters and also requires an exponentially large number of training patterns . to overcome this issue of `` curse of dimensionality '' , compressive representations of high resolution classifier boundaries are required . to show the superior compressive power of deep rectifier networks over shallow rectifier networks , we prove that the maximum boundary resolution of a single hidden layer rectifier network classifier grows exponentially with the number of units when this number is smaller than the dimension of the patterns . when the number of units is larger than the dimension of the patterns , the growth rate is reduced to a polynomial order . consequently , the capacity of generating a high resolution boundary will increase if the same large number of units are arranged in multiple layers instead of a single hidden layer . taking high dimensional spherical boundaries as examples , we show how deep rectifier networks can utilize geometric symmetries to approximate a boundary with the same accuracy but with a significantly fewer number of parameters than single hidden layer nets .

information integration and computational logic
information integration is a young and exciting field with enormous research and commercial significance in the new world of the information society . it stands at the crossroad of databases and artificial intelligence requiring novel techniques that bring together different methods from these fields . information from disparate heterogeneous sources often with no a-priori common schema needs to be synthesized in a flexible , transparent and intelligent way in order to respond to the demands of a query thus enabling a more informed decision by the user or application program . the field although relatively young has already found many practical applications particularly for integrating information over the world wide web . this paper gives a brief introduction of the field highlighting some of the main current and future research issues and application areas . it attempts to evaluate the current and potential role of computational logic in this and suggests some of the problems where logic-based techniques could be used .

task scheduling system for uav operations in indoor environment
application of uav in indoor environment is emerging nowadays due to the advancements in technology . uav brings more space-flexibility in an occupied or hardly-accessible indoor environment , e.g. , shop floor of manufacturing industry , greenhouse , nuclear powerplant . uav helps in creating an autonomous manufacturing system by executing tasks with less human intervention in time-efficient manner . consequently , a scheduler is one essential component to be focused on ; yet the number of reported studies on uav scheduling has been minimal . this work proposes a methodology with a heuristic ( based on earliest available time algorithm ) which assigns tasks to uavs with an objective of minimizing the makespan . in addition , a quick response towards uncertain events and a quick creation of new high-quality feasible schedule are needed . hence , the proposed heuristic is incorporated with particle swarm optimization ( pso ) algorithm to find a quick near optimal schedule . this proposed methodology is implemented into a scheduler and tested on a few scales of datasets generated based on a real flight demonstration . performance evaluation of scheduler is discussed in detail and the best solution obtained from a selected set of parameters is reported .

the digital synaptic neural substrate : size and quality matters
we investigate the 'digital synaptic neural substrate ' ( dsns ) computational creativity approach further with respect to the size and quality of images that can be used to seed the process . in previous work we demonstrated how combining photographs of people and sequences taken from chess games between weak players can be used to generate chess problems or puzzles of higher aesthetic quality , on average , compared to alternative approaches . in this work we show experimentally that using larger images as opposed to smaller ones improves the output quality even further . the same is also true for using clearer or less corrupted images . the reasons why these things influence the dsns process is presently not well-understood and debatable but the findings are nevertheless immediately applicable for obtaining better results .

dynamic sum product networks for tractable inference on sequence data ( extended version )
sum-product networks ( spn ) have recently emerged as a new class of tractable probabilistic graphical models . unlike bayesian networks and markov networks where inference may be exponential in the size of the network , inference in spns is in time linear in the size of the network . since spns represent distributions over a fixed set of variables only , we propose dynamic sum product networks ( dspns ) as a generalization of spns for sequence data of varying length . a dspn consists of a template network that is repeated as many times as needed to model data sequences of any length . we present a local search technique to learn the structure of the template network . in contrast to dynamic bayesian networks for which inference is generally exponential in the number of variables per time slice , dspns inherit the linear inference complexity of spns . we demonstrate the advantages of dspns over dbns and other models on several datasets of sequence data .

diversity in ranking using negative reinforcement
in this paper , we consider the problem of diversity in ranking of the nodes in a graph . the task is to pick the top-k nodes in the graph which are both 'central ' and 'diverse ' . many graph-based models of nlp like text summarization , opinion summarization involve the concept of diversity in generating the summaries . we develop a novel method which works in an iterative fashion based on random walks to achieve diversity . specifically , we use negative reinforcement as a main tool to introduce diversity in the personalized pagerank framework . experiments on two benchmark datasets show that our algorithm is competitive to the existing methods .

look-ahead before you leap : end-to-end active recognition by forecasting the effect of motion
visual recognition systems mounted on autonomous moving agents face the challenge of unconstrained data , but simultaneously have the opportunity to improve their performance by moving to acquire new views of test data . in this work , we first show how a recurrent neural network-based system may be trained to perform end-to-end learning of motion policies suited for this `` active recognition '' setting . further , we hypothesize that active vision requires an agent to have the capacity to reason about the effects of its motions on its view of the world . to verify this hypothesis , we attempt to induce this capacity in our active recognition pipeline , by simultaneously learning to forecast the effects of the agent 's motions on its internal representation of the environment conditional on all past views . results across two challenging datasets confirm both that our end-to-end system successfully learns meaningful policies for active category recognition , and that `` learning to look ahead '' further boosts recognition performance .

from spin glasses to hard satisfiable formulas
we introduce a highly structured family of hard satisfiable 3-sat formulas corresponding to an ordered spin-glass model from statistical physics . this model has provably `` glassy '' behavior ; that is , it has many local optima with large energy barriers between them , so that local search algorithms get stuck and have difficulty finding the true ground state , i.e. , the unique satisfying assignment . we test the hardness of our formulas with two davis-putnam solvers , satz and zchaff , the recently introduced survey propagation ( sp ) , and two local search algorithms , walksat and record-to-record travel ( rrt ) . we compare our formulas to random 3-xor-sat formulas and to two other generators of hard satisfiable instances , the minimum disagreement parity formulas of crawford et al. , and hirsch 's hgen . for the complete solvers the running time of our formulas grows exponentially in sqrt ( n ) , and exceeds that of random 3-xor-sat formulas for small problem sizes . sp is unable to solve our formulas with as few as 25 variables . for walksat , our formulas appear to be harder than any other known generator of satisfiable instances . finally , our formulas can be solved efficiently by rrt but only if the parameter d is tuned to the height of the barriers between local minima , and we use this parameter to measure the barrier heights in random 3-xor-sat formulas as well .

intelligent biohybrid neurotechnologies : are they really what they claim ?
in the era of intelligent biohybrid neurotechnologies for brain repair , new fanciful terms are appearing in the scientific dictionary to define what has so far been unimaginable . as the emerging neurotechnologies are becoming increasingly polyhedral and sophisticated , should we talk about evolution and rank the intelligence of these devices ?

sre : semantic rules engine for the industrial internet-of-things gateways
the advent of the internet-of-things ( iot ) paradigm has brought opportunities to solve many real-world problems . energy management , for example , has attracted huge interest from academia , industries , governments and regulatory bodies . it involves collecting energy usage data , analyzing it , and optimizing the energy consumption by applying control strategies . however , in industrial environments , performing such optimization is not trivial . the changes in business rules , process control , and customer requirements make it much more challenging . in this paper , a semantic rules engine ( sre ) for industrial gateways is presented that allows implementing dynamic and flexible rule-based control strategies . it is simple , expressive , and allows managing rules on-the-fly without causing any service interruption . additionally , it can handle semantic queries and provide results by inferring additional knowledge from previously defined concepts in ontologies . sre has been validated and tested on different hardware platforms and in commercial products . performance evaluations are also presented to validate its conformance to the customer requirements .

possibilistic conditioning and propagation
we give an axiomatization of confidence transfer - a known conditioning scheme - from the perspective of expectation-based inference in the sense of gardenfors and makinson . then , we use the notion of belief independence to `` filter out '' different proposal s of possibilistic conditioning rules , all are variations of confidence transfer . among the three rules that we consider , only dempster 's rule of conditioning passes the test of supporting the notion of belief independence . with the use of this conditioning rule , we then show that we can use local computation for computing desired conditional marginal possibilities of the joint possibility satisfying the given constraints . it turns out that our local computation scheme is already proposed by shenoy . however , our intuitions are completely different from that of shenoy . while shenoy just defines a local computation scheme that fits his framework of valuation-based systems , we derive that local computation scheme from ii ( ,8 ) = ti ( ,8 i a ) * ii ( a ) and appropriate independence assumptions , just like how the bayesians derive their local computation scheme .

measuring the non-asymptotic convergence of sequential monte carlo samplers using probabilistic programming
a key limitation of sampling algorithms for approximate inference is that it is difficult to quantify their approximation error . widely used sampling schemes , such as sequential importance sampling with resampling and metropolis-hastings , produce output samples drawn from a distribution that may be far from the target posterior distribution . this paper shows how to upper-bound the symmetric kl divergence between the output distribution of a broad class of sequential monte carlo ( smc ) samplers and their target posterior distributions , subject to assumptions about the accuracy of a separate gold-standard sampler . the proposed method applies to samplers that combine multiple particles , multinomial resampling , and rejuvenation kernels . the experiments show the technique being used to estimate bounds on the divergence of smc samplers for posterior inference in a bayesian linear regression model and a dirichlet process mixture model .

discounting and combination operations in evidential reasoning
evidential reasoning is now a leading topic in artificial intelligence . evidence is represented by a variety of evidential functions . evidential reasoning is carried out by certain kinds of fundamental operation on these functions . this paper discusses two of the basic operations on evidential functions , the discount operation and the well-known orthogonal sum operation . we show that the discount operation is not commutative with the orthogonal sum operation , and derive expressions for the two operations applied to the various evidential function .

a simple and realistic pedestrian model for crowd simulation and application
the simulation of pedestrian crowd that reflects reality is a major challenge for researches . several crowd simulation models have been proposed such as cellular automata model , agent-based model , fluid dynamic model , etc . it is important to note that agent-based model is able , over others approaches , to provide a natural description of the system and then to capture complex human behaviors . in this paper , we propose a multi-agent simulation model in which pedestrian positions are updated at discrete time intervals . it takes into account the major normal conditions of a simple pedestrian situated in a crowd such as preferences , realistic perception of environment , etc . our objective is to simulate the pedestrian crowd realistically towards a simulation of believable pedestrian behaviors . typical pedestrian phenomena , including the unidirectional and bidirectional movement in a corridor as well as the flow through bottleneck , are simulated . the conducted simulations show that our model is able to produce realistic pedestrian behaviors . the obtained fundamental diagram and flow rate at bottleneck agree very well with classic conclusions and empirical study results . it is hoped that the idea of this study may be helpful in promoting the modeling and simulation of pedestrian crowd in a simple way .

yggdrasil - a statistical package for learning split models
there are two main objectives of this paper . the first is to present a statistical framework for models with context specific independence structures , i.e. , conditional independences holding only for sepcific values of the conditioning variables . this framework is constituted by the class of split models . split models are extension of graphical models for contigency tables and allow for a more sophisticiated modelling than graphical models . the treatment of split models include estimation , representation and a markov property for reading off those independencies holding in a specific context . the second objective is to present a software package named yggdrasil which is designed for statistical inference in split models , i.e. , for learning such models on the basis of data .

abstract attribute exploration with partial object descriptions
attribute exploration has been investigated in several studies , with particular emphasis on the algorithmic aspects of this knowledge acquisition method . in its basic version the method itself is rather simple and transparent . but when background knowledge and partially described counter-examples are admitted , it gets more difficult . here we discuss this case in an abstract , somewhat `` axiomatic '' setting , providing a terminology that clarifies the abstract strategy of the method rather than its algorithmic implementation .

deep reinforcement learning for high precision assembly tasks
high precision assembly of mechanical parts requires accuracy exceeding the robot precision . conventional part mating methods used in the current manufacturing requires tedious tuning of numerous parameters before deployment . we show how the robot can successfully perform a tight clearance peg-in-hole task through training a recurrent neural network with reinforcement learning . in addition to saving the manual effort , the proposed technique also shows robustness against position and angle errors for the peg-in-hole task . the neural network learns to take the optimal action by observing the robot sensors to estimate the system state . the advantages of our proposed method is validated experimentally on a 7-axis articulated robot arm .

some logics of belief and disbelief
the introduction of explicit notions of rejection , or disbelief , into logics for knowledge representation can be justified in a number of ways . motivations range from the need for versions of negation weaker than classical negation , to the explicit recording of classic belief contraction operations in the area of belief change , and the additional levels of expressivity obtained from an extended version of belief change which includes disbelief contraction . in this paper we present four logics of disbelief which address some or all of these intuitions . soundness and completeness results are supplied and the logics are compared with respect to applicability and utility .

generalization and exploration via randomized value functions
we propose randomized least-squares value iteration ( rlsvi ) -- a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions . we explain why versions of least-squares value iteration that use boltzmann or epsilon-greedy exploration can be highly inefficient , and we present computational results that demonstrate dramatic efficiency gains enjoyed by rlsvi . further , we establish an upper bound on the expected regret of rlsvi that demonstrates near-optimality in a tabula rasa learning context . more broadly , our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning : synthesizing efficient exploration and effective generalization .

exploiting locality in searching the web
published experiments on spidering the web suggest that , given training data in the form of a ( relatively small ) subgraph of the web containing a subset of a selected class of target pages , it is possible to conduct a directed search and find additional target pages significantly faster ( with fewer page retrievals ) than by performing a blind or uninformed random or systematic search , e.g. , breadth-first search . if true , this claim motivates a number of practical applications . unfortunately , these experiments were carried out in specialized domains or under conditions that are difficult to replicate . we present and apply an experimental framework designed to reexamine and resolve the basic claims of the earlier work , so that the supporting experiments can be replicated and built upon . we provide high-performance tools for building experimental spiders , make use of the ground truth and static nature of the wt10g trec web corpus , and rely on simple well understand machine learning techniques to conduct our experiments . in this paper , we describe the basic framework , motivate the experimental design , and report on our findings supporting and qualifying the conclusions of the earlier research .

implementing an intelligent version of the classical sliding-puzzle game for unix terminals using golang 's concurrency primitives
an intelligent version of the sliding-puzzle game is developed using the new go programming language , which uses a concurrent version of the a* informed search algorithm to power solver-bot that runs in the background . the game runs in computer system 's terminals . mainly , it was developed for unix-type systems but it works pretty well in nearly all the operating systems because of cross-platform compatibility of the programming language used . the game uses language 's concurrency primitives to simplify most of the hefty parts of the game . a real-time notification delivery architecture is developed using language 's built-in concurrency support , which performs similar to event based context aware invocations like we see on the web platform .

what the language you tweet says about your occupation
many aspects of people 's lives are proven to be deeply connected to their jobs . in this paper , we first investigate the distinct characteristics of major occupation categories based on tweets . from multiple social media platforms , we gather several types of user information . from users ' linkedin webpages , we learn their proficiencies . to overcome the ambiguity of self-reported information , a soft clustering approach is applied to extract occupations from crowd-sourced data . eight job categories are extracted , including marketing , administrator , start-up , editor , software engineer , public relation , office clerk , and designer . meanwhile , users ' posts on twitter provide cues for understanding their linguistic styles , interests , and personalities . our results suggest that people of different jobs have unique tendencies in certain language styles and interests . our results also clearly reveal distinctive levels in terms of big five traits for different jobs . finally , a classifier is built to predict job types based on the features extracted from tweets . a high accuracy indicates a strong discrimination power of language features for job prediction task .

android malware characterization using metadata and machine learning techniques
android malware has emerged as a consequence of the increasing popularity of smartphones and tablets . while most previous work focuses on inherent characteristics of android apps to detect malware , this study analyses indirect features and meta-data to identify patterns in malware applications . our experiments show that : ( 1 ) the permissions used by an application offer only moderate performance results ; ( 2 ) other features publicly available at android markets are more relevant in detecting malware , such as the application developer and certificate issuer , and ( 3 ) compact and efficient classifiers can be constructed for the early detection of malware applications prior to code inspection or sandboxing .

learning what data to learn
machine learning is essentially the sciences of playing with data . an adaptive data selection strategy , enabling to dynamically choose different data at various training stages , can reach a more effective model in a more efficient way . in this paper , we propose a deep reinforcement learning framework , which we call \emph { \textbf { n } eural \textbf { d } ata \textbf { f } ilter } ( \textbf { ndf } ) , to explore automatic and adaptive data selection in the training process . in particular , ndf takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data , such that the future accumulative reward ( e.g. , the convergence speed ) is maximized . in contrast to previous studies in data selection that is mainly based on heuristic strategies , ndf is quite generic and thus can be widely suitable for many machine learning tasks . taking neural network training with stochastic gradient descent ( sgd ) as an example , comprehensive experiments with respect to various neural network modeling ( e.g. , multi-layer perceptron networks , convolutional neural networks and recurrent neural networks ) and several applications ( e.g. , image classification and text understanding ) demonstrate that ndf powered sgd can achieve comparable accuracy with standard sgd process by using less data and fewer iterations .

learning where to attend with deep architectures for image tracking
we discuss an attentional model for simultaneous object tracking and recognition that is driven by gaze data . motivated by theories of perception , the model consists of two interacting pathways : identity and control , intended to mirror the what and where pathways in neuroscience models . the identity pathway models object appearance and performs classification using deep ( factored ) -restricted boltzmann machines . at each point in time the observations consist of foveated images , with decaying resolution toward the periphery of the gaze . the control pathway models the location , orientation , scale and speed of the attended object . the posterior distribution of these states is estimated with particle filtering . deeper in the control pathway , we encounter an attentional mechanism that learns to select gazes so as to minimize tracking uncertainty . unlike in our previous work , we introduce gaze selection strategies which operate in the presence of partial information and on a continuous action space . we show that a straightforward extension of the existing approach to the partial information setting results in poor performance , and we propose an alternative method based on modeling the reward surface as a gaussian process . this approach gives good performance in the presence of partial information and allows us to expand the action space from a small , discrete set of fixation points to a continuous domain .

on the choice of regions for generalized belief propagation
generalized belief propagation ( gbp ) has proven to be a promising technique for approximate inference tasks in ai and machine learning . however , the choice of a good set of clusters to be used in gbp has remained more of an art then a science until this day . this paper proposes a sequential approach to adding new clusters of nodes and their interactions ( i.e . `` regions '' ) to the approximation . we first review and analyze the recently introduced region graphs and find that three kinds of operations ( `` split '' , `` merge '' and `` death '' ) leave the free energy and ( under some conditions ) the fixed points of gbp invariant . this leads to the notion of `` weakly irreducible '' regions as the natural candidates to be added to the approximation . computational complexity of the gbp algorithm is controlled by restricting attention to regions with small `` region-width '' . combining the above with an efficient ( i.e . local in the graph ) measure to predict the improved accuracy of gbp leads to the sequential `` region pursuit '' algorithm for adding new regions bottom-up to the region graph . experiments show that this algorithm can indeed perform close to optimally .

idl-expressions : a formalism for representing and parsing finite languages in natural language processing
we propose a formalism for representation of finite languages , referred to as the class of idl-expressions , which combines concepts that were only considered in isolation in existing formalisms . the suggested applications are in natural language processing , more specifically in surface natural language generation and in machine translation , where a sentence is obtained by first generating a large set of candidate sentences , represented in a compact way , and then by filtering such a set through a parser . we study several formal properties of idl-expressions and compare this new formalism with more standard ones . we also present a novel parsing algorithm for idl-expressions and prove a non-trivial upper bound on its time complexity .

randomised variable neighbourhood search for multi objective optimisation
various local search approaches have recently been applied to machine scheduling problems under multiple objectives . their foremost consideration is the identification of the set of pareto optimal alternatives . an important aspect of successfully solving these problems lies in the definition of an appropriate neighbourhood structure . unclear in this context remains , how interdependencies within the fitness landscape affect the resolution of the problem . the paper presents a study of neighbourhood search operators for multiple objective flow shop scheduling . experiments have been carried out with twelve different combinations of criteria . to derive exact conclusions , small problem instances , for which the optimal solutions are known , have been chosen . statistical tests show that no single neighbourhood operator is able to equally identify all pareto optimal alternatives . significant improvements however have been obtained by hybridising the solution algorithm using a randomised variable neighbourhood search technique .

prioritized norms in formal argumentation
to resolve conflicts among norms , various nonmonotonic formalisms can be used to perform prioritized normative reasoning . meanwhile , formal argumentation provides a way to represent nonmonotonic logics . in this paper , we propose a representation of prioritized normative reasoning by argumentation . using hierarchical abstract normative systems , we define three kinds of prioritized normative reasoning approaches , called greedy , reduction , and optimization . then , after formulating an argumentation theory for a hierarchical abstract normative system , we show that for a totally ordered hierarchical abstract normative system , greedy and reduction can be represented in argumentation by applying the weakest link and the last link principles respectively , and optimization can be represented by introducing additional defeats capturing the idea that for each argument that contains a norm not belonging to the maximal obeyable set then this argument should be rejected .

safety verification and control for collision avoidance at road intersections
this paper presents the design of a supervisory algorithm that monitors safety at road intersections and overrides drivers with a safe input when necessary . the design of the supervisor consists of two parts : safety verification and control design . safety verification is the problem to determine if vehicles will be able to cross the intersection without colliding with current drivers ' inputs . we translate this safety verification problem into a jobshop scheduling problem , which minimizes the maximum lateness and evaluates if the optimal cost is zero . the zero optimal cost corresponds to the case in which all vehicles can cross each conflict area without collisions . computing the optimal cost requires solving a mixed integer nonlinear programming ( minlp ) problem due to the nonlinear second-order dynamics of the vehicles . we therefore estimate this optimal cost by formulating two related mixed integer linear programming ( milp ) problems that assume simpler vehicle dynamics . we prove that these two milp problems yield lower and upper bounds of the optimal cost . we also quantify the worst case approximation errors of these milp problems . we design the supervisor to override the vehicles with a safe control input if the milp problem that computes the upper bound yields a positive optimal cost . we theoretically demonstrate that the supervisor keeps the intersection safe and is non-blocking . computer simulations further validate that the algorithms can run in real time for problems of realistic size .

distance semantics for belief revision
a vast and interesting family of natural semantics for belief revision is defined . suppose one is given a distance d between any two models . one may then define the revision of a theory k by a formula a as the theory defined by the set of all those models of a that are closest , by d , to the set of models of k. this family is characterized by a set of rationality postulates that extends the agm postulates . the new postulates describe properties of iterated revisions .

eliciting worker preference for task completion
current crowdsourcing platforms provide little support for worker feedback . workers are sometimes invited to post free text describing their experience and preferences in completing tasks . they can also use forums such as turker nation1 to exchange preferences on tasks and requesters . in fact , crowdsourcing platforms rely heavily on observing workers and inferring their preferences implicitly . in this work , we believe that asking workers to indicate their preferences explicitly improve their experience in task completion and hence , the quality of their contributions . explicit elicitation can indeed help to build more accurate worker models for task completion that captures the evolving nature of worker preferences . we design a worker model whose accuracy is improved iteratively by requesting preferences for task factors such as required skills , task payment , and task relevance . we propose a generic framework , develop efficient solutions in realistic scenarios , and run extensive experiments that show the benefit of explicit preference elicitation over implicit ones with statistical significance .

identifying on-time reward delivery projects with estimating delivery duration on kickstarter
in crowdfunding platforms , people turn their prototype ideas into real products by raising money from the crowd , or invest in someone else 's projects . in reward-based crowdfunding platforms such as kickstarter and indiegogo , selecting accurate reward delivery duration becomes crucial for creators , backers , and platform providers to keep the trust between the creators and the backers , and the trust between the platform providers and users . according to kickstarter , 35 % backers did not receive rewards on time . unfortunately , little is known about on-time and late reward delivery projects , and there is no prior work to estimate reward delivery duration . to fill the gap , in this paper , we ( i ) extract novel features that reveal latent difficulty levels of project rewards ; ( ii ) build predictive models to identify whether a creator will deliver all rewards in a project on time or not ; and ( iii ) build a regression model to estimate accurate reward delivery duration ( i.e. , how long it will take to produce and deliver all the rewards ) . experimental results show that our models achieve good performance -- 82.5 % accuracy , 78.1 rmse , and 0.108 nrmse at the first 5 % of the longest reward delivery duration .

a new rational algorithm for view updating in relational databases
the dynamics of belief and knowledge is one of the major components of any autonomous system that should be able to incorporate new pieces of information . in order to apply the rationality result of belief dynamics theory to various practical problems , it should be generalized in two respects : first it should allow a certain part of belief to be declared as immutable ; and second , the belief state need not be deductively closed . such a generalization of belief dynamics , referred to as base dynamics , is presented in this paper , along with the concept of a generalized revision algorithm for knowledge bases ( horn or horn logic with stratified negation ) . we show that knowledge base dynamics has an interesting connection with kernel change via hitting set and abduction . in this paper , we show how techniques from disjunctive logic programming can be used for efficient ( deductive ) database updates . the key idea is to transform the given database together with the update request into a disjunctive ( datalog ) logic program and apply disjunctive techniques ( such as minimal model reasoning ) to solve the original update problem . the approach extends and integrates standard techniques for efficient query answering and integrity checking . the generation of a hitting set is carried out through a hyper tableaux calculus and magic set that is focused on the goal of minimality .

conflict analysis for pythagorean fuzzy information systems with group decision making
pythagorean fuzzy sets provide stronger ability than intuitionistic fuzzy sets to model uncertainty information and knowledge , but little effort has been paid to conflict analysis of pythagorean fuzzy information systems . in this paper , we present three types of positive , central , and negative alliances with different thresholds , and employ examples to illustrate how to construct the positive , central , and negative alliances . then we study conflict analysis of pythagorean fuzzy information systems based on bayesian minimum risk theory . finally , we investigate group conflict analysis of pythagorean fuzzy information systems based on bayesian minimum risk theory .

lexicographic probability , conditional probability , and nonstandard probability
the relationship between popper spaces ( conditional probability spaces that satisfy some regularity conditions ) , lexicographic probability systems ( lps 's ) , and nonstandard probability spaces ( nps 's ) is considered . if countable additivity is assumed , popper spaces and a subclass of lps 's are equivalent ; without the assumption of countable additivity , the equivalence no longer holds . if the state space is finite , lps 's are equivalent to nps 's . however , if the state space is infinite , nps 's are shown to be more general than lps 's .

range-based argumentation semantics as 2-valued models
characterizations of semi-stable and stage extensions in terms of 2-valued logical models are presented . to this end , the so-called gl-supported and gl-stage models are defined . these two classes of logical models are logic programming counterparts of the notion of range which is an established concept in argumentation semantics .

automatic white-box testing of first-order logic ontologies
a long-standing dream of artificial intelligence ( ai ) has pursued to encode commonsense knowledge into computer programs enabling machines to reason about our world and problems . this work offers a new practical insight towards the automatic testing of first-order logic ( fol ) ontologies . we introduce a novel fully automatic white-box testing framework for first-order logic ( fol ) ontologies . the application of the proposed testing method is fully automatic since a ) the automated generation of tests is only guided by the syntax of axioms and b ) the evaluation of tests is performed by automated theorem provers . our proposal enables the detection of defective axioms and , additionally , it also serves to demonstrate the suitability for reasoning purposes of those formulas included into fol ontologies . we validate our proposal by its practical application to different fol ontologies . in particular , dolce -- -consisting of around 200 axioms -- - , fpk ( formal proof of the kepler conjecture ) -- -which has been derived from the flyspeck project for its use in the cade atp system competition casc-j8 -- - , and adimen-sumo -- -which is an ontology with more than 7,000 axioms derived from sumo -- - . as result , we have detected several non-trivial defects that were hidden in those ontologies . further , we have obtained an improved version of adimen-sumo ( v2.6 ) by correcting all the defects detected during the practical application of our white-box testing method .

an efficient algorithm for contextual bandits with knapsacks , and an extension to concave objectives
we consider a contextual version of multi-armed bandit problem with global knapsack constraints . in each round , the outcome of pulling an arm is a scalar reward and a resource consumption vector , both dependent on the context , and the global knapsack constraints require the total consumption for each resource to be below some pre-fixed budget . the learning agent competes with an arbitrary set of context-dependent policies . this problem was introduced by badanidiyuru et al . ( 2014 ) , who gave a computationally inefficient algorithm with near-optimal regret bounds for it . we give a computationally efficient algorithm for this problem with slightly better regret bounds , by generalizing the approach of agarwal et al . ( 2014 ) for the non-constrained version of the problem . the computational time of our algorithm scales logarithmically in the size of the policy space . this answers the main open question of badanidiyuru et al . ( 2014 ) . we also extend our results to a variant where there are no knapsack constraints but the objective is an arbitrary lipschitz concave function of the sum of outcome vectors .

coding-theorem like behaviour and emergence of the universal distribution from resource-bounded algorithmic probability
previously referred to as ` miraculous ' in the scientific literature because of its powerful properties and its wide application as optimal solution to the problem of induction/inference , ( approximations to ) algorithmic probability ( ap ) and the associated universal distribution are ( or should be ) of the greatest importance in science . here we investigate the emergence , the rates of emergence and convergence , and the coding-theorem like behaviour of ap in turing-subuniversal models of computation . we investigate empirical distributions of computing models in the chomsky hierarchy . we introduce measures of algorithmic probability and algorithmic complexity based upon resource-bounded computation , in contrast to previously thoroughly investigated distributions produced from the output distribution of turing machines . this approach allows for numerical approximations to algorithmic ( kolmogorov-chaitin ) complexity-based estimations at each of the levels of a computational hierarchy . we demonstrate that all these estimations are correlated in rank and that they converge both in rank and values as a function of computational power , despite fundamental differences between computational models . in the context of natural processes that operate below the turing universal level because of finite resources and physical degradation , the investigation of natural biases stemming from algorithmic rules may shed light on the distribution of outcomes . we show that up to 60\ % of the simplicity/complexity bias in distributions produced even by the weakest of the computational models can be accounted for by algorithmic probability in its approximation to the universal distribution .

a logic programming approach to integration network inference
the discovery , representation and reconstruction of ( technical ) integration networks from network mining ( nm ) raw data is a difficult problem for enterprises . this is due to large and complex it landscapes within and across enterprise boundaries , heterogeneous technology stacks , and fragmented data . to remain competitive , visibility into the enterprise and partner it networks on different , interrelated abstraction levels is desirable . we present an approach to represent and reconstruct the integration networks from nm raw data using logic programming based on first-order logic . the raw data expressed as integration network model is represented as facts , on which rules are applied to reconstruct the network . we have built a system that is used to apply this approach to real-world enterprise landscapes and we report on our experience with this system .

latent collaborative retrieval
retrieval tasks typically require a ranking of items given a query . collaborative filtering tasks , on the other hand , learn to model user 's preferences over items . in this paper we study the joint problem of recommending items to a user with respect to a given query , which is a surprisingly common task . this setup differs from the standard collaborative filtering one in that we are given a query x user x item tensor for training instead of the more traditional user x item matrix . compared to document retrieval we do have a query , but we may or may not have content features ( we will consider both cases ) and we can also take account of the user 's profile . we introduce a factorized model for this new task that optimizes the top-ranked items returned for the given query and user . we report empirical results where it outperforms several baselines .

on the qualitative comparison of decisions having positive and negative features
making a decision is often a matter of listing and comparing positive and negative arguments . in such cases , the evaluation scale for decisions should be considered bipolar , that is , negative and positive values should be explicitly distinguished . that is what is done , for example , in cumulative prospect theory . however , contraryto the latter framework that presupposes genuine numerical assessments , human agents often decide on the basis of an ordinal ranking of the pros and the cons , and by focusing on the most salient arguments . in other terms , the decision process is qualitative as well as bipolar . in this article , based on a bipolar extension of possibility theory , we define and axiomatically characterize several decision rules tailored for the joint handling of positive and negative arguments in an ordinal setting . the simplest rules can be viewed as extensions of the maximin and maximax criteria to the bipolar case , and consequently suffer from poor decisive power . more decisive rules that refine the former are also proposed . these refinements agree both with principles of efficiency and with the spirit of order-of-magnitude reasoning , that prevails in qualitative decision theory . the most refined decision rule uses leximin rankings of the pros and the cons , and the ideas of counting arguments of equal strength and cancelling pros by cons . it is shown to come down to a special case of cumulative prospect theory , and to subsume the take the best heuristic studied by cognitive psychologists .

solving constraint satisfaction problems through belief propagation-guided decimation
message passing algorithms have proved surprisingly successful in solving hard constraint satisfaction problems on sparse random graphs . in such applications , variables are fixed sequentially to satisfy the constraints . message passing is run after each step . its outcome provides an heuristic to make choices at next step . this approach has been referred to as ` decimation , ' with reference to analogous procedures in statistical physics . the behavior of decimation procedures is poorly understood . here we consider a simple randomized decimation algorithm based on belief propagation ( bp ) , and analyze its behavior on random k-satisfiability formulae . in particular , we propose a tree model for its analysis and we conjecture that it provides asymptotically exact predictions in the limit of large instances . this conjecture is confirmed by numerical simulations .

convergence analysis of optimization algorithms
the regret bound of an optimization algorithms is one of the basic criteria for evaluating the performance of the given algorithm . by inspecting the differences between the regret bounds of traditional algorithms and adaptive one , we provide a guide for choosing an optimizer with respect to the given data set and the loss function . for analysis , we assume that the loss function is convex and its gradient is lipschitz continuous .

population sizing for genetic programming based upon decision making
this paper derives a population sizing relationship for genetic programming ( gp ) . following the population-sizing derivation for genetic algorithms in goldberg , deb , and clark ( 1992 ) , it considers building block decision making as a key facet . the analysis yields a gp-unique relationship because it has to account for bloat and for the fact that gp solutions often use subsolution multiple times . the population-sizing relationship depends upon tree size , solution complexity , problem difficulty and building block expression probability . the relationship is used to analyze and empirically investigate population sizing for three model gp problems named order , on-off and loud . these problems exhibit bloat to differing extents and differ in whether their solutions require the use of a building block multiple times .

norm based causal reasoning in textual corpus
truth based entailments are not sufficient for a good comprehension of nl . in fact , it can not deduce implicit information necessary to understand a text . on the other hand , norm based entailments are able to reach this goal . this idea was behind the development of frames ( minsky 75 ) and scripts ( schank 77 , schank 79 ) in the 70 's . but these theories are not formalized enough and their adaptation to new situations is far from being obvious . in this paper , we present a reasoning system which uses norms in a causal reasoning process in order to find the cause of an accident from a text describing it .

solving planning domains with polytree causal graphs is np-complete
we show that solving planning domains on binary variables with polytree causal graph is \np-complete . this is in contrast to a polynomial-time algorithm of domshlak and brafman that solves these planning domains for polytree causal graphs of bounded indegree .

an adaptive simulated annealing-based satellite observation scheduling method combined with a dynamic task clustering strategy
efficient scheduling is of great significance to rationally make use of scarce satellite resources . task clustering has been demonstrated to realize an effective strategy to improve the efficiency of satellite scheduling . however , the previous task clustering strategy is static . that is , it is integrated into the scheduling in a two-phase manner rather than in a dynamic fashion , without expressing its full potential in improving the satellite scheduling performance . in this study , we present an adaptive simulated annealing based scheduling algorithm aggregated with a dynamic task clustering strategy ( or asa-dtc for short ) for satellite observation scheduling problems ( sosps ) . first , we develop a formal model for the scheduling of earth observing satellites . second , we analyze the related constraints involved in the observation task clustering process . thirdly , we detail an implementation of the dynamic task clustering strategy and the adaptive simulated annealing algorithm . the adaptive simulated annealing algorithm is efficient , with the endowment of some sophisticated mechanisms , i.e . adaptive temperature control , tabu-list based revisiting avoidance mechanism , and intelligent combination of neighborhood structures . finally , we report on experimental simulation studies to demonstrate the competitive performance of asa-dtc . moreover , we show that asa-dtc is especially effective when sosps contain a large number of targets or these targets are densely distributed in a certain area .

sap speaks pddl : exploiting a software-engineering model for planning in business process management
planning is concerned with the automated solution of action sequencing problems described in declarative languages giving the action preconditions and effects . one important application area for such technology is the creation of new processes in business process management ( bpm ) , which is essential in an ever more dynamic business environment . a major obstacle for the application of planning in this area lies in the modeling . obtaining a suitable model to plan with -- ideally a description in pddl , the most commonly used planning language -- is often prohibitively complicated and/or costly . our core observation in this work is that this problem can be ameliorated by leveraging synergies with model-based software development . our application at sap , one of the leading vendors of enterprise software , demonstrates that even one-to-one model re-use is possible . the model in question is called status and action management ( sam ) . it describes the behavior of business objects ( bo ) , i.e. , large-scale data structures , at a level of abstraction corresponding to the language of business experts . sam covers more than 400 kinds of bos , each of which is described in terms of a set of status variables and how their values are required for , and affected by , processing steps ( actions ) that are atomic from a business perspective . sam was developed by sap as part of a major model-based software engineering effort . we show herein that one can use this same model for planning , thus obtaining a bpm planning application that incurs no modeling overhead at all . we compile sam into a variant of pddl , and adapt an off-the-shelf planner to solve this kind of problem . thanks to the resulting technology , business experts may create new processes simply by specifying the desired behavior in terms of status variable value changes : effectively , by describing the process in their own language .

causal graph justifications of logic programs
in this work we propose a multi-valued extension of logic programs under the stable models semantics where each true atom in a model is associated with a set of justifications . these justifications are expressed in terms of causal graphs formed by rule labels and edges that represent their application ordering . for positive programs , we show that the causal justifications obtained for a given atom have a direct correspon- dence to ( relevant ) syntactic proofs of that atom using the program rules involved in the graphs . the most interesting contribution is that this causal information is obtained in a purely semantic way , by algebraic op- erations ( product , sum and application ) on a lattice of causal values whose ordering relation expresses when a justification is stronger than another . finally , for programs with negation , we define the concept of causal stable model by introducing an analogous transformation to gelfond and lifschitz 's program reduct . as a result , default negation behaves as `` absence of proof '' and no justification is derived from negative liter

long-term causal effects via behavioral game theory
planned experiments are the gold standard in reliably comparing the causal effect of switching from a baseline policy to a new policy . one critical shortcoming of classical experimental methods , however , is that they typically do not take into account the dynamic nature of response to policy changes . for instance , in an experiment where we seek to understand the effects of a new ad pricing policy on auction revenue , agents may adapt their bidding in response to the experimental pricing changes . thus , causal effects of the new pricing policy after such adaptation period , the { \em long-term causal effects } , are not captured by the classical methodology even though they clearly are more indicative of the value of the new policy . here , we formalize a framework to define and estimate long-term causal effects of policy changes in multiagent economies . central to our approach is behavioral game theory , which we leverage to formulate the ignorability assumptions that are necessary for causal inference . under such assumptions we estimate long-term causal effects through a latent space approach , where a behavioral model of how agents act conditional on their latent behaviors is combined with a temporal model of how behaviors evolve over time .

considerations on construction ontologies
the paper proposes an analysis on some existent ontologies , in order to point out ways to resolve semantic heterogeneity in information systems . authors are highlighting the tasks in a knowledge acquisiton system and identifying aspects related to the addition of new information to an intelligent system . a solution is proposed , as a combination of ontology reasoning services and natural languages generation . a multi-agent system will be conceived with an extractor agent , a reasoner agent and a competence management agent .

a framework for easing the development of applications embedding answer set programming
answer set programming ( asp ) is a well-established declarative problem solving paradigm which became widely used in ai and recognized as a powerful tool for knowledge representation and reasoning ( krr ) , especially for its high expressiveness and the ability to deal also with incomplete knowledge . recently , thanks to the availability of a number of robust and efficient implementations , asp has been increasingly employed in a number of different domains , and used for the development of industrial-level and enterprise applications . this made clear the need for proper development tools and interoperability mechanisms for easing interaction and integration with external systems in the widest range of real-world scenarios , including mobile applications and educational contexts . in this work we present a framework for integrating the krr capabilities of asp into generic applications . we show the use of the framework by illustrating proper specializations for some relevant asp systems over different platforms , including the mobile setting ; furthermore , the potential of the framework for educational purposes is illustrated by means of the development of several asp-based applications .

stratified labelings for abstract argumentation
we introduce stratified labelings as a novel semantical approach to abstract argumentation frameworks . compared to standard labelings , stratified labelings provide a more fine-grained assessment of the controversiality of arguments using ranks instead of the usual labels in , out , and undecided . we relate the framework of stratified labelings to conditional logic and , in particular , to the system z ranking functions .

a simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling
we introduce a simple and accurate neural model for dependency-based semantic role labeling . our model predicts predicate-argument dependencies relying on states of a bidirectional lstm encoder . the semantic role labeler achieves competitive performance on english , even without any kind of syntactic information and only using local inference . however , when automatically predicted part-of-speech tags are provided as input , it substantially outperforms all previous local models and approaches the best reported results on the english conll-2009 dataset . we also consider chinese , czech and spanish where our approach also achieves competitive results . syntactic parsers are unreliable on out-of-domain data , so standard ( i.e. , syntactically-informed ) srl models are hindered when tested in this setting . our syntax-agnostic model appears more robust , resulting in the best reported results on standard out-of-domain test sets .

review and evaluation of feature selection algorithms in synthetic problems
the main purpose of feature subset selection is to find a reduced subset of attributes from a data set described by a feature set . the task of a feature selection algorithm ( fsa ) is to provide with a computational solution motivated by a certain definition of relevance or by a reliable evaluation measure . in this paper several fundamental algorithms are studied to assess their performance in a controlled experimental scenario . a measure to evaluate fsas is devised that computes the degree of matching between the output given by a fsa and the known optimal solutions . an extensive experimental study on synthetic problems is carried out to assess the behaviour of the algorithms in terms of solution accuracy and size as a function of the relevance , irrelevance , redundancy and size of the data samples . the controlled experimental conditions facilitate the derivation of better-supported and meaningful conclusions .

scene image is non-mutually exclusive - a fuzzy qualitative scene understanding
ambiguity or uncertainty is a pervasive element of many real world decision making processes . variation in decisions is a norm in this situation when the same problem is posed to different subjects . psychological and metaphysical research had proven that decision making by human is subjective . it is influenced by many factors such as experience , age , background , etc . scene understanding is one of the computer vision problems that fall into this category . conventional methods relax this problem by assuming scene images are mutually exclusive ; and therefore , focus on developing different approaches to perform the binary classification tasks . in this paper , we show that scene images are non-mutually exclusive , and propose the fuzzy qualitative rank classifier ( fqrc ) to tackle the aforementioned problems . the proposed fqrc provides a ranking interpretation instead of binary decision . evaluations in term of qualitative and quantitative using large numbers and challenging public scene datasets have shown the effectiveness of our proposed method in modeling the non-mutually exclusive scene images .

deepdga : adversarially-tuned domain generation and detection
many malware families utilize domain generation algorithms ( dgas ) to establish command and control ( c & c ) connections . while there are many methods to pseudorandomly generate domains , we focus in this paper on detecting ( and generating ) domains on a per-domain basis which provides a simple and flexible means to detect known dga families . recent machine learning approaches to dga detection have been successful on fairly simplistic dgas , many of which produce names of fixed length . however , models trained on limited datasets are somewhat blind to new dga variants . in this paper , we leverage the concept of generative adversarial networks to construct a deep learning based dga that is designed to intentionally bypass a deep learning based detector . in a series of adversarial rounds , the generator learns to generate domain names that are increasingly more difficult to detect . in turn , a detector model updates its parameters to compensate for the adversarially generated domains . we test the hypothesis of whether adversarially generated domains may be used to augment training sets in order to harden other machine learning models against yet-to-be-observed dgas . we detail solutions to several challenges in training this character-based generative adversarial network ( gan ) . in particular , our deep learning architecture begins as a domain name auto-encoder ( encoder + decoder ) trained on domains in the alexa one million . then the encoder and decoder are reassembled competitively in a generative adversarial network ( detector + generator ) , with novel neural architectures and training strategies to improve convergence .

on the accuracy and running time of gsat
randomized algorithms for deciding satisfiability were shown to be effective in solving problems with thousands of variables . however , these algorithms are not complete . that is , they provide no guarantee that a satisfying assignment , if one exists , will be found . thus , when studying randomized algorithms , there are two important characteristics that need to be considered : the running time and , even more importantly , the accuracy -- - a measure of likelihood that a satisfying assignment will be found , provided one exists . in fact , we argue that without a reference to the accuracy , the notion of the running time for randomized algorithms is not well-defined . in this paper , we introduce a formal notion of accuracy . we use it to define a concept of the running time . we use both notions to study the random walk strategy gsat algorithm . we investigate the dependence of accuracy on properties of input formulas such as clause-to-variable ratio and the number of satisfying assignments . we demonstrate that the running time of gsat grows exponentially in the number of variables of the input formula for randomly generated 3-cnf formulas and for the formulas encoding 3- and 4-colorability of graphs .

web services : a process algebra approach
it is now well-admitted that formal methods are helpful for many issues raised in the web service area . in this paper we present a framework for the design and verification of wss using process algebras and their tools . we define a two-way mapping between abstract specifications written using these calculi and executable web services written in bpel4ws . several choices are available : design and correct errors in bpel4ws , using process algebra verification tools , or design and correct in process algebra and automatically obtaining the corresponding bpel4ws code . the approaches can be combined . process algebra are not useful only for temporal logic verification : we remark the use of simulation/bisimulation both for verification and for the hierarchical refinement design method . it is worth noting that our approach allows the use of any process algebra depending on the needs of the user at different levels ( expressiveness , existence of reasoning tools , user expertise ) .

reasoning with memory augmented neural networks for language comprehension
hypothesis testing is an important cognitive process that supports human reasoning . in this paper , we introduce a computational hypothesis testing approach based on memory augmented neural networks . our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test . we apply the proposed approach to language comprehension task by using neural semantic encoders ( nse ) . our nse models achieve the state-of-the-art results showing an absolute improvement of 1.2 % to 2.6 % accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the children 's book test ( cbt ) and who-did-what ( wdw ) news article datasets .

an optimized hybrid approach for path finding
path finding algorithm addresses problem of finding shortest path from source to destination avoiding obstacles . there exist various search algorithms namely a* , dijkstra 's and ant colony optimization . unlike most path finding algorithms which require destination co-ordinates to compute path , the proposed algorithm comprises of a new method which finds path using backtracking without requiring destination co-ordinates . moreover , in existing path finding algorithm , the number of iterations required to find path is large . hence , to overcome this , an algorithm is proposed which reduces number of iterations required to traverse the path . the proposed algorithm is hybrid of backtracking and a new technique ( modified 8- neighbor approach ) . the proposed algorithm can become essential part in location based , network , gaming applications . grid traversal , navigation , gaming applications , mobile robot and artificial intelligence .

mudos-ng : multi-document summaries using n-gram graphs ( tech report )
this report describes the mudos-ng summarization system , which applies a set of language-independent and generic methods for generating extractive summaries . the proposed methods are mostly combinations of simple operators on a generic character n-gram graph representation of texts . this work defines the set of used operators upon n-gram graphs and proposes using these operators within the multi-document summarization process in such subtasks as document analysis , salient sentence selection , query expansion and redundancy control . furthermore , a novel chunking methodology is used , together with a novel way to assign concepts to sentences for query expansion . the experimental results of the summarization system , performed upon widely used corpora from the document understanding and the text analysis conferences , are promising and provide evidence for the potential of the generic methods introduced . this work aims to designate core methods exploiting the n-gram graph representation , providing the basis for more advanced summarization systems .

a fuzzy directional distance measure
the measure of distance between two fuzzy sets is a fundamental tool within fuzzy set theory , however , distance measures currently within the literature use a crisp value to represent the distance between fuzzy sets . a real valued distance measure is developed into a fuzzy distance measure which better reflects the uncertainty inherent in fuzzy sets and a fuzzy directional distance measure is presented , which accounts for the direction of change between fuzzy sets . a multiplicative version is explored as a full maximal assignment is computationally intractable so an intermediate solution is offered .

dependent landmark drift : robust point set registration based on the gaussian mixture model with a statistical shape model
the goal of point set registration is to find point-by-point correspondences between point sets , each of which characterizes the shape of an object . because local preservation of object geometry is assumed , prevalent algorithms in the area can often elegantly solve the problems without using geometric information specific to the objects . this means that registration performance can be further improved by using prior knowledge of object geometry . in this paper , we propose a novel point set registration method using the gaussian mixture model with prior shape information encoded as a statistical shape model . our transformation model is defined as a combination of the similar transformation , motion coherence , and the statistical shape model . therefore , the proposed method works effectively if the target point set includes outliers and missing regions , or if it is rotated . the computational cost can be reduced to linear , and therefore the method is scalable to large point sets . the effectiveness of the method will be verified through comparisons with existing algorithms using datasets concerning human body shapes , hands , and faces .

the backtracking survey propagation algorithm for solving random k-sat problems
discrete combinatorial optimization has a central role in many scientific disciplines , however , for hard problems we lack linear time algorithms that would allow us to solve very large instances . moreover , it is still unclear what are the key features that make a discrete combinatorial optimization problem hard to solve . here we study random k-satisfiability problems with $ k=3,4 $ , which are known to be very hard close to the sat-unsat threshold , where problems stop having solutions . we show that the backtracking survey propagation algorithm , in a time practically linear in the problem size , is able to find solutions very close to the threshold , in a region unreachable by any other algorithm . all solutions found have no frozen variables , thus supporting the conjecture that only unfrozen solutions can be found in linear time , and that a problem becomes impossible to solve in linear time when all solutions contain frozen variables .

anisotropic selection in cellular genetic algorithms
in this paper we introduce a new selection scheme in cellular genetic algorithms ( cgas ) . anisotropic selection ( as ) promotes diversity and allows accurate control of the selective pressure . first we compare this new scheme with the classical rectangular grid shapes solution according to the selective pressure : we can obtain the same takeover time with the two techniques although the spreading of the best individual is different . we then give experimental results that show to what extent as promotes the emergence of niches that support low coupling and high cohesion . finally , using a cga with anisotropic selection on a quadratic assignment problem we show the existence of an anisotropic optimal value for which the best average performance is observed . further work will focus on the selective pressure self-adjustment ability provided by this new selection scheme .

multiset estimates and combinatorial synthesis
the paper addresses an approach to ordinal assessment of alternatives based on assignment of elements into an ordinal scale . basic versions of the assessment problems are formulated while taking into account the number of levels at a basic ordinal scale [ 1,2 , ... , l ] and the number of assigned elements ( e.g. , 1,2,3 ) . the obtained estimates are multisets ( or bags ) ( cardinality of the multiset equals a constant ) . scale-posets for the examined assessment problems are presented . 'interval multiset estimates ' are suggested . further , operations over multiset estimates are examined : ( a ) integration of multiset estimates , ( b ) proximity for multiset estimates , ( c ) comparison of multiset estimates , ( d ) aggregation of multiset estimates , and ( e ) alignment of multiset estimates . combinatorial synthesis based on morphological approach is examined including the modified version of the approach with multiset estimates of design alternatives . knapsack-like problems with multiset estimates are briefly described as well . the assessment approach , multiset-estimates , and corresponding combinatorial problems are illustrated by numerical examples .

clustering and feature selection using sparse principal component analysis
in this paper , we study the application of sparse principal component analysis ( pca ) to clustering and feature selection problems . sparse pca seeks sparse factors , or linear combinations of the data variables , explaining a maximum amount of variance in the data while having only a limited number of nonzero coefficients . pca is often used as a simple clustering technique and sparse factors allow us here to interpret the clusters in terms of a reduced set of variables . we begin with a brief introduction and motivation on sparse pca and detail our implementation of the algorithm in d'aspremont et al . ( 2005 ) . we then apply these results to some classic clustering and feature selection problems arising in biology .

modeling state in software debugging of vhdl-rtl designs -- a model-based diagnosis approach
in this paper we outline an approach of applying model-based diagnosis to the field of automatic software debugging of hardware designs . we present our value-level model for debugging vhdl-rtl designs and show how to localize the erroneous component responsible for an observed misbehavior . furthermore , we discuss an extension of our model that supports the debugging of sequential circuits , not only at a given point in time , but also allows for considering the temporal behavior of vhdl-rtl designs . the introduced model is capable of handling state inherently present in every sequential circuit . the principal applicability of the new model is outlined briefly and we use industrial-sized real world examples from the iscas'85 benchmark suite to discuss the scalability of our approach .

end-to-end attention-based large vocabulary speech recognition
many of the current state-of-the-art large vocabulary continuous speech recognition systems ( lvcsr ) are hybrids of neural networks and hidden markov models ( hmms ) . most of these systems contain separate components that deal with the acoustic modelling , language modelling and sequence decoding . we investigate a more direct approach in which the hmm is replaced with a recurrent neural network ( rnn ) that performs sequence prediction directly at the character level . alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the rnn . for each predicted character , the attention mechanism scans the input sequence and chooses relevant frames . we propose two methods to speed up this operation : limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames , thereby reducing source sequence length . integrating an n-gram language model into the decoding process yields recognition accuracies similar to other hmm-free rnn-based approaches .

a measure for dialog complexity and its application in streamlining service operations
dialog is a natural modality for interaction between customers and businesses in the service industry . as customers call up the service provider , their interactions may be routine or extraordinary . we believe that these interactions , when seen as dialogs , can be analyzed to obtain a better understanding of customer needs and how to efficiently address them . we introduce the idea of a dialog complexity measure to characterize multi-party interactions , propose a general data-driven method to calculate it , use it to discover insights in public and enterprise dialog datasets , and demonstrate its beneficial usage in facilitating better handling of customer requests and evaluating service agents .

proceedings of the twenty-fourth conference on uncertainty in artificial intelligence ( 2008 )
this is the proceedings of the twenty-fourth conference on uncertainty in artificial intelligence , which was held in helsinki , finland , july 9 - 12 2008 .

solving combinatorial optimization problems with quantum inspired evolutionary algorithm tuned using a novel heuristic method
quantum inspired evolutionary algorithms were proposed more than a decade ago and have been employed for solving a wide range of difficult search and optimization problems . a number of changes have been proposed to improve performance of canonical qea . however , canonical qea is one of the few evolutionary algorithms , which uses a search operator with relatively large number of parameters . it is well known that performance of evolutionary algorithms is dependent on specific value of parameters for a given problem . the advantage of having large number of parameters in an operator is that the search process can be made more powerful even with a single operator without requiring a combination of other operators for exploration and exploitation . however , the tuning of operators with large number of parameters is complex and computationally expensive . this paper proposes a novel heuristic method for tuning parameters of canonical qea . the tuned qea outperforms canonical qea on a class of discrete combinatorial optimization problems which , validates the design of the proposed parameter tuning framework . the proposed framework can be used for tuning other algorithms with both large and small number of tunable parameters .

artificial intelligence and asymmetric information theory
when human agents come together to make decisions , it is often the case that one human agent has more information than the other . this phenomenon is called information asymmetry and this distorts the market . often if one human agent intends to manipulate a decision in its favor the human agent can signal wrong or right information . alternatively , one human agent can screen for information to reduce the impact of asymmetric information on decisions . with the advent of artificial intelligence , signaling and screening have been made easier . this paper studies the impact of artificial intelligence on the theory of asymmetric information . it is surmised that artificial intelligent agents reduce the degree of information asymmetry and thus the market where these agents are deployed become more efficient . it is also postulated that the more artificial intelligent agents there are deployed in the market the less is the volume of trades in the market . this is because for many trades to happen the asymmetry of information on goods and services to be traded should exist , creating a sense of arbitrage .

reasoning about robocup soccer narratives
this paper presents an approach for learning to translate simple narratives , i.e. , texts ( sequences of sentences ) describing dynamic systems , into coherent sequences of events without the need for labeled training data . our approach incorporates domain knowledge in the form of preconditions and effects of events , and we show that it outperforms state-of-the-art supervised learning systems on the task of reconstructing robocup soccer games from their commentaries .

counterfactuals and policy analysis in structural models
evaluation of counterfactual queries ( e.g. , `` if a were true , would c have been true ? '' ) is important to fault diagnosis , planning , determination of liability , and policy analysis . we present a method of revaluating counterfactuals when the underlying causal model is represented by structural models - a nonlinear generalization of the simultaneous equations models commonly used in econometrics and social sciences . this new method provides a coherent means for evaluating policies involving the control of variables which , prior to enacting the policy were influenced by other variables in the system .

automatic pattern classification by unsupervised learning using dimensionality reduction of data with mirroring neural networks
this paper proposes an unsupervised learning technique by using multi-layer mirroring neural network and forgy 's clustering algorithm . multi-layer mirroring neural network is a neural network that can be trained with generalized data inputs ( different categories of image patterns ) to perform non-linear dimensionality reduction and the resultant low-dimensional code is used for unsupervised pattern classification using forgy 's algorithm . by adapting the non-linear activation function ( modified sigmoidal function ) and initializing the weights and bias terms to small random values , mirroring of the input pattern is initiated . in training , the weights and bias terms are changed in such a way that the input presented is reproduced at the output by back propagating the error . the mirroring neural network is capable of reducing the input vector to a great degree ( approximately 1/30th the original size ) and also able to reconstruct the input pattern at the output layer from this reduced code units . the feature set ( output of central hidden layer ) extracted from this network is fed to forgy 's algorithm , which classify input data patterns into distinguishable classes . in the implementation of forgy 's algorithm , initial seed points are selected in such a way that they are distant enough to be perfectly grouped into different categories . thus a new method of unsupervised learning is formulated and demonstrated in this paper . this method gave impressive results when applied to classification of different image patterns .

a knowledge compilation map
we propose a perspective on knowledge compilation which calls for analyzing different compilation approaches according to two key dimensions : the succinctness of the target compilation language , and the class of queries and transformations that the language supports in polytime . we then provide a knowledge compilation map , which analyzes a large number of existing target compilation languages according to their succinctness and their polytime transformations and queries . we argue that such analysis is necessary for placing new compilation approaches within the context of existing ones . we also go beyond classical , flat target compilation languages based on cnf and dnf , and consider a richer , nested class based on directed acyclic graphs ( such as obdds ) , which we show to include a relatively large number of target compilation languages .

learning modulo theories for preference elicitation in hybrid domains
this paper introduces cleo , a novel preference elicitation algorithm capable of recommending complex objects in hybrid domains , characterized by both discrete and continuous attributes and constraints defined over them . the algorithm assumes minimal initial information , i.e. , a set of catalog attributes , and defines decisional features as logic formulae combining boolean and algebraic constraints over the attributes . the ( unknown ) utility of the decision maker ( dm ) is modelled as a weighted combination of features . cleo iteratively alternates a preference elicitation step , where pairs of candidate solutions are selected based on the current utility model , and a refinement step where the utility is refined by incorporating the feedback received . the elicitation step leverages a max-smt solver to return optimal hybrid solutions according to the current utility model . the refinement step is implemented as learning to rank , and a sparsifying norm is used to favour the selection of few informative features in the combinatorial space of candidate decisional features . cleo is the first preference elicitation algorithm capable of dealing with hybrid domains , thanks to the use of max-smt technology , while retaining uncertainty in the dm utility and noisy feedback . experimental results on complex recommendation tasks show the ability of cleo to quickly focus towards optimal solutions , as well as its capacity to recover from suboptimal initial choices . while no competitors exist in the hybrid setting , cleo outperforms a state-of-the-art bayesian preference elicitation algorithm when applied to a purely discrete task .

sonet network design problems
this paper presents a new method and a constraint-based objective function to solve two problems related to the design of optical telecommunication networks , namely the synchronous optical network ring assignment problem ( srap ) and the intra-ring synchronous optical network design problem ( idp ) . these network topology problems can be represented as a graph partitioning with capacity constraints as shown in previous works . we present here a new objective function and a new local search algorithm to solve these problems . experiments conducted in comet allow us to compare our method to previous ones and show that we obtain better results .

book : storing algorithm-invariant episodes for deep reinforcement learning
we introduce a novel method to train agents of reinforcement learning ( rl ) by sharing knowledge in a way similar to the concept of using a book . the recorded information in the form of a book is the main means by which humans learn knowledge . nevertheless , the conventional deep rl methods have mainly focused either on experiential learning where the agent learns through interactions with the environment from the start or on imitation learning that tries to mimic the teacher . contrary to these , our proposed book learning shares key information among different agents in a book-like manner by delving into the following two characteristic features : ( 1 ) by defining the linguistic function , input states can be clustered semantically into a relatively small number of core clusters , which are forwarded to other rl agents in a prescribed manner . ( 2 ) by defining state priorities and the contents for recording , core experiences can be selected and stored in a small container . we call this container as ` book ' . our method learns hundreds to thousand times faster than the conventional methods by learning only a handful of core cluster information , which shows that deep rl agents can effectively learn through the shared knowledge from other agents .

formalizing scenario analysis
we propose a formal treatment of scenarios in the context of a dialectical argumentation formalism for qualitative reasoning about uncertain propositions . our formalism extends prior work in which arguments for and against uncertain propositions were presented and compared in interaction spaces called agoras . we now define the notion of a scenario in this framework and use it to define a set of qualitative uncertainty labels for propositions across a collection of scenarios . this work is intended to lead to a formal theory of scenarios and scenario analysis .

abstractive multi-document summarization via phrase selection and merging
we propose an abstraction-based multi-document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences , namely , noun/verb phrases . different from existing abstraction-based approaches , our method first constructs a pool of concepts and facts represented by phrases from the input documents . then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints . we employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary . experimental results on the benchmark data set tac 2011 show that our framework outperforms the state-of-the-art models under automated pyramid evaluation metric , and achieves reasonably well results on manual linguistic quality evaluation .

mining complex hydrobiological data with galois lattices
we have used galois lattices for mining hydrobiological data . these data are about macrophytes , that are macroscopic plants living in water bodies . these plants are characterized by several biological traits , that own several modalities . our aim is to cluster the plants according to their common traits and modalities and to find out the relations between traits . galois lattices are efficient methods for such an aim , but apply on binary data . in this article , we detail a few approaches we used to transform complex hydrobiological data into binary data and compare the first results obtained thanks to galois lattices .

dynamic safe interruptibility for decentralized multi-agent reinforcement learning
in reinforcement learning , agents learn by performing actions and observing their outcomes . sometimes , it is desirable for a human operator to \textit { interrupt } an agent in order to prevent dangerous situations from happening . yet , as part of their learning process , agents may link these interruptions , that impact their reward , to specific states and deliberately avoid them . the situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions , but also from those of other agents . orseau and armstrong defined \emph { safe interruptibility } for one learner , but their work does not naturally extend to multi-agent systems . this paper introduces \textit { dynamic safe interruptibility } , an alternative definition more suited to decentralized learning problems , and studies this notion in two learning frameworks : \textit { joint action learners } and \textit { independent learners } . we give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners , yet show that these conditions are not sufficient for independent learners . we show however that if agents can detect interruptions , it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners .

operator selection while planning under uncertainty
this paper describes the best first search strategy used by u-plan ( mansell 1993a ) , a planning system that constructs quantitatively ranked plans given an incomplete description of an uncertain environment . u-plan uses uncertain and incomplete evidence de scribing the environment , characterizes it using a dempster-shafer interval , and generates a set of possible world states . plan construction takes place in an abstraction hierarchy where strategic decisions are made before tactical decisions . search through this abstraction hierarchy is guided by a quantitative measure ( expected fulfillment ) based on decision theory . the search strategy is best first with the provision to update expected fulfillment and review previous decisions in the light of planning developments . u-plan generates multiple plans for multiple possible worlds , and attempts to use existing plans for new world situations . a super-plan is then constructed , based on merging the set of plans and appropriately timed knowledge acquisition operators , which are used to decide between plan alternatives during plan execution .

variations on memetic algorithms for graph coloring problems
graph vertex coloring with a given number of colors is a well-known and much-studied np-complete problem.the most effective methods to solve this problem are proved to be hybrid algorithms such as memetic algorithms or quantum annealing . those hybrid algorithms use a powerful local search inside a population-based algorithm.this paper presents a new memetic algorithm based on one of the most effective algorithms : the hybrid evolutionary algorithm hea from galinier and hao ( 1999 ) .the proposed algorithm , denoted head - for hea in duet - works with a population of only two individuals.moreover , a new way of managing diversity is brought by head.these two main differences greatly improve the results , both in terms of solution quality and computational time.head has produced several good results for the popular dimacs benchmark graphs , such as 222-colorings for \textless { } dsjc1000.9\textgreater { } , 81-colorings for \textless { } flat1000\_76\_0\textgreater { } and even 47-colorings for \textless { } dsjc500.5\textgreater { } and 82-colorings for \textless { } dsjc1000.5\textgreater { } .

boolvar/pb v1.0 , a java library for translating pseudo-boolean constraints into cnf formulae
boolvar/pb is an open source java library dedicated to the translation of pseudo-boolean constraints into cnf formulae . input constraints can be categorized with tags . several encoding schemes are implemented in a way that each input constraint can be translated using one or several encoders , according to the related tags . the library can be easily extended by adding new encoders and / or new output formats .

numvc : an efficient local search algorithm for minimum vertex cover
the minimum vertex cover ( mvc ) problem is a prominent np-hard combinatorial optimization problem of great importance in both theory and application . local search has proved successful for this problem . however , there are two main drawbacks in state-of-the-art mvc local search algorithms . first , they select a pair of vertices to exchange simultaneously , which is time-consuming . secondly , although using edge weighting techniques to diversify the search , these algorithms lack mechanisms for decreasing the weights . to address these issues , we propose two new strategies : two-stage exchange and edge weighting with forgetting . the two-stage exchange strategy selects two vertices to exchange separately and performs the exchange in two stages . the strategy of edge weighting with forgetting not only increases weights of uncovered edges , but also decreases some weights for each edge periodically . these two strategies are used in designing a new mvc local search algorithm , which is referred to as numvc . we conduct extensive experimental studies on the standard benchmarks , namely dimacs and bhoslib . the experiment comparing numvc with state-of-the-art heuristic algorithms show that numvc is at least competitive with the nearest competitor namely pls on the dimacs benchmark , and clearly dominates all competitors on the bhoslib benchmark . also , experimental results indicate that numvc finds an optimal solution much faster than the current best exact algorithm for maximum clique on random instances as well as some structured ones . moreover , we study the effectiveness of the two strategies and the run-time behaviour through experimental analysis .

making the v in vqa matter : elevating the role of image understanding in visual question answering
problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable . however , inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities , resulting in models that ignore visual information , leading to an inflated sense of their capability . we propose to counter these language priors for the task of visual question answering ( vqa ) and make vision ( the v in vqa ) matter ! specifically , we balance the popular vqa dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image , but rather a pair of similar images that result in two different answers to the question . our dataset is by construction more balanced than the original vqa dataset and has approximately twice the number of image-question pairs . our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the visual question answering dataset and challenge ( vqa v2.0 ) . we further benchmark a number of state-of-art vqa models on our balanced dataset . all models perform significantly worse on our balanced dataset , suggesting that these models have indeed learned to exploit language priors . this finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners . finally , our data collection protocol for identifying complementary images enables us to develop a novel interpretable model , which in addition to providing an answer to the given ( image , question ) pair , also provides a counter-example based explanation . specifically , it identifies an image that is similar to the original image , but it believes has a different answer to the same question . this can help in building trust for machines among their users .

maximum weight matching via max-product belief propagation
max-product `` belief propagation '' is an iterative , local , message-passing algorithm for finding the maximum a posteriori ( map ) assignment of a discrete probability distribution specified by a graphical model . despite the spectacular success of the algorithm in many application areas such as iterative decoding , computer vision and combinatorial optimization which involve graphs with many cycles , theoretical results about both correctness and convergence of the algorithm are known in few cases ( weiss-freeman wainwright , yeddidia-weiss-freeman , richardson-urbanke } . in this paper we consider the problem of finding the maximum weight matching ( mwm ) in a weighted complete bipartite graph . we define a probability distribution on the bipartite graph whose map assignment corresponds to the mwm . we use the max-product algorithm for finding the map of this distribution or equivalently , the mwm on the bipartite graph . even though the underlying bipartite graph has many short cycles , we find that surprisingly , the max-product algorithm always converges to the correct map assignment as long as the map assignment is unique . we provide a bound on the number of iterations required by the algorithm and evaluate the computational cost of the algorithm . we find that for a graph of size $ n $ , the computational cost of the algorithm scales as $ o ( n^3 ) $ , which is the same as the computational cost of the best known algorithm . finally , we establish the precise relation between the max-product algorithm and the celebrated { \em auction } algorithm proposed by bertsekas . this suggests possible connections between dual algorithm and max-product algorithm for discrete optimization problems .

extraction of evidence tables from abstracts of randomized clinical trials using a maximum entropy classifier and global constraints
systematic use of the published results of randomized clinical trials is increasingly important in evidence-based medicine . in order to collate and analyze the results from potentially numerous trials , evidence tables are used to represent trials concerning a set of interventions of interest . an evidence table has columns for the patient group , for each of the interventions being compared , for the criterion for the comparison ( e.g . proportion who survived after 5 years from treatment ) , and for each of the results . currently , it is a labour-intensive activity to read each published paper and extract the information for each field in an evidence table . there have been some nlp studies investigating how some of the features from papers can be extracted , or at least the relevant sentences identified . however , there is a lack of an nlp system for the systematic extraction of each item of information required for an evidence table . we address this need by a combination of a maximum entropy classifier , and integer linear programming . we use the later to handle constraints on what is an acceptable classification of the features to be extracted . with experimental results , we demonstrate substantial advantages in using global constraints ( such as the features describing the patient group , and the interventions , must occur before the features describing the results of the comparison ) .

measuring interesting rules in characteristic rule
finding interesting rule in the sixth strategy step about threshold control on generalized relations in attribute oriented induction , there is possibility to select candidate attribute for further generalization and merging of identical tuples until the number of tuples is no greater than the threshold value , as implemented in basic attribute oriented induction algorithm . at this strategy step there is possibility the number of tuples in final generalization result still greater than threshold value . in order to get the final generalization result which only small number of tuples and can be easy to transfer into simple logical formula , the seventh strategy step about rule transformation is evolved where there will be simplification by unioning or grouping the identical attribute . our approach to measure interesting rule is opposite with heuristic measurement approach by fudger and hamilton where the more complex concept hierarchies , more interesting results are likely to be found , but our approach the simpler concept hierarchies , more interesting results are likely to be found and the more complex concept hierarchies , more complex process generalization in concept tree . the decision to find interesting rule is influenced with wide or length and depth or level of concept tree .

learning with options that terminate off-policy
a temporally abstract action , or an option , is specified by a policy and a termination condition : the policy guides option behavior , and the termination condition roughly determines its length . generally , learning with longer options ( like learning with multi-step returns ) is known to be more efficient . however , if the option set for the task is not ideal , and can not express the primitive optimal policy exactly , shorter options offer more flexibility and can yield a better solution . thus , the termination condition puts learning efficiency at odds with solution quality . we propose to resolve this dilemma by decoupling the behavior and target terminations , just like it is done with policies in off-policy learning . to this end , we give a new algorithm , q ( \beta ) , that learns the solution with respect to any termination condition , regardless of how the options actually terminate . we derive q ( \beta ) by casting learning with options into a common framework with well-studied multi-step off-policy learning . we validate our algorithm empirically , and show that it holds up to its motivating claims .

indonesian earthquake decision support system
earthquake dss is an information technology environment which can be used by government to sharpen , make faster and better the earthquake mitigation decision . earthquake dss can be delivered as e-government which is not only for government itself but in order to guarantee each citizen 's rights for education , training and information about earthquake and how to overcome the earthquake . knowledge can be managed for future use and would become mining by saving and maintain all the data and information about earthquake and earthquake mitigation in indonesia . using web technology will enhance global access and easy to use . datawarehouse as unnormalized database for multidimensional analysis will speed the query process and increase reports variation . link with other disaster dss in one national disaster dss , link with other government information system and international will enhance the knowledge and sharpen the reports .

learning in the model space for fault diagnosis
the emergence of large scaled sensor networks facilitates the collection of large amounts of real-time data to monitor and control complex engineering systems . however , in many cases the collected data may be incomplete or inconsistent , while the underlying environment may be time-varying or un-formulated . in this paper , we have developed an innovative cognitive fault diagnosis framework that tackles the above challenges . this framework investigates fault diagnosis in the model space instead of in the signal space . learning in the model space is implemented by fitting a series of models using a series of signal segments selected with a rolling window . by investigating the learning techniques in the fitted model space , faulty models can be discriminated from healthy models using one-class learning algorithm . the framework enables us to construct fault library when unknown faults occur , which can be regarded as cognitive fault isolation . this paper also theoretically investigates how to measure the pairwise distance between two models in the model space and incorporates the model distance into the learning algorithm in the model space . the results on three benchmark applications and one simulated model for the barcelona water distribution network have confirmed the effectiveness of the proposed framework .

uavs using bayesian optimization to locate wifi devices
we address the problem of localizing non-collaborative wifi devices in a large region . our main motive is to localize humans by localizing their wifi devices , e.g . during search-and-rescue operations after a natural disaster . we use an active sensing approach that relies on unmanned aerial vehicles ( uavs ) to collect signal-strength measurements at informative locations . the problem is challenging since the measurement is received at arbitrary times and they are received only when the uav is in close proximity to the device . for these reasons , it is extremely important to make prudent decision with very few measurements . we use the bayesian optimization approach based on gaussian process ( gp ) regression . this approach works well for our application since gps give reliable predictions with very few measurements while bayesian optimization makes a judicious trade-off between exploration and exploitation . in field experiments conducted over a region of 1000 $ \times $ 1000 $ m^2 $ , we show that our approach reduces the search area to less than 100 meters around the wifi device within 5 minutes only . overall , our approach localizes the device in less than 15 minutes with an error of less than 20 meters .

automated classification of airborne laser scanning point clouds
making sense of the physical world has always been at the core of mapping . up until recently , this has always dependent on using the human eye . using airborne lasers , it has become possible to quickly `` see '' more of the world in many more dimensions . the resulting enormous point clouds serve as data sources for applications far beyond the original mapping purposes ranging from flooding protection and forestry to threat mitigation . in order to process these large quantities of data , novel methods are required . in this contribution , we develop models to automatically classify ground cover and soil types . using the logic of machine learning , we critically review the advantages of supervised and unsupervised methods . focusing on decision trees , we improve accuracy by including beam vector components and using a genetic algorithm . we find that our approach delivers consistently high quality classifications , surpassing classical methods .

towards a computational theory of human daydreaming
this paper examines the phenomenon of daydreaming : spontaneously recalling or imagining personal or vicarious experiences in the past or future . the following important roles of daydreaming in human cognition are postulated : plan preparation and rehearsal , learning from failures and successes , support for processes of creativity , emotion regulation , and motivation . a computational theory of daydreaming and its implementation as the program daydreamer are presented . daydreamer consists of 1 ) a scenario generator based on relaxed planning , 2 ) a dynamic episodic memory of experiences used by the scenario generator , 3 ) a collection of personal goals and control goals which guide the scenario generator , 4 ) an emotion component in which daydreams initiate , and are initiated by , emotional states arising from goal outcomes , and 5 ) domain knowledge of interpersonal relations and common everyday occurrences . the role of emotions and control goals in daydreaming is discussed . four control goals commonly used in guiding daydreaming are presented : rationalization , failure/success reversal , revenge , and preparation . the role of episodic memory in daydreaming is considered , including how daydreamed information is incorporated into memory and later used . an initial version of daydreamer which produces several daydreams ( in english ) is currently running .

generalized thompson sampling for sequential decision-making and causal inference
recently , it has been shown how sampling actions from the predictive distribution over the optimal action-sometimes called thompson sampling-can be applied to solve sequential adaptive control problems , when the optimal policy is known for each possible environment . the predictive distribution can then be constructed by a bayesian superposition of the optimal policies weighted by their posterior probability that is updated by bayesian inference and causal calculus . here we discuss three important features of this approach . first , we discuss in how far such thompson sampling can be regarded as a natural consequence of the bayesian modeling of policy uncertainty . second , we show how thompson sampling can be used to study interactions between multiple adaptive agents , thus , opening up an avenue of game-theoretic analysis . third , we show how thompson sampling can be applied to infer causal relationships when interacting with an environment in a sequential fashion . in summary , our results suggest that thompson sampling might not merely be a useful heuristic , but a principled method to address problems of adaptive sequential decision-making and causal inference .

language , logic and ontology : uncovering the structure of commonsense knowledge
the purpose of this paper is twofold : ( i ) we argue that the structure of commonsense knowledge must be discovered , rather than invented ; and ( ii ) we argue that natural language , which is the best known theory of our ( shared ) commonsense knowledge , should itself be used as a guide to discovering the structure of commonsense knowledge . in addition to suggesting a systematic method to the discovery of the structure of commonsense knowledge , the method we propose seems to also provide an explanation for a number of phenomena in natural language , such as metaphor , intensionality , and the semantics of nominal compounds . admittedly , our ultimate goal is quite ambitious , and it is no less than the systematic 'discovery ' of a well-typed ontology of commonsense knowledge , and the subsequent formulation of the long-awaited goal of a meaning algebra .

proteus : a hierarchical portfolio of solvers and transformations
in recent years , portfolio approaches to solving sat problems and csps have become increasingly common . there are also a number of different encodings for representing csps as sat instances . in this paper , we leverage advances in both sat and csp solving to present a novel hierarchical portfolio-based approach to csp solving , which we call proteus , that does not rely purely on csp solvers . instead , it may decide that it is best to encode a csp problem instance into sat , selecting an appropriate encoding and a corresponding sat solver . our experimental evaluation used an instance of proteus that involved four csp solvers , three sat encodings , and six sat solvers , evaluated on the most challenging problem instances from the csp solver competitions , involving global and intensional constraints . we show that significant performance improvements can be achieved by proteus obtained by exploiting alternative view-points and solvers for combinatorial problem-solving .

the utility of text : the case of amicus briefs and the supreme court
we explore the idea that authoring a piece of text is an act of maximizing one 's expected utility . to make this idea concrete , we consider the societally important decisions of the supreme court of the united states . extensive past work in quantitative political science provides a framework for empirically modeling the decisions of justices and how they relate to text . we incorporate into such a model texts authored by amici curiae ( `` friends of the court '' separate from the litigants ) who seek to weigh in on the decision , then explicitly model their goals in a random utility model . we demonstrate the benefits of this approach in improved vote prediction and the ability to perform counterfactual analysis .

the self-organization of interaction networks for nature-inspired optimization
over the last decade , significant progress has been made in understanding complex biological systems , however there have been few attempts at incorporating this knowledge into nature inspired optimization algorithms . in this paper , we present a first attempt at incorporating some of the basic structural properties of complex biological systems which are believed to be necessary preconditions for system qualities such as robustness . in particular , we focus on two important conditions missing in evolutionary algorithm populations ; a self-organized definition of locality and interaction epistasis . we demonstrate that these two features , when combined , provide algorithm behaviors not observed in the canonical evolutionary algorithm or in evolutionary algorithms with structured populations such as the cellular genetic algorithm . the most noticeable change in algorithm behavior is an unprecedented capacity for sustainable coexistence of genetically distinct individuals within a single population . this capacity for sustained genetic diversity is not imposed on the population but instead emerges as a natural consequence of the dynamics of the system .

direct optimization of ranking measures
web page ranking and collaborative filtering require the optimization of sophisticated performance measures . current support vector approaches are unable to optimize them directly and focus on pairwise comparisons instead . we present a new approach which allows direct optimization of the relevant loss functions . this is achieved via structured estimation in hilbert spaces . it is most related to max-margin-markov networks optimization of multivariate performance measures . key to our approach is that during training the ranking problem can be viewed as a linear assignment problem , which can be solved by the hungarian marriage algorithm . at test time , a sort operation is sufficient , as our algorithm assigns a relevance score to every ( document , query ) pair . experiments show that the our algorithm is fast and that it works very well .

harmonization of conflicting medical opinions using argumentation protocols and textual entailment - a case study on parkinson disease
parkinson 's disease is the second most common neurodegenerative disease , affecting more than 1.2 million people in europe . medications are available for the management of its symptoms , but the exact cause of the disease is unknown and there is currently no cure on the market . to better understand the relations between new findings and current medical knowledge , we need tools able to analyse published medical papers based on natural language processing and tools capable to identify various relationships of new findings with the current medical knowledge . our work aims to fill the above technological gap . to identify conflicting information in medical documents , we enact textual entailment technology . to encapsulate existing medical knowledge , we rely on ontologies . to connect the formal axioms in ontologies with natural text in medical articles , we exploit ontology verbalisation techniques . to assess the level of disagreement between human agents with respect to a medical issue , we rely on fuzzy aggregation . to harmonize this disagreement , we design mediation protocols within a multi-agent framework .

solving limited memory influence diagrams
we present a new algorithm for exactly solving decision making problems represented as influence diagrams . we do not require the usual assumptions of no forgetting and regularity ; this allows us to solve problems with simultaneous decisions and limited information . the algorithm is empirically shown to outperform a state-of-the-art algorithm on randomly generated problems of up to 150 variables and $ 10^ { 64 } $ solutions . we show that the problem is np-hard even if the underlying graph structure of the problem has small treewidth and the variables take on a bounded number of states , but that a fully polynomial time approximation scheme exists for these cases . moreover , we show that the bound on the number of states is a necessary condition for any efficient approximation scheme .

learning determinantal point processes
determinantal point processes ( dpps ) , which arise in random matrix theory and quantum physics , are natural models for subset selection problems where diversity is preferred . among many remarkable properties , dpps offer tractable algorithms for exact inference , including computing marginal probabilities and sampling ; however , an important open question has been how to learn a dpp from labeled training data . in this paper we propose a natural feature-based parameterization of conditional dpps , and show how it leads to a convex and efficient learning formulation . we analyze the relationship between our model and binary markov random fields with repulsive potentials , which are qualitatively similar but computationally intractable . finally , we apply our approach to the task of extractive summarization , where the goal is to choose a small subset of sentences conveying the most important information from a set of documents . in this task there is a fundamental tradeoff between sentences that are highly relevant to the collection as a whole , and sentences that are diverse and not repetitive . our parameterization allows us to naturally balance these two characteristics . we evaluate our system on data from the duc 2003/04 multi-document summarization task , achieving state-of-the-art results .

disan : directional self-attention network for rnn/cnn-free language understanding
recurrent neural nets ( rnn ) and convolutional neural nets ( cnn ) are widely used on nlp tasks to capture the long-term and local dependencies , respectively . attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation , significantly less training time , and flexibility in modeling dependencies . we propose a novel attention mechanism in which the attention between elements from input sequence ( s ) is directional and multi-dimensional ( i.e. , feature-wise ) . a light-weight neural net , `` directional self-attention network ( disan ) '' , is then proposed to learn sentence embedding , based solely on the proposed attention without any rnn/cnn structure . disan is only composed of a directional self-attention with temporal order encoded , followed by a multi-dimensional attention that compresses the sequence into a vector representation . despite its simple form , disan outperforms complicated rnn models on both prediction quality and time efficiency . it achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02 % on the stanford natural language inference ( snli ) dataset , and shows state-of-the-art test accuracy on the stanford sentiment treebank ( sst ) , multi-genre natural language inference ( multinli ) , sentences involving compositional knowledge ( sick ) , customer review , mpqa , trec question-type classification and subjectivity ( subj ) datasets .

converting cascade-correlation neural nets into probabilistic generative models
humans are not only adept in recognizing what class an input instance belongs to ( i.e. , classification task ) , but perhaps more remarkably , they can imagine ( i.e. , generate ) plausible instances of a desired class with ease , when prompted . inspired by this , we propose a framework which allows transforming cascade-correlation neural networks ( ccnns ) into probabilistic generative models , thereby enabling ccnns to generate samples from a category of interest . ccnns are a well-known class of deterministic , discriminative nns , which autonomously construct their topology , and have been successful in giving accounts for a variety of psychological phenomena . our proposed framework is based on a markov chain monte carlo ( mcmc ) method , called the metropolis-adjusted langevin algorithm , which capitalizes on the gradient information of the target distribution to direct its explorations towards regions of high probability , thereby achieving good mixing properties . through extensive simulations , we demonstrate the efficacy of our proposed framework .

hierarchical affinity propagation
affinity propagation is an exemplar-based clustering algorithm that finds a set of data-points that best exemplify the data , and associates each datapoint with one exemplar . we extend affinity propagation in a principled way to solve the hierarchical clustering problem , which arises in a variety of domains including biology , sensor networks and decision making in operational research . we derive an inference algorithm that operates by propagating information up and down the hierarchy , and is efficient despite the high-order potentials required for the graphical model formulation . we demonstrate that our method outperforms greedy techniques that cluster one layer at a time . we show that on an artificial dataset designed to mimic the hiv-strain mutation dynamics , our method outperforms related methods . for real hiv sequences , where the ground truth is not available , we show our method achieves better results , in terms of the underlying objective function , and show the results correspond meaningfully to geographical location and strain subtypes . finally we report results on using the method for the analysis of mass spectra , showing it performs favorably compared to state-of-the-art methods .

the effect of discrete vs. continuous-valued ratings on reputation and ranking systems
when users rate objects , a sophisticated algorithm that takes into account ability or reputation may produce a fairer or more accurate aggregation of ratings than the straightforward arithmetic average . recently a number of authors have proposed different co-determination algorithms where estimates of user and object reputation are refined iteratively together , permitting accurate measures of both to be derived directly from the rating data . however , simulations demonstrating these methods ' efficacy assumed a continuum of rating values , consistent with typical physical modelling practice , whereas in most actual rating systems only a limited range of discrete values ( such as a 5-star system ) is employed . we perform a comparative test of several co-determination algorithms with different scales of discrete ratings and show that this seemingly minor modification in fact has a significant impact on algorithms ' performance . paradoxically , where rating resolution is low , increased noise in users ' ratings may even improve the overall performance of the system .

efficient dodgson-score calculation using heuristics and parallel computing
conflict of interest is the permanent companion of any population of agents ( computational or biological ) . for that reason , the ability to compromise is of paramount importance , making voting a key element of societal mechanisms . one of the voting procedures most often discussed in the literature and , due to its intuitiveness , also conceptually quite appealing is charles dodgson 's scoring rule , basically using the respective closeness to being a condorcet winner for evaluating competing alternatives . in this paper , we offer insights on the practical limits of algorithms computing the exact dodgson scores from a number of votes . while the problem itself is theoretically intractable , this work proposes and analyses five different solutions which try distinct approaches to practically solve the issue in an effective manner . additionally , three of the discussed procedures can be run in parallel which has the potential of drastically reducing the problem size .

geometric lattice structure of covering-based rough sets through matroids
covering-based rough set theory is a useful tool to deal with inexact , uncertain or vague knowledge in information systems . geometric lattice has widely used in diverse fields , especially search algorithm design which plays important role in covering reductions . in this paper , we construct four geometric lattice structures of covering-based rough sets through matroids , and compare their relationships . first , a geometric lattice structure of covering-based rough sets is established through the transversal matroid induced by the covering , and its characteristics including atoms , modular elements and modular pairs are studied . we also construct a one-to-one correspondence between this type of geometric lattices and transversal matroids in the context of covering-based rough sets . second , sufficient and necessary conditions for three types of covering upper approximation operators to be closure operators of matroids are presented . we exhibit three types of matroids through closure axioms , and then obtain three geometric lattice structures of covering-based rough sets . third , these four geometric lattice structures are compared . some core concepts such as reducible elements in covering-based rough sets are investigated with geometric lattices . in a word , this work points out an interesting view , namely geometric lattice , to study covering-based rough sets .

tracking object 's type changes with fuzzy based fusion rule
in this paper the behavior of three combinational rules for temporal/sequential attribute data fusion for target type estimation are analyzed . the comparative analysis is based on : dempster 's fusion rule proposed in dempster-shafer theory ; proportional conflict redistribution rule no . 5 ( pcr5 ) , proposed in dezert-smarandache theory and one alternative class fusion rule , connecting the combination rules for information fusion with particular fuzzy operators , focusing on the t-norm based conjunctive rule as an analog of the ordinary conjunctive rule and t-conorm based disjunctive rule as an analog of the ordinary disjunctive rule . the way how different t-conorms and t-norms functions within tcn fusion rule influence over target type estimation performance is studied and estimated .

semantic specialisation of distributional word vector spaces using monolingual and cross-lingual constraints
we present attract-repel , an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources . attract-repel facilitates the use of constraints from mono- and cross-lingual resources , yielding semantically specialised cross-lingual vector spaces . our evaluation shows that the method can make use of existing cross-lingual lexicons to construct high-quality vector spaces for a plethora of different languages , facilitating semantic transfer from high- to lower-resource ones . the effectiveness of our approach is demonstrated with state-of-the-art results on semantic similarity datasets in six languages . we next show that attract-repel-specialised vectors boost performance in the downstream task of dialogue state tracking ( dst ) across multiple languages . finally , we show that cross-lingual vector spaces produced by our algorithm facilitate the training of multilingual dst models , which brings further performance improvements .

learning to rank query recommendations by semantic similarities
logs of the interactions with a search engine show that users often reformulate their queries . examining these reformulations shows that recommendations that precise the focus of a query are helpful , like those based on expansions of the original queries . but it also shows that queries that express some topical shift with respect to the original query can help user access more rapidly the information they need . we propose a method to identify from the query logs of past users queries that either focus or shift the initial query topic . this method combines various click-based , topic-based and session based ranking strategies and uses supervised learning in order to maximize the semantic similarities between the query and the recommendations , while at the same diversifying them . we evaluate our method using the query/click logs of a japanese web search engine and we show that the combination of the three methods proposed is significantly better than any of them taken individually .

the challenge of non-technical loss detection using artificial intelligence : a survey
detection of non-technical losses ( ntl ) which include electricity theft , faulty meters or billing errors has attracted increasing attention from researchers in electrical engineering and computer science . ntls cause significant harm to the economy , as in some countries they may range up to 40 % of the total electricity distributed . the predominant research direction is employing artificial intelligence to predict whether a customer causes ntl . this paper first provides an overview of how ntls are defined and their impact on economies , which include loss of revenue and profit of electricity providers and decrease of the stability and reliability of electrical power grids . it then surveys the state-of-the-art research efforts in a up-to-date and comprehensive review of algorithms , features and data sets used . it finally identifies the key scientific and engineering challenges in ntl detection and suggests how they could be addressed in the future .

marginality : a numerical mapping for enhanced treatment of nominal and hierarchical attributes
the purpose of statistical disclosure control ( sdc ) of microdata , a.k.a . data anonymization or privacy-preserving data mining , is to publish data sets containing the answers of individual respondents in such a way that the respondents corresponding to the released records can not be re-identified and the released data are analytically useful . sdc methods are either based on masking the original data , generating synthetic versions of them or creating hybrid versions by combining original and synthetic data . the choice of sdc methods for categorical data , especially nominal data , is much smaller than the choice of methods for numerical data . we mitigate this problem by introducing a numerical mapping for hierarchical nominal data which allows computing means , variances and covariances on them .

a sat model to mine flexible sequences in transactional datasets
traditional pattern mining algorithms generally suffer from a lack of flexibility . in this paper , we propose a sat formulation of the problem to successfully mine frequent flexible sequences occurring in transactional datasets . our sat-based approach can easily be extended with extra constraints to address a broad range of pattern mining applications . to demonstrate this claim , we formulate and add several constraints , such as gap and span constraints , to our model in order to extract more specific patterns . we also use interactive solving to perform important derived tasks , such as closed pattern mining or maximal pattern mining . finally , we prove the practical feasibility of our sat model by running experiments on two real datasets .

a formal solution to the grain of truth problem
a bayesian agent acting in a multi-agent environment learns to predict the other agents ' policies if its prior assigns positive probability to them ( in other words , its prior contains a \emph { grain of truth } ) . finding a reasonably large class of policies that contains the bayes-optimal policies with respect to this class is known as the \emph { grain of truth problem } . only small classes are known to have a grain of truth and the literature contains several related impossibility results . in this paper we present a formal and general solution to the full grain of truth problem : we construct a class of policies that contains all computable policies as well as bayes-optimal policies for every lower semicomputable prior over the class . when the environment is unknown , bayes-optimal agents may fail to act optimally even asymptotically . however , agents based on thompson sampling converge to play { \epsilon } -nash equilibria in arbitrary unknown computable multi-agent environments . while these results are purely theoretical , we show that they can be computationally approximated arbitrarily closely .

beyond parity : fairness objectives for collaborative filtering
we study fairness in collaborative-filtering recommender systems , which are sensitive to discrimination that exists in historical data . biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups . we identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness . these fairness metrics can be optimized by adding fairness terms to the learning objective . experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline , and that the fairness objectives effectively help reduce unfairness .

a selective macro-learning algorithm and its application to the nxn sliding-tile puzzle
one of the most common mechanisms used for speeding up problem solvers is macro-learning . macros are sequences of basic operators acquired during problem solving . macros are used by the problem solver as if they were basic operators . the major problem that macro-learning presents is the vast number of macros that are available for acquisition . macros increase the branching factor of the search space and can severely degrade problem-solving efficiency . to make macro learning useful , a program must be selective in acquiring and utilizing macros . this paper describes a general method for selective acquisition of macros . solvable training problems are generated in increasing order of difficulty . the only macros acquired are those that take the problem solver out of a local minimum to a better state . the utility of the method is demonstrated in several domains , including the domain of nxn sliding-tile puzzles . after learning on small puzzles , the system is able to efficiently solve puzzles of any size .

closed-form learning of markov networks from dependency networks
markov networks ( mns ) are a powerful way to compactly represent a joint probability distribution , but most mn structure learning methods are very slow , due to the high cost of evaluating candidates structures . dependency networks ( dns ) represent a probability distribution as a set of conditional probability distributions . dns are very fast to learn , but the conditional distributions may be inconsistent with each other and few inference algorithms support dns . in this paper , we present a closed-form method for converting a dn into an mn , allowing us to enjoy both the efficiency of dn learning and the convenience of the mn representation . when the dn is consistent , this conversion is exact . for inconsistent dns , we present averaging methods that significantly improve the approximation . in experiments on 12 standard datasets , our methods are orders of magnitude faster than and often more accurate than combining conditional distributions using weight learning .

proceedings first workshop on causal reasoning for embedded and safety-critical systems technologies
formal approaches for automated causality analysis , fault localization , explanation of events , accountability and blaming have been proposed independently by several communities -- - in particular , ai , concurrency , model-based diagnosis , formal methods . work on these topics has significantly gained speed during the last years . the goals of crest are to bring together and foster exchange between researchers from the different communities , and to present and discuss recent advances and new ideas in the field . the workshop program consisted of a set of invited and contributed presentations that illustrate different techniques for , and applications of , causality analysis and fault localization . the program was anchored by two keynote talks . the keynote by hana chockler ( king 's college ) provided a broad perspective on the application of causal reasoning based on halpern and pearl 's definitions of actual causality to a variety of application domains ranging from formal verification to legal reasoning . the keynote by chao wang ( virginia tech ) concentrated on constraint-based analysis techniques for debugging and verifying concurrent programs . workshop papers deal with compositional causality analysis and a wide spectrum of application for causal reasoning , such as debugging of probabilistic models , accountability and responsibility , hazard analysis in practice based on lewis ' counterfactuals , and fault localization and repair .

nonmonotonic reasoning as a temporal activity
a { \it dynamic reasoning system } ( drs ) is an adaptation of a conventional formal logical system that explicitly portrays reasoning as a temporal activity , with each extralogical input to the system and each inference rule application being viewed as occurring at a distinct time step . every drs incorporates some well-defined logic together with a controller that serves to guide the reasoning process in response to user inputs . logics are generic , whereas controllers are application-specific . every controller does , nonetheless , provide an algorithm for nonmonotonic belief revision . the general notion of a drs comprises a framework within which one can formulate the logic and algorithms for a given application and prove that the algorithms are correct , i.e. , that they serve to ( i ) derive all salient information and ( ii ) preserve the consistency of the belief set . this paper illustrates the idea with ordinary first-order predicate calculus , suitably modified for the present purpose , and an example . the example revisits some classic nonmonotonic reasoning puzzles ( opus the penguin , nixon diamond ) and shows how these can be resolved in the context of a drs , using an expanded version of first-order logic that incorporates typed predicate symbols . all concepts are rigorously defined and effectively computable , thereby providing the foundation for a future software implementation .

symphony from synapses : neocortex as a universal dynamical systems modeller using hierarchical temporal memory
reverse engineering the brain is proving difficult , perhaps impossible . while many believe that this is just a matter of time and effort , a different approach might help . here , we describe a very simple idea which explains the power of the brain as well as its structure , exploiting complex dynamics rather than abstracting it away . just as a turing machine is a universal digital computer operating in a world of symbols , we propose that the brain is a universal dynamical systems modeller , evolved bottom-up ( itself using nested networks of interconnected , self-organised dynamical systems ) to prosper in a world of dynamical systems . recent progress in applied mathematics has produced startling evidence of what happens when abstract dynamical systems interact . key latent information describing system a can be extracted by system b from very simple signals , and signals can be used by one system to control and manipulate others . using these facts , we show how a region of the neocortex uses its dynamics to intrinsically `` compute '' about the external and internal world . building on an existing `` static '' model of cortical computation ( hawkins ' hierarchical temporal memory - htm ) , we describe how a region of neocortex can be viewed as a network of components which together form a dynamical systems modelling module , connected via sensory and motor pathways to the external world , and forming part of a larger dynamical network in the brain . empirical modelling and simulations of dynamical htm are possible with simple extensions and combinations of currently existing open source software . we list a number of relevant projects .

applying interval type-2 fuzzy rule based classifiers through a cluster-based class representation
fuzzy rule-based classification systems ( frbcss ) have the potential to provide so-called interpretable classifiers , i.e . classifiers which can be introspective , understood , validated and augmented by human experts by relying on fuzzy-set based rules . this paper builds on prior work for interval type-2 fuzzy set based frbcs where the fuzzy sets and rules of the classifier are generated using an initial clustering stage . by introducing subtractive clustering in order to identify multiple cluster prototypes , the proposed approach has the potential to deliver improved classification performance while maintaining good interpretability , i.e . without resulting in an excessive number of rules . the paper provides a detailed overview of the proposed frbc framework , followed by a series of exploratory experiments on both linearly and non-linearly separable datasets , comparing results to existing rule-based and svm approaches . overall , initial results indicate that the approach enables comparable classification performance to non rule-based classifiers such as svm , while often achieving this with a very small number of rules .

efficient simulation of financial stress testing scenarios with suppes-bayes causal networks
the most recent financial upheavals have cast doubt on the adequacy of some of the conventional quantitative risk management strategies , such as var ( value at risk ) , in many common situations . consequently , there has been an increasing need for verisimilar financial stress testings , namely simulating and analyzing financial portfolios in extreme , albeit rare scenarios . unlike conventional risk management which exploits statistical correlations among financial instruments , here we focus our analysis on the notion of probabilistic causation , which is embodied by suppes-bayes causal networks ( sbcns ) , sbcns are probabilistic graphical models that have many attractive features in terms of more accurate causal analysis for generating financial stress scenarios . in this paper , we present a novel approach for conducting stress testing of financial portfolios based on sbcns in combination with classical machine learning classification tools . the resulting method is shown to be capable of correctly discovering the causal relationships among financial factors that affect the portfolios and thus , simulating stress testing scenarios with a higher accuracy and lower computational complexity than conventional monte carlo simulations .

a markovian-based approach for daily living activities recognition
recognizing the activities of daily living plays an important role in healthcare . it is necessary to use an adapted model to simulate the human behavior in a domestic space to monitor the patient harmonically and to intervene in the necessary time . in this paper , we tackle this problem using the hierarchical hidden markov model for representing and recognizing complex indoor activities . we propose a new grammar , called `` home by room activities language '' , to facilitate the complexity of human scenarios and consider the abnormal activities .

diagnosing editorial strategies of chilean media on twitter using an automatic news classifier
in chile , does not exist an independent entity that publishes quantitative or qualitative surveys to understand the traditional media environment and its adaptation on the social web . nowadays , chilean newsreaders are increasingly using social web platforms as their primary source of information , among which twitter plays a central role . historical media and pure players are developing different strategies to increase their audience and influence on this platform . in this article , we propose a methodology based on data mining techniques to provide a first level of analysis of the new chilean media environment . we use a crawling technique to mine news streams of 37 different chilean media actively presents on twitter and propose several indicators to compare them . we analyze their volumes of production , their potential audience , and using nlp techniques , we explore the content of their production : their editorial line and their geographic coverage .

approximate policy iteration with a policy language bias : solving relational markov decision processes
we study an approach to policy selection for large relational markov decision processes ( mdps ) . we consider a variant of approximate policy iteration ( api ) that replaces the usual value-function learning step with a learning step in policy space . this is advantageous in domains where good policies are easier to represent and learn than the corresponding value functions , which is often the case for the relational mdps we are interested in . in order to apply api to such problems , we introduce a relational policy language and corresponding learner . in addition , we introduce a new bootstrapping routine for goal-based planning domains , based on random walks . such bootstrapping is necessary for many large relational mdps , where reward is extremely sparse , as api is ineffective in such domains when initialized with an uninformed policy . our experiments show that the resulting system is able to find good policies for a number of classical planning domains and their stochastic variants by solving them as extremely large relational mdps . the experiments also point to some limitations of our approach , suggesting future work .

data science as a new frontier for design
the purpose of this paper is to contribute to the challenge of transferring know-how , theories and methods from design research to the design processes in information science and technologies . more specifically , we shall consider a domain , namely data-science , that is becoming rapidly a globally invested research and development axis with strong imperatives for innovation given the data deluge we are currently facing . we argue that , in order to rise to the data-related challenges that the society is facing , data-science initiatives should ensure a renewal of traditional research methodologies that are still largely based on trial-error processes depending on the talent and insights of a single ( or a restricted group of ) researchers . it is our claim that design theories and methods can provide , at least to some extent , the much-needed framework . we will use a worldwide data-science challenge organized to study a technical problem in physics , namely the detection of higgs boson , as a use case to demonstrate some of the ways in which design theory and methods can help in analyzing and shaping the innovation dynamics in such projects .

agenda separability in judgment aggregation
one of the better studied properties for operators in judgment aggregation is independence , which essentially dictates that the collective judgment on one issue should not depend on the individual judgments given on some other issue ( s ) in the same agenda . independence , although considered a desirable property , is too strong , because together with mild additional conditions it implies dictatorship . we propose here a weakening of independence , named agenda separability : a judgment aggregation rule satisfies it if , whenever the agenda is composed of several independent sub-agendas , the resulting collective judgment sets can be computed separately for each sub-agenda and then put together . we show that this property is discriminant , in the sense that among judgment aggregation rules so far studied in the literature , some satisfy it and some do not . we briefly discuss the implications of agenda separability on the computation of judgment aggregation rules .

storm - a novel information fusion and cluster interpretation technique
analysis of data without labels is commonly subject to scrutiny by unsupervised machine learning techniques . such techniques provide more meaningful representations , useful for better understanding of a problem at hand , than by looking only at the data itself . although abundant expert knowledge exists in many areas where unlabelled data is examined , such knowledge is rarely incorporated into automatic analysis . incorporation of expert knowledge is frequently a matter of combining multiple data sources from disparate hypothetical spaces . in cases where such spaces belong to different data types , this task becomes even more challenging . in this paper we present a novel immune-inspired method that enables the fusion of such disparate types of data for a specific set of problems . we show that our method provides a better visual understanding of one hypothetical space with the help of data from another hypothetical space . we believe that our model has implications for the field of exploratory data analysis and knowledge discovery .

scaling up decentralized mdps through heuristic search
decentralized partially observable markov decision processes ( dec-pomdps ) are rich models for cooperative decision-making under uncertainty , but are often intractable to solve optimally ( nexp-complete ) . the transition and observation independent dec-mdp is a general subclass that has been shown to have complexity in np , but optimal algorithms for this subclass are still inefficient in practice . in this paper , we first provide an updated proof that an optimal policy does not depend on the histories of the agents , but only the local observations . we then present a new algorithm based on heuristic search that is able to expand search nodes by using constraint optimization . we show experimental results comparing our approach with the state-of-the-art decmdp and dec-pomdp solvers . these results show a reduction in computation time and an increase in scalability by multiple orders of magnitude in a number of benchmarks .

symbol emergence in cognitive developmental systems : a survey
symbol emergence through a robot 's own interactive exploration of the world without human intervention has been investigated now for several decades . however , methods that enable a machine to form symbol systems in a robust bottom-up manner are still missing . clearly , this shows that we still do not have an appropriate computational understanding that explains symbol emergence in biological and artificial systems . over the years it became more and more clear that symbol emergence has to be posed as a multi-faceted problem . therefore , we will first review the history of the symbol emergence problem in different fields showing their mutual relations . then we will describe recent work and approaches to solve this problem with the aim of providing an integrative and comprehensive overview of symbol emergence for future research .

a neutrosophic recommender system for medical diagnosis based on algebraic neutrosophic measures
neutrosophic set has the ability to handle uncertain , incomplete , inconsistent , indeterminate information in a more accurate way . in this paper , we proposed a neutrosophic recommender system to predict the diseases based on neutrosophic set which includes single-criterion neutrosophic recommender system ( sc-nrs ) and multi-criterion neutrosophic recommender system ( mc-nrs ) . further , we investigated some algebraic operations of neutrosophic recommender system such as union , complement , intersection , probabilistic sum , bold sum , bold intersection , bounded difference , symmetric difference , convex linear sum of min and max operators , cartesian product , associativity , commutativity and distributive . based on these operations , we studied the algebraic structures such as lattices , kleen algebra , de morgan algebra , brouwerian algebra , bck algebra , stone algebra and mv algebra . in addition , we introduced several types of similarity measures based on these algebraic operations and studied some of their theoretic properties . moreover , we accomplished a prediction formula using the proposed algebraic similarity measure . we also proposed a new algorithm for medical diagnosis based on neutrosophic recommender system . finally to check the validity of the proposed methodology , we made experiments on the datasets heart , rhc , breast cancer , diabetes and dmd . at the end , we presented the mse and computational time by comparing the proposed algorithm with the relevant ones such as icsm , dsm , care , cfmd , as well as other variants namely variant 67 , variant 69 , and varian 71 both in tabular and graphical form to analyze the efficiency and accuracy . finally we analyzed the strength of all 8 algorithms by anova statistical tool .

time-critical dynamic decision making
recent interests in dynamic decision modeling have led to the development of several representation and inference methods . these methods however , have limited application under time critical conditions where a trade-off between model quality and computational tractability is essential . this paper presents an approach to time-critical dynamic decision modeling . a knowledge representation and modeling method called the time-critical dynamic influence diagram is proposed . the formalism has two forms . the condensed form is used for modeling and model abstraction , while the deployed form which can be converted from the condensed form is used for inference purposes . the proposed approach has the ability to represent space-temporal abstraction within the model . a knowledge-based meta-reasoning approach is proposed for the purpose of selecting the best abstracted model that provide the optimal trade-off between model quality and model tractability . an outline of the knowledge-based model construction algorithm is also provided .

map learning with indistinguishable locations
nearly all spatial reasoning problems involve uncertainty of one sort or another . uncertainty arises due to the inaccuracies of sensors used in measuring distances and angles . we refer to this as directional uncertainty . uncertainty also arises in combining spatial information when one location is mistakenly identified with another . we refer to this as recognition uncertainty . most problems in constructing spatial representations ( maps ) for the purpose of navigation involve both directional and recognition uncertainty . in this paper , we show that a particular class of spatial reasoning problems involving the construction of representations of large-scale space can be solved efficiently even in the presence of directional and recognition uncertainty . we pay particular attention to the problems that arise due to recognition uncertainty .

goedel machines : self-referential universal problem solvers making provably optimal self-improvements
we present the first class of mathematically rigorous , general , fully self-referential , self-improving , optimally efficient problem solvers . inspired by kurt goedel 's celebrated self-referential formulas ( 1931 ) , such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful , where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code . the searcher systematically and efficiently tests computable proof techniques ( programs whose outputs are proofs ) until it finds a provably useful , computable self-rewrite . we show that such a self-rewrite is globally optimal - no local maxima ! - since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites . unlike previous non-self-referential methods based on hardwired proof searchers , ours not only boasts an optimal order of complexity but can optimally reduce any slowdowns hidden by the o ( ) -notation , provided the utility of such speed-ups is provable at all .

stable models for infinitary formulas with extensional atoms
the definition of stable models for propositional formulas with infinite conjunctions and disjunctions can be used to describe the semantics of answer set programming languages . in this note , we enhance that definition by introducing a distinction between intensional and extensional atoms . the symmetric splitting theorem for first-order formulas is then extended to infinitary formulas and used to reason about infinitary definitions . this note is under consideration for publication in theory and practice of logic programming .

near-optimal brl using optimistic local transitions
model-based bayesian reinforcement learning ( brl ) allows a found formalization of the problem of acting optimally while facing an unknown environment , i.e. , avoiding the exploration-exploitation dilemma . however , algorithms explicitly addressing brl suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms . this paper introduces bolt , a simple and ( almost ) deterministic heuristic algorithm for brl which is optimistic about the transition function . we analyze bolt 's sample complexity , and show that under certain parameters , the algorithm is near-optimal in the bayesian sense with high probability . then , experimental results highlight the key differences of this method compared to previous work .

markov chains on orbits of permutation groups
we present a novel approach to detecting and utilizing symmetries in probabilistic graphical models with two main contributions . first , we present a scalable approach to computing generating sets of permutation groups representing the symmetries of graphical models . second , we introduce orbital markov chains , a novel family of markov chains leveraging model symmetries to reduce mixing times . we establish an insightful connection between model symmetries and rapid mixing of orbital markov chains . thus , we present the first lifted mcmc algorithm for probabilistic graphical models . both analytical and empirical results demonstrate the effectiveness and efficiency of the approach .

compressing binary decision diagrams
the paper introduces a new technique for compressing binary decision diagrams in those cases where random access is not required . using this technique , compression and decompression can be done in linear time in the size of the bdd and compression will in many cases reduce the size of the bdd to 1-2 bits per node . empirical results for our compression technique are presented , including comparisons with previously introduced techniques , showing that the new technique dominate on all tested instances .

online least squares estimation with self-normalized processes : an application to bandit problems
the analysis of online least squares estimation is at the heart of many stochastic sequential decision making problems . we employ tools from the self-normalized processes to provide a simple and self-contained proof of a tail bound of a vector-valued martingale . we use the bound to construct a new tighter confidence sets for the least squares estimate . we apply the confidence sets to several online decision problems , such as the multi-armed and the linearly parametrized bandit problems . the confidence sets are potentially applicable to other problems such as sleeping bandits , generalized linear bandits , and other linear control problems . we improve the regret bound of the upper confidence bound ( ucb ) algorithm of auer et al . ( 2002 ) and show that its regret is with high-probability a problem dependent constant . in the case of linear bandits ( dani et al. , 2008 ) , we improve the problem dependent bound in the dimension and number of time steps . furthermore , as opposed to the previous result , we prove that our bound holds for small sample sizes , and at the same time the worst case bound is improved by a logarithmic factor and the constant is improved .

deep voice 3 : scaling text-to-speech with convolutional sequence learning
we present deep voice 3 , a fully-convolutional attention-based neural text-to-speech ( tts ) system . deep voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster . we scale deep voice 3 to data set sizes unprecedented for tts , training on more than eight hundred hours of audio from over two thousand speakers . in addition , we identify common error modes of attention-based speech synthesis networks , demonstrate how to mitigate them , and compare several different waveform synthesis methods . we also describe how to scale inference to ten million queries per day on one single-gpu server .

memristor crossbar-based hardware implementation of fuzzy membership functions
in may 1 , 2008 , researchers at hewlett packard ( hp ) announced the first physical realization of a fundamental circuit element called memristor that attracted so much interest worldwide . this newly found element can easily be combined with crossbar interconnect technology which this new structure has opened a new field in designing configurable or programmable electronic systems . these systems in return can have applications in signal processing and artificial intelligence . in this paper , based on the simple memristor crossbar structure , we propose new and simple circuits for hardware implementation of fuzzy membership functions . in our proposed circuits , these fuzzy membership functions can have any shapes and resolutions . in addition , these circuits can be used as a basis in the construction of evolutionary systems .

bayesian networks for dependability analysis : an application to digital control reliability
bayesian networks ( bn ) provide robust probabilistic methods of reasoning under uncertainty , but despite their formal grounds are strictly based on the notion of conditional dependence , not much attention has been paid so far to their use in dependability analysis . the aim of this paper is to propose bn as a suitable tool for dependability analysis , by challenging the formalism with basic issues arising in dependability tasks . we will discuss how both modeling and analysis issues can be naturally dealt with by bn . moreover , we will show how some limitations intrinsic to combinatorial dependability methods such as fault trees can be overcome using bn . this will be pursued through the study of a real-world example concerning the reliability analysis of a redundant digital programmable logic controller ( plc ) with majority voting 2:3

parkinson 's disease motor symptoms in machine learning : a review
this paper reviews related work and state-of-the-art publications for recognizing motor symptoms of parkinson 's disease ( pd ) . it presents research efforts that were undertaken to inform on how well traditional machine learning algorithms can handle this task . in particular , four pd related motor symptoms are highlighted ( i.e . tremor , bradykinesia , freezing of gait and dyskinesia ) and their details summarized . thus the primary objective of this research is to provide a literary foundation for development and improvement of algorithms for detecting pd related motor symptoms .

stock trading using pe ratio : a dynamic bayesian network modeling on behavioral finance and fundamental investment
on a daily investment decision in a security market , the price earnings ( pe ) ratio is one of the most widely applied methods being used as a firm valuation tool by investment experts . unfortunately , recent academic developments in financial econometrics and machine learning rarely look at this tool . in practice , fundamental pe ratios are often estimated only by subjective expert opinions . the purpose of this research is to formalize a process of fundamental pe estimation by employing advanced dynamic bayesian network ( dbn ) methodology . the estimated pe ratio from our model can be used either as a information support for an expert to make investment decisions , or as an automatic trading system illustrated in experiments . forward-backward inference and em parameter estimation algorithms are derived with respect to the proposed dbn structure . unlike existing works in literatures , the economic interpretation of our dbn model is well-justified by behavioral finance evidences of volatility . a simple but practical trading strategy is invented based on the result of bayesian inference . extensive experiments show that our trading strategy equipped with the inferenced pe ratios consistently outperforms standard investment benchmarks .

towards an understanding of entity-oriented search intents
entity-oriented search deals with a wide variety of information needs , from displaying direct answers to interacting with services . in this work , we aim to understand what are prominent entity-oriented search intents and how they can be fulfilled . we develop a scheme of entity intent categories , and use them to annotate a sample of queries . specifically , we annotate unique query refiners on the level of entity types . we observe that , on average , over half of those refiners seek to interact with a service , while over a quarter of the refiners search for information that may be looked up in a knowledge base .

game information system
in this information system age many organizations consider information system as their weapon to compete or gain competitive advantage or give the best services for non profit organizations . game information system as combining information system and game is breakthrough to achieve organizations ' performance . the game information system will run the information system with game and how game can be implemented to run the information system . game is not only for fun and entertainment , but will be a challenge to combine fun and entertainment with information system . the challenge to run the information system with entertainment , deliver the entertainment with information system all at once . game information system can be implemented in many sectors as like the information system itself but in difference 's view . a view of game which people can joy and happy and do their transaction as a fun things .

proceedings of the twenty-seventh conference on uncertainty in artificial intelligence ( 2011 )
this is the proceedings of the twenty-seventh conference on uncertainty in artificial intelligence , which was held in barcelona , spain , july 14 - 17 2011 .

graph approximation and clustering on a budget
we consider the problem of learning from a similarity matrix ( such as spectral clustering and lowd imensional embedding ) , when computing pairwise similarities are costly , and only a limited number of entries can be observed . we provide a theoretical analysis using standard notions of graph approximation , significantly generalizing previous results ( which focused on spectral clustering with two clusters ) . we also propose a new algorithmic approach based on adaptive sampling , which experimentally matches or improves on previous methods , while being considerably more general and computationally cheaper .

is there a role for qualitative risk assessment ?
classically , risk is characterized by a point value probability indicating the likelihood of occurrence of an adverse effect . however , there are domains where the attainability of objective numerical risk characterizations is increasingly being questioned . this paper reviews the arguments in favour of extending classical techniques of risk assessment to incorporate meaningful qualitative and weak quantitative risk characterizations . a technique in which linguistic uncertainty terms are defined in terms of patterns of argument is then proposed . the technique is demonstrated using a prototype computer-based system for predicting the carcinogenic risk due to novel chemical compounds .

personalizing a dialogue system with transfer reinforcement learning
it is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient . personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs . one way to solve this problem is to consider a collection of multiple users ' data as a source domain and an individual user 's data as a target domain , and to perform a transfer learning from the source to the target domain . by following this idea , we propose `` petal '' ( personalized task-oriented dialogue ) , a transfer-learning framework based on pomdp to learn a personalized dialogue system . the system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user . this framework can avoid the negative transfer problem by considering differences between source and target users . the policy in the personalized pomdp can learn to choose different actions appropriately for different users . experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users , and thus effectively improve the dialogue quality under the personalized setting .

mining compressed repetitive gapped sequential patterns efficiently
mining frequent sequential patterns from sequence databases has been a central research topic in data mining and various efficient mining sequential patterns algorithms have been proposed and studied . recently , in many problem domains ( e.g , program execution traces ) , a novel sequential pattern mining research , called mining repetitive gapped sequential patterns , has attracted the attention of many researchers , considering not only the repetition of sequential pattern in different sequences but also the repetition within a sequence is more meaningful than the general sequential pattern mining which only captures occurrences in different sequences . however , the number of repetitive gapped sequential patterns generated by even these closed mining algorithms may be too large to understand for users , especially when support threshold is low . in this paper , we propose and study the problem of compressing repetitive gapped sequential patterns . inspired by the ideas of summarizing frequent itemsets , rpglobal , we develop an algorithm , crgsgrow ( compressing repetitive gapped sequential pattern grow ) , including an efficient pruning strategy , syncscan , and an efficient representative pattern checking scheme , -dominate sequential pattern checking . the crgsgrow is a two-step approach : in the first step , we obtain all closed repetitive sequential patterns as the candidate set of representative repetitive sequential patterns , and at the same time get the most of representative repetitive sequential patterns ; in the second step , we only spend a little time in finding the remaining the representative patterns from the candidate set . an empirical study with both real and synthetic data sets clearly shows that the crgsgrow has good performance .

an analysis of key factors for the success of the communal management of knowledge
this paper explores the links between knowledge management and new community-based models of the organization from both a theoretical and an empirical perspective . from a theoretical standpoint , we look at communities of practice ( cops ) and knowledge management ( km ) and explore the links between the two as they relate to the use of information systems to manage knowledge . we begin by reviewing technologically supported approaches to km and introduce the idea of `` systemes d'aide a la gestion des connaissances '' sagc ( systems to aid the management of knowledge ) . following this we examine the contribution that communal structures such as cops can make to intraorganizational km and highlight some of 'success factors ' for this approach to km that are found in the literature . from an empirical standpoint , we present the results of a survey involving the chief knowledge officers ( ckos ) of twelve large french businesses ; the objective of this study was to identify the factors that might influence the success of such approaches . the survey was analysed using thematic content analysis and the results are presented here with some short illustrative quotes from the ckos . finally , the paper concludes with some brief reflections on what can be learnt from looking at this problem from these two perspectives .

time matters : multi-scale temporalization of social media popularity
the evolution of social media popularity exhibits rich temporality , i.e. , popularities change over time at various levels of temporal granularity . this is influenced by temporal variations of public attentions or user activities . for example , popularity patterns of street snap on flickr are observed to depict distinctive fashion styles at specific time scales , such as season-based periodic fluctuations for trench coat or one-off peak in days for evening dress . however , this fact is often overlooked by existing research of popularity modeling . we present the first study to incorporate multiple time-scale dynamics into predicting online popularity . we propose a novel computational framework in the paper , named multi-scale temporalization , for estimating popularity based on multi-scale decomposition and structural reconstruction in a tensor space of user , post , and time by joint low-rank constraints . by considering the noise caused by context inconsistency , we design a data rearrangement step based on context aggregation as preprocessing to enhance contextual relevance of neighboring data in the tensor space . as a result , our approach can leverage multiple levels of temporal characteristics and reduce the noise of data decomposition to improve modeling effectiveness . we evaluate our approach on two large-scale flickr image datasets with over 1.8 million photos in total , for the task of popularity prediction . the results show that our approach significantly outperforms state-of-the-art popularity prediction techniques , with a relative improvement of 10.9 % -47.5 % in terms of prediction accuracy .

an algorithmic and a geometric characterization of coarsening at random
we show that the class of conditional distributions satisfying the coarsening at random ( car ) property for discrete data has a simple and robust algorithmic description based on randomized uniform multicovers : combinatorial objects generalizing the notion of partition of a set . however , the complexity of a given car mechanism can be large : the maximal `` height '' of the needed multicovers can be exponential in the number of points in the sample space . the results stem from a geometric interpretation of the set of car distributions as a convex polytope and a characterization of its extreme points . the hierarchy of car models defined in this way could be useful in parsimonious statistical modelling of car mechanisms , though the results also raise doubts in applied work as to the meaningfulness of the car assumption in its full generality .

modelling contextuality by probabilistic programs with hypergraph semantics
models of a phenomenon are often developed by examining it under different experimental conditions , or measurement contexts . the resultant probabilistic models assume that the underlying random variables , which define a measurable set of outcomes , can be defined independent of the measurement context . the phenomenon is deemed contextual when this assumption fails . contextuality is an important issue in quantum physics . however , there has been growing speculation that it manifests outside the quantum realm with human cognition being a particularly prominent area of investigation . this article contributes the foundations of a probabilistic programming language that allows convenient exploration of contextuality in wide range of applications relevant to cognitive science and artificial intelligence . specific syntax is proposed to allow the specification of `` measurement contexts '' . each such context delivers a partial model of the phenomenon based on the associated experimental condition described by the measurement context . the probabilistic program is translated into a hypergraph in a modular way . recent theoretical results from the field of quantum physics show that contextuality can be equated with the possibility of constructing a probabilistic model on the resulting hypergraph . the use of hypergraphs opens the door for a theoretically succinct and efficient computational semantics sensitive to modelling both contextual and non-contextual phenomena . finally , this article raises awareness of contextuality beyond quantum physics and to contribute formal methods to detect its presence by means of hypergraph semantics .

distribution over beliefs for memory bounded dec-pomdp planning
we propose a new point-based method for approximate planning in dec-pomdp which outperforms the state-of-the-art approaches in terms of solution quality . it uses a heuristic estimation of the prior probability of beliefs to choose a bounded number of policy trees : this choice is formulated as a combinatorial optimisation problem minimising the error induced by pruning .

étude de problèmes d'optimisation combinatoire à multiples composantes interdépendantes
this extended abstract presents an overview on np-hard optimization problems with multiple interdependent components . these problems occur in many real-world applications : industrial applications , engineering , and logistics . the fact that these problems are composed of many sub-problems that are np-hard makes them even more challenging to solve using exact algorithms . this is mainly due to the high complexity of this class of algorithms and the hardness of the problems themselves . the main source of difficulty of these problems is the presence of internal dependencies between sub-problems . this aspect of interdependence of components is presented , and some outlines on solving approaches are briefly introduced from a ( meta ) heuristics and evolutionary computation perspective .

an introduction to the dsm theory for the combination of paradoxical , uncertain , and imprecise sources of information
the management and combination of uncertain , imprecise , fuzzy and even paradoxical or high conflicting sources of information has always been , and still remains today , of primal importance for the development of reliable modern information systems involving artificial reasoning . in this introduction , we present a survey of our recent theory of plausible and paradoxical reasoning , known as dezert-smarandache theory ( dsmt ) in the literature , developed for dealing with imprecise , uncertain and paradoxical sources of information . we focus our presentation here rather on the foundations of dsmt , and on the two important new rules of combination , than on browsing specific applications of dsmt available in literature . several simple examples are given throughout the presentation to show the efficiency and the generality of this new approach .

a simple approach for finding the globally optimal bayesian network structure
we study the problem of learning the best bayesian network structure with respect to a decomposable score such as bde , bic or aic . this problem is known to be np-hard , which means that solving it becomes quickly infeasible as the number of variables increases . nevertheless , in this paper we show that it is possible to learn the best bayesian network structure with over 30 variables , which covers many practically interesting cases . our algorithm is less complicated and more efficient than the techniques presented earlier . it can be easily parallelized , and offers a possibility for efficient exploration of the best networks consistent with different variable orderings . in the experimental part of the paper we compare the performance of the algorithm to the previous state-of-the-art algorithm . free source-code and an online-demo can be found at http : //b-course.hiit.fi/bene .

unsupervised basis function adaptation for reinforcement learning
when using reinforcement learning ( rl ) algorithms to evaluate a policy it is common , given a large state space , to introduce some form of approximation architecture for the value function ( vf ) . the exact form of this architecture can have a significant effect on the accuracy of the vf estimate , however , and determining a suitable approximation architecture can often be a highly complex task . consequently there is a large amount of interest in the potential for allowing rl algorithms to adaptively generate approximation architectures . we investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail . this method is `` unsupervised '' in the sense that it makes no direct reference to reward or the vf estimate . we introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line . a common method of scoring a vf estimate is to weight the squared bellman error of each state-action by the probability of that state-action occurring . adopting this scoring method , and assuming $ s $ states , we demonstrate theoretically that - provided ( 1 ) the number of cells $ x $ in the state aggregation architecture is of order $ \sqrt { s } \log_2 { s } \ln { s } $ or greater , ( 2 ) the policy and transition function are close to deterministic , and ( 3 ) the prior for the transition function is uniformly distributed - our algorithm , used in conjunction with a suitable rl algorithm , can guarantee a score which is arbitrarily close to zero as $ s $ becomes large . it is able to do this despite having only $ o ( x \log_2s ) $ space complexity and negligible time complexity . the results take advantage of certain properties of the stationary distributions of markov chains .

semantic folding theory and its application in semantic fingerprinting
human language is recognized as a very complex domain since decades . no computer system has been able to reach human levels of performance so far . the only known computational system capable of proper language processing is the human brain . while we gather more and more data about the brain , its fundamental computational processes still remain obscure . the lack of a sound computational brain theory also prevents the fundamental understanding of natural language processing . as always when science lacks a theoretical foundation , statistical modeling is applied to accommodate as many sampled real-world data as possible . an unsolved fundamental issue is the actual representation of language ( data ) within the brain , denoted as the representational problem . starting with jeff hawkins ' hierarchical temporal memory ( htm ) theory , a consistent computational theory of the human cortex , we have developed a corresponding theory of language data representation : the semantic folding theory . the process of encoding words , by using a topographic semantic space as distributional reference frame into a sparse binary representational vector is called semantic folding and is the central topic of this document . semantic folding describes a method of converting language from its symbolic representation ( text ) into an explicit , semantically grounded representation that can be generically processed by hawkins ' htm networks . as it turned out , this change in representation , by itself , can solve many complex nlp problems by applying boolean operators and a generic similarity function like the euclidian distance . many practical problems of statistical nlp systems , like the high cost of computation , the fundamental incongruity of precision and recall , the complex tuning procedures etc. , can be elegantly overcome by applying semantic folding .

mattnet : modular attention network for referring expression comprehension
in this paper , we address referring expression comprehension : localizing an image region described by a natural language expression . while most recent work treats expressions as a single unit , we propose to decompose them into three modular components related to subject appearance , location , and relationship to other objects . this allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework . in our model , which we call the modular attention network ( mattnet ) , two types of attention are utilized : language-based attention that learns the module weights as well as the word/phrase attention that each module should focus on ; and visual attention that allows the subject and relationship modules to focus on relevant image components . module weights combine scores from all three modules dynamically to output an overall score . experiments show that mattnet outperforms previous state-of-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks . demo and code are provided .

counterfactuals , indicative conditionals , and negation under uncertainty : are there cross-cultural differences ?
in this paper we study selected argument forms involving counterfactuals and indicative conditionals under uncertainty . we selected argument forms to explore whether people with an eastern cultural background reason differently about conditionals compared to westerners , because of the differences in the location of negations . in a 2x2 between-participants design , 63 japanese university students were allocated to four groups , crossing indicative conditionals and counterfactuals , and each presented in two random task orders . the data show close agreement between the responses of easterners and westerners . the modal responses provide strong support for the hypothesis that conditional probability is the best predictor for counterfactuals and indicative conditionals . finally , the grand majority of the responses are probabilistically coherent , which endorses the psychological plausibility of choosing coherence-based probability logic as a rationality framework for psychological reasoning research .

solving rubik 's cube using sat solvers
rubik 's cube is an easily-understood puzzle , which is originally called the `` magic cube '' . it is a well-known planning problem , which has been studied for a long time . yet many simple properties remain unknown . this paper studies whether modern sat solvers are applicable to this puzzle . to our best knowledge , we are the first to translate rubik 's cube to a sat problem . to reduce the number of variables and clauses needed for the encoding , we replace a naive approach of 6 boolean variables to represent each color on each facelet with a new approach of 3 or 2 boolean variables . in order to be able to solve quickly rubik 's cube , we replace the direct encoding of 18 turns with the layer encoding of 18-subtype turns based on 6-type turns . to speed up the solving further , we encode some properties of two-phase algorithm as an additional constraint , and restrict some move sequences by adding some constraint clauses . using only efficient encoding can not solve this puzzle . for this reason , we improve the existing sat solvers , and develop a new sat solver based on precosat , though it is suited only for rubik 's cube . the new sat solver replaces the lookahead solving strategy with an alo ( \emph { at-least-one } ) solving strategy , and decomposes the original problem into sub-problems . each sub-problem is solved by precosat . the empirical results demonstrate both our sat translation and new solving technique are efficient . without the efficient sat encoding and the new solving technique , rubik 's cube will not be able to be solved still by any sat solver . using the improved sat solver , we can find always a solution of length 20 in a reasonable time . although our solver is slower than kociemba 's algorithm using lookup tables , but does not require a huge lookup table .

different types of conflicting knowledge in ami environments
we characterize different types of conflicts that may occur in complex distributed multi-agent scenarios , such as in ambient intelligence ( ami ) environments , and we argue that these conflicts should be resolved in a suitable order and with the appropriate strategies for each individual conflict type . we call for further research with the goal of turning conflict resolution in ami environments and similar multi-agent domains into a more coordinated and agreed upon process .

modeling the uncertainty in complex engineering systems
existing procedures for model validation have been deemed inadequate for many engineering systems . the reason of this inadequacy is due to the high degree of complexity of the mechanisms that govern these systems . it is proposed in this paper to shift the attention from modeling the engineering system itself to modeling the uncertainty that underlies its behavior . a mathematical framework for modeling the uncertainty in complex engineering systems is developed . this framework uses the results of computational learning theory . it is based on the premise that a system model is a learning machine .

on first-order model-based reasoning
reasoning semantically in first-order logic is notoriously a challenge . this paper surveys a selection of semantically-guided or model-based methods that aim at meeting aspects of this challenge . for first-order logic we touch upon resolution-based methods , tableaux-based methods , dpll-inspired methods , and we give a preview of a new method called sggs , for semantically-guided goal-sensitive reasoning . for first-order theories we highlight hierarchical and locality-based methods , concluding with the recent model-constructing satisfiability calculus .

open source software : how can design metrics facilitate architecture recovery ?
modern software development methodologies include reuse of open source code . reuse can be facilitated by architectural knowledge of the software , not necessarily provided in the documentation of open source software . the effort required to comprehend the system 's source code and discover its architecture can be considered a major drawback in reuse . in a recent study we examined the correlations between design metrics and classes ' architecture layer . in this paper , we apply our methodology in more open source projects to verify the applicability of our method . keywords : system understanding ; program comprehension ; object-oriented ; reuse ; architecture layer ; design metrics ;

search using n-gram technique based statistical analysis for knowledge extraction in case based reasoning systems
searching techniques for case based reasoning systems involve extensive methods of elimination . in this paper , we look at a new method of arriving at the right solution by performing a series of transformations upon the data . these involve n-gram based comparison and deduction of the input data with the case data , using morphemes and phonemes as the deciding parameters . a similar technique for eliminating possible errors using a noise removal function is performed . the error tracking and elimination is performed through a statistical analysis of obtained data , where the entire data set is analyzed as sub-categories of various etymological derivatives . a probability analysis for the closest match is then performed , which yields the final expression . this final expression is referred to the case base . the output is redirected through an expert system based on best possible match . the threshold for the match is customizable , and could be set by the knowledge-architect .

a frobenius model of information structure in categorical compositional distributional semantics
the categorical compositional distributional model of coecke , sadrzadeh and clark provides a linguistically motivated procedure for computing the meaning of a sentence as a function of the distributional meaning of the words therein . the theoretical framework allows for reasoning about compositional aspects of language and offers structural ways of studying the underlying relationships . while the model so far has been applied on the level of syntactic structures , a sentence can bring extra information conveyed in utterances via intonational means . in the current paper we extend the framework in order to accommodate this additional information , using frobenius algebraic structures canonically induced over the basis of finite-dimensional vector spaces . we detail the theory , provide truth-theoretic and distributional semantics for meanings of intonationally-marked utterances , and present justifications and extensive examples .

erratum : link prediction in drug-target interactions network using similarity indices
background : in silico drug-target interaction ( dti ) prediction plays an integral role in drug repositioning : the discovery of new uses for existing drugs . one popular method of drug repositioning is network-based dti prediction , which uses complex network theory to predict dtis from a drug-target network . currently , most network-based dti prediction is based on machine learning methods such as restricted boltzmann machines ( rbm ) or support vector machines ( svm ) . these methods require additional information about the characteristics of drugs , targets and dtis , such as chemical structure , genome sequence , binding types , causes of interactions , etc. , and do not perform satisfactorily when such information is unavailable . we propose a new , alternative method for dti prediction that makes use of only network topology information attempting to solve this problem . results : we compare our method for dti prediction against the well-known rbm approach . we show that when applied to the matador database , our approach based on node neighborhoods yield higher precision for high-ranking predictions than rbm when no information regarding dti types is available . conclusion : this demonstrates that approaches purely based on network topology provide a more suitable approach to dti prediction in the many real-life situations where little or no prior knowledge is available about the characteristics of drugs , targets , or their interactions .

practical challenges in explicit ethical machine reasoning
we examine implemented systems for ethical machine reasoning with a view to identifying the practical challenges ( as opposed to philosophical challenges ) posed by the area . we identify a need for complex ethical machine reasoning not only to be multi-objective , proactive , and scrutable but that it must draw on heterogeneous evidential reasoning . we also argue that , in many cases , it needs to operate in real time and be verifiable . we propose a general architecture involving a declarative ethical arbiter which draws upon multiple evidential reasoners each responsible for a particular ethical feature of the system 's environment . we claim that this architecture enables some separation of concerns among the practical challenges that ethical machine reasoning poses .

complexity analysis and variational inference for interpretation-based probabilistic description logic
this paper presents complexity analysis and variational methods for inference in probabilistic description logics featuring boolean operators , quantification , qualified number restrictions , nominals , inverse roles and role hierarchies . inference is shown to be pexp-complete , and variational methods are designed so as to exploit logical inference whenever possible .

a new upper bound on the capacity of a class of primitive relay channels
we obtain a new upper bound on the capacity of a class of discrete memoryless relay channels . for this class of relay channels , the relay observes an i.i.d . sequence $ t $ , which is independent of the channel input $ x $ . the channel is described by a set of probability transition functions $ p ( y|x , t ) $ for all $ ( x , t , y ) \in \mathcal { x } \times \mathcal { t } \times \mathcal { y } $ . furthermore , a noiseless link of finite capacity $ r_ { 0 } $ exists from the relay to the receiver . although the capacity for these channels is not known in general , the capacity of a subclass of these channels , namely when $ t=g ( x , y ) $ , for some deterministic function $ g $ , was obtained in [ 1 ] and it was shown to be equal to the cut-set bound . another instance where the capacity was obtained was in [ 2 ] , where the channel output $ y $ can be written as $ y=x\oplus z $ , where $ \oplus $ denotes modulo- $ m $ addition , $ z $ is independent of $ x $ , $ |\mathcal { x } |=|\mathcal { y } |=m $ , and $ t $ is some stochastic function of $ z $ . the compress-and-forward ( caf ) achievability scheme [ 3 ] was shown to be capacity achieving in both cases . using our upper bound we recover the capacity results of [ 1 ] and [ 2 ] . we also obtain the capacity of a class of channels which does not fall into either of the classes studied in [ 1 ] and [ 2 ] . for this class of channels , caf scheme is shown to be optimal but capacity is strictly less than the cut-set bound for certain values of $ r_ { 0 } $ . we also evaluate our outer bound for a particular relay channel with binary multiplicative states and binary additive noise for which the channel is given as $ y=tx+n $ . we show that our upper bound is strictly better than the cut-set upper bound for certain values of $ r_ { 0 } $ but it lies strictly above the rates yielded by the caf achievability scheme .

creating a social brain for cooperative connected autonomous vehicles : issues and challenges
the connected autonomous vehicle has been often touted as a technology that will become pervasive in society in the near future . rather than being stand alone , we examine the need for autonomous vehicles to cooperate and interact within their socio-cyber-physical environments , including the problems cooperation will solve , but also the issues and challenges .

a formal analysis of required cooperation in multi-agent planning
research on multi-agent planning has been popular in recent years . while previous research has been motivated by the understanding that , through cooperation , multi-agent systems can achieve tasks that are unachievable by single-agent systems , there are no formal characterizations of situations where cooperation is required to achieve a goal , thus warranting the application of multi-agent systems . in this paper , we provide such a formal discussion from the planning aspect . we first show that determining whether there is required cooperation ( rc ) is intractable is general . then , by dividing the problems that require cooperation ( referred to as rc problems ) into two classes -- problems with heterogeneous and homogeneous agents , we aim to identify all the conditions that can cause rc in these two classes . we establish that when none of these identified conditions hold , the problem is single-agent solvable . furthermore , with a few assumptions , we provide an upper bound on the minimum number of agents required for rc problems with homogeneous agents . this study not only provides new insights into multi-agent planning , but also has many applications . for example , in human-robot teaming , when a robot can not achieve a task , it may be due to rc . in such cases , the human teammate should be informed and , consequently , coordinate with other available robots for a solution .

truthful mechanisms for matching and clustering in an ordinal world
we study truthful mechanisms for matching and related problems in a partial information setting , where the agents ' true utilities are hidden , and the algorithm only has access to ordinal preference information . our model is motivated by the fact that in many settings , agents can not express the numerical values of their utility for different outcomes , but are still able to rank the outcomes in their order of preference . specifically , we study problems where the ground truth exists in the form of a weighted graph of agent utilities , but the algorithm can only elicit the agents ' private information in the form of a preference ordering for each agent induced by the underlying weights . against this backdrop , we design truthful algorithms to approximate the true optimum solution with respect to the hidden weights . our techniques yield universally truthful algorithms for a number of graph problems : a 1.76-approximation algorithm for max-weight matching , 2-approximation algorithm for max k-matching , a 6-approximation algorithm for densest k-subgraph , and a 2-approximation algorithm for max traveling salesman as long as the hidden weights constitute a metric . we also provide improved approximation algorithms for such problems when the agents are not able to lie about their preferences . our results are the first non-trivial truthful approximation algorithms for these problems , and indicate that in many situations , we can design robust algorithms even when the agents may lie and only provide ordinal information instead of precise utilities .

speeding up planning in markov decision processes via automatically constructed abstractions
in this paper , we consider planning in stochastic shortest path ( ssp ) problems , a subclass of markov decision problems ( mdp ) . we focus on medium-size problems whose state space can be fully enumerated . this problem has numerous important applications , such as navigation and planning under uncertainty . we propose a new approach for constructing a multi-level hierarchy of progressively simpler abstractions of the original problem . once computed , the hierarchy can be used to speed up planning by first finding a policy for the most abstract level and then recursively refining it into a solution to the original problem . this approach is fully automated and delivers a speed-up of two orders of magnitude over a state-of-the-art mdp solver on sample problems while returning near-optimal solutions . we also prove theoretical bounds on the loss of solution optimality resulting from the use of abstractions .

using local optimality criteria for efficient information retrieval with redundant information filters
we consider information retrieval when the data , for instance multimedia , is coputationally expensive to fetch . our approach uses `` information filters '' to considerably narrow the universe of possiblities before retrieval . we are especially interested in redundant information filters that save time over more general but more costly filters . efficient retrieval requires that decision must be made about the necessity , order , and concurrent processing of proposed filters ( an `` execution plan '' ) . we develop simple polynomial-time local criteria for optimal execution plans , and show that most forms of concurrency are suboptimal with information filters . although the general problem of finding an optimal execution plan is likely exponential in the number of filters , we show experimentally that our local optimality criteria , used in a polynomial-time algorithm , nearly always find the global optimum with 15 filters or less , a sufficient number of filters for most applications . our methods do not require special hardware and avoid the high processor idleness that is characteristic of massive parallelism solutions to this problem . we apply our ideas to an important application , information retrieval of cpationed data using natural-language understanding , a problem for which the natural-language processing can be the bottleneck if not implemented well .

multi-view constrained clustering with an incomplete mapping between views
multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process . however , many applications provide only a partial mapping between the views , creating a challenge for current methods . to address this problem , we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping . given a set of pairwise constraints in each view , our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views , allowing the propagated constraints to be transferred across views via the partial mapping . it uses co-em to iteratively estimate the propagation within each view based on the current clustering model , transfer the constraints across views , and then update the clustering model . by alternating the learning process between views , this approach produces a unified clustering model that is consistent with all views . we show that this approach significantly improves clustering performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views . our evaluation reveals that the propagated constraints have high precision with respect to the true clusters in the data , explaining their benefit to clustering performance in both single- and multi-view learning scenarios .

viqi : a new approach for visual interpretation of deep web query interfaces
deep web databases contain more than 90 % of pertinent information of the web . despite their importance , users do n't profit of this treasury . many deep web services are offering competitive services in term of prices , quality of service , and facilities . as the number of services is growing rapidly , users have difficulty to ask many web services in the same time . in this paper , we imagine a system where users have the possibility to formulate one query using one query interface and then the system translates query to the rest of query interfaces . however , interfaces are created by designers in order to be interpreted visually by users , machines can not interpret query from a given interface . we propose a new approach which emulates capacity of interpretation of users and extracts query from deep web query interfaces . our approach has proved good performances on two standard datasets .

an uncertainty management calculus for ordering searches in distributed dynamic databases
minds is a distributed system of cooperating query engines that customize , document retrieval for each user in a dynamic environment . it improves its performance and adapts to changing patterns of document distribution by observing system-user interactions and modifying the appropriate certainty factors , which act as search control parameters . it argued here that the uncertainty management calculus must account for temporal precedence , reliability of evidence , degree of support for a proposition , and saturation effects . the calculus presented here possesses these features . some results obtained with this scheme are discussed .

proceedings of the first conference on uncertainty in artificial intelligence ( 1985 )
this is the proceedings of the first conference on uncertainty in artificial intelligence , which was held in los angeles , ca , july 10-12 , 1985

mastering 2048 with delayed temporal coherence learning , multi-stage weight promotion , redundant encoding and carousel shaping
2048 is an engaging single-player , nondeterministic video puzzle game , which , thanks to the simple rules and hard-to-master gameplay , has gained massive popularity in recent years . as 2048 can be conveniently embedded into the discrete-state markov decision processes framework , we treat it as a testbed for evaluating existing and new methods in reinforcement learning . with the aim to develop a strong 2048 playing program , we employ temporal difference learning with systematic n-tuple networks . we show that this basic method can be significantly improved with temporal coherence learning , multi-stage function approximator with weight promotion , carousel shaping , and redundant encoding . in addition , we demonstrate how to take advantage of the characteristics of the n-tuple network , to improve the algorithmic effectiveness of the learning process by i ) delaying the ( decayed ) update and applying lock-free optimistic parallelism to effortlessly make advantage of multiple cpu cores . this way , we were able to develop the best known 2048 playing program to date , which confirms the effectiveness of the introduced methods for discrete-state markov decision problems .

case-based subgoaling in real-time heuristic search for video game pathfinding
real-time heuristic search algorithms satisfy a constant bound on the amount of planning per action , independent of problem size . as a result , they scale up well as problems become larger . this property would make them well suited for video games where artificial intelligence controlled agents must react quickly to user commands and to other agents actions . on the downside , real-time search algorithms employ learning methods that frequently lead to poor solution quality and cause the agent to appear irrational by re-visiting the same problem states repeatedly . the situation changed recently with a new algorithm , d lrta* , which attempted to eliminate learning by automatically selecting subgoals . d lrta* is well poised for video games , except it has a complex and memory-demanding pre-computation phase during which it builds a database of subgoals . in this paper , we propose a simpler and more memory-efficient way of pre-computing subgoals thereby eliminating the main obstacle to applying state-of-the-art real-time search methods in video games . the new algorithm solves a number of randomly chosen problems off-line , compresses the solutions into a series of subgoals and stores them in a database . when presented with a novel problem on-line , it queries the database for the most similar previously solved case and uses its subgoals to solve the problem . in the domain of pathfinding on four large video game maps , the new algorithm delivers solutions eight times better while using 57 times less memory and requiring 14 % less pre-computation time .

ontologies in system engineering : a field report
in recent years ontologies enjoyed a growing popularity outside specialized ai communities . system engineering is no exception to this trend , with ontologies being proposed as a basis for several tasks in complex industrial implements , including system design , monitoring and diagnosis . in this paper , we consider four different contributions to system engineering wherein ontologies are instrumental to provide enhancements over traditional ad-hoc techniques . for each application , we briefly report the methodologies , the tools and the results obtained with the goal to provide an assessment of merits and limits of ontologies in such domains .

comparing svm and naive bayes classifiers for text categorization with wikitology as knowledge enrichment
the activity of labeling of documents according to their content is known as text categorization . many experiments have been carried out to enhance text categorization by adding background knowledge to the document using knowledge repositories like word net , open project directory ( opd ) , wikipedia and wikitology . in our previous work , we have carried out intensive experiments by extracting knowledge from wikitology and evaluating the experiment on support vector machine with 10- fold cross-validations . the results clearly indicate wikitology is far better than other knowledge bases . in this paper we are comparing support vector machine ( svm ) and na\ '' ive bayes ( nb ) classifiers under text enrichment through wikitology . we validated results with 10-fold cross validation and shown that nb gives an improvement of +28.78 % , on the other hand svm gives an improvement of +6.36 % when compared with baseline results . na\ '' ive bayes classifier is better choice when external enriching is used through any external knowledge base .

transforming cooling optimization for green data center via deep reinforcement learning
cooling system plays a key role in modern data center . developing an optimal control policy for data center cooling system is a challenging task . the prevailing approaches often rely on approximated system models that are built upon the knowledge of mechanical cooling , electrical and thermal management , which is difficult to design and may lead to sub-optimal or unstable performances . in this paper we propose to utilize the large amount of monitoring data in data center to optimize the control policy . to do so , we cast the cooling control policy design into an energy cost minimization problem with temperature constraints , and tab it into the emerging deep reinforcement learning ( drl ) framework . specifically , we propose an end-to-end neural control algorithm that is based on the actor-critic framework and the deep deterministic policy gradient ( ddpg ) technique . to improve the robustness of the control algorithm , we test various drl related optimization techniques , such as recurrent decision making , discounted return , different neural network architectures , and different stochastic gradient descent algorithms , and adding additional constraints on the output of the policy network . we evaluate the proposed algorithms on the energyplus simulation platform and on a real data trace collected from the national super computing centre ( nscc ) of singapore . our results show that the proposed end-to-end cooling control algorithm can achieve about 10 % cooling cost saving on the simulation platform compared with a canonical two stage optimization algorithm ; and it can achieve about 13.6 % cooling energy saving on the nscc data trace . furthermore , it shows high accuracy in predicting the temperature of the racks ( with mean absolute error 0.1 degree ) and can control the temperature of the data center zone close to the predefined threshold with variation lower to 0.2 degree .

parallelized tensor train learning of polynomial classifiers
in pattern classification , polynomial classifiers are well-studied methods as they are capable of generating complex decision surfaces . unfortunately , the use of multivariate polynomials is limited to kernels as in support vector machines , because polynomials quickly become impractical for high-dimensional problems . in this paper , we effectively overcome the curse of dimensionality by employing the tensor train format to represent a polynomial classifier . based on the structure of tensor trains , two learning algorithms are proposed which involve solving different optimization problems of low computational complexity . furthermore , we show how both regularization to prevent overfitting and parallelization , which enables the use of large training sets , are incorporated into these methods . both the efficiency and efficacy of our tensor-based polynomial classifier are then demonstrated on the two popular datasets usps and mnist .

multi-task learning with gradient guided policy specialization
we present a method for efficient learning of control policies for multiple related robotic motor skills . our approach consists of two stages , joint training and specialization training . during the joint training stage , a neural network policy is trained with minimal information to disambiguate the motor skills . this forces the policy to learn a common representation of the different tasks . then , during the specialization training stage we selectively split the weights of the policy based on a per-weight metric that measures the disagreement among the multiple tasks . by splitting part of the control policy , it can be further trained to specialize to each task . to update the control policy during learning , we use trust region policy optimization with generalized advantage function ( trpogae ) . we propose a modification to the gradient update stage of trpo to better accommodate multi-task learning scenarios . we evaluate our approach on three continuous motor skill learning problems in simulation : 1 ) a locomotion task where three single legged robots with considerable difference in shape and size are trained to hop forward , 2 ) a manipulation task where three robot manipulators with different sizes and joint types are trained to reach different locations in 3d space , and 3 ) locomotion of a two-legged robot , whose range of motion of one leg is constrained in different ways . we compare our training method to three baselines . the first baseline uses only joint training for the policy , the second trains independent policies for each task , and the last randomly selects weights to split . we show that our approach learns more efficiently than each of the baseline methods .

sharing deep generative representation for perceived image reconstruction from human brain activity
decoding human brain activities via functional magnetic resonance imaging ( fmri ) has gained increasing attention in recent years . while encouraging results have been reported in brain states classification tasks , reconstructing the details of human visual experience still remains difficult . two main challenges that hinder the development of effective models are the perplexing fmri measurement noise and the high dimensionality of limited data instances . existing methods generally suffer from one or both of these issues and yield dissatisfactory results . in this paper , we tackle this problem by casting the reconstruction of visual stimulus as the bayesian inference of missing view in a multiview latent variable model . sharing a common latent representation , our joint generative model of external stimulus and brain response is not only `` deep '' in extracting nonlinear features from visual images , but also powerful in capturing correlations among voxel activities of fmri recordings . the nonlinearity and deep structure endow our model with strong representation ability , while the correlations of voxel activities are critical for suppressing noise and improving prediction . we devise an efficient variational bayesian method to infer the latent variables and the model parameters . to further improve the reconstruction accuracy , the latent representations of testing instances are enforced to be close to that of their neighbours from the training set via posterior regularization . experiments on three fmri recording datasets demonstrate that our approach can more accurately reconstruct visual stimuli .

human-recognizable robotic gestures
for robots to be accommodated in human spaces and in humans daily activities , robots should be able to understand messages from the human conversation partner . in the same light , humans must also understand the messages that are being communicated by robots , including the non-verbal ones . we conducted a web-based video study wherein participants gave interpretations on the iconic gestures and emblems that were produced by an anthropomorphic robot . out of the 15 gestures presented , we found 6 robotic gestures that can be accurately recognized by the human observer . these were nodding , clapping , hugging , expressing anger , walking , and flying . we reviewed these gestures for their meaning from literatures in human and animal behavior . we conclude by discussing the possible implications of these gestures for the design of social robots that are aimed to have engaging interactions with humans .

a comparison of lex bounds for multiset variables in constraint programming
set and multiset variables in constraint programming have typically been represented using subset bounds . however , this is a weak representation that neglects potentially useful information about a set such as its cardinality . for set variables , the length-lex ( ll ) representation successfully provides information about the length ( cardinality ) and position in the lexicographic ordering . for multiset variables , where elements can be repeated , we consider richer representations that take into account additional information . we study eight different representations in which we maintain bounds according to one of the eight different orderings : length- ( co ) lex ( ll/lc ) , variety- ( co ) lex ( vl/vc ) , length-variety- ( co ) lex ( lvl/lvc ) , and variety-length- ( co ) lex ( vll/vlc ) orderings . these representations integrate together information about the cardinality , variety ( number of distinct elements in the multiset ) , and position in some total ordering . theoretical and empirical comparisons of expressiveness and compactness of the eight representations suggest that length-variety- ( co ) lex ( lvl/lvc ) and variety-length- ( co ) lex ( vll/vlc ) usually give tighter bounds after constraint propagation . we implement the eight representations and evaluate them against the subset bounds representation with cardinality and variety reasoning . results demonstrate that they offer significantly better pruning and runtime .

feature-weighted linear stacking
ensemble methods , such as stacking , are designed to boost predictive accuracy by blending the predictions of multiple machine learning models . recent work has shown that the use of meta-features , additional inputs describing each example in a dataset , can boost the performance of ensemble methods , but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time . here , we present a linear technique , feature-weighted linear stacking ( fwls ) , that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed , stability , and interpretability . fwls combines model predictions linearly using coefficients that are themselves linear functions of meta-features . this technique was a key facet of the solution of the second place team in the recently concluded netflix prize competition . significant increases in accuracy over standard linear stacking are demonstrated on the netflix prize collaborative filtering dataset .

challenging images for minds and machines
there is no denying the tremendous leap in the performance of machine learning methods in the past half-decade . some might even say that specific sub-fields in pattern recognition , such as machine-vision , are as good as solved , reaching human and super-human levels . arguably , lack of training data and computation power are all that stand between us and solving the remaining ones . in this position paper we underline cases in vision which are challenging to machines and even to human observers . this is to show limitations of contemporary models that are hard to ameliorate by following the current trend to increase training data , network capacity or computational power . moreover , we claim that attempting to do so is in principle a suboptimal approach . we provide a taster of such examples in hope to encourage and challenge the machine learning community to develop new directions to solve the said difficulties .

efficient model-based deep reinforcement learning with variational state tabulation
modern reinforcement learning algorithms reach super-human performance in many board and video games , but they are sample inefficient , i.e . they typically require significantly more playing experience than humans to reach an equal performance level . to improve sample efficiency , an agent may build a model of the environment and use planning methods to update its policy . in this article we introduce vast ( variational state tabulation ) , which maps an environment with a high-dimensional state space ( e.g . the space of visual inputs ) to an abstract tabular environment . prioritized sweeping with small backups , a highly efficient planning method , can then be used to update state-action values . we show how vast can rapidly learn to maximize reward in tasks like 3d navigation and efficiently adapt to sudden changes in rewards or transition probabilities .

a probabilistic framework for location inference from social media
we study the extent to which we can infer users ' geographical locations from social media . location inference from social media can benefit many applications , such as disaster management , targeted advertising , and news content tailoring . in recent years , a number of algorithms have been proposed for identifying user locations on social media platforms such as twitter and facebook from message contents , friend networks , and interactions between users . in this paper , we propose a novel probabilistic model based on factor graphs for location inference that offers several unique advantages for this task . first , the model generalizes previous methods by incorporating content , network , and deep features learned from social context . the model is also flexible enough to support both supervised learning and semi-supervised learning . second , we explore several learning algorithms for the proposed model , and present a two-chain metropolis-hastings ( mh+ ) algorithm , which improves the inference accuracy . third , we validate the proposed model on three different genres of data - twitter , weibo , and facebook - and demonstrate that the proposed model can substantially improve the inference accuracy ( +3.3-18.5 % by f1-score ) over that of several state-of-the-art methods .

combining multiple time series models through a robust weighted mechanism
improvement of time series forecasting accuracy through combining multiple models is an important as well as a dynamic area of research . as a result , various forecasts combination methods have been developed in literature . however , most of them are based on simple linear ensemble strategies and hence ignore the possible relationships between two or more participating models . in this paper , we propose a robust weighted nonlinear ensemble technique which considers the individual forecasts from different models as well as the correlations among them while combining . the proposed ensemble is constructed using three well-known forecasting models and is tested for three real-world time series . a comparison is made among the proposed scheme and three other widely used linear combination methods , in terms of the obtained forecast errors . this comparison shows that our ensemble scheme provides significantly lower forecast errors than each individual model as well as each of the four linear combination methods .

mapping big data into knowledge space with cognitive cyber-infrastructure
big data research has attracted great attention in science , technology , industry and society . it is developing with the evolving scientific paradigm , the fourth industrial revolution , and the transformational innovation of technologies . however , its nature and fundamental challenge have not been recognized , and its own methodology has not been formed . this paper explores and answers the following questions : what is big data ? what are the basic methods for representing , managing and analyzing big data ? what is the relationship between big data and knowledge ? can we find a mapping from big data into knowledge space ? what kind of infrastructure is required to support not only big data management and analysis but also knowledge discovery , sharing and management ? what is the relationship between big data and science paradigm ? what is the nature and fundamental challenge of big data computing ? a multi-dimensional perspective is presented toward a methodology of big data computing .

a novel biologically mechanism-based visual cognition model -- automatic extraction of semantics , formation of integrated concepts and re-selection features for ambiguity
integration between biology and information science benefits both fields . many related models have been proposed , such as computational visual cognition models , computational motor control models , integrations of both and so on . in general , the robustness and precision of recognition is one of the key problems for object recognition models . in this paper , inspired by features of human recognition process and their biological mechanisms , a new integrated and dynamic framework is proposed to mimic the semantic extraction , concept formation and feature re-selection in human visual processing . the main contributions of the proposed model are as follows : ( 1 ) semantic feature extraction : local semantic features are learnt from episodic features that are extracted from raw images through a deep neural network ; ( 2 ) integrated concept formation : concepts are formed with local semantic information and structural information learnt through network . ( 3 ) feature re-selection : when ambiguity is detected during recognition process , distinctive features according to the difference between ambiguous candidates are re-selected for recognition . experimental results on hand-written digits and facial shape dataset show that , compared with other methods , the new proposed model exhibits higher robustness and precision for visual recognition , especially in the condition when input samples are smantic ambiguous . meanwhile , the introduced biological mechanisms further strengthen the interaction between neuroscience and information science .

scene learning , recognition and similarity detection in a fuzzy ontology via human examples
this paper introduces a fuzzy logic framework for scene learning , recognition and similarity detection , where scenes are taught via human examples . the framework allows a robot to : ( i ) deal with the intrinsic vagueness associated with determining spatial relations among objects ; ( ii ) infer similarities and dissimilarities in a set of scenes , and represent them in a hierarchical structure represented in a fuzzy ontology . in this paper , we briefly formalize our approach and we provide a few use cases by way of illustration . nevertheless , we discuss how the framework can be used in real-world scenarios .

detecting motifs in system call sequences
the search for patterns or motifs in data represents an area of key interest to many researchers . in this paper we present the motif tracking algorithm , a novel immune inspired pattern identification tool that is able to identify unknown motifs which repeat within time series data . the power of the algorithm is derived from its use of a small number of parameters with minimal assumptions . the algorithm searches from a completely neutral perspective that is independent of the data being analysed , and the underlying motifs . in this paper the motif tracking algorithm is applied to the search for patterns within sequences of low level system calls between the linux kernel and the operating system 's user space . the mta is able to compress data found in large system call data sets to a limited number of motifs which summarise that data . the motifs provide a resource from which a profile of executed processes can be built . the potential for these profiles and new implications for security research are highlighted . a higher level call system language for measuring similarity between patterns of such calls is also suggested .

anthropic decision theory
this paper sets out to resolve how agents ought to act in the sleeping beauty problem and various related anthropic ( self-locating belief ) problems , not through the calculation of anthropic probabilities , but through finding the correct decision to make . it creates an anthropic decision theory ( adt ) that decides these problems from a small set of principles . by doing so , it demonstrates that the attitude of agents with regards to each other ( selfish or altruistic ) changes the decisions they reach , and that it is very important to take this into account . to illustrate adt , it is then applied to two major anthropic problems and paradoxes , the presumptuous philosopher and doomsday problems , thus resolving some issues about the probability of human extinction .

sample efficient deep reinforcement learning for dialogue systems with large action spaces
in spoken dialogue systems , we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans . a part of this effort is the policy optimisation task , which attempts to find a policy describing how to respond to humans , in the form of a function taking the current state of the dialogue and returning the response of the system . in this paper , we investigate deep reinforcement learning approaches to solve this problem . particular attention is given to actor-critic methods , off-policy reinforcement learning with experience replay , and various methods aimed at reducing the bias and variance of estimators . when combined , these methods result in the previously proposed acer algorithm that gave competitive results in gaming environments . these environments however are fully observable and have a relatively small action set so in this paper we examine the application of acer to dialogue policy optimisation . we show that this method beats the current state-of-the-art in deep learning approaches for spoken dialogue systems . this not only leads to a more sample efficient algorithm that can train faster , but also allows us to apply the algorithm in more difficult environments than before . we thus experiment with learning in a very large action space , which has two orders of magnitude more actions than previously considered . we find that acer trains significantly faster than the current state-of-the-art .

procedural generation of angry birds levels using building constructive grammar with chinese-style and/or japanese-style models
this paper presents a procedural generation method that creates visually attractive levels for the angry birds game . besides being an immensely popular mobile game , angry birds has recently become a test bed for various artificial intelligence technologies . we propose a new approach for procedurally generating angry birds levels using chinese style and japanese style building structures . a conducted experiment confirms the effectiveness of our approach with statistical significance .

the dilated triple
the basic unit of meaning on the semantic web is the rdf statement , or triple , which combines a distinct subject , predicate and object to make a definite assertion about the world . a set of triples constitutes a graph , to which they give a collective meaning . it is upon this simple foundation that the rich , complex knowledge structures of the semantic web are built . yet the very expressiveness of rdf , by inviting comparison with real-world knowledge , highlights a fundamental shortcoming , in that rdf is limited to statements of absolute fact , independent of the context in which a statement is asserted . this is in stark contrast with the thoroughly context-sensitive nature of human thought . the model presented here provides a particularly simple means of contextualizing an rdf triple by associating it with related statements in the same graph . this approach , in combination with a notion of graph similarity , is sufficient to select only those statements from an rdf graph which are subjectively most relevant to the context of the requesting process .

operations for learning with graphical models
this paper is a multidisciplinary review of empirical , statistical learning from a graphical model perspective . well-known examples of graphical models include bayesian networks , directed graphs representing a markov chain , and undirected networks representing a markov field . these graphical models are extended to model data analysis and empirical learning using the notation of plates . graphical operations for simplifying and manipulating a problem are provided including decomposition , differentiation , and the manipulation of probability models from the exponential family . two standard algorithm schemas for learning are reviewed in a graphical framework : gibbs sampling and the expectation maximization algorithm . using these operations and schemas , some popular algorithms can be synthesized from their graphical specification . this includes versions of linear regression , techniques for feed-forward networks , and learning gaussian and discrete bayesian networks from data . the paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented . the main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms .

improving the accuracy and efficiency of map inference for markov logic
in this work we present cutting plane inference ( cpi ) , a maximum a posteriori ( map ) inference method for statistical relational learning . framed in terms of markov logic and inspired by the cutting plane method , it can be seen as a meta algorithm that instantiates small parts of a large and complex markov network and then solves these using a conventional map method . we evaluate cpi on two tasks , semantic role labelling and joint entity resolution , while plugging in two different map inference methods : the current method of choice for map inference in markov logic , maxwalksat , and integer linear programming . we observe that when used with cpi both methods are significantly faster than when used alone . in addition , cpi improves the accuracy of maxwalksat and maintains the exactness of integer linear programming .

decision-analytic approaches to operational decision making : application and observation
decision analysis ( da ) and the rich set of tools developed by researchers in decision making under uncertainty show great potential to penetrate the technological content of the products and services delivered by firms in a variety of industries as well as the business processes used to deliver those products and services to market . in this paper i describe work in progress at sun microsystems in the application of decision-analytic methods to operational decision making ( odm ) in its world-wide operations ( wwops ) business management group . working with membersof product engineering , marketing , and sales , operations planners from wwops have begun to use a decision-analytic framework called scram ( supply communication/risk assessment and management ) to structure and solve problems in product planning , tracking , and transition . concepts such as information value provide a powerful method of managing huge information sets and thereby enable managers to focus attention on factors that matter most for their business . finally , our process-oriented introduction of decision-analytic methods to sun managers has led to a focused effort to develop decision support software based on methods from decision making under uncertainty .

understanding the effective receptive field in deep convolutional neural networks
we study characteristics of receptive fields of units in deep convolutional networks . the receptive field size is a crucial issue in many visual tasks , as the output must respond to large enough areas in the image to capture information about large objects . we introduce the notion of an effective receptive field , and show that it both has a gaussian distribution and only occupies a fraction of the full theoretical receptive field . we analyze the effective receptive field in several architecture designs , and the effect of nonlinear activations , dropout , sub-sampling and skip connections on it . this leads to suggestions for ways to address its tendency to be too small .

boosted generative models
we propose a novel approach for using unsupervised boosting to create an ensemble of generative models , where models are trained in sequence to correct earlier mistakes . our meta-algorithmic framework can leverage any existing base learner that permits likelihood evaluation , including recent deep expressive models . further , our approach allows the ensemble to include discriminative models trained to distinguish real data from model-generated data . we show theoretical conditions under which incorporating a new model in the ensemble will improve the fit and empirically demonstrate the effectiveness of our black-box boosting algorithms on density estimation , classification , and sample generation on benchmark datasets for a wide range of generative models .

sr-clustering : semantic regularized clustering for egocentric photo streams segmentation
while wearable cameras are becoming increasingly popular , locating relevant information in large unstructured collections of egocentric images is still a tedious and time consuming processes . this paper addresses the problem of organizing egocentric photo streams acquired by a wearable camera into semantically meaningful segments . first , contextual and semantic information is extracted for each image by employing a convolutional neural networks approach . later , by integrating language processing , a vocabulary of concepts is defined in a semantic space . finally , by exploiting the temporal coherence in photo streams , images which share contextual and semantic attributes are grouped together . the resulting temporal segmentation is particularly suited for further analysis , ranging from activity and event recognition to semantic indexing and summarization . experiments over egocentric sets of nearly 17,000 images , show that the proposed approach outperforms state-of-the-art methods .

optimal sparse linear auto-encoders and sparse pca
principal components analysis ( pca ) is the optimal linear auto-encoder of data , and it is often used to construct features . enforcing sparsity on the principal components can promote better generalization , while improving the interpretability of the features . we study the problem of constructing optimal sparse linear auto-encoders . two natural questions in such a setting are : i ) given a level of sparsity , what is the best approximation to pca that can be achieved ? ii ) are there low-order polynomial-time algorithms which can asymptotically achieve this optimal tradeoff between the sparsity and the approximation quality ? in this work , we answer both questions by giving efficient low-order polynomial-time algorithms for constructing asymptotically \emph { optimal } linear auto-encoders ( in particular , sparse features with near-pca reconstruction error ) and demonstrate the performance of our algorithms on real data .

learn & fuzz : machine learning for input fuzzing
fuzzing consists of repeatedly testing an application with modified , or fuzzed , inputs with the goal of finding security vulnerabilities in input-parsing code . in this paper , we show how to automate the generation of an input grammar suitable for input fuzzing using sample inputs and neural-network-based statistical machine-learning techniques . we present a detailed case study with a complex input format , namely pdf , and a large complex security-critical parser for this format , namely , the pdf parser embedded in microsoft 's new edge browser . we discuss ( and measure ) the tension between conflicting learning and fuzzing goals : learning wants to capture the structure of well-formed inputs , while fuzzing wants to break that structure in order to cover unexpected code paths and find bugs . we also present a new algorithm for this learn & fuzz challenge which uses a learnt input probability distribution to intelligently guide where to fuzz inputs .

end-to-end lstm-based dialog control optimized with supervised and reinforcement learning
this paper presents a model for end-to-end learning of task-oriented dialog systems . the main component of the model is a recurrent neural network ( an lstm ) , which maps from raw dialog history directly to a distribution over system actions . the lstm automatically infers a representation of dialog history , which relieves the system developer of much of the manual feature engineering of dialog state . in addition , the developer can provide software that expresses business rules and provides access to programmatic apis , enabling the lstm to take actions in the real world on behalf of the user . the lstm can be optimized using supervised learning ( sl ) , where a domain expert provides example dialogs which the lstm should imitate ; or using reinforcement learning ( rl ) , where the system improves by interacting directly with end users . experiments show that sl and rl are complementary : sl alone can derive a reasonable initial policy from a small number of training dialogs ; and starting rl optimization with a policy trained with sl substantially accelerates the learning rate of rl .

continuous implicit authentication for mobile devices based on adaptive neuro-fuzzy inference system
as mobile devices have become indispensable in modern life , mobile security is becoming much more important . traditional password or pin-like point-of-entry security measures score low on usability and are vulnerable to brute force and other types of attacks . in order to improve mobile security , an adaptive neuro-fuzzy inference system ( anfis ) -based implicit authentication system is proposed in this paper to provide authentication in a continuous and transparent manner.to illustrate the applicability and capability of anfis in our implicit authentication system , experiments were conducted on behavioural data collected for up to 12 weeks from different android users . the ability of the anfis-based system to detect an adversary is also tested with scenarios involving an attacker with varying levels of knowledge . the results demonstrate that anfis is a feasible and efficient approach for implicit authentication with an average of 95 % user recognition rate . moreover , the use of anfis-based system for implicit authentication significantly reduces manual tuning and configuration tasks due to its selflearning capability .

comparison of ontology alignment algorithms across single matching task via the mcnemar test
ontology alignment is widely used to find the correspondences between different ontologies in diverse fields . after discovering the alignment by methods , several performance scores are available to evaluate them . the scores require the produced alignment by a method and the reference alignment containing the underlying actual correspondences of the given ontologies . the current trend in alignment evaluation is to put forward a new score and to compare various alignments by juxtaposing their performance scores . however , it is substantially provocative to select one performance score among others for comparison . on top of that , claiming if one method has a better performance than one another can not be substantiated by solely comparing the scores . in this paper , we propose the statistical procedures which enable us to theoretically favor one method over one another . the mcnemar test is considered as a reliable and suitable means for comparing two ontology alignment methods over one matching task . the test applies to a 2 x 2 contingency table which can be constructed in two different ways based on the alignments , each of which has their own merits/pitfalls . the ways of the contingency table construction and various apposite statistics from the mcnemar test are elaborated in minute detail . in the case of having more than two alignment methods for comparison , the family-wise error rate is expected to happen . thus , the ways of preventing such an error are also discussed . a directed graph visualizes the outcome of the mcnemar test in the presence of multiple alignment methods . from this graph , it is readily understood if one method is better than one another or if their differences are imperceptible . our investigation on the methods participated in the anatomy track of oaei 2016 demonstrates that aml and cromatcher are the top two methods and dkp-aom and alin are the bottom two ones .

concurrent cube-and-conquer
recent work introduced the cube-and-conquer technique to solve hard sat instances . it partitions the search space into cubes using a lookahead solver . each cube is tackled by a conflict-driven clause learning ( cdcl ) solver . crucial for strong performance is the cutoff heuristic that decides when to switch from lookahead to cdcl . yet , this offline heuristic is far from ideal . in this paper , we present a novel hybrid solver that applies the cube and conquer steps simultaneously . a lookahead and a cdcl solver work together on each cube , while communication is restricted to synchronization . our concurrent cube-and-conquer solver can solve many instances faster than pure lookahead , pure cdcl and offline cube-and-conquer , and can abort early in favor of a pure cdcl search if an instance is not suitable for cube-and-conquer techniques .

hordeqbf : a modular and massively parallel qbf solver
the recently developed massively parallel satisfiability ( sat ) solver hordesat was designed in a modular way to allow the integration of any sequential cdcl-based sat solver in its core . we integrated the qcdcl-based quantified boolean formula ( qbf ) solver depqbf in hordesat to obtain a massively parallel qbf solver -- -hordeqbf . in this paper we describe the details of this integration and report on results of the experimental evaluation of hordeqbf 's performance . hordeqbf achieves superlinear average and median speedup on the hard application instances of the 2014 qbf gallery .

stable marriage problems with quantitative preferences
the stable marriage problem is a well-known problem of matching men to women so that no man and woman , who are not married to each other , both prefer each other . such a problem has a wide variety of practical applications , ranging from matching resident doctors to hospitals , to matching students to schools or more generally to any two-sided market . in the classical stable marriage problem , both men and women express a strict preference order over the members of the other sex , in a qualitative way . here we consider stable marriage problems with quantitative preferences : each man ( resp. , woman ) provides a score for each woman ( resp. , man ) . such problems are more expressive than the classical stable marriage problems . moreover , in some real-life situations it is more natural to express scores ( to model , for example , profits or costs ) rather than a qualitative preference ordering . in this context , we define new notions of stability and optimality , and we provide algorithms to find marriages which are stable and/or optimal according to these notions . while expressivity greatly increases by adopting quantitative preferences , we show that in most cases the desired solutions can be found by adapting existing algorithms for the classical stable marriage problem .

a batch noise contrastive estimation approach for training large vocabulary language models
training large vocabulary neural network language models ( nnlms ) is a difficult task due to the explicit requirement of the output layer normalization , which typically involves the evaluation of the full softmax function over the complete vocabulary . this paper proposes a batch noise contrastive estimation ( b-nce ) approach to alleviate this problem . this is achieved by reducing the vocabulary , at each time step , to the target words in the batch and then replacing the softmax by the noise contrastive estimation approach , where these words play the role of targets and noise samples at the same time . in doing so , the proposed approach can be fully formulated and implemented using optimal dense matrix operations . applying b-nce to train different nnlms on the large text compression benchmark ( ltcb ) and the one billion word benchmark ( obwb ) shows a significant reduction of the training time with no noticeable degradation of the models performance . this paper also presents a new baseline comparative study of different standard nnlms on the large obwb on a single titan-x gpu .

building the signature of set theory using the mathsem program
knowledge representation is a popular research field in it . as mathematical knowledge is most formalized , its representation is important and interesting . mathematical knowledge consists of various mathematical theories . in this paper we consider a deductive system that derives mathematical notions , axioms and theorems . all these notions , axioms and theorems can be considered as the part of elementary set theory . this theory will be represented as a semantic net .

learning to design games : strategic environments in deep reinforcement learning
in typical reinforcement learning ( rl ) , the environment is assumed given and the goal of the learning is to identify an optimal policy for the agent taking actions through its interactions with the environment . in this paper , we extend this setting by considering the environment is not given , but controllable and learnable through its interaction with the agent at the same time . theoretically , we find a dual markov decision process ( mdp ) w.r.t . the environment to that w.r.t . the agent , and solving the dual mdp-policy pair yields a policy gradient solution to optimizing the parametrized environment . furthermore , environments with discontinuous parameters are addressed by a proposed general generative framework . while the idea is illustrated by an extended two-agent rock-paper-scissors game , our experiments on a maze game design task show the effectiveness of the proposed algorithm in generating diverse and challenging mazes against different agents with various settings .

a preliminary analysis on metaheuristics methods applied to the haplotype inference problem
haplotype inference is a challenging problem in bioinformatics that consists in inferring the basic genetic constitution of diploid organisms on the basis of their genotype . this information allows researchers to perform association studies for the genetic variants involved in diseases and the individual responses to therapeutic agents . a notable approach to the problem is to encode it as a combinatorial problem ( under certain hypotheses , such as the pure parsimony criterion ) and to solve it using off-the-shelf combinatorial optimization techniques . the main methods applied to haplotype inference are either simple greedy heuristic or exact methods ( integer linear programming , semidefinite programming , sat encoding ) that , at present , are adequate only for moderate size instances . we believe that metaheuristic and hybrid approaches could provide a better scalability . moreover , metaheuristics can be very easily combined with problem specific heuristics and they can also be integrated with tree-based search techniques , thus providing a promising framework for hybrid systems in which a good trade-off between effectiveness and efficiency can be reached . in this paper we illustrate a feasibility study of the approach and discuss some relevant design issues , such as modeling and design of approximate solvers that combine constructive heuristics , local search-based improvement strategies and learning mechanisms . besides the relevance of the haplotype inference problem itself , this preliminary analysis is also an interesting case study because the formulation of the problem poses some challenges in modeling and hybrid metaheuristic solver design that can be generalized to other problems .

seeing unseeability to see the unseeable
we present a framework that allows an observer to determine occluded portions of a structure by finding the maximum-likelihood estimate of those occluded portions consistent with visible image evidence and a consistency model . doing this requires determining which portions of the structure are occluded in the first place . since each process relies on the other , we determine a solution to both problems in tandem . we extend our framework to determine confidence of one 's assessment of which portions of an observed structure are occluded , and the estimate of that occluded structure , by determining the sensitivity of one 's assessment to potential new observations . we further extend our framework to determine a robotic action whose execution would allow a new observation that would maximally increase one 's confidence .

effectiveness of pre- and inprocessing for cdcl-based sat solving
applying pre- and inprocessing techniques to simplify cnf formulas both before and during search can considerably improve the performance of modern sat solvers . these algorithms mostly aim at reducing the number of clauses , literals , and variables in the formula . however , to be worthwhile , it is necessary that their additional runtime does not exceed the runtime saved during the subsequent sat solver execution . in this paper we investigate the efficiency and the practicability of selected simplification algorithms for cdcl-based sat solving . we first analyze them by means of their expected impact on the cnf formula and sat solving at all . while testing them on real-world and combinatorial sat instances , we show which techniques and combinations of them yield a desirable speedup and which ones should be avoided .

foundations for uniform interpolation and forgetting in expressive description logics
we study uniform interpolation and forgetting in the description logic alc . our main results are model-theoretic characterizations of uniform inter- polants and their existence in terms of bisimula- tions , tight complexity bounds for deciding the existence of uniform interpolants , an approach to computing interpolants when they exist , and tight bounds on their size . we use a mix of model- theoretic and automata-theoretic methods that , as a by-product , also provides characterizations of and decision procedures for conservative extensions .

learning a unified control policy for safe falling
being able to fall safely is a necessary motor skill for humanoids performing highly dynamic tasks , such as running and jumping . we propose a new method to learn a policy that minimizes the maximal impulse during the fall . the optimization solves for both a discrete contact planning problem and a continuous optimal control problem . once trained , the policy can compute the optimal next contacting body part ( e.g . left foot , right foot , or hands ) , contact location and timing , and the required joint actuation . we represent the policy as a mixture of actor-critic neural network , which consists of n control policies and the corresponding value functions . each pair of actor-critic is associated with one of the n possible contacting body parts . during execution , the policy corresponding to the highest value function will be executed while the associated body part will be the next contact with the ground . with this mixture of actor-critic architecture , the discrete contact sequence planning is solved through the selection of the best critics while the continuous control problem is solved by the optimization of actors . we show that our policy can achieve comparable , sometimes even higher , rewards than a recursive search of the action space using dynamic programming , while enjoying 50 to 400 times of speed gain during online execution .

the mysterious optimality of naive bayes : estimation of the probability in the system of `` classifiers ''
bayes classifiers are widely used currently for recognition , identification and knowledge discovery . the fields of application are , for example , image processing , medicine , chemistry ( qsar ) . but by mysterious way the naive bayes classifier usually gives a very nice and good presentation of a recognition . it can not be improved considerably by more complex models of bayes classifier . we demonstrate here a very nice and simple proof of the naive bayes classifier optimality , that can explain this interesting fact.the derivation in the current paper is based on arxiv : cs/0202020v1

probabilistic exploration in planning while learning
sequential decision tasks with incomplete information are characterized by the exploration problem ; namely the trade-off between further exploration for learning more about the environment and immediate exploitation of the accrued information for decision-making . within artificial intelligence , there has been an increasing interest in studying planning-while-learning algorithms for these decision tasks . in this paper we focus on the exploration problem in reinforcement learning and q-learning in particular . the existing exploration strategies for q-learning are of a heuristic nature and they exhibit limited scaleability in tasks with large ( or infinite ) state and action spaces . efficient experimentation is needed for resolving uncertainties when possible plans are compared ( i.e . exploration ) . the experimentation should be sufficient for selecting with statistical significance a locally optimal plan ( i.e . exploitation ) . for this purpose , we develop a probabilistic hill-climbing algorithm that uses a statistical selection procedure to decide how much exploration is needed for selecting a plan which is , with arbitrarily high probability , arbitrarily close to a locally optimal one . due to its generality the algorithm can be employed for the exploration strategy of robust q-learning . an experiment on a relatively complex control task shows that the proposed exploration strategy performs better than a typical exploration strategy .

learning bayesian network parameters with prior knowledge about context-specific qualitative influences
we present a method for learning the parameters of a bayesian network with prior knowledge about the signs of influences between variables . our method accommodates not just the standard signs , but provides for context-specific signs as well . we show how the various signs translate into order constraints on the network parameters and how isotonic regression can be used to compute order-constrained estimates from the available data . our experimental results show that taking prior knowledge about the signs of influences into account leads to an improved fit of the true distribution , especially when only a small sample of data is available . moreover , the computed estimates are guaranteed to be consistent with the specified signs , thereby resulting in a network that is more likely to be accepted by experts in its domain of application .

ups and downs : modeling the visual evolution of fashion trends with one-class collaborative filtering
building a successful recommender system depends on understanding both the dimensions of people 's preferences as well as their dynamics . in certain domains , such as fashion , modeling such preferences can be incredibly difficult , due to the need to simultaneously model the visual appearance of products as well as their evolution over time . the subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets . in this paper we build novel models for the one-class collaborative filtering setting , where our goal is to estimate users ' fashion-aware personalized ranking functions based on their past feedback . to uncover the complex and evolving visual factors that people consider when evaluating products , our method combines high-level visual features extracted from a deep convolutional neural network , users ' past feedback , as well as evolving trends within the community . experimentally we evaluate our method on two large real-world datasets from amazon.com , where we show it to outperform state-of-the-art personalized ranking measures , and also use it to visualize the high-level fashion trends across the 11-year span of our dataset .

fitness uniform selection to preserve genetic diversity
in evolutionary algorithms , the fitness of a population increases with time by mutating and recombining individuals and by a biased selection of more fit individuals . the right selection pressure is critical in ensuring sufficient optimization progress on the one hand and in preserving genetic diversity to be able to escape from local optima on the other . we propose a new selection scheme , which is uniform in the fitness values . it generates selection pressure towards sparsely populated fitness regions , not necessarily towards higher fitness , as is the case for all other selection schemes . we show that the new selection scheme can be much more effective than standard selection schemes .

the complete extensions do not form a complete semilattice
in his seminal paper that inaugurated abstract argumentation , dung proved that the set of complete extensions forms a complete semilattice with respect to set inclusion . in this note we demonstrate that this proof is incorrect with counterexamples . we then trace the error in the proof and explain why it arose . we then examine the implications for the grounded extension . [ reason for withdrawal continued ] page 4 , example 2 is not a counterexample to dung 1995 theorem 25 ( 3 ) . it was believed to be a counter-example because the author misunderstood `` glb '' to be set-theoretic intersection . but in this case , `` glb '' is defined to be other than set-theoretic intersection such that theorem 25 ( 3 ) is true . the author was motivated to fully understand the lattice-theoretic claims of dung 1995 in writing this note and was not aware that this issue is probably folklore ; the author bears full responsibility for this error .

applying the wizard-of-oz technique to multimodal human-robot dialogue
our overall program objective is to provide more natural ways for soldiers to interact and communicate with robots , much like how soldiers communicate with other soldiers today . we describe how the wizard-of-oz ( woz ) method can be applied to multimodal human-robot dialogue in a collaborative exploration task . while the woz method can help design robot behaviors , traditional approaches place the burden of decisions on a single wizard . in this work , we consider two wizards to stand in for robot navigation and dialogue management software components . the scenario used to elicit data is one in which a human-robot team is tasked with exploring an unknown environment : a human gives verbal instructions from a remote location and the robot follows them , clarifying possible misunderstandings as needed via dialogue . we found the division of labor between wizards to be workable , which holds promise for future software development .

a self-tuning firefly algorithm to tune the parameters of ant colony system ( acsfa )
ant colony system ( acs ) is a promising approach which has been widely used in problems such as travelling salesman problems ( tsp ) , job shop scheduling problems ( jsp ) and quadratic assignment problems ( qap ) . in its original implementation , parameters of the algorithm were selected by trial and error approach . over the last few years , novel approaches have been proposed on adapting the parameters of acs in improving its performance . the aim of this paper is to use a framework introduced for self-tuning optimization algorithms combined with the firefly algorithm ( fa ) to tune the parameters of the acs solving symmetric tsp problems . the fa optimizes the problem specific parameters of acs while the parameters of the fa are tuned by the selected framework itself . with this approach , the user neither has to work with the parameters of acs nor the parameters of fa . using common symmetric tsp problems we demonstrate that the framework fits well for the acs . a detailed statistical analysis further verifies the goodness of the new acs over the existing acs and also of the other techniques used to tune the parameters of acs .

every formula-based logic program has a least infinite-valued model
every definite logic program has as its meaning a least herbrand model with respect to the program-independent ordering `` set-inclusion '' . in the case of normal logic programs there do not exist least models in general . however , according to a recent approach by rondogiannis and wadge , who consider infinite-valued models , every normal logic program does have a least model with respect to a program-independent ordering . we show that this approach can be extended to formula-based logic programs ( i.e. , finite sets of rules of the form a\leftarrowf where a is an atom and f an arbitrary first-order formula ) . we construct for a given program p an interpretation m_p and show that it is the least of all models of p. keywords : logic programming , semantics of programs , negation-as-failure , infinite-valued logics , set theory

cheaper and better : selecting good workers for crowdsourcing
crowdsourcing provides a popular paradigm for data collection at scale . we study the problem of selecting subsets of workers from a given worker pool to maximize the accuracy under a budget constraint . one natural question is whether we should hire as many workers as the budget allows , or restrict on a small number of top-quality workers . by theoretically analyzing the error rate of a typical setting in crowdsourcing , we frame the worker selection problem into a combinatorial optimization problem and propose an algorithm to solve it efficiently . empirical results on both simulated and real-world datasets show that our algorithm is able to select a small number of high-quality workers , and performs as good as , sometimes even better than , the much larger crowds as the budget allows .

reasoning in complex environments with the selectscript declarative language
selectscript is an extendable , adaptable , and declarative domain-specific language aimed at information retrieval from simulation environments and robotic world models in an sql-like manner . in this work we have extended the language in two directions . first , we have implemented hierarchical queries ; second , we improve efficiency enabling manual design space exploration on different `` search '' strategies . we demonstrate the applicability of such extensions in two application problems ; the basic language concepts are explained by solving the classical problem of the towers of hanoi and then a common path planning problem in a complex 3d environment is implemented .

conditional similarity networks
what makes images similar ? to measure the similarity between images , they are typically embedded in a feature-vector space , in which their distance preserve the relative dissimilarity . however , when learning such similarity embeddings the simplifying assumption is commonly made that images are only compared to one unique measure of similarity . a main reason for this is that contradicting notions of similarities can not be captured in a single space . to address this shortcoming , we propose conditional similarity networks ( csns ) that learn embeddings differentiated into semantically distinct subspaces that capture the different notions of similarities . csns jointly learn a disentangled embedding where features for different similarities are encoded in separate dimensions as well as masks that select and reweight relevant dimensions to induce a subspace that encodes a specific similarity notion . we show that our approach learns interpretable image representations with visually relevant semantic subspaces . further , when evaluating on triplet questions from multiple similarity notions our model even outperforms the accuracy obtained by training individual specialized networks for each notion separately .

non-myopic learning in repeated stochastic games
in repeated stochastic games ( rsgs ) , an agent must quickly adapt to the behavior of previously unknown associates , who may themselves be learning . this machine-learning problem is particularly challenging due , in part , to the presence of multiple ( even infinite ) equilibria and inherently large strategy spaces . in this paper , we introduce a method to reduce the strategy space of two-player general-sum rsgs to a handful of expert strategies . this process , called mega , effectually reduces an rsg to a bandit problem . we show that the resulting strategy space preserves several important properties of the original rsg , thus enabling a learner to produce robust strategies within a reasonably small number of interactions . to better establish strengths and weaknesses of this approach , we empirically evaluate the resulting learning system against other algorithms in three different rsgs .

autonomous extracting a hierarchical structure of tasks in reinforcement learning and multi-task reinforcement learning
reinforcement learning ( rl ) , while often powerful , can suffer from slow learning speeds , particularly in high dimensional spaces . the autonomous decomposition of tasks and use of hierarchical methods hold the potential to significantly speed up learning in such domains . this paper proposes a novel practical method that can autonomously decompose tasks , by leveraging association rule mining , which discovers hidden relationship among entities in data mining . we introduce a novel method called arm-hstrl ( association rule mining to extract hierarchical structure of tasks in reinforcement learning ) . it extracts temporal and structural relationships of sub-goals in rl , and multi-task rl . in particular , it finds sub-goals and relationship among them . it is shown the significant efficiency and performance of the proposed method in two main topics of rl .

autonomous navigation by robust scan matching technique
for effective autonomous navigation , estimation of the pose of the robot is essential at every sampling time . for computing an accurate estimation , odometric error needs to be reduced with the help of data from external sensor . in this work , a technique has been developed for accurate pose estimation of mobile robot by using laser range data . the technique is robust to noisy data , which may contain considerable amount of outliers . a grey image is formed from laser range data and the key points from this image are extracted by harris corner detector . the matching of the key points from consecutive data sets have been done while outliers have been rejected by ransac method . robot state is measured by the correspondence between the two sets of keypoints . finally , optimal robot state is estimated by extended kalman filter . the technique has been applied to an operational robot in the laboratory environment to show the robustness of the technique in presence of noisy sensor data . the performance of this new technique has been compared with that of conventional icp method . through this method , effective and accurate navigation has been achieved even in presence of substantial noise in the sensor data at the cost of a small amount of additional computational complexity .

rasa : open source language understanding and dialogue management
we introduce a pair of tools , rasa nlu and rasa core , which are open source python libraries for building conversational software . their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers . in terms of design philosophy , we aim for ease of use , and bootstrapping from minimal ( or no ) initial training data . both packages are extensively documented and ship with a comprehensive suite of tests . the code is available at https : //github.com/rasahq/

c3a : a cognitive collaborative control architecture for an intelligent wheelchair
retention of residual skills for persons who partially lose their cognitive or physical ability is of utmost importance . research is focused on developing systems that provide need-based assistance for retention of such residual skills . this paper describes a novel cognitive collaborative control architecture c3a , designed to address the challenges of developing need- based assistance for wheelchair navigation . organization of c3a is detailed and results from simulation of the proposed architecture is presented . for simulation of our proposed architecture , we have used ros ( robot operating system ) as a control framework and a 3d robotic simulator called usarsim ( unified system for automation and robot simulation ) .

geometric gan
generative adversarial nets ( gans ) represent an important milestone for effective generative models , which has inspired numerous variants seemingly different from each other . one of the main contributions of this paper is to reveal a unified geometric structure in gan and its variants . specifically , we show that the adversarial generative model training can be decomposed into three geometric steps : separating hyperplane search , discriminator parameter update away from the separating hyperplane , and the generator update along the normal vector direction of the separating hyperplane . this geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric gan using svm separating hyperplane that maximizes the margin . our theoretical analysis shows that the geometric gan converges to a nash equilibrium between the discriminator and generator . in addition , extensive numerical results show that the superior performance of geometric gan .

communicative capital for prosthetic agents
this work presents an overarching perspective on the role that machine intelligence can play in enhancing human abilities , especially those that have been diminished due to injury or illness . as a primary contribution , we develop the hypothesis that assistive devices , and specifically artificial arms and hands , can and should be viewed as agents in order for us to most effectively improve their collaboration with their human users . we believe that increased agency will enable more powerful interactions between human users and next generation prosthetic devices , especially when the sensorimotor space of the prosthetic technology greatly exceeds the conventional control and communication channels available to a prosthetic user . to more concretely examine an agency-based view on prosthetic devices , we propose a new schema for interpreting the capacity of a human-machine collaboration as a function of both the human 's and machine 's degrees of agency . we then introduce the idea of communicative capital as a way of thinking about the communication resources developed by a human and a machine during their ongoing interaction . using this schema of agency and capacity , we examine the benefits and disadvantages of increasing the agency of a prosthetic limb . to do so , we present an analysis of examples from the literature where building communicative capital has enabled a progression of fruitful , task-directed interactions between prostheses and their human users . we then describe further work that is needed to concretely evaluate the hypothesis that prostheses are best thought of as agents . the agent-based viewpoint developed in this article significantly extends current thinking on how best to support the natural , functional use of increasingly complex prosthetic enhancements , and opens the door for more powerful interactions between humans and their assistive technologies .

estimating uncertain spatial relationships in robotics
in this paper , we describe a representation for spatial information , called the stochastic map , and associated procedures for building it , reading information from it , and revising it incrementally as new information is obtained . the map contains the estimates of relationships among objects in the map , and their uncertainties , given all the available information . the procedures provide a general solution to the problem of estimating uncertain relative spatial relationships . the estimates are probabilistic in nature , an advance over the previous , very conservative , worst-case approaches to the problem . finally , the procedures are developed in the context of state-estimation and filtering theory , which provides a solid basis for numerous extensions .

enhancements to acl2 in versions 6.2 , 6.3 , and 6.4
we report on improvements to acl2 made since the 2013 acl2 workshop .

to fall or not to fall : a visual approach to physical stability prediction
understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel object and their configurations . developmental psychology has shown that such skills are acquired by infants from observations at a very early stage . in this paper , we contrast a more traditional approach of taking a model-based route with explicit 3d representations and physical simulation by an end-to-end approach that directly predicts stability and related quantities from appearance . we ask the question if and to what extent and quality such a skill can directly be acquired in a data-driven way bypassing the need for an explicit simulation . we present a learning-based approach based on simulated data that predicts stability of towers comprised of wooden blocks under different conditions and quantities related to the potential fall of the towers . the evaluation is carried out on synthetic data and compared to human judgments on the same stimuli .

an intelligent approach for negotiating between chains in supply chain management systems
holding commercial negotiations and selecting the best supplier in supply chain management systems are among weaknesses of producers in production process . therefore , applying intelligent systems may have an effective role in increased speed and improved quality in the selections .this paper introduces a system which tries to trade using multi-agents systems and holding negotiations between any agents . in this system , an intelligent agent is considered for each segment of chains which it tries to send order and receive the response with attendance in negotiation medium and communication with other agents .this paper introduces how to communicate between agents , characteristics of multi-agent and standard registration medium of each agent in the environment . jade ( java application development environment ) was used for implementation and simulation of agents cooperation .

text matching as image recognition
matching two texts is a fundamental problem in many natural language processing tasks . an effective way is to extract meaningful matching patterns from words , phrases , and sentences to produce the matching score . inspired by the success of convolutional neural network in image recognition , where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners , we propose to model text matching as the problem of image recognition . firstly , a matching matrix whose entries represent the similarities between words is constructed and viewed as an image . then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way . we show that by resembling the compositional hierarchies of patterns in image recognition , our model can successfully identify salient signals such as n-gram and n-term matchings . experimental results demonstrate its superiority against the baselines .

qualitative order of magnitude energy-flow-based failure modes and effects analysis
this paper presents a structured power and energy-flow-based qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow . the modelling is split into two parts . power flow is a global phenomenon and is therefore naturally represented and analysed by a network comprised of the relevant structural elements from the components of a system . the power flow analysis is a platform for higher-level behaviour prediction of energy related aspects using local component behaviour models to capture a state-based representation with a global time . the primary application is failure modes and effects analysis ( fmea ) and a form of exaggeration reasoning is used , combined with an order of magnitude representation to derive the worst case failure modes . the novel aspects of the work are an order of magnitude ( om ) qualitative network analyser to represent any power domain and topology , including multiple power sources , a feature that was not required for earlier specialised electrical versions of the approach . secondly , the representation of generalised energy related behaviour as state-based local models is presented as a modelling strategy that can be more vivid and intuitive for a range of topologically complex applications than qualitative equation-based representations.the two-level modelling strategy allows the broad system behaviour coverage of qualitative simulation to be exploited for the fmea task , while limiting the difficulties of qualitative ambiguity explanation that can arise from abstracted numerical models . we have used the method to support an automated fmea system with examples of an aircraft fuel system and domestic a heating system discussed in this paper .

explanations based on the missing : towards contrastive explanations with pertinent negatives
in this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network . given an input we find what should be minimally and sufficiently present ( viz . important object pixels in an image ) to justify its classification and analogously what should be minimally and necessarily \emph { absent } ( viz . certain background pixels ) . we argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology . what is minimally but critically \emph { absent } is an important part of an explanation , which to the best of our knowledge , has not been touched upon by current explanation methods that attempt to explain predictions of neural networks . we validate our approach on three real datasets obtained from diverse domains ; namely , a handwritten digits dataset mnist , a large procurement fraud dataset and an fmri brain imaging dataset . in all three cases , we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate .

intelligent subset selection of power generators for economic dispatch
sustainable and economical generation of electrical power is an essential and mandatory component of infrastructure in today 's world . optimal generation ( generator subset selection ) of power requires a careful evaluation of various factors like type of source , generation , transmission & storage capacities , congestion among others which makes this a difficult task . we created a grid to simulate various conditions including stimuli like generator supply , weather and load demand using siemens pss/e software and this data is trained using deep learning methods and subsequently tested . the results are highly encouraging . as per our knowledge , this is the first paper to propose a working and scalable deep learning model for this problem .

stratified knowledge bases as interpretable probabilistic models ( extended abstract )
in this paper , we advocate the use of stratified logical theories for representing probabilistic models . we argue that such encodings can be more interpretable than those obtained in existing frameworks such as markov logic networks . among others , this allows for the use of domain experts to improve learned models by directly removing , adding , or modifying logical formulas .

a self-organizing system for urban traffic control based on predictive interval microscopic model
this paper introduces a self-organizing traffic signal system for an urban road network . the key elements of this system are agents that control traffic signals at intersections . each agent uses an interval microscopic traffic model to predict effects of its possible control actions in a short time horizon . the executed control action is selected on the basis of predicted delay intervals . since the prediction results are represented by intervals , the agents can recognize and suspend those control actions , whose positive effect on the performance of traffic control is uncertain . evaluation of the proposed traffic control system was performed in a simulation environment . the simulation experiments have shown that the proposed approach results in an improved performance , particularly for non-uniform traffic streams .

spatiotemporal articulated models for dynamic slam
we propose an online spatiotemporal articulation model estimation framework that estimates both articulated structure as well as a temporal prediction model solely using passive observations . the resulting model can predict future mo- tions of an articulated object with high confidence because of the spatial and temporal structure . we demonstrate the effectiveness of the predictive model by incorporating it within a standard simultaneous localization and mapping ( slam ) pipeline for mapping and robot localization in previously unexplored dynamic environments . our method is able to localize the robot and map a dynamic scene by explaining the observed motion in the world . we demonstrate the effectiveness of the proposed framework for both simulated and real-world dynamic environments .

issues in exploiting germanet as a resource in real applications
this paper reports about experiments with germanet as a resource within domain specific document analysis . the main question to be answered is : how is the coverage of germanet in a specific domain ? we report about results of a field test of germanet for analyses of autopsy protocols and present a sketch about the integration of germanet inside xdoc . our remarks will contribute to a germanet user 's wish list .

a framework for comparing uncertain inference systems to probability
several different uncertain inference systems ( uiss ) have been developed for representing uncertainty in rule-based expert systems . some of these , such as mycin 's certainty factors , prospector , and bayes ' networks were designed as approximations to probability , and others , such as fuzzy set theory and dempstershafer belief functions were not . how different are these uiss in practice , and does it matter which you use ? when combining and propagating uncertain information , each uis must , at least by implication , make certain assumptions about correlations not explicily specified . the maximum entropy principle with minimum cross-entropy updating , provides a way of making assumptions about the missing specification that minimizes the additional information assumed , and thus offers a standard against which the other uiss can be compared . we describe a framework for the experimental comparison of the performance of different uiss , and provide some illustrative results .

relaxation in graph coloring and satisfiability problems
using t=0 monte carlo simulation , we study the relaxation of graph coloring ( k-col ) and satisfiability ( k-sat ) , two hard problems that have recently been shown to possess a phase transition in solvability as a parameter is varied . a change from exponentially fast to power law relaxation , and a transition to freezing behavior are found . these changes take place for smaller values of the parameter than the solvability transition . results for the coloring problem for colorable and clustered graphs and for the fraction of persistent spins for satisfiability are also presented .

loss surfaces , mode connectivity , and fast ensembling of dnns
the loss functions of deep neural networks are complex and their geometric properties are not well understood . we show that the optima of these complex loss functions are in fact connected by a simple polygonal chain with only one bend , over which training and test accuracy are nearly constant . we introduce a training procedure to discover these high-accuracy pathways between modes . inspired by this new geometric insight , we also propose a new ensembling method entitled fast geometric ensembling ( fge ) . using fge we can train high-performing ensembles in the time required to train a single model . we achieve improved performance compared to the recent state-of-the-art snapshot ensembles , on cifar-10 and cifar-100 , using state-of-the-art deep residual networks . on imagenet we improve the top-1 error-rate of a pre-trained resnet by 0.56 % by running fge for just 5 epochs .

argumentative inference in uncertain and inconsistent knowledge bases
this paper presents and discusses several methods for reasoning from inconsistent knowledge bases . a so-called argumentative-consequence relation taking into account the existence of consistent arguments in favor of a conclusion and the absence of consistent arguments in favor of its contrary , is particularly investigated . flat knowledge bases , i.e . without any priority between their elements , as well as prioritized ones where some elements are considered as more strongly entrenched than others are studied under different consequence relations . lastly a paraconsistent-like treatment of prioritized knowledge bases is proposed , where both the level of entrenchment and the level of paraconsistency attached to a formula are propagated . the priority levels are handled in the framework of possibility theory .

smodels : a system for answer set programming
the smodels system implements the stable model semantics for normal logic programs . it handles a subclass of programs which contain no function symbols and are domain-restricted but supports extensions including built-in functions as well as cardinality and weight constraints . on top of this core engine more involved systems can be built . as an example , we have implemented total and partial stable model computation for disjunctive logic programs . an interesting application method is based on answer set programming , i.e. , encoding an application problem as a set of rules so that its solutions are captured by the stable models of the rules . smodels has been applied to a number of areas including planning , model checking , reachability analysis , product configuration , dynamic constraint satisfaction , and feature interaction .

a causal and-or graph model for visibility fluent reasoning in human-object interactions
tracking humans that are interacting with the other subjects or environment remains unsolved in visual tracking , because the visibility of the human of interests in videos is unknown and might vary over times . in particular , it is still difficult for state-of-the-art human trackers to recover complete human trajectories in crowded scenes with frequent human interactions . in this work , we consider the visibility status of a subject as a fluent variable , whose changes are mostly attributed to the subject 's interactions with the surrounding , e.g. , crossing behind another objects , entering a building , or getting into a vehicle , etc . we introduce a causal and-or graph ( c-aog ) to represent the causal-effect relations between an object 's visibility fluents and its activities , and develop a probabilistic graph model to jointly reason the visibility fluent change ( e.g. , from visible to invisible ) and track humans in videos . we formulate the above joint task as an iterative search of feasible causal graph structure that enables fast search algorithm , e.g. , dynamic programming method . we apply the proposed method on challenging video sequences to evaluate its capabilities of estimating visibility fluent changes of subjects and tracking subjects of interests over time . results with comparisons demonstrated that our method clearly outperforms the alternative trackers and can recover complete trajectories of humans in complicated scenarios with frequent human interactions .

non-sentential utterances in dialogue : experiments in classification and interpretation
non-sentential utterances ( nsus ) are utterances that lack a complete sentential form but whose meaning can be inferred from the dialogue context , such as `` ok '' , `` where ? `` , `` probably at his apartment '' . the interpretation of non-sentential utterances is an important problem in computational linguistics since they constitute a frequent phenomena in dialogue and they are intrinsically context-dependent . the interpretation of nsus is the task of retrieving their full semantic content from their form and the dialogue context . the first half of this thesis is devoted to the nsu classification task . our work builds upon fern\'andez et al . ( 2007 ) which present a series of machine-learning experiments on the classification of nsus . we extended their approach with a combination of new features and semi-supervised learning techniques . the empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance . the consecutive , yet independent , problem is how to infer an appropriate semantic representation of such nsus on the basis of the dialogue context . fern\'andez ( 2006 ) formalizes this task in terms of `` resolution rules '' built on top of the type theory with records ( ttr ) . our work is focused on the reimplementation of the resolution rules from fern\'andez ( 2006 ) with a probabilistic account of the dialogue state . the probabilistic rules formalism lison ( 2014 ) is particularly suited for this task because , similarly to the framework developed by ginzburg ( 2012 ) and fern\'andez ( 2006 ) , it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation . however , the probabilistic rules can also encode probabilistic knowledge , thereby providing a principled account of ambiguities in the nsu resolution process .

decentralized constraint satisfaction
we show that several important resource allocation problems in wireless networks fit within the common framework of constraint satisfaction problems ( csps ) . inspired by the requirements of these applications , where variables are located at distinct network devices that may not be able to communicate but may interfere , we define natural criteria that a csp solver must possess in order to be practical . we term these algorithms decentralized csp solvers . the best known csp solvers were designed for centralized problems and do not meet these criteria . we introduce a stochastic decentralized csp solver and prove that it will find a solution in almost surely finite time , should one exist , also showing it has many practically desirable properties . we benchmark the algorithm 's performance on a well-studied class of csps , random k-sat , illustrating that the time the algorithm takes to find a satisfying assignment is competitive with stochastic centralized solvers on problems with order a thousand variables despite its decentralized nature . we demonstrate the solver 's practical utility for the problems that motivated its introduction by using it to find a non-interfering channel allocation for a network formed from data from downtown manhattan .

incremental dynamic construction of layered polytree networks
certain classes of problems , including perceptual data understanding , robotics , discovery , and learning , can be represented as incremental , dynamically constructed belief networks . these automatically constructed networks can be dynamically extended and modified as evidence of new individuals becomes available . the main result of this paper is the incremental extension of the singly connected polytree network in such a way that the network retains its singly connected polytree structure after the changes . the algorithm is deterministic and is guaranteed to have a complexity of single node addition that is at most of order proportional to the number of nodes ( or size ) of the network . additional speed-up can be achieved by maintaining the path information . despite its incremental and dynamic nature , the algorithm can also be used for probabilistic inference in belief networks in a fashion similar to other exact inference algorithms .

enumerating markov equivalence classes of acyclic digraph models
graphical markov models determined by acyclic digraphs ( adgs ) , also called directed acyclic graphs ( dags ) , are widely studied in statistics , computer science ( as bayesian networks ) , operations research ( as influence diagrams ) , and many related fields . because different adgs may determine the same markov equivalence class , it long has been of interest to determine the efficiency gained in model specification and search by working directly with markov equivalence classes of adgs rather than with adgs themselves . a computer program was written to enumerate the equivalence classes of adg models as specified by pearl & verma 's equivalence criterion . the program counted equivalence classes for models up to and including 10 vertices . the ratio of number of classes to adgs appears to approach an asymptote of about 0.267. classes were analyzed according to number of edges and class size . by edges , the distribution of number of classes approaches a gaussian shape . by class size , classes of size 1 are most common , with the proportions for larger sizes initially decreasing but then following a more irregular pattern . the maximum number of classes generated by any undirected graph was found to increase approximately factorially . the program also includes a new variation of orderly algorithm for generating undirected graphs .

a diffusion and clustering-based approach for finding coherent motions and understanding crowd scenes
this paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding : semantic region detection and recurrent activity mining . it processes input motion fields ( e.g. , optical flow fields ) and produces a coherent motion filed , named as thermal energy field . the thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them . we further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions . these semantic regions can be used to recognize pre-defined activities in crowd scenes . finally , we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions . experiments on various videos demonstrate the effectiveness of our approach .

extracting hierarchies of search tasks & subtasks via a bayesian nonparametric approach
a significant amount of search queries originate from some real world information need or tasks . in order to improve the search experience of the end users , it is important to have accurate representations of tasks . as a result , significant amount of research has been devoted to extracting proper representations of tasks in order to enable search systems to help users complete their tasks , as well as providing the end user with better query suggestions , for better recommendations , for satisfaction prediction , and for improved personalization in terms of tasks . most existing task extraction methodologies focus on representing tasks as flat structures . however , tasks often tend to have multiple subtasks associated with them and a more naturalistic representation of tasks would be in terms of a hierarchy , where each task can be composed of multiple ( sub ) tasks . to this end , we propose an efficient bayesian nonparametric model for extracting hierarchies of such tasks \ & subtasks . we evaluate our method based on real world query log data both through quantitative and crowdsourced experiments and highlight the importance of considering task/subtask hierarchies .

crowd flow prediction by deep spatio-temporal transfer learning
crowd flow prediction is a fundamental urban computing problem . recently , deep learning has been successfully applied to solve this problem , but it relies on rich historical data . in reality , many cities may suffer from data scarcity issue when their targeted service or infrastructure is new . to overcome this issue , this paper proposes a novel deep spatio-temporal transfer learning framework , called regiontrans , which can predict future crowd flow in a data-scarce ( target ) city by transferring knowledge from a data-rich ( source ) city . leveraging social network check-ins , regiontrans first links a region in the target city to certain regions in the source city , expecting that these inter-city region pairs will share similar crowd flow dynamics . then , we propose a deep spatio-temporal neural network structure , in which a hidden layer is dedicated to keeping the region representation . a source city model is then trained on its rich historical data with this network structure . finally , we propose a region-based cross-city transfer learning algorithm to learn the target city model from the source city model by minimizing the hidden representation discrepancy between the inter-city region pairs previously linked by check-ins . with experiments on real crowd flow , regiontrans can outperform state-of-the-arts by reducing up to 10.7 % prediction error .

fair division via social comparison
in the classical cake cutting problem , a resource must be divided among agents with different utilities so that each agent believes they have received a fair share of the resource relative to the other agents . we introduce a variant of the problem in which we model an underlying social network on the agents with a graph , and agents only evaluate their shares relative to their neighbors ' in the network . this formulation captures many situations in which it is unrealistic to assume a global view , and also exposes interesting phenomena in the original problem . specifically , we say an allocation is locally envy-free if no agent envies a neighbor 's allocation and locally proportional if each agent values her own allocation as much as the average value of her neighbor 's allocations , with the former implying the latter . while global envy-freeness implies local envy-freeness , global proportionality does not imply local proportionality , or vice versa . a general result is that for any two distinct graphs on the same set of nodes and an allocation , there exists a set of valuation functions such that the allocation is locally proportional on one but not the other . we fully characterize the set of graphs for which an oblivious single-cutter protocol -- a protocol that uses a single agent to cut the cake into pieces -- admits a bounded protocol with $ o ( n^2 ) $ query complexity for locally envy-free allocations in the robertson-webb model . we also consider the price of envy-freeness , which compares the total utility of an optimal allocation to the best utility of an allocation that is envy-free . we show that a lower bound of $ \omega ( \sqrt { n } ) $ on the price of envy-freeness for global allocations in fact holds for local envy-freeness in any connected undirected graph . thus , sparse graphs surprisingly do not provide more flexibility with respect to the quality of envy-free allocations .

path planning for robotic mobile fulfillment systems
this paper presents a collection of path planning algorithms for real-time movement of multiple robots across a robotic mobile fulfillment system ( rmfs ) . robots are assigned to move storage units to pickers at working stations instead of requiring pickers to go to the storage area . path planning algorithms aim to find paths for the robots to fulfill the requests without collisions or deadlocks . the state-of-the-art path planning algorithms , including whca* , far , bcp , od & id and cbs , were adapted to suit path planning in rmfs and integrated within a simulation tool to guide the robots from their starting points to their destinations during the storage and retrieval processes . ten different layouts with a variety of numbers of robots , floors , pods , stations and the sizes of storage areas were considered in the simulation study . performance metrics of throughput , path length and search time were monitored . simulation results demonstrate the best algorithm based on each performance metric .

computability logic : a formal theory of interaction
computability logic is a formal theory of ( interactive ) computability in the same sense as classical logic is a formal theory of truth . this approach was initiated very recently in `` introduction to computability logic '' ( annals of pure and applied logic 123 ( 2003 ) , pp.1-99 ) . the present paper reintroduces computability logic in a more compact and less technical way . it is written in a semitutorial style with a general computer science , logic or mathematics audience in mind . an internet source on the subject is available at http : //www.cis.upenn.edu/~giorgi/cl.html , and additional material at http : //www.csc.villanova.edu/~japaridz/cl/gsoll.html .

go game formal revealing by ising model
go gaming is a struggle for territory control between rival , black and white , stones on a board . we model the go dynamics in a game by means of the ising model whose interaction coefficients reflect essential rules and tactics employed in go to build long-term strategies . at any step of the game , the energy functional of the model provides the control degree ( strength ) of a player over the board . a close fit between predictions of the model with actual games is obtained .

an evolutionary algorithm to learn sparql queries for source-target-pairs : finding patterns for human associations in dbpedia
efficient usage of the knowledge provided by the linked data community is often hindered by the need for domain experts to formulate the right sparql queries to answer questions . for new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the sparql query . in this work we present an evolutionary algorithm to help with this challenging task . given a training list of source-target node-pair examples our algorithm can learn patterns ( sparql queries ) from a sparql endpoint . the learned patterns can be visualised to form the basis for further investigation , or they can be used to predict target nodes for new source nodes . amongst others , we apply our algorithm to a dataset of several hundred human associations ( such as `` circle - square '' ) to find patterns for them in dbpedia . we show the scalability of the algorithm by running it against a sparql endpoint loaded with > 7.9 billion triples . further , we use the resulting sparql queries to mimic human associations with a mean average precision ( map ) of 39.9 % and a recall @ 10 of 63.9 % .

towards the self-constructive brain : emergence of adaptive behavior
adaptive behavior is mainly the result of adaptive brains . we go a step beyond and claim that the brain does not only adapt to its surrounding reality but rather , it builds itself up to constructs its own reality . that is , rather than just trying to passively understand its environment , the brain is the architect of its own reality in an active process where its internal models of the external world frame how its new interactions with the environment are assimilated . these internal models represent relevant predictive patterns of interaction all over the different brain structures : perceptual , sensorimotor , motor , etc . the emergence of adaptive behavior arises from this self-constructive nature of the brain , based on the following principles of organization : self-experimental , self- growing , and self-repairing . self-experimental , since to ensure survival , the self-constructive brain ( scb ) is an active machine capable of performing experiments of its own interactions with the environment by mental simulation . self-growing , since it dynamically and incrementally constructs internal structures in order to build a model of the world as it gathers statistics from its interactions with the environment . self-repairing , since to survive the scb must also be robust and capable of finding ways to repair parts of previously working structures and hence re-construct a previous relevant pattern of activity .

reluplex : an efficient smt solver for verifying deep neural networks
deep neural networks have emerged as a widely used and effective means for tackling complex , real-world problems . however , a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior . we present a novel , scalable , and efficient technique for verifying properties of deep neural networks ( or providing counter-examples ) . the technique is based on the simplex method , extended to handle the non-convex rectified linear unit ( relu ) activation function , which is a crucial ingredient in many modern neural networks . the verification procedure tackles neural networks as a whole , without making any simplifying assumptions . we evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft ( acas xu ) . results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods .

low-cost scene modeling using a density function improves segmentation performance
we propose a low cost and effective way to combine a free simulation software and free cad models for modeling human-object interaction in order to improve human & object segmentation . it is intended for research scenarios related to safe human-robot collaboration ( shrc ) and interaction ( shri ) in the industrial domain . the task of human and object modeling has been used for detecting activity , and for inferring and predicting actions , different from those works , we do human and object modeling in order to learn interactions in rgb-d data for improving segmentation . for this purpose , we define a novel density function to model a three dimensional ( 3d ) scene in a virtual environment ( vrep ) . this density function takes into account various possible configurations of human-object and object-object relationships and interactions governed by their affordances . using this function , we synthesize a large , realistic and highly varied synthetic rgb-d dataset that we use for training . we train a random forest classifier , and the pixelwise predictions obtained is integrated as a unary term in a pairwise conditional random fields ( crf ) . our evaluation shows that modeling these interactions improves segmentation performance by ~7\ % in mean average precision and recall over state-of-the-art methods that ignore these interactions in real-world data . our approach is computationally efficient , robust and can run real-time on consumer hardware .

how is non-knowledge represented in economic theory ?
in this article , we address the question of how non-knowledge about future events that influence economic agents ' decisions in choice settings has been formally represented in economic theory up to date . to position our discussion within the ongoing debate on uncertainty , we provide a brief review of historical developments in economic theory and decision theory on the description of economic agents ' choice behaviour under conditions of uncertainty , understood as either ( i ) ambiguity , or ( ii ) unawareness . accordingly , we identify and discuss two approaches to the formalisation of non-knowledge : one based on decision-making in the context of a state space representing the exogenous world , as in savage 's axiomatisation and some successor concepts ( ambiguity as situations with unknown probabilities ) , and one based on decision-making over a set of menus of potential future opportunities , providing the possibility of derivation of agents ' subjective state spaces ( unawareness as situation with imperfect subjective knowledge of all future events possible ) . we also discuss impeding challenges of the formalisation of non-knowledge .

finding optimal bayesian networks
in this paper , we derive optimality results for greedy bayesian-network search algorithms that perform single-edge modifications at each step and use asymptotically consistent scoring criteria . our results extend those of meek ( 1997 ) and chickering ( 2002 ) , who demonstrate that in the limit of large datasets , if the generative distribution is perfect with respect to a dag defined over the observable variables , such search algorithms will identify this optimal ( i.e . generative ) dag model . we relax their assumption about the generative distribution , and assume only that this distribution satisfies the { em composition property } over the observable variables , which is a more realistic assumption for real domains . under this assumption , we guarantee that the search algorithms identify an { em inclusion-optimal } model ; that is , a model that ( 1 ) contains the generative distribution and ( 2 ) has no sub-model that contains this distribution . in addition , we show that the composition property is guaranteed to hold whenever the dependence relationships in the generative distribution can be characterized by paths between singleton elements in some generative graphical model ( e.g . a dag , a chain graph , or a markov network ) even when the generative model includes unobserved variables , and even when the observed data is subject to selection bias .

generating factoid questions with recurrent neural networks : the 30m factoid question-answer corpus
over the past decade , large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances . however , to this date , there are no large-scale question-answer corpora available . in this paper we present the 30m factoid question-answer corpus , an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base freebase to transduce facts into natural language questions . the produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics , including well-established machine translation and sentence similarity metrics . across all evaluation criteria the question-generation model outperforms the competing template-based baseline . furthermore , when presented to human evaluators , the generated questions appear comparable in quality to real human-generated questions .

linear temporal logic and propositional schemata , back and forth ( extended version )
this paper relates the well-known linear temporal logic with the logic of propositional schemata introduced by the authors . we prove that ltl is equivalent to a class of schemata in the sense that polynomial-time reductions exist from one logic to the other . some consequences about complexity are given . we report about first experiments and the consequences about possible improvements in existing implementations are analyzed .

e-generalization using grammars
we extend the notion of anti-unification to cover equational theories and present a method based on regular tree grammars to compute a finite representation of e-generalization sets . we present a framework to combine inductive logic programming and e-generalization that includes an extension of plotkin 's lgg theorem to the equational case . we demonstrate the potential power of e-generalization by three example applications : computation of suggestions for auxiliary lemmas in equational inductive proofs , computation of construction laws for given term sequences , and learning of screen editor command sequences .

a comparative study between fuzzy clustering algorithm and hard clustering algorithm
data clustering is an important area of data mining . this is an unsupervised study where data of similar types are put into one cluster while data of another types are put into different cluster . fuzzy c means is a very important clustering technique based on fuzzy logic . also we have some hard clustering techniques available like k-means among the popular ones . in this paper a comparative study is done between fuzzy clustering algorithm and hard clustering algorithm

a new intelligence based approach for computer-aided diagnosis of dengue fever
identification of the influential clinical symptoms and laboratory features that help in the diagnosis of dengue fever in early phase of the illness would aid in designing effective public health management and virological surveillance strategies . keeping this as our main objective we develop in this paper , a new computational intelligence based methodology that predicts the diagnosis in real time , minimizing the number of false positives and false negatives . our methodology consists of three major components ( i ) a novel missing value imputation procedure that can be applied on any data set consisting of categorical ( nominal ) and/or numeric ( real or integer ) ( ii ) a wrapper based features selection method with genetic search for extracting a subset of most influential symptoms that can diagnose the illness and ( iii ) an alternating decision tree method that employs boosting for generating highly accurate decision rules . the predictive models developed using our methodology are found to be more accurate than the state-of-the-art methodologies used in the diagnosis of the dengue fever .

mining knowledge in astrophysical massive data sets
modern scientific data mainly consist of huge datasets gathered by a very large number of techniques and stored in very diversified and often incompatible data repositories . more in general , in the e-science environment , it is considered as a critical and urgent requirement to integrate services across distributed , heterogeneous , dynamic `` virtual organizations '' formed by different resources within a single enterprise . in the last decade , astronomy has become an immensely data rich field due to the evolution of detectors ( plates to digital to mosaics ) , telescopes and space instruments . the virtual observatory approach consists into the federation under common standards of all astronomical archives available worldwide , as well as data analysis , data mining and data exploration applications . the main drive behind such effort being that once the infrastructure will be completed , it will allow a new type of multi-wavelength , multi-epoch science which can only be barely imagined . data mining , or knowledge discovery in databases , while being the main methodology to extract the scientific information contained in such mds ( massive data sets ) , poses crucial problems since it has to orchestrate complex problems posed by transparent access to different computing environments , scalability of algorithms , reusability of resources , etc . in the present paper we summarize the present status of the mds in the virtual observatory and what is currently done and planned to bring advanced data mining methodologies in the case of the dame ( data mining & exploration ) project .

the max $ k $ -armed bandit : pac lower bounds and efficient algorithms
we consider the max $ k $ -armed bandit problem , where a learning agent is faced with several stochastic arms , each a source of i.i.d . rewards of unknown distribution . at each time step the agent chooses an arm , and observes the reward of the obtained sample . each sample is considered here as a separate item with the reward designating its value , and the goal is to find an item with the highest possible value . our basic assumption is a known lower bound on the { \em tail function } of the reward distributions . under the pac framework , we provide a lower bound on the sample complexity of any $ ( \epsilon , \delta ) $ -correct algorithm , and propose an algorithm that attains this bound up to logarithmic factors . we analyze the robustness of the proposed algorithm and in addition , we compare the performance of this algorithm to the variant in which the arms are not distinguishable by the agent and are chosen randomly at each stage . interestingly , when the maximal rewards of the arms happen to be similar , the latter approach may provide better performance .

fast predictive simple geodesic regression
deformable image registration and regression are important tasks in medical image analysis . however , they are computationally expensive , especially when analyzing large-scale datasets that contain thousands of images . hence , cluster computing is typically used , making the approaches dependent on such computational infrastructure . even larger computational resources are required as study sizes increase . this limits the use of deformable image registration and regression for clinical applications and as component algorithms for other image analysis approaches . we therefore propose using a fast predictive approach to perform image registrations . in particular , we employ these fast registration predictions to approximate a simplified geodesic regression model to capture longitudinal brain changes . the resulting method is orders of magnitude faster than the standard optimization-based regression model and hence facilitates large-scale analysis on a single graphics processing unit ( gpu ) . we evaluate our results on 3d brain magnetic resonance images ( mri ) from the adni datasets .

kiwi : a scalable subspace clustering algorithm for gene expression analysis
subspace clustering has gained increasing popularity in the analysis of gene expression data . among subspace cluster models , the recently introduced order-preserving sub-matrix ( opsm ) has demonstrated high promise . an opsm , essentially a pattern-based subspace cluster , is a subset of rows and columns in a data matrix for which all the rows induce the same linear ordering of columns . existing opsm discovery methods do not scale well to increasingly large expression datasets . in particular , twig clusters having few genes and many experiments incur explosive computational costs and are completely pruned off by existing methods . however , it is of particular interest to determine small groups of genes that are tightly coregulated across many conditions . in this paper , we present kiwi , an opsm subspace clustering algorithm that is scalable to massive datasets , capable of discovering twig clusters and identifying negative as well as positive correlations . we extensively validate kiwi using relevant biological datasets and show that kiwi correctly assigns redundant probes to the same cluster , groups experiments with common clinical annotations , differentiates real promoter sequences from negative control sequences , and shows good association with cis-regulatory motif predictions .

expertbayes : automatically refining manually built bayesian networks
bayesian network structures are usually built using only the data and starting from an empty network or from a naive bayes structure . very often , in some domains , like medicine , a prior structure knowledge is already known . this structure can be automatically or manually refined in search for better performance models . in this work , we take bayesian networks built by specialists and show that minor perturbations to this original network can yield better classifiers with a very small computational cost , while maintaining most of the intended meaning of the original model .

design of automatically adaptable web wrappers
nowadays , the huge amount of information distributed through the web motivates studying techniques to be adopted in order to extract relevant data in an efficient and reliable way . both academia and enterprises developed several approaches of web data extraction , for example using techniques of artificial intelligence or machine learning . some commonly adopted procedures , namely wrappers , ensure a high degree of precision of information extracted from web pages , and , at the same time , have to prove robustness in order not to compromise quality and reliability of data themselves . in this paper we focus on some experimental aspects related to the robustness of the data extraction process and the possibility of automatically adapting wrappers . we discuss the implementation of algorithms for finding similarities between two different version of a web page , in order to handle modifications , avoiding the failure of data extraction tasks and ensuring reliability of information extracted . our purpose is to evaluate performances , advantages and draw-backs of our novel system of automatic wrapper adaptation .

evolutionary computation plus dynamic programming for the bi-objective travelling thief problem
this research proposes a novel indicator-based hybrid evolutionary approach that combines approximate and exact algorithms . we apply it to a new bi-criteria formulation of the travelling thief problem , which is known to the evolutionary computation community as a benchmark multi-component optimisation problem that interconnects two classical np-hard problems : the travelling salesman problem and the 0-1 knapsack problem . our approach employs the exact dynamic programming algorithm for the underlying packing-while-travelling ( pwt ) problem as a subroutine within a bi-objective evolutionary algorithm . this design takes advantage of the data extracted from pareto fronts generated by the dynamic program to achieve better solutions . furthermore , we develop a number of novel indicators and selection mechanisms to strengthen synergy of the two algorithmic components of our approach . the results of computational experiments show that the approach is capable to outperform the state-of-the-art results for the single-objective case of the problem .

automatically discovering hidden transformation chaining constraints
model transformations operate on models conforming to precisely defined metamodels . consequently , it often seems relatively easy to chain them : the output of a transformation may be given as input to a second one if metamodels match . however , this simple rule has some obvious limitations . for instance , a transformation may only use a subset of a metamodel . therefore , chaining transformations appropriately requires more information . we present here an approach that automatically discovers more detailed information about actual chaining constraints by statically analyzing transformations . the objective is to provide developers who decide to chain transformations with more data on which to base their choices . this approach has been successfully applied to the case of a library of endogenous transformations . they all have the same source and target metamodel but have some hidden chaining constraints . in such a case , the simple metamodel matching rule given above does not provide any useful information .

general-purpose computing on a semantic network substrate
this article presents a model of general-purpose computing on a semantic network substrate . the concepts presented are applicable to any semantic network representation . however , due to the standards and technological infrastructure devoted to the semantic web effort , this article is presented from this point of view . in the proposed model of computing , the application programming interface , the run-time program , and the state of the computing virtual machine are all represented in the resource description framework ( rdf ) . the implementation of the concepts presented provides a practical computing paradigm that leverages the highly-distributed and standardized representational-layer of the semantic web .

neural block sampling
efficient monte carlo inference often requires manual construction of model-specific proposals . we propose an approach to automated proposal construction by training neural networks to provide fast approximations to block gibbs conditionals . the learned proposals generalize to occurrences of common structural motifs both within a given model and across models , allowing for the construction of a library of learned inference primitives that can accelerate inference on unseen models with no model-specific training required . we explore several applications including open-universe gaussian mixture models , in which our learned proposals outperform a hand-tuned sampler , and a real-world named entity recognition task , in which our sampler 's ability to escape local modes yields higher final f1 scores than single-site gibbs .

activity networks with delays an application to toxicity analysis
andy , activity networks with delays , is a discrete time framework aimed at the qualitative modelling of time-dependent activities . the modular and concise syntax makes andy suitable for an easy and natural modelling of time-dependent biological systems ( i.e. , regulatory pathways ) . activities involve entities playing the role of activators , inhibitors or products of biochemical network operation . activities may have given duration , i.e. , the time required to obtain results . an entity may represent an object ( e.g. , an agent , a biochemical species or a family of thereof ) with a local attribute , a state denoting its level ( e.g. , concentration , strength ) . entities levels may change as a result of an activity or may decay gradually as time passes by . the semantics of andy is formally given via high-level petri nets ensuring this way some modularity . as main results we show that andy systems have finite state representations even for potentially infinite processes and it well adapts to the modelling of toxic behaviours . as an illustration , we present a classification of toxicity properties and give some hints on how they can be verified with existing tools on andy systems . a small case study on blood glucose regulation is provided to exemplify the andy framework and the toxicity properties .

identifying hierarchical structure in sequences : a linear-time algorithm
sequitur is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase , and continuing this process recursively . the result is a hierarchical representation of the original sequence , which offers insights into its lexical structure . the algorithm is driven by two constraints that reduce the size of the grammar , and produce structure as a by-product . sequitur breaks new ground by operating incrementally . moreover , the method 's simple structure permits a proof that it operates in space and time that is linear in the size of the input . our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences .

ilexicon : toward an ecd-compliant interlingual lexical ontology described with semantic web formalisms
we are interested in bridging the world of natural language and the world of the semantic web in particular to support natural multilingual access to the web of data . in this paper we introduce a new type of lexical ontology called interlingual lexical ontology ( ilexicon ) , which uses semantic web formalisms to make each interlingual lexical unit class ( iluc ) support the projection of its semantic decomposition on itself . after a short overview of existing lexical ontologies , we briefly introduce the semantic web formalisms we use . we then present the three layered architecture of our approach : i ) the interlingual lexical meta-ontology ( ileximon ) ; ii ) the ilexicon where ilucs are formally defined ; iii ) the data layer . we illustrate our approach with a standalone ilexicon , and introduce and explain a concise human-readable notation to represent ilexicons . finally , we show how semantic web formalisms enable the projection of a semantic decomposition on the decomposed iluc .

prediction , expectation , and surprise : methods , designs , and study of a deployed traffic forecasting service
we present research on developing models that forecast traffic flow and congestion in the greater seattle area . the research has led to the deployment of a service named jambayes , that is being actively used by over 2,500 users via smartphones and desktop versions of the system . we review the modeling effort and describe experiments probing the predictive accuracy of the models . finally , we present research on building models that can identify current and future surprises , via efforts on modeling and forecasting unexpected situations .

learning probabilistic programs
we develop a technique for generalising from data in which models are samplers represented as program text . we establish encouraging empirical results that suggest that markov chain monte carlo probabilistic programming inference techniques coupled with higher-order probabilistic programming languages are now sufficiently powerful to enable successful inference of this kind in nontrivial domains . we also introduce a new notion of probabilistic program compilation and show how the same machinery might be used in the future to compile probabilistic programs for efficient reusable predictive inference .

maximum production of transmission messages rate for service discovery protocols
minimizing the number of dropped user datagram protocol ( udp ) messages in a network is regarded as a challenge by researchers . this issue represents serious problems for many protocols particularly those that depend on sending messages as part of their strategy , such us service discovery protocols . this paper proposes and evaluates an algorithm to predict the minimum period of time required between two or more consecutive messages and suggests the minimum queue sizes for the routers , to manage the traffic and minimise the number of dropped messages that has been caused by either congestion or queue overflow or both together . the algorithm has been applied to the universal plug and play ( upnp ) protocol using ns2 simulator . it was tested when the routers were connected in two configurations ; as a centralized and de centralized . the message length and bandwidth of the links among the routers were taken in the consideration . the result shows better improvement in number of dropped messages ` among the routers .

rational counterfactuals
this paper introduces the concept of rational countefactuals which is an idea of identifying a counterfactual from the factual ( whether perceived or real ) that maximizes the attainment of the desired consequent . in counterfactual thinking if we have a factual statement like : saddam hussein invaded kuwait and consequently george bush declared war on iraq then its counterfactuals is : if saddam hussein did not invade kuwait then george bush would not have declared war on iraq . the theory of rational counterfactuals is applied to identify the antecedent that gives the desired consequent necessary for rational decision making . the rational countefactual theory is applied to identify the values of variables allies , contingency , distance , major power , capability , democracy , as well as economic interdependency that gives the desired consequent peace .

ai safety and reproducibility : establishing robust foundations for the neuroscience of human values
we propose the creation of a systematic effort to identify and replicate key findings in neuroscience and allied fields related to understanding human values . our aim is to ensure that research underpinning the value alignment problem of artificial intelligence has been sufficiently validated to play a role in the design of ai systems .

a betting interpretation for probabilities and dempster-shafer degrees of belief
there are at least two ways to interpret numerical degrees of belief in terms of betting : ( 1 ) you can offer to bet at the odds defined by the degrees of belief , or ( 2 ) you can judge that a strategy for taking advantage of such betting offers will not multiply the capital it risks by a large factor . both interpretations can be applied to ordinary additive probabilities and used to justify updating by conditioning . only the second can be applied to dempster-shafer degrees of belief and used to justify dempster 's rule of combination .

opportunistic adaptation knowledge discovery
adaptation has long been considered as the achilles ' heel of case-based reasoning since it requires some domain-specific knowledge that is difficult to acquire . in this paper , two strategies are combined in order to reduce the knowledge engineering cost induced by the adaptation knowledge ( ca ) acquisition task : ca is learned from the case base by the means of knowledge discovery techniques , and the ca acquisition sessions are opportunistically triggered , i.e. , at problem-solving time .

progressive joint modeling in unsupervised single-channel overlapped speech recognition
unsupervised single-channel overlapped speech recognition is one of the hardest problems in automatic speech recognition ( asr ) . permutation invariant training ( pit ) is a state of the art model-based approach , which applies a single neural network to solve this single-input , multiple-output modeling problem . we propose to advance the current state of the art by imposing a modular structure on the neural network , applying a progressive pretraining regimen , and improving the objective function with transfer learning and a discriminative training criterion . the modular structure splits the problem into three sub-tasks : frame-wise interpreting , utterance-level speaker tracing , and speech recognition . the pretraining regimen uses these modules to solve progressively harder tasks . transfer learning leverages parallel clean speech to improve the training targets for the network . our discriminative training formulation is a modification of standard formulations , that also penalizes competing outputs of the system . experiments are conducted on the artificial overlapped switchboard and hub5e-swb dataset . the proposed framework achieves over 30 % relative improvement of wer over both a strong jointly trained system , pit for asr , and a separately optimized system , pit for speech separation with clean speech asr model . the improvement comes from better model generalization , training efficiency and the sequence level linguistic knowledge integration .

a distributed ai aided 3d domino game
in the article a turn-based game played on four computers connected via network is investigated . there are three computers with natural intelligence and one with artificial intelligence . game table is seen by each player 's own view point in all players ' monitors . domino pieces are three dimensional . for distributed systems tcp/ip protocol is used . in order to get 3d image , microsoft xna technology is applied . domino 101 game is nondeterministic game that is result of the game depends on the initial random distribution of the pieces . number of the distributions is equal to the multiplication of following combinations : . moreover , in this game that is played by four people , players are divided into 2 pairs . accordingly , we can not predict how the player uses the dominoes that is according to the dominoes of his/her partner or according to his/her own dominoes . the fact that the natural intelligence can be a player in any level affects the outcome . these reasons make it difficult to develop an ai . in the article four levels of ai are developed . the ai in the first level is equivalent to the intelligence of a child who knows the rules of the game and recognizes the numbers . the ai in this level plays if it has any domino , suitable to play or says pass . in most of the games which can be played on the internet , the ai does the same . but the ai in the last level is a master player , and it can develop itself according to its competitors ' levels .

tuned and gpu-accelerated parallel data mining from comparable corpora
the multilingual nature of the world makes translation a crucial requirement today . parallel dictionaries constructed by humans are a widely-available resource , but they are limited and do not provide enough coverage for good quality translation purposes , due to out-of-vocabulary words and neologisms . this motivates the use of statistical translation systems , which are unfortunately dependent on the quantity and quality of training data . such has a very limited availability especially for some languages and very narrow text domains . is this research we present our improvements to yalign mining methodology by reimplementing the comparison algorithm , introducing a tuning scripts and by improving performance using gpu computing acceleration . the experiments are conducted on various text domains and bi-data is extracted from the wikipedia dumps .

grasp and path-relinking for coalition structure generation
in artificial intelligence with coalition structure generation ( csg ) one refers to those cooperative complex problems that require to find an optimal partition , maximising a social welfare , of a set of entities involved in a system into exhaustive and disjoint coalitions . the solution of the csg problem finds applications in many fields such as machine learning ( covering machines , clustering ) , data mining ( decision tree , discretization ) , graph theory , natural language processing ( aggregation ) , semantic web ( service composition ) , and bioinformatics . the problem of finding the optimal coalition structure is np-complete . in this paper we present a greedy adaptive search procedure ( grasp ) with path-relinking to efficiently search the space of coalition structures . experiments and comparisons to other algorithms prove the validity of the proposed method in solving this hard combinatorial problem .

song from pi : a musically plausible network for pop music generation
we present a novel framework for generating pop music . our model is a hierarchical recurrent neural network , where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed . in particular , the bottom layers generate the melody , while the higher levels produce the drums and chords . we conduct several human studies that show strong preference of our generated music over that produced by the recent method by google . we additionally show two applications of our framework : neural dancing and karaoke , as well as neural story singing .

cost-optimal learning of causal graphs
we consider the problem of learning a causal graph over a set of variables with interventions . we study the cost-optimal causal graph learning problem : for a given skeleton ( undirected version of the causal graph ) , design the set of interventions with minimum total cost , that can uniquely identify any causal graph with the given skeleton . we show that this problem is solvable in polynomial time . later , we consider the case when the number of interventions is limited . for this case , we provide polynomial time algorithms when the skeleton is a tree or a clique tree . for a general chordal skeleton , we develop an efficient greedy algorithm , which can be improved when the causal graph skeleton is an interval graph .

mobile agent based solutions for knowledge assessment in elearning environments
e-learning is nowadays one of the most interesting of the `` e- `` domains available through the internet . the main problem to create a web-based , virtual environment is to model the traditional domain and to implement the model using the most suitable technologies . we analyzed the distance learning domain and investigated the possibility to implement some e-learning services using mobile agent technologies . this paper presents a model of the student assessment service ( sas ) and an agent-based framework developed to be used for implementing specific applications . a specific student assessment application that relies on the framework was developed .

differentially private empirical risk minimization
privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data , such as medical or financial records , are analyzed . we provide general techniques to produce privacy-preserving approximations of classifiers learned via ( regularized ) empirical risk minimization ( erm ) . these algorithms are private under the $ \epsilon $ -differential privacy definition due to dwork et al . ( 2006 ) . first we apply the output perturbation ideas of dwork et al . ( 2006 ) , to erm classification . then we propose a new method , objective perturbation , for privacy-preserving machine learning algorithm design . this method entails perturbing the objective function before optimizing over classifiers . if the loss and regularizer satisfy certain convexity and differentiability criteria , we prove theoretical results showing that our algorithms preserve privacy , and provide generalization bounds for linear and nonlinear kernels . we further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms , thereby providing end-to-end privacy guarantees for the training process . we apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines . we obtain encouraging results from evaluating their performance on real demographic and benchmark data sets . our results show that both theoretically and empirically , objective perturbation is superior to the previous state-of-the-art , output perturbation , in managing the inherent tradeoff between privacy and learning performance .

semantic preserving embeddings for generalized graphs
a new approach to the study of generalized graphs as semantic data structures using machine learning techniques is presented . we show how vector representations maintaining semantic characteristics of the original data can be obtained from a given graph using neural encoding architectures and considering the topological properties of the graph . semantic features of these new representations are tested by using some machine learning tasks and new directions on efficient link discovery , entitity retrieval and long distance query methodologies on large relational datasets are investigated using real datasets . -- -- en este trabajo se presenta un nuevo enfoque en el contexto del aprendizaje autom\'atico multi-relacional para el estudio de grafos generalizados . se muestra c\'omo se pueden obtener representaciones vectoriales que mantienen caracter\'isticas sem\'anticas del grafo original utilizando codificadores neuronales y considerando las propiedades topol\'ogicas del grafo . adem\'as , se eval\'uan las caracter\'isticas sem\'anticas capturadas por estas nuevas representaciones y se investigan nuevas metodolog\'ias eficientes relacionadas con link discovery , entity retrieval y consultas a larga distancia en grandes conjuntos de datos relacionales haciendo uso de bases de datos reales .

query strategy for sequential ontology debugging
debugging of ontologies is an important prerequisite for their wide-spread application , especially in areas that rely upon everyday users to create and maintain knowledge bases , as in the case of the semantic web . recent approaches use diagnosis methods to identify causes of inconsistent or incoherent ontologies . however , in most debugging scenarios these methods return many alternative diagnoses , thus placing the burden of fault localization on the user . this paper demonstrates how the target diagnosis can be identified by performing a sequence of observations , that is , by querying an oracle about entailments of the target ontology . we exploit a-priori probabilities of typical user errors to formulate information-theoretic concepts for query selection . our evaluation showed that the proposed method significantly reduces the number of required queries compared to myopic strategies . we experimented with different probability distributions of user errors and different qualities of the a-priori probabilities . our measurements showed the advantageousness of information-theoretic approach to query selection even in cases where only a rough estimate of the priors is available .

automatic bridge bidding using deep reinforcement learning
bridge is among the zero-sum games for which artificial intelligence has not yet outperformed expert human players . the main difficulty lies in the bidding phase of bridge , which requires cooperative decision making under partial information . existing artificial intelligence systems for bridge bidding rely on and are thus restricted by human-designed bidding systems or features . in this work , we propose a pioneering bridge bidding system without the aid of human domain knowledge . the system is based on a novel deep reinforcement learning model , which extracts sophisticated features and learns to bid automatically based on raw card data . the model includes an upper-confidence-bound algorithm and additional techniques to achieve a balance between exploration and exploitation . our experiments validate the promising performance of our proposed model . in particular , the model advances from having no knowledge about bidding to achieving superior performance when compared with a champion-winning computer bridge program that implements a human-designed bidding system .

deep active object recognition by joint label and action prediction
an active object recognition system has the advantage of being able to act in the environment to capture images that are more suited for training and that lead to better performance at test time . in this paper , we propose a deep convolutional neural network for active object recognition that simultaneously predicts the object label , and selects the next action to perform on the object with the aim of improving recognition performance . we treat active object recognition as a reinforcement learning problem and derive the cost function to train the network for joint prediction of the object label and the action . a generative model of object similarities based on the dirichlet distribution is proposed and embedded in the network for encoding the state of the system . the training is carried out by simultaneously minimizing the label and action prediction errors using gradient descent . we empirically show that the proposed network is able to predict both the object label and the actions on germs , a dataset for active object recognition . we compare the test label prediction accuracy of the proposed model with dirichlet and naive bayes state encoding . the results of experiments suggest that the proposed model equipped with dirichlet state encoding is superior in performance , and selects images that lead to better training and higher accuracy of label prediction at test time .

robustness in sparse linear models : relative efficiency based on robust approximate message passing
understanding efficiency in high dimensional linear models is a longstanding problem of interest . classical work with smaller dimensional problems dating back to huber and bickel has illustrated the benefits of efficient loss functions . when the number of parameters $ p $ is of the same order as the sample size $ n $ , $ p \approx n $ , an efficiency pattern different from the one of huber was recently established . in this work , we consider the effects of model selection on the estimation efficiency of penalized methods . in particular , we explore whether sparsity , results in new efficiency patterns when $ p > n $ . in the interest of deriving the asymptotic mean squared error for regularized m-estimators , we use the powerful framework of approximate message passing . we propose a novel , robust and sparse approximate message passing algorithm ( ramp ) , that is adaptive to the error distribution . our algorithm includes many non-quadratic and non-differentiable loss functions . we derive its asymptotic mean squared error and show its convergence , while allowing $ p , n , s \to \infty $ , with $ n/p \in ( 0,1 ) $ and $ n/s \in ( 1 , \infty ) $ . we identify new patterns of relative efficiency regarding a number of penalized $ m $ estimators , when $ p $ is much larger than $ n $ . we show that the classical information bound is no longer reachable , even for light -- tailed error distributions . we show that the penalized least absolute deviation estimator dominates the penalized least square estimator , in cases of heavy -- tailed distributions . we observe this pattern for all choices of the number of non-zero parameters $ s $ , both $ s \leq n $ and $ s \approx n $ . in non-penalized problems where $ s =p \approx n $ , the opposite regime holds . therefore , we discover that the presence of model selection significantly changes the efficiency patterns .

infinite-horizon policy-gradient estimation
gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods . in this paper we introduce gpomdp , a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in partially observable markov decision processes pomdps controlled by parameterized stochastic policies . a similar algorithm was proposed by ( kimura et al . 1995 ) . the algorithm 's chief advantages are that it requires storage of only twice the number of policy parameters , uses one free beta ( which has a natural interpretation in terms of bias-variance trade-off ) , and requires no knowledge of the underlying state . we prove convergence of gpomdp , and show how the correct choice of the parameter beta is related to the mixing time of the controlled pomdp . we briefly describe extensions of gpomdp to controlled markov chains , continuous state , observation and control spaces , multiple-agents , higher-order derivatives , and a version for training stochastic policies with internal states . in a companion paper ( baxter et al. , this volume ) we show how the gradient estimates generated by gpomdp can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward .

generalized neutrosophic soft set
in this paper we present a new concept called generalized neutrosophic soft set . this concept incorporates the beneficial properties of both generalized neutrosophic set introduced by a.a. salama [ 7 ] and soft set techniques proposed by molodtsov [ 4 ] . we also study some properties of this concept . some definitions and operations have been introduced on generalized neutrosophic soft set . finally we present an application of generalized neuutrosophic soft set in decision making problem .

a computer composes a fabled problem : four knights vs. queen
we explain how the prototype automatic chess problem composer , chesthetica , successfully composed a rare and interesting chess problem using the new digital synaptic neural substrate ( dsns ) computational creativity approach . this problem represents a greater challenge from a creative standpoint because the checkmate is not always clear and the method of winning even less so . creating a decisive chess problem of this type without the aid of an omniscient 7-piece endgame tablebase ( and one that also abides by several chess composition conventions ) would therefore be a challenge for most human players and composers working on their own . the fact that a small computer with relatively low processing power and memory was sufficient to compose such a problem using the dsns approach in just 10 days is therefore noteworthy . in this report we document the event and result in some detail . it lends additional credence to the dsns as a viable new approach in the field of computational creativity . in particular , in areas where human-like creativity is required for targeted or specific problems with no clear path to the solution .

a generalized dempster -- shafer evidence theory
dempster-shafer evidence theory has been widely used in various fields of applications . besides , it has been proven that the quantum theory has powerful capabilities of solving the decision making problems . however , due to the inconsistency of the expression , the classical dempster-shafer evidence theory modelled by real numbers can not be integrated directly with the quantum theory modelled by complex numbers . the main contribution in this study is that , unlike the existing evidence theory , a mass function in the generalized dempster-shafer evidence theory is modelled by a complex number , called as a complex mass function . when the complex mass function is degenerated from complex numbers to real numbers , the generalized dempster 's combination rule degenerates to the classical evidence theory . this generalized dempster-shafer evidence theory provides a promising way to model and handle more uncertain information . numerical examples are illustrated to show the efficiency of the generalized dempster-shafer evidence theory .

causal discovery from a mixture of experimental and observational data
this paper describes a bayesian method for combining an arbitrary mixture of observational and experimental data in order to learn causal bayesian networks . observational data are passively observed . experimental data , such as that produced by randomized controlled trials , result from the experimenter manipulating one or more variables ( typically randomly ) and observing the states of other variables . the paper presents a bayesian method for learning the causal structure and parameters of the underlying causal process that is generating the data , given that ( 1 ) the data contains a mixture of observational and experimental case records , and ( 2 ) the causal process is modeled as a causal bayesian network . this learning method was applied using as input various mixtures of experimental and observational data that were generated from the alarm causal bayesian network . in these experiments , the absolute and relative quantities of experimental and observational data were varied systematically . for each of these training datasets , the learning method was applied to predict the causal structure and to estimate the causal parameters that exist among randomly selected pairs of nodes in alarm that are not confounded . the paper reports how these structure predictions and parameter estimates compare with the true causal structures and parameters as given by the alarm network .

an evolutionary solver for linear integer programming
in this paper we introduce an evolutionary algorithm for the solution of linear integer programs . the strategy is based on the separation of the variables into the integer subset and the continuous subset ; the integer variables are fixed by the evolutionary system , and the continuous ones are determined in function of them , by a linear program solver . we report results obtained for some standard benchmark problems , and compare them with those obtained by branch-and-bound . the performance of the evolutionary algorithm is promising . good feasible solutions were generally obtained , and in some of the difficult benchmark tests it outperformed branch-and-bound .

selecting computations : theory and applications
sequential decision problems are often approximately solvable by simulating possible future action sequences . { \em metalevel } decision procedures have been developed for selecting { \em which } action sequences to simulate , based on estimating the expected improvement in decision quality that would result from any particular simulation ; an example is the recent work on using bandit algorithms to control monte carlo tree search in the game of go . in this paper we develop a theoretical basis for metalevel decisions in the statistical framework of bayesian { \em selection problems } , arguing ( as others have done ) that this is more appropriate than the bandit framework . we derive a number of basic results applicable to monte carlo selection problems , including the first finite sampling bounds for optimal policies in certain cases ; we also provide a simple counterexample to the intuitive conjecture that an optimal policy will necessarily reach a decision in all cases . we then derive heuristic approximations in both bayesian and distribution-free settings and demonstrate their superiority to bandit-based heuristics in one-shot decision problems and in go .

generative prior knowledge for discriminative classification
we present a novel framework for integrating prior knowledge into discriminative classifiers . our framework allows discriminative classifiers such as support vector machines ( svms ) to utilize prior knowledge specified in the generative setting . the dual objective of fitting the data and respecting prior knowledge is formulated as a bilevel program , which is solved ( approximately ) via iterative application of second-order cone programming . to test our approach , we consider the problem of using wordnet ( a semantic database of english language ) to improve low-sample classification accuracy of newsgroup categorization . wordnet is viewed as an approximate , but readily available source of background knowledge , and our framework is capable of utilizing it in a flexible way .

evolving localizations in reaction-diffusion cellular automata
we consider hexagonal cellular automata with immediate cell neighbourhood and three cell-states . every cell calculates its next state depending on the integral representation of states in its neighbourhood , i.e . how many neighbours are in each one state . we employ evolutionary algorithms to breed local transition functions that support mobile localizations ( gliders ) , and characterize sets of the functions selected in terms of quasi-chemical systems . analysis of the set of functions evolved allows to speculate that mobile localizations are likely to emerge in the quasi-chemical systems with limited diffusion of one reagent , a small number of molecules is required for amplification of travelling localizations , and reactions leading to stationary localizations involve relatively equal amount of quasi-chemical species . techniques developed can be applied in cascading signals in nature-inspired spatially extended computing devices , and phenomenological studies and classification of non-linear discrete systems .

a decision calculus for belief functions in valuation-based systems
valuation-based system ( vbs ) provides a general framework for representing knowledge and drawing inferences under uncertainty . recent studies have shown that the semantics of vbs can represent and solve bayesian decision problems ( shenoy , 1991a ) . the purpose of this paper is to propose a decision calculus for dempster-shafer ( d-s ) theory in the framework of vbs . the proposed calculus uses a weighting factor whose role is similar to the probabilistic interpretation of an assumption that disambiguates decision problems represented with belief functions ( strat 1990 ) . it will be shown that with the presented calculus , if the decision problems are represented in the valuation network properly , we can solve the problems by using fusion algorithm ( shenoy 1991a ) . it will also be shown the presented decision calculus can be reduced to the calculus for bayesian probability theory when probabilities , instead of belief functions , are given .

advancing multi-context systems by inconsistency management
multi-context systems are an expressive formalism to model ( possibly ) non-monotonic information exchange between heterogeneous knowledge bases . such information exchange , however , often comes with unforseen side-effects leading to violation of constraints , making the system inconsistent , and thus unusable . although there are many approaches to assess and repair a single inconsistent knowledge base , the heterogeneous nature of multi-context systems poses problems which have not yet been addressed in a satisfying way : how to identify and explain a inconsistency that spreads over multiple knowledge bases with different logical formalisms ( e.g. , logic programs and ontologies ) ? what are the causes of inconsistency if inference/information exchange is non-monotonic ( e.g. , absent information as cause ) ? how to deal with inconsistency if access to knowledge bases is restricted ( e.g. , companies exchange information , but do not allow arbitrary modifications to their knowledge bases ) ? many traditional approaches solely aim for a consistent system , but automatic removal of inconsistency is not always desireable . therefore a human operator has to be supported in finding the erroneous parts contributing to the inconsistency . in my thesis those issues will be adressed mainly from a foundational perspective , while our research project also provides algorithms and prototype implementations .

representation and coding of signal geometry
approaches to signal representation and coding theory have traditionally focused on how to best represent signals using parsimonious representations that incur the lowest possible distortion . classical examples include linear and non-linear approximations , sparse representations , and rate-distortion theory . very often , however , the goal of processing is to extract specific information from the signal , and the distortion should be measured on the extracted information . the corresponding representation should , therefore , represent that information as parsimoniously as possible , without necessarily accurately representing the signal itself . in this paper , we examine the problem of encoding signals such that sufficient information is preserved about their pairwise distances and their inner products . for that goal , we consider randomized embeddings as an encoding mechanism and provide a framework to analyze their performance . we also demonstrate that it is possible to design the embedding such that it represents different ranges of distances with different precision . these embeddings also allow the computation of kernel inner products with control on their inner product-preserving properties . our results provide a broad framework to design and analyze embeddins , and generalize existing results in this area , such as random fourier kernels and universal embeddings .

the most persistent soft-clique in a set of sampled graphs
when searching for characteristic subpatterns in potentially noisy graph data , it appears self-evident that having multiple observations would be better than having just one . however , it turns out that the inconsistencies introduced when different graph instances have different edge sets pose a serious challenge . in this work we address this challenge for the problem of finding maximum weighted cliques . we introduce the concept of most persistent soft-clique . this is subset of vertices , that 1 ) is almost fully or at least densely connected , 2 ) occurs in all or almost all graph instances , and 3 ) has the maximum weight . we present a measure of clique-ness , that essentially counts the number of edge missing to make a subset of vertices into a clique . with this measure , we show that the problem of finding the most persistent soft-clique problem can be cast either as : a ) a max-min two person game optimization problem , or b ) a min-min soft margin optimization problem . both formulations lead to the same solution when using a partial lagrangian method to solve the optimization problems . by experiments on synthetic data and on real social network data , we show that the proposed method is able to reliably find soft cliques in graph data , even if that is distorted by random noise or unreliable observations .

bayesian reinforcement learning : a survey
bayesian methods for machine learning have been widely investigated , yielding principled methods for incorporating prior information into inference algorithms . in this survey , we provide an in-depth review of the role of bayesian methods for the reinforcement learning ( rl ) paradigm . the major incentives for incorporating bayesian reasoning in rl are : 1 ) it provides an elegant approach to action-selection ( exploration/exploitation ) as a function of the uncertainty in learning ; and 2 ) it provides a machinery to incorporate prior knowledge into the algorithms . we first discuss models and methods for bayesian inference in the simple single-step bandit model . we then review the extensive recent literature on bayesian methods for model-based rl , where prior information can be expressed on the parameters of the markov model . we also present bayesian methods for model-free rl , where priors are expressed over the value function or policy class . the objective of the paper is to provide a comprehensive survey on bayesian rl algorithms and their theoretical and empirical properties .

sensitivity analysis for threshold decision making with dynamic networks
the effect of inaccuracies in the parameters of a dynamic bayesian network can be investigated by subjecting the network to a sensitivity analysis . having detailed the resulting sensitivity functions in our previous work , we now study the effect of parameter inaccuracies on a recommended decision in view of a threshold decision-making model . we detail the effect of varying a single and multiple parameters from a conditional probability table and present a computational procedure for establishing bounds between which assessments for these parameters can be varied without inducing a change in the recommended decision . we illustrate the various concepts involved by means of a real-life dynamic network in the field of infectious disease .

another perspective on default reasoning
the lexicographic closure of any given finite set d of normal defaults is defined . a conditional assertion `` if a then b '' is in this lexicographic closure if , given the defaults d and the fact a , one would conclude b. the lexicographic closure is essentially a rational extension of d , and of its rational closure , defined in a previous paper . it provides a logic of normal defaults that is different from the one proposed by r. reiter and that is rich enough not to require the consideration of non-normal defaults . a large number of examples are provided to show that the lexicographic closure corresponds to the basic intuitions behind reiter 's logic of defaults .

order-consistent programs are cautiously monotonic
some normal logic programs under the answer set ( stable model ) semantics lack the appealing property of `` cautious monotonicity . '' that is , augmenting a program with one of its consequences may cause it to lose another of its consequences . the syntactic condition of `` order-consistency '' was shown by fages to guarantee existence of an answer set . this note establishes that order-consistent programs are not only consistent , but cautiously monotonic . from this it follows that they are also `` cumulative . '' that is , augmenting an order-consistent with some of its consequences does not alter its consequences . in fact , as we show , its answer sets remain unchanged .

approximation of the two-part mdl code
approximation of the optimal two-part mdl code for given data , through successive monotonically length-decreasing two-part mdl codes , has the following properties : ( i ) computation of each step may take arbitrarily long ; ( ii ) we may not know when we reach the optimum , or whether we will reach the optimum at all ; ( iii ) the sequence of models generated may not monotonically improve the goodness of fit ; but ( iv ) the model associated with the optimum has ( almost ) the best goodness of fit . to express the practically interesting goodness of fit of individual models for individual data sets we have to rely on kolmogorov complexity .

learning the structure of dynamic probabilistic networks
dynamic probabilistic networks are a compact representation of complex stochastic processes . in this paper we examine how to learn the structure of a dpn from data . we extend structure scoring rules for standard probabilistic networks to the dynamic case , and show how to search for structure when some of the variables are hidden . finally , we examine two applications where such a technology might be useful : predicting and classifying dynamic behaviors , and learning causal orderings in biological processes . we provide empirical results that demonstrate the applicability of our methods in both domains .

achieving compositionality of the stable model semantics for smodels programs
in this paper , a gaifman-shapiro-style module architecture is tailored to the case of smodels programs under the stable model semantics . the composition of smodels program modules is suitably limited by module conditions which ensure the compatibility of the module system with stable models . hence the semantics of an entire smodels program depends directly on stable models assigned to its modules . this result is formalized as a module theorem which truly strengthens lifschitz and turner 's splitting-set theorem for the class of smodels programs . to streamline generalizations in the future , the module theorem is first proved for normal programs and then extended to cover smodels programs using a translation from the latter class of programs to the former class . moreover , the respective notion of module-level equivalence , namely modular equivalence , is shown to be a proper congruence relation : it is preserved under substitutions of modules that are modularly equivalent . principles for program decomposition are also addressed . the strongly connected components of the respective dependency graph can be exploited in order to extract a module structure when there is no explicit a priori knowledge about the modules of a program . the paper includes a practical demonstration of tools that have been developed for automated ( de ) composition of smodels programs . to appear in theory and practice of logic programming .

a competitive comparison of different types of evolutionary algorithms
this paper presents comparison of several stochastic optimization algorithms developed by authors in their previous works for the solution of some problems arising in civil engineering . the introduced optimization methods are : the integer augmented simulated annealing ( iasa ) , the real-coded augmented simulated annealing ( rasa ) , the differential evolution ( de ) in its original fashion developed by r. storn and k. price and simplified real-coded differential genetic algorithm ( sade ) . each of these methods was developed for some specific optimization problem ; namely the chebychev trial polynomial problem , the so called type 0 function and two engineering problems - the reinforced concrete beam layout and the periodic unit cell problem respectively . detailed and extensive numerical tests were performed to examine the stability and efficiency of proposed algorithms . the results of our experiments suggest that the performance and robustness of rasa , iasa and sade methods are comparable , while the de algorithm performs slightly worse . this fact together with a small number of internal parameters promotes the sade method as the most robust for practical use .

a clustering approach to solving large stochastic matching problems
in this work we focus on efficient heuristics for solving a class of stochastic planning problems that arise in a variety of business , investment , and industrial applications . the problem is best described in terms of future buy and sell contracts . by buying less reliable , but less expensive , buy ( supply ) contracts , a company or a trader can cover a position of more reliable and more expensive sell contracts . the goal is to maximize the expected net gain ( profit ) by constructing a dose to optimum portfolio out of the available buy and sell contracts . this stochastic planning problem can be formulated as a two-stage stochastic linear programming problem with recourse . however , this formalization leads to solutions that are exponential in the number of possible failure combinations . thus , this approach is not feasible for large scale problems . in this work we investigate heuristic approximation techniques alleviating the efficiency problem . we primarily focus on the clustering approach and devise heuristics for finding clusterings leading to good approximations . we illustrate the quality and feasibility of the approach through experimental data .

modelling information incorporation in markets , with application to detecting and explaining events
we develop a model of how information flows into a market , and derive algorithms for automatically detecting and explaining relevant events . we analyze data from twenty-two `` political stock markets '' ( i.e. , betting markets on political outcomes ) on the iowa electronic market ( iem ) . we prove that , under certain efficiency assumptions , prices in such betting markets will on average approach the correct outcomes over time , and show that iem data conforms closely to the theory . we present a simple model of a betting market where information is revealed over time , and show a qualitative correspondence between the model and real market data . we also present an algorithm for automatically detecting significant events and generating semantic explanations of their origin . the algorithm operates by discovering significant changes in vocabulary on online news sources ( using expected entropy loss ) that align with major price spikes in related betting markets .

personalized emphasis framing for persuasive message generation
in this paper , we present a study on personalized emphasis framing which can be used to tailor the content of a message to enhance its appeal to different individuals . with this framework , we directly model content selection decisions based on a set of psychologically-motivated domain-independent personal traits including personality ( e.g. , extraversion and conscientiousness ) and basic human values ( e.g. , self-transcendence and hedonism ) . we also demonstrate how the analysis results can be used in automated personalized content selection for persuasive message generation .

integration of navigation and action selection functionalities in a computational model of cortico-basal ganglia-thalamo-cortical loops
this article describes a biomimetic control architecture affording an animat both action selection and navigation functionalities . it satisfies the survival constraint of an artificial metabolism and supports several complementary navigation strategies . it builds upon an action selection model based on the basal ganglia of the vertebrate brain , using two interconnected cortico-basal ganglia-thalamo-cortical loops : a ventral one concerned with appetitive actions and a dorsal one dedicated to consummatory actions . the performances of the resulting model are evaluated in simulation . the experiments assess the prolonged survival permitted by the use of high level navigation strategies and the complementarity of navigation strategies in dynamic environments . the correctness of the behavioral choices in situations of antagonistic or synergetic internal states are also tested . finally , the modelling choices are discussed with regard to their biomimetic plausibility , while the experimental results are estimated in terms of animat adaptivity .

knowledge-based decision model construction for hierarchical diagnosis : a preliminary report
numerous methods for probabilistic reasoning in large , complex belief or decision networks are currently being developed . there has been little research on automating the dynamic , incremental construction of decision models . a uniform value-driven method of decision model construction is proposed for the hierarchical complete diagnosis . hierarchical complete diagnostic reasoning is formulated as a stochastic process and modeled using influence diagrams . given observations , this method creates decision models in order to obtain the best actions sequentially for locating and repairing a fault at minimum cost . this method construct decision models incrementally , interleaving probe actions with model construction and evaluation . the method treats meta-level and baselevel tasks uniformly . that is , the method takes a decision-theoretic look at the control of search in causal pathways and structural hierarchies .

photometric catalogue of quasars and other point sources in the sloan digital sky survey
we present a catalogue of about 6 million unresolved photometric detections in the sloan digital sky survey seventh data release classifying them into stars , galaxies and quasars . we use a machine learning classifier trained on a subset of spectroscopically confirmed objects from 14th to 22nd magnitude in the sdss { \it i } -band . our catalogue consists of 2,430,625 quasars , 3,544,036 stars and 63,586 unresolved galaxies from 14th to 24th magnitude in the sdss { \it i } -band . our algorithm recovers 99.96 % of spectroscopically confirmed quasars and 99.51 % of stars to i $ \sim $ 21.3 in the colour window that we study . the level of contamination due to data artefacts for objects beyond $ i=21.3 $ is highly uncertain and all mention of completeness and contamination in the paper are valid only for objects brighter than this magnitude . however , a comparison of the predicted number of quasars with the theoretical number counts shows reasonable agreement .

exploring programmable self-assembly in non-dna based molecular computing
self-assembly is a phenomenon observed in nature at all scales where autonomous entities build complex structures , without external influences nor centralised master plan . modelling such entities and programming correct interactions among them is crucial for controlling the manufacture of desired complex structures at the molecular and supramolecular scale . this work focuses on a programmability model for non dna-based molecules and complex behaviour analysis of their self-assembled conformations . in particular , we look into modelling , programming and simulation of porphyrin molecules self-assembly and apply kolgomorov complexity-based techniques to classify and assess simulation results in terms of information content . the analysis focuses on phase transition , clustering , variability and parameter discovery which as a whole pave the way to the notion of complex systems programmability .

multiset ordering constraints
we identify a new and important global ( or non-binary ) constraint . this constraint ensures that the values taken by two vectors of variables , when viewed as multisets , are ordered . this constraint is useful for a number of different applications including breaking symmetry and fuzzy constraint satisfaction . we propose and implement an efficient linear time algorithm for enforcing generalised arc consistency on such a multiset ordering constraint . experimental results on several problem domains show considerable promise .

exact indexing for massive time series databases under time warping distance
among many existing distance measures for time series data , dynamic time warping ( dtw ) distance has been recognized as one of the most accurate and suitable distance measures due to its flexibility in sequence alignment . however , dtw distance calculation is computationally intensive . especially in very large time series databases , sequential scan through the entire database is definitely impractical , even with random access that exploits some index structures since high dimensionality of time series data incurs extremely high i/o cost . more specifically , a sequential structure consumes high cpu but low i/o costs , while an index structure requires low cpu but high i/o costs . in this work , we therefore propose a novel indexed sequential structure called twist ( time warping in indexed sequential structure ) which benefits from both sequential access and index structure . when a query sequence is issued , twist calculates lower bounding distances between a group of candidate sequences and the query sequence , and then identifies the data access order in advance , hence reducing a great number of both sequential and random accesses . impressively , our indexed sequential structure achieves significant speedup in a querying process by a few orders of magnitude . in addition , our method shows superiority over existing rival methods in terms of query processing time , number of page accesses , and storage requirement with no false dismissal guaranteed .

integrating existing cone-shaped and projection-based cardinal direction relations and a tcsp-like decidable generalisation
we consider the integration of existing cone-shaped and projection-based calculi of cardinal direction relations , well-known in qsr . the more general , integrating language we consider is based on convex constraints of the qualitative form $ r ( x , y ) $ , $ r $ being a cone-shaped or projection-based cardinal direction atomic relation , or of the quantitative form $ ( \alpha , \beta ) ( x , y ) $ , with $ \alpha , \beta\in [ 0,2\pi ) $ and $ ( \beta -\alpha ) \in [ 0 , \pi ] $ : the meaning of the quantitative constraint , in particular , is that point $ x $ belongs to the ( convex ) cone-shaped area rooted at $ y $ , and bounded by angles $ \alpha $ and $ \beta $ . the general form of a constraint is a disjunction of the form $ [ r_1\vee ... \vee r_ { n_1 } \vee ( \alpha_1 , \beta_1 ) \vee ... \vee ( \alpha _ { n_2 } , \beta_ { n_2 } ) ] ( x , y ) $ , with $ r_i ( x , y ) $ , $ i=1 ... n_1 $ , and $ ( \alpha _i , \beta_i ) ( x , y ) $ , $ i=1 ... n_2 $ , being convex constraints as described above : the meaning of such a general constraint is that , for some $ i=1 ... n_1 $ , $ r_i ( x , y ) $ holds , or , for some $ i=1 ... n_2 $ , $ ( \alpha_i , \beta_i ) ( x , y ) $ holds . a conjunction of such general constraints is a $ \tcsp $ -like csp , which we will refer to as an $ \scsp $ ( spatial constraint satisfaction problem ) . an effective solution search algorithm for an $ \scsp $ will be described , which uses ( 1 ) constraint propagation , based on a composition operation to be defined , as the filtering method during the search , and ( 2 ) the simplex algorithm , guaranteeing completeness , at the leaves of the search tree . the approach is particularly suited for large-scale high-level vision , such as , e.g. , satellite-like surveillance of a geographic area .

trapezoidal fuzzy numbers for the transportation problem
transportation problem is an important problem which has been widely studied in operations research domain . it has been often used to simulate different real life problems . in particular , application of this problem in np hard problems has a remarkable significance . in this paper , we present the closed , bounded and non empty feasible region of the transportation problem using fuzzy trapezoidal numbers which ensures the existence of an optimal solution to the balanced transportation problem . the multivalued nature of fuzzy sets allows handling of uncertainty and vagueness involved in the cost values of each cells in the transportation table . for finding the initial solution of the transportation problem we use the fuzzy vogel approximation method and for determining the optimality of the obtained solution fuzzy modified distribution method is used . the fuzzification of the cost of the transportation problem is discussed with the help of a numerical example . finally , we discuss the computational complexity involved in the problem . to the best of our knowledge , this is the first work on obtaining the solution of the transportation problem using fuzzy trapezoidal numbers .

fault-tolerant , but paradoxical path-finding in physical and conceptual systems
we report our initial investigations into reliability and path-finding based models and propose future areas of interest . inspired by broken sidewalks during on-campus construction projects , we develop two models for navigating this `` unreliable network . '' these are based on a concept of `` accumulating risk '' backward from the destination , and both operate on directed acyclic graphs with a probability of failure associated with each edge . the first serves to introduce and has faults addressed by the second , more conservative model . next , we show a paradox when these models are used to construct polynomials on conceptual networks , such as design processes and software development life cycles . when the risk of a network increases uniformly , the most reliable path changes from wider and longer to shorter and narrower . if we let professional inexperience -- such as with entry level cooks and software developers -- represent probability of edge failure , does this change in path imply that the novice should follow instructions with fewer `` back-up '' plans , yet those with alternative routes should be followed by the expert ?

heterogeneous information network embedding for meta path based proximity
a network embedding is a representation of a large graph in a low-dimensional space , where vertices are modeled as vectors . the objective of a good embedding is to preserve the proximity between vertices in the original graph . this way , typical search and mining methods can be applied in the embedded space with the help of off-the-shelf multidimensional indexing approaches . existing network embedding techniques focus on homogeneous networks , where all vertices are considered to belong to a single class .

an analysis of reduced error pruning
top-down induction of decision trees has been observed to suffer from the inadequate functioning of the pruning phase . in particular , it is known that the size of the resulting tree grows linearly with the sample size , even though the accuracy of the tree does not improve . reduced error pruning is an algorithm that has been used as a representative technique in attempts to explain the problems of decision tree learning . in this paper we present analyses of reduced error pruning in three different settings . first we study the basic algorithmic properties of the method , properties that hold independent of the input decision tree and pruning examples . then we examine a situation that intuitively should lead to the subtree under consideration to be replaced by a leaf node , one in which the class label and attribute values of the pruning examples are independent of each other . this analysis is conducted under two different assumptions . the general analysis shows that the pruning probability of a node fitting pure noise is bounded by a function that decreases exponentially as the size of the tree grows . in a specific analysis we assume that the examples are distributed uniformly to the tree . this assumption lets us approximate the number of subtrees that are pruned because they do not receive any pruning examples . this paper clarifies the different variants of the reduced error pruning algorithm , brings new insight to its algorithmic properties , analyses the algorithm with less imposed assumptions than before , and includes the previously overlooked empty subtrees to the analysis .

et-lda : joint topic modeling for aligning events and their twitter feedback
during broadcast events such as the superbowl , the u.s. presidential and primary debates , etc. , twitter has become the de facto platform for crowds to share perspectives and commentaries about them . given an event and an associated large-scale collection of tweets , there are two fundamental research problems that have been receiving increasing attention in recent years . one is to extract the topics covered by the event and the tweets ; the other is to segment the event . so far these problems have been viewed separately and studied in isolation . in this work , we argue that these problems are in fact inter-dependent and should be addressed together . we develop a joint bayesian model that performs topic modeling and event segmentation in one unified framework . we evaluate the proposed model both quantitatively and qualitatively on two large-scale tweet datasets associated with two events from different domains to show that it improves significantly over baseline models .

relational learning and feature extraction by querying over heterogeneous information networks
many real world systems need to operate on heterogeneous information networks that consist of numerous interacting components of different types . examples include systems that perform data analysis on biological information networks ; social networks ; and information extraction systems processing unstructured data to convert raw text to knowledge graphs . many previous works describe specialized approaches to perform specific types of analysis , mining and learning on such networks . in this work , we propose a unified framework consisting of a data model -a graph with a first order schema along with a declarative language for constructing , querying and manipulating such networks in ways that facilitate relational and structured machine learning . in particular , we provide an initial prototype for a relational and graph traversal query language where queries are directly used as relational features for structured machine learning models . feature extraction is performed by making declarative graph traversal queries . learning and inference models can directly operate on this relational representation and augment it with new data and knowledge that , in turn , is integrated seamlessly into the relational structure to support new predictions . we demonstrate this system 's capabilities by showcasing tasks in natural language processing and computational biology domains .

safety verification of deep neural networks
deep neural networks have achieved impressive experimental results in image classification , but can surprisingly be unstable with respect to adversarial perturbations , that is , minimal changes to the input image that cause the network to misclassify it . with potential applications including perception modules and end-to-end controllers for self-driving cars , this raises concerns about their safety . we develop a novel automated verification framework for feed-forward multi-layer neural networks based on satisfiability modulo theory ( smt ) . we focus on safety of image classification decisions with respect to image manipulations , such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human , and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image . we enable exhaustive search of the region by employing discretisation , and propagate the analysis layer by layer . our method works directly with the network code and , in contrast to existing methods , can guarantee that adversarial examples , if they exist , are found for the given region and family of manipulations . if found , adversarial examples can be shown to human testers and/or used to fine-tune the network . we implement the techniques using z3 and evaluate them on state-of-the-art networks , including regularised and deep learning networks . we also compare against existing techniques to search for adversarial examples and estimate network robustness .

from relational databases to belief networks
the relationship between belief networks and relational databases is examined . based on this analysis , a method to construct belief networks automatically from statistical relational data is proposed . a comparison between our method and other methods shows that our method has several advantages when generalization or prediction is deeded .

semtk : an ontology-first , open source semantic toolkit for managing and querying knowledge graphs
the relatively recent adoption of knowledge graphs as an enabling technology in multiple high-profile artificial intelligence and cognitive applications has led to growing interest in the semantic web technology stack . many semantics-related tools , however , are focused on serving experts with a deep understanding of semantic technologies . for example , triplification of relational data is available but there is no open source tool that allows a user unfamiliar with owl/rdf to import data into a semantic triple store in an intuitive manner . further , many tools require users to have a working understanding of sparql to query data . casual users interested in benefiting from the power of knowledge graphs have few tools available for exploring , querying , and managing semantic data . we present semtk , the semantics toolkit , a user-friendly suite of tools that allow both expert and non-expert semantics users convenient ingestion of relational data , simplified query generation , and more . the exploration of ontologies and instance data is performed through sparqlgraph , an intuitive web-based user interface in semtk understandable and navigable by a lay user . the open source version of semtk is available at http : //semtk.research.ge.com .

a latent-class model for estimating product-choice probabilities from clickstream data
this paper analyzes customer product-choice behavior based on the recency and frequency of each customer 's page views on e-commerce sites . recently , we devised an optimization model for estimating product-choice probabilities that satisfy monotonicity , convexity , and concavity constraints with respect to recency and frequency . this shape-restricted model delivered high predictive performance even when there were few training samples . however , typical e-commerce sites deal in many different varieties of products , so the predictive performance of the model can be further improved by integration of such product heterogeneity . for this purpose , we develop a novel latent-class shape-restricted model for estimating product-choice probabilities for each latent class of products . we also give a tailored expectation-maximization algorithm for parameter estimation . computational results demonstrate that higher predictive performance is achieved with our latent-class model than with the previous shape-restricted model and common latent-class logistic regression .

dynamic frame skip deep q network
deep reinforcement learning methods have achieved state of the art performance in learning control policies for the games in the atari 2600 domain . one of the important parameters in the arcade learning environment ( ale ) is the frame skip rate . it decides the granularity at which agents can control game play . a frame skip value of $ k $ allows the agent to repeat a selected action $ k $ number of times . the current state of the art architectures like deep q-network ( dqn ) and dueling network architectures ( dudqn ) consist of a framework with a static frame skip rate , where the action output from the network is repeated for a fixed number of frames regardless of the current state . in this paper , we propose a new architecture , dynamic frame skip deep q-network ( dfdqn ) which makes the frame skip rate a dynamic learnable parameter . this allows us to choose the number of times an action is to be repeated based on the current state . we show empirically that such a setting improves the performance on relatively harder games like seaquest .

decision making with interval influence diagrams
in previous work ( fertig and breese , 1989 ; fertig and breese , 1990 ) we defined a mechanism for performing probabilistic reasoning in influence diagrams using interval rather than point-valued probabilities . in this paper we extend these procedures to incorporate decision nodes and interval-valued value functions in the diagram . we derive the procedures for chance node removal ( calculating expected value ) and decision node removal ( optimization ) in influence diagrams where lower bounds on probabilities are stored at each chance node and interval bounds are stored on the value function associated with the diagram 's value node . the output of the algorithm are a set of admissible alternatives for each decision variable and a set of bounds on expected value based on the imprecision in the input . the procedure can be viewed as an approximation to a full e-dimensional sensitivity analysis where n are the number of imprecise probability distributions in the input . we show the transformations are optimal and sound . the performance of the algorithm on an influence diagrams is investigated and compared to an exact algorithm .

a note on privacy preserving iteratively reweighted least squares
iteratively reweighted least squares ( irls ) is a widely-used method in machine learning to estimate the parameters in the generalised linear models . in particular , irls for l1 minimisation under the linear model provides a closed-form solution in each step , which is a simple multiplication between the inverse of the weighted second moment matrix and the weighted first moment vector . when dealing with privacy sensitive data , however , developing a privacy preserving irls algorithm faces two challenges . first , due to the inversion of the second moment matrix , the usual sensitivity analysis in differential privacy incorporating a single datapoint perturbation gets complicated and often requires unrealistic assumptions . second , due to its iterative nature , a significant cumulative privacy loss occurs . however , adding a high level of noise to compensate for the privacy loss hinders from getting accurate estimates . here , we develop a practical algorithm that overcomes these challenges and outputs privatised and accurate irls solutions . in our method , we analyse the sensitivity of each moments separately and treat the matrix inversion and multiplication as a post-processing step , which simplifies the sensitivity analysis . furthermore , we apply the { \it { concentrated differential privacy } } formalism , a more relaxed version of differential privacy , which requires adding a significantly less amount of noise for the same level of privacy guarantee , compared to the conventional and advanced compositions of differentially private mechanisms .

cause , responsibility , and blame : oa structural-model approach
a definition of causality introduced by halpern and pearl , which uses structural equations , is reviewed . a more refined definition is then considered , which takes into account issues of normality and typicality , which are well known to affect causal ascriptions . causality is typically an all-or-nothing notion : either a is a cause of b or it is not . an extension of the definition of causality to capture notions of degree of responsibility and degree of blame , due to chockler and halpern , is reviewed . for example , if someone wins an election 11-0 , then each person who votes for him is less responsible for the victory than if he had won 6-5. degree of blame takes into account an agent 's epistemic state . roughly speaking , the degree of blame of a for b is the expected degree of responsibility of a for b , taken over the epistemic state of an agent . finally , the structural-equations definition of causality is compared to wright 's ness test .

examples as interaction : on humans teaching a computer to play a game
this paper reviews an experiment in human-computer interaction , where interaction takes place when humans attempt to teach a computer to play a strategy board game . we show that while individually learned models can be shown to improve the playing performance of the computer , their straightforward composition results in diluting what was earlier learned . this observation suggests that interaction can not be easily distributed when one hopes to harness multiple human experts to develop a quality computer player . this is related to similar approaches in robot task learning and to classic approaches to human learning and reinforces the need to develop tools that facilitate the mix of human-based tuition and computer self-learning .

stochastic and-or grammars : a unified framework and logic perspective
stochastic and-or grammars ( aog ) extend traditional stochastic grammars of language to model other types of data such as images and events . in this paper we propose a representation framework of stochastic aogs that is agnostic to the type of the data being modeled and thus unifies various domain-specific aogs . many existing grammar formalisms and probabilistic models in natural language processing , computer vision , and machine learning can be seen as special cases of this framework . we also propose a domain-independent inference algorithm of stochastic context-free aogs and show its tractability under a reasonable assumption . furthermore , we provide two interpretations of stochastic context-free aogs as a subset of probabilistic logic , which connects stochastic aogs to the field of statistical relational learning and clarifies their relation with a few existing statistical relational models .

on image filtering , noise and morphological size intensity diagrams
in the absence of a pure noise-free image it is hard to define what noise is , in any original noisy image , and as a consequence also where it is , and in what amount . in fact , the definition of noise depends largely on our own aim in the whole image analysis process , and ( perhaps more important ) in our self-perception of noise . for instance , when we perceive noise as disconnected and small it is normal to use mm-asf filters to treat it . there is two evidences of this . first , in many instances there is no ideal and pure noise-free image to compare our filtering process ( nothing but our self-perception of its pure image ) ; second , and related with this first point , mm transformations that we chose are only based on our self - and perhaps - fuzzy notion . the present proposal combines the results of two mm filtering transformations ( ft1 , ft2 ) and makes use of some measures and quantitative relations on their size/intensity diagrams to find the most appropriate noise removal process . results can also be used for finding the most appropriate stop criteria , and the right sequence of mm operators combination on alternating sequential filters ( asf ) , if these measures are applied , for instance , on a genetic algorithm 's target function .

pruning search space in defeasible argumentation
defeasible argumentation has experienced a considerable growth in ai in the last decade . theoretical results have been combined with development of practical applications in ai & law , case-based reasoning and various knowledge-based systems . however , the dialectical process associated with inference is computationally expensive . this paper focuses on speeding up this inference process by pruning the involved search space . our approach is twofold . on one hand , we identify distinguished literals for computing defeat . on the other hand , we restrict ourselves to a subset of all possible conflicting arguments by introducing dialectical constraints .

semantic filtering by inference on domain knowledge in spoken dialogue systems
general natural dialogue processing requires large amounts of domain knowledge as well as linguistic knowledge in order to ensure acceptable coverage and understanding . there are several ways of integrating lexical resources ( e.g . dictionaries , thesauri ) and knowledge bases or ontologies at different levels of dialogue processing . we concentrate in this paper on how to exploit domain knowledge for filtering interpretation hypotheses generated by a robust semantic parser . we use domain knowledge to semantically constrain the hypothesis space . moreover , adding an inference mechanism allows us to complete the interpretation when information is not explicitly available . further , we discuss briefly how this can be generalized towards a predictive natural interactive system .

using the structure of d-connecting paths as a qualitative measure of the strength of dependence
pearls concept of a d - connecting path is one of the foundations of the modern theory of graphical models : the absence of a d - connecting path in a dag indicates that conditional independence will hold in any distribution factorising according to that graph . in this paper we show that in singly - connected gaussian dags it is possible to use the form of a d - connection to obtain qualitative information about the strength of conditional dependence.more precisely , the squared partial correlations between two given variables , conditioned on different subsets may be partially ordered by examining the relationship between the d - connecting path and the set of variables conditioned upon .

welldefined decision scenarios
influence diagrams serve as a powerful tool for modelling symmetric decision problems . when solving an influence diagram we determine a set of strategies for the decisions involved . a strategy for a decision variable is in principle a function over its past . however , some of the past may be irrelevant for the decision , and for computational reasons it is important not to deal with redundant variables in the strategies . we show that current methods ( e.g . the `` decision bayes-ball '' algorithm by shachter uai98 ) do not determine the relevant past , and we present a complete algorithm . actually , this paper takes a more general outset : when formulating a decision scenario as an influence diagram , a linear temporal ordering of the decisions variables is required . this constraint ensures that the decision scenario is welldefined . however , the structure of a decision scenario often yields certain decisions conditionally independent , and it is therefore unnecessary to impose a linear temporal ordering on the decisions . in this paper we deal with partial influence diagrams i.e . influence diagrams with only a partial temporal ordering specified . we present a set of conditions which are necessary and sufficient to ensure that a partial influence diagram is welldefined . these conditions are used as a basis for the construction of an algorithm for determining whether or not a partial influence diagram is welldefined .

sparse markov decision processes with causal sparse tsallis entropy regularization for reinforcement learning
in this paper , a sparse markov decision process ( mdp ) with novel causal sparse tsallis entropy regularization is proposed.the proposed policy regularization induces a sparse and multi-modal optimal policy distribution of a sparse mdp . the full mathematical analysis of the proposed sparse mdp is provided.we first analyze the optimality condition of a sparse mdp . then , we propose a sparse value iteration method which solves a sparse mdp and then prove the convergence and optimality of sparse value iteration using the banach fixed point theorem . the proposed sparse mdp is compared to soft mdps which utilize causal entropy regularization . we show that the performance error of a sparse mdp has a constant bound , while the error of a soft mdp increases logarithmically with respect to the number of actions , where this performance error is caused by the introduced regularization term . in experiments , we apply sparse mdps to reinforcement learning problems . the proposed method outperforms existing methods in terms of the convergence speed and performance .

identification and classification of tcm syndrome types among patients with vascular mild cognitive impairment using latent tree analysis
objective : to treat patients with vascular mild cognitive impairment ( vmci ) using tcm , it is necessary to classify the patients into tcm syndrome types and to apply different treatments to different types . we investigate how to properly carry out the classification using a novel data-driven method known as latent tree analysis . method : a cross-sectional survey on vmci was carried out in several regions in northern china from 2008 to 2011 , which resulted in a data set that involves 803 patients and 93 symptoms . latent tree analysis was performed on the data to reveal symptom co-occurrence patterns , and the patients were partitioned into clusters in multiple ways based on the patterns . the patient clusters were matched up with syndrome types , and population statistics of the clusters are used to quantify the syndrome types and to establish classification rules . results : eight syndrome types are identified : qi deficiency , qi stagnation , blood deficiency , blood stasis , phlegm-dampness , fire-heat , yang deficiency , and yin deficiency . the prevalence and symptom occurrence characteristics of each syndrome type are determined . quantitative classification rules are established for determining whether a patient belongs to each of the syndrome types . conclusions : a solution for the tcm syndrome classification problem associated with vmci is established based on the latent tree analysis of unlabeled symptom survey data . the results can be used as a reference in clinic practice to improve the quality of syndrome differentiation and to reduce diagnosis variances across physicians . they can also be used for patient selection in research projects aimed at finding biomarkers for the syndrome types and in randomized control trials aimed at determining the efficacy of tcm treatments of vmci .

revisiting the training of logic models of protein signaling networks with a formal approach based on answer set programming
a fundamental question in systems biology is the construction and training to data of mathematical models . logic formalisms have become very popular to model signaling networks because their simplicity allows us to model large systems encompassing hundreds of proteins . an approach to train ( boolean ) logic models to high-throughput phospho-proteomics data was recently introduced and solved using optimization heuristics based on stochastic methods . here we demonstrate how this problem can be solved using answer set programming ( asp ) , a declarative problem solving paradigm , in which a problem is encoded as a logical program such that its answer sets represent solutions to the problem . asp has significant improvements over heuristic methods in terms of efficiency and scalability , it guarantees global optimality of solutions as well as provides a complete set of solutions . we illustrate the application of asp with in silico cases based on realistic networks and data .

two-dimensional indirect binary search for the positive one-in-three satisfiability problem
in this paper , we propose an algorithm for the positive one-in-three satisfiability problem ( pos1in3sat ) . the proposed algorithm can efficiently decide the existence of a satisfying assignment in all assignments for a given formula by using a 2-dimensional binary search method without constructing an exponential number of assignments .

solving the `` false positives '' problem in fraud prediction
in this paper , we present an automated feature engineering based approach to dramatically reduce false positives in fraud prediction . false positives plague the fraud prediction industry . it is estimated that only 1 in 5 declared as fraud are actually fraud and roughly 1 in every 6 customers have had a valid transaction declined in the past year . to address this problem , we use the deep feature synthesis algorithm to automatically derive behavioral features based on the historical data of the card associated with a transaction . we generate 237 features ( > 100 behavioral patterns ) for each transaction , and use a random forest to learn a classifier . we tested our machine learning model on data from a large multinational bank and compared it to their existing solution . on an unseen data of 1.852 million transactions , we were able to reduce the false positives by 54 % and provide a savings of 190k euros . we also assess how to deploy this solution , and whether it necessitates streaming computation for real time scoring . we found that our solution can maintain similar benefits even when historical features are computed once every 7 days .

evaluating the competency of a first-order ontology
we report on the results of evaluating the competency of a first-order ontology for its use with automated theorem provers ( atps ) . the evaluation follows the adaptation of the methodology based on competency questions ( cqs ) [ gr\ '' uninger & fox,1995 ] to the framework of first-order logic , which is presented in [ \'alvez & lucio & rigau,2015 ] , and is applied to adimen-sumo [ \'alvez & lucio & rigau,2015 ] . the set of cqs used for this evaluation has been automatically generated from a small set of semantic patterns and the mapping of wordnet to sumo . analysing the results , we can conclude that it is feasible to use atps for working with adimen-sumo v2.4 , enabling the resolution of goals by means of performing non-trivial inferences .

a programming language with a pomdp inside
we present poaps , a novel planning system for defining partially observable markov decision processes ( pomdps ) that abstracts away from pomdp details for the benefit of non-expert practitioners . poaps includes an expressive adaptive programming language based on lisp that has constructs for choice points that can be dynamically optimized . non-experts can use our language to write adaptive programs that have partially observable components without needing to specify belief/hidden states or reason about probabilities . poaps is also a compiler that defines and performs the transformation of any program written in our language into a pomdp with control knowledge . we demonstrate the generality and power of poaps in the rapidly growing domain of human computation by describing its expressiveness and simplicity by writing several poaps programs for common crowdsourcing tasks .

exploiting subgraph structure in multi-robot path planning
multi-robot path planning is difficult due to the combinatorial explosion of the search space with every new robot added . complete search of the combined state-space soon becomes intractable . in this paper we present a novel form of abstraction that allows us to plan much more efficiently . the key to this abstraction is the partitioning of the map into subgraphs of known structure with entry and exit restrictions which we can represent compactly . planning then becomes a search in the much smaller space of subgraph configurations . once an abstract plan is found , it can be quickly resolved into a correct ( but possibly sub-optimal ) concrete plan without the need for further search . we prove that this technique is sound and complete and demonstrate its practical effectiveness on a real map . a contending solution , prioritised planning , is also evaluated and shown to have similar performance albeit at the cost of completeness . the two approaches are not necessarily conflicting ; we demonstrate how they can be combined into a single algorithm which outperforms either approach alone .

liftago on-demand transport dataset and market formation algorithm based on machine learning
this document serves as a technical report for the analysis of on-demand transport dataset . moreover we show how the dataset can be used to develop a market formation algorithm based on machine learning . data used in this work comes from liftago , a prague based company which connects taxi drivers and customers through a smartphone app . the dataset is analysed from the machine-learning perspective : we give an overview of features available as well as results of feature ranking . later we propose the simple data-driven market formation ( sidmaf ) algorithm which aims to improve a relevance while connecting customers with relevant drivers . we compare the heuristics currently used by liftago with sidmaf using two key performance indicators .

modeling the mind : a brief review
the brain is a powerful tool used to achieve amazing feats . there have been several significant advances in neuroscience and artificial brain research in the past two decades . this article is a review of such advances , ranging from the concepts of connectionism , to neural network architectures and high-dimensional representations . there have also been advances in biologically inspired cognitive architectures of which we will cite a few . we will be positioning relatively specific models in a much broader perspective , while comparing and contrasting their advantages and weaknesses . the projects presented are targeted to model the brain at different levels , utilizing different methodologies .

parameterized complexity of problems in coalitional resource games
coalition formation is a key topic in multi-agent systems . coalitions enable agents to achieve goals that they may not have been able to achieve on their own . previous work has shown problems in coalitional games to be computationally hard . wooldridge and dunne ( artificial intelligence 2006 ) studied the classical computational complexity of several natural decision problems in coalitional resource games ( crg ) - games in which each agent is endowed with a set of resources and coalitions can bring about a set of goals if they are collectively endowed with the necessary amount of resources . the input of coalitional resource games bundles together several elements , e.g. , the agent set ag , the goal set g , the resource set r , etc . shrot , aumann and kraus ( aamas 2009 ) examine coalition formation problems in the crg model using the theory of parameterized complexity . their refined analysis shows that not all parts of input act equal - some instances of the problem are indeed tractable while others still remain intractable . we answer an important question left open by shrot , aumann and kraus by showing that the sc problem ( checking whether a coalition is successful ) is w [ 1 ] -hard when parameterized by the size of the coalition . then via a single theme of reduction from sc , we are able to show that various problems related to resources , resource bounds and resource conflicts introduced by wooldridge et al are 1. w [ 1 ] -hard or co-w [ 1 ] -hard when parameterized by the size of the coalition . 2. para-np-hard or co-para-np-hard when parameterized by |r| . 3. fpt when parameterized by either |g| or |ag|+|r| .

born to learn : the inspiration , progress , and future of evolved plastic artificial neural networks
biological plastic neural networks are systems of extraordinary computational capabilities shaped by evolution , development , and lifetime learning . the interplay of these elements leads to the emergence of adaptive behavior and intelligence . inspired by such intricate natural phenomena , evolved plastic artificial neural networks ( epanns ) use simulated evolution in-silico to breed plastic neural networks with a large variety of dynamics , architectures , and plasticity rules : these artificial systems are composed of inputs , outputs , and plastic components that change in response to experiences in an environment . these systems may autonomously discover novel adaptive algorithms , and lead to hypotheses on the emergence of biological adaptation . epanns have seen considerable progress over the last two decades . current scientific and technological advances in artificial neural networks are now setting the conditions for radically new approaches and results . in particular , the limitations of hand-designed networks could be overcome by more flexible and innovative solutions . this paper brings together a variety of inspiring ideas that define the field of epanns . the main methods and results are reviewed . finally , new opportunities and developments are presented .

cross-lingual entity alignment via joint attribute-preserving embedding
entity alignment is the task of finding entities in two knowledge bases ( kbs ) that represent the same real-world object . when facing kbs in different natural languages , conventional cross-lingual entity alignment methods rely on machine translation to eliminate the language barriers . these approaches often suffer from the uneven quality of translations between languages . while recent embedding-based techniques encode entities and relationships in kbs and do not need machine translation for cross-lingual entity alignment , a significant number of attributes remain largely unexplored . in this paper , we propose a joint attribute-preserving embedding model for cross-lingual entity alignment . it jointly embeds the structures of two kbs into a unified vector space and further refines it by leveraging attribute correlations in the kbs . our experimental results on real-world datasets show that this approach significantly outperforms the state-of-the-art embedding approaches for cross-lingual entity alignment and could be complemented with methods based on machine translation .

a semantic approach for the requirement-driven discovery of web services in the life sciences
research in the life sciences depends on the integration of large , distributed and heterogeneous data sources and web services . the discovery of which of these resources are the most appropriate to solve a given task is a complex research question , since there is a large amount of plausible candidates and there is little , mostly unstructured , metadata to be able to decide among them.we contribute a semi-automatic approach , based on semantic techniques , to assist researchers in the discovery of the most appropriate web services to full a set of given requirements .

relating complexity-theoretic parameters with sat solver performance
over the years complexity theorists have proposed many structural parameters to explain the surprising efficiency of conflict-driven clause-learning ( cdcl ) sat solvers on a wide variety of large industrial boolean instances . while some of these parameters have been studied empirically , until now there has not been a unified comparative study of their explanatory power on a comprehensive benchmark . we correct this state of affairs by conducting a large-scale empirical evaluation of cdcl sat solver performance on nearly 7000 industrial and crafted formulas against several structural parameters such as backdoors , treewidth , backbones , and community structure . our study led us to several results . first , we show that while such parameters only weakly correlate with cdcl solving time , certain combinations of them yield much better regression models . second , we show how some parameters can be used as a `` lens '' to better understand the efficiency of different solving heuristics . finally , we propose a new complexity-theoretic parameter , which we call learning-sensitive with restarts ( lsr ) backdoors , that extends the notion of learning-sensitive ( ls ) backdoors to incorporate restarts and discuss algorithms to compute them . we mathematically prove that for certain class of instances minimal lsr-backdoors are exponentially smaller than minimal-ls backdoors .

xml framework for concept description and knowledge representation
an xml framework for concept description is given , based upon the fact that the tree structure of xml implies the logical structure of concepts as defined by attributional calculus . especially , the attribute-value representation is implementable in the xml framework . since the attribute-value representation is an important way to represent knowledge in ai , the framework offers a further and simpler way than the powerful rdf technology .

convolutional neural networks that teach microscopes how to image
deep learning algorithms offer a powerful means to automatically analyze the content of medical images . however , many biological samples of interest are primarily transparent to visible light and contain features that are difficult to resolve with a standard optical microscope . here , we use a convolutional neural network ( cnn ) not only to classify images , but also to optimize the physical layout of the imaging device itself . we increase the classification accuracy of a microscope 's recorded images by merging an optical model of image formation into the pipeline of a cnn . the resulting network simultaneously determines an ideal illumination arrangement to highlight important sample features during image acquisition , along with a set of convolutional weights to classify the detected images post-capture . we demonstrate our joint optimization technique with an experimental microscope configuration that automatically identifies malaria-infected cells with 5-10 % higher accuracy than standard and alternative microscope lighting designs .

from data to city indicators : a knowledge graph for supporting automatic generation of dashboards
in the context of smart cities , indicator definitions have been used to calculate values that enable the comparison among different cities . the calculation of an indicator values has challenges as the calculation may need to combine some aspects of quality while addressing different levels of abstraction . knowledge graphs ( kgs ) have been used successfully to support flexible representation , which can support improved understanding and data analysis in similar settings . this paper presents an operational description for a city kg , an indicator ontology that support indicator discovery and data visualization and an application capable of performing metadata analysis to automatically build and display dashboards according to discovered indicators . we describe our implementation in an urban mobility setting .

assumption-based approaches to reasoning with priorities
this paper maps out the relation between different approaches for handling preferences in argumentation with strict rules and defeasible assumptions by offering translations between them . the systems we compare are : non-prioritized defeats i.e . attacks , preference-based defeats , and preference-based defeats extended with reverse defeat .

model revision inference for extensions of first order logic
i am joachim jansen and this is my research summary , part of my application to the doctoral consortium at iclp'14 . i am a phd student in the knowledge representation and reasoning ( krr ) research group , a subgroup of the declarative languages and artificial intelligence ( dtai ) group at the department of computer science at ku leuven . i started my phd in september 2012. my promotor is prof. dr. ir . gerda janssens and my co-promotor is prof. dr. marc denecker . i can be contacted at joachim.jansen @ cs.kuleuven.be or at : room 01.167 celestijnenlaan 200a 3001 heverlee belgium an extended abstract / full version of a paper accepted to be presented at the doctoral consortium of the 30th international conference on logic programming ( iclp 2014 ) , july 19-22 , vienna , austria

on forgetting in tractable propositional fragments
distilling from a knowledge base only the part that is relevant to a subset of alphabet , which is recognized as forgetting , has attracted extensive interests in ai community . in standard propositional logic , a general algorithm of forgetting and its computation-oriented investigation in various fragments whose satisfiability are tractable are still lacking . the paper aims at filling the gap . after exploring some basic properties of forgetting in propositional logic , we present a resolution-based algorithm of forgetting for cnf fragment , and some complexity results about forgetting in horn , renamable horn , q-horn , krom , dnf and cnf fragments of propositional logic .

decentralized supply chain formation : a market protocol and competitive equilibrium analysis
supply chain formation is the process of determining the structure and terms of exchange relationships to enable a multilevel , multiagent production activity . we present a simple model of supply chains , highlighting two characteristic features : hierarchical subtask decomposition , and resource contention . to decentralize the formation process , we introduce a market price system over the resources produced along the chain . in a competitive equilibrium for this system , agents choose locally optimal allocations with respect to prices , and outcomes are optimal overall . to determine prices , we define a market protocol based on distributed , progressive auctions , and myopic , non-strategic agent bidding policies . in the presence of resource contention , this protocol produces better solutions than the greedy protocols common in the artificial intelligence and multiagent systems literature . the protocol often converges to high-value supply chains , and when competitive equilibria exist , typically to approximate competitive equilibria . however , complementarities in agent production technologies can cause the protocol to wastefully allocate inputs to agents that do not produce their outputs . a subsequent decommitment phase recovers a significant fraction of the lost surplus .

'computing ' as information compression by multiple alignment , unification and search
this paper argues that the operations of a 'universal turing machine ' ( utm ) and equivalent mechanisms such as the 'post canonical system ' ( pcs ) - which are widely accepted as definitions of the concept of ` computing ' - may be interpreted as *information compression by multiple alignment , unification and search* ( icmaus ) . the motivation for this interpretation is that it suggests ways in which the utm/pcs model may be augmented in a proposed new computing system designed to exploit the icmaus principles as fully as possible . the provision of a relatively sophisticated search mechanism in the proposed 'sp ' system appears to open the door to the integration and simplification of a range of functions including unsupervised inductive learning , best-match pattern recognition and information retrieval , probabilistic reasoning , planning and problem solving , and others . detailed consideration of how the icmaus principles may be applied to these functions is outside the scope of this article but relevant sources are cited in this article .

direction-aware semi-dense slam
to aide simultaneous localization and mapping ( slam ) , future perception systems will incorporate forms of scene understanding . in a step towards fully integrated probabilistic geometric scene understanding , localization and mapping we propose the first direction-aware semi-dense slam system . it jointly infers the directional stata center world ( scw ) segmentation and a surfel-based semi-dense map while performing real-time camera tracking . the joint scw map model connects a scene-wide bayesian nonparametric dirichlet process von-mises-fisher mixture model ( dp-vmf ) prior on surfel orientations with the local surfel locations via a conditional random field ( crf ) . camera tracking leverages the scw segmentation to improve efficiency via guided observation selection . results demonstrate improved slam accuracy and tracking efficiency at state of the art performance .

reliability assessment of distribution system using fuzzy logic for modelling of transformer and line uncertainties
reliability assessment of distribution system , based on historical data and probabilistic methods , leads to an unreliable estimation of reliability indices since the data for the distribution components are usually inaccurate or unavailable . fuzzy logic is an efficient method to deal with the uncertainty in reliability inputs . in this paper , the ens index along with other commonly used indices in reliability assessment are evaluated for the distribution system using fuzzy logic . accordingly , the influential variables on the failure rate and outage duration time of the distribution components , which are natural or human-made , are explained using proposed fuzzy membership functions . the reliability indices are calculated and compared for different cases of the system operations by simulation on the ieee rbts bus 2. the results of simulation show how utilities can significantly improve the reliability of their distribution system by considering the risk of the influential variables .

exponentiated gradient linucb for contextual multi-armed bandits
we present exponentiated gradient linucb , an algorithm for con-textual multi-armed bandits . this algorithm uses exponentiated gradient to find the optimal exploration of the linucb . within a deliberately designed offline simulation framework we conduct evaluations with real online event log data . the experimental results demonstrate that our algorithm outperforms surveyed algorithms .

towards open-text semantic parsing via multi-task learning of structured embeddings
open-text ( or open-domain ) semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation ( mr ) . unfortunately , large scale systems can not be easily machine-learned due to lack of directly supervised data . we propose here a method that learns to assign mrs to a wide range of text ( using a dictionary of more than 70,000 words , which are mapped to more than 40,000 entities ) thanks to a training scheme that combines learning from wordnet and conceptnet with learning from raw text . the model learns structured embeddings of words , entities and mrs via a multi-task training process operating on these diverse sources of data that integrates all the learnt knowledge into a single system . this work ends up combining methods for knowledge acquisition , semantic parsing , and word-sense disambiguation . experiments on various tasks indicate that our approach is indeed successful and can form a basis for future more sophisticated systems .

computing the ramsey number r ( 4,3,3 ) using abstraction and symmetry breaking
the number $ r ( 4,3,3 ) $ is often presented as the unknown ramsey number with the best chances of being found `` soon '' . yet , its precise value has remained unknown for almost 50 years . this paper presents a methodology based on \emph { abstraction } and \emph { symmetry breaking } that applies to solve hard graph edge-coloring problems . the utility of this methodology is demonstrated by using it to compute the value $ r ( 4,3,3 ) =30 $ . along the way it is required to first compute the previously unknown set $ { \cal r } ( 3,3,3 ; 13 ) $ consisting of 78 { , } 892 ramsey colorings .

beslutstödssystemet dezzy - en översikt
within the scope of the three-year anti-submarine warfare project of the national defence research establishment , the information systems subproject has developed the demonstration prototype dezzy for handling and analysis of intelligence reports concerning foreign underwater activities . -- -- - inom ramen f\ '' or foa : s tre { \aa } riga huvudprojekt ub { \aa } tsskydd har delprojekt informationssystem utvecklat demonstrationsprototypen dezzy till ett beslutsst\ '' odsystem f\ '' or hantering och analys av underr\ '' attelser om fr\ '' ammande undervattensverksamhet .

learnings options end-to-end for continuous action tasks
we present new results on learning temporally extended actions for continuoustasks , using the options framework ( suttonet al . [ 1999b ] , precup [ 2000 ] ) . in orderto achieve this goal we work with the option-critic architecture ( baconet al . [ 2017 ] ) using a deliberation cost and train it with proximal policy optimization ( schulmanet al . [ 2017 ] ) instead of vanilla policy gradient . results on mujoco domains arepromising , but lead to interesting questions aboutwhena given option should beused , an issue directly connected to the use of initiation sets .

generalizing modular logic programs
even though modularity has been studied extensively in conventional logic programming , there are few approaches on how to incorporate modularity into answer set programming , a prominent rule-based declarative programming paradigm . a major approach is oikarinnen and janhunen 's gaifman-shapiro-style architecture of program modules , which provides the composition of program modules . their module theorem properly strengthens lifschitz and turner 's splitting set theorem for normal logic programs . however , this approach is limited by module conditions that are imposed in order to ensure the compatibility of their module system with the stable model semantics , namely forcing output signatures of composing modules to be disjoint and disallowing positive cyclic dependencies between different modules . these conditions turn out to be too restrictive in practice and in this paper we discuss alternative ways of lift both restrictions independently , effectively solving the first , widening the applicability of this framework and the scope of the module theorem .

hyper-dimensional computing for a visual question-answering system that is trainable end-to-end
in this work we propose a system for visual question answering . our architecture is composed of two parts , the first part creates the logical knowledge base given the image . the second part evaluates questions against the knowledge base . differently from previous work , the knowledge base is represented using hyper-dimensional computing . this choice has the advantage that all the operations in the system , namely creating the knowledge base and evaluating the questions against it , are differentiable , thereby making the system easily trainable in an end-to-end fashion .

testing hypotheses by regularized maximum mean discrepancy
do two data samples come from different distributions ? recent studies of this fundamental problem focused on embedding probability distributions into sufficiently rich characteristic reproducing kernel hilbert spaces ( rkhss ) , to compare distributions by the distance between their embeddings . we show that regularized maximum mean discrepancy ( rmmd ) , our novel measure for kernel-based hypothesis testing , yields substantial improvements even when sample sizes are small , and excels at hypothesis tests involving multiple comparisons with power control . we derive asymptotic distributions under the null and alternative hypotheses , and assess power control . outstanding results are obtained on : challenging eeg data , mnist , the berkley covertype , and the flare-solar dataset .

building the information kernel and the problem of recognition
at this point in time there is a need for a new representation of different information , to identify and organize descending its characteristics . today , science is a powerful tool for the description of reality - the numbers . why the most important property of numbers . suppose we have a number 0.2351734 , it is clear that the figures are there in order of importance . if necessary , we can round the number up to some value , eg 0.235. arguably , the 0,235 - the most important information of 0.2351734. thus , we can reduce the size of numbers is not losing much with the accuracy . clearly , if learning to provide a graphical or audio information kernel , we can provide the most relevant information , discarding the rest . introduction of various kinds of information in an information kernel , is an important task , to solve many problems in artificial intelligence and information theory .

reasoning in systems with elements that randomly switch characteristics
we examine the issue of stability of probability in reasoning about complex systems with uncertainty in structure . normally , propositions are viewed as probability functions on an abstract random graph where it is implicitly assumed that the nodes of the graph have stable properties . but what if some of the nodes change their characteristics ? this is a situation that can not be covered by abstractions of either static or dynamic sets when these changes take place at regular intervals . we propose the use of sets with elements that change , and modular forms are proposed to account for one type of such change . an expression for the dependence of the mean on the probability of the switching elements has been determined . the system is also analyzed from the perspective of decision between different hypotheses . such sets are likely to be of use in complex system queries and in analysis of surveys .

neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning
model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills , but typically require a very large number of samples to achieve good performance . model-based algorithms , in principle , can provide for much more efficient learning , but have proven difficult to extend to expressive , high-capacity models such as deep neural networks . in this work , we demonstrate that medium-sized neural network models can in fact be combined with model predictive control ( mpc ) to achieve excellent sample complexity in a model-based reinforcement learning algorithm , producing stable and plausible gaits to accomplish various complex locomotion tasks . we also propose using deep neural network dynamics models to initialize a model-free learner , in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods . we empirically demonstrate on mujoco locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency , and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks , achieving sample efficiency gains of 3-5x on swimmer , cheetah , hopper , and ant agents . videos can be found at https : //sites.google.com/view/mbmf

towards an intelligent database system founded on the sp theory of computing and cognition
the sp theory of computing and cognition , described in previous publications , is an attractive model for intelligent databases because it provides a simple but versatile format for different kinds of knowledge , it has capabilities in artificial intelligence , and it can also function like established database models when that is required . this paper describes how the sp model can emulate other models used in database applications and compares the sp model with those other models . the artificial intelligence capabilities of the sp model are reviewed and its relationship with other artificial intelligence systems is described . also considered are ways in which current prototypes may be translated into an 'industrial strength ' working system .

separation of concerns in reinforcement learning
in this paper , we propose a framework for solving a single-agent task by using multiple agents , each focusing on different aspects of the task . this approach has two main advantages : 1 ) it allows for training specialized agents on different parts of the task , and 2 ) it provides a new way to transfer knowledge , by transferring trained agents . our framework generalizes the traditional hierarchical decomposition , in which , at any moment in time , a single agent has control until it has solved its particular subtask . we illustrate our framework with empirical experiments on two domains .

automated big text security classification
in recent years , traditional cybersecurity safeguards have proven ineffective against insider threats . famous cases of sensitive information leaks caused by insiders , including the wikileaks release of diplomatic cables and the edward snowden incident , have greatly harmed the u.s. government 's relationship with other governments and with its own citizens . data leak prevention ( dlp ) is a solution for detecting and preventing information leaks from within an organization 's network . however , state-of-art dlp detection models are only able to detect very limited types of sensitive information , and research in the field has been hindered due to the lack of available sensitive texts . many researchers have focused on document-based detection with artificially labeled `` confidential documents '' for which security labels are assigned to the entire document , when in reality only a portion of the document is sensitive . this type of whole-document based security labeling increases the chances of preventing authorized users from accessing non-sensitive information within sensitive documents . in this paper , we introduce automated classification enabled by security similarity ( acess ) , a new and innovative detection model that penetrates the complexity of big text security classification/detection . to analyze the acess system , we constructed a novel dataset , containing formerly classified paragraphs from diplomatic cables made public by the wikileaks organization . to our knowledge this paper is the first to analyze a dataset that contains actual formerly sensitive information annotated at paragraph granularity .

the lexicographic closure as a revision process
the connections between nonmonotonic reasoning and belief revision are well-known . a central problem in the area of nonmonotonic reasoning is the problem of default entailment , i.e. , when should an item of default information representing `` if a is true then , normally , b is true '' be said to follow from a given set of items of such information . many answers to this question have been proposed but , surprisingly , virtually none have attempted any explicit connection to belief revision . the aim of this paper is to give an example of how such a connection can be made by showing how the lexicographic closure of a set of defaults may be conceptualised as a process of iterated revision by sets of sentences . specifically we use the revision process of nayak .

bandit models of human behavior : reward processing in mental disorders
drawing an inspiration from behavioral studies of human decision making , we propose here a general parametric framework for multi-armed bandit problem , which extends the standard thompson sampling approach to incorporate reward processing biases associated with several neurological and psychiatric conditions , including parkinson 's and alzheimer 's diseases , attention-deficit/hyperactivity disorder ( adhd ) , addiction , and chronic pain . we demonstrate empirically that the proposed parametric approach can often outperform the baseline thompson sampling on a variety of datasets . moreover , from the behavioral modeling perspective , our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions .

exploiting variable associations to configure efficient local search algorithms in large-scale binary integer programs
we present a data mining approach for reducing the search space of local search algorithms in a class of binary integer programs including the set covering and partitioning problems . the quality of locally optimal solutions typically improves if a larger neighborhood is used , while the computation time of searching the neighborhood increases exponentially . to overcome this , we extract variable associations from the instance to be solved in order to identify promising pairs of flipping variables in the neighborhood search . based on this , we develop a 4-flip neighborhood local search algorithm that incorporates an efficient incremental evaluation of solutions and an adaptive control of penalty weights . computational results show that the proposed method improves the performance of the local search algorithm for large-scale set covering and partitioning problems .

representation learning for visual-relational knowledge graphs
a visual-relational knowledge graph ( kg ) is a kg whose entities are associated with images . we propose representation learning for relation and entity prediction in visual-relational kgs as a novel machine learning problem . we introduce \textsc { imagegraph } , a kg with 1,330 relation types , 14,870 entities , and 829,931 images . visual-relational kgs lead to novel probabilistic query types treating images as first-class citizens . we approach the query answering problems by combining ideas from the areas of computer vision and embedding learning for kgs . the resulting ml models can answer queries such as \textit { `` how are these two unseen images related to each other ? '' } we also explore a novel zero-shot learning scenario where an image of an entirely new entity is linked with multiple relations to entities of an existing kg . our experiments show that the proposed deep neural networks are able to answer the visual-relational queries efficiently and accurately .

conceptual spaces for cognitive architectures : a lingua franca for different levels of representation
during the last decades , many cognitive architectures ( cas ) have been realized adopting different assumptions about the organization and the representation of their knowledge level . some of them ( e.g . soar [ laird ( 2012 ) ] ) adopt a classical symbolic approach , some ( e.g . leabra [ o'reilly and munakata ( 2000 ) ] ) are based on a purely connectionist model , while others ( e.g . clarion [ sun ( 2006 ) ] adopt a hybrid approach combining connectionist and symbolic representational levels . additionally , some attempts ( e.g . bisoar ) trying to extend the representational capacities of cas by integrating diagrammatical representations and reasoning are also available [ kurup and chandrasekaran ( 2007 ) ] . in this paper we propose a reflection on the role that conceptual spaces , a framework developed by peter g\ '' ardenfors [ g\ '' ardenfors ( 2000 ) ] more than fifteen years ago , can play in the current development of the knowledge level in cognitive systems and architectures . in particular , we claim that conceptual spaces offer a lingua franca that allows to unify and generalize many aspects of the symbolic , sub-symbolic and diagrammatic approaches ( by overcoming some of their typical problems ) and to integrate them on a common ground . in doing so we extend and detail some of the arguments explored by g\ '' ardenfors [ g\ '' ardenfors ( 1997 ) ] for defending the need of a conceptual , intermediate , representation level between the symbolic and the sub-symbolic one .

emergent translation in multi-agent communication
while most machine translation systems to date are trained on large parallel corpora , humans learn language in a different way : by being grounded in an environment and interacting with other humans . in this work , we propose a communication game where two agents , native speakers of their own respective languages , jointly learn to solve a visual referential task . we find that the ability to understand and translate a foreign language emerges as a means to achieve shared goals . the emergent translation is interactive and multimodal , and crucially does not require parallel corpora , but only monolingual , independent text and corresponding images . our proposed translation model achieves this by grounding the source and target languages into a shared visual modality , and outperforms several baselines on both word-level and sentence-level translation tasks . furthermore , we show that agents in a multilingual community learn to translate better and faster than in a bilingual communication setting .

improvement/extension of modular systems as combinatorial reengineering ( survey )
the paper describes development ( improvement/extension ) approaches for composite ( modular ) systems ( as combinatorial reengineering ) . the following system improvement/extension actions are considered : ( a ) improvement of systems component ( s ) ( e.g. , improvement of a system component , replacement of a system component ) ; ( b ) improvement of system component interconnection ( compatibility ) ; ( c ) joint improvement improvement of system components ( s ) and their interconnection ; ( d ) improvement of system structure ( replacement of system part ( s ) , addition of a system part , deletion of a system part , modification of system structure ) . the study of system improvement approaches involve some crucial issues : ( i ) scales for evaluation of system components and component compatibility ( quantitative scale , ordinal scale , poset-like scale , scale based on interval multiset estimate ) , ( ii ) evaluation of integrated system quality , ( iii ) integration methods to obtain the integrated system quality . the system improvement/extension strategies can be examined as seleciton/combination of the improvement action ( s ) above and as modification of system structure . the strategies are based on combinatorial optimization problems ( e.g. , multicriteria selection , knapsack problem , multiple choice problem , combinatorial synthesis based on morphological clique problem , assignment/reassignment problem , graph recoloring problem , spanning problems , hotlink assignment ) . here , heuristics are used . various system improvement/extension strategies are presented including illustrative numerical examples .

validation of enhanced emotion enabled cognitive agent using virtual overlay multi-agent system approach
making roads safer by avoiding road collisions is one of the main reasons for inventing autonomous vehicles ( avs ) . in this context , designing agent-based collision avoidance components of avs which truly represent human cognition and emotions look is a more feasible approach as agents can replace human drivers . however , to the best of our knowledge , very few human emotion and cognition-inspired agent-based studies have previously been conducted in this domain . furthermore , these agent-based solutions have not been validated using any key validation technique . keeping in view this lack of validation practices , we have selected state-of-the-art emotion enabled cognitive agent ( eec_agent ) , which was proposed to avoid lateral collisions between semi-avs . the architecture of eec_agent has been revised using exploratory agent based modeling ( eabm ) level of the cognitive agent based computing ( cabc ) framework and real-time fear emotion generation mechanism using the ortony , clore & collins ( occ ) model has also been introduced . then the proposed fear generation mechanism has been validated using the validated agent based modeling level of cabc framework using a virtual overlay multiagent system ( vomas ) . extensive simulation and practical experiments demonstrate that the enhanced eec_agent exhibits the capability to feel different levels of fear , according to different traffic situations and also needs a smaller stopping sight distance ( ssd ) and overtaking sight distance ( osd ) as compared to human drivers .

a microkernel architecture for constraint programming
this paper presents a microkernel architecture for constraint programming organized around a number of small number of core functionalities and minimal interfaces . the architecture contrasts with the monolithic nature of many implementations . experimental results indicate that the software engineering benefits are not incompatible with runtime efficiency .

ramp : fast frequent itemset mining with efficient bit-vector projection technique
mining frequent itemset using bit-vector representation approach is very efficient for dense type datasets , but highly inefficient for sparse datasets due to lack of any efficient bit-vector projection technique . in this paper we present a novel efficient bit-vector projection technique , for sparse and dense datasets . to check the efficiency of our bit-vector projection technique , we present a new frequent itemset mining algorithm ramp ( real algorithm for mining patterns ) build upon our bit-vector projection technique . the performance of the ramp is compared with the current best ( all , maximal and closed ) frequent itemset mining algorithms on benchmark datasets . different experimental results on sparse and dense datasets show that mining frequent itemset using ramp is faster than the current best algorithms , which show the effectiveness of our bit-vector projection idea . we also present a new local maximal frequent itemsets propagation and maximal itemset superset checking approach fastlmfi , build upon our pbr bit-vector projection technique . our different computational experiments suggest that itemset maximality checking using fastlmfi is fast and efficient than a previous will known progressive focusing approach .

scalable algorithms for tractable schatten quasi-norm minimization
the schatten-p quasi-norm $ ( 0 < p < 1 ) $ is usually used to replace the standard nuclear norm in order to approximate the rank function more accurately . however , existing schatten-p quasi-norm minimization algorithms involve singular value decomposition ( svd ) or eigenvalue decomposition ( evd ) in each iteration , and thus may become very slow and impractical for large-scale problems . in this paper , we first define two tractable schatten quasi-norms , i.e. , the frobenius/nuclear hybrid and bi-nuclear quasi-norms , and then prove that they are in essence the schatten-2/3 and 1/2 quasi-norms , respectively , which lead to the design of very efficient algorithms that only need to update two much smaller factor matrices . we also design two efficient proximal alternating linearized minimization algorithms for solving representative matrix completion problems . finally , we provide the global convergence and performance guarantees for our algorithms , which have better convergence properties than existing algorithms . experimental results on synthetic and real-world data show that our algorithms are more accurate than the state-of-the-art methods , and are orders of magnitude faster .

algorithms for computing the greatest simulations and bisimulations between fuzzy automata
recently , two types of simulations ( forward and backward simulations ) and four types of bisimulations ( forward , backward , forward-backward , and backward-forward bisimulations ) between fuzzy automata have been introduced . if there is at least one simulation/bisimulation of some of these types between the given fuzzy automata , it has been proved that there is the greatest simulation/bisimulation of this kind . in the present paper , for any of the above-mentioned types of simulations/bisimulations we provide an effective algorithm for deciding whether there is a simulation/bisimulation of this type between the given fuzzy automata , and for computing the greatest one , whenever it exists . the algorithms are based on the method developed in [ j. ignjatovi\'c , m. \'ciri\'c , s. bogdanovi\'c , on the greatest solutions to certain systems of fuzzy relation inequalities and equations , fuzzy sets and systems 161 ( 2010 ) 3081-3113 ] , which comes down to the computing of the greatest post-fixed point , contained in a given fuzzy relation , of an isotone function on the lattice of fuzzy relations .

hierarchy through composition with linearly solvable markov decision processes
hierarchical architectures are critical to the scalability of reinforcement learning methods . current hierarchical frameworks execute actions serially , with macro-actions comprising sequences of primitive actions . we propose a novel alternative to these control hierarchies based on concurrent execution of many actions in parallel . our scheme uses the concurrent compositionality provided by the linearly solvable markov decision process ( lmdp ) framework , which naturally enables a learning agent to draw on several macro-actions simultaneously to solve new tasks . we introduce the multitask lmdp module , which maintains a parallel distributed representation of tasks and may be stacked to form deep hierarchies abstracted in space and time .

method and apparatus for automatic text input insertion in digital devices with a restricted number of keys
a device which contains number of symbol input keys , where the number of available keys is less than the number of symbols of an alphabet of any given language , screen , and dynamic reordering table of the symbols which are mapped onto those keys , according to a disambiguation method based on the previously entered symbols . the device incorporates a previously entered keystrokes tracking mechanism , and the key selected by the user detector , as well as a mechanism to select the dynamic symbol reordering mapped onto this key according to the information contained to the reordering table . the reordering table occurs from a disambiguation method which reorders the symbol appearance . the reordering information occurs from bayesian belief network construction and training from text corpora of the specific language .

constructing folksonomies from user-specified relations on flickr
many social web sites allow users to publish content and annotate with descriptive metadata . in addition to flat tags , some social web sites have recently began to allow users to organize their content and metadata hierarchically . the social photosharing site flickr , for example , allows users to group related photos in sets , and related sets in collections . the social bookmarking site del.icio.us similarly lets users group related tags into bundles . although the sites themselves do n't impose any constraints on how these hierarchies are used , individuals generally use them to capture relationships between concepts , most commonly the broader/narrower relations . collective annotation of content with hierarchical relations may lead to an emergent classification system , called a folksonomy . while some researchers have explored using tags as evidence for learning folksonomies , we believe that hierarchical relations described above offer a high-quality source of evidence for this task . we propose a simple approach to aggregate shallow hierarchies created by many distinct flickr users into a common folksonomy . our approach uses statistics to determine if a particular relation should be retained or discarded . the relations are then woven together into larger hierarchies . although we have not carried out a detailed quantitative evaluation of the approach , it looks very promising since it generates very reasonable , non-trivial hierarchies .

( dual ) hoops have unique halving
continuous logic extends the multi-valued lukasiewicz logic by adding a halving operator on propositions . this extension is designed to give a more satisfactory model theory for continuous structures . the semantics of these logics can be given using specialisations of algebraic structures known as hoops . as part of an investigation into the metatheory of propositional continuous logic , we were indebted to prover9 for finding a proof of an important algebraic law .

posterior sampling for large scale reinforcement learning
posterior sampling for reinforcement learning ( psrl ) is a popular algorithm for learning to control an unknown markov decision process ( mdp ) . psrl maintains a distribution over mdp parameters and in an episodic fashion samples mdp parameters , computes the optimal policy for them and executes it . a special case of psrl is where at the end of each episode the mdp resets to the initial state distribution . extensions of this idea to general mdps without state resetting has so far produced non-practical algorithms and in some cases buggy theoretical analysis . this is due to the difficulty of analyzing regret under episode switching schedules that depend on random variables of the true underlying model . we propose a solution to this problem that involves using a deterministic , model-independent episode switching schedule , and establish a bayes regret bound under mild assumptions . our algorithm termed deterministic schedule psrl ( ds-psrl ) is efficient in terms of time , sample , and space complexity . our result is more generally applicable to continuous state action problems . we demonstrate how this algorithm is well suited for sequential recommendation problems such as points of interest ( poi ) . we derive a general procedure for parameterizing the underlying mdps , to create action condition dynamics from passive data , that do not contain actions . we prove that such parameterization satisfies the assumptions of our analysis .

programmable agents
we build deep rl agents that execute declarative programs expressed in formal language . the agents learn to ground the terms in this language in their environment , and can generalize their behavior at test time to execute new programs that refer to objects that were not referenced during training . the agents develop disentangled interpretable representations that allow them to generalize to a wide variety of zero-shot semantic tasks .

teacher-student curriculum learning
we propose teacher-student curriculum learning ( tscl ) , a framework for automatic curriculum learning , where the student tries to learn a complex task and the teacher automatically chooses subtasks from a given set for the student to train on . we describe a family of teacher algorithms that rely on the intuition that the student should practice more those tasks on which it makes the fastest progress , i.e . where the slope of the learning curve is highest . in addition , the teacher algorithms address the problem of forgetting by also choosing tasks where the student 's performance is getting worse . we demonstrate that tscl matches or surpasses the results of carefully hand-crafted curricula in two tasks : addition of decimal numbers with lstm and navigation in minecraft . using our automatically generated curriculum enabled to solve a minecraft maze that could not be solved at all when training directly on solving the maze , and the learning was an order of magnitude faster than uniform sampling of subtasks .

classifying signals with local classifiers
this paper deals with the problem of classifying signals . the new method for building so called local classifiers and local features is presented . the method is a combination of the lifting scheme and the support vector machines . its main aim is to produce effective and yet comprehensible classifiers that would help in understanding processes hidden behind classified signals . to illustrate the method we present the results obtained on an artificial and a real dataset .

a mip backend for the idp system
the idp knowledge base system currently uses minisat ( id ) as its backend constraint programming ( cp ) solver . a few similar systems have used a mixed integer programming ( mip ) solver as backend . however , so far little is known about when the mip solver is preferable . this paper explores this question . it describes the use of cplex as a backend for idp and reports on experiments comparing both backends .

the power of randomization : distributed submodular maximization on massive datasets
a wide variety of problems in machine learning , including exemplar clustering , document summarization , and sensor placement , can be cast as constrained submodular maximization problems . unfortunately , the resulting submodular optimization problems are often too large to be solved on a single machine . we develop a simple distributed algorithm that is embarrassingly parallel and it achieves provable , constant factor , worst-case approximation guarantees . in our experiments , we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting .

parameter sharing deep deterministic policy gradient for cooperative multi-agent reinforcement learning
deep reinforcement learning for multi-agent cooperation and competition has been a hot topic recently . this paper focuses on cooperative multi-agent problem based on actor-critic methods under local observations settings . multi agent deep deterministic policy gradient obtained state of art results for some multi-agent games , whereas , it can not scale well with growing amount of agents . in order to boost scalability , we propose a parameter sharing deterministic policy gradient method with three variants based on neural networks , including actor-critic sharing , actor sharing and actor sharing with partially shared critic . benchmarks from rllab show that the proposed method has advantages in learning speed and memory efficiency , well scales with growing amount of agents , and moreover , it can make full use of reward sharing and exchangeability if possible .

a spectral algorithm for learning hidden markov models
hidden markov models ( hmms ) are one of the most fundamental and widely used statistical tools for modeling discrete time series . in general , learning hmms from data is computationally hard ( under cryptographic assumptions ) , and practitioners typically resort to search heuristics which suffer from the usual local optima issues . we prove that under a natural separation condition ( bounds on the smallest singular value of the hmm parameters ) , there is an efficient and provably correct algorithm for learning hmms . the sample complexity of the algorithm does not explicitly depend on the number of distinct ( discrete ) observations -- -it implicitly depends on this quantity through spectral properties of the underlying hmm . this makes the algorithm particularly applicable to settings with a large number of observations , such as those in natural language processing where the space of observation is sometimes the words in a language . the algorithm is also simple , employing only a singular value decomposition and matrix multiplications .

automatic synthesis of geometry problems for an intelligent tutoring system
this paper presents an intelligent tutoring system , geotutor , for euclidean geometry that is automatically able to synthesize proof problems and their respective solutions given a geometric figure together with a set of properties true of it . geotutor can provide personalized practice problems that address student deficiencies in the subject matter .

posterior dispersion indices
probabilistic modeling is cyclical : we specify a model , infer its posterior , and evaluate its performance . evaluation drives the cycle , as we revise our model based on how it performs . this requires a metric . traditionally , predictive accuracy prevails . yet , predictive accuracy does not tell the whole story . we propose to evaluate a model through posterior dispersion . the idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure . we propose a family of posterior dispersion indices ( pdi ) that capture this idea . a pdi identifies rich patterns of model mismatch in three real data examples : voting preferences , supermarket shopping , and population genetics .

oil price trackers inspired by immune memory
we outline initial concepts for an immune inspired algorithm to evaluate and predict oil price time series data . the proposed solution evolves a short term pool of trackers dynamically , with each member attempting to map trends and anticipate future price movements . successful trackers feed into a long term memory pool that can generalise across repeating trend patterns . the resulting sequence of trackers , ordered in time , can be used as a forecasting tool . examination of the pool of evolving trackers also provides valuable insight into the properties of the crude oil market .

fuzzy human motion analysis : a review
human motion analysis ( hma ) is currently one of the most popularly active research domains as such significant research interests are motivated by a number of real world applications such as video surveillance , sports analysis , healthcare monitoring and so on . however , most of these real world applications face high levels of uncertainties that can affect the operations of such applications . hence , the fuzzy set theory has been applied and showed great success in the recent past . in this paper , we aim at reviewing the fuzzy set oriented approaches for hma , individuating how the fuzzy set may improve the hma , envisaging and delineating the future perspectives . to the best of our knowledge , there is not found a single survey in the current literature that has discussed and reviewed fuzzy approaches towards the hma . for ease of understanding , we conceptually classify the human motion into three broad levels : low-level ( lol ) , mid-level ( mil ) , and high-level ( hil ) hma .

detecting falls with x-factor hidden markov models
identification of falls while performing normal activities of daily living ( adl ) is important to ensure personal safety and well-being . however , falling is a short term activity that occurs infrequently . this poses a challenge to traditional classification algorithms , because there may be very little training data for falls ( or none at all ) . this paper proposes an approach for the identification of falls using a wearable device in the absence of training data for falls but with plentiful data for normal adl . we propose three ` x-factor ' hidden markov model ( xhmms ) approaches . the xhmms model unseen falls using `` inflated '' output covariances ( observation models ) . to estimate the inflated covariances , we propose a novel cross validation method to remove `` outliers '' from the normal adl that serve as proxies for the unseen falls and allow learning the xhmms using only normal activities . we tested the proposed xhmm approaches on two activity recognition datasets and show high detection rates for falls in the absence of fall-specific training data . we show that the traditional method of choosing a threshold based on maximum of negative of log-likelihood to identify unseen falls is ill-posed for this problem . we also show that supervised classification methods perform poorly when very limited fall data are available during the training phase .

agm-style revision of beliefs and intentions from a database perspective ( preliminary version )
we introduce a logic for temporal beliefs and intentions based on shoham 's database perspective . we separate strong beliefs from weak beliefs . strong beliefs are independent from intentions , while weak beliefs are obtained by adding intentions to strong beliefs and everything that follows from that . we formalize coherence conditions on strong beliefs and intentions . we provide agm-style postulates for the revision of strong beliefs and intentions . we show in a representation theorem that a revision operator satisfying our postulates can be represented by a pre-order on interpretations of the beliefs , together with a selection function for the intentions .

a tool for implementation of a domain model based on fuzzy relationships
the domain model is one of the important components used by adaptive learning systems to automatically generate customized courses for the learners . in this paper our contribution is to propose a new tool for implementation of a domain model based on fuzzy relationships among concepts . this tool allows the experts and teachers to find the best parameters in order to adapt the learners 's differences .

neuronal spectral analysis of eeg and expert knowledge integration for automatic classification of sleep stages
being able to analyze and interpret signal coming from electroencephalogram ( eeg ) recording can be of high interest for many applications including medical diagnosis and brain-computer interfaces . indeed , human experts are today able to extract from this signal many hints related to physiological as well as cognitive states of the recorded subject and it would be very interesting to perform such task automatically but today no completely automatic system exists . in previous studies , we have compared human expertise and automatic processing tools , including artificial neural networks ( ann ) , to better understand the competences of each and determine which are the difficult aspects to integrate in a fully automatic system . in this paper , we bring more elements to that study in reporting the main results of a practical experiment which was carried out in an hospital for sleep pathology study . an eeg recording was studied and labeled by a human expert and an ann . we describe here the characteristics of the experiment , both human and neuronal procedure of analysis , compare their performances and point out the main limitations which arise from this study .

perfect tree-like markovian distributions
we show that if a strictly positive joint probability distribution for a set of binary random variables factors according to a tree , then vertex separation represents all and only the independence relations enclosed in the distribution . the same result is shown to hold also for multivariate strictly positive normal distributions . our proof uses a new property of conditional independence that holds for these two classes of probability distributions .

geometry and determinism of optimal stationary control in partially observable markov decision processes
it is well known that for any finite state markov decision process ( mdp ) there is a memoryless deterministic policy that maximizes the expected reward . for partially observable markov decision processes ( pomdps ) , optimal memoryless policies are generally stochastic . we study the expected reward optimization problem over the set of memoryless stochastic policies . we formulate this as a constrained linear optimization problem and develop a corresponding geometric framework . we show that any pomdp has an optimal memoryless policy of limited stochasticity , which allows us to reduce the dimensionality of the search space . experiments demonstrate that this approach enables better and faster convergence of the policy gradient on the evaluated systems .

kaggle lshtc4 winning solution
our winning submission to the 2014 kaggle competition for large scale hierarchical text classification ( lshtc ) consists mostly of an ensemble of sparse generative models extending multinomial naive bayes . the base-classifiers consist of hierarchically smoothed models combining document , label , and hierarchy level multinomials , with feature pre-processing using variants of tf-idf and bm25 . additional diversification is introduced by different types of folds and random search optimization for different measures . the ensemble algorithm optimizes macrofscore by predicting the documents for each label , instead of the usual prediction of labels per document . scores for documents are predicted by weighted voting of base-classifier outputs with a variant of feature-weighted linear stacking . the number of documents per label is chosen using label priors and thresholding of vote scores . this document describes the models and software used to build our solution . reproducing the results for our solution can be done by running the scripts included in the kaggle package . a package omitting precomputed result files is also distributed . all code is open source , released under gnu gpl 2.0 , and gpl 3.0 for weka and meka dependencies .

a simple method for decision making in robocup soccer simulation 3d environment
in this paper new hierarchical hybrid fuzzy-crisp methods for decision making and action selection of an agent in soccer simulation 3d environment are presented . first , the skills of an agent are introduced , implemented and classified in two layers , the basicskills and the highlevel skills . in the second layer , a twophase mechanism for decision making is introduced . in phase one , some useful methods are implemented which check the agent 's situation for performing required skills . in the next phase , the team str ategy , team for mation , agent 's role and the agent 's positioning system are introduced . a fuzzy logical approach is employed to recognize the team strategy and further more to tell the player the best position to move . at last , we comprised our implemented algor ithm in the robocup soccer simulation 3d environment and results showed th eefficiency of the introduced methodology .

auditing black-box models using transparent model distillation with side information
black-box risk scoring models permeate our lives , yet are typically proprietary or opaque . we propose a transparent model distillation approach to audit such models . model distillation was first introduced to transfer knowledge from a large , complex teacher model to a faster , simpler student model without significant loss in prediction accuracy . to this we add a third criterion - transparency . to gain insight into black-box models , we treat them as teachers , training transparent student models to mimic the risk scores assigned by the teacher . moreover , we use side information in the form of the actual outcomes the teacher scoring model was intended to predict in the first place . by training a second transparent model on the outcomes , we can compare the two models to each other . when comparing models trained on risk scores to models trained on outcomes , we show that it is necessary to calibrate the risk-scoring model 's predictions to remove distortion that may have been added to the black-box risk-scoring model during or after its training process . we also show how to compute confidence intervals for the particular class of transparent student models we use - tree-based additive models with pairwise interactions ( ga2ms ) - to support comparison of the two transparent models . we demonstrate the methods on four public datasets : compas , lending club , stop-and-frisk , and chicago police .

3d-prnn : generating shape primitives with recurrent neural networks
the success of various applications including robotics , digital content creation , and visualization demand a structured and abstract representation of the 3d world from limited sensor data . inspired by the nature of human perception of 3d shapes as a collection of simple parts , we explore such an abstract shape representation based on primitives . given a single depth image of an object , we present 3d-prnn , a generative recurrent neural network that synthesizes multiple plausible shapes composed of a set of primitives . our generative model encodes symmetry characteristics of common man-made objects , preserves long-range structural coherence , and describes objects of varying complexity with a compact representation . we also propose a method based on gaussian fields to generate a large scale dataset of primitive-based shape representations to train our network . we evaluate our approach on a wide range of examples and show that it outperforms nearest-neighbor based shape retrieval methods and is on-par with voxel-based generative models while using a significantly reduced parameter space .

causality-aided falsification
falsification is drawing attention in quality assurance of heterogeneous systems whose complexities are beyond most verification techniques ' scalability . in this paper we introduce the idea of causality aid in falsification : by providing a falsification solver -- that relies on stochastic optimization of a certain cost function -- with suitable causal information expressed by a bayesian network , search for a falsifying input value can be efficient . our experiment results show the idea 's viability .

sada : a general framework to support robust causation discovery with theoretical guarantee
causation discovery without manipulation is considered a crucial problem to a variety of applications . the state-of-the-art solutions are applicable only when large numbers of samples are available or the problem domain is sufficiently small . motivated by the observations of the local sparsity properties on causal structures , we propose a general split-and-merge framework , named sada , to enhance the scalability of a wide class of causation discovery algorithms . in sada , the variables are partitioned into subsets , by finding causal cut on the sparse causal structure over the variables . by running mainstream causation discovery algorithms as basic causal solvers on the subproblems , complete causal structure can be reconstructed by combining the partial results . sada benefits from the recursive division technique , since each small subproblem generates more accurate result under the same number of samples . we theoretically prove that sada always reduces the scales of problems without sacrifice on accuracy , under the condition of local causal sparsity and reliable conditional independence tests . we also present sufficient condition to accuracy enhancement by sada , even when the conditional independence tests are vulnerable . extensive experiments on both simulated and real-world datasets verify the improvements on scalability and accuracy by applying sada together with existing causation discovery algorithms .

multi-task neural network for non-discrete attribute prediction in knowledge graphs
many popular knowledge graphs such as freebase , yago or dbpedia maintain a list of non-discrete attributes for each entity . intuitively , these attributes such as height , price or population count are able to richly characterize entities in knowledge graphs . this additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs . unfortunately , many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs . in this paper , we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete attribute information in a relational setting . specifically , we train a neural network for triplet prediction along with a separate network for attribute value regression . via multi-task learning , we are able to learn representations of entities , relations and attributes that encode information about both tasks . moreover , such attributes are not only central to many predictive tasks as an information source but also as a prediction target . therefore , models that are able to encode , incorporate and predict such information in a relational learning context are highly attractive as well . we show that our approach outperforms many state-of-the-art methods for the tasks of relational triplet classification and attribute value prediction .

relaxed survey propagation for the weighted maximum satisfiability problem
the survey propagation ( sp ) algorithm has been shown to work well on large instances of the random 3-sat problem near its phase transition . it was shown that sp estimates marginals over covers that represent clusters of solutions . the sp-y algorithm generalizes sp to work on the maximum satisfiability ( max-sat ) problem , but the cover interpretation of sp does not generalize to sp-y . in this paper , we formulate the relaxed survey propagation ( rsp ) algorithm , which extends the sp algorithm to apply to the weighted max-sat problem . we show that rsp has an interpretation of estimating marginals over covers violating a set of clauses with minimal weight . this naturally generalizes the cover interpretation of sp . empirically , we show that rsp outperforms sp-y and other state-of-the-art max-sat solvers on random max-sat instances . rsp also outperforms state-of-the-art weighted max-sat solvers on random weighted max-sat instances .

towards a continuous knowledge learning engine for chatbots
although chatbots have been very popular in recent years , they still have some serious weaknesses which limit the scope of their applications . one major weakness is that they can not learn new knowledge during the conversation process , i.e. , their knowledge is fixed beforehand and can not be expanded or updated during conversation . in this paper , we propose to build a general knowledge learning engine for chatbots to enable them to continuously and interactively learn new knowledge during conversations . as time goes by , they become more and more knowledgeable and better and better at learning and conversation . we model the task as an open-world knowledge base completion problem and propose a novel technique called lifelong interactive learning and inference ( lili ) to solve it . lili works by imitating how humans acquire knowledge and perform inference during an interactive conversation . our experimental results show lili is highly promising .

clingcon : the next generation
we present the third generation of the constraint answer set system clingcon , combining answer set programming ( asp ) with finite domain constraint processing ( cp ) . while its predecessors rely on a black-box approach to hybrid solving by integrating the cp solver gecode , the new clingcon system pursues a lazy approach using dedicated constraint propagators to extend propagation in the underlying asp solver clasp . no extension is needed for parsing and grounding clingcon 's hybrid modeling language since both can be accommodated by the new generic theory handling capabilities of the asp grounder gringo . as a whole , clingcon 3 is thus an extension of the asp system clingo 5 , which itself relies on the grounder gringo and the solver clasp . the new approach of clingcon offers a seamless integration of cp propagation into asp solving that benefits from the whole spectrum of clasp 's reasoning modes , including for instance multi-shot solving and advanced optimization techniques . this is accomplished by a lazy approach that unfolds the representation of constraints and adds it to that of the logic program only when needed . although the unfolding is usually dictated by the constraint propagators during solving , it can already be partially ( or even totally ) done during preprocessing . moreover , clingcon 's constraint preprocessing and propagation incorporate several well established cp techniques that greatly improve its performance . we demonstrate this via an extensive empirical evaluation contrasting , first , the various techniques in the context of csp solving and , second , the new clingcon system with other hybrid asp systems . under consideration in theory and practice of logic programming ( tplp )

an analysis of arithmetic constraints on integer intervals
arithmetic constraints on integer intervals are supported in many constraint programming systems . we study here a number of approaches to implement constraint propagation for these constraints . to describe them we introduce integer interval arithmetic . each approach is explained using appropriate proof rules that reduce the variable domains . we compare these approaches using a set of benchmarks . for the most promising approach we provide results that characterize the effect of constraint propagation . this is a full version of our earlier paper , cs.pl/0403016 .

probabilistic models for computerized adaptive testing
in this paper we follow our previous research in the area of computerized adaptive testing ( cat ) . we present three different methods for cat . one of them , the item response theory , is a well established method , while the other two , bayesian and neural networks , are new in the area of educational testing . in the first part of this paper , we present the concept of cat and its advantages and disadvantages . we collected data from paper tests performed with grammar school students . we provide the summary of data used for our experiments in the second part . next , we present three different model types for cat . they are based on the item response theory , bayesian networks , and neural networks . the general theory associated with each type is briefly explained and the utilization of these models for cat is analyzed . future research is outlined in the concluding part of the paper . it shows many interesting research paths that are important not only for cat but also for other areas of artificial intelligence .

computing lpmln using asp and mln solvers
lpmln is a recent addition to probabilistic logic programming languages . its main idea is to overcome the rigid nature of the stable model semantics by assigning a weight to each rule in a way similar to markov logic is defined . we present two implementations of lpmln , $ \text { lpmln2asp } $ and $ \text { lpmln2mln } $ . system $ \text { lpmln2asp } $ translates lpmln programs into the input language of answer set solver $ \text { clingo } $ , and using weak constraints and stable model enumeration , it can compute most probable stable models as well as exact conditional and marginal probabilities . system $ \text { lpmln2mln } $ translates lpmln programs into the input language of markov logic solvers , such as $ \text { alchemy } $ , $ \text { tuffy } $ , and $ \text { rockit } $ , and allows for performing approximate probabilistic inference on lpmln programs . we also demonstrate the usefulness of the lpmln systems for computing other languages , such as problog and pearl 's causal models , that are shown to be translatable into lpmln . ( under consideration for acceptance in tplp )

inductive learning for rule generation from ontology
this paper presents an idea of inductive learning use for rule generation from ontologies . the main purpose of the paper is to evaluate the possibility of inductive learning use in rule generation from ontologies and to develop the way how this can be done . generated rules are necessary to supplement or even to develop the semantic web expert system ( swes ) knowledge base . the swes emerges as the result of evolution of expert system concept toward the web , and the swes is based on the semantic web technologies . available publications show that the problem of rule generation from ontologies based on inductive learning is not investigated deeply enough .

perch : perception via search for multi-object recognition and localization
in many robotic domains such as flexible automated manufacturing or personal assistance , a fundamental perception task is that of identifying and localizing objects whose 3d models are known . canonical approaches to this problem include discriminative methods that find correspondences between feature descriptors computed over the model and observed data . while these methods have been employed successfully , they can be unreliable when the feature descriptors fail to capture variations in observed data ; a classic cause being occlusion . as a step towards deliberative reasoning , we present perch : perception via search , an algorithm that seeks to find the best explanation of the observed sensor data by hypothesizing possible scenes in a generative fashion . our contributions are : i ) formulating the multi-object recognition and localization task as an optimization problem over the space of hypothesized scenes , ii ) exploiting structure in the optimization to cast it as a combinatorial search problem on what we call the monotone scene generation tree , and iii ) leveraging parallelization and recent advances in multi-heuristic search in making combinatorial search tractable . we prove that our system can guaranteedly produce the best explanation of the scene under the chosen cost function , and validate our claims on real world rgb-d test data . our experimental results show that we can identify and localize objects under heavy occlusion -- cases where state-of-the-art methods struggle .

large-scale detection of non-technical losses in imbalanced data sets
non-technical losses ( ntl ) such as electricity theft cause significant harm to our economies , as in some countries they may range up to 40 % of the total electricity distributed . detecting ntls requires costly on-site inspections . accurate prediction of ntls for customers using machine learning is therefore crucial . to date , related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced , that ntl proportions may change and mostly consider small data sets , often not allowing to deploy the results in production . in this paper , we present a comprehensive approach to assess three ntl detection models for different ntl proportions in large real world data sets of 100ks of customers : boolean rules , fuzzy logic and support vector machine . this work has resulted in appreciable results that are about to be deployed in a leading industry solution . we believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets .

neurogenesis-inspired dictionary learning : online model adaption in a changing world
in this paper , we focus on online representation learning in non-stationary environments which may require continuous adaptation of model architecture . we propose a novel online dictionary-learning ( sparse-coding ) framework which incorporates the addition and deletion of hidden units ( dictionary elements ) , and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus , known to be associated with improved cognitive function and adaptation to new environments . in the online learning setting , where new input instances arrive sequentially in batches , the neuronal-birth is implemented by adding new units with random initial weights ( random dictionary elements ) ; the number of new units is determined by the current performance ( representation error ) of the dictionary , higher error causing an increase in the birth rate . neuronal-death is implemented by imposing l1/l2-regularization ( group sparsity ) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme , which iterates between the code and dictionary updates . finally , hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements . our empirical evaluation on several real-life datasets ( images and language ) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size ( nonadaptive ) online sparse coding of mairal et al . ( 2009 ) in the presence of nonstationary data . moreover , we identify certain properties of the data ( e.g. , sparse inputs with nearly non-overlapping supports ) and of the model ( e.g. , dictionary sparsity ) associated with such improvements .

a greedy approach to adapting the trace parameter for temporal difference learning
one of the main obstacles to broad application of reinforcement learning methods is the parameter sensitivity of our core learning algorithms . in many large-scale applications , online computation and function approximation represent key strategies in scaling up reinforcement learning algorithms . in this setting , we have effective and reasonably well understood algorithms for adapting the learning-rate parameter , online during learning . such meta-learning approaches can improve robustness of learning and enable specialization to current task , improving learning speed . for temporal-difference learning algorithms which we study here , there is yet another parameter , $ \lambda $ , that similarly impacts learning speed and stability in practice . unfortunately , unlike the learning-rate parameter , $ \lambda $ parametrizes the objective function that temporal-difference methods optimize . different choices of $ \lambda $ produce different fixed-point solutions , and thus adapting $ \lambda $ online and characterizing the optimization is substantially more complex than adapting the learning-rate parameter . there are no meta-learning method for $ \lambda $ that can achieve ( 1 ) incremental updating , ( 2 ) compatibility with function approximation , and ( 3 ) maintain stability of learning under both on and off-policy sampling . in this paper we contribute a novel objective function for optimizing $ \lambda $ as a function of state rather than time . we derive a new incremental , linear complexity $ \lambda $ -adaption algorithm that does not require offline batch updating or access to a model of the world , and present a suite of experiments illustrating the practicality of our new algorithm in three different settings . taken together , our contributions represent a concrete step towards black-box application of temporal-difference learning methods in real world problems .

towards the patterns of hard csps with association rule mining
the hardness of finite domain constraint satisfaction problems ( csps ) is a very important research area in constraint programming ( cp ) community . however , this problem has not yet attracted much attention from the researchers in the association rule mining community . as a popular data mining technique , association rule mining has an extremely wide application area and it has already been successfully applied to many interdisciplines . in this paper , we study the association rule mining techniques and propose a cascaded approach to extract the interesting patterns of the hard csps . as far as we know , this problem is investigated with the data mining techniques for the first time . specifically , we generate the random csps and collect their characteristics by solving all the csp instances , and then apply the data mining techniques on the data set and further to discover the interesting patterns of the hardness of the randomly generated csps

the nnn formalization : review and development of guideline specification in the care domain
due to an ageing society , it can be expected that less nursing personnel will be responsible for an increasing number of patients in the future . one way to address this challenge is to provide system-based support for nursing personnel in creating , executing , and adapting patient care processes . in care practice , these processes are following the general care process definition and individually specified according to patient-specific data as well as diagnoses and guidelines from the nanda , nic , and noc ( nnn ) standards . in addition , adaptations to running patient processes become necessary frequently and are to be conducted by nursing personnel including nnn knowledge . in order to provide semi-automatic support for design and adaption of care processes , a formalization of nnn knowledge is indispensable . this technical report presents the nnn formalization that is developed targeting at goals such as completeness , flexibility , and later exploitation for creating and adapting patient care processes . the formalization also takes into consideration an extensive evaluation of existing formalization standards for clinical guidelines . the nnn formalization as well as its usage are evaluated based on case study fatigue .

annotation order matters : recurrent image annotator for arbitrary length image tagging
automatic image annotation has been an important research topic in facilitating large scale image management and retrieval . existing methods focus on learning image-tag correlation or correlation between tags to improve annotation accuracy . however , most of these methods evaluate their performance using top-k retrieval performance , where k is fixed . although such setting gives convenience for comparing different methods , it is not the natural way that humans annotate images . the number of annotated tags should depend on image contents . inspired by the recent progress in machine translation and image captioning , we propose a novel recurrent image annotator ( ria ) model that forms image annotation task as a sequence generation problem so that ria can natively predict the proper length of tags according to image contents . we evaluate the proposed model on various image annotation datasets . in addition to comparing our model with existing methods using the conventional top-k evaluation measures , we also provide our model as a high quality baseline for the arbitrary length image tagging task . moreover , the results of our experiments show that the order of tags in training phase has a great impact on the final annotation performance .

belief revision in structured probabilistic argumentation
in real-world applications , knowledge bases consisting of all the information at hand for a specific domain , along with the current state of affairs , are bound to contain contradictory data coming from different sources , as well as data with varying degrees of uncertainty attached . likewise , an important aspect of the effort associated with maintaining knowledge bases is deciding what information is no longer useful ; pieces of information ( such as intelligence reports ) may be outdated , may come from sources that have recently been discovered to be of low quality , or abundant evidence may be available that contradicts them . in this paper , we propose a probabilistic structured argumentation framework that arises from the extension of presumptive defeasible logic programming ( predelp ) with probabilistic models , and argue that this formalism is capable of addressing the basic issues of handling contradictory and uncertain data . then , to address the last issue , we focus on the study of non-prioritized belief revision operations over probabilistic predelp programs . we propose a set of rationality postulates -- based on well-known ones developed for classical knowledge bases -- that characterize how such operations should behave , and study a class of operators along with theoretical relationships with the proposed postulates , including a representation theorem stating the equivalence between this class and the class of operators characterized by the postulates .

strong asymptotic assertions for discrete mdl in regression and classification
we study the properties of the mdl ( or maximum penalized complexity ) estimator for regression and classification , where the underlying model class is countable . we show in particular a finite bound on the hellinger losses under the only assumption that there is a `` true '' model contained in the class . this implies almost sure convergence of the predictive distribution to the true one at a fast rate . it corresponds to solomonoff 's central theorem of universal induction , however with a bound that is exponentially larger .

price and profit awareness in recommender systems
academic research in the field of recommender systems mainly focuses on the problem of maximizing the users ' utility by trying to identify the most relevant items for each user . however , such items are not necessarily the ones that maximize the utility of the service provider ( e.g. , an online retailer ) in terms of the business value , such as profit . one approach to increasing the providers ' utility is to incorporate purchase-oriented information , e.g. , the price , sales probabilities , and the resulting profit , into the recommendation algorithms . in this paper we specifically focus on price- and profit-aware recommender systems . we provide a brief overview of the relevant literature and use numerical simulations to illustrate the potential business benefit of such approaches .

graphical models : an extension to random graphs , trees , and other objects
in this work , we consider an extension of graphical models to random graphs , trees , and other objects . to do this , many fundamental concepts for multivariate random variables ( e.g. , marginal variables , gibbs distribution , markov properties ) must be extended to other mathematical objects ; it turns out that this extension is possible , as we will discuss , if we have a consistent , complete system of projections on a given object . each projection defines a marginal random variable , allowing one to specify independence assumptions between them . furthermore , these independencies can be specified in terms of a small subset of these marginal variables ( which we call the atomic variables ) , allowing the compact representation of independencies by a directed graph . projections also define factors , functions on the projected object space , and hence a projection family defines a set of possible factorizations for a distribution ; these can be compactly represented by an undirected graph . the invariances used in graphical models are essential for learning distributions , not just on multivariate random variables , but also on other objects . when they are applied to random graphs and random trees , the result is a general class of models that is applicable to a broad range of problems , including those in which the graphs and trees have complicated edge structures . these models need not be conditioned on a fixed number of vertices , as is often the case in the literature for random graphs , and can be used for problems in which attributes are associated with vertices and edges . for graphs , applications include the modeling of molecules , neural networks , and relational real-world scenes ; for trees , applications include the modeling of infectious diseases , cell fusion , the structure of language , and the structure of objects in visual scenes . many classic models are particular instances of this framework .

latent contextual bandits and their application to personalized recommendations for new users
personalized recommendations for new users , also known as the cold-start problem , can be formulated as a contextual bandit problem . existing contextual bandit algorithms generally rely on features alone to capture user variability . such methods are inefficient in learning new users ' interests . in this paper we propose latent contextual bandits . we consider both the benefit of leveraging a set of learned latent user classes for new users , and how we can learn such latent classes from prior users . we show that our approach achieves a better regret bound than existing algorithms . we also demonstrate the benefit of our approach using a large real world dataset and a preliminary user study .

multimodal transfer : a hierarchical deep convolutional neural network for fast artistic style transfer
transferring artistic styles onto everyday photographs has become an extremely popular task in both academia and industry . recently , offline training has replaced on-line iterative optimization , enabling nearly real-time stylization . when those stylization networks are applied directly to high-resolution images , however , the style of localized regions often appears less similar to the desired artistic style . this is because the transfer process fails to capture small , intricate textures and maintain correct texture scales of the artworks . here we propose a multimodal convolutional neural network that takes into consideration faithful representations of both color and luminance channels , and performs stylization hierarchically with multiple losses of increasing scales . compared to state-of-the-art networks , our network can also perform style transfer in nearly real-time by conducting much more sophisticated training offline . by properly handling style and texture cues at multiple scales using several modalities , we can transfer not just large-scale , obvious style cues but also subtle , exquisite ones . that is , our scheme can generate results that are visually pleasing and more similar to multiple desired artistic styles with color and texture cues at multiple scales .

sensitivity analysis for probability assessments in bayesian networks
when eliciting probability models from experts , knowledge engineers may compare the results of the model with expert judgment on test scenarios , then adjust model parameters to bring the behavior of the model more in line with the expert 's intuition . this paper presents a methodology for analytic computation of sensitivity values to measure the impact of small changes in a network parameter on a target probability value or distribution . these values can be used to guide knowledge elicitation . they can also be used in a gradient descent algorithm to estimate parameter values that maximize a measure of goodness-of-fit to both local and holistic probability assessments .

building an interpretable fuzzy rule base from data using orthogonal least squares application to a depollution problem
in many fields where human understanding plays a crucial role , such as bioprocesses , the capacity of extracting knowledge from data is of critical importance . within this framework , fuzzy learning methods , if properly used , can greatly help human experts . amongst these methods , the aim of orthogonal transformations , which have been proven to be mathematically robust , is to build rules from a set of training data and to select the most important ones by linear regression or rank revealing techniques . the ols algorithm is a good representative of those methods . however , it was originally designed so that it only cared about numerical performance . thus , we propose some modifications of the original method to take interpretability into account . after recalling the original algorithm , this paper presents the changes made to the original method , then discusses some results obtained from benchmark problems . finally , the algorithm is applied to a real-world fault detection depollution problem .

relations such as hypernymy : identifying and exploiting hearst patterns in distributional vectors for lexical entailment
we consider the task of predicting lexical entailment using distributional vectors . we perform a novel qualitative analysis of one existing model which was previously shown to only measure the prototypicality of word pairs . we find that the model strongly learns to identify hypernyms using hearst patterns , which are well known to be predictive of lexical relations . we present a novel model which exploits this behavior as a method of feature extraction in an iterative procedure similar to principal component analysis . our model combines the extracted features with the strengths of other proposed models in the literature , and matches or outperforms prior work on multiple data sets .

permeability analysis based on information granulation theory
this paper describes application of information granulation theory , on the analysis of `` lugeon data '' . in this manner , using a combining of self organizing map ( som ) and neuro-fuzzy inference system ( nfis ) , crisp and fuzzy granules are obtained . balancing of crisp granules and sub- fuzzy granules , within non fuzzy information ( initial granulation ) , is rendered in open-close iteration . using two criteria , `` simplicity of rules `` and `` suitable adaptive threshold error level '' , stability of algorithm is guaranteed . in other part of paper , rough set theory ( rst ) , to approximate analysis , has been employed > .validation of the proposed methods , on the large data set of in-situ permeability in rock masses , in the shivashan dam , iran , has been highlighted . by the implementation of the proposed algorithm on the lugeon data set , was proved the suggested method , relating the approximate analysis on the permeability , could be applied .

weighted regret-based likelihood : a new approach to describing uncertainty
recently , halpern and leung suggested representing uncertainty by a weighted set of probability measures , and suggested a way of making decisions based on this representation of uncertainty : maximizing weighted regret . their paper does not answer an apparently simpler question : what it means , according to this representation of uncertainty , for an event e to be more likely than an event e ' . in this paper , a notion of comparative likelihood when uncertainty is represented by a weighted set of probability measures is defined . it generalizes the ordering defined by probability ( and by lower probability ) in a natural way ; a generalization of upper probability can also be defined . a complete axiomatic characterization of this notion of regret-based likelihood is given .

one model for the learning of language
a major target of linguistics and cognitive science has been to understand what class of learning systems can acquire the key structures of natural language . until recently , the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space . here , we describe a learning system that is maximally unconstrained , operating over the space of all computations , and is able to acquire several of the key structures present natural language from positive evidence alone . the model successfully acquires regular ( e.g . $ ( ab ) ^n $ ) , context-free ( e.g . $ a^n b^n $ , $ x x^r $ ) , and context-sensitive ( e.g . $ a^nb^nc^n $ , $ a^nb^mc^nd^m $ , $ xx $ ) formal languages . our approach develops the concept of factorized programs in bayesian program induction in order to help manage the complexity of representation . we show in learning , the model predicts several phenomena empirically observed in human grammar acquisition experiments .

online prediction of ovarian cancer
in this paper we apply computer learning methods to diagnosing ovarian cancer using the level of the standard biomarker ca125 in conjunction with information provided by mass-spectrometry . we are working with a new data set collected over a period of 7 years . using the level of ca125 and mass-spectrometry peaks , our algorithm gives probability predictions for the disease . to estimate classification accuracy we convert probability predictions into strict predictions . our algorithm makes fewer errors than almost any linear combination of the ca125 level and one peak 's intensity ( taken on the log scale ) . to check the power of our algorithm we use it to test the hypothesis that ca125 and the peaks do not contain useful information for the prediction of the disease at a particular time before the diagnosis . our algorithm produces $ p $ -values that are better than those produced by the algorithm that has been previously applied to this data set . our conclusion is that the proposed algorithm is more reliable for prediction on new data .

marginalization in composed probabilistic models
composition of low-dimensional distributions , whose foundations were laid in the papaer published in the proceeding of uai'97 ( jirousek 1997 ) , appeared to be an alternative apparatus to describe multidimensional probabilistic models . in contrast to graphical markov models , which define multidomensinoal distributions in a declarative way , this approach is rather procedural . ordering of low-dimensional distributions into a proper sequence fully defines the resepctive computational procedure ; therefore , a stury of different type of generating sequences is one fo the central problems in this field . thus , it appears that an important role is played by special sequences that are called perfect . their main characterization theorems are presetned in this paper . however , the main result of this paper is a solution to the problem of margnialization for general sequences . the main theorem describes a way to obtain a generating sequence that defines the model corresponding to the marginal of the distribution defined by an arbitrary genearting sequence . from this theorem the reader can see to what extent these comutations are local ; i.e. , the sequence consists of marginal distributions whose computation must be made by summing up over the values of the variable eliminated ( the paper deals with finite model ) .

querying geometric figures using a controlled language , ontological graphs and dependency lattices
dynamic geometry systems ( dgs ) have become basic tools in many areas of geometry as , for example , in education . geometry automated theorem provers ( gatp ) are an active area of research and are considered as being basic tools in future enhanced educational software as well as in a next generation of mechanized mathematics assistants . recently emerged web repositories of geometric knowledge , like tgtp and intergeo , are an attempt to make the already vast data set of geometric knowledge widely available . considering the large amount of geometric information already available , we face the need of a query mechanism for descriptions of geometric constructions . in this paper we discuss two approaches for describing geometric figures ( declarative and procedural ) , and present algorithms for querying geometric figures in declaratively and procedurally described corpora , by using a dgs or a dedicated controlled natural language for queries .

feature markov decision processes
general purpose intelligent learning agents cycle through ( complex , non-mdp ) sequences of observations , actions , and rewards . on the other hand , reinforcement learning is well-developed for small finite state markov decision processes ( mdps ) . so far it is an art performed by human designers to extract the right state representation out of the bare observations , i.e . to reduce the agent setup to the mdp framework . before we can think of mechanizing this search for suitable mdps , we need a formal objective criterion . the main contribution of this article is to develop such a criterion . i also integrate the various parts into one learning algorithm . extensions to more realistic dynamic bayesian networks are developed in a companion article .

exchangeable choice functions
we investigate how to model exchangeability with choice functions . exchangeability is a structural assessment on a sequence of uncertain variables . we show how such assessments are a special indifference assessment , and how that leads to a counterpart of de finetti 's representation theorem , both in a finite and a countable context .

a heuristic routing mechanism using a new addressing scheme
current methods of routing are based on network information in the form of routing tables , in which routing protocols determine how to update the tables according to the network changes . despite the variability of data in routing tables , node addresses are constant . in this paper , we first introduce the new concept of variable addresses , which results in a novel framework to cope with routing problems using heuristic solutions . then we propose a heuristic routing mechanism based on the application of genes for determination of network addresses in a variable address network and describe how this method flexibly solves different problems and induces new ideas in providing integral solutions for variety of problems . the case of ad-hoc networks is where simulation results are more supportive and original solutions have been proposed for issues like mobility .

satzilla : portfolio-based algorithm selection for sat
it has been widely observed that there is no single `` dominant '' sat solver ; instead , different solvers perform best on different instances . rather than following the traditional approach of choosing the best solver for a given class of instances , we advocate making this decision online on a per-instance basis . building on previous work , we describe satzilla , an automated approach for constructing per-instance algorithm portfolios for sat that use so-called empirical hardness models to choose among their constituent solvers . this approach takes as input a distribution of problem instances and a set of component solvers , and constructs a portfolio optimizing a given objective function ( such as mean runtime , percent of instances solved , or score in a competition ) . the excellent performance of satzilla was independently verified in the 2007 sat competition , where our satzilla07 solvers won three gold , one silver and one bronze medal . in this article , we go well beyond satzilla07 by making the portfolio construction scalable and completely automated , and improving it by integrating local search solvers as candidate solvers , by predicting performance score instead of runtime , and by using hierarchical hardness models that take into account different types of sat instances . we demonstrate the effectiveness of these new techniques in extensive experimental results on data sets including instances from the most recent sat competition .

knowledge base completion : baselines strike back
many papers have been published on the knowledge base completion task in the past few years . most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as fb15k and wn18 . this paper shows that the accuracy of almost all models published on the fb15k can be outperformed by an appropriately tuned baseline - our reimplementation of the distmult model . our findings cast doubt on the claim that the performance improvements of recent models are due to architectural changes as opposed to hyper-parameter tuning or different training objectives . this should prompt future research to re-consider how the performance of models is evaluated and reported .

dlolis-a : description logic based text ontology learning
ontology learning has been the subject of intensive study for the past decade . researchers in this field have been motivated by the possibility of automatically building a knowledge base on top of text documents so as to support reasoning based knowledge extraction . while most works in this field have been primarily statistical ( known as light-weight ontology learning ) not much attempt has been made in axiomatic ontology learning ( called heavy-weight ontology learning ) from natural language text documents . heavy-weight ontology learning supports more precise formal logic-based reasoning when compared to statistical ontology learning . in this paper we have proposed a sound ontology learning tool dlol_ ( is-a ) that maps english language is-a sentences into their equivalent description logic ( dl ) expressions in order to automatically generate a consistent pair of t-box and a-box thereby forming both regular ( definitional form ) and generalized ( axiomatic form ) dl ontology . the current scope of the paper is strictly limited to is-a sentences that exclude the possible structures of : ( i ) implicative is-a sentences , and ( ii ) `` wh '' is-a questions . other linguistic nuances that arise out of pragmatics and epistemic of is-a sentences are beyond the scope of this present work . we have adopted gold standard based ontology learning evaluation on chosen is-a rich wikipedia documents .

hi , how can i help you ? : automating enterprise it support help desks
question answering is one of the primary challenges of natural language understanding . in realizing such a system , providing complex long answers to questions is a challenging task as opposed to factoid answering as the former needs context disambiguation . the different methods explored in the literature can be broadly classified into three categories namely : 1 ) classification based , 2 ) knowledge graph based and 3 ) retrieval based . individually , none of them address the need of an enterprise wide assistance system for an it support and maintenance domain . in this domain the variance of answers is large ranging from factoid to structured operating procedures ; the knowledge is present across heterogeneous data sources like application specific documentation , ticket management systems and any single technique for a general purpose assistance is unable to scale for such a landscape . to address this , we have built a cognitive platform with capabilities adopted for this domain . further , we have built a general purpose question answering system leveraging the platform that can be instantiated for multiple products , technologies in the support domain . the system uses a novel hybrid answering model that orchestrates across a deep learning classifier , a knowledge graph based context disambiguation module and a sophisticated bag-of-words search system . this orchestration performs context switching for a provided question and also does a smooth hand-off of the question to a human expert if none of the automated techniques can provide a confident answer . this system has been deployed across 675 internal enterprise it support and maintenance projects .

algebraic net class rewriting systems , syntax and semantics for knowledge representation and automated problem solving
the intention of the present study is to establish general framework for automated problem solving by approaching the task universal algebraically introducing knowledge as realizations of generalized free algebra based nets , graphs with gluing forms connecting in- and out-edges to nodes . nets are caused to undergo transformations in conceptual level by type wise differentiated intervening net rewriting systems dispersing problems to abstract parts , matching being determined by substitution relations . achieved sets of conceptual nets constitute congruent classes . new results are obtained within construction of problem solving systems where solution algorithms are derived parallel with other candidates applied to the same net classes . by applying parallel transducer paths consisting of net rewriting systems to net classes congruent quotient algebras are established and the manifested class rewriting comprises all solution candidates whenever produced nets are in anticipated languages liable to acceptance of net automata .

judgment aggregation in multi-agent argumentation
given a set of conflicting arguments , there can exist multiple plausible opinions about which arguments should be accepted , rejected , or deemed undecided . we study the problem of how multiple such judgments can be aggregated . we define the problem by adapting various classical social-choice-theoretic properties for the argumentation domain . we show that while argument-wise plurality voting satisfies many properties , it fails to guarantee the collective rationality of the outcome , and struggles with ties . we then present more general results , proving multiple impossibility results on the existence of any good aggregation operator . after characterising the sufficient and necessary conditions for satisfying collective rationality , we study whether restricting the domain of argument-wise plurality voting to classical semantics allows us to escape the impossibility result . we close by listing graph-theoretic restrictions under which argument-wise plurality rule does produce collectively rational outcomes . in addition to identifying fundamental barriers to collective argument evaluation , our results open up the door for a new research agenda for the argumentation and computational social choice communities .

model counting in product configuration
we describe how to use propositional model counting for a quantitative analysis of product configuration data . our approach computes valuable meta information such as the total number of valid configurations or the relative frequency of components . this information can be used to assess the severity of documentation errors or to measure documentation quality . as an application example we show how we apply these methods to product documentation formulas of the mercedes-benz line of vehicles . in order to process these large formulas we developed and implemented a new model counter for non-cnf formulas . our model counter can process formulas , whose cnf representations could not be processed up till now .

open problems in universal induction & intelligence
specialized intelligent systems can be found everywhere : finger print , handwriting , speech , and face recognition , spam filtering , chess and other game programs , robots , et al . this decade the first presumably complete mathematical theory of artificial intelligence based on universal induction-prediction-decision-action has been proposed . this information-theoretic approach solidifies the foundations of inductive inference and artificial intelligence . getting the foundations right usually marks a significant progress and maturing of a field . the theory provides a gold standard and guidance for researchers working on intelligent algorithms . the roots of universal induction have been laid exactly half-a-century ago and the roots of universal intelligence exactly one decade ago . so it is timely to take stock of what has been achieved and what remains to be done . since there are already good recent surveys , i describe the state-of-the-art only in passing and refer the reader to the literature . this article concentrates on the open problems in universal induction and its extension to universal intelligence .

fast bayesian non-negative matrix factorisation and tri-factorisation
we present a fast variational bayesian algorithm for performing non-negative matrix factorisation and tri-factorisation . we show that our approach achieves faster convergence per iteration and timestep ( wall-clock ) than gibbs sampling and non-probabilistic approaches , and do not require additional samples to estimate the posterior . we show that in particular for matrix tri-factorisation convergence is difficult , but our variational bayesian approach offers a fast solution , allowing the tri-factorisation approach to be used more effectively .

probabilistic causal reasoning
predicting the future is an important component of decision making . in most situations , however , there is not enough information to make accurate predictions . in this paper , we develop a theory of causal reasoning for predictive inference under uncertainty . we emphasize a common type of prediction that involves reasoning about persistence : whether or not a proposition once made true remains true at some later time . we provide a decision procedure with a polynomial-time algorithm for determining the probability of the possible consequences of a set events and initial conditions . the integration of simple probability theory with temporal projection enables us to circumvent problems that nonmonotonic temporal reasoning schemes have in dealing with persistence . the ideas in this paper have been implemented in a prototype system that refines a database of causal rules in the course of applying those rules to construct and carry out plans in a manufacturing domain .

auxiliary deep generative models
deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning . we extend deep generative models with auxiliary variables which improves the variational approximation . the auxiliary variables leave the generative model unchanged but make the variational distribution more expressive . inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections . our findings suggest that more expressive and properly specified deep generative models converge faster with better results . we show state-of-the-art performance within semi-supervised learning on mnist , svhn and norb datasets .

generative bridging network in neural sequence prediction
maximum likelihood estimation ( mle ) suffers from data sparsity problem in sequence prediction tasks where training resource is rare . in order to alleviate this problem , in this paper , we propose a novel generative bridging network ( gbn ) to train sequence prediction models , which contains a generator and a bridge . unlike mle directly maximizing the likelihood of the ground truth , the bridge extends the point-wise ground truth to a bridge distribution ( containing inexhaustible examples ) , and the generator is trained to minimize their kl-divergence . in order to guide the training of generator with additional signals , the bridge distribution can be set or trained to possess specific properties , by using different constraints . more specifically , to increase output diversity , enhance language smoothness and relieve learning burden , three different regularization constraints are introduced to construct bridge distributions . by combining these bridges with a sequence generator , three independent gbns are proposed , namely uniform gbn , language-model gbn and coaching gbn . experiment conducted on two recognized sequence prediction tasks ( machine translation and abstractive text summarization ) shows that our proposed gbns can yield significant improvements over strong baseline systems . furthermore , by analyzing samples drawn from bridge distributions , expected influences on the sequence model training are verified .

connectivity for matroids based on rough sets
in mathematics and computer science , connectivity is one of the basic concepts of matroid theory : it asks for the minimum number of elements which need to be removed to disconnect the remaining nodes from each other . it is closely related to the theory of network flow problems . the connectivity of a matroid is an important measure of its robustness as a network . therefore , it is very necessary to investigate the conditions under which a matroid is connected . in this paper , the connectivity for matroids is studied through relation-based rough sets . first , a symmetric and transitive relation is introduced from a general matroid and its properties are explored from the viewpoint of matroids . moreover , through the relation introduced by a general matroid , an undirected graph is generalized . specifically , the connection of the graph can be investigated by the relation-based rough sets . second , we study the connectivity for matroids by means of relation-based rough sets and some conditions under which a general matroid is connected are presented . finally , it is easy to prove that the connectivity for a general matroid with some special properties and its induced undirected graph is equivalent . these results show an important application of relation-based rough sets to matroids .

automatic taxonomy generation - a use-case in the legal domain
a key challenge in the legal domain is the adaptation and representation of the legal knowledge expressed through texts , in order for legal practitioners and researchers to access this information easier and faster to help with compliance related issues . one way to approach this goal is in the form of a taxonomy of legal concepts . while this task usually requires a manual construction of terms and their relations by domain experts , this paper describes a methodology to automatically generate a taxonomy of legal noun concepts . we apply and compare two approaches on a corpus consisting of statutory instruments for uk , wales , scotland and northern ireland laws .

learning feature representations for keyphrase extraction
in supervised approaches for keyphrase extraction , a candidate phrase is encoded with a set of hand-crafted features and machine learning algorithms are trained to discriminate keyphrases from non-keyphrases . although the manually-designed features have shown to work well in practice , feature engineering is a difficult process that requires expert knowledge and normally does not generalize well . in this paper , we present surfke , a feature learning framework that exploits the text itself to automatically discover patterns that keyphrases exhibit . our model represents the document as a graph and automatically learns feature representation of phrases . the proposed model obtains remarkable improvements in performance over strong baselines .

ai challenges in human-robot cognitive teaming
among the many anticipated roles for robots in the future is that of being a human teammate . aside from all the technological hurdles that have to be overcome with respect to hardware and control to make robots fit to work with humans , the added complication here is that humans have many conscious and subconscious expectations of their teammates - indeed , we argue that teaming is mostly a cognitive rather than physical coordination activity . this introduces new challenges for the ai and robotics community and requires fundamental changes to the traditional approach to the design of autonomy . with this in mind , we propose an update to the classical view of the intelligent agent architecture , highlighting the requirements for mental modeling of the human in the deliberative process of the autonomous agent . in this article , we outline briefly the recent efforts of ours , and others in the community , towards developing cognitive teammates along these guidelines .

anytime neural network : a versatile trade-off between computation and accuracy
anytime predictors first produce crude results quickly , and then continuously refine them until the test-time computational budget is depleted . such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets , and to reduce average prediction cost via early-exits . however , anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks ( dnns ) , because dnns are often computationally expensive without competitive intermediate results . in this work , we propose to add auxiliary predictions in dnns to generate anytime predictions , and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses , where the weights also oscillate during training . the proposed anytime neural networks ( anns ) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation . this enables us to assemble a sequence of exponentially deepening anns , and it achieves , both theoretically and practically , near-optimal anytime predictions at every budget after spending a constant fraction of extra cost . the proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets , including ilsvrc2012 .

prediction with a short memory
we consider the problem of predicting the next observation given a sequence of past observations , and consider the extent to which accurate prediction requires complex algorithms that explicitly leverage long-range dependencies . perhaps surprisingly , our positive results show that for a broad class of sequences , there is an algorithm that predicts well on average , and bases its predictions only on the most recent few observation together with a set of simple summary statistics of the past observations . specifically , we show that for any distribution over observations , if the mutual information between past observations and future observations is upper bounded by $ i $ , then a simple markov model over the most recent $ i/\epsilon $ observations obtains expected kl error $ \epsilon $ -- -and hence $ \ell_1 $ error $ \sqrt { \epsilon } $ -- -with respect to the optimal predictor that has access to the entire past and knows the data generating distribution . for a hidden markov model with $ n $ hidden states , $ i $ is bounded by $ \log n $ , a quantity that does not depend on the mixing time , and we show that the trivial prediction algorithm based on the empirical frequencies of length $ o ( \log n/\epsilon ) $ windows of observations achieves this error , provided the length of the sequence is $ d^ { \omega ( \log n/\epsilon ) } $ , where $ d $ is the size of the observation alphabet . we also establish that this result can not be improved upon , even for the class of hmms , in the following two senses : first , for hmms with $ n $ hidden states , a window length of $ \log n/\epsilon $ is information-theoretically necessary to achieve expected $ \ell_1 $ error $ \sqrt { \epsilon } $ . second , the $ d^ { \theta ( \log n/\epsilon ) } $ samples required to estimate the markov model for an observation alphabet of size $ d $ is necessary for any computationally tractable learning algorithm , assuming the hardness of strongly refuting a certain class of csps .

learning latent vector spaces for product search
we introduce a novel latent vector space model that jointly learns the latent representations of words , e-commerce products and a mapping between the two without the need for explicit annotations . the power of the model lies in its ability to directly model the discriminative relation between products and a particular word . we compare our method to existing latent vector space models ( lsi , lda and word2vec ) and evaluate it as a feature in a learning to rank setting . our latent vector space model achieves its enhanced performance as it learns better product representations . furthermore , the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation . we provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations .

pretraining deep actor-critic reinforcement learning algorithms with expert demonstrations
pretraining with expert demonstrations have been found useful in speeding up the training process of deep reinforcement learning algorithms since less online simulation data is required . some people use supervised learning to speed up the process of feature learning , others pretrain the policies by imitating expert demonstrations . however , these methods are unstable and not suitable for actor-critic reinforcement learning algorithms . also , some existing methods rely on the global optimum assumption , which is not true in most scenarios . in this paper , we employ expert demonstrations in a actor-critic reinforcement learning framework , and meanwhile ensure that the performance is not affected by the fact that expert demonstrations are not global optimal . we theoretically derive a method for computing policy gradients and value estimators with only expert demonstrations . our method is theoretically plausible for actor-critic reinforcement learning algorithms that pretrains both policy and value functions . we apply our method to two of the typical actor-critic reinforcement learning algorithms , ddpg and acer , and demonstrate with experiments that our method not only outperforms the rl algorithms without pretraining process , but also is more simulation efficient .

statistical mechanics algorithm for response to targets ( smart )
it is proposed to apply modern methods of nonlinear nonequilibrium statistical mechanics to develop software algorithms that will optimally respond to targets within short response times with minimal computer resources . this statistical mechanics algorithm for response to targets ( smart ) can be developed with a view towards its future implementation into a hardwired statistical algorithm multiprocessor ( sam ) to enhance the efficiency and speed of response to targets ( smart_sam ) .

the mind as a computational system
the present document is an excerpt of an essay that i wrote as part of my application material to graduate school in computer science ( with a focus on artificial intelligence ) , in 1986. i was not invited by any of the schools that received it , so i became a theoretical physicist instead . the essay 's full title was `` some topics in philosophy and computer science '' . i am making this text ( unchanged from 1985 , preserving the typesetting as much as possible ) available now in memory of jerry fodor , whose writings had influenced me significantly at the time ( even though i did not always agree ) .

technical report : giving hints for logic programming examples without revealing solutions
we introduce a framework for supporting learning to program in the paradigm of answer set programming ( asp ) , which is a declarative logic programming formalism . based on the idea of teaching by asking the student to complete small example asp programs , we introduce a three-stage method for giving hints to the student without revealing the correct solution of an example . we categorize mistakes into ( i ) syntactic mistakes , ( ii ) unexpected but syntactically correct input , and ( iii ) semantic mistakes , describe mathematical definitions of these mistakes , and show how to compute hints from these definitions .

a dynamic programming algorithm for inference in recursive probabilistic programs
we describe a dynamic programming algorithm for computing the marginal distribution of discrete probabilistic programs . this algorithm takes a functional interpreter for an arbitrary probabilistic programming language and turns it into an efficient marginalizer . because direct caching of sub-distributions is impossible in the presence of recursion , we build a graph of dependencies between sub-distributions . this factored sum-product network makes ( potentially cyclic ) dependencies between subproblems explicit , and corresponds to a system of equations for the marginal distribution . we solve these equations by fixed-point iteration in topological order . we illustrate this algorithm on examples used in teaching probabilistic models , computational cognitive science research , and game theory .

on the performance bounds of some policy search dynamic programming algorithms
we consider the infinite-horizon discounted optimal control problem formalized by markov decision processes . we focus on policy search algorithms , that compute an approximately optimal policy by following the standard policy iteration ( pi ) scheme via an -approximate greedy operator ( kakade and langford , 2002 ; lazaric et al. , 2010 ) . we describe existing and a few new performance bounds for direct policy iteration ( dpi ) ( lagoudakis and parr , 2003 ; fern et al. , 2006 ; lazaric et al. , 2010 ) and conservative policy iteration ( cpi ) ( kakade and langford , 2002 ) . by paying a particular attention to the concentrability constants involved in such guarantees , we notably argue that the guarantee of cpi is much better than that of dpi , but this comes at the cost of a relative -- exponential in $ \frac { 1 } { \epsilon } $ -- increase of time complexity . we then describe an algorithm , non-stationary direct policy iteration ( nsdpi ) , that can either be seen as 1 ) a variation of policy search by dynamic programming by bagnell et al . ( 2003 ) to the infinite horizon situation or 2 ) a simplified version of the non-stationary pi with growing period of scherrer and lesner ( 2012 ) . we provide an analysis of this algorithm , that shows in particular that it enjoys the best of both worlds : its performance guarantee is similar to that of cpi , but within a time complexity similar to that of dpi .

feedforward and recurrent neural networks backward propagation and hessian in matrix form
in this paper we focus on the linear algebra theory behind feedforward ( fnn ) and recurrent ( rnn ) neural networks . we review backward propagation , including backward propagation through time ( bptt ) . also , we obtain a new exact expression for hessian , which represents second order effects . we show that for $ t $ time steps the weight gradient can be expressed as a rank- $ t $ matrix , while the weight hessian is as a sum of $ t^ { 2 } $ kronecker products of rank- $ 1 $ and $ w^ { t } aw $ matrices , for some matrix $ a $ and weight matrix $ w $ . also , we show that for a mini-batch of size $ r $ , the weight update can be expressed as a rank- $ rt $ matrix . finally , we briefly comment on the eigenvalues of the hessian matrix .

ensemble uct needs high exploitation
recent results have shown that the mcts algorithm ( a new , adaptive , randomized optimization algorithm ) is effective in a remarkably diverse set of applications in artificial intelligence , operations research , and high energy physics . mcts can find good solutions without domain dependent heuristics , using the uct formula to balance exploitation and exploration . it has been suggested that the optimum in the exploitation- exploration balance differs for different search tree sizes : small search trees needs more exploitation ; large search trees need more exploration . small search trees occur in variations of mcts , such as parallel and ensemble approaches . this paper investigates the possibility of improving the performance of ensemble uct by increasing the level of exploitation . as the search trees becomes smaller we achieve an improved performance . the results are important for improving the performance of large scale parallelism of mcts .

automated construction of sparse bayesian networks from unstructured probabilistic models and domain information
an algorithm for automated construction of a sparse bayesian network given an unstructured probabilistic model and causal domain information from an expert has been developed and implemented . the goal is to obtain a network that explicitly reveals as much information regarding conditional independence as possible . the network is built incrementally adding one node at a time . the expert 's information and a greedy heuristic that tries to keep the number of arcs added at each step to a minimum are used to guide the search for the next node to add . the probabilistic model is a predicate that can answer queries about independencies in the domain . in practice the model can be implemented in various ways . for example , the model could be a statistical independence test operating on empirical data or a deductive prover operating on a set of independence statements about the domain .

efficient transfer learning schemes for personalized language modeling using recurrent neural network
in this paper , we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture . with our proposed fast transfer learning schemes , a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource . these methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes . through experiments on dialogue data in a drama , it is verified that our transfer learning methods have successfully generated the personalized language model , whose output is more similar to the personal language style in both qualitative and quantitative aspects .

modélisation d'une analyse pragma-linguistique d'un forum de discussion
we present in this paper , a modelling of an expertise in pragmatics . we follow knowledge engineering techniques and observe the expert when he analyses a social discussion forum . then a number of models are defined . these models emphasises the process followed by the expert and a number of criteria used in his analysis . results can be used as guides that help to understand and annotate discussion forum . we aim at modelling other pragmatics analysis in order to complete the base of guides ; criteria , process , etc . of discussion analysis

causal interfaces
the interaction of two binary variables , assumed to be empirical observations , has three degrees of freedom when expressed as a matrix of frequencies . usually , the size of causal influence of one variable on the other is calculated as a single value , as increase in recovery rate for a medical treatment , for example . we examine what is lost in this simplification , and propose using two interface constants to represent positive and negative implications separately . given certain assumptions about non-causal outcomes , the set of resulting epistemologies is a continuum . we derive a variety of particular measures and contrast them with the one-dimensional index .

the alldifferent constraint : a survey
the constraint of difference is known to the constraint programming community since lauriere introduced alice in 1978. since then , several solving strategies have been designed for this constraint . in this paper we give both a practical overview and an abstract comparison of these different strategies .

the mean and median criterion for automatic kernel bandwidth selection for support vector data description
support vector data description ( svdd ) is a popular technique for detecting anomalies . the svdd classifier partitions the whole space into an inlier region , which consists of the region near the training data , and an outlier region , which consists of points away from the training data . the computation of the svdd classifier requires a kernel function , and the gaussian kernel is a common choice for the kernel function . the gaussian kernel has a bandwidth parameter , whose value is important for good results . a small bandwidth leads to overfitting , and the resulting svdd classifier overestimates the number of anomalies . a large bandwidth leads to underfitting , and the classifier fails to detect many anomalies . in this paper we present a new automatic , unsupervised method for selecting the gaussian kernel bandwidth . the selected value can be computed quickly , and it is competitive with existing bandwidth selection methods .

bayesian causal induction
discovering causal relationships is a hard task , often hindered by the need for intervention , and often requiring large amounts of data to resolve statistical uncertainty . however , humans quickly arrive at useful causal relationships . one possible reason is that humans extrapolate from past experience to new , unseen situations : that is , they encode beliefs over causal invariances , allowing for sound generalization from the observations they obtain from directly acting in the world . here we outline a bayesian model of causal induction where beliefs over competing causal hypotheses are modeled using probability trees . based on this model , we illustrate why , in the general case , we need interventions plus constraints on our causal hypotheses in order to extract causal information from our experience .

validation of nonlinear pca
linear principal component analysis ( pca ) can be extended to a nonlinear pca by using artificial neural networks . but the benefit of curved components requires a careful control of the model complexity . moreover , standard techniques for model selection , including cross-validation and more generally the use of an independent test set , fail when applied to nonlinear pca because of its inherent unsupervised characteristics . this paper presents a new approach for validating the complexity of nonlinear pca models by using the error in missing data estimation as a criterion for model selection . it is motivated by the idea that only the model of optimal complexity is able to predict missing values with the highest accuracy . while standard test set validation usually favours over-fitted nonlinear pca models , the proposed model validation approach correctly selects the optimal model complexity .

a practical approach to ontology-enabled control systems for astronomical instrumentation
even though modern service-oriented and data-oriented architectures promise to deliver loosely coupled control systems , they are inherently brittle as they commonly depend on a priori agreed interfaces and data models . at the same time , the semantic web and a whole set of accompanying standards and tools are emerging , advocating ontologies as the basis for knowledge exchange . in this paper we aim to identify a number of key ideas from the myriad of knowledge-based practices that can readily be implemented by control systems today . we demonstrate with a practical example ( a three-channel imager for the mercator telescope ) how ontologies developed in the web ontology language ( owl ) can serve as a meta-model for our instrument , covering as many engineering aspects of the project as needed . we show how a concrete system model can be built on top of this meta-model via a set of domain specific languages ( dsls ) , supporting both formal verification and the generation of software and documentation artifacts . finally we reason how the available semantics can be exposed at run-time by adding a `` semantic layer '' that can be browsed , queried , monitored etc . by any opc ua-enabled client .

organic computing in the spotlight
organic computing is an initiative in the field of systems engineering that proposed to make use of concepts such as self-adaptation and self-organisation to increase the robustness of technical systems . based on the observation that traditional design and operation concepts reach their limits , transferring more autonomy to the systems themselves should result in a reduction of complexity for users , administrators , and developers . however , there seems to be a need for an updated definition of the term `` organic computing '' , of desired properties of technical , organic systems , and the objectives of the organic computing initiative . with this article , we will address these points .

the spaces of data , information , and knowledge
we study the data space $ d $ of any given data set $ x $ and explain how functions and relations are defined over $ d $ . from $ d $ and for a specific domain $ \delta $ we construct the information space $ i $ of $ x $ by interpreting variables , functions , and explicit relations over $ d $ in $ \delta $ and by including other relations that $ d $ implies under the interpretation in $ \delta $ . then from $ i $ we build up the knowledge space $ k $ of $ x $ as the product of two spaces $ k_t $ and $ k_p $ , where $ k_t $ is obtained from $ i $ by using the induction principle to generalize propositional relations to quantified relations , the deduction principle to generate new relations , and standard mechanisms to validate relations and $ k_p $ is the space of specifications of methods with operational instructions which are valid in $ k_t $ . through our construction of the three topological spaces the following key observation is made clear : the retrieval of information from the given data set for $ \delta $ consists essentially in mining domain objects and relations , and the discovery of knowledge from the retrieved information consists essentially in applying the induction and deduction principles to generate propositions , synthesizing and modeling the information to generate specifications of methods with operational instructions , and validating the propositions and specifications . based on this observation , efficient approaches may be designed to discover profound knowledge automatically from simple data , as demonstrated by the result of our study in the case of geometry .

some complexity considerations in the combination of belief networks
one topic that is likely to attract an increasing amount of attention within the knowledge-base systems research community is the coordination of information provided by multiple experts . we envision a situation in which several experts independently encode information as belief networks . a potential user must then coordinate the conclusions and recommendations of these networks to derive some sort of consensus . one approach to such a consensus is the fusion of the contributed networks into a single , consensus model prior to the consideration of any case-specific data ( specific observations , test results ) . this approach requires two types of combination procedures , one for probabilities , and one for graphs . since the combination of probabilities is relatively well understood , the key barriers to this approach lie in the realm of graph theory . this paper provides formal definitions of some of the operations necessary to effect the necessary graphical combinations , and provides complexity analyses of these procedures . the paper 's key result is that most of these operations are np-hard , and its primary message is that the derivation of ? good ? consensus networks must be done heuristically .

a fast integrated planning and control framework for autonomous driving via imitation learning
for safe and efficient planning and control in autonomous driving , we need a driving policy which can achieve desirable driving quality in long-term horizon with guaranteed safety and feasibility . optimization-based approaches , such as model predictive control ( mpc ) , can provide such optimal policies , but their computational complexity is generally unacceptable for real-time implementation . to address this problem , we propose a fast integrated planning and control framework that combines learning- and optimization-based approaches in a two-layer hierarchical structure . the first layer , defined as the `` policy layer '' , is established by a neural network which learns the long-term optimal driving policy generated by mpc . the second layer , called the `` execution layer '' , is a short-term optimization-based controller that tracks the reference trajecotries given by the `` policy layer '' with guaranteed short-term safety and feasibility . moreover , with efficient and highly-representative features , a small-size neural network is sufficient in the `` policy layer '' to handle many complicated driving scenarios . this renders online imitation learning with dataset aggregation ( dagger ) so that the performance of the `` policy layer '' can be improved rapidly and continuously online . several exampled driving scenarios are demonstrated to verify the effectiveness and efficiency of the proposed framework .

three approaches to probability model selection
this paper compares three approaches to the problem of selecting among probability models to fit data ( 1 ) use of statistical criteria such as akaike 's information criterion and schwarz 's `` bayesian information criterion , '' ( 2 ) maximization of the posterior probability of the model , and ( 3 ) maximization of an effectiveness ratio ? trading off accuracy and computational cost . the unifying characteristic of the approaches is that all can be viewed as maximizing a penalized likelihood function . the second approach with suitable prior distributions has been shown to reduce to the first . this paper shows that the third approach reduces to the second for a particular form of the effectiveness ratio , and illustrates all three approaches with the problem of selecting the number of components in a mixture of gaussian distributions . unlike the first two approaches , the third can be used even when the candidate models are chosen for computational efficiency , without regard to physical interpretation , so that the likelihood and the prior distribution over models can not be interpreted literally . as the most general and computationally oriented of the approaches , it is especially useful for artificial intelligence applications .

an architecture for the evaluation of intelligent systems
one of the main research areas in artificial intelligence is the coding of agents ( programs ) which are able to learn by themselves in any situation . this means that agents must be useful for purposes other than those they were created for , as , for example , playing chess . in this way we try to get closer to the pristine goal of artificial intelligence . one of the problems to decide whether an agent is really intelligent or not is the measurement of its intelligence , since there is currently no way to measure it in a reliable way . the purpose of this project is to create an interpreter that allows for the execution of several environments , including those which are generated randomly , so that an agent ( a person or a program ) can interact with them . once the interaction between the agent and the environment is over , the interpreter will measure the intelligence of the agent according to the actions , states and rewards the agent has undergone inside the environment during the test . as a result we will be able to measure agents ' intelligence in any possible environment , and to make comparisons between several agents , in order to determine which of them is the most intelligent . in order to perform the tests , the interpreter must be able to randomly generate environments that are really useful to measure agents ' intelligence , since not any randomly generated environment will serve that purpose .

tableau-based decision procedure for the multi-agent epistemic logic with all coalitional operators for common and distributed knowledge
we develop a conceptually clear , intuitive , and feasible decision procedure for testing satisfiability in the full multi-agent epistemic logic cmael ( cd ) with operators for common and distributed knowledge for all coalitions of agents mentioned in the language . to that end , we introduce hintikka structures for cmael ( cd ) and prove that satisfiability in such structures is equivalent to satisfiability in standard models . using that result , we design an incremental tableau-building procedure that eventually constructs a satisfying hintikka structure for every satisfiable input set of formulae of cmael ( cd ) and closes for every unsatisfiable input set of formulae .

conditional probability generation methods for high reliability effects-based decision making
decision making is often based on bayesian networks . the building blocks for bayesian networks are its conditional probability tables ( cpts ) . these tables are obtained by parameter estimation methods , or they are elicited from subject matter experts ( sme ) . some of these knowledge representations are insufficient approximations . using knowledge fusion of cause and effect observations lead to better predictive decisions . we propose three new methods to generate cpts , which even work when only soft evidence is provided . the first two are novel ways of mapping conditional expectations to the probability space . the third is a column extraction method , which obtains cpts from nonlinear functions such as the multinomial logistic regression . case studies on military effects and burnt forest desertification have demonstrated that so derived cpts have highly reliable predictive power , including superiority over the cpts obtained from smes . in this context , new quality measures for determining the goodness of a cpt and for comparing cpts with each other have been introduced . the predictive power and enhanced reliability of decision making based on the novel cpt generation methods presented in this paper have been confirmed and validated within the context of the case studies .

pre-training neural networks with human demonstrations for deep reinforcement learning
deep reinforcement learning ( deep rl ) has achieved superior performance in complex sequential tasks by using a deep neural network as its function approximator and by learning directly from raw images . a drawback of using raw images is that deep rl must learn the state feature representation from the raw images in addition to learning a policy . as a result , deep rl can require a prohibitively large amount of training time and data to reach reasonable performance , making it difficult to use deep rl in real-world applications , especially when data is expensive . in this work , we speed up training by addressing half of what deep rl is trying to solve -- - learning features . our approach is to learn some of the important features by pre-training deep rl network 's hidden layers via supervised learning using a small set of human demonstrations . we empirically evaluate our approach using deep q-network ( dqn ) and asynchronous advantage actor-critic ( a3c ) algorithms on the atari 2600 games of pong , freeway , and beamrider . our results show that : 1 ) pre-training with human demonstrations in a supervised learning manner is better at discovering features relative to pre-training naively in dqn , and 2 ) initializing a deep rl network with a pre-trained model provides a significant improvement in training time even when pre-training from a small number of human demonstrations .

meteorology-aware multi-goal path planning for large-scale inspection missions with long-endurance solar-powered aircraft
solar-powered aircraft promise significantly increased flight endurance over conventional aircraft . while this makes them promising candidates for large-scale aerial inspection missions , their structural fragility necessitates that adverse weather is avoided using appropriate path planning methods . this paper therefore presents metpass , the meteorology-aware path planning and analysis software for solar-powered uavs . metpass is the first path planning framework in the literature that considers all aspects that influence the safety or performance of solar-powered flight : it avoids environmental risks ( thunderstorms , rain , wind , wind gusts and humidity ) and exploits advantageous regions ( high sun radiation or tailwind ) . it also avoids system risks such as low battery state of charge and returns safe paths through cluttered terrain . metpass imports weather data from global meteorological models , propagates the aircraft state through an energetic system model , and then combines both into a cost function . a combination of dynamic programming techniques and an a*-search-algorithm with a custom heuristic is leveraged to plan globally optimal paths in station-keeping , point-to-point or multi-goal aerial inspection missions with coverage guarantees . a full software implementation including a gui is provided . the planning methods are verified using three missions of eth zurich 's atlantiksolar uav : an 81-hour continuous solar-powered station-keeping flight , a 4000km atlantic crossing from newfoundland to portugal , and two multi-glacier aerial inspection missions above the arctic ocean performed near greenland in summer 2017 .

evaluation measures for hierarchical classification : a unified view and novel approaches
hierarchical classification addresses the problem of classifying items into a hierarchy of classes . an important issue in hierarchical classification is the evaluation of different classification algorithms , which is complicated by the hierarchical relations among the classes . several evaluation measures have been proposed for hierarchical classification using the hierarchy in different ways . this paper studies the problem of evaluation in hierarchical classification by analyzing and abstracting the key components of the existing performance measures . it also proposes two alternative generic views of hierarchical evaluation and introduces two corresponding novel measures . the proposed measures , along with the state-of-the art ones , are empirically tested on three large datasets from the domain of text classification . the empirical results illustrate the undesirable behavior of existing approaches and how the proposed methods overcome most of these methods across a range of cases .

how hard is it to control an election by breaking ties ?
we study the computational complexity of controlling the result of an election by breaking ties strategically . this problem is equivalent to the problem of deciding the winner of an election under parallel universes tie-breaking . when the chair of the election is only asked to break ties to choose between one of the co-winners , the problem is trivially easy . however , in multi-round elections , we prove that it can be np-hard for the chair to compute how to break ties to ensure a given result . additionally , we show that the form of the tie-breaking function can increase the opportunities for control . indeed , we prove that it can be np-hard to control an election by breaking ties even with a two-stage voting rule .

information/relevance influence diagrams
in this paper we extend the influence diagram ( id ) representation for decisions under uncertainty . in the standard id , arrows into a decision node are only informational ; they do not represent constraints on what the decision maker can do . we can represent such constraints only indirectly , using arrows to the children of the decision and sometimes adding more variables to the influence diagram , thus making the id more complicated . users of influence diagrams often want to represent constraints by arrows into decision nodes . we represent constraints on decisions by allowing relevance arrows into decision nodes . we call the resulting representation information/relevance influence diagrams ( irids ) . information/relevance influence diagrams allow for direct representation and specification of constrained decisions . we use a combination of stochastic dynamic programming and gibbs sampling to solve irids . this method is especially useful when exact methods for solving ids fail .

fundamental parameters of main-sequence stars in an instant with machine learning
owing to the remarkable photometric precision of space observatories like kepler , stellar and planetary systems beyond our own are now being characterized en masse for the first time . these characterizations are pivotal for endeavors such as searching for earth-like planets and solar twins , understanding the mechanisms that govern stellar evolution , and tracing the dynamics of our galaxy . the volume of data that is becoming available , however , brings with it the need to process this information accurately and rapidly . while existing methods can constrain fundamental stellar parameters such as ages , masses , and radii from these observations , they require substantial computational efforts to do so . we develop a method based on machine learning for rapidly estimating fundamental parameters of main-sequence solar-like stars from classical and asteroseismic observations . we first demonstrate this method on a hare-and-hound exercise and then apply it to the sun , 16 cyg a & b , and 34 planet-hosting candidates that have been observed by the kepler spacecraft . we find that our estimates and their associated uncertainties are comparable to the results of other methods , but with the additional benefit of being able to explore many more stellar parameters while using much less computation time . we furthermore use this method to present evidence for an empirical diffusion-mass relation . our method is open source and freely available for the community to use . the source code for all analyses and for all figures appearing in this manuscript can be found electronically at https : //github.com/earlbellinger/asteroseismology

integrating learning from examples into the search for diagnostic policies
this paper studies the problem of learning diagnostic policies from training examples . a diagnostic policy is a complete description of the decision-making actions of a diagnostician ( i.e. , tests followed by a diagnostic decision ) for all possible combinations of test results . an optimal diagnostic policy is one that minimizes the expected total cost , which is the sum of measurement costs and misdiagnosis costs . in most diagnostic settings , there is a tradeoff between these two kinds of costs . this paper formalizes diagnostic decision making as a markov decision process ( mdp ) . the paper introduces a new family of systematic search algorithms based on the ao* algorithm to solve this mdp . to make ao* efficient , the paper describes an admissible heuristic that enables ao* to prune large parts of the search space . the paper also introduces several greedy algorithms including some improvements over previously-published methods . the paper then addresses the question of learning diagnostic policies from examples . when the probabilities of diseases and test results are computed from training data , there is a great danger of overfitting . to reduce overfitting , regularizers are integrated into the search algorithms . finally , the paper compares the proposed methods on five benchmark diagnostic data sets . the studies show that in most cases the systematic search methods produce better diagnostic policies than the greedy methods . in addition , the studies show that for training sets of realistic size , the systematic search algorithms are practical on todays desktop computers .

learning to make predictions on graphs with autoencoders
we examine two fundamental tasks associated with graph representation learning : link prediction and semi-supervised node classification . we present a densely connected autoencoder architecture capable of learning a joint representation of both local graph structure and available external node features for the multi-task learning of link prediction and node classification . to the best of our knowledge , this is the first architecture that can be efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification . we provide comprehensive empirical evaluation of our models on a range of challenging benchmark graph-structured datasets , and demonstrate significant improvement in accuracy over related methods for graph representation learning . code implementation is available at https : //github.com/vuptran/graph-representation-learning

reduction of maximum entropy models to hidden markov models
we show that maximum entropy ( maxent ) models can be modeled with certain kinds of hmms , allowing us to construct maxent models with hidden variables , hidden state sequences , or other characteristics . the models can be trained using the forward-backward algorithm . while the results are primarily of theoretical interest , unifying apparently unrelated concepts , we also give experimental results for a maxent model with a hidden variable on a word disambiguation task ; the model outperforms standard techniques .

regnet : multimodal sensor registration using deep neural networks
in this paper , we present regnet , the first deep convolutional neural network ( cnn ) to infer a 6 degrees of freedom ( dof ) extrinsic calibration between multimodal sensors , exemplified using a scanning lidar and a monocular camera . compared to existing approaches , regnet casts all three conventional calibration steps ( feature extraction , feature matching and global regression ) into a single real-time capable cnn . our method does not require any human interaction and bridges the gap between classical offline and target-less online calibration approaches as it provides both a stable initial estimation as well as a continuous online correction of the extrinsic parameters . during training we randomly decalibrate our system in order to train regnet to infer the correspondence between projected depth measurements and rgb image and finally regress the extrinsic calibration . additionally , with an iterative execution of multiple cnns , that are trained on different magnitudes of decalibration , our approach compares favorably to state-of-the-art methods in terms of a mean calibration error of 0.28 degrees for the rotational and 6 cm for the translation components even for large decalibrations up to 1.5 m and 20 degrees .

a quantitative analysis of multi-winner rules
to choose a multi-winner rule , i.e. , a voting rule that selects a subset of $ k $ alternatives based on preferences of a certain population , is a hard and ambiguous task . depending on the context , it varies widely what constitutes an `` optimal '' committee . in this paper , we offer a new perspective to measure the quality of committees and -- -consequently -- -multi-winner rules . we provide a quantitative analysis using methods from the theory of approximation algorithms and estimate how well multi-winner rules approximate two extreme objectives : diversity as captured by the ( approval ) chamberlin -- courant rule ( cc ) and individual excellence as captured by approval voting ( av ) . with both theoretical and experimental methods we establish a classification of multi-winner rules in terms of their quantitative alignment with these two opposing objectives .

self corrective perturbations for semantic segmentation and classification
convolutional neural networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems . however , the behavior of deep networks is yet to be fully understood and is still an active area of research . in this work , we present an intriguing behavior : pre-trained cnns can be made to improve their predictions by structurally perturbing the input . we observe that these perturbations - referred as guided perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights . we perform various ablative experiments to understand how these perturbations affect the local context and feature representations . furthermore , we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the pascal voc dataset and supervised classification tasks on mnist and cifar10 datasets .

markov chains on orbits of permutation groups
we present a novel approach to detecting and utilizing symmetries in probabilistic graphical models with two main contributions . first , we present a scalable approach to computing generating sets of permutation groups representing the symmetries of graphical models . second , we introduce orbital markov chains , a novel family of markov chains leveraging model symmetries to reduce mixing times . we establish an insightful connection between model symmetries and rapid mixing of orbital markov chains . thus , we present the first lifted mcmc algorithm for probabilistic graphical models . both analytical and empirical results demonstrate the effectiveness and efficiency of the approach .

path consistency learning in tsallis entropy regularized mdps
we study the sparse entropy-regularized reinforcement learning ( erl ) problem in which the entropy term is a special form of the tsallis entropy . the optimal policy of this formulation is sparse , i.e. , ~at each state , it has non-zero probability for only a small number of actions . this addresses the main drawback of the standard shannon entropy-regularized rl ( soft erl ) formulation , in which the optimal policy is softmax , and thus , may assign a non-negligible probability mass to non-optimal actions . this problem is aggravated as the number of actions is increased . in this paper , we follow the work of nachum et al . ( 2017 ) in the soft erl setting , and propose a class of novel path consistency learning ( pcl ) algorithms , called { \em sparse pcl } , for the sparse erl problem that can work with both on-policy and off-policy data . we first derive a { \em sparse consistency } equation that specifies a relationship between the optimal value function and policy of the sparse erl along any system trajectory . crucially , a weak form of the converse is also true , and we quantify the sub-optimality of a policy which satisfies sparse consistency , and show that as we increase the number of actions , this sub-optimality is better than that of the soft erl optimal policy . we then use this result to derive the sparse pcl algorithms . we empirically compare sparse pcl with its soft counterpart , and show its advantage , especially in problems with a large number of actions .

on the existence and convergence computable universal priors
solomonoff unified occam 's razor and epicurus ' principle of multiple explanations to one elegant , formal , universal theory of inductive inference , which initiated the field of algorithmic information theory . his central result is that the posterior of his universal semimeasure m converges rapidly to the true sequence generating posterior mu , if the latter is computable . hence , m is eligible as a universal predictor in case of unknown mu . we investigate the existence and convergence of computable universal ( semi ) measures for a hierarchy of computability classes : finitely computable , estimable , enumerable , and approximable . for instance , m is known to be enumerable , but not finitely computable , and to dominate all enumerable semimeasures . we define seven classes of ( semi ) measures based on these four computability concepts . each class may or may not contain a ( semi ) measure which dominates all elements of another class . the analysis of these 49 cases can be reduced to four basic cases , two of them being new . the results hold for discrete and continuous semimeasures . we also investigate more closely the types of convergence , possibly implied by universality : in difference and in ratio , with probability 1 , in mean sum , and for martin-loef random sequences . we introduce a generalized concept of randomness for individual sequences and use it to exhibit difficulties regarding these issues .

argudas : arguing with gene expression information
in situ hybridisation gene expression information helps biologists identify where a gene is expressed . however , the databases that republish the experimental information are often both incomplete and inconsistent . this paper examines a system , argudas , designed to help tackle these issues . argudas is an evolution of an existing system , and so that system is reviewed as a means of both explaining and justifying the behaviour of argudas . throughout the discussion of argudas a number of issues will be raised including the appropriateness of argumentation in biology and the challenges faced when integrating apparently similar online biological databases .

fast algorithms for game-theoretic centrality measures
in this dissertation , we analyze the computational properties of game-theoretic centrality measures . the key idea behind game-theoretic approach to network analysis is to treat nodes as players in a cooperative game , where the value of each coalition of nodes is determined by certain graph properties . next , the centrality of any individual node is determined by a chosen game-theoretic solution concept ( notably , the shapley value ) in the same way as the payoff of a player in a cooperative game . on one hand , the advantage of game-theoretic centrality measures is that nodes are ranked not only according to their individual roles but also according to how they contribute to the role played by all possible subsets of nodes . on the other hand , the disadvantage is that the game-theoretic solution concepts are typically computationally challenging . the main contribution of this dissertation is that we show that a wide variety of game-theoretic solution concepts on networks can be computed in polynomial time . our focus is on centralities based on the shapley value and its various extensions , such as the semivalues and coalitional semivalues . furthermore , we prove # p-hardness of computing the shapley value in connectivity games and propose an algorithm to compute it . finally , we analyse computational properties of generalized version of cooperative games in which order of player matters . we propose a new representation for such games , called generalized marginal contribution networks , that allows for polynomial computation in the size of the representation of two dedicated extensions of the shapley value to this class of games .

improving the competency of first-order ontologies
we introduce a new framework to evaluate and improve first-order ( fo ) ontologies using automated theorem provers ( atps ) on the basis of competency questions ( cqs ) . our framework includes both the adaptation of a methodology for evaluating ontologies to the framework of first-order logic and a new set of non-trivial cqs designed to evaluate fo versions of sumo , which significantly extends the very small set of cqs proposed in the literature . most of these new cqs have been automatically generated from a small set of patterns and the mapping of wordnet to sumo . applying our framework , we demonstrate that adimen-sumo v2.2 outperforms tptp-sumo . in addition , using the feedback provided by atps we have set an improved version of adimen-sumo ( v2.4 ) . this new version outperforms the previous ones in terms of competency . for instance , `` humans can reason '' is automatically inferred from adimen-sumo v2.4 , while it is neither deducible from tptp-sumo nor adimen-sumo v2.2 .

probabilistic models for agents ' beliefs and decisions
many applications of intelligent systems require reasoning about the mental states of agents in the domain . we may want to reason about an agent 's beliefs , including beliefs about other agents ; we may also want to reason about an agent 's preferences , and how his beliefs and preferences relate to his behavior . we define a probabilistic epistemic logic ( pel ) in which belief statements are given a formal semantics , and provide an algorithm for asserting and querying pel formulas in bayesian networks . we then show how to reason about an agent 's behavior by modeling his decision process as an influence diagram and assuming that he behaves rationally . pel can then be used for reasoning from an agent 's observed actions to conclusions about other aspects of the domain , including unobserved domain variables and the agent 's mental states .

local optima networks and the performance of iterated local search
local optima networks ( lons ) have been recently proposed as an alternative model of combinatorial fitness landscapes . the model compresses the information given by the whole search space into a smaller mathematical object that is the graph having as vertices the local optima and as edges the possible weighted transitions between them . a new set of metrics can be derived from this model that capture the distribution and connectivity of the local optima in the underlying configuration space . this paper departs from the descriptive analysis of local optima networks , and actively studies the correlation between network features and the performance of a local search heuristic . the nk family of landscapes and the iterated local search metaheuristic are considered . with a statistically-sound approach based on multiple linear regression , it is shown that some lons ' features strongly influence and can even partly predict the performance of a heuristic search algorithm . this study validates the expressive power of lons as a model of combinatorial fitness landscapes .

intelligent anticipated exploration of web sites
in this paper we describe a web search agent , called global search agent ( hereafter gsa for short ) . gsa integrates and enhances several search techniques in order to achieve significant improvements in the user-perceived quality of delivered information as compared to usual web search engines . gsa features intelligent merging of relevant documents from different search engines , anticipated selective exploration and evaluation of links from the current result set , automated derivation of refined queries based on user relevance feedback . system architecture as well as experimental accounts are also illustrated .

large neighborhood-based metaheuristic and branch-and-price for the pickup and delivery problem with split loads
we consider the multi-vehicle one-to-one pickup and delivery problem with split loads , a np-hard problem linked with a variety of applications for bulk product transportation , bike-sharing systems and inventory re-balancing . this problem is notoriously difficult due to the interaction of two challenging vehicle routing attributes , `` pickups and deliveries '' and `` split deliveries '' . this possibly leads to optimal solutions of a size that grows exponentially with the instance size , containing multiple visits per customer pair , even in the same route . to solve this problem , we propose an iterated local search metaheuristic as well as a branch-and-price algorithm . the core of the metaheuristic consists of a new large neighborhood search , which reduces the problem of finding the best insertion combination of a pickup and delivery pair into a route ( with possible splits ) to a resource-constrained shortest path and knapsack problem . similarly , the branch-and-price algorithm uses sophisticated labeling techniques , route relaxations , pre-processing and branching rules for an efficient resolution . our computational experiments on classical single-vehicle instances demonstrate the excellent performance of the metaheuristic , which produces new best known solutions for 92 out of 93 test instances , and outperforms all previous algorithms . experimental results on new multi-vehicle instances with distance constraints are also reported . the branch-and-price algorithm produces optimal solutions for instances with up to 20 pickup-and-delivery pairs , and very accurate solutions are found by the metaheuristic .

modular multi-objective deep reinforcement learning with decision values
in this work we present a method for using deep q-networks ( dqns ) in multi-objective environments . deep q-networks provide remarkable performance in single objective problems learning from high-level visual state representations . however , in many scenarios ( e.g in robotics , games ) , the agent needs to pursue multiple objectives simultaneously . we propose an architecture in which separate dqns are used to control the agent 's behaviour with respect to particular objectives . in this architecture we introduce decision values to improve the scalarization of multiple dqns into a single action . our architecture enables the decomposition of the agent 's behaviour into controllable and replaceable sub-behaviours learned by distinct modules . moreover , it allows to change the priorities of particular objectives post-learning , while preserving the overall performance of the agent . to evaluate our solution we used a game-like simulator in which an agent - provided with high-level visual input - pursues multiple objectives in a 2d world .

steady state resource allocation analysis of the stochastic diffusion search
this article presents the long-term behaviour analysis of stochastic diffusion search ( sds ) , a distributed agent-based system for best-fit pattern matching . sds operates by allocating simple agents into different regions of the search space . agents independently pose hypotheses about the presence of the pattern in the search space and its potential distortion . assuming a compositional structure of hypotheses about pattern matching agents perform an inference on the basis of partial evidence from the hypothesised solution . agents posing mutually consistent hypotheses about the pattern support each other and inhibit agents with inconsistent hypotheses . this results in the emergence of a stable agent population identifying the desired solution . positive feedback via diffusion of information between the agents significantly contributes to the speed with which the solution population is formed . the formulation of the sds model in terms of interacting markov chains enables its characterisation in terms of the allocation of agents , or computational resources . the analysis characterises the stationary probability distribution of the activity of agents , which leads to the characterisation of the solution population in terms of its similarity to the target pattern .

mean field variational approximation for continuous-time bayesian networks
continuous-time bayesian networks is a natural structured representation language for multicomponent stochastic processes that evolve continuously over time . despite the compact representation , inference in such models is intractable even in relatively simple structured networks . here we introduce a mean field variational approximation in which we use a product of inhomogeneous markov processes to approximate a distribution over trajectories . this variational approach leads to a globally consistent distribution , which can be efficiently queried . additionally , it provides a lower bound on the probability of observations , thus making it attractive for learning tasks . we provide the theoretical foundations for the approximation , an efficient implementation that exploits the wide range of highly optimized ordinary differential equations ( ode ) solvers , experimentally explore characterizations of processes for which this approximation is suitable , and show applications to a large-scale realworld inference problem .

the decision-theoretic interactive video advisor
the need to help people choose among large numbers of items and to filter through large amounts of information has led to a flood of research in construction of personal recommendation agents . one of the central issues in constructing such agents is the representation and elicitation of user preferences or interests . this topic has long been studied in decision theory , but surprisingly little work in the area of recommender systems has made use of formal decision-theoretic techniques . this paper describes diva , a decision-theoretic agent for recommending movies that contains a number of novel features . diva represents user preferences using pairwise comparisons among items , rather than numeric ratings . it uses a novel similarity measure based on the concept of the probability of conflict between two orderings of items . the system has a rich representation of preference , distinguishing between a user 's general taste in movies and his immediate interests . it takes an incremental approach to preference elicitation in which the user can provide feedback if not satisfied with the recommendation list . we empirically evaluate the performance of the system using the eachmovie collaborative filtering database .

a mathematical theory of deep convolutional neural networks for feature extraction
deep convolutional neural networks have led to breakthrough results in numerous practical machine learning tasks such as classification of images in the imagenet data set , control-policy-learning to play atari games or the board game go , and image captioning . many of these applications first perform feature extraction and then feed the results thereof into a trainable classifier . the mathematical analysis of deep convolutional neural networks for feature extraction was initiated by mallat , 2012. specifically , mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer , and proved translation invariance ( asymptotically in the wavelet scale parameter ) and deformation stability of the corresponding feature extractor . this paper complements mallat 's results by developing a theory that encompasses general convolutional transforms , or in more technical parlance , general semi-discrete frames ( including weyl-heisenberg filters , curvelets , shearlets , ridgelets , wavelets , and learned filters ) , general lipschitz-continuous non-linearities ( e.g. , rectified linear units , shifted logistic sigmoids , hyperbolic tangents , and modulus functions ) , and general lipschitz-continuous pooling operators emulating , e.g. , sub-sampling and averaging . in addition , all of these elements can be different in different network layers . for the resulting feature extractor we prove a translation invariance result of vertical nature in the sense of the features becoming progressively more translation-invariant with increasing network depth , and we establish deformation sensitivity bounds that apply to signal classes such as , e.g. , band-limited functions , cartoon functions , and lipschitz functions .

characterizing and reasoning about probabilistic and non-probabilistic expectation
expectation is a central notion in probability theory . the notion of expectation also makes sense for other notions of uncertainty . we introduce a propositional logic for reasoning about expectation , where the semantics depends on the underlying representation of uncertainty . we give sound and complete axiomatizations for the logic in the case that the underlying representation is ( a ) probability , ( b ) sets of probability measures , ( c ) belief functions , and ( d ) possibility measures . we show that this logic is more expressive than the corresponding logic for reasoning about likelihood in the case of sets of probability measures , but equi-expressive in the case of probability , belief , and possibility . finally , we show that satisfiability for these logics is np-complete , no harder than satisfiability for propositional logic .

logarithmic-time updates and queries in probabilistic networks
traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database . our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency . we propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected bayesian networks . in the conventional algorithm , new evidence is absorbed in o ( 1 ) time and queries are processed in time o ( n ) , where n is the size of the network . we propose an algorithm which , after a preprocessing phase , allows us to answer queries in time o ( log n ) at the expense of o ( log n ) time per evidence absorption . the usefulness of sub-linear processing time manifests itself in applications requiring ( near ) real-time response over large probabilistic databases . we briefly discuss a potential application of dynamic probabilistic reasoning in computational biology .

interactive policy learning through confidence-based autonomy
we present confidence-based autonomy ( cba ) , an interactive algorithm for policy learning from demonstration . the cba algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents . the first component , confident execution , enables the agent to identify states in which demonstration is required , to request a demonstration from the human teacher and to learn a policy based on the acquired data . the algorithm selects demonstrations based on a measure of action selection confidence , and our results show that using confident execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher . the second algorithmic component , corrective demonstration , enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance . cba and its individual components are compared and evaluated in a complex simulated driving domain . the complete cba algorithm results in the best overall learning performance , successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning .

study : symmetry breaking for asp
in their nature configuration problems are combinatorial ( optimization ) problems . in order to find a configuration a solver has to instantiate a number of components of a some type and each of these components can be used in a relation defined for a type . therefore , many solutions of a configuration problem have symmetric ones which can be obtained by replacing some component of a solution by another one of the same type . these symmetric solutions decrease performance of optimization algorithms because of two reasons : a ) they satisfy all requirements and can not be pruned out from the search space ; and b ) existence of symmetric optimal solutions does not allow to prove the optimum in a feasible time .

a novel regularized principal graph learning framework on explicit graph representation
many scientific datasets are of high dimension , and the analysis usually requires visual manipulation by retaining the most important structures of data . principal curve is a widely used approach for this purpose . however , many existing methods work only for data with structures that are not self-intersected , which is quite restrictive for real applications . a few methods can overcome the above problem , but they either require complicated human-made rules for a specific task with lack of convergence guarantee and adaption flexibility to different tasks , or can not obtain explicit structures of data . to address these issues , we develop a new regularized principal graph learning framework that captures the local information of the underlying graph structure based on reversed graph embedding . as showcases , models that can learn a spanning tree or a weighted undirected $ \ell_1 $ graph are proposed , and a new learning algorithm is developed that learns a set of principal points and a graph structure from data , simultaneously . the new algorithm is simple with guaranteed convergence . we then extend the proposed framework to deal with large-scale data . experimental results on various synthetic and six real world datasets show that the proposed method compares favorably with baselines and can uncover the underlying structure correctly .

characterizing quantifier fuzzification mechanisms : a behavioral guide for practical applications
important advances have been made in the fuzzy quantification field . nevertheless , some problems remain when we face the decision of selecting the most convenient model for a specific application . in the literature , several desirable adequacy properties have been proposed , but theoretical limits impede quantification models from simultaneously fulfilling every adequacy property that has been defined . besides , the complexity of model definitions and adequacy properties makes very difficult for real users to understand the particularities of the different models that have been presented . in this work we will present several criteria conceived to help in the process of selecting the most adequate quantifier fuzzification mechanisms for specific practical applications . in addition , some of the best known well-behaved models will be compared against this list of criteria . based on this analysis , some guidance to choose fuzzy quantification models for practical applications will be provided .

eca-ruleml : an approach combining eca rules with temporal interval-based kr event/action logics and transactional update logics
an important problem to be addressed within event-driven architecture ( eda ) is how to correctly and efficiently capture and process the event/action-based logic . this paper endeavors to bridge the gap between the knowledge representation ( kr ) approaches based on durable events/actions and such formalisms as event calculus , on one hand , and event-condition-action ( eca ) reaction rules extending the approach of active databases that view events as instantaneous occurrences and/or sequences of events , on the other . we propose formalism based on reaction rules ( eca rules ) and a novel interval-based event logic and present concrete ruleml-based syntax , semantics and implementation . we further evaluate this approach theoretically , experimentally and on an example derived from common industry use cases and illustrate its benefits .

parallel local search for solving constraint problems on the cell broadband engine ( preliminary results )
we explore the use of the cell broadband engine ( cell/be for short ) for combinatorial optimization applications : we present a parallel version of a constraint-based local search algorithm that has been implemented on a multiprocessor bladecenter machine with twin cell/be processors ( total of 16 spus per blade ) . this algorithm was chosen because it fits very well the cell/be architecture and requires neither shared memory nor communication between processors , while retaining a compact memory footprint . we study the performance on several large optimization benchmarks and show that this achieves mostly linear time speedups , even sometimes super-linear . this is possible because the parallel implementation might explore simultaneously different parts of the search space and therefore converge faster towards the best sub-space and thus towards a solution . besides getting speedups , the resulting times exhibit a much smaller variance , which benefits applications where a timely reply is critical .

the opacity of backbones and backdoors under a weak assumption
backdoors and backbones of boolean formulas are hidden structural properties . a natural goal , already in part realized , is that solver algorithms seek to obtain substantially better performance by exploiting these structures . however , the present paper is not intended to improve the performance of sat solvers , but rather is a cautionary paper . in particular , the theme of this paper is that there is a potential chasm between the existence of such structures in the boolean formula and being able to effectively exploit them . this does not mean that these structures are not useful to solvers . it does mean that one must be very careful not to assume that it is computationally easy to go from the existence of a structure to being able to get one 's hands on it and/or being able to exploit the structure . for example , in this paper we show that , under the assumption that p $ \neq $ np , there are easily recognizable sets of boolean formulas for which it is hard to determine whether they have a large backbone . we also show that , also under the assumption p $ \neq $ np , there are easily recognizable families of boolean formulas with strong backdoors that are easy to find , yet for which it is hard to determine whether they are satisfiable .

causes of ineradicable spurious predictions in qualitative simulation
it was recently proved that a sound and complete qualitative simulator does not exist , that is , as long as the input-output vocabulary of the state-of-the-art qsim algorithm is used , there will always be input models which cause any simulator with a coverage guarantee to make spurious predictions in its output . in this paper , we examine whether a meaningfully expressive restriction of this vocabulary is possible so that one can build a simulator with both the soundness and completeness properties . we prove several negative results : all sound qualitative simulators , employing subsets of the qsim representation which retain the operating region transition feature , and support at least the addition and constancy constraints , are shown to be inherently incomplete . even when the simulations are restricted to run in a single operating region , a constraint vocabulary containing just the addition , constancy , derivative , and multiplication relations makes the construction of sound and complete qualitative simulators impossible .

task-driven visual saliency and attention-based visual question answering
visual question answering ( vqa ) has witnessed great progress since may , 2015 as a classic problem unifying visual and textual data into a system . many enlightening vqa works explore deep into the image and question encodings and fusing methods , of which attention is the most effective and infusive mechanism . current attention based methods focus on adequate fusion of visual and textual features , but lack the attention to where people focus to ask questions about the image . traditional attention based methods attach a single value to the feature at each spatial location , which losses many useful information . to remedy these problems , we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional lstm ( bilstm ) , and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features . we conduct experiments on the large-scale coco-vqa dataset and analyze the effectiveness of our model demonstrated by strong empirical results .

solving map exactly using systematic search
map is the problem of finding a most probable instantiation of a set of variables in a bayesian network given some evidence . unlike computing posterior probabilities , or mpe ( a special case of map ) , the time and space complexity of structural solutions for map are not only exponential in the network treewidth , but in a larger parameter known as the `` constrained '' treewidth . in practice , this means that computing map can be orders of magnitude more expensive than computing posterior probabilities or mpe . this paper introduces a new , simple upper bound on the probability of a map solution , which admits a tradeoff between the bound quality and the time needed to compute it . the bound is shown to be generally much tighter than those of other methods of comparable complexity . we use this proposed upper bound to develop a branch-and-bound search algorithm for solving map exactly . experimental results demonstrate that the search algorithm is able to solve many problems that are far beyond the reach of any structure-based method for map . for example , we show that the proposed algorithm can compute map exactly and efficiently for some networks whose constrained treewidth is more than 40 .

coalition formation for multi-agent pursuit based on neural network and agrmf model
an approach for coalition formation of multi-agent pursuit based on neural network and agrmf model is proposed.this paper constructs a novel neural work called agrmf-ann which consists of feature extraction part and group generation part . on one hand , the convolutional layers of feature extraction part can abstract the features of agent group role membership function ( agrmf ) for all of the groups , on the other hand , those features will be fed to the group generation part based on self-organizing map ( som ) layer which is used to group the pursuers with similar features in the same group . besides , we also come up the group attractiveness function ( gaf ) to evaluate the quality of groups and the pursuers contribution in order to adjust the main ability indicators of agrmf and other weight of all neural network . the simulation experiment showed that this proposal can improve the effectiveness of coalition formation for multi-agent pursuit and ability to adopt pursuit-evasion problem with the scale of pursuer team growing .

dimension reduction in singularly perturbed continuous-time bayesian networks
continuous-time bayesian networks ( ctbns ) are graphical representations of multi-component continuous-time markov processes as directed graphs . the edges in the network represent direct influences among components . the joint rate matrix of the multi-component process is specified by means of conditional rate matrices for each component separately . this paper addresses the situation where some of the components evolve on a time scale that is much shorter compared to the time scale of the other components . in this paper , we prove that in the limit where the separation of scales is infinite , the markov process converges ( in distribution , or weakly ) to a reduced , or effective markov process that only involves the slow components . we also demonstrate that for reasonable separation of scale ( an order of magnitude ) the reduced process is a good approximation of the marginal process over the slow components . we provide a simple procedure for building a reduced ctbn for this effective process , with conditional rate matrices that can be directly calculated from the original ctbn , and discuss the implications for approximate reasoning in large systems .

arc consistency and friends
a natural and established way to restrict the constraint satisfaction problem is to fix the relations that can be used to pose constraints ; such a family of relations is called a constraint language . in this article , we study arc consistency , a heavily investigated inference method , and three extensions thereof from the perspective of constraint languages . we conduct a comparison of the studied methods on the basis of which constraint languages they solve , and we present new polynomial-time tractability results for singleton arc consistency , the most powerful method studied .

modelling serendipity in a computational context
building on a survey of previous theories of serendipity and creativity , we advance a sequential model of serendipitous occurrences . we distinguish between serendipity as a service and serendipity in the system itself , clarify the role of invention and discovery , and provide a measure for the serendipity potential of a system . while a system can arguably not be guaranteed to be serendipitous , it can have a high potential for serendipity . practitioners can use these theoretical tools to evaluate a computational system 's potential for unexpected behaviour that may have a beneficial outcome . in addition to a qualitative features of serendipity potential , the model also includes quantitative ratings that can guide development work . we show how the model is used in three case studies of existing and hypothetical systems , in the context of evolutionary computing , automated programming , and ( next-generation ) recommender systems . from this analysis , we extract recommendations for practitioners working with computational serendipity , and outline future directions for research .

clustering co-occurrence of maximal frequent patterns in streams
one way of getting a better view of data is using frequent patterns . in this paper frequent patterns are subsets that occur a minimal number of times in a stream of itemsets . however , the discovery of frequent patterns in streams has always been problematic . because streams are potentially endless it is in principle impossible to say if a pattern is often occurring or not . furthermore the number of patterns can be huge and a good overview of the structure of the stream is lost quickly . the proposed approach will use clustering to facilitate the analysis of the structure of the stream . a clustering on the co-occurrence of patterns will give the user an improved view on the structure of the stream . some patterns might occur so much together that they should form a combined pattern . in this way the patterns in the clustering will be the largest frequent patterns : maximal frequent patterns . our approach to decide if patterns occur often together will be based on a method of clustering when only the distance between pairs is known . the number of maximal frequent patterns is much smaller and combined with clustering methods these patterns provide a good view on the structure of the stream .

semantic evolutionary concept distances for effective information retrieval in query expansion
in this work several semantic approaches to concept-based query expansion and reranking schemes are studied and compared with different ontology-based expansion methods in web document search and retrieval . in particular , we focus on concept-based query expansion schemes , where , in order to effectively increase the precision of web document retrieval and to decrease the users browsing time , the main goal is to quickly provide users with the most suitable query expansion . two key tasks for query expansion in web document retrieval are to find the expansion candidates , as the closest concepts in web document domain , and to rank the expanded queries properly . the approach we propose aims at improving the expansion phase for better web document retrieval and precision . the basic idea is to measure the distance between candidate concepts using the pming distance , a collaborative semantic proximity measure , i.e . a measure which can be computed by using statistical results from web search engine . experiments show that the proposed technique can provide users with more satisfying expansion results and improve the quality of web document retrieval .

towards an intelligent tutor for mathematical proofs
computer-supported learning is an increasingly important form of study since it allows for independent learning and individualized instruction . in this paper , we discuss a novel approach to developing an intelligent tutoring system for teaching textbook-style mathematical proofs . we characterize the particularities of the domain and discuss common its design models . our approach is motivated by phenomena found in a corpus of tutorial dialogs that were collected in a wizard-of-oz experiment . we show how an intelligent tutor for textbook-style mathematical proofs can be built on top of an adapted assertion-level proof assistant by reusing representations and proof search strategies originally developed for automated and interactive theorem proving . the resulting prototype was successfully evaluated on a corpus of tutorial dialogs and yields good results .

second international nurse rostering competition ( inrc-ii ) -- - problem description and rules -- -
in this paper , we provide all information to participate to the second international nurse rostering competition ( inrc-ii ) . first , we describe the problem formulation , which , differently from inrc-i , is a multi-stage procedure . second , we illustrate all the necessary infrastructure do be used together with the participant 's solver , including the testbed , the file formats , and the validation/simulation tools . finally , we state the rules of the competition . all update-to-date information about the competition is available at http : //mobiz.vives.be/inrc2/ .

distributed deep reinforcement learning : learn how to play atari games in 21 minutes
we present a study in distributed deep reinforcement learning ( ddrl ) focused on scalability of a state-of-the-art deep reinforcement learning algorithm known as batch asynchronous advantage actorcritic ( ba3c ) . we show that using the adam optimization algorithm with a batch size of up to 2048 is a viable choice for carrying out large scale machine learning computations . this , combined with careful reexamination of the optimizer 's hyperparameters , using synchronous training on the node level ( while keeping the local , single node part of the algorithm asynchronous ) and minimizing the memory footprint of the model , allowed us to achieve linear scaling for up to 64 cpu nodes . this corresponds to a training time of 21 minutes on 768 cpu cores , as opposed to 10 hours when using a single node with 24 cores achieved by a baseline single-node implementation .

output space search for structured prediction
we consider a framework for structured prediction based on search in the space of complete structured outputs . given a structured input , an output is produced by running a time-bounded search procedure guided by a learned cost function , and then returning the least cost output uncovered during the search . this framework can be instantiated for a wide range of search spaces and search procedures , and easily incorporates arbitrary structured-prediction loss functions . in this paper , we make two main technical contributions . first , we define the limited-discrepancy search space over structured outputs , which is able to leverage powerful classification learning algorithms to improve the search space quality . second , we give a generic cost function learning approach , where the key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function . our experiments on six benchmark domains demonstrate that using our framework with only a small amount of search is sufficient for significantly improving on state-of-the-art structured-prediction performance .

from algorithmic black boxes to adaptive white boxes : declarative decision-theoretic ethical programs as codes of ethics
ethics of algorithms is an emerging topic in various disciplines such as social science , law , and philosophy , but also artificial intelligence ( ai ) . the value alignment problem expresses the challenge of ( machine ) learning values that are , in some way , aligned with human requirements or values . in this paper i argue for looking at how humans have formalized and communicated values , in professional codes of ethics , and for exploring declarative decision-theoretic ethical programs ( ddtep ) to formalize codes of ethics . this renders machine ethical reasoning and decision-making , as well as learning , more transparent and hopefully more accountable . the paper includes proof-of-concept examples of known toy dilemmas and gatekeeping domains such as archives and libraries .

word embedding based correlation model for question/answer matching
with the development of community based question answering ( q & a ) services , a large scale of q & a archives have been accumulated and are an important information and knowledge resource on the web . question and answer matching has been attached much importance to for its ability to reuse knowledge stored in these systems : it can be useful in enhancing user experience with recurrent questions . in this paper , we try to improve the matching accuracy by overcoming the lexical gap between question and answer pairs . a word embedding based correlation ( wec ) model is proposed by integrating advantages of both the translation model and word embedding , given a random pair of words , wec can score their co-occurrence probability in q & a pairs and it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text . an experimental study on yahoo ! answers dataset and baidu zhidao dataset shows this new method 's promising potential .

roborobo ! a fast robot simulator for swarm and collective robotics
roborobo ! is a multi-platform , highly portable , robot simulator for large-scale collective robotics experiments . roborobo ! is coded in c++ , and follows the kiss guideline ( `` keep it simple '' ) . therefore , its external dependency is solely limited to the widely available sdl library for fast 2d graphics . roborobo ! is based on a khepera/epuck model . it is targeted for fast single and multi-robots simulation , and has already been used in more than a dozen published research mainly concerned with evolutionary swarm robotics , including environment-driven self-adaptation and distributed evolutionary optimization , as well as online onboard embodied evolution and embodied morphogenesis .

knowledge will propel machine understanding of content : extrapolating from current examples
machine learning has been a big success story during the ai resurgence . one particular stand out success relates to learning from a massive amount of data . in spite of early assertions of the unreasonable effectiveness of data , there is increasing recognition for utilizing knowledge whenever it is available or can be created purposefully . in this paper , we discuss the indispensable role of knowledge for deeper understanding of content where ( i ) large amounts of training data are unavailable , ( ii ) the objects to be recognized are complex , ( e.g. , implicit entities and highly subjective content ) , and ( iii ) applications need to use complementary or related data in multiple modalities/media . what brings us to the cusp of rapid progress is our ability to ( a ) create relevant and reliable knowledge and ( b ) carefully exploit knowledge to enhance ml/nlp techniques . using diverse examples , we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data and continued incorporation of knowledge in learning techniques .

verification of semantically-enhanced artifact systems ( extended version )
artifact-centric systems have emerged in the last years as a suitable framework to model business-relevant entities , by combining their static and dynamic aspects . in particular , the guard-stage-milestone ( gsm ) approach has been recently proposed to model artifacts and their lifecycle in a declarative way . in this paper , we enhance gsm with a semantic layer , constituted by a full-fledged owl 2 ql ontology linked to the artifact information models through mapping specifications . the ontology provides a conceptual view of the domain under study , and allows one to understand the evolution of the artifact system at a higher level of abstraction . in this setting , we present a technique to specify temporal properties expressed over the semantic layer , and verify them according to the evolution in the underlying gsm model . this technique has been implemented in a tool that exploits state-of-the-art ontology-based data access technologies to manipulate the temporal properties according to the ontology and the mappings , and that relies on the gsmc model checker for verification .

ordonnancement d'entités pour la rencontre du web des documents et du web des données
the advances of the linked open data ( lod ) initiative are giving rise to a more structured web of data . indeed , a few datasets act as hubs ( e.g. , dbpedia ) connecting many other datasets . they also made possible new web services for entity detection inside plain text ( e.g. , dbpedia spotlight ) , thus allowing for new applications that will benefit from a combination of the web of documents and the web of data . to ease the emergence of these new use-cases , we propose a query-biased algorithm for the ranking of entities detected inside a web page . our algorithm combine link analysis with dimensionality reduction . we use crowdsourcing for building a publicly available and reusable dataset on which we compare our algorithm to the state of the art . finally , we use this algorithm for the construction of semantic snippets for which we evaluate the usability and the usefulness with a crowdsourcing-based approach .

abductive logic programs with penalization : semantics , complexity and implementation
abduction , first proposed in the setting of classical logics , has been studied with growing interest in the logic programming area during the last years . in this paper we study abduction with penalization in the logic programming framework . this form of abductive reasoning , which has not been previously analyzed in logic programming , turns out to represent several relevant problems , including optimization problems , very naturally . we define a formal model for abduction with penalization over logic programs , which extends the abductive framework proposed by kakas and mancarella . we address knowledge representation issues , encoding a number of problems in our abductive framework . in particular , we consider some relevant problems , taken from different domains , ranging from optimization theory to diagnosis and planning ; their encodings turn out to be simple and elegant in our formalism . we thoroughly analyze the computational complexity of the main problems arising in the context of abduction with penalization from logic programs . finally , we implement a system supporting the proposed abductive framework on top of the dlv engine . to this end , we design a translation from abduction problems with penalties into logic programs with weak constraints . we prove that this approach is sound and complete .

constrained sampling and counting : universal hashing meets sat solving
constrained sampling and counting are two fundamental problems in artificial intelligence with a diverse range of applications , spanning probabilistic reasoning and planning to constrained-random verification . while the theory of these problems was thoroughly investigated in the 1980s , prior work either did not scale to industrial size instances or gave up correctness guarantees to achieve scalability . recently , we proposed a novel approach that combines universal hashing and sat solving and scales to formulas with hundreds of thousands of variables without giving up correctness guarantees . this paper provides an overview of the key ingredients of the approach and discusses challenges that need to be overcome to handle larger real-world instances .

dendritic cells for real-time anomaly detection
dendritic cells ( dcs ) are innate immune system cells which have the power to activate or suppress the immune system . the behaviour of human of human dcs is abstracted to form an algorithm suitable for anomaly detection . we test this algorithm on the real-time problem of port scan detection . our results show a significant difference in artificial dc behaviour for an outgoing portscan when compared to behaviour for normal processes .

transparallel mind : classical computing with quantum power
inspired by the extraordinary computing power promised by quantum computers , the quantum mind hypothesis postulated that quantum mechanical phenomena are the source of neuronal synchronization , which , in turn , might underlie consciousness . here , i present an alternative inspired by a classical computing method with quantum power . this method relies on special distributed representations called hyperstrings . hyperstrings are superpositions of up to an exponential number of strings , which -- by a single-processor classical computer -- can be evaluated in a transparallel fashion , that is , simultaneously as if only one string were concerned . building on a neurally plausible model of human visual perceptual organization , in which hyperstrings are formal counterparts of transient neural assemblies , i postulate that synchronization in such assemblies is a manifestation of transparallel information processing . this accounts for the high combinatorial capacity and speed of human visual perceptual organization and strengthens ideas that self-organizing cognitive architecture bridges the gap between neurons and consciousness .

predicting network attacks using ontology-driven inference
graph knowledge models and ontologies are very powerful modeling and re asoning tools . we propose an effective approach to model network attacks and attack prediction which plays important roles in security management . the goals of this study are : first we model network attacks , their prerequisites and consequences using knowledge representation methods in order to provide description logic reasoning and inference over attack domain concepts . and secondly , we propose an ontology-based system which predicts potential attacks using inference and observing information which provided by sensory inputs . we generate our ontology and evaluate corresponding methods using capec , cwe , and cve hierarchical datasets . results from experiments show significant capability improvements comparing to traditional hierarchical and relational models . proposed method also reduces false alarms and improves intrusion detection effectiveness .

ranking basic belief assignments in decision making under uncertain environment
dempster-shafer theory ( d-s theory ) is widely used in decision making under the uncertain environment . ranking basic belief assignments ( bbas ) now is an open issue . existing evidence distance measures can not rank the bbas in the situations when the propositions have their own ranking order or their inherent measure of closeness . to address this issue , a new ranking evidence distance ( red ) measure is proposed . compared with the existing evidence distance measures including the jousselme 's distance and the distance between betting commitments , the proposed red measure is much more general due to the fact that the order of the propositions in the systems is taken into consideration . if there is no order or no inherent measure of closeness in the propositions , our proposed red measure is reduced to the existing evidence distance . numerical examples show that the proposed red measure is an efficient alternative to rank bbas in decision making under uncertain environment .

optimization of ensemble supervised learning algorithms for increased sensitivity , specificity , and auc of population-based colorectal cancer screenings
over 150,000 new people in the united states are diagnosed with colorectal cancer each year . nearly a third die from it ( american cancer society ) . the only approved noninvasive diagnosis tools currently involve fecal blood count tests ( fobts ) or stool dna tests . fecal blood count tests take only five minutes and are available over the counter for as low as \ $ 15 . they are highly specific , yet not nearly as sensitive , yielding a high percentage ( 25 % ) of false negatives ( colon cancer alliance ) . moreover , fobt results are far too generalized , meaning that a positive result could mean much more than just colorectal cancer , and could just as easily mean hemorrhoids , anal fissure , proctitis , crohn 's disease , diverticulosis , ulcerative colitis , rectal ulcer , rectal prolapse , ischemic colitis , angiodysplasia , rectal trauma , proctitis from radiation therapy , and others . stool dna tests , the modern benchmark for crc screening , have a much higher sensitivity and specificity , but also cost \ $ 600 , take two weeks to process , and are not for high-risk individuals or people with a history of polyps . to yield a cheap and effective crc screening alternative , a unique ensemble-based classification algorithm is put in place that considers the fit result , bmi , smoking history , and diabetic status of patients . this method is tested under ten-fold cross validation to have a .95 auc , 92 % specificity , 89 % sensitivity , .88 f1 , and 90 % precision . once clinically validated , this test promises to be cheaper , faster , and potentially more accurate when compared to a stool dna test .

formalization of psychological knowledge in answer set programming and its application
in this paper we explore the use of answer set programming ( asp ) to formalize , and reason about , psychological knowledge . in the field of psychology , a considerable amount of knowledge is still expressed using only natural language . this lack of a formalization complicates accurate studies , comparisons , and verification of theories . we believe that asp , a knowledge representation formalism allowing for concise and simple representation of defaults , uncertainty , and evolving domains , can be used successfully for the formalization of psychological knowledge . to demonstrate the viability of asp for this task , in this paper we develop an asp-based formalization of the mechanics of short-term memory . we also show that our approach can have rather immediate practical uses by demonstrating an application of our formalization to the task of predicting a user 's interaction with a graphical interface .

rough sets and matroidal contraction
rough sets are efficient for data pre-processing in data mining . as a generalization of the linear independence in vector spaces , matroids provide well-established platforms for greedy algorithms . in this paper , we apply rough sets to matroids and study the contraction of the dual of the corresponding matroid . first , for an equivalence relation on a universe , a matroidal structure of the rough set is established through the lower approximation operator . second , the dual of the matroid and its properties such as independent sets , bases and rank function are investigated . finally , the relationships between the contraction of the dual matroid to the complement of a single point set and the contraction of the dual matroid to the complement of the equivalence class of this point are studied .

the seqbin constraint revisited
we revisit the seqbin constraint . this meta-constraint subsumes a number of important global constraints like change , smooth and increasingnvalue . we show that the previously proposed filtering algorithm for seqbin has two drawbacks even under strong restrictions : it does not detect bounds disentailment and it is not idempotent . we identify the cause for these problems , and propose a new propagator that overcomes both issues . our algorithm is based on a connection to the problem of finding a path of a given cost in a restricted $ n $ -partite graph . our propagator enforces domain consistency in o ( nd^2 ) and , for special cases of seqbin that include change , smooth and increasingnvalue , in o ( nd ) time .

incremental network quantization : towards lossless cnns with low-precision weights
this paper presents incremental network quantization ( inq ) , a novel method , targeting to efficiently convert any pre-trained full-precision convolutional neural network ( cnn ) model into a low-precision version whose weights are constrained to be either powers of two or zero . unlike existing methods which are struggled in noticeable accuracy loss , our inq has the potential to resolve this issue , as benefiting from two innovations . on one hand , we introduce three interdependent operations , namely weight partition , group-wise quantization and re-training . a well-proven measure is employed to divide the weights in each layer of a pre-trained cnn model into two disjoint groups . the weights in the first group are responsible to form a low-precision base , thus they are quantized by a variable-length encoding method . the weights in the other group are responsible to compensate for the accuracy loss from the quantization , thus they are the ones to be re-trained . on the other hand , these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones , acting as an incremental network quantization and accuracy enhancement procedure . extensive experiments on the imagenet classification task using almost all known deep cnn architectures including alexnet , vgg-16 , googlenet and resnets well testify the efficacy of the proposed method . specifically , at 5-bit quantization , our models have improved accuracy than the 32-bit floating-point references . taking resnet-18 as an example , we further show that our quantized models with 4-bit , 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline . besides , impressive results with the combination of network pruning and inq are also reported . the code is available at https : //github.com/zhouaojun/incremental-network-quantization .

overlapping mixtures of gaussian processes for the data association problem
in this work we introduce a mixture of gps to address the data association problem , i.e . to label a group of observations according to the sources that generated them . unlike several previously proposed gp mixtures , the novel mixture has the distinct characteristic of using no gating function to determine the association of samples and mixture components . instead , all the gps in the mixture are global and samples are clustered following `` trajectories '' across input space . we use a non-standard variational bayesian algorithm to efficiently recover sample labels and learn the hyperparameters . we show how multi-object tracking problems can be disambiguated and also explore the characteristics of the model in traditional regression settings .

self-optimizing and pareto-optimal policies in general environments based on bayes-mixtures
the problem of making sequential decisions in unknown probabilistic environments is studied . in cycle $ t $ action $ y_t $ results in perception $ x_t $ and reward $ r_t $ , where all quantities in general may depend on the complete history . the perception $ x_t $ and reward $ r_t $ are sampled from the ( reactive ) environmental probability distribution $ \mu $ . this very general setting includes , but is not limited to , ( partial observable , k-th order ) markov decision processes . sequential decision theory tells us how to act in order to maximize the total expected reward , called value , if $ \mu $ is known . reinforcement learning is usually used if $ \mu $ is unknown . in the bayesian approach one defines a mixture distribution $ \xi $ as a weighted sum of distributions $ \nu\in\m $ , where $ \m $ is any class of distributions including the true environment $ \mu $ . we show that the bayes-optimal policy $ p^\xi $ based on the mixture $ \xi $ is self-optimizing in the sense that the average value converges asymptotically for all $ \mu\in\m $ to the optimal value achieved by the ( infeasible ) bayes-optimal policy $ p^\mu $ which knows $ \mu $ in advance . we show that the necessary condition that $ \m $ admits self-optimizing policies at all , is also sufficient . no other structural assumptions are made on $ \m $ . as an example application , we discuss ergodic markov decision processes , which allow for self-optimizing policies . furthermore , we show that $ p^\xi $ is pareto-optimal in the sense that there is no other policy yielding higher or equal value in { \em all } environments $ \nu\in\m $ and a strictly higher value in at least one .

unfair items detection in educational measurement
measurement professionals can not come to an agreement on the definition of the term 'item fairness ' . in this paper a continuous measure of item unfairness is proposed . the more the unfairness measure deviates from zero , the less fair the item is . if the measure exceeds the cutoff value , the item is identified as definitely unfair . the new approach can identify unfair items that would not be identified with conventional procedures . the results are in accord with experts ' judgments on the item qualities . since no assumptions about scores distributions and/or correlations are assumed , the method is applicable to any educational test . its performance is illustrated through application to scores of a real test .

mixed integer linear programming for exact finite-horizon planning in decentralized pomdps
we consider the problem of finding an n-agent joint-policy for the optimal finite-horizon control of a decentralized pomdp ( dec-pomdp ) . this is a problem of very high complexity ( nexp-hard in n > = 2 ) . in this paper , we propose a new mathematical programming approach for the problem . our approach is based on two ideas : first , we represent each agent 's policy in the sequence-form and not in the tree-form , thereby obtaining a very compact representation of the set of joint-policies . second , using this compact representation , we solve this problem as an instance of combinatorial optimization for which we formulate a mixed integer linear program ( milp ) . the optimal solution of the milp directly yields an optimal joint-policy for the dec-pomdp . computational experience shows that formulating and solving the milp requires significantly less time to solve benchmark dec-pomdp problems than existing algorithms . for example , the multi-agent tiger problem for horizon 4 is solved in 72 secs with the milp whereas existing algorithms require several hours to solve it .

protecting privacy through distributed computation in multi-agent decision making
as large-scale theft of data from corporate servers is becoming increasingly common , it becomes interesting to examine alternatives to the paradigm of centralizing sensitive data into large databases . instead , one could use cryptography and distributed computation so that sensitive data can be supplied and processed in encrypted form , and only the final result is made known . in this paper , we examine how such a paradigm can be used to implement constraint satisfaction , a technique that can solve a broad class of ai problems such as resource allocation , planning , scheduling , and diagnosis . most previous work on privacy in constraint satisfaction only attempted to protect specific types of information , in particular the feasibility of particular combinations of decisions . we formalize and extend these restricted notions of privacy by introducing four types of private information , including the feasibility of decisions and the final decisions made , but also the identities of the participants and the topology of the problem . we present distributed algorithms that allow computing solutions to constraint satisfaction problems while maintaining these four types of privacy . we formally prove the privacy properties of these algorithms , and show experiments that compare their respective performance on benchmark problems .

bilateral multi-perspective matching for natural language sentences
natural language sentence matching is a fundamental technology for a variety of tasks . previous approaches either match sentences from a single direction or only apply single granular ( word-by-word or sentence-by-sentence ) matching . in this work , we propose a bilateral multi-perspective matching ( bimpm ) model under the `` matching-aggregation '' framework . given two sentences $ p $ and $ q $ , our model first encodes them with a bilstm encoder . next , we match the two encoded sentences in two directions $ p \rightarrow q $ and $ p \leftarrow q $ . in each matching direction , each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives . then , another bilstm layer is utilized to aggregate the matching results into a fix-length matching vector . finally , based on the matching vector , the decision is made through a fully connected layer . we evaluate our model on three tasks : paraphrase identification , natural language inference and answer sentence selection . experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks .

do you see what i mean ? visual resolution of linguistic ambiguities
understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception . in this work , we present a novel task for grounded language understanding : disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence . to this end , we introduce a new multimodal corpus containing ambiguous sentences , representing a wide range of syntactic , semantic and discourse ambiguities , coupled with videos that visualize the different interpretations for each sentence . we address this task by extending a vision model which determines if a sentence is depicted by a video . we demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence , allowing to disambiguate sentences in a unified fashion across the different ambiguity types .

item recommendation with continuous experience evolution of users using brownian motion
online review communities are dynamic as users join and leave , adopt new vocabulary , and adapt to evolving trends . recent work has shown that recommender systems benefit from explicit consideration of user experience . however , prior work assumes a fixed number of discrete experience levels , whereas in reality users gain experience and mature continuously over time . this paper presents a new model that captures the continuous evolution of user experience , and the resulting language model in reviews and other posts . our model is unsupervised and combines principles of geometric brownian motion , brownian motion , and latent dirichlet allocation to trace a smooth temporal progression of user experience and language model respectively . we develop practical algorithms for estimating the model parameters from data and for inference with our model ( e.g. , to recommend items ) . extensive experiments with five real-world datasets show that our model not only fits data better than discrete-model baselines , but also outperforms state-of-the-art methods for predicting item ratings .

modal logics for topological spaces
in this thesis we shall present two logical systems , mp and mp , for the purpose of reasoning about knowledge and effort . these logical systems will be interpreted in a spatial context and therefore , the abstract concepts of knowledge and effort will be defined by concrete mathematical concepts .

exploring models and data for image question answering
this work aims to address the problem of image-based question-answering ( qa ) with new models and datasets . in our work , we propose to use neural networks and visual semantic embeddings , without intermediate stages such as object detection and image segmentation , to predict answers to simple questions about images . our model performs 1.8 times better than the only published results on an existing image qa dataset . we also present a question generation algorithm that converts image descriptions , which are widely available , into qa form . we used this algorithm to produce an order-of-magnitude larger dataset , with more evenly distributed answers . a suite of baseline results on this new dataset are also presented .

vision-based navigation i : a navigation filter for fusing dtm/correspondence updates
an algorithm for pose and motion estimation using corresponding features in images and a digital terrain map is proposed . using a digital terrain ( or digital elevation ) map ( dtm/dem ) as a global reference enables recovering the absolute position and orientation of the camera . in order to do this , the dtm is used to formulate a constraint between corresponding features in two consecutive frames . the utilization of data is shown to improve the robustness and accuracy of the inertial navigation algorithm . extended kalman filter was used to combine results of inertial navigation algorithm and proposed vision-based navigation algorithm . the feasibility of this algorithms is established through numerical simulations .

quadratically constrained quadratic programming for classification using particle swarms and applications
particle swarm optimization is used in several combinatorial optimization problems . in this work , particle swarms are used to solve quadratic programming problems with quadratic constraints . the approach of particle swarms is an example for interior point methods in optimization as an iterative technique . this approach is novel and deals with classification problems without the use of a traditional classifier . our method determines the optimal hyperplane or classification boundary for a data set . in a binary classification problem , we constrain each class as a cluster , which is enclosed by an ellipsoid . the estimation of the optimal hyperplane between the two clusters is posed as a quadratically constrained quadratic problem . the optimization problem is solved in distributed format using modified particle swarms . our method has the advantage of using the direction towards optimal solution rather than searching the entire feasible region . our results on the iris , pima , wine , and thyroid datasets show that the proposed method works better than a neural network and the performance is close to that of svm .

hidden physics models : machine learning of nonlinear partial differential equations
while there is currently a lot of enthusiasm about `` big data '' , useful data is usually `` small '' and expensive to acquire . in this paper , we present a new paradigm of learning partial differential equations from { \em small } data . in particular , we introduce \emph { hidden physics models } , which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics , expressed by time dependent and nonlinear partial differential equations , to extract patterns from high-dimensional data generated from experiments . the proposed methodology may be applied to the problem of learning , system identification , or data-driven discovery of partial differential equations . our framework relies on gaussian processes , a powerful tool for probabilistic inference over functions , that enables us to strike a balance between model complexity and data fitting . the effectiveness of the proposed approach is demonstrated through a variety of canonical problems , spanning a number of scientific domains , including the navier-stokes , schr\ '' odinger , kuramoto-sivashinsky , and time dependent linear fractional equations . the methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data .

blistr : the blind strategymaker
blistr is a system that automatically develops strategies for e prover on a large set of problems . the main idea is to interleave ( i ) iterated low-timelimit local search for new strategies on small sets of similar easy problems with ( ii ) higher-timelimit evaluation of the new strategies on all problems . the accumulated results of the global higher-timelimit runs are used to define and evolve the notion of `` similar easy problems '' , and to control the selection of the next strategy to be improved . the technique was used to significantly strengthen the set of e strategies used by the malarea , ps-e , e-males , and e systems in the casc @ turing 2012 competition , particularly in the mizar division . similar improvement was obtained on the problems created from the flyspeck corpus .

embedding non-ground logic programs into autoepistemic logic for knowledge base combination
in the context of the semantic web , several approaches to the combination of ontologies , given in terms of theories of classical first-order logic and rule bases , have been proposed . they either cast rules into classical logic or limit the interaction between rules and ontologies . autoepistemic logic ( ael ) is an attractive formalism which allows to overcome these limitations , by serving as a uniform host language to embed ontologies and nonmonotonic logic programs into it . for the latter , so far only the propositional setting has been considered . in this paper , we present three embeddings of normal and three embeddings of disjunctive non-ground logic programs under the stable model semantics into first-order ael . while the embeddings all correspond with respect to objective ground atoms , differences arise when considering non-atomic formulas and combinations with first-order theories . we compare the embeddings with respect to stable expansions and autoepistemic consequences , considering the embeddings by themselves , as well as combinations with classical theories . our results reveal differences and correspondences of the embeddings and provide useful guidance in the choice of a particular embedding for knowledge combination .

bound propagation
in this article we present an algorithm to compute bounds on the marginals of a graphical model . for several small clusters of nodes upper and lower bounds on the marginal values are computed independently of the rest of the network . the range of allowed probability distributions over the surrounding nodes is restricted using earlier computed bounds . as we will show , this can be considered as a set of constraints in a linear programming problem of which the objective function is the marginal probability of the center nodes . in this way knowledge about the maginals of neighbouring clusters is passed to other clusters thereby tightening the bounds on their marginals . we show that sharp bounds can be obtained for undirected and directed graphs that are used for practical applications , but for which exact computations are infeasible .

dynamic adjustment of the motivation degree in an action selection mechanism
this paper presents a model for dynamic adjustment of the motivation degree , using a reinforcement learning approach , in an action selection mechanism previously developed by the authors . the learning takes place in the modification of a parameter of the model of combination of internal and external stimuli . experiments that show the claimed properties are presented , using a vr simulation developed for such purposes . the importance of adaptation by learning in action selection is also discussed .

hierarchically-attentive rnn for album summarization and storytelling
we address the problem of end-to-end visual storytelling . given a photo album , our model first selects the most representative ( summary ) photos , and then composes a natural language story for the album . for this task , we make use of the visual storytelling dataset and a model composed of three hierarchically-attentive recurrent neural nets ( rnns ) to : encode the album photos , select representative ( summary ) photos , and compose the story . automatic and human evaluations show our model achieves better performance on selection , generation , and retrieval than baselines .

episodic exploration for deep deterministic policies : an application to starcraft micromanagement tasks
we consider scenarios from the real-time strategy game starcraft as new benchmarks for reinforcement learning algorithms . we propose micromanagement tasks , which present the problem of the short-term , low-level control of army members during a battle . from a reinforcement learning point of view , these scenarios are challenging because the state-action space is very large , and because there is no obvious feature representation for the state-action evaluation function . we describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine . in addition , we present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation . this algorithm allows for the collection of traces for learning using deterministic policies , which appears much more efficient than , for example , { \epsilon } -greedy exploration . experiments show that with this algorithm , we successfully learn non-trivial strategies for scenarios with armies of up to 15 agents , where both q-learning and reinforce struggle .

human uncertainty and ranking error -- the secret of successful evaluation in predictive data mining
one of the most crucial issues in data mining is to model human behaviour in order to provide personalisation , adaptation and recommendation . this usually involves implicit or explicit knowledge , either by observing user interactions , or by asking users directly . but these sources of information are always subject to the volatility of human decisions , making utilised data uncertain to a particular extent . in this contribution , we elaborate on the impact of this human uncertainty when it comes to comparative assessments of different data mining approaches . in particular , we reveal two problems : ( 1 ) biasing effects on various metrics of model-based prediction and ( 2 ) the propagation of uncertainty and its thus induced error probabilities for algorithm rankings . for this purpose , we introduce a probabilistic view and prove the existence of those problems mathematically , as well as provide possible solution strategies . we exemplify our theory mainly in the context of recommender systems along with the metric rmse as a prominent example of precision quality measures .

towards an indexical model of situated language comprehension for cognitive agents in physical worlds
we propose a computational model of situated language comprehension based on the indexical hypothesis that generates meaning representations by translating amodal linguistic symbols to modal representations of beliefs , knowledge , and experience external to the linguistic system . this indexical model incorporates multiple information sources , including perceptions , domain knowledge , and short-term and long-term experiences during comprehension . we show that exploiting diverse information sources can alleviate ambiguities that arise from contextual use of underspecific referring expressions and unexpressed argument alternations of verbs . the model is being used to support linguistic interactions in rosie , an agent implemented in soar that learns from instruction .

approximately optimal monitoring of plan preconditions
monitoring plan preconditions can allow for replanning when a precondition fails , generally far in advance of the point in the plan where the precondition is relevant . however , monitoring is generally costly , and some precondition failures have a very small impact on plan quality . we formulate a model for optimal precondition monitoring , using partially-observable markov decisions processes , and describe methods for solving this model efficitively , though approximately . specifically , we show that the single-precondition monitoring problem is generally tractable , and the multiple-precondition monitoring policies can be efficitively approximated using single-precondition soultions .

structure and problem hardness : goal asymmetry and dpll proofs in < br > sat-based planning
in verification and in ( optimal ) ai planning , a successful method is to formulate the application as boolean satisfiability ( sat ) , and solve it with state-of-the-art dpll-based procedures . there is a lack of understanding of why this works so well . focussing on the planning context , we identify a form of problem structure concerned with the symmetrical or asymmetrical nature of the cost of achieving the individual planning goals . we quantify this sort of structure with a simple numeric parameter called asymratio , ranging between 0 and 1. we run experiments in 10 benchmark domains from the international planning competitions since 2000 ; we show that asymratio is a good indicator of sat solver performance in 8 of these domains . we then examine carefully crafted synthetic planning domains that allow control of the amount of structure , and that are clean enough for a rigorous analysis of the combinatorial search space . the domains are parameterized by size , and by the amount of structure . the cnfs we examine are unsatisfiable , encoding one planning step less than the length of the optimal plan . we prove upper and lower bounds on the size of the best possible dpll refutations , under different settings of the amount of structure , as a function of size . we also identify the best possible sets of branching variables ( backdoors ) . with minimum asymratio , we prove exponential lower bounds , and identify minimal backdoors of size linear in the number of variables . with maximum asymratio , we identify logarithmic dpll refutations ( and backdoors ) , showing a doubly exponential gap between the two structural extreme cases . the reasons for this behavior -- the proof arguments -- illuminate the prototypical patterns of structure causing the empirical behavior observed in the competition benchmarks .

ai safety gridworlds
we present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents . these problems include safe interruptibility , avoiding side effects , absent supervisor , reward gaming , safe exploration , as well as robustness to self-modification , distributional shift , and adversaries . to measure compliance with the intended safe behavior , we equip each environment with a performance function that is hidden from the agent . this allows us to categorize ai safety problems into robustness and specification problems , depending on whether the performance function corresponds to the observed reward function . we evaluate a2c and rainbow , two recent deep reinforcement learning agents , on our environments and show that they are not able to solve them satisfactorily .

reasoning about cardinal directions between extended objects
direction relations between extended spatial objects are important commonsense knowledge . recently , goyal and egenhofer proposed a formal model , known as cardinal direction calculus ( cdc ) , for representing direction relations between connected plane regions . cdc is perhaps the most expressive qualitative calculus for directional information , and has attracted increasing interest from areas such as artificial intelligence , geographical information science , and image retrieval . given a network of cdc constraints , the consistency problem is deciding if the network is realizable by connected regions in the real plane . this paper provides a cubic algorithm for checking consistency of basic cdc constraint networks , and proves that reasoning with cdc is in general an np-complete problem . for a consistent network of basic cdc constraints , our algorithm also returns a 'canonical ' solution in cubic time . this cubic algorithm is also adapted to cope with cardinal directions between possibly disconnected regions , in which case currently the best algorithm is of time complexity o ( n^5 ) .

from texts to structured documents : the case of health practice guidelines
this paper describes a system capable of semi-automatically filling an xml template from free texts in the clinical domain ( practice guidelines ) . the xml template includes semantic information not explicitly encoded in the text ( pairs of conditions and actions/recommendations ) . therefore , there is a need to compute the exact scope of conditions over text sequences expressing the required actions . we present in this paper the rules developed for this task . we show that the system yields good performance when applied to the analysis of french practice guidelines .

new liftable classes for first-order probabilistic inference
statistical relational models provide compact encodings of probabilistic dependencies in relational domains , but result in highly intractable graphical models . the goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately , by instead treating exchangeable , undistinguished objects as a whole . in this paper , we study the domain recursion inference rule , which , despite its central role in early theoretical results on domain-lifted inference , has later been believed redundant . we show that this rule is more powerful than expected , and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain . this includes an open problem called s4 , the symmetric transitivity model , and a first-order logic encoding of the birthday paradox . we further identify new classes s2fo2 and s2ru of domain-liftable theories , which respectively subsume fo2 and recursively unary theories , the largest classes of domain-liftable theories known so far , and show that using domain recursion can achieve exponential speedup even in theories that can not fully be lifted with the existing set of inference rules .

concept of e-machine : how does a `` dynamical '' brain learn to process `` symbolic '' information ? part i
the human brain has many remarkable information processing characteristics that deeply puzzle scientists and engineers . among the most important and the most intriguing of these characteristics are the brain 's broad universality as a learning system and its mysterious ability to dynamically change ( reconfigure ) its behavior depending on a combinatorial number of different contexts . this paper discusses a class of hypothetically brain-like dynamically reconfigurable associative learning systems that shed light on the possible nature of these brain 's properties . the systems are arranged on the general principle referred to as the concept of e-machine . the paper addresses the following questions : 1. how can `` dynamical '' neural networks function as universal programmable `` symbolic '' machines ? 2. what kind of a universal programmable symbolic machine can form arbitrarily complex software in the process of programming similar to the process of biological associative learning ? 3. how can a universal learning machine dynamically reconfigure its software depending on a combinatorial number of possible contexts ?

the eigenoption-critic framework
eigenoptions ( eos ) have been recently introduced as a promising idea for generating a diverse set of options through the graph laplacian , having been shown to allow efficient exploration . despite its initial promising results , a couple of issues in current algorithms limit its application , namely : ( 1 ) eo methods require two separate steps ( eigenoption discovery and reward maximization ) to learn a control policy , which can incur a significant amount of storage and computation ; ( 2 ) eos are only defined for problems with discrete state-spaces and ; ( 3 ) it is not easy to take the environment 's reward function into consideration when discovering eos . to addresses these issues , we introduce an algorithm termed eigenoption-critic ( eoc ) based on the option-critic ( oc ) framework [ bacon17 ] , a general hierarchical reinforcement learning ( rl ) algorithm that allows learning the intra-option policies simultaneously with the policy over options . we also propose a generalization of eoc to problems with continuous state-spaces through the nystr\ '' om approximation . eoc can also be seen as extending oc to nonstationary settings , where the discovered options are not tailored for a single task .

learning scalable deep kernels with recurrent structure
many applications in speech , robotics , finance , and biology deal with sequential data , where ordering matters and recurrent structures are common . however , this structure can not be easily captured by standard kernel functions . to model such structure , we propose expressive closed-form kernel functions for gaussian processes . the resulting model , gp-lstm , fully encapsulates the inductive biases of long short-term memory ( lstm ) recurrent networks , while retaining the non-parametric probabilistic advantages of gaussian processes . we learn the properties of the proposed kernels by optimizing the gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction . this approach provides a practical representation for bayesian lstms . we demonstrate state-of-the-art performance on several benchmarks , and thoroughly investigate a consequential autonomous driving application , where the predictive uncertainties provided by gp-lstm are uniquely valuable .

display of information for time-critical decision making
we describe methods for managing the complexity of information displayed to people responsible for making high-stakes , time-critical decisions . the techniques provide tools for real-time control of the configuration and quantity of information displayed to a user , and a methodology for designing flexible human-computer interfaces for monitoring applications . after defining a prototypical set of display decision problems , we introduce the expected value of revealed information ( evri ) and the related measure of expected value of displayed information ( evdi ) . we describe how these measures can be used to enhance computer displays used for monitoring complex systems . we motivate the presentation by discussing our efforts to employ decision-theoretic control of displays for a time-critical monitoring application at the nasa mission control center in houston .

understanding sampling style adversarial search methods
uct has recently emerged as an exciting new adversarial reasoning technique based on cleverly balancing exploration and exploitation in a monte-carlo sampling setting . it has been particularly successful in the game of go but the reasons for its success are not well understood and attempts to replicate its success in other domains such as chess have failed . we provide an in-depth analysis of the potential of uct in domain-independent settings , in cases where heuristic values are available , and the effect of enhancing random playouts to more informed playouts between two weak minimax players . to provide further insights , we develop synthetic game tree instances and discuss interesting properties of uct , both empirically and analytically .

fuzzy controller design for assisted omni-directional treadmill therapy
one of the defining characteristic of human being is their ability to walk upright . loss or restriction of such ability whether due to the accident , spine problem , stroke or other neurological injuries can cause tremendous stress on the patients and hence will contribute negatively to their quality of life . modern research shows that physical exercise is very important for maintaining physical fitness and adopting a healthier life style . in modern days treadmill is widely used for physical exercises and training which enables the user to set up an exercise regime that can be adhered to irrespective of the weather conditions . among the users of treadmills today are medical facilities such as hospitals , rehabilitation centres , medical and physiotherapy clinics etc . the process of assisted training or doing rehabilitation exercise through treadmill is referred to as treadmill therapy . a modern treadmill is an automated machine having built in functions and predefined features . most of the treadmills used today are one dimensional and user can only walk in one direction . this paper presents the idea of using omnidirectional treadmills which will be more appealing to the patients as they can walk in any direction , hence encouraging them to do exercises more frequently . this paper proposes a fuzzy control design and possible implementation strategy to assist patients in treadmill therapy . by intelligently controlling the safety belt attached to the treadmill user , one can help them steering left , right or in any direction . the use of intelligent treadmill therapy can help patients to improve their walking ability without being continuously supervised by the specialists . the patients can walk freely within a limited space and the support system will provide continuous evaluation of their position and can adjust the control parameters of treadmill accordingly to provide best possible assistance .

using a diathesis model for semantic parsing
this paper presents a semantic parsing approach for unrestricted texts . semantic parsing is one of the major bottlenecks of natural language understanding ( nlu ) systems and usually requires building expensive resources not easily portable to other domains . our approach obtains a case-role analysis , in which the semantic roles of the verb are identified . in order to cover all the possible syntactic realisations of a verb , our system combines their argument structure with a set of general semantic labelled diatheses models . combining them , the system builds a set of syntactic-semantic patterns with their own role-case representation . once the patterns are build , we use an approximate tree pattern-matching algorithm to identify the most reliable pattern for a sentence . the pattern matching is performed between the syntactic-semantic patterns and the feature-structure tree representing the morphological , syntactical and semantic information of the analysed sentence . for sentences assigned to the correct model , the semantic parsing system we are presenting identifies correctly more than 73 % of possible semantic case-roles .

learning strips operators from noisy and incomplete observations
agents learning to act autonomously in real-world domains must acquire a model of the dynamics of the domain in which they operate . learning domain dynamics can be challenging , especially where an agent only has partial access to the world state , and/or noisy external sensors . even in standard strips domains , existing approaches can not learn from noisy , incomplete observations typical of real-world domains . we propose a method which learns strips action models in such domains , by decomposing the problem into first learning a transition function between states in the form of a set of classifiers , and then deriving explicit strips rules from the classifiers ' parameters . we evaluate our approach on simulated standard planning domains from the international planning competition , and show that it learns useful domain descriptions from noisy , incomplete observations .

a unified approach for learning the parameters of sum-product networks
we present a unified approach for learning the parameters of sum-product networks ( spns ) . we prove that any complete and decomposable spn is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions . based on the mixture model perspective , we characterize the objective function when learning spns based on the maximum likelihood estimation ( mle ) principle and show that the optimization problem can be formulated as a signomial program . we construct two parameter learning algorithms for spns by using sequential monomial approximations ( sma ) and the concave-convex procedure ( cccp ) , respectively . the two proposed methods naturally admit multiplicative updates , hence effectively avoiding the projection operation . with the help of the unified framework , we also show that , in the case of spns , cccp leads to the same algorithm as expectation maximization ( em ) despite the fact that they are different in general .

the complexification of engineering
this paper deals with the arrow of complexification of engineering . we claim that the complexification of engineering consists in ( a ) that shift throughout which engineering becomes a science ; thus it ceases to be a ( mere ) praxis or profession ; ( b ) becoming a science , engineering can be considered as one of the sciences of complexity . in reality , the complexification of engineering is the process by which engineering can be studied , achieved and understood in terms of knowledge , and not of goods and services any longer . complex engineered systems and bio-inspired engineering are so far the two expressions of a complex engineering .

proceedings of the fifth conference on uncertainty in artificial intelligence ( 1989 )
this is the proceedings of the fifth conference on uncertainty in artificial intelligence , which was held in windsor , on , august 18-20 , 1989

automated planning in repeated adversarial games
game theory 's prescriptive power typically relies on full rationality and/or self-play interactions . in contrast , this work sets aside these fundamental premises and focuses instead on heterogeneous autonomous interactions between two or more agents . specifically , we introduce a new and concise representation for repeated adversarial ( constant-sum ) games that highlight the necessary features that enable an automated planing agent to reason about how to score above the game 's nash equilibrium , when facing heterogeneous adversaries . to this end , we present teamup , a model-based rl algorithm designed for learning and planning such an abstraction . in essence , it is somewhat similar to r-max with a cleverly engineered reward shaping that treats exploration as an adversarial optimization problem . in practice , it attempts to find an ally with which to tacitly collude ( in more than two-player games ) and then collaborates on a joint plan of actions that can consistently score a high utility in adversarial repeated games . we use the inaugural lemonade stand game tournament to demonstrate the effectiveness of our approach , and find that teamup is the best performing agent , demoting the tournament 's actual winning strategy into second place . in our experimental analysis , we show hat our strategy successfully and consistently builds collaborations with many different heterogeneous ( and sometimes very sophisticated ) adversaries .

generalizing fuzzy logic probabilistic inferences
linear representations for a subclass of boolean symmetric functions selected by a parity condition are shown to constitute a generalization of the linear constraints on probabilities introduced by boole . these linear constraints are necessary to compute probabilities of events with relations between the . arbitrarily specified with propositional calculus boolean formulas .

axiomatic foundations for a class of generalized expected utility : algebraic expected utility
expected utility : algebraic expected utility in this paper , we provide two axiomatizations of algebraic expected utility , which is a particular generalized expected utility , in a von neumann-morgenstern setting , i.e . uncertainty representation is supposed to be given and here to be described by a plausibility measure valued on a semiring , which could be partially ordered . we show that axioms identical to those for expected utility entail that preferences are represented by an algebraic expected utility . this algebraic approach allows many previous propositions ( expected utility , binary possibilistic utility , ... ) to be unified in a same general framework and proves that the obtained utility enjoys the same nice features as expected utility : linearity , dynamic consistency , autoduality of the underlying uncertainty measure , autoduality of the decision criterion and possibility of modeling decision maker 's attitude toward uncertainty .

properties of answer set programming with convex generalized atoms
in recent years , answer set programming ( asp ) , logic programming under the stable model or answer set semantics , has seen several extensions by generalizing the notion of an atom in these programs : be it aggregate atoms , hex atoms , generalized quantifiers , or abstract constraints , the idea is to have more complicated satisfaction patterns in the lattice of herbrand interpretations than traditional , simple atoms . in this paper we refer to any of these constructs as generalized atoms . several semantics with differing characteristics have been proposed for these extensions , rendering the big picture somewhat blurry . in this paper , we analyze the class of programs that have convex generalized atoms ( originally proposed by liu and truszczynski in [ 10 ] ) in rule bodies and show that for this class many of the proposed semantics coincide . this is an interesting result , since recently it has been shown that this class is the precise complexity boundary for the flp semantics . we investigate whether similar results also hold for other semantics , and discuss the implications of our findings .

wavelet residual network for low-dose ct via deep convolutional framelets
model based iterative reconstruction ( mbir ) algorithms for low-dose x-ray ct are computationally expensive . to address this problem , we recently proposed the world-first deep convolutional neural network ( cnn ) for low-dose x-ray ct and won the second place in 2016 aapm low-dose ct grand challenge . however , some of the texture were not fully recovered . to cope with this problem , here we propose a deep residual learning approach in directional wavelet domain . the proposed method is motivated by an observation that a deep convolutional neural network can be interpreted as a multilayer convolutional framelets expansion using non-local basis convolved with data-driven local basis . we further extend the idea to derive a deep convolutional framelet expansion by combining global redundant transforms and signal boosting from multiple signal representations . extensive experimental results confirm that the proposed network has significantly improved performance and preserves the detail texture of the original images

question answering via integer programming over semi-structured knowledge
answering science questions posed in natural language is an important ai challenge . answering such questions often requires non-trivial inference and knowledge that goes beyond factoid retrieval . yet , most systems for this task are based on relatively shallow information retrieval ( ir ) and statistical correlation techniques operating on large unstructured corpora . we propose a structured inference system for this task , formulated as an integer linear program ( ilp ) , that answers natural language questions using a semi-structured knowledge base derived from text , including questions requiring multi-step inference and a combination of multiple facts . on a dataset of real , unseen science questions , our system significantly outperforms ( +14 % ) the best previous attempt at structured reasoning for this task , which used markov logic networks ( mlns ) . it also improves upon a previous ilp formulation by 17.7 % . when combined with unstructured inference methods , the ilp system significantly boosts overall performance ( +10 % ) . finally , we show our approach is substantially more robust to a simple answer perturbation compared to statistical correlation methods .

computational analysis of perfect-information position auctions
after experimentation with other designs , the major search engines converged on the weighted , generalized second-price auction ( wgsp ) for selling keyword advertisements . notably , this convergence occurred before position auctions were well understood ( or , indeed , widely studied ) theoretically . while much progress has been made since , theoretical analysis is still not able to settle the question of why search engines found wgsp preferable to other position auctions . we approach this question in a new way , adopting a new analytical paradigm we dub `` computational mechanism analysis . '' by sampling position auction games from a given distribution , encoding them in a computationally efficient representation language , computing their nash equilibria , and then calculating economic quantities of interest , we can quantitatively answer questions that theoretical methods have not . we considered seven widely studied valuation models from the literature and three position auction variants ( generalized first price , unweighted generalized second price , and wgsp ) . we found that wgsp consistently showed the best ads of any position auction , measured both by social welfare and by relevance ( expected number of clicks ) . even in models where wgsp was already known to have bad worse-case efficiency , we found that it almost always performed well on average . in contrast , we found that revenue was extremely variable across auction mechanisms , and was highly sensitive to equilibrium selection , the preference model , and the valuation distribution .

blue sky ideas in artificial intelligence education from the eaai 2017 new and future ai educator program
the 7th symposium on educational advances in artificial intelligence ( eaai'17 , co-chaired by sven koenig and eric eaton ) launched the eaai new and future ai educator program to support the training of early-career university faculty , secondary school faculty , and future educators ( phd candidates or postdocs who intend a career in academia ) . as part of the program , awardees were asked to address one of the following `` blue sky '' questions : * how could/should artificial intelligence ( ai ) courses incorporate ethics into the curriculum ? * how could we teach ai topics at an early undergraduate or a secondary school level ? * ai has the potential for broad impact to numerous disciplines . how could we make ai education more interdisciplinary , specifically to benefit non-engineering fields ? this paper is a collection of their responses , intended to help motivate discussion around these issues in ai education .

penetration testing == pomdp solving ?
penetration testing is a methodology for assessing network security , by generating and executing possible attacks . doing so automatically allows for regular and systematic testing without a prohibitive amount of human labor . a key question then is how to generate the attacks . this is naturally formulated as a planning problem . previous work ( lucangeli et al . 2010 ) used classical planning and hence ignores all the incomplete knowledge that characterizes hacking . more recent work ( sarraute et al . 2011 ) makes strong independence assumptions for the sake of scaling , and lacks a clear formal concept of what the attack planning problem actually is . herein , we model that problem in terms of partially observable markov decision processes ( pomdp ) . this grounds penetration testing in a well-researched formalism , highlighting important aspects of this problem 's nature . pomdps allow to model information gathering as an integral part of the problem , thus providing for the first time a means to intelligently mix scanning actions with actual exploits .

a bayesian method for constructing bayesian belief networks from databases
this paper presents a bayesian method for constructing bayesian belief networks from a database of cases . potential applications include computer-assisted hypothesis testing , automated scientific discovery , and automated construction of probabilistic expert systems . results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases . we relate the methods in this paper to previous work , and we discuss open problems .

distinguishing cause from effect based on exogeneity
recent developments in structural equation modeling have produced several methods that can usually distinguish cause from effect in the two-variable case . for that purpose , however , one has to impose substantial structural constraints or smoothness assumptions on the functional causal models . in this paper , we consider the problem of determining the causal direction from a related but different point of view , and propose a new framework for causal direction determination . we show that it is possible to perform causal inference based on the condition that the cause is `` exogenous '' for the parameters involved in the generating process from the cause to the effect . in this way , we avoid the structural constraints required by the sem-based approaches . in particular , we exploit nonparametric methods to estimate marginal and conditional distributions , and propose a bootstrap-based approach to test for the exogeneity condition ; the testing results indicate the causal direction between two variables . the proposed method is validated on both synthetic and real data .

discovering more precise process models from event logs by filtering out chaotic activities
process discovery is concerned with the automatic generation of a process model that describes a business process from execution data of that business process . real life event logs can contain chaotic activities . these activities are independent of the state of the process and can , therefore , happen at rather arbitrary points in time . we show that the presence of such chaotic activities in an event log heavily impacts the quality of the process models that can be discovered with process discovery techniques . the current modus operandi for filtering activities from event logs is to simply filter out infrequent activities . we show that frequency-based filtering of activities does not solve the problems that are caused by chaotic activities . moreover , we propose a novel technique to filter out chaotic activities from event logs . we evaluate this technique on a collection of seventeen real-life event logs that originate from both the business process management domain and the smart home environment domain . as demonstrated , the developed activity filtering methods enable the discovery of process models that are more behaviorally specific compared to process models that are discovered using standard frequency-based filtering .

perception lie paradox : mathematically proved uncertainty about humans perception similarity
agents ' judgment depends on perception and previous knowledge . assuming that previous knowledge depends on perception , we can say that judgment depends on perception . so , if judgment depends on perception , can agents judge that they have the same perception ? in few words , this is the addressed paradox through this document . while illustrating on the paradox , it 's found that to reach agreement in communication , it 's not necessary for parties to have the same perception however the necessity is to have perception correspondence . the attempted solution to this paradox reveals a potential uncertainty in judging the matter thus supporting the skeptical view of the problem . moreover , relating perception to intelligence , the same uncertainty is inherited by judging the level of intelligence of an agent compared to others not necessarily from the same kind ( e.g . machine intelligence compared to human intelligence ) . using a proposed simple mathematical model for perception and action , a tool is developed to construct scenarios , and the problem is addressed mathematically such that conclusions are drawn systematically based on mathematically defined properties . when it comes to formalization , philosophical arguments and views become more visible and explicit .

fairness in supervised learning : an information theoretic approach
automated decision making systems are increasingly being used in real-world applications . in these systems for the most part , the decision rules are derived by minimizing the training error on the available historical data . therefore , if there is a bias related to a sensitive attribute such as gender , race , religion , etc . in the data , say , due to cultural/historical discriminatory practices against a certain demographic , the system could continue discrimination in decisions by including the said bias in its decision rule . we present an information theoretic framework for designing fair predictors from data , which aim to prevent discrimination against a specified sensitive attribute in a supervised learning setting . we use equalized odds as the criterion for discrimination , which demands that the prediction should be independent of the protected attribute conditioned on the actual label . to ensure fairness and generalization simultaneously , we compress the data to an auxiliary variable , which is used for the prediction task . this auxiliary variable is chosen such that it is decontaminated from the discriminatory attribute in the sense of equalized odds . the final predictor is obtained by applying a bayesian decision rule to the auxiliary variable .

stochastic local search for pattern set mining
local search methods can quickly find good quality solutions in cases where systematic search methods might take a large amount of time . moreover , in the context of pattern set mining , exhaustive search methods are not applicable due to the large search space they have to explore . in this paper , we propose the application of stochastic local search to solve the pattern set mining . specifically , to the task of concept learning . we applied a number of local search algorithms on a standard benchmark instances for pattern set mining and the results show the potentials for further exploration .

controlling search in very large commonsense knowledge bases : a machine learning approach
very large commonsense knowledge bases ( kbs ) often have thousands to millions of axioms , of which relatively few are relevant for answering any given query . a large number of irrelevant axioms can easily overwhelm resolution-based theorem provers . therefore , methods that help the reasoner identify useful inference paths form an essential part of large-scale reasoning systems . in this paper , we describe two ordering heuristics for optimization of reasoning in such systems . first , we discuss how decision trees can be used to select inference steps that are more likely to succeed . second , we identify a small set of problem instance features that suffice to guide searches away from intractable regions of the search space . we show the efficacy of these techniques via experiments on thousands of queries from the cyc kb . results show that these methods lead to an order of magnitude reduction in inference time .

object-oriented knowledge representation and data storage using inhomogeneous classes
this paper contains analysis of concept of a class within different object-oriented knowledge representation models . the main attention is paid to structure of the class and its efficiency in the context of data storage , using object-relational mapping . the main achievement of the paper is extension of concept of homogeneous class of objects by introducing concepts of single-core and multi-core inhomogeneous classes of objects , which allow simultaneous defining of a few different types within one class of objects , avoiding duplication of properties and methods in representation of types , decreasing sizes of program codes and providing more efficient information storage in the databases . in addition , the paper contains results of experiment , which show that data storage in relational database , using proposed extensions of the class , in some cases is more efficient in contrast to usage of homogeneous classes of objects .

deep survival analysis
the electronic health record ( ehr ) provides an unprecedented opportunity to build actionable tools to support physicians at the point of care . in this paper , we investigate survival analysis in the context of ehr data . we introduce deep survival analysis , a hierarchical generative approach to survival analysis . it departs from previous approaches in two primary ways : ( 1 ) all observations , including covariates , are modeled jointly conditioned on a rich latent structure ; and ( 2 ) the observations are aligned by their failure time , rather than by an arbitrary time zero as in traditional survival analysis . further , it ( 3 ) scalably handles heterogeneous ( continuous and discrete ) data types that occur in the ehr . we validate deep survival analysis model by stratifying patients according to risk of developing coronary heart disease ( chd ) . specifically , we study a dataset of 313,000 patients corresponding to 5.5 million months of observations . when compared to the clinically validated framingham chd risk score , deep survival analysis is significantly superior in stratifying patients according to their risk .

m-best solutions for a class of fuzzy constraint satisfaction problems
the article considers one of the possible generalizations of constraint satisfaction problems where relations are replaced by multivalued membership functions . in this case operations of disjunction and conjunction are replaced by maximum and minimum , and consistency of a solution becomes multivalued rather than binary . the article studies the problem of finding d most admissible solutions for a given d. a tractable subclass of these problems is defined by the concepts of invariants and polymorphisms similar to the classic constraint satisfaction approach . these concepts are adapted in two ways . firstly , the correspondence of `` invariant-polymorphism '' is generalized to ( min , max ) semirings . secondly , we consider non-uniform polymorphisms , where each variable has its own operator , in contrast to the case of one operator common for all variables . the article describes an algorithm that finds $ d $ most admissible solutions in polynomial time , provided that the problem is invariant with respect to some non-uniform majority operator . it is essential that this operator needs not to be known for the algorithm to work . moreover , even a guarantee for the existence of such an operator is not necessary . the algorithm either finds the solution or discards the problem . the latter is possible only if the problem has no majority polymorphism .

an approach to temporal planning and scheduling in domains with predictable exogenous events
the treatment of exogenous events in planning is practically important in many real-world domains where the preconditions of certain plan actions are affected by such events . in this paper we focus on planning in temporal domains with exogenous events that happen at known times , imposing the constraint that certain actions in the plan must be executed during some predefined time windows . when actions have durations , handling such temporal constraints adds an extra difficulty to planning . we propose an approach to planning in these domains which integrates constraint-based temporal reasoning into a graph-based planning framework using local search . our techniques are implemented in a planner that took part in the 4th international planning competition ( ipc-4 ) . a statistical analysis of the results of ipc-4 demonstrates the effectiveness of our approach in terms of both cpu-time and plan quality . additional experiments show the good performance of the temporal reasoning techniques integrated into our planner .

the use of probabilistic systems to mimic the behaviour of idiotypic ais robot controllers
previous work has shown that robot navigation systems that employ an architecture based upon the idiotypic network theory of the immune system have an advantage over control techniques that rely on reinforcement learning only . this is thought to be a result of intelligent behaviour selection on the part of the idiotypic robot . in this paper an attempt is made to imitate idiotypic dynamics by creating controllers that use reinforcement with a number of different probabilistic schemes to select robot behaviour . the aims are to show that the idiotypic system is not merely performing some kind of periodic random behaviour selection , and to try to gain further insight into the processes that govern the idiotypic mechanism . trials are carried out using simulated pioneer robots that undertake navigation exercises . results show that a scheme that boosts the probability of selecting highly-ranked alternative behaviours to 50 % during stall conditions comes closest to achieving the properties of the idiotypic system , but remains unable to match it in terms of all round performance .

summary - terpret : a probabilistic programming language for program induction
we study machine learning formulations of inductive program synthesis ; that is , given input-output examples , synthesize source code that maps inputs to corresponding outputs . our key contribution is terpret , a domain-specific language for expressing program synthesis problems . a terpret model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs . the inference task is to observe a set of input-output examples and infer the underlying program . from a terpret model we automatically perform inference using four different back-ends : gradient descent ( thus each terpret model can be seen as defining a differentiable interpreter ) , linear program ( lp ) relaxations for graphical models , discrete satisfiability solving , and the sketch program synthesis system . terpret has two main benefits . first , it enables rapid exploration of a range of domains , program representations , and interpreter models . second , it separates the model specification from the inference algorithm , allowing proper comparisons between different approaches to inference . we illustrate the value of terpret by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models . to our knowledge , this is the first work to compare gradient-based search over program space to traditional search-based alternatives . our key empirical finding is that constraint solvers dominate the gradient descent and lp-based formulations . this is a workshop summary of a longer report at arxiv:1608.04428

distance-based self-attention network for natural language inference
attention mechanism has been used as an ancillary means to help rnn or cnn . however , the transformer ( vaswani et al. , 2017 ) recently recorded the state-of-the-art performance in machine translation with a dramatic reduction in training time by solely using attention . motivated by the transformer , directional self attention network ( shen et al. , 2017 ) , a fully attention-based sentence encoder , was proposed . it showed good performance with various data by using forward and backward directional information in a sentence . but in their study , not considered at all was the distance between words , an important feature when learning the local dependency to help understand the context of input text . we propose distance-based self-attention network , which considers the word distance by using a simple distance mask in order to model the local dependency without losing the ability of modeling global dependency which attention has inherent . our model shows good performance with nli data , and it records the new state-of-the-art result with snli data . additionally , we show that our model has a strength in long sentences or documents .

reduced basis decomposition : a certified and fast lossy data compression algorithm
dimension reduction is often needed in the area of data mining . the goal of these methods is to map the given high-dimensional data into a low-dimensional space preserving certain properties of the initial data . there are two kinds of techniques for this purpose . the first , projective methods , builds an explicit linear projection from the high-dimensional space to the low-dimensional one . on the other hand , the nonlinear methods utilizes nonlinear and implicit mapping between the two spaces . in both cases , the methods considered in literature have usually relied on computationally very intensive matrix factorizations , frequently the singular value decomposition ( svd ) . the computational burden of svd quickly renders these dimension reduction methods infeasible thanks to the ever-increasing sizes of the practical datasets . in this paper , we present a new decomposition strategy , reduced basis decomposition ( rbd ) , which is inspired by the reduced basis method ( rbm ) . given $ x $ the high-dimensional data , the method approximates it by $ y \ , t ( \approx x ) $ with $ y $ being the low-dimensional surrogate and $ t $ the transformation matrix . $ y $ is obtained through a greedy algorithm thus extremely efficient . in fact , it is significantly faster than svd with comparable accuracy . $ t $ can be computed on the fly . moreover , unlike many compression algorithms , it easily finds the mapping for an arbitrary `` out-of-sample '' vector and it comes with an `` error indicator '' certifying the accuracy of the compression . numerical results are shown validating these claims .

model-based hierarchical clustering
we present an approach to model-based hierarchical clustering by formulating an objective function based on a bayesian analysis . this model organizes the data into a cluster hierarchy while specifying a complex feature-set partitioning that is a key component of our model . features can have either a unique distribution in every cluster or a common distribution over some ( or even all ) of the clusters . the cluster subsets over which these features have such a common distribution correspond to the nodes ( clusters ) of the tree representing the hierarchy . we apply this general model to the problem of document clustering for which we use a multinomial likelihood function and dirichlet priors . our algorithm consists of a two-stage process wherein we first perform a flat clustering followed by a modified hierarchical agglomerative merging process that includes determining the features that will have common distributions over the merged clusters . the regularization induced by using the marginal likelihood automatically determines the optimal model structure including number of clusters , the depth of the tree and the subset of features to be modeled as having a common distribution at each node . we present experimental results on both synthetic data and a real document collection .

automatically restructuring practice guidelines using the gem dtd
this paper describes a system capable of semi-automatically filling an xml template from free texts in the clinical domain ( practice guidelines ) . the xml template includes semantic information not explicitly encoded in the text ( pairs of conditions and actions/recommendations ) . therefore , there is a need to compute the exact scope of conditions over text sequences expressing the required actions . we present a system developed for this task . we show that it yields good performance when applied to the analysis of french practice guidelines .

consistent query answering via asp from different perspectives : theory and practice
a data integration system provides transparent access to different data sources by suitably combining their data , and providing the user with a unified view of them , called global schema . however , source data are generally not under the control of the data integration process , thus integrated data may violate global integrity constraints even in presence of locally-consistent data sources . in this scenario , it may be anyway interesting to retrieve as much consistent information as possible . the process of answering user queries under global constraint violations is called consistent query answering ( cqa ) . several notions of cqa have been proposed , e.g. , depending on whether integrated information is assumed to be sound , complete , exact or a variant of them . this paper provides a contribution in this setting : it uniforms solutions coming from different perspectives under a common asp-based core , and provides query-driven optimizations designed for isolating and eliminating inefficiencies of the general approach for computing consistent answers . moreover , the paper introduces some new theoretical results enriching existing knowledge on decidability and complexity of the considered problems . the effectiveness of the approach is evidenced by experimental results . to appear in theory and practice of logic programming ( tplp ) .

an all-in-one network for dehazing and beyond
this paper proposes an image dehazing model built with a convolutional neural network ( cnn ) , called all-in-one dehazing network ( aod-net ) . it is designed based on a re-formulated atmospheric scattering model . instead of estimating the transmission matrix and the atmospheric light separately as most previous models did , aod-net directly generates the clean image through a light-weight cnn . such a novel end-to-end design makes it easy to embed aod-net into other deep models , e.g. , faster r-cnn , for improving high-level task performance on hazy images . experimental results on both synthesized and natural hazy image datasets demonstrate our superior performance than the state-of-the-art in terms of psnr , ssim and the subjective visual quality . furthermore , when concatenating aod-net with faster r-cnn and training the joint pipeline from end to end , we witness a large improvement of the object detection performance on hazy images .

les représentations génétiques d'objets : simples analogies ou modèles pertinents ? le point de vue de l ' `` évolutique '' . < br > & ndash ; & ndash ; & ndash ; < br > genetic representations of objects : simple analogies or efficient models ? the `` evolutic '' point of view
depuis une trentaine d'ann\ ' { e } es , les ing\ ' { e } nieurs utilisent couramment des analogies avec l'\ ' { e } volution naturelle pour optimiser des dispositifs techniques . le plus souvent , ces m\ ' { e } thodes `` g\ ' { e } n\ ' { e } tiques '' ou `` \ ' { e } volutionnaires '' sont consid\ ' { e } r\ ' { e } es uniquement du point de vue pratique , comme des m\ ' { e } thodes d'optimisation performantes , qu'on peut utiliser \ ` { a } la place d'autres m\ ' { e } thodes ( gradients , simplexes , ... ) . dans cet article , nous essayons de montrer que les sciences et les techniques , mais aussi les organisations humaines , et g\ ' { e } n\ ' { e } ralement tous les syst\ ` { e } mes complexes , ob\ ' { e } issent \ ` { a } des lois d'\ ' { e } volution dont la g\ ' { e } n\ ' { e } tique est un bon mod\ ` { e } le repr\ ' { e } sentatif , m\^ { e } me si g\^ { e } nes et chromosomes sont `` virtuels '' : ainsi loin d'\^ { e } tre seulement un outil ponctuel d'aide \ ` { a } la synth\ ` { e } se de solutions technologiques , la repr\ ' { e } sentation g\ ' { e } n\ ' { e } tique est-elle un mod\ ` { e } le dynamique global de l'\ ' { e } volution du monde fa\c { c } onn\ ' { e } par l'agitation humaine. & ndash ; & ndash ; & ndash ; & ndash ; for thirty years , engineers commonly use analogies with natural evolution to optimize technical devices . more often that not , these `` genetic '' or `` evolutionary '' methods are only view as efficient tools , which could replace other optimization techniques ( gradient methods , simplex , ... ) . in this paper , we try to show that sciences , techniques , human organizations , and more generally all complex systems , obey to evolution rules , whose the genetic is a good representative model , even if genes and chromosomes are `` virtual '' . thus , the genetic representation is not only a specific tool helping for the design of technological solutions , but also a global and dynamic model for the action of the human agitation on our world .

learning arithmetic circuits
graphical models are usually learned without regard to the cost of doing inference with them . as a result , even if a good model is learned , it may perform poorly at prediction , because it requires approximate inference . we propose an alternative : learning models with a score function that directly penalizes the cost of inference . specifically , we learn arithmetic circuits with a penalty on the number of edges in the circuit ( in which the cost of inference is linear ) . our algorithm is equivalent to learning a bayesian network with context-specific independence by greedily splitting conditional distributions , at each step scoring the candidates by compiling the resulting network into an arithmetic circuit , and using its size as the penalty . we show how this can be done efficiently , without compiling a circuit from scratch for each candidate . experiments on several real-world domains show that our algorithm is able to learn tractable models with very large treewidth , and yields more accurate predictions than a standard context-specific bayesian network learner , in far less time .

a new algorithm for finding map assignments to belief networks
we present a new algorithm for finding maximum a-posterior ) ( map ) assignments of values to belief networks . the belief network is compiled into a network consisting only of nodes with boolean ( i.e . only 0 or 1 ) conditional probabilities . the map assignment is then found using a best-first search on the resulting network . we argue that , as one would anticipate , the algorithm is exponential for the general case , but only linear in the size of the network for poly trees .

programming in logic without logic programming
in previous work , we proposed a logic-based framework in which computation is the execution of actions in an attempt to make reactive rules of the form if antecedent then consequent true in a canonical model of a logic program determined by an initial state , sequence of events , and the resulting sequence of subsequent states . in this model-theoretic semantics , reactive rules are the driving force , and logic programs play only a supporting role . in the canonical model , states , actions and other events are represented with timestamps . but in the operational semantics , for the sake of efficiency , timestamps are omitted and only the current state is maintained . state transitions are performed reactively by executing actions to make the consequents of rules true whenever the antecedents become true . this operational semantics is sound , but incomplete . it can not make reactive rules true by preventing their antecedents from becoming true , or by proactively making their consequents true before their antecedents become true . in this paper , we characterize the notion of reactive model , and prove that the operational semantics can generate all and only such models . in order to focus on the main issues , we omit the logic programming component of the framework .

strategic disclosure of opinions on a social network
we study the strategic aspects of social influence in a society of agents linked by a trust network , introducing a new class of games called games of influence . a game of influence is an infinite repeated game with incomplete information in which , at each stage of interaction , an agent can make her opinions visible ( public ) or invisible ( private ) in order to influence other agents ' opinions . the influence process is mediated by a trust network , as we assume that the opinion of a given agent is only affected by the opinions of those agents that she considers trustworthy ( i.e. , the agents in the trust network that are directly linked to her ) . each agent is endowed with a goal , expressed in a suitable temporal language inspired from linear temporal logic ( ltl ) . we show that games of influence provide a simple abstraction to explore the effects of the trust network structure on the agents ' behaviour , by considering solution concepts from game-theory such as nash equilibrium , weak dominance and winning strategies .

a new hedging algorithm and its application to inferring latent random variables
we present a new online learning algorithm for cumulative discounted gain . this learning algorithm does not use exponential weights on the experts . instead , it uses a weighting scheme that depends on the regret of the master algorithm relative to the experts . in particular , experts whose discounted cumulative gain is smaller ( worse ) than that of the master algorithm receive zero weight . we also sketch how a regret-based algorithm can be used as an alternative to bayesian averaging in the context of inferring latent random variables .

emotional metaheuristics for in-situ foraging using sensor constrained robot swarms
we present a new social animal inspired emotional swarm intelligence technique . this technique is used to solve a variant of the popular collective robots problem called foraging . we show with a simulation study how simple interaction rules based on sensations like hunger and loneliness can lead to globally coherent emergent behavior which allows sensor constrained robots to solve the given problem

reinforcement learning on web interfaces using workflow-guided exploration
reinforcement learning ( rl ) agents improve through trial-and-error , but when reward is sparse and the agent can not discover successful action sequences , learning stagnates . this has been a notable problem in training deep rl agents to perform web-based tasks , such as booking flights or replying to emails , where a single mistake can ruin the entire sequence of actions . a common remedy is to `` warm-start '' the agent by pre-training it to mimic expert demonstrations , but this is prone to overfitting . instead , we propose to constrain exploration using demonstrations . from each demonstration , we induce high-level `` workflows '' which constrain the allowable actions at each time step to be similar to those in the demonstration ( e.g. , `` step 1 : click on a textbox ; step 2 : enter some text '' ) . our exploration policy then learns to identify successful workflows and samples actions that satisfy these workflows . workflows prune out bad exploration directions and accelerate the agent 's ability to discover rewards . we use our approach to train a novel neural policy designed to handle the semi-structured nature of websites , and evaluate on a suite of web tasks , including the recent world of bits benchmark . we achieve new state-of-the-art results , and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 100x .

a discovery algorithm for directed cyclis graphs
directed acyclic graphs have been used fruitfully to represent causal strucures ( pearl 1988 ) . however , in the social sciences and elsewhere models are often used which correspond both causally and statistically to directed graphs with directed cycles ( spirtes 1995 ) . pearl ( 1993 ) discussed predicting the effects of intervention in models of this kind , so-called linear non-recursive structural equation models . this raises the question of whether it is possible to make inferences about causal structure with cycles , form sample data . in particular do there exist general , informative , feasible and reliable precedures for inferring causal structure from conditional independence relations among variables in a sample generated by an unknown causal structure ? in this paper i present a discovery algorithm that is correct in the large sample limit , given commonly ( but often implicitly ) made plausible assumptions , and which provides information about the existence or non-existence of causal pathways from one variable to another . the algorithm is polynomial on sparse graphs .

document clustering based on topic maps
importance of document clustering is now widely acknowledged by researchers for better management , smart navigation , efficient filtering , and concise summarization of large collection of documents like world wide web ( www ) . the next challenge lies in semantically performing clustering based on the semantic contents of the document . the problem of document clustering has two main components : ( 1 ) to represent the document in such a form that inherently captures semantics of the text . this may also help to reduce dimensionality of the document , and ( 2 ) to define a similarity measure based on the semantic representation such that it assigns higher numerical values to document pairs which have higher semantic relationship . feature space of the documents can be very challenging for document clustering . a document may contain multiple topics , it may contain a large set of class-independent general-words , and a handful class-specific core-words . with these features in mind , traditional agglomerative clustering algorithms , which are based on either document vector model ( dvm ) or suffix tree model ( stc ) , are less efficient in producing results with high cluster quality . this paper introduces a new approach for document clustering based on the topic map representation of the documents . the document is being transformed into a compact form . a similarity measure is proposed based upon the inferred information through topic maps data and structures . the suggested method is implemented using agglomerative hierarchal clustering and tested on standard information retrieval ( ir ) datasets . the comparative experiment reveals that the proposed approach is effective in improving the cluster quality .

learning optimized or 's of and 's
or 's of and 's ( oa ) models are comprised of a small number of disjunctions of conjunctions , also called disjunctive normal form . an example of an oa model is as follows : if ( $ x_1 = $ ` blue ' and $ x_2= $ ` middle ' ) or ( $ x_1 = $ ` yellow ' ) , then predict $ y=1 $ , else predict $ y=0 $ . or 's of and 's models have the advantage of being interpretable to human experts , since they are a set of conditions that concisely capture the characteristics of a specific subset of data . we present two optimization-based machine learning frameworks for constructing oa models , optimized oa ( ooa ) and its faster version , optimized oa with approximations ( ooax ) . we prove theoretical bounds on the properties of patterns in an oa model . we build oa models as a diagnostic screening tool for obstructive sleep apnea , that achieves high accuracy with a substantial gain in interpretability over other methods .

reasoning with qualitative probabilities can be tractable
we recently described a formalism for reasoning with if-then rules that re expressed with different levels of firmness [ 18 ] . the formalism interprets these rules as extreme conditional probability statements , specifying orders of magnitude of disbelief , which impose constraints over possible rankings of worlds . it was shown that , once we compute a priority function z+ on the rules , the degree to which a given query is confirmed or denied can be computed in o ( log n ` ) propositional satisfiability tests , where n is the number of rules in the knowledge base . in this paper , we show that computing z+ requires o ( n2 x log n ) satisfiability tests , not an exponential number as was conjectured in [ 18 ] , which reduces to polynomial complexity in the case of horn expressions . we also show how reasoning with imprecise observations can be incorporated in our formalism and how the popular notions of belief revision and epistemic entrenchment are embodied naturally and tractably .

network-based coverage of mutational profiles reveals cancer genes
a central goal in cancer genomics is to identify the somatic alterations that underpin tumor initiation and progression . this task is challenging as the mutational profiles of cancer genomes exhibit vast heterogeneity , with many alterations observed within each individual , few shared somatically mutated genes across individuals , and important roles in cancer for both frequently and infrequently mutated genes . while commonly mutated cancer genes are readily identifiable , those that are rarely mutated across samples are difficult to distinguish from the large numbers of other infrequently mutated genes . here , we introduce a method that considers per-individual mutational profiles within the context of protein-protein interaction networks in order to identify small connected subnetworks of genes that , while not individually frequently mutated , comprise pathways that are perturbed across ( i.e. , `` cover '' ) a large fraction of the individuals . we devise a simple yet intuitive objective function that balances identifying a small subset of genes with covering a large fraction of individuals . we show how to solve this problem optimally using integer linear programming and also give a fast heuristic algorithm that works well in practice . we perform a large-scale evaluation of our resulting method , ncop , on 6,038 tcga tumor samples across 24 different cancer types . we demonstrate that our approach ncop is more effective in identifying cancer genes than both methods that do not utilize any network information as well as state-of-the-art network-based methods that aggregate mutational information across individuals . overall , our work demonstrates the power of combining per-individual mutational information with interaction networks in order to uncover genes functionally relevant in cancers , and in particular those genes that are less frequently mutated .

algorithms for the greater good ! on mental modeling and acceptable symbiosis in human-ai collaboration
effective collaboration between humans and ai-based systems requires effective modeling of the human in the loop , both in terms of the mental state as well as the physical capabilities of the latter . however , these models can also open up pathways for manipulating and exploiting the human in the hopes of achieving some greater good , especially when the intent or values of the ai and the human are not aligned or when they have an asymmetrical relationship with respect to knowledge or computation power . in fact , such behavior does not necessarily require any malicious intent but can rather be borne out of cooperative scenarios . it is also beyond simple misinterpretation of intents , as in the case of value alignment problems , and thus can be effectively engineered if desired . such techniques already exist and pose several unresolved ethical and moral questions with regards to the design of autonomy . in this paper , we illustrate some of these issues in a teaming scenario and investigate how they are perceived by participants in a thought experiment .

knowledge as a teacher : knowledge-guided structural attention networks
natural language understanding ( nlu ) is a core component of a spoken dialogue system . recently recurrent neural networks ( rnn ) obtained strong results on nlu due to their superior ability of preserving sequential information over time . traditionally , the nlu module tags semantic slots for utterances considering their flat structures , as the underlying rnn structure is a linear chain . however , natural language exhibits linguistic properties that provide rich , structured information for better understanding . this paper introduces a novel model , knowledge-guided structural attention networks ( k-san ) , a generalization of rnn to additionally incorporate non-flat network topologies guided by prior knowledge . there are two characteristics : 1 ) important substructures can be captured from small training data , allowing the model to generalize to previously unseen test data ; 2 ) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences , so that the understanding performance can be improved . the experiments on the benchmark air travel information system ( atis ) data show that the proposed k-san architecture can effectively extract salient knowledge from substructures with an attention mechanism , and outperform the performance of the state-of-the-art neural network based frameworks .

étude longitudinale d'une procédure de modélisation de connaissances en matière de gestion du territoire agricole
this paper gives an introduction to this issue , and presents the framework and the main steps of the rosa project . four teams of researchers , agronomists , computer scientists , psychologists and linguists were involved during five years within this project that aimed at the development of a knowledge based system . the purpose of the rosa system is the modelling and the comparison of farm spatial organizations . it relies on a formalization of agronomical knowledge and thus induces a joint knowledge building process involving both the agronomists and the computer scientists . the paper describes the steps of the modelling process as well as the filming procedures set up by the psychologists and linguists in order to make explicit and to analyze the underlying knowledge building process .

algebraic properties of qualitative spatio-temporal calculi
qualitative spatial and temporal reasoning is based on so-called qualitative calculi . algebraic properties of these calculi have several implications on reasoning algorithms . but what exactly is a qualitative calculus ? and to which extent do the qualitative calculi proposed meet these demands ? the literature provides various answers to the first question but only few facts about the second . in this paper we identify the minimal requirements to binary spatio-temporal calculi and we discuss the relevance of the according axioms for representation and reasoning . we also analyze existing qualitative calculi and provide a classification involving different notions of a relation algebra .

crest : convolutional residual learning for visual tracking
discriminative correlation filters ( dcfs ) have been shown to perform superiorly in visual tracking . they only need a small set of training samples from the initial frame to generate an appearance model . however , existing dcfs learn the filters separately from feature extraction , and update these filters using a moving average operation with an empirical weight . these dcf trackers hardly benefit from the end-to-end training . in this paper , we propose the crest algorithm to reformulate dcfs as a one-layer convolutional neural network . our method integrates feature extraction , response map generation as well as model update into the neural networks for an end-to-end training . to reduce model degradation during online update , we apply residual learning to take appearance changes into account . extensive experiments on the benchmark datasets demonstrate that our crest tracker performs favorably against state-of-the-art trackers .

trust from the past : bayesian personalized ranking based link prediction in knowledge graphs
link prediction , or predicting the likelihood of a link in a knowledge graph based on its existing state is a key research task . it differs from a traditional link prediction task in that the links in a knowledge graph are categorized into different predicates and the link prediction performance of different predicates in a knowledge graph generally varies widely . in this work , we propose a latent feature embedding based link prediction model which considers the prediction task for each predicate disjointly . to learn the model parameters it utilizes a bayesian personalized ranking based optimization technique . experimental results on large-scale knowledge bases such as yago2 show that our link prediction approach achieves substantially higher performance than several state-of-art approaches . we also show that for a given predicate the topological properties of the knowledge graph induced by the given predicate edges are key indicators of the link prediction performance of that predicate in the knowledge graph .

icabidas : intuition centred architecture for big data analysis and synthesis
humans are expert in the amount of sensory data they deal with each moment . human brain not only analyses these data but also starts synthesizing new information from the existing data . the current age big-data systems are needed not just to analyze data but also to come up new interpretation . we believe that the pivotal ability in human brain which enables us to do this is what is known as `` intuition '' . here , we present an intuition based architecture for big data analysis and synthesis .

the top 10 topics in machine learning revisited : a quantitative meta-study
which topics of machine learning are most commonly addressed in research ? this question was initially answered in 2007 by doing a qualitative survey among distinguished researchers . in our study , we revisit this question from a quantitative perspective . concretely , we collect 54k abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences . we then use machine learning in order to determine the top 10 topics in machine learning . we not only include models , but provide a holistic view across optimization , data , features , etc . this quantitative approach allows reducing the bias of surveys . it reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are . this allows researchers to identify popular topics as well as new and rising topics for their research .

a logic programming framework for possibilistic argumentation with vague knowledge
defeasible argumentation frameworks have evolved to become a sound setting to formalize commonsense , qualitative reasoning from incomplete and potentially inconsistent knowledge . defeasible logic programming ( delp ) is a defeasible argumentation formalism based on an extension of logic programming . although delp has been successfully integrated in a number of different real-world applications , delp can not deal with explicit uncertainty , nor with vague knowledge , as defeasibility is directly encoded in the object language . this paper introduces p-delp , a new logic programming language that extends original delp capabilities for qualitative reasoning by incorporating the treatment of possibilistic uncertainty and fuzzy knowledge . such features will be formalized on the basis of pgl , a possibilistic logic based on godel fuzzy logic .

the complexity of reasoning with global constraints
constraint propagation is one of the techniques central to the success of constraint programming . to reduce search , fast algorithms associated with each constraint prune the domains of variables . with global ( or non-binary ) constraints , the cost of such propagation may be much greater than the quadratic cost for binary constraints . we therefore study the computational complexity of reasoning with global constraints . we first characterise a number of important questions related to constraint propagation . we show that such questions are intractable in general , and identify dependencies between the tractability and intractability of the different questions . we then demonstrate how the tools of computational complexity can be used in the design and analysis of specific global constraints . in particular , we illustrate how computational complexity can be used to determine when a lesser level of local consistency should be enforced , when constraints can be safely generalized , when decomposing constraints will reduce the amount of pruning , and when combining constraints is tractable .

towards a constructive multilayer perceptron for regression task using non-parametric clustering . a case study of photo-z redshift reconstruction
the choice of architecture of artificial neuron network ( ann ) is still a challenging task that users face every time . it greatly affects the accuracy of the built network . in fact there is no optimal method that is applicable to various implementations at the same time . in this paper we propose a method to construct ann based on clustering , that resolves the problems of random and ad hoc approaches for multilayer ann architecture . our method can be applied to regression problems . experimental results obtained with different datasets , reveals the efficiency of our method .

an approach to the analysis of the south slavic medieval labels using image texture
the paper presents a new script classification method for the discrimination of the south slavic medieval labels . it consists in the textural analysis of the script types . in the first step , each letter is coded by the equivalent script type , which is defined by its typographical features . obtained coded text is subjected to the run-length statistical analysis and to the adjacent local binary pattern analysis in order to extract the features . the result shows a diversity between the extracted features of the scripts , which makes the feature classification more effective . it is the basis for the classification process of the script identification by using an extension of a state-of-the-art approach for document clustering . the proposed method is evaluated on an example of hand-engraved in stone and hand-printed in paper labels in old cyrillic , angular and round glagolitic . experiments demonstrate very positive results , which prove the effectiveness of the proposed method .

agent-based perception of an environment in an emergency situation
we are interested in the problem of multiagent systems development for risk detecting and emergency response in an uncertain and partially perceived environment . the evaluation of the current situation passes by three stages inside the multiagent system . in a first time , the situation is represented in a dynamic way . the second step , consists to characterise the situation and finally , it is compared with other similar known situations . in this paper , we present an information modelling of an observed environment , that we have applied on the robocuprescue simulation system . information coming from the environment are formatted according to a taxonomy and using semantic features . the latter are defined thanks to a fine ontology of the domain and are managed by factual agents that aim to represent dynamically the current situation .

a simple reinforcement learning mechanism for resource allocation in lte-a networks with markov decision process and q-learning
resource allocation is still a difficult issue to deal with in wireless networks . the unstable channel condition and traffic demand for quality of service ( qos ) raise some barriers that interfere with the process . it is significant that an optimal policy takes into account some resources available to each traffic class while considering the spectral efficiency and other related channel issues . reinforcement learning is a dynamic and effective method to support the accomplishment of resource allocation properly maintaining qos levels for applications . the technique can track the system state as feedback to enhance the performance of a given task . herein , it is proposed a simple reinforcement learning mechanism introduced in lte-a networks and aimed to choose and limit the number of resources allocated for each traffic class , regarding the qos class identifier ( qci ) , at each transmission time interval ( tti ) along the scheduling procedure . the proposed mechanism implements a markov decision process ( mdp ) solved by the q-learning algorithm to find an optimal action-state decision policy . the results obtained from simulation exhibit good performance , especially for the real-time video application .

a convolutional autoencoder for multi-subject fmri data aggregation
finding the most effective way to aggregate multi-subject fmri data is a long-standing and challenging problem . it is of increasing interest in contemporary fmri studies of human cognition due to the scarcity of data per subject and the variability of brain anatomy and functional response across subjects . recent work on latent factor models shows promising results in this task but this approach does not preserve spatial locality in the brain . we examine two ways to combine the ideas of a factor model and a searchlight based analysis to aggregate multi-subject fmri data while preserving spatial locality . we first do this directly by combining a recent factor method known as a shared response model with searchlight analysis . then we design a multi-view convolutional autoencoder for the same task . both approaches preserve spatial locality and have competitive or better performance compared with standard searchlight analysis and the shared response model applied across the whole brain . we also report a system design to handle the computational challenge of training the convolutional autoencoder .

automated conjecturing vii : the graph brain project & big mathematics
the graph brain project is an experiment in how the use of automated mathematical discovery software , databases , large collaboration , and systematic investigation provide a model for how mathematical research might proceed in the future . our project began with the development of a program that can be used to generate invariant-relation and property-relation conjectures in many areas of mathematics . this program can produce conjectures which are not implied by existing ( published ) theorems . here we propose a new approach to push forward existing mathematical research goals -- -using automated mathematical discovery software . we suggest how to initiate and harness large-scale collaborative mathematics . we envision mathematical research labs similar to what exist in other sciences , new avenues for funding , new opportunities for training students , and a more efficient and effective use of published mathematical research . and our experiment in graph theory can be imitated in many other areas of mathematical research . big mathematics is the idea of large , systematic , collaborative research on problems of existing mathematical interest . what is possible when we put our skills , tools , and results together systematically ?

online planning algorithms for pomdps
partially observable markov decision processes ( pomdps ) provide a rich framework for sequential decision-making under uncertainty in stochastic domains . however , solving a pomdp is often intractable except for small problems due to their complexity . here , we focus on online approaches that alleviate the computational complexity by computing good local policies at each decision step during the execution . online algorithms generally consist of a lookahead search to find the best action to execute at each time step in an environment . our objectives here are to survey the various existing online pomdp methods , analyze their properties and discuss their advantages and disadvantages ; and to thoroughly evaluate these online approaches in different environments under various metrics ( return , error bound reduction , lower bound improvement ) . our experimental results indicate that state-of-the-art online heuristic search methods can handle large pomdp domains efficiently .

symmetry within and between solutions
symmetry can be used to help solve many problems . for instance , einstein 's famous 1905 paper ( `` on the electrodynamics of moving bodies '' ) uses symmetry to help derive the laws of special relativity . in artificial intelligence , symmetry has played an important role in both problem representation and reasoning . i describe recent work on using symmetry to help solve constraint satisfaction problems . symmetries occur within individual solutions of problems as well as between different solutions of the same problem . symmetry can also be applied to the constraints in a problem to give new symmetric constraints . reasoning about symmetry can speed up problem solving , and has led to the discovery of new results in both graph and number theory .

logic and constraint logic programming for distributed constraint optimization
the field of distributed constraint optimization problems ( dcops ) has gained momentum , thanks to its suitability in capturing complex problems ( e.g. , multi-agent coordination and resource allocation problems ) that are naturally distributed and can not be realistically addressed in a centralized manner . the state of the art in solving dcops relies on the use of ad-hoc infrastructures and ad-hoc constraint solving procedures . this paper investigates an infrastructure for solving dcops that is completely built on logic programming technologies . in particular , the paper explores the use of a general constraint solver ( a constraint logic programming system in this context ) to handle the agent-level constraint solving . the preliminary experiments show that logic programming provides benefits over a state-of-the-art dcop system , in terms of performance and scalability , opening the doors to the use of more advanced technology ( e.g. , search strategies and complex constraints ) for solving dcops .

attend and predict : understanding gene regulation by selective attention on chromatin
the past decade has seen a revolution in genomic technologies that enable a flood of genome-wide profiling of chromatin marks . recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements . two fundamental challenges exist for such learning tasks : ( 1 ) genome-wide chromatin signals are spatially structured , high-dimensional and highly modular ; and ( 2 ) the core aim is to understand what are the relevant factors and how they work together ? previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions . this paper presents an attention-based deep learning approach ; we call attentivechrome , that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation . attentivechrome uses a hierarchy of multiple long short-term memory ( lstm ) modules to encode the input signals and to model how various chromatin marks cooperate automatically . attentivechrome trains two levels of attention jointly with the target prediction , enabling it to attend differentially to relevant marks and to locate important positions per mark . we evaluate the model across 56 different cell types ( tasks ) in human . not only is the proposed architecture more accurate , but its attention scores also provide a better interpretation than state-of-the-art feature visualization methods such as saliency map . code and data are shared at www.deepchrome.org

approximate inference and constrained optimization
loopy and generalized belief propagation are popular algorithms for approximate inference in markov random fields and bayesian networks . fixed points of these algorithms correspond to extrema of the bethe and kikuchi free energy . however , belief propagation does not always converge , which explains the need for approaches that explicitly minimize the kikuchi/bethe free energy , such as cccp and ups . here we describe a class of algorithms that solves this typically nonconvex constrained minimization of the kikuchi free energy through a sequence of convex constrained minimizations of upper bounds on the kikuchi free energy . intuitively one would expect tighter bounds to lead to faster algorithms , which is indeed convincingly demonstrated in our simulations . several ideas are applied to obtain tight convex bounds that yield dramatic speed-ups over cccp .

cooperative searching for stochastic targets
spatial search problems abound in the real world , from locating hidden nuclear or chemical sources to finding skiers after an avalanche . we exemplify the formalism and solution for spatial searches involving two agents that may or may not choose to share information during a search . for certain classes of tasks , sharing information between multiple searchers makes cooperative searching advantageous . in some examples , agents are able to realize synergy by aggregating information and moving based on local judgments about maximal information gathering expectations . we also explore one- and two-dimensional simplified situations analytically and numerically to provide a framework for analyzing more complex problems . these general considerations provide a guide for designing optimal algorithms for real-world search problems .

on the read-once property of branching programs and cnfs of bounded treewidth
in this paper we prove a space lower bound of $ n^ { \omega ( k ) } $ for non-deterministic ( syntactic ) read-once branching programs ( { \sc nrobp } s ) on functions expressible as { \sc cnf } s with treewidth at most $ k $ of their primal graphs . this lower bound rules out the possibility of fixed-parameter space complexity of { \sc nrobp } s parameterized by $ k $ . we use lower bound for { \sc nrobp } s to obtain a quasi-polynomial separation between free binary decision diagrams and decision decomposable negation normal forms , essentially matching the existing upper bound introduced by beame et al . and thus proving the tightness of the latter .

further connections between contract-scheduling and ray-searching problems
this paper addresses two classes of different , yet interrelated optimization problems . the first class of problems involves a robot that must locate a hidden target in an environment that consists of a set of concurrent rays . the second class pertains to the design of interruptible algorithms by means of a schedule of contract algorithms . we study several variants of these families of problems , such as searching and scheduling with probabilistic considerations , redundancy and fault-tolerance issues , randomized strategies , and trade-offs between performance and preemptions . for many of these problems we present the first known results that apply to multi-ray and multi-problem domains . our objective is to demonstrate that several well-motivated settings can be addressed using the same underlying approach .

a cookbook for temporal conceptual data modelling with description logics
we design temporal description logics suitable for reasoning about temporal conceptual data models and investigate their computational complexity . our formalisms are based on dl-lite logics with three types of concept inclusions ( ranging from atomic concept inclusions and disjointness to the full booleans ) , as well as cardinality constraints and role inclusions . in the temporal dimension , they capture future and past temporal operators on concepts , flexible and rigid roles , the operators ` always ' and ` some time ' on roles , data assertions for particular moments of time and global concept inclusions . the logics are interpreted over the cartesian products of object domains and the flow of time ( z , < ) , satisfying the constant domain assumption . we prove that the most expressive of our temporal description logics ( which can capture lifespan cardinalities and either qualitative or quantitative evolution constraints ) turn out to be undecidable . however , by omitting some of the temporal operators on concepts/roles or by restricting the form of concept inclusions we obtain logics whose complexity ranges between pspace and nlogspace . these positive results were obtained by reduction to various clausal fragments of propositional temporal logic , which opens a way to employ propositional or first-order temporal provers for reasoning about temporal data models .

constructing datasets for multi-hop reading comprehension across documents
most reading comprehension methods limit themselves to queries which can be answered using a single sentence , paragraph , or document . enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods , but currently there exist no resources to train and test this capability . we propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods . in our task , a model learns to seek and combine evidence - effectively performing multi-hop ( alias multi-step ) inference . we devise a methodology to produce datasets for this task , given a collection of query-answer pairs and thematically linked documents . two datasets from different domains are induced , and we identify potential pitfalls and devise circumvention strategies . we evaluate two previously proposed competitive models and find that one can integrate information across documents . however , both models struggle to select relevant information , as providing documents guaranteed to be relevant greatly improves their performance . while the models outperform several strong baselines , their best accuracy reaches 42.9 % compared to human performance at 74.0 % - leaving ample room for improvement .

une mesure d'expertise pour le crowdsourcing
crowdsourcing , a major economic issue , is the fact that the firm outsources internal task to the crowd . it is a form of digital subcontracting for the general public . the evaluation of the participants work quality is a major issue in crowdsourcing . indeed , contributions must be controlled to ensure the effectiveness and relevance of the campaign . we are particularly interested in small , fast and not automatable tasks . several methods have been proposed to solve this problem , but they are applicable when the `` golden truth '' is not always known . this work has the particularity to propose a method for calculating the degree of expertise in the presence of gold data in crowdsourcing . this method is based on the belief function theory and proposes a structuring of data using graphs . the proposed approach will be assessed and applied to the data .

using data analytics to detect anomalous states in vehicles
vehicles are becoming more and more connected , this opens up a larger attack surface which not only affects the passengers inside vehicles , but also people around them . these vulnerabilities exist because modern systems are built on the comparatively less secure and old can bus framework which lacks even basic authentication . since a new protocol can only help future vehicles and not older vehicles , our approach tries to solve the issue as a data analytics problem and use machine learning techniques to secure cars . we develop a hidden markov model to detect anomalous states from real data collected from vehicles . using this model , while a vehicle is in operation , we are able to detect and issue alerts . our model could be integrated as a plug-n-play device in all new and old cars .

an enhanced branch-and-bound algorithm for the talent scheduling problem
the talent scheduling problem is a simplified version of the real-world film shooting problem , which aims to determine a shooting sequence so as to minimize the total cost of the actors involved . in this article , we first formulate the problem as an integer linear programming model . next , we devise a branch-and-bound algorithm to solve the problem . the branch-and-bound algorithm is enhanced by several accelerating techniques , including preprocessing , dominance rules and caching search states . extensive experiments over two sets of benchmark instances suggest that our algorithm is superior to the current best exact algorithm . finally , the impacts of different parameter settings are disclosed by some additional experiments .

differential performance debugging with discriminant regression trees
differential performance debugging is a technique to find performance problems . it applies in situations where the performance of a program is ( unexpectedly ) different for different classes of inputs . the task is to explain the differences in asymptotic performance among various input classes in terms of program internals . we propose a data-driven technique based on discriminant regression tree ( drt ) learning problem where the goal is to discriminate among different classes of inputs . we propose a new algorithm for drt learning that first clusters the data into functional clusters , capturing different asymptotic performance classes , and then invokes off-the-shelf decision tree learning algorithms to explain these clusters . we focus on linear functional clusters and adapt classical clustering algorithms ( k-means and spectral ) to produce them . for the k-means algorithm , we generalize the notion of the cluster centroid from a point to a linear function . we adapt spectral clustering by defining a novel kernel function to capture the notion of linear similarity between two data points . we evaluate our approach on benchmarks consisting of java programs where we are interested in debugging performance . we show that our algorithm significantly outperforms other well-known regression tree learning algorithms in terms of running time and accuracy of classification .

a literature based approach to define the scope of biomedical ontologies : a case study on a rehabilitation therapy ontology
in this article , we investigate our early attempts at building an ontology describing rehabilitation therapies following brain injury . these therapies are wide-ranging , involving interventions of many different kinds . as a result , these therapies are hard to describe . as well as restricting actual practice , this is also a major impediment to evidence-based medicine as it is hard to meaningfully compare two treatment plans . ontology development requires significant effort from both ontologists and domain experts . knowledge elicited from domain experts forms the scope of the ontology . the process of knowledge elicitation is expensive , consumes experts ' time and might have biases depending on the selection of the experts . various methodologies and techniques exist for enabling this knowledge elicitation , including community groups and open development practices . a related problem is that of defining scope . by defining the scope , we can decide whether a concept ( i.e . term ) should be represented in the ontology . this is the opposite of knowledge elicitation , in the sense that it defines what should not be in the ontology . this can be addressed by pre-defining a set of competency questions . these approaches are , however , expensive and time-consuming . here , we describe our work toward an alternative approach , bootstrapping the ontology from an initially small corpus of literature that will define the scope of the ontology , expanding this to a set covering the domain , then using information extraction to define an initial terminology to provide the basis and the competencies for the ontology . here , we discuss four approaches to building a suitable corpus that is both sufficiently covering and precise .

towards synthesizing complex programs from input-output examples
in recent years , deep learning techniques have been developed to improve the performance of program synthesis from input-output examples . albeit its significant progress , the programs that can be synthesized by state-of-the-art approaches are still simple in terms of their complexity . in this work , we move a significant step forward along this direction by proposing a new class of challenging tasks in the domain of program synthesis from input-output examples : learning a context-free parser from pairs of input programs and their parse trees . we show that this class of tasks are much more challenging than previously studied tasks , and the test accuracy of existing approaches is almost 0 % . we tackle the challenges by developing three novel techniques inspired by three novel observations , which reveal the key ingredients of using deep learning to synthesize a complex program . first , the use of a non-differentiable machine is the key to effectively restrict the search space . thus our proposed approach learns a neural program operating a domain-specific non-differentiable machine . second , recursion is the key to achieve generalizability . thus , we bake-in the notion of recursion in the design of our non-differentiable machine . third , reinforcement learning is the key to learn how to operate the non-differentiable machine , but it is also hard to train the model effectively with existing reinforcement learning algorithms from a cold boot . we develop a novel two-phase reinforcement learning-based search algorithm to overcome this issue . in our evaluation , we show that using our novel approach , neural parsing programs can be learned to achieve 100 % test accuracy on test inputs that are 500x longer than the training samples .

integral policy iterations for reinforcement learning problems in continuous time and space
policy iteration ( pi ) is a recursive process of policy evaluation and improvement to solve an optimal decision-making , e.g. , reinforcement learning ( rl ) or optimal control problem and has served as the fundamental to develop rl methods . motivated by integral pi ( ipi ) schemes in optimal control and rl methods in continuous time and space ( cts ) , this paper proposes on-policy ipi to solve the general rl problem in cts , with its environment modeled by an ordinary differential equation ( ode ) . in such continuous domain , we also propose four off-policy ipi methods -- -two are the ideal pi forms that use advantage and q-functions , respectively , and the other two are natural extensions of the existing off-policy ipi schemes to our general rl framework . compared to the ipi methods in optimal control , the proposed ipi schemes can be applied to more general situations and do not require an initial stabilizing policy to run ; they are also strongly relevant to the rl algorithms in cts such as advantage updating , q-learning , and value-gradient based ( vgb ) greedy policy improvement . our on-policy ipi is basically model-based but can be made partially model-free ; each off-policy method is also either partially or completely model-free . the mathematical properties of the ipi methods -- -admissibility , monotone improvement , and convergence towards the optimal solution -- -are all rigorously proven , together with the equivalence of on- and off-policy ipi . finally , the ipi methods are simulated with an inverted-pendulum model to support the theory and verify the performance .

learning multiple levels of representations with kernel machines
we propose a connectionist-inspired kernel machine model with three key advantages over traditional kernel machines . first , it is capable of learning distributed and hierarchical representations . second , its performance is highly robust to the choice of kernel function . third , the solution space is not limited to the span of images of training data in reproducing kernel hilbert space ( rkhs ) . together with the architecture , we propose a greedy learning algorithm that allows the proposed multilayer network to be trained layer-wise without backpropagation by optimizing the geometric properties of images in rkhs . with a single fixed generic kernel for each layer and two layers in total , our model compares favorably with state-of-the-art multiple kernel learning algorithms using significantly more kernels and popular deep architectures on widely used classification benchmarks .

discovering shared and individual latent structure in multiple time series
this paper proposes a nonparametric bayesian method for exploratory data analysis and feature construction in continuous time series . our method focuses on understanding shared features in a set of time series that exhibit significant individual variability . our method builds on the framework of latent diricihlet allocation ( lda ) and its extension to hierarchical dirichlet processes , which allows us to characterize each series as switching between latent `` topics '' , where each topic is characterized as a distribution over `` words '' that specify the series dynamics . however , unlike standard applications of lda , we discover the words as we learn the model . we apply this model to the task of tracking the physiological signals of premature infants ; our model obtains clinically significant insights as well as useful features for supervised learning tasks .

role of simplicity in creative behaviour : the case of the poietic generator
we propose to apply simplicity theory ( st ) to model interest in creative situations . st has been designed to describe and predict interest in communication . here we use st to derive a decision rule that we apply to a simplified version of a creative game , the poietic generator . the decision rule produces what can be regarded as an elementary form of creativity . this study is meant as a proof of principle . it suggests that some creative actions may be motivated by the search for unexpected simplicity .

great expectations . part ii : generalized expected utility as a universal decision rule
many different rules for decision making have been introduced in the literature . we show that a notion of generalized expected utility proposed in part i of this paper is a universal decision rule , in the sense that it can represent essentially all other decision rules .

solving transition-independent multi-agent mdps with sparse interactions ( extended version )
in cooperative multi-agent sequential decision making under uncertainty , agents must coordinate to find an optimal joint policy that maximises joint value . typical algorithms exploit additive structure in the value function , but in the fully-observable multi-agent mdp setting ( mmdp ) such structure is not present . we propose a new optimal solver for transition-independent mmdps , in which agents can only affect their own state but their reward depends on joint transitions . we represent these dependencies compactly in conditional return graphs ( crgs ) . using crgs the value of a joint policy and the bounds on partially specified joint policies can be efficiently computed . we propose core , a novel branch-and-bound policy search algorithm building on crgs . core typically requires less runtime than the available alternatives and finds solutions to problems previously unsolvable .

on nonspecific evidence
when simultaneously reasoning with evidences about several different events it is necessary to separate the evidence according to event . these events should then be handled independently . however , when propositions of evidences are weakly specified in the sense that it may not be certain to which event they are referring , this may not be directly possible . in this paper a criterion for partitioning evidences into subsets representing events is established . this criterion , derived from the conflict within each subset , involves minimising a criterion function for the overall conflict of the partition . an algorithm based on characteristics of the criterion function and an iterative optimisation among partitionings of evidences is proposed .

segmentation of skin lesions based on fuzzy classification of pixels and histogram thresholding
this paper proposes an innovative method for segmentation of skin lesions in dermoscopy images developed by the authors , based on fuzzy classification of pixels and histogram thresholding .

quantum enhanced inference in markov logic networks
markov logic networks ( mlns ) reconcile two opposing schools in machine learning and artificial intelligence : causal networks , which account for uncertainty extremely well , and first-order logic , which allows for formal deduction . an mln is essentially a first-order logic template to generate markov networks . inference in mlns is probabilistic and it is often performed by approximate methods such as markov chain monte carlo ( mcmc ) gibbs sampling . an mln has many regular , symmetric structures that can be exploited at both first-order level and in the generated markov network . we analyze the graph structures that are produced by various lifting methods and investigate the extent to which quantum protocols can be used to speed up gibbs sampling with state preparation and measurement schemes . we review different such approaches , discuss their advantages , theoretical limitations , and their appeal to implementations . we find that a straightforward application of a recent result yields exponential speedup compared to classical heuristics in approximate probabilistic inference , thereby demonstrating another example where advanced quantum resources can potentially prove useful in machine learning .

variable annealing length and parallelism in simulated annealing
in this paper , we propose : ( a ) a restart schedule for an adaptive simulated annealer , and ( b ) parallel simulated annealing , with an adaptive and parameter-free annealing schedule . the foundation of our approach is the modified lam annealing schedule , which adaptively controls the temperature parameter to track a theoretically ideal rate of acceptance of neighboring states . a sequential implementation of modified lam simulated annealing is almost parameter-free . however , it requires prior knowledge of the annealing length . we eliminate this parameter using restarts , with an exponentially increasing schedule of annealing lengths . we then extend this restart schedule to parallel implementation , executing several modified lam simulated annealers in parallel , with varying initial annealing lengths , and our proposed parallel annealing length schedule . to validate our approach , we conduct experiments on an np-hard scheduling problem with sequence-dependent setup constraints . we compare our approach to fixed length restarts , both sequentially and in parallel . our results show that our approach can achieve substantial performance gains , throughout the course of the run , demonstrating our approach to be an effective anytime algorithm .

learning measures of semi-additive behaviour
in business analytics , measure values , such as sales numbers or volumes of cargo transported , are often summed along values of one or more corresponding categories , such as time or shipping container . however , not every measure should be added by default ( e.g. , one might more typically want a mean over the heights of a set of people ) ; similarly , some measures should only be summed within certain constraints ( e.g. , population measures need not be summed over years ) . in systems such as watson analytics , the exact additive behaviour of a measure is often determined by a human expert . in this work , we propose a small set of features for this issue . we use these features in a case-based reasoning approach , where the system suggests an aggregation behaviour , with 86 % accuracy in our collected dataset .

how many folders do you really need ?
email classification is still a mostly manual task . consequently , most web mail users never define a single folder . recently however , automatic classification offering the same categories to all users has started to appear in some web mail clients , such as aol or gmail . we adopt this approach , rather than previous ( unsuccessful ) personalized approaches because of the change in the nature of consumer email traffic , which is now dominated by ( non-spam ) machine-generated email . we propose here a novel approach for ( 1 ) automatically distinguishing between personal and machine-generated email and ( 2 ) classifying messages into latent categories , without requiring users to have defined any folder . we report how we have discovered that a set of 6 `` latent '' categories ( one for human- and the others for machine-generated messages ) can explain a significant portion of email traffic . we describe in details the steps involved in building a web-scale email categorization system , from the collection of ground-truth labels , the selection of features to the training of models . experimental evaluation was performed on more than 500 billion messages received during a period of six months by users of yahoo mail service , who elected to be part of such research studies . our system achieved precision and recall rates close to 90 % and the latent categories we discovered were shown to cover 70 % of both email traffic and email search queries . we believe that these results pave the way for a change of approach in the web mail industry , and could support the invention of new large-scale email discovery paradigms that had not been possible before .

allsat compressed with wildcards . part 1 : converting cnf 's to orthogonal dnf 's
for most branching algorithms in boolean logic `` branching '' means `` variable-wise branching '' . we present the apparently novel technique of clause-wise branching , which is used to solve the allsat problem for arbitrary boolean functions in cnf format . specifically , it converts a cnf into an orthogonal dnf , i.e . into an exclusive sum of products . our method is enhanced by two ingredients : the use of a good sat-solver and wildcards beyond the common don't-care symbol .

extending term subsumption systems for uncertainty management
a major difficulty in developing and maintaining very large knowledge bases originates from the variety of forms in which knowledge is made available to the kb builder . the objective of this research is to bring together two complementary knowledge representation schemes : term subsumption languages , which represent and reason about defining characteristics of concepts , and proximate reasoning models , which deal with uncertain knowledge and data in expert systems . previous works in this area have primarily focused on probabilistic inheritance . in this paper , we address two other important issues regarding the integration of term subsumption-based systems and approximate reasoning models . first , we outline a general architecture that specifies the interactions between the deductive reasoner of a term subsumption system and an approximate reasoner . second , we generalize the semantics of terminological language so that terminological knowledge can be used to make plausible inferences . the architecture , combined with the generalized semantics , forms the foundation of a synergistic tight integration of term subsumption systems and approximate reasoning models .

self-organising networks for classification : developing applications to science analysis for astroparticle physics
physics analysis in astroparticle experiments requires the capability of recognizing new phenomena ; in order to establish what is new , it is important to develop tools for automatic classification , able to compare the final result with data from different detectors . a typical example is the problem of gamma ray burst detection , classification , and possible association to known sources : for this task physicists will need in the next years tools to associate data from optical databases , from satellite experiments ( egret , glast ) , and from cherenkov telescopes ( magic , hess , cangaroo , veritas ) .

best-first and depth-first minimax search in practice
most practitioners use a variant of the alpha-beta algorithm , a simple depth-first pro- cedure , for searching minimax trees . sss* , with its best-first search strategy , reportedly offers the potential for more efficient search . however , the complex formulation of the al- gorithm and its alleged excessive memory requirements preclude its use in practice . for two decades , the search efficiency of `` smart '' best-first sss* has cast doubt on the effectiveness of `` dumb '' depth-first alpha-beta . this paper presents a simple framework for calling alpha-beta that allows us to create a variety of algorithms , including sss* and dual* . in effect , we formulate a best-first algorithm using depth-first search . expressed in this framework sss* is just a special case of alpha-beta , solving all of the perceived drawbacks of the algorithm . in practice , alpha-beta variants typically evaluate less nodes than sss* . a new instance of this framework , mtd ( f ) , out-performs sss* and negascout , the alpha-beta variant of choice by practitioners .

a generalised quantifier theory of natural language in categorical compositional distributional semantics with bialgebras
categorical compositional distributional semantics is a model of natural language ; it combines the statistical vector space models of words with the compositional models of grammar . we formalise in this model the generalised quantifier theory of natural language , due to barwise and cooper . the underlying setting is a compact closed category with bialgebras . we start from a generative grammar formalisation and develop an abstract categorical compositional semantics for it , then instantiate the abstract setting to sets and relations and to finite dimensional vector spaces and linear maps . we prove the equivalence of the relational instantiation to the truth theoretic semantics of generalised quantifiers . the vector space instantiation formalises the statistical usages of words and enables us to , for the first time , reason about quantified phrases and sentences compositionally in distributional semantics .

introducing dendritic cells as a novel immune-inspired algorithm for anomoly detection
dendritic cells are antigen presenting cells that provide a vital link between the innate and adaptive immune system . research into this family of cells has revealed that they perform the role of coordinating t-cell based immune responses , both reactive and for generating tolerance . we have derived an algorithm based on the functionality of these cells , and have used the signals and differentiation pathways to build a control mechanism for an artificial immune system . we present our algorithmic details in addition to some preliminary results , where the algorithm was applied for the purpose of anomaly detection . we hope that this algorithm will eventually become the key component within a large , distributed immune system , based on sound immunological concepts .

example selection for dictionary learning
in unsupervised learning , an unbiased uniform sampling strategy is typically used , in order that the learned features faithfully encode the statistical structure of the training data . in this work , we explore whether active example selection strategies - algorithms that select which examples to use , based on the current estimate of the features - can accelerate learning . specifically , we investigate effects of heuristic and saliency-inspired selection algorithms on the dictionary learning task with sparse activations . we show that some selection algorithms do improve the speed of learning , and we speculate on why they might work .

aspartame : solving constraint satisfaction problems with answer set programming
encoding finite linear csps as boolean formulas and solving them by using modern sat solvers has proven to be highly effective , as exemplified by the award-winning sugar system . we here develop an alternative approach based on asp . this allows us to use first-order encodings providing us with a high degree of flexibility for easy experimentation with different implementations . the resulting system aspartame re-uses parts of sugar for parsing and normalizing csps . the obtained set of facts is then combined with an asp encoding that can be grounded and solved by off-the-shelf asp systems . we establish the competitiveness of our approach by empirically contrasting aspartame and sugar .

a new characterization of probabilities in bayesian networks
we characterize probabilities in bayesian networks in terms of algebraic expressions called quasi-probabilities . these are arrived at by casting bayesian networks as noisy and-or-not networks , and viewing the subnetworks that lead to a node as arguments for or against a node . quasi-probabilities are in a sense the `` natural '' algebra of bayesian networks : we can easily compute the marginal quasi-probability of any node recursively , in a compact form ; and we can obtain the joint quasi-probability of any set of nodes by multiplying their marginals ( using an idempotent product operator ) . quasi-probabilities are easily manipulated to improve the efficiency of probabilistic inference . they also turn out to be representable as square-wave pulse trains , and joint and marginal distributions can be computed by multiplication and complementation of pulse trains .

predicting demographics , moral foundations , and human values from digital behaviors
personal electronic devices such as smartphones give access to a broad range of behavioral signals that can be used to learn about the characteristics and preferences of individuals . in this study we explore the connection between demographic and psychological attributes and digital records for a cohort of 7,633 people , closely representative of the us population with respect to gender , age , geographical distribution , education , and income . we collected self-reported assessments on validated psychometric questionnaires based on both the moral foundations and basic human values theories , and combined this information with passively-collected multi-modal digital data from web browsing behavior , smartphone usage and demographic data . then , we designed a machine learning framework to infer both the demographic and psychological attributes from the behavioral data . in a cross-validated setting , our model is found to predict demographic attributes with good accuracy ( weighted auc scores of 0.90 for gender , 0.71 for age , 0.74 for ethnicity ) . our weighted auc scores for moral foundation attributes ( 0.66 ) and human values attributes ( 0.60 ) suggest that accurate prediction of complex psychometric attributes is more challenging but feasible . this connection might prove useful for designing personalized services , communication strategies , and interventions , and can be used to sketch a portrait of people with similar worldviews .

low-rank and sparse soft targets to learn better dnn acoustic models
conventional deep neural networks ( dnn ) for speech acoustic modeling rely on gaussian mixture models ( gmm ) and hidden markov model ( hmm ) to obtain binary class labels as the targets for dnn training . subword classes in speech recognition systems correspond to context-dependent tied states or senones . the present work addresses some limitations of gmm-hmm senone alignments for dnn training . we hypothesize that the senone probabilities obtained from a dnn trained with binary labels can provide more accurate targets to learn better acoustic models . however , dnn outputs bear inaccuracies which are exhibited as high dimensional unstructured noise , whereas the informative components are structured and low-dimensional . we exploit principle component analysis ( pca ) and sparse coding to characterize the senone subspaces . enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for dnn acoustic modeling , that also enables training with untranscribed data . experiments conducted on ami corpus shows 4.6 % relative reduction in word error rate .

a generic approach for escaping saddle points
a central challenge to using first-order methods for optimizing nonconvex problems is the presence of saddle points . first-order methods often get stuck at saddle points , greatly deteriorating their performance . typically , to escape from saddles one has to use second-order methods . however , most works on second-order methods rely extensively on expensive hessian-based computations , making them impractical in large-scale settings . to tackle this challenge , we introduce a generic framework that minimizes hessian based computations while at the same time provably converging to second-order critical points . our framework carefully alternates between a first-order and a second-order subroutine , using the latter only close to saddle points , and yields convergence results competitive to the state-of-the-art . empirical results suggest that our strategy also enjoys a good practical performance .

exploiting reputation in distributed virtual environments
the cognitive research on reputation has shown several interesting properties that can improve both the quality of services and the security in distributed electronic environments . in this paper , the impact of reputation on decision-making under scarcity of information will be shown . first , a cognitive theory of reputation will be presented , then a selection of simulation experimental results from different studies will be discussed . such results concern the benefits of reputation when agents need to find out good sellers in a virtual market-place under uncertainty and informational cheating .

simple cortex : a model of cells in the sensory nervous system
neuroscience research has produced many theories and computational neural models of sensory nervous systems . notwithstanding many different perspectives towards developing intelligent machines , artificial intelligence has ultimately been influenced by neuroscience . therefore , this paper provides an introduction to biologically inspired machine intelligence by exploring the basic principles of sensation and perception as well as the structure and behavior of biological sensory nervous systems like the neocortex . concepts like spike timing , synaptic plasticity , inhibition , neural structure , and neural behavior are applied to a new model , simple cortex ( sc ) . a software implementation of sc has been built and demonstrates fast observation , learning , and prediction of spatio-temporal sensory-motor patterns and sequences . finally , this paper suggests future areas of improvement and growth for simple cortex and other related machine intelligence models .

toward a dynamic programming solution for the 4-peg tower of hanoi problem with configurations
the frame-stewart algorithm for the 4-peg variant of the tower of hanoi , introduced in 1941 , partitions disks into intermediate towers before moving the remaining disks to their destination . algorithms that partition the disks have not been proven to be optimal , although they have been verified for up to 30 disks . this paper presents a dynamic programming approach to this algorithm , using tabling in b-prolog . this study uses a variation of the problem , involving configurations of disks , in order to contrast the tabling approach with the approaches utilized by other solvers . a comparison of different partitioning locations for the frame-stewart algorithm indicates that , although certain partitions are optimal for the classic problem , they need to be modified for certain configurations , and that random configurations might require an entirely new algorithm .

human motion primitive discovery and recognition
we present a novel framework for the automatic discovery and recognition of human motion primitives from motion capture data . human motion primitives are discovered by optimizing the 'motion flux ' , a quantity which depends on the motion of a group of skeletal joints . models of each primitive category are computed via non-parametric bayes methods and recognition is performed based on their geometric properties . a normalization of the primitives is proposed in order to make them invariant with respect to anatomical variations and data sampling rate . using our framework we build a publicly available dataset of human motion primitives based on motion capture sequences taken from well-known datasets . we expect that our framework , by providing an objective way for discovering and categorizing human motion , will be a useful tool in numerous research fields related to robotics including human inspired motion generation , learning by demonstration , and intuitive human-robot interaction .

proceedings of the second summer school on argumentation : computational and linguistic perspectives ( ssa'16 )
this volume contains the thesis abstracts presented at the second summer school on argumentation : computational and linguistic perspectives ( ssa'2016 ) held on september 8-12 in potsdam , germany .

an exact and two heuristic strategies for truthful bidding in combinatorial transport auctions
to support a freight carrier in a combinatorial transport auction , we proposes an exact and two heuristic strategies for bidding on subsets of requests . the exact bidding strategy is based on the concept of elementary request combinations . we show that it is sufficient and necessary for a carrier to bid on each elementary request combination in order to guarantee the same result as bidding on each element of the powerset of the set of tendered requests . both heuristic bidding strategies identify promising request combinations . for this , pairwise synergies based on saving values as well as the capacitated p-median problem are used . the bidding strategies are evaluated by a computational study that simulates an auction . it is based on 174 benchmark instances and therefore easily extendable by other researchers . on average , the two heuristic strategies achieve 91 percent and 81 percent of the available sales potential while generating 36 and only 4 percent of the bundle bids of the exact strategy . therefore , the proposed bidding strategies help a carrier to increase her chance to win and at the same time reduce the computational burden to participate in a combinatorial transport auction .

intuitive visualization of the intelligence for the run-down of terrorist wire-pullers
the investigation of the terrorist attack is a time-critical task . the investigators have a limited time window to diagnose the organizational background of the terrorists , to run down and arrest the wire-pullers , and to take an action to prevent or eradicate the terrorist attack . the intuitive interface to visualize the intelligence data set stimulates the investigators ' experience and knowledge , and aids them in decision-making for an immediately effective action . this paper presents a computational method to analyze the intelligence data set on the collective actions of the perpetrators of the attack , and to visualize it into the form of a social network diagram which predicts the positions where the wire-pullers conceals themselves .

learning to generalize to new compositions in image understanding
recurrent neural networks have recently been used for learning to describe images using natural language . however , it has been observed that these models generalize poorly to scenes that were not observed during training , possibly depending too strongly on the statistics of the text in the training data . here we propose to describe images using short structured representations , aiming to capture the crux of a description . these structured representations allow us to tease-out and evaluate separately two types of generalization : standard generalization to new images with similar scenes , and generalization to new combinations of known entities . we compare two learning approaches on the ms-coco dataset : a state-of-the-art recurrent network based on an lstm ( show , attend and tell ) , and a simple structured prediction model on top of a deep network . we find that the structured model generalizes to new compositions substantially better than the lstm , ~7 times the accuracy of predicting structured representations . by providing a concrete method to quantify generalization for unseen combinations , we argue that structured representations and compositional splits are a useful benchmark for image captioning , and advocate compositional models that capture linguistic and visual structure .

lifelong multi-agent path finding for online pickup and delivery tasks
the multi-agent path-finding ( mapf ) problem has recently received a lot of attention . however , it does not capture important characteristics of many real-world domains , such as automated warehouses , where agents are constantly engaged with new tasks . in this paper , we therefore study a lifelong version of the mapf problem , called the multi-agent pickup and delivery ( mapd ) problem . in the mapd problem , agents have to attend to a stream of delivery tasks in an online setting . one agent has to be assigned to each delivery task . this agent has to first move to a given pickup location and then to a given delivery location while avoiding collisions with other agents . we present two decoupled mapd algorithms , token passing ( tp ) and token passing with task swaps ( tpts ) . theoretically , we show that they solve all well-formed mapd instances , a realistic subclass of mapd instances . experimentally , we compare them against a centralized strawman mapd algorithm without this guarantee in a simulated warehouse system . tp can easily be extended to a fully distributed mapd algorithm and is the best choice when real-time computation is of primary concern since it remains efficient for mapd instances with hundreds of agents and tasks . tpts requires limited communication among agents and balances well between tp and the centralized mapd algorithm .

ultimate intelligence part ii : physical measure and complexity of intelligence
we continue our analysis of volume and energy measures that are appropriate for quantifying inductive inference systems . we extend logical depth and conceptual jump size measures in ait to stochastic problems , and physical measures that involve volume and energy . we introduce a graphical model of computational complexity that we believe to be appropriate for intelligent machines . we show several asymptotic relations between energy , logical depth and volume of computation for inductive inference . in particular , we arrive at a `` black-hole equation '' of inductive inference , which relates energy , volume , space , and algorithmic information for an optimal inductive inference solution . we introduce energy-bounded algorithmic entropy . we briefly apply our ideas to the physical limits of intelligent computation in our universe .

review-level sentiment classification with sentence-level polarity correction
we propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction . our polarity correction technique takes into account the consistency of the polarities ( positive and negative ) of sentences within each product review before performing the actual machine learning task . while sentences with inconsistent polarities are removed , sentences with consistent polarities are used to learn state-of-the-art classifiers . the technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique . experimental results show an average of 82 % f-measure on four different product review domains .

directed cycles in belief networks
the most difficult task in probabilistic reasoning may be handling directed cycles in belief networks . to the best knowledge of this author , there is no serious discussion of this problem at all in the literature of probabilistic reasoning so far .

totally corrective boosting for regularized risk minimization
consideration of the primal and dual problems together leads to important new insights into the characteristics of boosting algorithms . in this work , we propose a general framework that can be used to design new boosting algorithms . a wide variety of machine learning problems essentially minimize a regularized risk functional . we show that the proposed boosting framework , termed cgboost , can accommodate various loss functions and different regularizers in a totally-corrective optimization fashion . we show that , by solving the primal rather than the dual , a large body of totally-corrective boosting algorithms can actually be efficiently solved and no sophisticated convex optimization solvers are needed . we also demonstrate that some boosting algorithms like adaboost can be interpreted in our framework -- even their optimization is not totally corrective . we empirically show that various boosting algorithms based on the proposed framework perform similarly on the ucirvine machine learning datasets [ 1 ] that we have used in the experiments .

the application of a dendritic cell algorithm to a robotic classifier
the dendritic cell algorithm is an immune-inspired technique for processing time-dependant data . here we propose it as a possible solution for a robotic classification problem . the dendritic cell algorithm is implemented on a real robot and an investigation is performed into the effects of varying the migration threshold median for the cell population . the algorithm performs well on a classification task with very little tuning . ways of extending the implementation to allow it to be used as a classifier within the field of robotic security are suggested .

a minimalistic approach to sum-product network learning for real applications
sum-product networks ( spns ) are a class of expressive yet tractable hierarchical graphical models . learnspn is a structure learning algorithm for spns that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features . the original learnspn algorithm assumes that all the variables are discrete and there is no missing data . we introduce a practical , simplified version of learnspn , minispn , that runs faster and can handle missing data and heterogeneous features common in real applications . we demonstrate the performance of minispn on standard benchmark datasets and on two datasets from google 's knowledge graph exhibiting high missingness rates and a mix of discrete and continuous features .

comparative uncertainty , belief functions and accepted beliefs
this paper relates comparative belief structures and a general view of belief management in the setting of deductively closed logical representations of accepted beliefs . we show that the range of compatibility between the classical deductive closure and uncertain reasoning covers precisely the nonmonotonic 'preferential ' inference system of kraus , lehmann and magidor and nothing else . in terms of uncertain reasoning any possibility or necessity measure gives birth to a structure of accepted beliefs . the classes of probability functions and of shafer 's belief functions which yield belief sets prove to be very special ones .

building a fine-grained entity typing system overnight for a new x ( x = language , domain , genre )
recent research has shown great progress on fine-grained entity typing . most existing methods require pre-defining a set of types and training a multi-class classifier from a large labeled data set based on multi-level linguistic features . they are thus limited to certain domains , genres and languages . in this paper , we propose a novel unsupervised entity typing framework by combining symbolic and distributional semantics . we start from learning general embeddings for each entity mention , compose the embeddings of specific contexts using linguistic structures , link the mention to knowledge bases and learn its related knowledge representations . then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations . this framework does n't rely on any annotated data , predefined typing schema , or hand-crafted features , therefore it can be quickly adapted to a new domain , genre and language . furthermore , it has great flexibility at incorporating linguistic structures ( e.g. , abstract meaning representation ( amr ) , dependency relations ) to improve specific context representation . experiments on genres ( news and discussion forum ) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data . results on various languages ( english , chinese , japanese , hausa , and yoruba ) and domains ( general and biomedical ) demonstrate the portability of our framework .

a personalized system for conversational recommendations
searching for and making decisions about information is becoming increasingly difficult as the amount of information and number of choices increases . recommendation systems help users find items of interest of a particular type , such as movies or restaurants , but are still somewhat awkward to use . our solution is to take advantage of the complementary strengths of personalized recommendation systems and dialogue systems , creating personalized aides . we present a system -- the adaptive place advisor -- that treats item selection as an interactive , conversational process , with the program inquiring about item attributes and the user responding . individual , long-term user preferences are unobtrusively obtained in the course of normal recommendation dialogues and used to direct future conversations with the same user . we present a novel user model that influences both item search and the questions asked during a conversation . we demonstrate the effectiveness of our system in significantly reducing the time and number of interactions required to find a satisfactory item , as compared to a control group of users interacting with a non-adaptive version of the system .

ct-nor : representing and reasoning about events in continuous time
we present a generative model for representing and reasoning about the relationships among events in continuous time . we apply the model to the domain of networked and distributed computing environments where we fit the parameters of the model from timestamp observations , and then use hypothesis testing to discover dependencies between the events and changes in behavior for monitoring and diagnosis . after introducing the model , we present an em algorithm for fitting the parameters and then present the hypothesis testing approach for both dependence discovery and change-point detection . we validate the approach for both tasks using real data from a trace of network events at microsoft research cambridge . finally , we formalize the relationship between the proposed model and the noisy-or gate for cases when time can be discretized .

sentence object notation : multilingual sentence notation based on wordnet
the representation of sentences is a very important task . it can be used as a way to exchange data inter-applications . one main characteristic , that a notation must have , is a minimal size and a representative form . this can reduce the transfer time , and hopefully the processing time as well . usually , sentence representation is associated to the processed language . the grammar of this language affects how we represent the sentence . to avoid language-dependent notations , we have to come up with a new representation which do n't use words , but their meanings . this can be done using a lexicon like wordnet , instead of words we use their synsets . as for syntactic relations , they have to be universal as much as possible . our new notation is called ston `` sentences object notation '' , which somehow has similarities to json . it is meant to be minimal , representative and language-independent syntactic representation . also , we want it to be readable and easy to be created . this simplifies developing simple automatic generators and creating test banks manually . its benefit is to be used as a medium between different parts of applications like : text summarization , language translation , etc . the notation is based on 4 languages : arabic , english , franch and japanese ; and there are some cases where these languages do n't agree on one representation . also , given the diversity of grammatical structure of different world languages , this annotation may fail for some languages which allows more future improvements .

neural models for key phrase detection and question generation
we propose a two-stage neural model to tackle question generation from documents . our model first estimates the probability that word sequences in a document compose `` interesting '' answers using a neural model trained on a question-answering corpus . we thus take a data-driven approach to interestingness . predicted key phrases then act as target answers that condition a sequence-to-sequence question generation model with a copy mechanism . empirically , our neural key phrase detection model significantly outperforms an entity-tagging baseline system and existing rule-based approaches . we demonstrate that the question generator formulates good quality natural language questions from extracted key phrases , and a human study indicates that our system 's generated question-answer pairs are competitive with those of an earlier approach . we foresee our system being used in an educational setting to assess reading comprehension and also as a data augmentation technique for semi-supervised learning .

the information-theoretic and algorithmic approach to human , animal and artificial cognition
we survey concepts at the frontier of research connecting artificial , animal and human cognition to computation and information processing -- -from the turing test to searle 's chinese room argument , from integrated information theory to computational and algorithmic complexity . we start by arguing that passing the turing test is a trivial computational problem and that its pragmatic difficulty sheds light on the computational nature of the human mind more than it does on the challenge of artificial intelligence . we then review our proposed algorithmic information-theoretic measures for quantifying and characterizing cognition in various forms . these are capable of accounting for known biases in human behavior , thus vindicating a computational algorithmic view of cognition as first suggested by turing , but this time rooted in the concept of algorithmic probability , which in turn is based on computational universality while being independent of computational model , and which has the virtue of being predictive and testable as a model theory of cognitive behavior .

consid { é } rant la d { é } pendance dans la th { é } orie des fonctions de croyance
in this paper , we propose to learn sources independence in order to choose the appropriate type of combination rules when aggregating their beliefs . some combination rules are used with the assumption of their sources independence whereas others combine beliefs of dependent sources . therefore , the choice of the combination rule depends on the independence of sources involved in the combination . in this paper , we propose also a measure of independence , positive and negative dependence to integrate in mass functions before the combinaision with the independence assumption .

implementing probabilistic reasoning
general problems in analyzing information in a probabilistic database are considered . the practical difficulties ( and occasional advantages ) of storing uncertain data , of using it conventional forward- or backward-chaining inference engines , and of working with a probabilistic version of resolution are discussed . the background for this paper is the incorporation of uncertain reasoning facilities in mrs , a general-purpose expert system building tool .

evidential supplier selection based on interval data fusion
supplier selection is a typical multi-criteria decision making ( mcdm ) problem and lots of uncertain information exist inevitably . to address this issue , a new method was proposed based on interval data fusion . our method follows the original way to generate classical basic probability assignment ( bpa ) determined by the distance among the evidences . however , the weights of criteria are kept as interval numbers to generate interval bpas and do the fusion of interval bpas . finally , the order is ranked and the decision is made according to the obtained interval bpas . in this paper , a numerical example of supplier selection is applied to verify the feasibility and validity of our method . the new method is presented aiming at solving multiple-criteria decision-making problems in which the weights of criteria or experts are described in fuzzy data like linguistic terms or interval data .

inverse reinforcement learning with simultaneous estimation of rewards and dynamics
inverse reinforcement learning ( irl ) describes the problem of learning an unknown reward function of a markov decision process ( mdp ) from observed behavior of an agent . since the agent 's behavior originates in its policy and mdp policies depend on both the stochastic system dynamics as well as the reward function , the solution of the inverse problem is significantly influenced by both . current irl approaches assume that if the transition model is unknown , additional samples from the system 's dynamics are accessible , or the observed behavior provides enough samples of the system 's dynamics to solve the inverse problem accurately . these assumptions are often not satisfied . to overcome this , we present a gradient-based irl approach that simultaneously estimates the system 's dynamics . by solving the combined optimization problem , our approach takes into account the bias of the demonstrations , which stems from the generating policy . the evaluation on a synthetic mdp and a transfer learning task shows improvements regarding the sample efficiency as well as the accuracy of the estimated reward functions and transition models .

lossy source encoding via message-passing and decimation over generalized codewords of ldgm codes
we describe message-passing and decimation approaches for lossy source coding using low-density generator matrix ( ldgm ) codes . in particular , this paper addresses the problem of encoding a bernoulli ( 0.5 ) source : for randomly generated ldgm codes with suitably irregular degree distributions , our methods yield performance very close to the rate distortion limit over a range of rates . our approach is inspired by the survey propagation ( sp ) algorithm , originally developed by mezard et al . for solving random satisfiability problems . previous work by maneva et al . shows how sp can be understood as belief propagation ( bp ) for an alternative representation of satisfiability problems . in analogy to this connection , our approach is to define a family of markov random fields over generalized codewords , from which local message-passing rules can be derived in the standard way . the overall source encoding method is based on message-passing , setting a subset of bits to their preferred values ( decimation ) , and reducing the code .

a robust linguistic platform for efficient and domain specific web content analysis
web semantic access in specific domains calls for specialized search engines with enhanced semantic querying and indexing capacities , which pertain both to information retrieval ( ir ) and to information extraction ( ie ) . a rich linguistic analysis is required either to identify the relevant semantic units to index and weight them according to linguistic specific statistical distribution , or as the basis of an information extraction process . recent developments make natural language processing ( nlp ) techniques reliable enough to process large collections of documents and to enrich them with semantic annotations . this paper focuses on the design and the development of a text processing platform , ogmios , which has been developed in the alvis project . the ogmios platform exploits existing nlp modules and resources , which may be tuned to specific domains and produces linguistically annotated documents . we show how the three constraints of genericity , domain semantic awareness and performance can be handled all together .

the ethics of robotics
the three laws of robotics first appeared together in isaac asimov 's story 'runaround ' after being mentioned in some form or the other in previous works by asimov . these three laws commonly known as the three laws of robotics are the earliest forms of depiction for the needs of ethics in robotics . in simplistic language isaac asimov is able to explain what rules a robot must confine itself to in order to maintain societal sanctity . however , even though they are outdated they still represent some of our innate fears which are beginning to resurface in present day 21st century . our society is on the advent of a new revolution ; a revolution led by advances in computer science , artificial intelligence & nanotechnology . some of our advances have been so phenomenal that we surpassed what was predicted by the moore 's law . with these advancements comes the fear that our future may be at the mercy of these androids . humans today are scared that we , ourselves , might create something which we can not control . we may end up creating something which can not only learn much faster than anyone of us can , but also evolve faster than what the theory of evolution has allowed us to . the greatest fear is not only that we might lose our jobs to these intelligent beings , but that these beings might end up replacing us at the top of the cycle . the public hysteria has been heightened more so by a number of cultural works which depict annihilation of the human race by robots . right from frankenstein to i , robot mass media has also depicted such issues . this paper is an effort to understand the need for ethics in robotics or simply termed as roboethics . this is achieved by the study of artificial beings and the thought being put behind them . by the end of the paper , however , it is concluded that there is n't a need for ethical robots but more so ever a need for ethical roboticists .

similarity-based models of word cooccurrence probabilities
in many applications of natural language processing ( nlp ) it is necessary to determine the likelihood of a given word combination . for example , a speech recognizer may need to determine which of the two word combinations `` eat a peach '' and `` eat a beach '' is more likely . statistical nlp methods determine the likelihood of a word combination from its frequency in a training corpus . however , the nature of language is such that many word combinations are infrequent and do not occur in any given corpus . in this work we propose a method for estimating the probability of such previously unseen word combinations using available information on `` most similar '' words . we describe probabilistic word association models based on distributional word similarity , and apply them to two tasks , language modeling and pseudo-word disambiguation . in the language modeling task , a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model . the similarity-based method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error . we also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations . the similarity-based methods perform up to 40 % better on this particular task .

feature-based tuning of simulated annealing applied to the curriculum-based course timetabling problem
we consider the university course timetabling problem , which is one of the most studied problems in educational timetabling . in particular , we focus our attention on the formulation known as the curriculum-based course timetabling problem , which has been tackled by many researchers and for which there are many available benchmarks . the contribution of this paper is twofold . first , we propose an effective and robust single-stage simulated annealing method for solving the problem . secondly , we design and apply an extensive and statistically-principled methodology for the parameter tuning procedure . the outcome of this analysis is a methodology for modeling the relationship between search method parameters and instance features that allows us to set the parameters for unseen instances on the basis of a simple inspection of the instance itself . using this methodology , our algorithm , despite its apparent simplicity , has been able to achieve high quality results on a set of popular benchmarks . a final contribution of the paper is a novel set of real-world instances , which could be used as a benchmark for future comparison .

x.ent : r package for entities and relations extraction based on unsupervised learning and document structure
relation extraction with accurate precision is still a challenge when processing full text databases . we propose an approach based on cooccurrence analysis in each document for which we used document organization to improve accuracy of relation extraction . this approach is implemented in a r package called \emph { x.ent } . another facet of extraction relies on use of extracted relation into a querying system for expert end-users . two datasets had been used . one of them gets interest from specialists of epidemiology in plant health . for this dataset usage is dedicated to plant-disease exploration through agricultural information news . an open-data platform exploits exports from \emph { x.ent } and is publicly available .

automating computer bottleneck detection with belief nets
we describe an application of belief networks to the diagnosis of bottlenecks in computer systems . the technique relies on a high-level functional model of the interaction between application workloads , the windows nt operating system , and system hardware . given a workload description , the model predicts the values of observable system counters available from the windows nt performance monitoring tool . uncertainty in workloads , predictions , and counter values are characterized with gaussian distributions . during diagnostic inference , we use observed performance monitor values to find the most probable assignment to the workload parameters . in this paper we provide some background on automated bottleneck detection , describe the structure of the system model , and discuss empirical procedures for model calibration and verification . part of the calibration process includes generating a dataset to estimate a multivariate gaussian error model . initial results in diagnosing bottlenecks are presented .

a network-based end-to-end trainable task-oriented dialogue system
teaching machines to accomplish tasks by conversing naturally with humans is challenging . currently , developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting , or acquiring costly labelled datasets to solve a statistical learning problem for each component . in this work we introduce a neural network-based text-in , text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined wizard-of-oz framework . this approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand . the results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain .

a comparison of learning algorithms on the arcade learning environment
reinforcement learning agents have traditionally been evaluated on small toy problems . with advances in computing power and the advent of the arcade learning environment , it is now possible to evaluate algorithms on diverse and difficult problems within a consistent framework . we discuss some challenges posed by the arcade learning environment which do not manifest in simpler environments . we then provide a comparison of model-free , linear learning algorithms on this challenging problem set .

does restraining end effect matter in emd-based modeling framework for time series prediction ? some experimental evidences
following the `` decomposition-and-ensemble '' principle , the empirical mode decomposition ( emd ) -based modeling framework has been widely used as a promising alternative for nonlinear and nonstationary time series modeling and prediction . the end effect , which occurs during the sifting process of emd and is apt to distort the decomposed sub-series and hurt the modeling process followed , however , has been ignored in previous studies . addressing the end effect issue , this study proposes to incorporate end condition methods into emd-based decomposition and ensemble modeling framework for one- and multi-step ahead time series prediction . four well-established end condition methods , mirror method , coughlin 's method , slope-based method , and rato 's method , are selected , and support vector regression ( svr ) is employed as the modeling technique . for the purpose of justification and comparison , well-known nn3 competition data sets are used and four well-established prediction models are selected as benchmarks . the experimental results demonstrated that significant improvement can be achieved by the proposed emd-based svr models with end condition methods . the emd-sbm-svr model and emd-rato-svr model , in particular , achieved the best prediction performances in terms of goodness of forecast measures and equality of accuracy of competing forecasts test .

graph aggregation
graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs , each provided by a different source . one needs to perform graph aggregation in a wide variety of situations , e.g. , when applying a voting rule ( graphs as preference orders ) , when consolidating conflicting views regarding the relationships between arguments in a debate ( graphs as abstract argumentation frameworks ) , or when computing a consensus between several alternative clusterings of a given dataset ( graphs as equivalence relations ) . in this paper , we introduce a formal framework for graph aggregation grounded in social choice theory . our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule . we consider both common properties of graphs , such as transitivity and reflexivity , and arbitrary properties expressible in certain fragments of modal logic . our results establish several connections between the types of properties preserved under aggregation and the choice-theoretic axioms satisfied by the rules used . the most important of these results is a powerful impossibility theorem that generalises arrow 's seminal result for the aggregation of preference orders to a large collection of different types of graphs .

adversarial message passing for graphical models
bayesian inference on structured models typically relies on the ability to infer posterior distributions of underlying hidden variables . however , inference in implicit models or complex posterior distributions is hard . a popular tool for learning implicit models are generative adversarial networks ( gans ) which learn parameters of generators by fooling discriminators . typically , gans are considered to be models themselves and are not understood in the context of inference . current techniques rely on inefficient global discrimination of joint distributions to perform learning , or only consider discriminating a single output variable . we overcome these limitations by treating gans as a basis for likelihood-free inference in generative models and generalize them to bayesian posterior inference over factor graphs . we propose local learning rules based on message passing minimizing a global divergence criterion involving cooperating local adversaries used to sidestep explicit likelihood evaluations . this allows us to compose models and yields a unified inference and learning framework for adversarial learning . our framework treats model specification and inference separately and facilitates richly structured models within the family of directed acyclic graphs , including components such as intractable likelihoods , non-differentiable models , simulators and generally cumbersome models . a key result of our treatment is the insight that bayesian inference on structured models can be performed only with sampling and discrimination when using nonparametric variational families , without access to explicit distributions . as a side-result , we discuss the link to likelihood maximization . these approaches hold promise to be useful in the toolbox of probabilistic modelers and enrich the gamut of current probabilistic programming applications .

maximum likelihood bounded tree-width markov networks
chow and liu ( 1968 ) studied the problem of learning a maximumlikelihood markov tree . we generalize their work to more complexmarkov networks by considering the problem of learning a maximumlikelihood markov network of bounded complexity . we discuss howtree-width is in many ways the appropriate measure of complexity andthus analyze the problem of learning a maximum likelihood markovnetwork of bounded tree-width.similar to the work of chow and liu , we are able to formalize thelearning problem as a combinatorial optimization problem on graphs . weshow that learning a maximum likelihood markov network of boundedtree-width is equivalent to finding a maximum weight hypertree . thisequivalence gives rise to global , integer-programming based , approximation algorithms with provable performance guarantees , for thelearning problem . this contrasts with heuristic local-searchalgorithms which were previously suggested ( e.g . by malvestuto 1991 ) .the equivalence also allows us to study the computational hardness ofthe learning problem . we show that learning a maximum likelihoodmarkov network of bounded tree-width is np-hard , and discuss thehardness of approximation .

attribute exploration of discrete temporal transitions
discrete temporal transitions occur in a variety of domains , but this work is mainly motivated by applications in molecular biology : explaining and analyzing observed transcriptome and proteome time series by literature and database knowledge . the starting point of a formal concept analysis model is presented . the objects of a formal context are states of the interesting entities , and the attributes are the variable properties defining the current state ( e.g . observed presence or absence of proteins ) . temporal transitions assign a relation to the objects , defined by deterministic or non-deterministic transition rules between sets of pre- and postconditions . this relation can be generalized to its transitive closure , i.e . states are related if one results from the other by a transition sequence of arbitrary length . the focus of the work is the adaptation of the attribute exploration algorithm to such a relational context , so that questions concerning temporal dependencies can be asked during the exploration process and be answered from the computed stem base . results are given for the abstract example of a game and a small gene regulatory network relevant to a biomedical question .

averaged hausdorff approximations of pareto fronts based on multiobjective estimation of distribution algorithms
in the a posteriori approach of multiobjective optimization the pareto front is approximated by a finite set of solutions in the objective space . the quality of the approximation can be measured by different indicators that take into account the approximation 's closeness to the pareto front and its distribution along the pareto front . in particular , the averaged hausdorff indicator prefers an almost uniform distribution . an observed drawback of multiobjective estimation of distribution algorithms ( medas ) is that - as common for randomized metaheuristics - the final population usually is not uniformly distributed along the pareto front . therefore , we propose a postprocessing strategy which consists of applying the averaged hausdorff indicator to the complete archive of generated solutions after optimization in order to select a uniformly distributed subset of nondominated solutions from the archive . in this paper , we put forward a strategy for extracting the above described subset . the effectiveness of the proposal is contrasted in a series of experiments that involve different medas and filtering techniques .

evaluation of a simple , scalable , parallel best-first search strategy
large-scale , parallel clusters composed of commodity processors are increasingly available , enabling the use of vast processing capabilities and distributed ram to solve hard search problems . we investigate hash-distributed a* ( hda* ) , a simple approach to parallel best-first search that asynchronously distributes and schedules work among processors based on a hash function of the search state . we use this approach to parallelize the a* algorithm in an optimal sequential version of the fast downward planner , as well as a 24-puzzle solver . the scaling behavior of hda* is evaluated experimentally on a shared memory , multicore machine with 8 cores , a cluster of commodity machines using up to 64 cores , and large-scale high-performance clusters , using up to 2400 processors . we show that this approach scales well , allowing the effective utilization of large amounts of distributed memory to optimally solve problems which require terabytes of ram . we also compare hda* to transposition-table driven scheduling ( tds ) , a hash-based parallelization of ida* , and show that , in planning , hda* significantly outperforms tds . a simple hybrid which combines hda* and tds to exploit strengths of both algorithms is proposed and evaluated .

notes on a model for fuzzy computing
in these notes we propose a setting for fuzzy computing in a framework similar to that of well-established theories of computation : boolean , and quantum computing . our efforts have been directed towards stressing the formal similarities : there is a common pattern underlying these three theories . we tried to conform our approach , as much as possible , to this pattern . this work was part of a project jointly with professor vittorio cafagna . professor cafagna passed away unexpectedly in 2007. his intellectual breadth and inspiring passion for mathematics is still very well alive .

pomdp-lite for robust robot planning under uncertainty
the partially observable markov decision process ( pomdp ) provides a principled general model for planning under uncertainty . however , solving a general pomdp is computationally intractable in the worst case . this paper introduces pomdp-lite , a subclass of pomdps in which the hidden state variables are constant or only change deterministically . we show that a pomdp-lite is equivalent to a set of fully observable markov decision processes indexed by a hidden parameter and is useful for modeling a variety of interesting robotic tasks . we develop a simple model-based bayesian reinforcement learning algorithm to solve pomdp-lite models . the algorithm performs well on large-scale pomdp-lite models with up to $ 10^ { 20 } $ states and outperforms the state-of-the-art general-purpose pomdp algorithms . we further show that the algorithm is near-bayesian-optimal under suitable conditions .

universal convergence of semimeasures on individual random sequences
solomonoff 's central result on induction is that the posterior of a universal semimeasure m converges rapidly and with probability 1 to the true sequence generating posterior mu , if the latter is computable . hence , m is eligible as a universal sequence predictor in case of unknown mu . despite some nearby results and proofs in the literature , the stronger result of convergence for all ( martin-loef ) random sequences remained open . such a convergence result would be particularly interesting and natural , since randomness can be defined in terms of m itself . we show that there are universal semimeasures m which do not converge for all random sequences , i.e . we give a partial negative answer to the open problem . we also provide a positive answer for some non-universal semimeasures . we define the incomputable measure d as a mixture over all computable measures and the enumerable semimeasure w as a mixture over all enumerable nearly-measures . we show that w converges to d and d to mu on all random sequences . the hellinger distance measuring closeness of two distributions plays a central role .

causal independence for knowledge acquisition and inference
i introduce a temporal belief-network representation of causal independence that a knowledge engineer can use to elicit probabilistic models . like the current , atemporal belief-network representation of causal independence , the new representation makes knowledge acquisition tractable . unlike the atemproal representation , however , the temporal representation can simplify inference , and does not require the use of unobservable variables . the representation is less general than is the atemporal representation , but appears to be useful for many practical applications .

logical and inequality implications for reducing the size and complexity of quadratic unconstrained binary optimization problems
the quadratic unconstrained binary optimization ( qubo ) problem arises in diverse optimization applications ranging from ising spin problems to classical problems in graph theory and binary discrete optimization . the use of preprocessing to transform the graph representing the qubo problem into a smaller equivalent graph is important for improving solution quality and time for both exact and metaheuristic algorithms and is a step towards mapping large scale qubo to hardware graphs used in quantum annealing computers . in an earlier paper ( lewis and glover , 2016 ) a set of rules was introduced that achieved significant qubo reductions as verified through computational testing . here this work is extended with additional rules that provide further reductions that succeed in exactly solving 10 % of the benchmark qubo problems . an algorithm and associated data structures to efficiently implement the entire set of rules is detailed and computational experiments are reported that demonstrate their efficacy .

training probabilistic spiking neural networks with first-to-spike decoding
third-generation neural networks , or spiking neural networks ( snns ) , aim at harnessing the energy efficiency of spike-domain processing by building on computing elements that operate on , and exchange , spikes . in this paper , the problem of training a two-layer snn is studied for the purpose of classification , under a generalized linear model ( glm ) probabilistic neural model that was previously considered within the computational neuroscience literature . conventional classification rules for snns operate offline based on the number of output spikes at each output neuron . in contrast , a novel training method is proposed here for a first-to-spike decoding rule , whereby the snn can perform an early classification decision once spike firing is detected at an output neuron . numerical results bring insights into the optimal parameter selection for the glm neuron and on the accuracy-complexity trade-off performance of conventional and first-to-spike decoding .

predicting the behavior of interacting humans by fusing data from multiple sources
multi-fidelity methods combine inexpensive low-fidelity simulations with costly but high-fidelity simulations to produce an accurate model of a system of interest at minimal cost . they have proven useful in modeling physical systems and have been applied to engineering problems such as wing-design optimization . during human-in-the-loop experimentation , it has become increasingly common to use online platforms , like mechanical turk , to run low-fidelity experiments to gather human performance data in an efficient manner . one concern with these experiments is that the results obtained from the online environment generalize poorly to the actual domain of interest . to address this limitation , we extend traditional multi-fidelity approaches to allow us to combine fewer data points from high-fidelity human-in-the-loop experiments with plentiful but less accurate data from low-fidelity experiments to produce accurate models of how humans interact . we present both model-based and model-free methods , and summarize the predictive performance of each method under different conditions .

action schema networks : generalised policies with deep learning
in this paper , we introduce the action schema network ( asnet ) : a neural network architecture for learning generalised policies for probabilistic planning problems . by mimicking the relational structure of planning problems , asnets are able to adopt a weight-sharing scheme which allows the network to be applied to any problem from a given planning domain . this allows the cost of training the network to be amortised over all problems in that domain . further , we propose a training method which balances exploration and supervised training on small problems to produce a policy which remains robust when evaluated on larger problems . in experiments , we show that asnet 's learning capability allows it to significantly outperform traditional non-learning planners in several challenging domains .

label ranking with abstention : predicting partial orders by thresholding probability distributions ( extended abstract )
we consider an extension of the setting of label ranking , in which the learner is allowed to make predictions in the form of partial instead of total orders . predictions of that kind are interpreted as a partial abstention : if the learner is not sufficiently certain regarding the relative order of two alternatives , it may abstain from this decision and instead declare these alternatives as being incomparable . we propose a new method for learning to predict partial orders that improves on an existing approach , both theoretically and empirically . our method is based on the idea of thresholding the probabilities of pairwise preferences between labels as induced by a predicted ( parameterized ) probability distribution on the set of all rankings .

generalized totalizer encoding for pseudo-boolean constraints
pseudo-boolean constraints , also known as 0-1 integer linear constraints , are used to model many real-world problems . a common approach to solve these constraints is to encode them into a sat formula . the runtime of the sat solver on such formula is sensitive to the manner in which the given pseudo-boolean constraints are encoded . in this paper , we propose generalized totalizer encoding ( gte ) , which is an arc-consistency preserving extension of the totalizer encoding to pseudo-boolean constraints . unlike some other encodings , the number of auxiliary variables required for gte does not depend on the magnitudes of the coefficients . instead , it depends on the number of distinct combinations of these coefficients . we show the superiority of gte with respect to other encodings when large pseudo-boolean constraints have low number of distinct coefficients . our experimental results also show that gte remains competitive even when the pseudo-boolean constraints do not have this characteristic .

multi-agent coordination using nearest neighbor rules : revisiting the vicsek model
recently , jadbabaie , lin , and morse ( ieee tac , 48 ( 6 ) 2003:988-1001 ) offered a mathematical analysis of the discrete time model of groups of mobile autonomous agents raised by vicsek et al . in 1995. in their paper , jadbabaie et al . showed that all agents shall move in the same heading , provided that these agents are periodically linked together . this paper sharpens this result by showing that coordination will be reached under a very weak condition that requires all agents are finally linked together . this condition is also strictly weaker than the one jadbabaie et al . desired .

separators and adjustment sets in causal graphs : complete criteria and an algorithmic framework
principled reasoning about the identifiability of causal effects from non-experimental data is an important application of graphical causal models . we present an algorithmic framework for efficiently testing , constructing , and enumerating $ m $ -separators in ancestral graphs ( ags ) , a class of graphical causal models that can represent uncertainty about the presence of latent confounders . furthermore , we prove a reduction from causal effect identification by covariate adjustment to $ m $ -separation in a subgraph for directed acyclic graphs ( dags ) and maximal ancestral graphs ( mags ) . jointly , these results yield constructive criteria that characterize all adjustment sets as well as all minimal and minimum adjustment sets for identification of a desired causal effect with multivariate exposures and outcomes in the presence of latent confounding . our results extend several existing solutions for special cases of these problems . our efficient algorithms allowed us to empirically quantify the identifiability gap between covariate adjustment and the do-calculus in random dags , covering a wide range of scenarios . implementations of our algorithms are provided in the r package dagitty .

reducing validity in epistemic atl to validity in epistemic ctl
we propose a validity preserving translation from a subset of epistemic alternating-time temporal logic ( atl ) to epistemic computation tree logic ( ctl ) . the considered subset of epistemic atl is known to have the finite model property and decidable model-checking . this entails the decidability of validity but the implied algorithm is unfeasible . reducing the validity problem to that in a corresponding system of ctl makes the techniques for automated deduction for that logic available for the handling of the apparently more complex system of atl .

random logic programs : linear model
this paper proposes a model , the linear model , for randomly generating logic programs with low density of rules and investigates statistical properties of such random logic programs . it is mathematically shown that the average number of answer sets for a random program converges to a constant when the number of atoms approaches infinity . several experimental results are also reported , which justify the suitability of the linear model . it is also experimentally shown that , under this model , the size distribution of answer sets for random programs tends to a normal distribution when the number of atoms is sufficiently large .

learning bayesian networks with local structure
in this paper we examine a novel addition to the known methods for learning bayesian networks from data that improves the quality of the learned networks . our approach explicitly represents and learns the local structure in the conditional probability tables ( cpts ) , that quantify these networks . this increases the space of possible models , enabling the representation of cpts with a variable number of parameters that depends on the learned local structures . the resulting learning procedure is capable of inducing models that better emulate the real complexity of the interactions present in the data . we describe the theoretical foundations and practical aspects of learning local structures , as well as an empirical evaluation of the proposed method . this evaluation indicates that learning curves characterizing the procedure that exploits the local structure converge faster than these of the standard procedure . our results also show that networks learned with local structure tend to be more complex ( in terms of arcs ) , yet require less parameters .

learning from scarce experience
searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems . typically , with each change of the target policy , its value is estimated from the results of following that very policy . this requires a large number of interactions with the environment as different polices are considered . we present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy ( or collection of policies ) to estimate the value of a different policy . the algorithms combine estimation and optimization stages . the former utilizes experience to build a non-parametric representation of an optimized function . the latter performs optimization on this estimate . we show positive empirical results and provide the sample complexity bound .

the new ai : general & sound & relevant for physics
most traditional artificial intelligence ( ai ) systems of the past 50 years are either very limited , or based on heuristics , or both . the new millennium , however , has brought substantial progress in the field of theoretically optimal and practically feasible algorithms for prediction , search , inductive inference based on occam 's razor , problem solving , decision making , and reinforcement learning in environments of a very general type . since inductive inference is at the heart of all inductive sciences , some of the results are relevant not only for ai and computer science but also for physics , provoking nontraditional predictions based on zuse 's thesis of the computer-generated universe .

a survey on artificial intelligence and data mining for moocs
massive open online courses ( moocs ) have gained tremendous popularity in the last few years . thanks to moocs , millions of learners from all over the world have taken thousands of high-quality courses for free . putting together an excellent mooc ecosystem is a multidisciplinary endeavour that requires contributions from many different fields . artificial intelligence ( ai ) and data mining ( dm ) are two such fields that have played a significant role in making moocs what they are today . by exploiting the vast amount of data generated by learners engaging in moocs , dm improves our understanding of the mooc ecosystem and enables mooc practitioners to deliver better courses . similarly , ai , supported by dm , can greatly improve student experience and learning outcomes . in this survey paper , we first review the state-of-the-art artificial intelligence and data mining research applied to moocs , emphasising the use of ai and dm tools and techniques to improve student engagement , learning outcomes , and our understanding of the mooc ecosystem . we then offer an overview of key trends and important research to carry out in the fields of ai and dm so that moocs can reach their full potential .

solving graph coloring problems with abstraction and symmetry
this paper introduces a general methodology , based on abstraction and symmetry , that applies to solve hard graph edge-coloring problems and demonstrates its use to provide further evidence that the ramsey number $ r ( 4,3,3 ) =30 $ . the number $ r ( 4,3,3 ) $ is often presented as the unknown ramsey number with the best chances of being found `` soon '' . yet , its precise value has remained unknown for more than 50 years . we illustrate our approach by showing that : ( 1 ) there are precisely 78 { , } 892 $ ( 3,3,3 ; 13 ) $ ramsey colorings ; and ( 2 ) if there exists a $ ( 4,3,3 ; 30 ) $ ramsey coloring then it is ( 13,8,8 ) regular . specifically each node has 13 edges in the first color , 8 in the second , and 8 in the third . we conjecture that these two results will help provide a proof that no $ ( 4,3,3 ; 30 ) $ ramsey coloring exists implying that $ r ( 4,3,3 ) =30 $ .

deep visual foresight for planning robot motion
a key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision , so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback . model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions , which could provide flexible predictive models for a wide range of tasks and environments , without detailed human supervision . we develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data . our approach does not require a calibrated camera , an instrumented training set-up , nor precise sensing and actuation . our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training .

robot language learning , generation , and comprehension
we present a unified framework which supports grounding natural-language semantics in robotic driving . this framework supports acquisition ( learning grounded meanings of nouns and prepositions from human annotation of robotic driving paths ) , generation ( using such acquired meanings to generate sentential description of new robotic driving paths ) , and comprehension ( using such acquired meanings to support automated driving to accomplish navigational goals specified in natural language ) . we evaluate the performance of these three tasks by having independent human judges rate the semantic fidelity of the sentences associated with paths , achieving overall average correctness of 94.6 % and overall average completeness of 85.6 % .

symbol grounding via chaining of morphisms
a new model of symbol grounding is presented , in which the structures of natural language , logical semantics , perception and action are represented categorically , and symbol grounding is modeled via the composition of morphisms between the relevant categories . this model gives conceptual insight into the fundamentally systematic nature of symbol grounding , and also connects naturally to practical real-world ai systems in current research and commercial use . specifically , it is argued that the structure of linguistic syntax can be modeled as a certain asymmetric monoidal category , as e.g . implicit in the link grammar formalism ; the structure of spatiotemporal relationships and action plans can be modeled similarly using `` image grammars '' and `` action grammars '' ; and common-sense logical semantic structure can be modeled using dependently-typed lambda calculus with uncertain truth values . given these formalisms , the grounding of linguistic descriptions in spatiotemporal perceptions and coordinated actions consists of following morphisms from language to logic through to spacetime and body ( for comprehension ) , and vice versa ( for generation ) . the mapping is indicated between the spatial relationships in the region connection calculus and allen interval algebra and corresponding entries in the link grammar syntax parsing dictionary . further , the abstractions introduced here are shown to naturally model the structures and systems currently being deployed in the context of using the opencog cognitive architecture to control hanson robotics humanoid robots .

the complexity of causality and responsibility for query answers and non-answers
an answer to a query has a well-defined lineage expression ( alternatively called how-provenance ) that explains how the answer was derived . recent work has also shown how to compute the lineage of a non-answer to a query . however , the cause of an answer or non-answer is a more subtle notion and consists , in general , of only a fragment of the lineage . in this paper , we adapt halpern , pearl , and chockler 's recent definitions of causality and responsibility to define the causes of answers and non-answers to queries , and their degree of responsibility . responsibility captures the notion of degree of causality and serves to rank potentially many causes by their relative contributions to the effect . then , we study the complexity of computing causes and responsibilities for conjunctive queries . it is known that computing causes is np-complete in general . our first main result shows that all causes to conjunctive queries can be computed by a relational query which may involve negation . thus , causality can be computed in ptime , and very efficiently so . next , we study computing responsibility . here , we prove that the complexity depends on the conjunctive query and demonstrate a dichotomy between ptime and np-complete cases . for the ptime cases , we give a non-trivial algorithm , consisting of a reduction to the max-flow computation problem . finally , we prove that , even when it is in ptime , responsibility is complete for logspace , implying that , unlike causality , it can not be computed by a relational query .

a connection between generative adversarial networks , inverse reinforcement learning , and energy-based models
generative adversarial networks ( gans ) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator . while the idea of learning cost functions is relatively new to the field of generative modeling , learning costs has long been studied in control and reinforcement learning ( rl ) domains , typically for imitation learning from demonstrations . in these fields , learning cost function underlying observed behavior is known as inverse reinforcement learning ( irl ) or inverse optimal control . while at first the connection between cost learning in rl and cost learning in generative modeling may appear to be a superficial one , we show in this paper that certain irl methods are in fact mathematically equivalent to gans . in particular , we demonstrate an equivalence between a sample-based algorithm for maximum entropy irl and a gan in which the generator 's density can be evaluated and is provided as an additional input to the discriminator . interestingly , maximum entropy irl is a special case of an energy-based model . we discuss the interpretation of gans as an algorithm for training energy-based models , and relate this interpretation to other recent work that seeks to connect gans and ebms . by formally highlighting the connection between gans , irl , and ebms , we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another , particularly for developing more stable and scalable algorithms : a major challenge in all three domains .

cascading a* : a parallel approach to approximate heuristic search
in this paper , we proposed a new approximate heuristic search algorithm : cascading a* , which is a two-phrase algorithm combining a* and ida* by a new concept `` envelope ball '' . the new algorithm ca* is efficient , able to generate approximate solution and any-time solution , and parallel friendly .

finite sample analyses for td ( 0 ) with function approximation
td ( 0 ) is one of the most commonly used algorithms in reinforcement learning . despite this , there is no existing finite sample analysis for td ( 0 ) with function approximation , even for the linear case . our work is the first to provide such results . existing convergence rates for temporal difference ( td ) methods apply only to somewhat modified versions , e.g. , projected variants or ones where stepsizes depend on unknown problem parameters . our analyses obviate these artificial alterations by exploiting strong properties of td ( 0 ) . we provide convergence rates both in expectation and with high-probability . the two are obtained via different approaches that use relatively unknown , recently developed stochastic approximation techniques .

machine learning with operational costs
this work proposes a way to align statistical modeling with decision making . we provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost , where operational cost is the amount spent by the practitioner in solving the problem . the method allows us to explore the range of operational costs associated with the set of reasonable statistical models , so as to provide a useful way for practitioners to understand uncertainty . to do this , the operational cost is cast as a regularization term in a learning algorithm 's objective function , allowing either an optimistic or pessimistic view of possible costs , depending on the regularization parameter . from another perspective , if we have prior knowledge about the operational cost , for instance that it should be low , this knowledge can help to restrict the hypothesis space , and can help with generalization . we provide a theoretical generalization bound for this scenario . we also show that learning with operational costs is related to robust optimization .

knowledge graph embedding with iterative guidance from soft rules
embedding knowledge graphs ( kgs ) into continuous vector spaces is a focus of current research . combining such an embedding model with logic rules has recently attracted increasing attention . most previous attempts made a one-time injection of logic rules , ignoring the interactive nature between embedding learning and logical inference . and they focused only on hard rules , which always hold with no exception and usually require extensive manual effort to create or validate . in this paper , we propose rule-guided embedding ( ruge ) , a novel paradigm of kg embedding with iterative guidance from soft rules . ruge enables an embedding model to learn simultaneously from 1 ) labeled triples that have been directly observed in a given kg , 2 ) unlabeled triples whose labels are going to be predicted iteratively , and 3 ) soft rules with various confidence levels extracted automatically from the kg . in the learning process , ruge iteratively queries rules to obtain soft labels for unlabeled triples , and integrates such newly labeled triples to update the embedding model . through this iterative procedure , knowledge embodied in logic rules may be better transferred into the learned embeddings . we evaluate ruge in link prediction on freebase and yago . experimental results show that : 1 ) with rule knowledge injected iteratively , ruge achieves significant and consistent improvements over state-of-the-art baselines ; and 2 ) despite their uncertainties , automatically extracted soft rules are highly beneficial to kg embedding , even those with moderate confidence levels . the code and data used for this paper can be obtained from https : //github.com/iieir-km/ruge .

computational theories of curiosity-driven learning
what are the functions of curiosity ? what are the mechanisms of curiosity-driven learning ? we approach these questions using concepts and tools from machine learning and developmental robotics . we argue that curiosity-driven learning enables organisms to make discoveries to solve complex problems with rare or deceptive rewards . by fostering exploration and discovery of a diversity of behavioural skills , and ignoring these rewards , curiosity can be efficient to bootstrap learning when there is no information , or deceptive information , about local improvement towards these problems . we review both normative and heuristic computational frameworks used to understand the mechanisms of curiosity in humans , conceptualizing the child as a sense-making organism . these frameworks enable us to discuss the bi-directional causal links between curiosity and learning , and to provide new hypotheses about the fundamental role of curiosity in self-organizing developmental structures through curriculum learning . we present various developmental robotics experiments that study these mechanisms in action , both supporting these hypotheses and opening new research avenues in machine learning and artificial intelligence . finally , we discuss challenges for the design of experimental paradigms for studying curiosity in psychology and cognitive neuroscience . keywords : curiosity , intrinsic motivation , lifelong learning , predictions , world model , rewards , free-energy principle , learning progress , machine learning , ai , developmental robotics , development , curriculum learning , self-organization .

deep learning reconstruction for 9-view dual energy ct baggage scanner
for homeland and transportation security applications , 2d x-ray explosive detection system ( eds ) have been widely used , but they have limitations in recognizing 3d shape of the hidden objects . among various types of 3d computed tomography ( ct ) systems to address this issue , this paper is interested in a stationary ct using fixed x-ray sources and detectors . however , due to the limited number of projection views , analytic reconstruction algorithms produce severe streaking artifacts . inspired by recent success of deep learning approach for sparse view ct reconstruction , here we propose a novel image and sinogram domain deep learning architecture for 3d reconstruction from very sparse view measurement . the algorithm has been tested with the real data from a prototype 9-view dual energy stationary ct eds carry-on baggage scanner developed by gemss medical systems , korea , which confirms the superior reconstruction performance over the existing approaches .

efficient computation of exact irv margins
the margin of victory is easy to compute for many election schemes but difficult for instant runoff voting ( irv ) . this is important because arguments about the correctness of an election outcome usually rely on the size of the electoral margin . for example , risk-limiting audits require a knowledge of the margin of victory in order to determine how much auditing is necessary . this paper presents a practical branch-and-bound algorithm for exact irv margin computation that substantially improves on the current best-known approach . although exponential in the worst case , our algorithm runs efficiently in practice on all the real examples we could find . we can efficiently discover exact margins on election instances that can not be solved by the current state-of-the-art .

the alldifferent constraint with precedences
we propose alldiffprecedence , a new global constraint that combines together an alldifferent constraint with precedence constraints that strictly order given pairs of variables . we identify a number of applications for this global constraint including instruction scheduling and symmetry breaking . we give an efficient propagation algorithm that enforces bounds consistency on this global constraint . we show how to implement this propagator using a decomposition that extends the bounds consistency enforcing decomposition proposed for the alldifferent constraint . finally , we prove that enforcing domain consistency on this global constraint is np-hard in general .

sensitivity and generalization in neural networks : an empirical study
in practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts , an observation that appears to conflict with classical notions of function complexity , which typically favor smaller models . in this work , we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations . our experiments survey thousands of models with various fully-connected architectures , optimizers , and other hyper-parameters , as well as four different image classification datasets . we find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold , as measured by the norm of the input-output jacobian of the network , and that it correlates well with generalization . we further establish that factors associated with poor generalization $ - $ such as full-batch training or using random labels $ - $ correspond to lower robustness , while factors associated with good generalization $ - $ such as data augmentation and relu non-linearities $ - $ give rise to more robust functions . finally , we demonstrate how the input-output jacobian norm can be predictive of generalization at the level of individual test points .

random-key cuckoo search for the travelling salesman problem
combinatorial optimization problems are typically np-hard , and thus very challenging to solve . in this paper , we present the random key cuckoo search ( rkcs ) algorithm for solving the famous travelling salesman problem ( tsp ) . we used a simplified random-key encoding scheme to pass from a continuous space ( real numbers ) to a combinatorial space . we also consider the displacement of a solution in both spaces using levy flights . the performance of the proposed rkcs is tested against a set of benchmarks of symmetric tsp from the well-known tsplib library . the results of the tests show that rkcs is superior to some other metaheuristic algorithms .

simulation approaches to general probabilistic inference on belief networks
a number of algorithms have been developed to solve probabilistic inference problems on belief networks . these algorithms can be divided into two main groups : exact techniques which exploit the conditional independence revealed when the graph structure is relatively sparse , and probabilistic sampling techniques which exploit the `` conductance '' of an embedded markov chain when the conditional probabilities have non-extreme values . in this paper , we investigate a family of `` forward '' monte carlo sampling techniques similar to logic sampling [ henrion , 1988 ] which appear to perform well even in some multiply connected networks with extreme conditional probabilities , and thus would be generally applicable . we consider several enhancements which reduce the posterior variance using this approach and propose a framework and criteria for choosing when to use those enhancements .

effective multi-robot spatial task allocation using model approximations
real-world multi-agent planning problems can not be solved using decision-theoretic planning methods due to the exponential complexity . we approximate firefighting in rescue simulation as a spatially distributed task and model with multi-agent markov decision process . we use recent approximation methods for spatial task problems to reduce the model complexity . our approximations are single-agent , static task , shortest path pruning , dynamic planning horizon , and task clustering . we create scenarios from robocup rescue simulation maps and evaluate our methods on these graph worlds . the results show that our approach is faster and better than comparable methods and has negligible performance loss compared to the optimal policy . we also show that our method has a similar performance as dcop methods on example rcrs scenarios .

markov decision processes with continuous side information
we consider a reinforcement learning ( rl ) setting in which the agent interacts with a sequence of episodic mdps . at the start of each episode the agent has access to some side-information or context that determines the dynamics of the mdp for that episode . our setting is motivated by applications in healthcare where baseline measurements of a patient at the start of a treatment episode form the context that may provide information about how the patient might respond to treatment decisions . we propose algorithms for learning in such contextual markov decision processes ( cmdps ) under an assumption that the unobserved mdp parameters vary smoothly with the observed context . we also give lower and upper pac bounds under the smoothness assumption . because our lower bound has an exponential dependence on the dimension , we consider a tractable linear setting where the context is used to create linear combinations of a finite set of mdps . for the linear setting , we give a pac learning algorithm based on kwik learning techniques .

