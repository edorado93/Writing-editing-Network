{"title": "variational bayes for merging noisy databases", "abstract": "bayesian entity resolution merges together multiple , noisy databases and returns the minimal collection of unique individuals represented , together with their true , latent record values . bayesian methods allow flexible generative models that share power across databases as well as principled quantification of uncertainty for queries of the final , resolved database . however , existing bayesian methods for entity resolution use markov monte carlo method ( mcmc ) approximations and are too slow to run on modern databases containing millions or billions of records . instead , we propose applying variational approximations to allow scalable bayesian inference in these models . we derive a coordinate-ascent approximation for mean-field variational bayes , qualitatively compare our algorithm to existing methods , note unique challenges for inference that arise from the expected distribution of cluster sizes in entity resolution , and discuss directions for future work in this domain .", "topics": ["generative model", "calculus of variations"]}
{"title": "non-markovian control with gated end-to-end memory policy networks", "abstract": "partially observable environments present an important open challenge in the domain of sequential control learning with delayed rewards . despite numerous attempts during the two last decades , the majority of reinforcement learning algorithms and associated approximate models , applied to this context , still assume markovian state transitions . in this paper , we explore the use of a recently proposed attention-based model , the gated end-to-end memory network , for sequential control . we call the resulting model the gated end-to-end memory policy network . more precisely , we use a model-free value-based algorithm to learn policies for partially observed domains using this memory-enhanced neural network . this model is end-to-end learnable and it features unbounded memory . indeed , because of its attention mechanism and associated non-parametric memory , the proposed model allows us to define an attention mechanism over the observation stream unlike recurrent models . we show encouraging results that illustrate the capability of our attention-based model in the context of the continuous-state non-stationary control problem of stock trading . we also present an openai gym environment for simulated stock exchange and explain its relevance as a benchmark for the field of non-markovian decision process learning .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "maximum margin principal components", "abstract": "principal component analysis ( pca ) is a very successful dimensionality reduction technique , widely used in predictive modeling . a key factor in its widespread use in this domain is the fact that the projection of a dataset onto its first $ k $ principal components minimizes the sum of squared errors between the original data and the projected data over all possible rank $ k $ projections . thus , pca provides optimal low-rank representations of data for least-squares linear regression under standard modeling assumptions . on the other hand , when the loss function for a prediction problem is not the least-squares error , pca is typically a heuristic choice of dimensionality reduction -- in particular for classification problems under the zero-one loss . in this paper we target classification problems by proposing a straightforward alternative to pca that aims to minimize the difference in margin distribution between the original and the projected data . extensive experiments show that our simple approach typically outperforms pca on any particular dataset , in terms of classification error , though this difference is not always statistically significant , and despite being a filter method is frequently competitive with partial least squares ( pls ) and lasso on a wide range of datasets .", "topics": ["loss function", "heuristic"]}
{"title": "test set selection using active information acquisition for predictive models", "abstract": "in this paper , we consider active information acquisition when the prediction model is meant to be applied on a targeted subset of the population . the goal is to label a pre-specified fraction of customers in the target or test set by iteratively querying for information from the non-target or training set . the number of queries is limited by an overall budget . arising in the context of two rather disparate applications- banking and medical diagnosis , we pose the active information acquisition problem as a constrained optimization problem . we propose two greedy iterative algorithms for solving the above problem . we conduct experiments with synthetic data and compare results of our proposed algorithms with few other baseline approaches . the experimental results show that our proposed approaches perform better than the baseline schemes .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "learning compressed representations of blood samples time series with missing data", "abstract": "clinical measurements collected over time are naturally represented as multivariate time series ( mts ) , which often contain missing data . an autoencoder can learn low dimensional vectorial representations of mts that preserve important data characteristics , but can not deal explicitly with missing data . in this work , we propose a new framework that combines an autoencoder with the time series cluster kernel ( tck ) , a kernel that accounts for missingness patterns in mts . via kernel alignment , we incorporate tck in the autoencoder to improve the learned representations in presence of missing data . we consider a classification problem of mts with missing values , representing blood samples of patients with surgical site infection . with our approach , rather than with a standard autoencoder , we learn representations in low dimensions that can be classified better .", "topics": ["kernel ( operating system )", "time series"]}
{"title": "gauss quadrature for matrix inverse forms with applications", "abstract": "we present a framework for accelerating a spectrum of machine learning algorithms that require computation of bilinear inverse forms $ u^\\top a^ { -1 } u $ , where $ a $ is a positive definite matrix and $ u $ a given vector . our framework is built on gauss-type quadrature and easily scales to large , sparse matrices . further , it allows retrospective computation of lower and upper bounds on $ u^\\top a^ { -1 } u $ , which in turn accelerates several algorithms . we prove that these bounds tighten iteratively and converge at a linear ( geometric ) rate . to our knowledge , ours is the first work to demonstrate these key properties of gauss-type quadrature , which is a classical and deeply studied topic . we illustrate empirical consequences of our results by using quadrature to accelerate machine learning tasks involving determinantal point processes and submodular optimization , and observe tremendous speedups in several instances .", "topics": ["nonlinear system", "polynomial"]}
{"title": "smooth primal-dual coordinate descent algorithms for nonsmooth convex optimization", "abstract": "we propose a new randomized coordinate descent method for a convex optimization template with broad applications . our analysis relies on a novel combination of four ideas applied to the primal-dual gap function : smoothing , acceleration , homotopy , and coordinate descent with non-uniform sampling . as a result , our method features the first convergence rate guarantees among the coordinate descent methods , that are the best-known under a variety of common structure assumptions on the template . we provide numerical evidence to support the theoretical results with a comparison to state-of-the-art algorithms .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "multifaceted feature visualization : uncovering the different types of features learned by each neuron in deep neural networks", "abstract": "we can better understand deep neural networks by identifying which features each of their neurons have learned to detect . to do so , researchers have created deep visualization techniques including activation maximization , which synthetically generates inputs ( e.g . images ) that maximally activate each neuron . a limitation of current techniques is that they assume each neuron detects only one type of feature , but we know that neurons can be multifaceted , in that they fire in response to many different types of features : for example , a grocery store class neuron must activate either for rows of produce or for a storefront . previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron , creating inappropriate mixes of colors , parts of objects , scales , orientations , etc . here , we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron . we also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization . by separately synthesizing each type of image a neuron fires in response to , the visualizations have more appropriate colors and coherent global structure . multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron .", "topics": ["synthetic data", "neural networks"]}
{"title": "neighborhood mixture model for knowledge base completion", "abstract": "knowledge bases are useful resources for many natural language processing tasks , however , they are far from complete . in this paper , we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on transe-a well-known embedding model for knowledge base completion . experimental results show that the neighborhood information significantly helps to improve the results of the transe model , leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification , entity prediction and relation prediction tasks .", "topics": ["natural language processing"]}
{"title": "analysis of multibeam sonar data using dissimilarity representations", "abstract": "this paper considers the problem of low-dimensional visualisation of very high dimensional information sources for the purpose of situation awareness in the maritime environment . in response to the requirement for human decision support aids to reduce information overload ( and specifically , data amenable to inter-point relative similarity measures ) appropriate to the below-water maritime domain , we are investigating a preliminary prototype topographic visualisation model . the focus of the current paper is on the mathematical problem of exploiting a relative dissimilarity representation of signals in a visual informatics mapping model , driven by real-world sonar systems . an independent source model is used to analyse the sonar beams from which a simple probabilistic input model to represent uncertainty is mapped to a latent visualisation space where data uncertainty can be accommodated . the use of euclidean and non-euclidean measures are used and the motivation for future use of non-euclidean measures is made . concepts are illustrated using a simulated 64 beam weak snr dataset with realistic sonar targets .", "topics": ["simulation"]}
{"title": "decomposition bounds for marginal map", "abstract": "marginal map inference involves making map predictions in systems defined with latent variables or missing information . it is significantly more difficult than pure marginalization and map tasks , for which a large class of efficient and convergent variational algorithms , such as dual decomposition , exist . in this work , we generalize dual decomposition to a generic power sum inference task , which includes marginal map , along with pure marginalization and map , as special cases . our method is based on a block coordinate descent algorithm on a new convex decomposition bound , that is guaranteed to converge monotonically , and can be parallelized efficiently . we demonstrate our approach on marginal map queries defined on real-world problems from the uai approximate inference challenge , showing that our framework is faster and more reliable than previous methods .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "bag-of-words representation for biomedical time series classification", "abstract": "automatic analysis of biomedical time series such as electroencephalogram ( eeg ) and electrocardiographic ( ecg ) signals has attracted great interest in the community of biomedical engineering due to its important applications in medicine . in this work , a simple yet effective bag-of-words representation that is able to capture both local and global structure similarity information is proposed for biomedical time series representation . in particular , similar to the bag-of-words model used in text document domain , the proposed method treats a time series as a text document and extracts local segments from the time series as words . the biomedical time series is then represented as a histogram of codewords , each entry of which is the count of a codeword appeared in the time series . although the temporal order of the local segments is ignored , the bag-of-words representation is able to capture high-level structural information because both local and global structural information are well utilized . the performance of the bag-of-words model is validated on three datasets extracted from real eeg and ecg signals . the experimental results demonstrate that the proposed method is not only insensitive to parameters of the bag-of-words model such as local segment length and codebook size , but also robust to noise .", "topics": ["time series", "high- and low-level"]}
{"title": "semantic context forests for learning-based knee cartilage segmentation in 3d mr images", "abstract": "the automatic segmentation of human knee cartilage from 3d mr images is a useful yet challenging task due to the thin sheet structure of the cartilage with diffuse boundaries and inhomogeneous intensities . in this paper , we present an iterative multi-class learning method to segment the femoral , tibial and patellar cartilage simultaneously , which effectively exploits the spatial contextual constraints between bone and cartilage , and also between different cartilages . first , based on the fact that the cartilage grows in only certain area of the corresponding bone surface , we extract the distance features of not only to the surface of the bone , but more informatively , to the densely registered anatomical landmarks on the bone surface . second , we introduce a set of iterative discriminative classifiers that at each iteration , probability comparison features are constructed from the class confidence maps derived by previously learned classifiers . these features automatically embed the semantic context information between different cartilages of interest . validated on a total of 176 volumes from the osteoarthritis initiative ( oai ) dataset , the proposed approach demonstrates high robustness and accuracy of segmentation in comparison with existing state-of-the-art mr cartilage segmentation methods .", "topics": ["map", "iteration"]}
{"title": "symmetry learning for function approximation in reinforcement learning", "abstract": "in this paper we explore methods to exploit symmetries for ensuring sample efficiency in reinforcement learning ( rl ) , this problem deserves ever increasing attention with the recent advances in the use of deep networks for complex rl tasks which require large amount of training data . we introduce a novel method to detect symmetries using reward trails observed during episodic experience and prove its completeness . we also provide a framework to incorporate the discovered symmetries for functional approximation . finally we show that the use of potential based reward shaping is especially effective for our symmetry exploitation mechanism . experiments on various classical problems show that our method improves the learning performance significantly by utilizing symmetry information .", "topics": ["test set", "reinforcement learning"]}
{"title": "identifying and harnessing the building blocks of machine learning pipelines for sensible initialization of a data science automation tool", "abstract": "as data science continues to grow in popularity , there will be an increasing need to make data science tools more scalable , flexible , and accessible . in particular , automated machine learning ( automl ) systems seek to automate the process of designing and optimizing machine learning pipelines . in this chapter , we present a genetic programming-based automl system called tpot that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification problem . further , we analyze a large database of pipelines that were previously used to solve various supervised classification problems and identify 100 short series of machine learning operations that appear the most frequently , which we call the building blocks of machine learning pipelines . we harness these building blocks to initialize tpot with promising solutions , and find that this sensible initialization method significantly improves tpot 's performance on one benchmark at no cost of significantly degrading performance on the others . thus , sensible initialization with machine learning pipeline building blocks shows promise for gp-based automl systems , and should be further refined in future work .", "topics": ["supervised learning", "scalability"]}
{"title": "recovering the number of clusters in data sets with noise features using feature rescaling factors", "abstract": "in this paper we introduce three methods for re-scaling data sets aiming at improving the likelihood of clustering validity indexes to return the true number of spherical gaussian clusters with additional noise features . our method obtains feature re-scaling factors taking into account the structure of a given data set and the intuitive idea that different features may have different degrees of relevance at different clusters . we experiment with the silhouette ( using squared euclidean , manhattan , and the p $ ^ { th } $ power of the minkowski distance ) , dunn 's , calinski-harabasz and hartigan indexes on data sets with spherical gaussian clusters with and without noise features . we conclude that our methods indeed increase the chances of estimating the true number of clusters in a data set .", "topics": ["cluster analysis", "relevance"]}
{"title": "shaping level sets with submodular functions", "abstract": "we consider a class of sparsity-inducing regularization terms based on submodular functions . while previous work has focused on non-decreasing functions , we explore symmetric submodular functions and their \\lova extensions . we show that the lovasz extension may be seen as the convex envelope of a function that depends on level sets ( i.e . , the set of indices whose corresponding components of the underlying predictor are greater than a given constant ) : this leads to a class of convex structured regularization terms that impose prior knowledge on the level sets , and not only on the supports of the underlying predictors . we provide a unified set of optimization algorithms , such as proximal operators , and theoretical guarantees ( allowed level sets and recovery conditions ) . by selecting specific submodular functions , we give a new interpretation to known norms , such as the total variation ; we also define new norms , in particular ones that are based on order statistics with application to clustering and outlier detection , and on noisy cuts in graphs with application to change point detection in the presence of outliers .", "topics": ["cluster analysis", "matrix regularization"]}
{"title": "learning object arrangements in 3d scenes using human context", "abstract": "we consider the problem of learning object arrangements in a 3d scene . the key idea here is to learn how objects relate to human poses based on their affordances , ease of use and reachability . in contrast to modeling object-object relationships , modeling human-object relationships scales linearly in the number of objects . we design appropriate density functions based on 3d spatial features to capture this . we learn the distribution of human poses in a scene using a variant of the dirichlet process mixture model that allows sharing of the density function parameters across the same object types . then we can reason about arrangements of the objects in the room based on these meaningful human poses . in our extensive experiments on 20 different rooms with a total of 47 objects , our algorithm predicted correct placements with an average error of 1.6 meters from ground truth . in arranging five real scenes , it received a score of 4.3/5 compared to 3.7 for the best baseline method .", "topics": ["baseline ( configuration management )", "ground truth"]}
{"title": "conditional chow-liu tree structures for modeling discrete-valued vector time series", "abstract": "we consider the problem of modeling discrete-valued vector time series data using extensions of chow-liu tree models to capture both dependencies across time and dependencies across variables . conditional chow-liu tree models are introduced , as an extension to standard chow-liu trees , for modeling conditional rather than joint densities . we describe learning algorithms for such models and show how they can be used to learn parsimonious representations for the output distributions in hidden markov models . these models are applied to the important problem of simulating and forecasting daily precipitation occurrence for networks of rain stations . to demonstrate the effectiveness of the models , we compare their performance versus a number of alternatives using historical precipitation data from southwestern australia and the western united states . we illustrate how the structure and parameters of the models can be used to provide an improved meteorological interpretation of such data .", "topics": ["time series", "simulation"]}
{"title": "building pattern recognition applications with the spare library", "abstract": "this paper presents the spare c++ library , an open source software tool conceived to build pattern recognition and soft computing systems . the library follows the requirement of the generality : most of the implemented algorithms are able to process user-defined input data types transparently , such as labeled graphs and sequences of objects , as well as standard numeric vectors . here we present a high-level picture of the spare library characteristics , focusing instead on the specific practical possibility of constructing pattern recognition systems for different input data types . in particular , as a proof of concept , we discuss two application instances involving clustering of real-valued multidimensional sequences and classification of labeled graphs .", "topics": ["cluster analysis", "feature vector"]}
{"title": "end-to-end training for whole image breast cancer diagnosis using an all convolutional design", "abstract": "we develop an end-to-end training algorithm for whole-image breast cancer diagnosis based on mammograms . it requires lesion annotations only at the first stage of training . after that , a whole image classifier can be trained using only image level labels . this greatly reduced the reliance on lesion annotations . our approach is implemented using an all convolutional design that is simple yet provides superior performance in comparison with the previous methods . on ddsm , our best single-model achieves a per-image auc score of 0.88 and three-model averaging increases the score to 0.91 . on inbreast , our best single-model achieves a per-image auc score of 0.96 . using ddsm as benchmark , our models compare favorably with the current state-of-the-art . we also demonstrate that a whole image model trained on ddsm can be easily transferred to inbreast without using its lesion annotations and using only a small amount of training data . code and model availability : https : //github.com/lishen/end2end-all-conv", "topics": ["test set", "end-to-end principle"]}
{"title": "winning arguments : interaction dynamics and persuasion strategies in good-faith online discussions", "abstract": "changing someone 's opinion is arguably one of the most important challenges of social interaction . the underlying process proves difficult to study : it is hard to know how someone 's opinions are formed and whether and how someone 's views shift . fortunately , changemyview , an active community on reddit , provides a platform where users present their own opinions and reasoning , invite others to contest them , and acknowledge when the ensuing discussions change their original views . in this work , we study these interactions to understand the mechanisms behind persuasion . we find that persuasive arguments are characterized by interesting patterns of interaction dynamics , such as participant entry-order and degree of back-and-forth exchange . furthermore , by comparing similar counterarguments to the same opinion , we show that language factors play an essential role . in particular , the interplay between the language of the opinion holder and that of the counterargument provides highly predictive cues of persuasiveness . finally , since even in this favorable setting people may not be persuaded , we investigate the problem of determining whether someone 's opinion is susceptible to being changed at all . for this more difficult task , we show that stylistic choices in how the opinion is expressed carry predictive power .", "topics": ["interaction"]}
{"title": "estimating optimal active learning via model retraining improvement", "abstract": "a central question for active learning ( al ) is : `` what is the optimal selection ? '' defining optimality by classifier loss produces a new characterisation of optimal al behaviour , by treating expected loss reduction as a statistical target for estimation . this target forms the basis of model retraining improvement ( mri ) , a novel approach providing a statistical estimation framework for al . this framework is constructed to address the central question of al optimality , and to motivate the design of estimation algorithms . mri allows the exploration of optimal al behaviour , and the examination of al heuristics , showing precisely how they make sub-optimal selections . the abstract formulation of mri is used to provide a new guarantee for al , that an unbiased mri estimator should outperform random selection . this mri framework reveals intricate estimation issues that in turn motivate the construction of new statistical al algorithms . one new algorithm in particular performs strongly in a large-scale experimental study , compared to standard al methods . this competitive performance suggests that practical efforts to minimise estimation bias may be important for al applications .", "topics": ["mathematical optimization", "heuristic"]}
{"title": "syntax-directed variational autoencoder for structured data", "abstract": "deep generative models have been enjoying success in modeling continuous data . however it remains challenging to capture the representations for discrete structures with formal grammars and semantics , e.g . , computer programs and molecular structures . how to generate both syntactically and semantically correct data still remains largely an open problem . inspired by the theory of compiler where the syntax and semantics check is done via syntax-directed translation ( sdt ) , we propose a novel syntax-directed variational autoencoder ( sd-vae ) by introducing stochastic lazy attributes . this approach converts the offline sdt check into on-the-fly generated guidance for constraining the decoder . comparing to the state-of-the-art methods , our approach enforces constraints on the output space so that the output will be not only syntactically valid , but also semantically reasonable . we evaluate the proposed model with applications in programming language and molecules , including reconstruction and program/molecule optimization . the results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models , which is significantly better than current state-of-the-art approaches .", "topics": ["generative model", "calculus of variations"]}
{"title": "teaching categories to human learners with visual explanations", "abstract": "we study the problem of computer-assisted teaching with explanations . conventional approaches for machine teaching typically only provide feedback at the instance level e.g . , the category or label of the instance . however , it is intuitive that clear explanations from a knowledgeable teacher can significantly improve a student 's ability to learn a new concept . to address these existing limitations , we propose a teaching framework that provides interpretable explanations as feedback and models how the learner incorporates this additional information . in the case of images , we show that we can automatically generate explanations that highlight the parts of the image that are responsible for the class label . experiments on human learners illustrate that , on average , participants achieve better test set performance on challenging categorization tasks when taught with our interpretable approach compared to existing methods .", "topics": ["test set"]}
{"title": "incorporating copying mechanism in image captioning for learning novel objects", "abstract": "image captioning often requires a large set of training image-sentence pairs . in practice , however , acquiring sufficient training pairs is always expensive , making the recent captioning models limited in their ability to describe objects outside of training corpora ( i.e . , novel objects ) . in this paper , we present long short-term memory with copying mechanism ( lstm-c ) -- - a new architecture that incorporates copying into the convolutional neural networks ( cnn ) plus recurrent neural networks ( rnn ) image captioning framework , for describing novel objects in captions . specifically , freely available object recognition datasets are leveraged to develop classifiers for novel objects . our lstm-c then nicely integrates the standard word-by-word sentence generation by a decoder rnn with copying mechanism which may instead select words from novel objects at proper places in the output sentence . extensive experiments are conducted on both mscoco image captioning and imagenet datasets , demonstrating the ability of our proposed lstm-c architecture to describe novel objects . furthermore , superior results are reported when compared to state-of-the-art deep models .", "topics": ["recurrent neural network", "statistical classification"]}
{"title": "b-spline shape from motion & shading : an automatic free-form surface modeling for face reconstruction", "abstract": "recently , many methods have been proposed for face reconstruction from multiple images , most of which involve fundamental principles of shape from shading and structure from motion . however , a majority of the methods just generate discrete surface model of face . in this paper , b-spline shape from motion and shading ( bssfms ) is proposed to reconstruct continuous b-spline surface for multi-view face images , according to an assumption that shading and motion information in the images contain 1st- and 0th-order derivative of b-spline face respectively . face surface is expressed as a b-spline surface that can be reconstructed by optimizing b-spline control points . therefore , normals and 3d feature points computed from shading and motion of images respectively are used as the 1st- and 0th- order derivative information , to be jointly applied in optimizing the b-spline face . additionally , an imls ( iterative multi-least-square ) algorithm is proposed to handle the difficult control point optimization . furthermore , synthetic samples and lfw dataset are introduced and conducted to verify the proposed approach , and the experimental results demonstrate the effectiveness with different poses , illuminations , expressions etc . , even with wild images .", "topics": ["synthetic data"]}
{"title": "a richer theory of convex constrained optimization with reduced projections and improved rates", "abstract": "this paper focuses on convex constrained optimization problems , where the solution is subject to a convex inequality constraint . in particular , we aim at challenging problems for which both projection into the constrained domain and a linear optimization under the inequality constraint are time-consuming , which render both projected gradient methods and conditional gradient methods ( a.k.a . the frank-wolfe algorithm ) expensive . in this paper , we develop projection reduced optimization algorithms for both smooth and non-smooth optimization with improved convergence rates under a certain regularity condition of the constraint function . we first present a general theory of optimization with only one projection . its application to smooth optimization with only one projection yields $ o ( 1/\\epsilon ) $ iteration complexity , which improves over the $ o ( 1/\\epsilon^2 ) $ iteration complexity established before for non-smooth optimization and can be further reduced under strong convexity . then we introduce a local error bound condition and develop faster algorithms for non-strongly convex optimization at the price of a logarithmic number of projections . in particular , we achieve an iteration complexity of $ \\widetilde o ( 1/\\epsilon^ { 2 ( 1-\\theta ) } ) $ for non-smooth optimization and $ \\widetilde o ( 1/\\epsilon^ { 1-\\theta } ) $ for smooth optimization , where $ \\theta\\in ( 0,1 ] $ appearing the local error bound condition characterizes the functional local growth rate around the optimal solutions . novel applications in solving the constrained $ \\ell_1 $ minimization problem and a positive semi-definite constrained distance metric learning problem demonstrate that the proposed algorithms achieve significant speed-up compared with previous algorithms .", "topics": ["mathematical optimization", "iteration"]}
{"title": "on the challenges of learning with inference networks on sparse , high-dimensional data", "abstract": "we study parameter estimation in nonlinear factor analysis ( nfa ) where the generative model is parameterized by a deep neural network . recent work has focused on learning such models using inference ( or recognition ) networks ; we identify a crucial problem when modeling large , sparse , high-dimensional datasets -- underfitting . we study the extent of underfitting , highlighting that its severity increases with the sparsity of the data . we propose methods to tackle it via iterative optimization inspired by stochastic variational inference \\citep { hoffman2013stochastic } and improvements in the sparse data representation used for inference . the proposed techniques drastically improve the ability of these powerful models to fit sparse data , achieving state-of-the-art results on a benchmark text-count dataset and excellent results on the task of top-n recommendation .", "topics": ["generative model", "calculus of variations"]}
{"title": "efficient dictionary learning with sparseness-enforcing projections", "abstract": "learning dictionaries suitable for sparse coding instead of using engineered bases has proven effective in a variety of image processing tasks . this paper studies the optimization of dictionaries on image data where the representation is enforced to be explicitly sparse with respect to a smooth , normalized sparseness measure . this involves the computation of euclidean projections onto level sets of the sparseness measure . while previous algorithms for this optimization problem had at least quasi-linear time complexity , here the first algorithm with linear time complexity and constant space complexity is proposed . the key for this is the mathematically rigorous derivation of a characterization of the projection 's result based on a soft-shrinkage function . this theory is applied in an original algorithm called easy dictionary learning ( ezdl ) , which learns dictionaries with a simple and fast-to-compute hebbian-like learning rule . the new algorithm is efficient , expressive and particularly simple to implement . it is demonstrated that despite its simplicity , the proposed learning algorithm is able to generate a rich variety of dictionaries , in particular a topographic organization of atoms or separable atoms . further , the dictionaries are as expressive as those of benchmark learning algorithms in terms of the reproduction quality on entire images , and result in an equivalent denoising performance . ezdl learns approximately 30 % faster than the already very efficient online dictionary learning algorithm , and is therefore eligible for rapid data set analysis and problems with vast quantities of learning samples .", "topics": ["image processing", "optimization problem"]}
{"title": "on the estimation of initial conditions in kernel-based system identification", "abstract": "recent developments in system identification have brought attention to regularized kernel-based methods , where , adopting the recently introduced stable spline kernel , prior information on the unknown process is enforced . this reduces the variance of the estimates and thus makes kernel-based methods particularly attractive when few input-output data samples are available . in such cases however , the influence of the system initial conditions may have a significant impact on the output dynamics . in this paper , we specifically address this point . we propose three methods that deal with the estimation of initial conditions using different types of information . the methods consist in various mixed maximum likelihood -- a posteriori estimators which estimate the initial conditions and tune the hyperparameters characterizing the stable spline kernel . to solve the related optimization problems , we resort to the expectation-maximization method , showing that the solutions can be attained by iterating among simple update steps . numerical experiments show the advantages , in terms of accuracy in reconstructing the system impulse response , of the proposed strategies , compared to other kernel-based schemes not accounting for the effect initial conditions .", "topics": ["kernel ( operating system )", "numerical analysis"]}
{"title": "learning dynamic boltzmann machines with spike-timing dependent plasticity", "abstract": "we propose a particularly structured boltzmann machine , which we refer to as a dynamic boltzmann machine ( dybm ) , as a stochastic model of a multi-dimensional time-series . the dybm can have infinitely many layers of units but allows exact and efficient inference and learning when its parameters have a proposed structure . this proposed structure is motivated by postulates and observations , from biological neural networks , that the synaptic weight is strengthened or weakened , depending on the timing of spikes ( i.e . , spike-timing dependent plasticity or stdp ) . we show that the learning rule of updating the parameters of the dybm in the direction of maximizing the likelihood of given time-series can be interpreted as stdp with long term potentiation and long term depression . the learning rule has a guarantee of convergence and can be performed in a distributed matter ( i.e . , local in space ) with limited memory ( i.e . , local in time ) .", "topics": ["time series"]}
{"title": "learning item trees for probabilistic modelling of implicit feedback", "abstract": "user preferences for items can be inferred from either explicit feedback , such as item ratings , or implicit feedback , such as rental histories . research in collaborative filtering has concentrated on explicit feedback , resulting in the development of accurate and scalable models . however , since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback . we introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user 's item selection process . in the interests of scalability , we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data . we also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data .", "topics": ["scalability"]}
{"title": "learning the dimensionality of hidden variables", "abstract": "a serious problem in learning probabilistic models is the presence of hidden variables . these variables are not observed , yet interact with several of the observed variables . detecting hidden variables poses two problems : determining the relations to other variables in the model and determining the number of states of the hidden variable . in this paper , we address the latter problem in the context of bayesian networks . we describe an approach that utilizes a score-based agglomerative state-clustering . as we show , this approach allows us to efficiently evaluate models with a range of cardinalities for the hidden variable . we show how to extend this procedure to deal with multiple interacting hidden variables . we demonstrate the effectiveness of this approach by evaluating it on synthetic and real-life data . we show that our approach learns models with hidden variables that generalize better and have better structure than previous approaches .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "sgd with variance reduction beyond empirical risk minimization", "abstract": "we introduce a doubly stochastic proximal gradient algorithm for optimizing a finite average of smooth convex functions , whose gradients depend on numerically expensive expectations . our main motivation is the acceleration of the optimization of the regularized cox partial-likelihood ( the core model used in survival analysis ) , but our algorithm can be used in different settings as well . the proposed algorithm is doubly stochastic in the sense that gradient steps are done using stochastic gradient descent ( sgd ) with variance reduction , where the inner expectations are approximated by a monte-carlo markov-chain ( mcmc ) algorithm . we derive conditions on the mcmc number of iterations guaranteeing convergence , and obtain a linear rate of convergence under strong convexity and a sublinear rate without this assumption . we illustrate the fact that our algorithm improves the state-of-the-art solver for regularized cox partial-likelihood on several datasets from survival analysis .", "topics": ["numerical analysis", "gradient descent"]}
{"title": "on overfitting and asymptotic bias in batch reinforcement learning with partial observability", "abstract": "this paper stands in the context of reinforcement learning with partial observability and limited data . in this setting , we focus on the tradeoff between asymptotic bias ( suboptimality with unlimited data ) and overfitting ( additional suboptimality due to limited data ) , and theoretically show that while potentially increasing the asymptotic bias , a smaller state representation decreases the risk of overfitting . our analysis relies on expressing the quality of a state representation by bounding l1 error terms of the associated belief states . theoretical results are empirically illustrated when the state representation is a truncated history of observations . finally , we also discuss and empirically illustrate how using function approximators and adapting the discount factor may enhance the tradeoff between asymptotic bias and overfitting .", "topics": ["reinforcement learning"]}
{"title": "a supervised neural autoregressive topic model for simultaneous image classification and annotation", "abstract": "topic modeling based on latent dirichlet allocation ( lda ) has been a framework of choice to perform scene recognition and annotation . recently , a new type of topic model called the document neural autoregressive distribution estimator ( docnade ) was proposed and demonstrated state-of-the-art performance for document modeling . in this work , we show how to successfully apply and extend this model to the context of visual scene modeling . specifically , we propose supdocnade , a supervised extension of docnade , that increases the discriminative power of the hidden topic features by incorporating label information into the training objective of the model . we also describe how to leverage information about the spatial position of the visual words and how to embed additional image annotations , so as to simultaneously perform image classification and annotation . we test our model on the scene15 , labelme and uiuc-sports datasets and show that it compares favorably to other topic models such as the supervised variant of lda .", "topics": ["computer vision"]}
{"title": "from collective adaptive systems to human centric computation and back : spatial model checking for medical imaging", "abstract": "recent research on formal verification for collective adaptive systems ( cas ) pushed advancements in spatial and spatio-temporal model checking , and as a side result provided novel image analysis methodologies , rooted in logical methods for topological spaces . medical imaging ( mi ) is a field where such technologies show potential for ground-breaking innovation . in this position paper , we present a preliminary investigation centred on applications of spatial model checking to mi . the focus is shifted from pure logics to a mixture of logical , statistical and algorithmic approaches , driven by the logical nature intrinsic to the specification of the properties of interest in the field . as a result , novel operators are introduced , that could as well be brought back to the setting of cas .", "topics": ["computation"]}
{"title": "deep spatio-temporal residual networks for citywide crowd flows prediction", "abstract": "forecasting the flow of crowds is of great importance to traffic management and public safety , yet a very challenging task affected by many complex factors , such as inter-region traffic , events and weather . in this paper , we propose a deep-learning-based approach , called st-resnet , to collectively forecast the in-flow and out-flow of crowds in each and every region through a city . we design an end-to-end structure of st-resnet based on unique properties of spatio-temporal data . more specifically , we employ the framework of the residual neural networks to model the temporal closeness , period , and trend properties of the crowd traffic , respectively . for each property , we design a branch of residual convolutional units , each of which models the spatial properties of the crowd traffic . st-resnet learns to dynamically aggregate the output of the three residual neural networks based on data , assigning different weights to different branches and regions . the aggregation is further combined with external factors , such as weather and day of the week , to predict the final traffic of crowds in each and every region . we evaluate st-resnet based on two types of crowd flows in beijing and nyc , finding that its performance exceeds six well-know methods .", "topics": ["end-to-end principle"]}
{"title": "a basic recurrent neural network model", "abstract": "we present a model of a basic recurrent neural network ( or brnn ) that includes a separate linear term with a slightly `` stable '' fixed matrix to guarantee bounded solutions and fast dynamic response . we formulate a state space viewpoint and adapt the constrained optimization lagrange multiplier ( clm ) technique and the vector calculus of variations ( cov ) to derive the ( stochastic ) gradient descent . in this process , one avoids the commonly used re-application of the circular chain-rule and identifies the error back-propagation with the co-state backward dynamic equations . we assert that this brnn can successfully perform regression tracking of time-series . moreover , the `` vanishing and exploding '' gradients are explicitly quantified and explained through the co-state dynamics and the update laws . the adapted cov framework , in addition , can correctly and principally integrate new loss functions in the network on any variable and for varied goals , e.g . , for supervised learning on the outputs and unsupervised learning on the internal ( hidden ) states .", "topics": ["calculus of variations", "supervised learning"]}
{"title": "cnndroid : gpu-accelerated execution of trained deep convolutional neural networks on android", "abstract": "many mobile applications running on smartphones and wearable devices would potentially benefit from the accuracy and scalability of deep cnn-based machine learning algorithms . however , performance and energy consumption limitations make the execution of such computationally intensive algorithms on mobile devices prohibitive . we present a gpu-accelerated library , dubbed cnndroid , for execution of trained deep cnns on android-based mobile devices . empirical evaluations show that cnndroid achieves up to 60x speedup and 130x energy saving on current mobile devices . the cnndroid open source library is available for download at https : //github.com/encp/cnndroid", "topics": ["scalability"]}
{"title": "learning semantically coherent and reusable kernels in convolution neural nets for sentence classification", "abstract": "the state-of-the-art cnn models give good performance on sentence classification tasks . the purpose of this work is to empirically study desirable properties such as semantic coherence , attention mechanism and reusability of cnns in these tasks . semantically coherent kernels are preferable as they are a lot more interpretable for explaining the decision of the learned cnn model . we observe that the learned kernels do not have semantic coherence . motivated by this observation , we propose to learn kernels with semantic coherence using clustering scheme combined with word2vec representation and domain knowledge such as sentiwordnet . we suggest a technique to visualize attention mechanism of cnns for decision explanation purpose . reusable property enables kernels learned on one problem to be used in another problem . this helps in efficient learning as only a few additional domain specific filters may have to be learned . we demonstrate the efficacy of our core ideas of learning semantically coherent kernels and leveraging reusable kernels for efficient learning on several benchmark datasets . experimental results show the usefulness of our approach by achieving performance close to the state-of-the-art methods but with semantic and reusable properties .", "topics": ["cluster analysis", "convolution"]}
{"title": "sparse overlapping sets lasso for multitask learning and its application to fmri analysis", "abstract": "multitask learning can be effective when features useful in one task are also useful for other tasks , and the group lasso is a standard method for selecting a common subset of features . in this paper , we are interested in a less restrictive form of multitask learning , wherein ( 1 ) the available features can be organized into subsets according to a notion of similarity and ( 2 ) features useful in one task are similar , but not necessarily identical , to the features best suited for other tasks . the main contribution of this paper is a new procedure called sparse overlapping sets ( sos ) lasso , a convex optimization that automatically selects similar features for related learning tasks . error bounds are derived for soslasso and its consistency is established for squared error loss . in particular , soslasso is motivated by multi- subject fmri studies in which functional activity is classified using brain voxels as features . experiments with real and synthetic data demonstrate the advantages of soslasso compared to the lasso and group lasso .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "brain-inspired deep networks for image aesthetics assessment", "abstract": "image aesthetics assessment has been challenging due to its subjective nature . inspired by the scientific advances in the human visual perception and neuroaesthetics , we design brain-inspired deep networks ( bdn ) for this task . bdn first learns attributes through the parallel supervised pathways , on a variety of selected feature dimensions . a high-level synthesis network is trained to associate and transform those attributes into the overall aesthetics rating . we then extend bdn to predicting the distribution of human ratings , since aesthetics ratings are often subjective . another highlight is our first-of-its-kind study of label-preserving transformations in the context of aesthetics assessment , which leads to an effective data augmentation approach . experimental results on the ava dataset show that our biological inspired and task-specific bdn model gains significantly performance improvement , compared to other state-of-the-art models with the same or higher parameter capacity .", "topics": ["high- and low-level"]}
{"title": "learning human pose estimation features with convolutional networks", "abstract": "this paper introduces a new architecture for human pose estimation using a multi- layer convolutional network architecture and a modified learning technique that learns low-level features and higher-level weak spatial models . unconstrained human pose estimation is one of the hardest problems in computer vision , and our new architecture and learning schema shows significant improvement over the current state-of-the-art results . the main contribution of this paper is showing , for the first time , that a specific variation of deep learning is able to outperform all existing traditional architectures on this task . the paper also discusses several lessons learned while researching alternatives , most notably , that it is possible to learn strong low-level feature detectors on features that might even just cover a few pixels in the image . higher-level spatial models improve somewhat the overall result , but to a much lesser extent then expected . many researchers previously argued that the kinematic structure and top-down information is crucial for this domain , but with our purely bottom up , and weak spatial model , we could improve other more complicated architectures that currently produce the best results . this mirrors what many other researchers , like those in the speech recognition , object recognition , and other domains have experienced .", "topics": ["high- and low-level", "computer vision"]}
{"title": "accelerated mini-batch stochastic dual coordinate ascent", "abstract": "stochastic dual coordinate ascent ( sdca ) is an effective technique for solving regularized loss minimization problems in machine learning . this paper considers an extension of sdca under the mini-batch setting that is often used in practice . our main contribution is to introduce an accelerated mini-batch version of sdca and prove a fast convergence rate for this method . we discuss an implementation of our method over a parallel computing system , and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of \\cite { nesterov2007gradient } .", "topics": ["gradient descent", "gradient"]}
{"title": "towards using unlabeled data in a sparse-coding framework for human activity recognition", "abstract": "we propose a sparse-coding framework for activity recognition in ubiquitous and mobile computing that alleviates two fundamental problems of current supervised learning approaches . ( i ) it automatically derives a compact , sparse and meaningful feature representation of sensor data that does not rely on prior expert knowledge and generalizes extremely well across domain boundaries . ( ii ) it exploits unlabeled sample data for bootstrapping effective activity recognizers , i.e . , substantially reduces the amount of ground truth annotation required for model estimation . such unlabeled data is trivial to obtain , e.g . , through contemporary smartphones carried by users as they go about their everyday activities . based on the self-taught learning paradigm we automatically derive an over-complete set of basis vectors from unlabeled data that captures inherent patterns present within activity data . through projecting raw sensor data onto the feature space defined by such over-complete sets of basis vectors effective feature extraction is pursued . given these learned feature representations , classification backends are then trained using small amounts of labeled training data . we study the new approach in detail using two datasets which differ in terms of the recognition tasks and sensor modalities . primarily we focus on transportation mode analysis task , a popular task in mobile-phone based sensing . the sparse-coding framework significantly outperforms the state-of-the-art in supervised learning approaches . furthermore , we demonstrate the great practical potential of the new approach by successfully evaluating its generalization capabilities across both domain and sensor modalities by considering the popular opportunity dataset . our feature learning approach outperforms state-of-the-art approaches to analyzing activities in daily living .", "topics": ["feature learning", "test set"]}
{"title": "convergence analysis of gradient descent algorithms with proportional updates", "abstract": "the rise of deep learning in recent years has brought with it increasingly clever optimization methods to deal with complex , non-linear loss functions . these methods are often designed with convex optimization in mind , but have been shown to work well in practice even for the highly non-convex optimization associated with neural networks . however , one significant drawback of these methods when they are applied to deep learning is that the magnitude of the update step is sometimes disproportionate to the magnitude of the weights ( much smaller or larger ) , leading to training instabilities such as vanishing and exploding gradients . an idea to combat this issue is gradient descent with proportional updates . gradient descent with proportional updates was introduced in 2017 . it was independently developed by you et al ( layer-wise adaptive rate scaling ( lars ) algorithm ) and by abu-el-haija ( percentdelta algorithm ) . the basic idea of both of these algorithms is to make each step of the gradient descent proportional to the current weight norm and independent of the gradient magnitude . it is common in the context of new optimization methods to prove convergence or derive regret bounds under the assumption of lipschitz continuity and convexity . however , even though lars and percentdelta were shown to work well in practice , there is no theoretical analysis of the convergence properties of these algorithms . thus it is not clear if the idea of gradient descent with proportional updates is used in the optimal way , or if it could be improved by using a different norm or specific learning rate schedule , for example . moreover , it is not clear if these algorithms can be extended to other problems , besides neural networks . we attempt to answer these questions by establishing the theoretical analysis of gradient descent with proportional updates , and verifying this analysis with empirical examples .", "topics": ["nonlinear system", "gradient descent"]}
{"title": "coupling distributed and symbolic execution for natural language queries", "abstract": "building neural networks to query a knowledge base ( a table ) with natural language is an emerging research topic in deep learning . an executor for table querying typically requires multiple steps of execution because queries may have complicated structures . in previous studies , researchers have developed either fully distributed executors or symbolic executors for table querying . a distributed executor can be trained in an end-to-end fashion , but is weak in terms of execution efficiency and explicit interpretability . a symbolic executor is efficient in execution , but is very difficult to train especially at initial stages . in this paper , we propose to couple distributed and symbolic execution for natural language queries , where the symbolic executor is pretrained with the distributed executor 's intermediate execution results in a step-by-step fashion . experiments show that our approach significantly outperforms both distributed and symbolic executors , exhibiting high accuracy , high learning efficiency , high execution efficiency , and high interpretability .", "topics": ["reinforcement learning", "natural language"]}
{"title": "determining the best attributes for surveillance video keywords generation", "abstract": "automatic video keyword generation is one of the key ingredients in reducing the burden of security officers in analyzing surveillance videos . keywords or attributes are generally chosen manually based on expert knowledge of surveillance . most existing works primarily aim at either supervised learning approaches relying on extensive manual labelling or hierarchical probabilistic models that assume the features are extracted using the bag-of-words approach ; thus limiting the utilization of the other features . to address this , we turn our attention to automatic attribute discovery approaches . however , it is not clear which automatic discovery approach can discover the most meaningful attributes . furthermore , little research has been done on how to compare and choose the best automatic attribute discovery methods . in this paper , we propose a novel approach , based on the shared structure exhibited amongst meaningful attributes , that enables us to compare between different automatic attribute discovery approaches.we then validate our approach by comparing various attribute discovery methods such as picodes on two attribute datasets . the evaluation shows that our approach is able to select the automatic discovery approach that discovers the most meaningful attributes . we then employ the best discovery approach to generate keywords for videos recorded from a surveillance system . this work shows it is possible to massively reduce the amount of manual work in generating video keywords without limiting ourselves to a particular video feature descriptor .", "topics": ["supervised learning"]}
{"title": "on the synthesis of guaranteed-quality plans for robot fleets in logistics scenarios via optimization modulo theories", "abstract": "in manufacturing , the increasing involvement of autonomous robots in production processes poses new challenges on the production management . in this paper we report on the usage of optimization modulo theories ( omt ) to solve certain multi-robot scheduling problems in this area . whereas currently existing methods are heuristic , our approach guarantees optimality for the computed solution . we do not only present our final method but also its chronological development , and draw some general observations for the development of omt-based approaches .", "topics": ["mathematical optimization", "heuristic"]}
{"title": "mixture-of-parents maximum entropy markov models", "abstract": "we present the mixture-of-parents maximum entropy markov model ( mop-memm ) , a class of directed graphical models extending memms . the mop-memm allows tractable incorporation of long-range dependencies between nodes by restricting the conditional distribution of each node to be a mixture of distributions given the parents . we show how to efficiently compute the exact marginal posterior node distributions , regardless of the range of the dependencies . this enables us to model non-sequential correlations present within text documents , as well as between interconnected documents , such as hyperlinked web pages . we apply the mop-memm to a named entity recognition task and a web page classification task . in each , our model shows significant improvement over the basic memm , and is competitive with other long-range sequence models that use approximate inference .", "topics": ["graphical model"]}
{"title": "modelling and analysis of temporal preference drifts using a component-based factorised latent approach", "abstract": "the changes in user preferences can originate from substantial reasons , like personality shift , or transient and circumstantial ones , like seasonal changes in item popularities . disregarding these temporal drifts in modelling user preferences can result in unhelpful recommendations . moreover , different temporal patterns can be associated with various preference domains , and preference components and their combinations . these components comprise preferences over features , preferences over feature values , conditional dependencies between features , socially-influenced preferences , and bias . for example , in the movies domain , the user can change his rating behaviour ( bias shift ) , her preference for genre over language ( feature preference shift ) , or start favouring drama over comedy ( feature value preference shift ) . in this paper , we first propose a novel latent factor model to capture the domain-dependent component-specific temporal patterns in preferences . the component-based approach followed in modelling the aspects of preferences and their temporal effects enables us to arbitrarily switch components on and off . we evaluate the proposed method on three popular recommendation datasets and show that it significantly outperforms the most accurate state-of-the-art static models . the experiments also demonstrate the greater robustness and stability of the proposed dynamic model in comparison with the most successful models to date . we also analyse the temporal behaviour of different preference components and their combinations and show that the dynamic behaviour of preference components is highly dependent on the preference dataset and domain . therefore , the results also highlight the importance of modelling temporal effects but also underline the advantages of a component-based architecture that is better suited to capture domain-specific balances in the contributions of the aspects .", "topics": ["value ( ethics )", "interaction"]}
{"title": "a simple genome-wide association study algorithm", "abstract": "a computationally simple genome-wide association study ( gwas ) algorithm for estimating the main and epistatic effects of markers or single nucleotide polymorphisms ( snps ) is proposed . it is based on the intuitive assumption that changes of alleles corresponding to important snps in a pair of individuals lead to large difference of phenotype values of these individuals . the algorithm is based on considering pairs of individuals instead of snps or pairs of snps . the main advantage of the algorithm is that it weakly depends on the number of snps in a genotype matrix . it mainly depends on the number of individuals , which is typically very small in comparison with the number of snps . numerical experiments with real data sets illustrate the proposed algorithm .", "topics": ["numerical analysis"]}
{"title": "a non-parametric conditional factor regression model for high-dimensional input and response", "abstract": "in this paper , we propose a non-parametric conditional factor regression ( ncfr ) model for domains with high-dimensional input and response . ncfr enhances linear regression in two ways : a ) introducing low-dimensional latent factors leading to dimensionality reduction and b ) integrating an indian buffet process as a prior for the latent factors to derive unlimited sparse dimensions . experimental results comparing ncrf to several alternatives give evidence to remarkable prediction performance .", "topics": ["sparse matrix"]}
{"title": "definition of visual speech element and research on a method of extracting feature vector for korean lip-reading", "abstract": "in this paper , we defined the viseme ( visual speech element ) and described about the method of extracting visual feature vector . we defined the 10 visemes based on vowel by analyzing of korean utterance and proposed the method of extracting the 20-dimensional visual feature vector , combination of static features and dynamic features . lastly , we took an experiment in recognizing words based on 3-viseme hmm and evaluated the efficiency .", "topics": ["feature vector", "feature extraction"]}
{"title": "training probabilistic spiking neural networks with first-to-spike decoding", "abstract": "third-generation neural networks , or spiking neural networks ( snns ) , aim at harnessing the energy efficiency of spike-domain processing by building on computing elements that operate on , and exchange , spikes . in this paper , the problem of training a two-layer snn is studied for the purpose of classification , under a generalized linear model ( glm ) probabilistic neural model that was previously considered within the computational neuroscience literature . conventional classification rules for snns operate offline based on the number of output spikes at each output neuron . in contrast , a novel training method is proposed here for a first-to-spike decoding rule , whereby the snn can perform an early classification decision once spike firing is detected at an output neuron . numerical results bring insights into the optimal parameter selection for the glm neuron and on the accuracy-complexity trade-off performance of conventional and first-to-spike decoding .", "topics": ["statistical classification", "neural networks"]}
{"title": "hand-guided 3d surface acquisition by combining simple light sectioning with real-time algorithms", "abstract": "precise 3d measurements of rigid surfaces are desired in many fields of application like quality control or surgery . often , views from all around the object have to be acquired for a full 3d description of the object surface . we present a sensor principle called `` flying triangulation '' which avoids an elaborate `` stop-and-go '' procedure . it combines a low-cost classical light-section sensor with an algorithmic pipeline . a hand-guided sensor captures a continuous movie of 3d views while being moved around the object . the views are automatically aligned and the acquired 3d model is displayed in real time . in contrast to most existing sensors no bandwidth is wasted for spatial or temporal encoding of the projected lines . nor is an expensive color camera necessary for 3d acquisition . the achievable measurement uncertainty and lateral resolution of the generated 3d data is merely limited by physics . an alternating projection of vertical and horizontal lines guarantees the existence of corresponding points in successive 3d views . this enables a precise registration without surface interpolation . for registration , a variant of the iterative closest point algorithm - adapted to the specific nature of our 3d views - is introduced . furthermore , data reduction and smoothing without losing lateral resolution as well as the acquisition and mapping of a color texture is presented . the precision and applicability of the sensor is demonstrated by simulation and measurement results .", "topics": ["simulation", "sensor"]}
{"title": "multi-objects association in perception of dynamical situation", "abstract": "in current perception systems applied to the rebuilding of the environment for intelligent vehicles , the part reserved to object association for the tracking is increasingly significant . this allows firstly to follow the objects temporal evolution and secondly to increase the reliability of environment perception . we propose in this communication the development of a multi-objects association algorithm with ambiguity removal entering into the design of such a dynamic perception system for intelligent vehicles . this algorithm uses the belief theory and data modelling with fuzzy mathematics in order to be able to handle inaccurate as well as uncertain information due to imperfect sensors . these theories also allow the fusion of numerical as well as symbolic data . we develop in this article the problem of matching between known and perceived objects . this makes it possible to update a dynamic environment map for a vehicle . the belief theory will enable us to quantify the belief in the association of each perceived object with each known object . conflicts can appear in the case of object appearance or disappearance , or in the case of a confused situation or bad perception . these conflicts are removed or solved using an assignment algorithm , giving a solution called the `` best `` and so ensuring the tracking of some objects present in our environment .", "topics": ["numerical analysis", "sensor"]}
{"title": "bidirectional helmholtz machines", "abstract": "efficient unsupervised training and inference in deep generative models remains a challenging problem . one basic approach , called helmholtz machine , involves training a top-down directed generative model together with a bottom-up auxiliary model used for approximate inference . recent results indicate that better generative models can be obtained with better approximate inference procedures . instead of improving the inference procedure , we here propose a new model which guarantees that the top-down and bottom-up distributions can efficiently invert each other . we achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two . we present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized . this approach results in state of the art generative models which prefer significantly deeper architectures while it allows for orders of magnitude more efficient approximate inference .", "topics": ["generative model", "approximation algorithm"]}
{"title": "deeptravel : a neural network based travel time estimation model with auxiliary supervision", "abstract": "estimating the travel time of a path is of great importance to smart urban mobility . existing approaches are either based on estimating the time cost of each road segment which are not able to capture many cross-segment complex factors , or designed heuristically in a non-learning-based way which fail to utilize the existing abundant temporal labels of the data , i.e . , the time stamp of each trajectory point . in this paper , we leverage on new development of deep neural networks and propose a novel auxiliary supervision model , namely deeptravel , that can automatically and effectively extract different features , as well as make full use of the temporal labels of the trajectory data . we have conducted comprehensive experiments on real datasets to demonstrate the out-performance of deeptravel over existing approaches .", "topics": ["heuristic"]}
{"title": "to tune or not to tune the number of trees in random forest ?", "abstract": "the number of trees t in the random forest ( rf ) algorithm for supervised learning has to be set by the user . it is controversial whether t should simply be set to the largest computationally manageable value or whether a smaller t may in some cases be better . while the principle underlying bagging is that `` more trees are better '' , in practice the classification error rate sometimes reaches a minimum before increasing again for increasing number of trees . the goal of this paper is four-fold : ( i ) providing theoretical results showing that the expected error rate may be a non-monotonous function of the number of trees and explaining under which circumstances this happens ; ( ii ) providing theoretical results showing that such non-monotonous patterns can not be observed for other performance measures such as the brier score and the logarithmic loss ( for classification ) and the mean squared error ( for regression ) ; ( iii ) illustrating the extent of the problem through an application to a large number ( n = 306 ) of datasets from the public database openml ; ( iv ) finally arguing in favor of setting it to a computationally feasible large number , depending on convergence properties of the desired performance measure .", "topics": ["supervised learning", "database"]}
{"title": "an embedded segmental k-means model for unsupervised segmentation and clustering of speech", "abstract": "unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing . most approaches lie at methodological extremes : some use probabilistic bayesian models with convergence guarantees , while others opt for more efficient heuristic techniques . despite competitive performance in previous work , the full bayesian approach is difficult to scale to large speech corpora . we introduce an approximation to a recent bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full bayesian inference . like its bayesian counterpart , this embedded segmental k-means model ( es-kmeans ) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings . we first compare es-kmeans to previous approaches on common english and xitsonga data sets ( 5 and 2.5 hours of speech ) : es-kmeans outperforms a leading heuristic method in word segmentation , giving similar scores to the bayesian model while being 5 times faster with fewer hyperparameters . however , its clusters are less pure than those of the other models . we then show that es-kmeans scales to larger corpora by applying it to the 5 languages of the zero resource speech challenge 2017 ( up to 45 hours ) , where it performs competitively compared to the challenge baseline .", "topics": ["cluster analysis", "loss function"]}
{"title": "bayesian markov blanket estimation", "abstract": "this paper considers a bayesian view for estimating a sub-network in a markov random field . the sub-network corresponds to the markov blanket of a set of query variables , where the set of potential neighbours here is big . we factorize the posterior such that the markov blanket is conditionally independent of the network of the potential neighbours . by exploiting this blockwise decoupling , we derive analytic expressions for posterior conditionals . subsequently , we develop an inference scheme which makes use of the factorization . as a result , estimation of a sub-network is possible without inferring an entire network . since the resulting gibbs sampler scales linearly with the number of variables , it can handle relatively large neighbourhoods . the proposed scheme results in faster convergence and superior mixing of the markov chain than existing bayesian network estimation techniques .", "topics": ["sampling ( signal processing )", "bayesian network"]}
{"title": "efficient per-example gradient computations", "abstract": "this technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters . this gradient norm can be computed efficiently for every example .", "topics": ["loss function", "gradient"]}
{"title": "automated query learning with wikipedia and genetic programming", "abstract": "most of the existing information retrieval systems are based on bag of words model and are not equipped with common world knowledge . work has been done towards improving the efficiency of such systems by using intelligent algorithms to generate search queries , however , not much research has been done in the direction of incorporating human-and-society level knowledge in the queries . this paper is one of the first attempts where such information is incorporated into the search queries using wikipedia semantics . the paper presents an essential shift from conventional token based queries to concept based queries , leading to an enhanced efficiency of information retrieval systems . to efficiently handle the automated query learning problem , we propose wikipedia-based evolutionary semantics ( wiki-es ) framework where concept based queries are learnt using a co-evolving evolutionary procedure . learning concept based queries using an intelligent evolutionary procedure yields significant improvement in performance which is shown through an extensive study using reuters newswire documents . comparison of the proposed framework is performed with other information retrieval systems . concept based approach has also been implemented on other information retrieval systems to justify the effectiveness of a transition from token based queries to concept based queries .", "topics": ["relevance"]}
{"title": "joint denoising and distortion correction of atomic scale scanning transmission electron microscopy images", "abstract": "nowadays , modern electron microscopes deliver images at atomic scale . the precise atomic structure encodes information about material properties . thus , an important ingredient in the image analysis is to locate the centers of the atoms shown in micrographs as precisely as possible . here , we consider scanning transmission electron microscopy ( stem ) , which acquires data in a rastering pattern , pixel by pixel . due to this rastering combined with the magnification to atomic scale , movements of the specimen even at the nanometer scale lead to random image distortions that make precise atom localization difficult . given a series of stem images , we derive a bayesian method that jointly estimates the distortion in each image and reconstructs the underlying atomic grid of the material by fitting the atom bumps with suitable bump functions . the resulting highly non-convex minimization problems are solved numerically with a trust region approach . well-posedness of the reconstruction method and the model behavior for faster and faster rastering are investigated using variational techniques . the performance of the method is finally evaluated on both synthetic and real experimental data .", "topics": ["calculus of variations", "numerical analysis"]}
{"title": "predictive state representations : a new theory for modeling dynamical systems", "abstract": "modeling dynamical systems , both for control purposes and to make predictions about their behavior , is ubiquitous in science and engineering . predictive state representations ( psrs ) are a recently introduced class of models for discrete-time dynamical systems . the key idea behind psrs and the closely related ooms ( jaeger 's observable operator models ) is to represent the state of the system as a set of predictions of observable outcomes of experiments one can do in the system . this makes psrs rather different from history-based models such as nth-order markov models and hidden-state-based models such as hmms and pomdps . we introduce an interesting construct , the systemdynamics matrix , and show how psrs can be derived simply from it . we also use this construct to show formally that psrs are more general than both nth-order markov models and hmms/pomdps . finally , we discuss the main difference between psrs and ooms and conclude with directions for future work .", "topics": ["markov chain"]}
{"title": "value function approximation via low-rank models", "abstract": "we propose a novel value function approximation technique for markov decision processes . we consider the problem of compactly representing the state-action value function using a low-rank and sparse matrix model . the problem is to decompose a matrix that encodes the true value function into low-rank and sparse components , and we achieve this using robust principal component analysis ( pca ) . under minimal assumptions , this robust pca problem can be solved exactly via the principal component pursuit convex optimization problem . we experiment the procedure on several examples and demonstrate that our method yields approximations essentially identical to the true function .", "topics": ["optimization problem", "sparse matrix"]}
{"title": "development of an ideal observer that incorporates nuisance parameters and processes list-mode data", "abstract": "observer models were developed to process data in list-mode format in order to perform binary discrimination tasks for use in an arms-control-treaty context . data used in this study was generated using geant4 monte carlo simulations for photons using custom models of plutonium inspection objects and a radiation imaging system . observer model performance was evaluated and presented using the area under the receiver operating characteristic curve . the ideal observer was studied under both signal-known-exactly conditions and in the presence of unknowns such as object orientation and absolute count-rate variability ; when these additional sources of randomness were present , their incorporation into the observer yielded superior performance .", "topics": ["simulation"]}
{"title": "decentralized online big data classification - a bandit framework", "abstract": "distributed , online data mining systems have emerged as a result of applications requiring analysis of large amounts of correlated and high-dimensional data produced by multiple distributed data sources . we propose a distributed online data classification framework where data is gathered by distributed data sources and processed by a heterogeneous set of distributed learners which learn online , at run-time , how to classify the different data streams either by using their locally available classification functions or by helping each other by classifying each other 's data . importantly , since the data is gathered at different locations , sending the data to another learner to process incurs additional costs such as delays , and hence this will be only beneficial if the benefits obtained from a better classification will exceed the costs . we assume that the classification functions available to each processing element are fixed , but their prediction accuracy for various types of incoming data are unknown and can change dynamically over time , and thus they need to be learned online . we model the problem of joint classification by the distributed and heterogeneous learners from multiple data sources as a distributed contextual bandit problem where each data is characterized by a specific context . we develop distributed online learning algorithms for which we can prove that they have sublinear regret . compared to prior work in distributed online data mining , our work is the first to provide analytic regret results characterizing the performance of the proposed algorithms .", "topics": ["regret ( decision theory )", "data mining"]}
{"title": "a theory of feature learning", "abstract": "feature learning aims to extract relevant information contained in data sets in an automated fashion . it is driving force behind the current deep learning trend , a set of methods that have had widespread empirical success . what is lacking is a theoretical understanding of different feature learning schemes . this work provides a theoretical framework for feature learning and then characterizes when features can be learnt in an unsupervised fashion . we also provide means to judge the quality of features via rate-distortion theory and its generalizations .", "topics": ["feature learning", "unsupervised learning"]}
{"title": "a greedy , flexible algorithm to learn an optimal bayesian network structure", "abstract": "in this report paper we first present a report of the advanced machine learning course project on the provided data set and then present a novel heuristic algorithm for exact bayesian network ( bn ) structure discovery that uses decomposable scoring functions . our algorithm follows a different approach to solve the problem of bn structure discovery than the previously used methods such as dynamic programming ( dp ) and branch and bound to reduce the search space and find the global optima space for the problem . the algorithm we propose has some degree of flexibility that can make it more or less greedy . the more the algorithm is set to be greedy , the more the speed of the algorithm will be , and the less optimal the final structure . our algorithm runs in a much less time than the previously known methods and guarantees to have an optimality of close to 99 % . therefore , it sacrifices less than one percent of score of an optimal structure in order to gain a much lower running time and make the algorithm feasible for large data sets ( we may note that we never used any toolbox except for result validation )", "topics": ["time complexity", "heuristic"]}
{"title": "character-aware neural language models", "abstract": "we describe a simple neural language model that relies only on character-level inputs . predictions are still made at the word-level . our model employs a convolutional neural network ( cnn ) and a highway network over characters , whose output is given to a long short-term memory ( lstm ) recurrent neural network language model ( rnn-lm ) . on the english penn treebank the model is on par with the existing state-of-the-art despite having 60 % fewer parameters . on languages with rich morphology ( arabic , czech , french , german , spanish , russian ) , the model outperforms word-level/morpheme-level lstm baselines , again with fewer parameters . the results suggest that on many languages , character inputs are sufficient for language modeling . analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode , from characters only , both semantic and orthographic information .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "learning efficient structured sparse models", "abstract": "we present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes . for this purpose , we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems , and show an efficient feed forward architecture derived from its iteration . this architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods . we also show that by using different training objective functions , learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary , as in earlier formulations , but can be rather used as full-featured sparse encoders or even modelers . a simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation , making the proposed framework suitable for real time and large-scale applications .", "topics": ["approximation algorithm", "sparse matrix"]}
{"title": "context-aware sentiment word identification : sentiword2vec", "abstract": "traditional sentiment analysis often uses sentiment dictionary to extract sentiment information in text and classify documents . however , emerging informal words and phrases in user generated content call for analysis aware to the context . usually , they have special meanings in a particular context . because of its great performance in representing inter-word relation , we use sentiment word vectors to identify the special words . based on the distributed language model word2vec , in this paper we represent a novel method about sentiment representation of word under particular context , to be detailed , to identify the words with abnormal sentiment polarity in long answers . result shows the improved model shows better performance in representing the words with special meaning , while keep doing well in representing special idiomatic pattern . finally , we will discuss the meaning of vectors representing in the field of sentiment , which may be different from general object-based conditions .", "topics": ["dictionary"]}
{"title": "fast approximate l_infty minimization : speeding up robust regression", "abstract": "minimization of the $ l_\\infty $ norm , which can be viewed as approximately solving the non-convex least median estimation problem , is a powerful method for outlier removal and hence robust regression . however , current techniques for solving the problem at the heart of $ l_\\infty $ norm minimization are slow , and therefore can not scale to large problems . a new method for the minimization of the $ l_\\infty $ norm is presented here , which provides a speedup of multiple orders of magnitude for data with high dimension . this method , termed fast $ l_\\infty $ minimization , allows robust regression to be applied to a class of problems which were previously inaccessible . it is shown how the $ l_\\infty $ norm minimization problem can be broken up into smaller sub-problems , which can then be solved extremely efficiently . experimental results demonstrate the radical reduction in computation time , along with robustness against large numbers of outliers in a few model-fitting problems .", "topics": ["time complexity", "computation"]}
{"title": "an experimental comparison of several clustering and initialization methods", "abstract": "we examine methods for clustering in high dimensions . in the first part of the paper , we perform an experimental comparison between three batch clustering algorithms : the expectation-maximization ( em ) algorithm , a winner take all version of the em algorithm reminiscent of the k-means algorithm , and model-based hierarchical agglomerative clustering . we learn naive-bayes models with a hidden root node , using high-dimensional discrete-variable data sets ( both real and synthetic ) . we find that the em algorithm significantly outperforms the other methods , and proceed to investigate the effect of various initialization schemes on the final solution produced by the em algorithm . the initializations that we consider are ( 1 ) parameters sampled from an uninformative prior , ( 2 ) random perturbations of the marginal distribution of the data , and ( 3 ) the output of hierarchical agglomerative clustering . although the methods are substantially different , they lead to learned models that are strikingly similar in quality .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "dynamic screening : accelerating first-order algorithms for the lasso and group-lasso", "abstract": "recent computational strategies based on screening tests have been proposed to accelerate algorithms addressing penalized sparse regression problems such as the lasso . such approaches build upon the idea that it is worth dedicating some small computational effort to locate inactive atoms and remove them from the dictionary in a preprocessing stage so that the regression algorithm working with a smaller dictionary will then converge faster to the solution of the initial problem . we believe that there is an even more efficient way to screen the dictionary and obtain a greater acceleration : inside each iteration of the regression algorithm , one may take advantage of the algorithm computations to obtain a new screening test for free with increasing screening effects along the iterations . the dictionary is henceforth dynamically screened instead of being screened statically , once and for all , before the first iteration . we formalize this dynamic screening principle in a general algorithmic scheme and apply it by embedding inside a number of first-order algorithms adapted existing screening tests to solve the lasso or new screening tests to solve the group-lasso . computational gains are assessed in a large set of experiments on synthetic data as well as real-world sounds and images . they show both the screening efficiency and the gain in terms running times .", "topics": ["synthetic data", "iteration"]}
{"title": "feature selection via probabilistic outputs", "abstract": "this paper investigates two feature-scoring criteria that make use of estimated class probabilities : one method proposed by \\citet { shen } and a complementary approach proposed below . we develop a theoretical framework to analyze each criterion and show that both estimate the spread ( across all values of a given feature ) of the probability that an example belongs to the positive class . based on our analysis , we predict when each scoring technique will be advantageous over the other and give empirical results validating our predictions .", "topics": ["value ( ethics )", "eisenstein 's criterion"]}
{"title": "a hierarchical approach for generating descriptive image paragraphs", "abstract": "recent progress on image captioning has made it possible to generate novel sentences describing images in natural language , but compressing an image into a single sentence can describe visual content in only coarse detail . while one new captioning approach , dense captioning , can potentially describe images in finer levels of detail by captioning many regions within an image , it in turn is unable to produce a coherent story for an image . in this paper we overcome these limitations by generating entire paragraphs for describing images , which can tell detailed , unified stories . we develop a model that decomposes both images and paragraphs into their constituent parts , detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language . linguistic analysis confirms the complexity of the paragraph generation task , and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach .", "topics": ["recurrent neural network", "natural language"]}
{"title": "a new family of near-metrics for universal similarity", "abstract": "we propose a family of near-metrics based on local graph diffusion to capture similarity for a wide class of data sets . these quasi-metametrics , as their names suggest , dispense with one or two standard axioms of metric spaces , specifically distinguishability and symmetry , so that similarity between data points of arbitrary type and form could be measured broadly and effectively . the proposed near-metric family includes the forward k-step diffusion and its reverse , typically on the graph consisting of data objects and their features . by construction , this family of near-metrics is particularly appropriate for categorical data , continuous data , and vector representations of images and text extracted via deep learning approaches . we conduct extensive experiments to evaluate the performance of this family of similarity measures and compare and contrast with traditional measures of similarity used for each specific application and with the ground truth when available . we show that for structured data including categorical and continuous data , the near-metrics corresponding to normalized forward k-step diffusion ( k small ) work as one of the best performing similarity measures ; for vector representations of text and images including those extracted from deep learning , the near-metrics derived from normalized and reverse k-step graph diffusion ( k very small ) exhibit outstanding ability to distinguish data points from different classes .", "topics": ["ground truth"]}
{"title": "robust multilingual part-of-speech tagging via adversarial training", "abstract": "adversarial training ( at ) is a powerful regularization method for neural networks , aiming to achieve robustness to input perturbations . yet , the specific effects of the robustness obtained by at are still unclear in the context of natural language processing . in this paper , we propose and analyze a neural pos tagging model that exploits adversarial training ( at ) . in our experiments on the penn treebank wsj corpus and the universal dependencies ( ud ) dataset ( 28 languages ) , we find that at not only improves the overall tagging accuracy , but also 1 ) largely prevents overfitting in low resource languages and 2 ) boosts tagging accuracy for rare / unseen words . the proposed pos tagger achieves state-of-the-art performance on nearly all of the languages in ud v1.2 . we also demonstrate that 3 ) the improved tagging performance by at contributes to the downstream task of dependency parsing , and that 4 ) at helps the model to learn cleaner word and internal representations . these positive results motivate further use of at for natural language tasks .", "topics": ["natural language processing", "natural language"]}
{"title": "an online mechanism for ridesharing in autonomous mobility-on-demand systems", "abstract": "with proper management , autonomous mobility-on-demand ( amod ) systems have great potential to satisfy the transport demands of urban populations by providing safe , convenient , and affordable ridesharing services . meanwhile , such systems can substantially decrease private car ownership and use , and thus significantly reduce traffic congestion , energy consumption , and carbon emissions . to achieve this objective , an amod system requires private information about the demand from passengers . however , due to self-interestedness , passengers are unlikely to cooperate with the service providers in this regard . therefore , an online mechanism is desirable if it incentivizes passengers to truthfully report their actual demand . for the purpose of promoting ridesharing , we hereby introduce a posted-price , integrated online ridesharing mechanism ( iors ) that satisfies desirable properties such as ex-post incentive compatibility , individual rationality , and budget-balance . numerical results indicate the competitiveness of iors compared with two benchmarks , namely the optimal assignment and an offline , auction-based mechanism .", "topics": ["time complexity", "optimization problem"]}
{"title": "a fully data-driven method to identify ( correlated ) changes in diachronic corpora", "abstract": "in this paper , a method for measuring synchronic corpus ( dis- ) similarity put forward by kilgarriff ( 2001 ) is adapted and extended to identify trends and correlated changes in diachronic text data , using the corpus of historical american english ( davies 2010a ) and the google ngram corpora ( michel et al . 2010a ) . this paper shows that this fully data-driven method , which extracts word types that have undergone the most pronounced change in frequency in a given period of time , is computationally very cheap and that it allows interpretations of diachronic trends that are both intuitively plausible and motivated from the perspective of information theory . furthermore , it demonstrates that the method is able to identify correlated linguistic changes and diachronic shifts that can be linked to historical events . finally , it can help to improve diachronic pos tagging and complement existing nlp approaches . this indicates that the approach can facilitate an improved understanding of diachronic processes in language change .", "topics": ["natural language processing", "text corpus"]}
{"title": "lexis : an optimization framework for discovering the hierarchical structure of sequential data", "abstract": "data represented as strings abounds in biology , linguistics , document mining , web search and many other fields . such data often have a hierarchical structure , either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings . we propose a framework , referred to as `` lexis '' , that produces an optimized hierarchical representation of a given set of `` target '' strings . the resulting hierarchy , `` lexis-dag '' , shows how to construct each target through the concatenation of intermediate substrings , minimizing the total number of such concatenations or dag edges . the lexis optimization problem is related to the smallest grammar problem . after we prove its np-hardness for two cost formulations , we propose an efficient greedy algorithm for the construction of lexis-dags . we also consider the problem of identifying the set of intermediate nodes ( substrings ) that collectively form the `` core '' of a lexis-dag , which is important in the analysis of lexis-dags . we show that the lexis framework can be applied in diverse applications such as optimized synthesis of dna fragments in genomic libraries , hierarchical structure discovery in protein sequences , dictionary-based text compression , and feature extraction from a set of documents .", "topics": ["optimization problem", "feature extraction"]}
{"title": "summarized network behavior prediction", "abstract": "this work studies the entity-wise topical behavior from massive network logs . both the temporal and the spatial relationships of the behavior are explored with the learning architectures combing the recurrent neural network ( rnn ) and the convolutional neural network ( cnn ) . to make the behavioral data appropriate for the spatial learning in cnn , several reduction steps are taken to form the topical metrics and place them homogeneously like pixels in the images . the experimental result shows both the temporal- and the spatial- gains when compared to a multilayer perceptron ( mlp ) network . a new learning framework called spatially connected convolutional networks ( sccn ) is introduced to more efficiently predict the behavior .", "topics": ["recurrent neural network", "pixel"]}
{"title": "tree-structured neural machine for linguistics-aware sentence generation", "abstract": "different from other sequential data , sentences in natural language are structured by linguistic grammars . previous generative conversational models with chain-structured decoder ignore this structure in human language and might generate plausible responses with less satisfactory relevance and fluency . in this study , we aim to incorporate the results from linguistic analysis into the process of sentence generation for high-quality conversation generation . specifically , we use a dependency parser to transform each response sentence into a dependency tree and construct a training corpus of sentence-tree pairs . a tree-structured decoder is developed to learn the mapping from a sentence to its tree , where different types of hidden states are used to depict the local dependencies from an internal tree node to its children . for training acceleration , we propose a tree canonicalization method , which transforms trees into equivalent ternary trees . then , with a proposed tree-structured search method , the model is able to generate the most probable responses in the form of dependency trees , which are finally flattened into sequences as the system output . experimental results demonstrate that the proposed x2tree framework outperforms baseline methods over 11.15 % increase of acceptance ratio .", "topics": ["baseline ( configuration management )", "natural language"]}
{"title": "disunited nations ? a multiplex network approach to detecting preference affinity blocs using texts and votes", "abstract": "this paper contributes to an emerging literature that models votes and text in tandem to better understand polarization of expressed preferences . it introduces a new approach to estimate preference polarization in multidimensional settings , such as international relations , based on developments in the natural language processing and network science literatures -- namely word embeddings , which retain valuable syntactical qualities of human language , and community detection in multilayer networks , which locates densely connected actors across multiple , complex networks . we find that the employment of these tools in tandem helps to better estimate states ' foreign policy preferences expressed in un votes and speeches beyond that permitted by votes alone . the utility of these located affinity blocs is demonstrated through an application to conflict onset in international relations , though these tools will be of interest to all scholars faced with the measurement of preferences and polarization in multidimensional settings .", "topics": ["natural language processing"]}
{"title": "on learning by exchanging advice", "abstract": "one of the main questions concerning learning in multi-agent systems is : ( how ) can agents benefit from mutual interaction during the learning process ? . this paper describes the study of an interactive advice-exchange mechanism as a possible way to improve agents ' learning performance . the advice-exchange technique , discussed here , uses supervised learning ( backpropagation ) , where reinforcement is not directly coming from the environment but is based on advice given by peers with better performance score ( higher confidence ) , to enhance the performance of a heterogeneous group of learning agents ( las ) . the las are facing similar problems , in an environment where only reinforcement information is available . each la applies a different , well known , learning technique : random walk ( hill-climbing ) , simulated annealing , evolutionary algorithms and q-learning . the problem used for evaluation is a simplified traffic-control simulation . initial results indicate that advice-exchange can improve learning speed , although bad advice and/or blind reliance can disturb the learning performance .", "topics": ["supervised learning", "simulation"]}
{"title": "patterns for learning with side information", "abstract": "supervised , semi-supervised , and unsupervised learning estimate a function given input/output samples . generalization of the learned function to unseen data can be improved by incorporating side information into learning . side information are data that are neither from the input space nor from the output space of the function , but include useful information for learning it . in this paper we show that learning with side information subsumes a variety of related approaches , e.g . multi-task learning , multi-view learning and learning using privileged information . our main contributions are ( i ) a new perspective that connects these previously isolated approaches , ( ii ) insights about how these methods incorporate different types of prior knowledge , and hence implement different patterns , ( iii ) facilitating the application of these methods in novel tasks , as well as ( iv ) a systematic experimental evaluation of these patterns in two supervised learning tasks .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "image clustering without ground truth", "abstract": "cluster analysis has become one of the most exercised research areas over the past few decades in computer science . as a consequence , numerous clustering algorithms have already been developed to find appropriate partitions of a set of objects . given multiple such clustering solutions , it is a challenging task to obtain an ensemble of these solutions . this becomes more challenging when the ground truth about the number of clusters is unavailable . in this paper , we introduce a crowd-powered model to collect solutions of image clustering from the general crowd and pose it as a clustering ensemble problem with variable number of clusters . the varying number of clusters basically reflects the crowd workers ' perspective toward a particular set of objects . we allow a set of crowd workers to independently cluster the images as per their perceptions . we address the problem by finding out centroid of the clusters using an appropriate distance measure and prioritize the likelihood of similarity of the individual cluster sets . the effectiveness of the proposed method is demonstrated by applying it on multiple artificial datasets obtained from crowd .", "topics": ["cluster analysis", "ground truth"]}
{"title": "on distances , paths and connections for hyperspectral image segmentation", "abstract": "the present paper introduces the $ \\eta $ and { \\eta } connections in order to add regional information on $ \\lambda $ -flat zones , which only take into account a local information . a top-down approach is considered . first $ \\lambda $ -flat zones are built in a way leading to a sub-segmentation . then a finer segmentation is obtained by computing $ \\eta $ -bounded regions and $ \\mu $ -geodesic balls inside the $ \\lambda $ -flat zones . the proposed algorithms for the construction of new partitions are based on queues with an ordered selection of seeds using the cumulative distance . $ \\eta $ -bounded regions offers a control on the variations of amplitude in the class from a point , called center , and $ \\mu $ -geodesic balls controls the `` size '' of the class . these results are applied to hyperspectral images .", "topics": ["image segmentation"]}
{"title": "group sparse priors for covariance estimation", "abstract": "recently it has become popular to learn sparse gaussian graphical models ( ggms ) by imposing l1 or group l1,2 penalties on the elements of the precision matrix . thispenalized likelihood approach results in a tractable convex optimization problem . in this paper , we reinterpret these results as performing map estimation under a novel prior which we call the group l1 and l1,2 positivedefinite matrix distributions . this enables us to build a hierarchical model in which the l1 regularization terms vary depending on which group the entries are assigned to , which in turn allows us to learn block structured sparse ggms with unknown group assignments . exact inference in this hierarchical model is intractable , due to the need to compute the normalization constant of these matrix distributions . however , we derive upper bounds on the partition functions , which lets us use fast variational inference ( optimizing a lower bound on the joint posterior ) . we show that on two real world data sets ( motion capture and financial data ) , our method which infers the block structure outperforms a method that uses a fixed block structure , which in turn outperforms baseline methods that ignore block structure .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "recurrent and contextual models for visual question answering", "abstract": "we propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the visual7w dataset . motivated by divergent trends in model complexities in the literature , we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures . we start with lstm-encoding of input questions and answers ; build on this with context generation by lstm-encodings of neural image and question representations and attention over images ; and evaluate the diversity and predictive power of our models and the ensemble thereof . all models are evaluated against a simple baseline inspired by the current state-of-the-art , consisting of involving simple concatenation of bag-of-words and cnn representations for the text and images , respectively . generally , we observe marked variation in image-reasoning performance between our models not obvious from their overall performance , as well as evidence of dataset bias . our standalone models achieve accuracies up to $ 64.6\\ % $ , while the ensemble of all models achieves the best accuracy of $ 66.67\\ % $ , within $ 0.5\\ % $ of the current state-of-the-art for visual7w .", "topics": ["baseline ( configuration management )"]}
{"title": "3d-prnn : generating shape primitives with recurrent neural networks", "abstract": "the success of various applications including robotics , digital content creation , and visualization demand a structured and abstract representation of the 3d world from limited sensor data . inspired by the nature of human perception of 3d shapes as a collection of simple parts , we explore such an abstract shape representation based on primitives . given a single depth image of an object , we present 3d-prnn , a generative recurrent neural network that synthesizes multiple plausible shapes composed of a set of primitives . our generative model encodes symmetry characteristics of common man-made objects , preserves long-range structural coherence , and describes objects of varying complexity with a compact representation . we also propose a method based on gaussian fields to generate a large scale dataset of primitive-based shape representations to train our network . we evaluate our approach on a wide range of examples and show that it outperforms nearest-neighbor based shape retrieval methods and is on-par with voxel-based generative models while using a significantly reduced parameter space .", "topics": ["generative model", "recurrent neural network"]}
{"title": "clustering of local optima in combinatorial fitness landscapes", "abstract": "using the recently proposed model of combinatorial landscapes : local optima networks , we study the distribution of local optima in two classes of instances of the quadratic assignment problem . our results indicate that the two problem instance classes give rise to very different configuration spaces . for the so-called real-like class , the optima networks possess a clear modular structure , while the networks belonging to the class of random uniform instances are less well partitionable into clusters . we briefly discuss the consequences of the findings for heuristically searching the corresponding problem spaces .", "topics": ["cluster analysis", "heuristic"]}
{"title": "visual analytics of image-centric cohort studies in epidemiology", "abstract": "epidemiology characterizes the influence of causes to disease and health conditions of defined populations . cohort studies are population-based studies involving usually large numbers of randomly selected individuals and comprising numerous attributes , ranging from self-reported interview data to results from various medical examinations , e.g . , blood and urine samples . since recently , medical imaging has been used as an additional instrument to assess risk factors and potential prognostic information . in this chapter , we discuss such studies and how the evaluation may benefit from visual analytics . cluster analysis to define groups , reliable image analysis of organs in medical imaging data and shape space exploration to characterize anatomical shapes are among the visual analytics tools that may enable epidemiologists to fully exploit the potential of their huge and complex data . to gain acceptance , visual analytics tools need to complement more classical epidemiologic tools , primarily hypothesis-driven statistical analysis .", "topics": ["cluster analysis"]}
{"title": "aveid : automatic video system for measuring engagement in dementia", "abstract": "engagement in dementia is typically measured using behavior observational scales ( bos ) that are tedious and involve intensive manual labor to annotate , and are therefore not easily scalable . we propose aveid , a low cost and easy-to-use video-based engagement measurement tool to determine the engagement level of a person with dementia ( pwd ) during digital interaction . we show that the objective behavioral measures computed via aveid correlate well with subjective expert impressions for the popular mpes and ome bos , confirming its viability and effectiveness . moreover , aveid measures can be obtained for a variety of engagement designs , thereby facilitating large-scale studies with pwd populations .", "topics": ["scalability"]}
{"title": "a new automatic method to adjust parameters for object recognition", "abstract": "to recognize an object in an image , the user must apply a combination of operators , where each operator has a set of parameters . these parameters must be well adjusted in order to reach good results . usually , this adjustment is made manually by the user . in this paper we propose a new method to automate the process of parameter adjustment for an object recognition task . our method is based on reinforcement learning , we use two types of agents : user agent that gives the necessary information and parameter agent that adjusts the parameters of each operator . due to the nature of reinforcement learning the results do not depend only on the system characteristics but also on the user favorite choices .", "topics": ["image segmentation", "reinforcement learning"]}
{"title": "learning to identify regular expressions that describe email campaigns", "abstract": "this paper addresses the problem of inferring a regular expression from a given set of strings that resembles , as closely as possible , the regular expression that a human expert would have written to identify the language . this is motivated by our goal of automating the task of postmasters of an email service who use regular expressions to describe and blacklist email spam campaigns . training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist . we model this task as a learning problem with structured output spaces and an appropriate loss function , derive a decoder and the resulting optimization problem , and a report on a case study conducted with an email service .", "topics": ["optimization problem", "loss function"]}
{"title": "understanding intermediate layers using linear classifier probes", "abstract": "neural network models have a reputation for being black boxes . we propose a new method to understand better the roles and dynamics of the intermediate layers . this has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics ( such as the auxiliary heads in the inception model ) . our method uses linear classifiers , referred to as `` probes '' , where a probe can only use the hidden units of a given intermediate layer as discriminating features . moreover , these probes can not affect the training phase of a model , and they are generally added after training . they allow the user to visualize the state of the model at multiple steps of training . we demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems .", "topics": ["heuristic"]}
{"title": "blind system identification using kernel-based methods", "abstract": "we propose a new method for blind system identification . resorting to a gaussian regression framework , we model the impulse response of the unknown linear system as a realization of a gaussian process . the structure of the covariance matrix ( or kernel ) of such a process is given by the stable spline kernel , which has been recently introduced for system identification purposes and depends on an unknown hyperparameter . we assume that the input can be linearly described by few parameters . we estimate these parameters , together with the kernel hyperparameter and the noise variance , using an empirical bayes approach . the related optimization problem is efficiently solved with a novel iterative scheme based on the expectation-maximization method . in particular , we show that each iteration consists of a set of simple update rules . we show , through some numerical experiments , very promising performance of the proposed method .", "topics": ["kernel ( operating system )", "optimization problem"]}
{"title": "jointly extracting relations with class ties via effective deep ranking", "abstract": "connections between relations in relation extraction , which we call class ties , are common . in distantly supervised scenario , one entity tuple may have multiple relation facts . exploiting class ties between relations of one entity tuple will be promising for distantly supervised relation extraction . however , previous models are not effective or ignore to model this property . in this work , to effectively leverage class ties , we propose to make joint relation extraction with a unified model that integrates convolutional neural network ( cnn ) with a general pairwise ranking framework , in which three novel ranking loss functions are introduced . additionally , an effective method is presented to relieve the severe class imbalance problem from nr ( not relation ) for model training . experiments on a widely used dataset show that leveraging class ties will enhance extraction and demonstrate the effectiveness of our model to learn class ties . our model outperforms the baselines significantly , achieving state-of-the-art performance .", "topics": ["supervised learning", "noise reduction"]}
{"title": "speaker identification using mfcc-domain support vector machine", "abstract": "speech recognition and speaker identification are important for authentication and verification in security purpose , but they are difficult to achieve . speaker identification methods can be divided into text-independent and text-dependent . this paper presents a technique of text-dependent speaker identification using mfcc-domain support vector machine ( svm ) . in this work , melfrequency cepstrum coefficients ( mfccs ) and their statistical distribution properties are used as features , which will be inputs to the neural network . this work firstly used sequential minimum optimization ( smo ) learning technique for svm that improve performance over traditional techniques chunking , osuna . the cepstrum coefficients representing the speaker characteristics of a speech segment are computed by nonlinear filter bank analysis and discrete cosine transform . the speaker identification ability and convergence speed of the svms are investigated for different combinations of features . extensive experimental results on several samples show the effectiveness of the proposed approach .", "topics": ["support vector machine", "speech recognition"]}
{"title": "strategic attentive writer for learning macro-actions", "abstract": "we present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting . the network builds an internal plan , which is continuously updated upon observation of the next input from the environment . it can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e . followed without re-planing . combining these properties , the proposed model , dubbed strategic attentive writer ( straw ) can learn high-level , temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information . these macro-actions enable both structured exploration and economic computation . we experimentally demonstrate that straw delivers strong improvements on several atari games by employing temporally extended planning strategies ( e.g . ms. pacman and frostbite ) . it is at the same time a general algorithm that can be applied on any sequence data . to that end , we also show that when trained on text prediction task , straw naturally predicts frequent n-grams ( instead of macro-actions ) , demonstrating the generality of the approach .", "topics": ["recurrent neural network", "high- and low-level"]}
{"title": "robust real-time multi-view eye tracking", "abstract": "despite significant advances in improving the gaze tracking accuracy under controlled conditions , the tracking robustness under real-world conditions , such as large head pose and movements , use of eyeglasses , illumination and eye type variations , remains a major challenge in eye tracking . in this paper , we revisit this challenge and introduce a real-time multi-camera eye tracking framework to improve the tracking robustness . first , differently from previous work , we design a multi-view tracking setup that allows for acquiring multiple eye appearances simultaneously . leveraging multi-view appearances enables to more reliably detect gaze features under challenging conditions , particularly when they are obstructed in conventional single-view appearance due to large head movements or eyewear effects . the features extracted on various appearances are then used for estimating multiple gaze outputs . second , we propose to combine estimated gaze outputs through an adaptive fusion mechanism to compute user 's overall point of regard . the proposed mechanism firstly determines the estimation reliability of each gaze output according to user 's momentary head pose and predicted gazing behavior , and then performs a reliability-based weighted fusion . we demonstrate the efficacy of our framework with extensive simulations and user experiments on a collected dataset featuring 20 subjects . our results show that in comparison with state-of-the-art eye trackers , the proposed framework provides not only a significant enhancement in accuracy but also a notable robustness . our prototype system runs at 30 frames-per-second ( fps ) and achieves 1 degree accuracy under challenging experimental scenarios , which makes it suitable for applications demanding high accuracy and robustness .", "topics": ["simulation"]}
{"title": "learning user intent from action sequences on interactive systems", "abstract": "interactive systems have taken over the web and mobile space with increasing participation from users . applications across every marketing domain can now be accessed through mobile or web where users can directly perform certain actions and reach a desired outcome . actions of user on a system , though , can be representative of a certain intent . ability to learn this intent through user 's actions can help draw certain insight into the behavior of users on a system . in this paper , we present models to optimize interactive systems by learning and analyzing user intent through their actions on the system . we present a four phased model that uses time-series of interaction actions sequentially using a long short-term memory ( lstm ) based sequence learning system that helps build a model for intent recognition . our system then provides an objective specific maximization followed by analysis and contrasting methods in order to identify spaces of improvement in the interaction system . we discuss deployment scenarios for such a system and present results from evaluation on an online marketplace using user clickstream data .", "topics": ["time series"]}
{"title": "targeting optimal active learning via example quality", "abstract": "in many classification problems unlabelled data is abundant and a subset can be chosen for labelling . this defines the context of active learning ( al ) , where methods systematically select that subset , to improve a classifier by retraining . given a classification problem , and a classifier trained on a small number of labelled examples , consider the selection of a single further example . this example will be labelled by the oracle and then used to retrain the classifier . this example selection raises a central question : given a fully specified stochastic description of the classification problem , which example is the optimal selection ? if optimality is defined in terms of loss , this definition directly produces expected loss reduction ( elr ) , a central quantity whose maximum yields the optimal example selection . this work presents a new theoretical approach to al , example quality , which defines optimal al behaviour in terms of elr . once optimal al behaviour is defined mathematically , reasoning about this abstraction provides insights into al . in a theoretical context the optimal selection is compared to existing al methods , showing that heuristics can make sub-optimal selections . algorithms are constructed to estimate example quality directly . a large-scale experimental study shows these algorithms to be competitive with standard al methods .", "topics": ["statistical classification", "mathematical optimization"]}
{"title": "structured probabilistic pruning for convolutional neural network acceleration", "abstract": "although deep convolutional neural network ( cnn ) has shown better performance in various computer vision tasks , its application is restricted by a significant increase in storage and computation . among cnn simplification techniques , parameter pruning is a promising approach which aims at reducing the number of weights of various layers without intensively reducing the original accuracy . in this paper , we propose a novel progressive parameter pruning method , named structured probabilistic pruning ( spp ) , which effectively prunes weights of convolutional layers in a probabilistic manner . specifically , unlike existing deterministic pruning approaches , where unimportant weights are permanently eliminated , spp introduces a pruning probability for each weight , and pruning is guided by sampling from the pruning probabilities . a mechanism is designed to increase and decrease pruning probabilities based on importance criteria for the training process . experiments show that , with 4x speedup , spp can accelerate alexnet with only 0.3 % loss of top-5 accuracy and vgg-16 with 0.8 % loss of top-5 accuracy in imagenet classification . moreover , spp can be directly applied to accelerate multi-branch cnn networks , such as resnet , without specific adaptations . our 2x speedup resnet-50 only suffers 0.8 % loss of top-5 accuracy on imagenet . we further prove the effectiveness of our method on transfer learning task on flower-102 dataset with alexnet .", "topics": ["iteration", "computation"]}
{"title": "configurable , photorealistic image rendering and ground truth synthesis by sampling stochastic grammars representing indoor scenes", "abstract": "we propose the configurable rendering of massive quantities of photorealistic images with ground truth for the purposes of training , benchmarking , and diagnosing computer vision models . in contrast to the conventional ( crowd-sourced ) manual labeling of ground truth for a relatively modest number of rgb-d images captured by kinect-like sensors , we devise a non-trivial configurable pipeline of algorithms capable of generating a potentially infinite variety of indoor scenes using a stochastic grammar , specifically , one represented by an attributed spatial and-or graph . we employ physics-based rendering to synthesize photorealistic rgb images while automatically synthesizing detailed , per-pixel ground truth data , including visible surface depth and normal , object identity and material information , as well as illumination . our pipeline is configurable inasmuch as it enables the precise customization and control of important attributes of the generated scenes . we demonstrate that our generated scenes achieve a performance similar to the nyu v2 dataset on pre-trained deep learning models . by modifying pipeline components in a controllable manner , we furthermore provide diagnostics on common scene understanding tasks ; eg . , depth and surface normal prediction , semantic segmentation , etc .", "topics": ["computer vision", "ground truth"]}
{"title": "output-sensitive adaptive metropolis-hastings for probabilistic programs", "abstract": "we introduce an adaptive output-sensitive metropolis-hastings algorithm for probabilistic models expressed as programs , adaptive lightweight metropolis-hastings ( adlmh ) . the algorithm extends lightweight metropolis-hastings ( lmh ) by adjusting the probabilities of proposing random variables for modification to improve convergence of the program output . we show that adlmh converges to the correct equilibrium distribution and compare convergence of adlmh to that of lmh on several test problems to highlight different aspects of the adaptation scheme . we observe consistent improvement in convergence on the test problems .", "topics": ["markov chain"]}
{"title": "futility analysis in the cross-validation of machine learning models", "abstract": "many machine learning models have important structural tuning parameters that can not be directly estimated from the data . the common tactic for setting these parameters is to use resampling methods , such as cross -- validation or the bootstrap , to evaluate a candidate set of values and choose the best based on some pre -- defined criterion . unfortunately , this process can be time consuming . however , the model tuning process can be streamlined by adaptively resampling candidate values so that settings that are clearly sub-optimal can be discarded . the notion of futility analysis is introduced in this context . an example is shown that illustrates how adaptive resampling can be used to reduce training time . simulation studies are used to understand how the potential speed -- up is affected by parallel processing techniques .", "topics": ["value ( ethics )", "simulation"]}
{"title": "towards a job title classification system", "abstract": "document classification for text , images and other applicable entities has long been a focus of research in academia and also finds application in many industrial settings . amidst a plethora of approaches to solve such problems , machine-learning techniques have found success in a variety of scenarios . in this paper we discuss the design of a machine learning-based semi-supervised job title classification system for the online job recruitment domain currently in production at careerbuilder.com and propose enhancements to it . the system leverages a varied collection of classification as well clustering algorithms . these algorithms are encompassed in an architecture that facilitates leveraging existing off-the-shelf machine learning tools and techniques while keeping into consideration the challenges of constructing a scalable classification system for a large taxonomy of categories . as a continuously evolving system that is still under development we first discuss the existing semi-supervised classification system which is composed of both clustering and classification components in a proximity-based classifier setup and results of which are already used across numerous products at careerbuilder . we then elucidate our long-term goals for job title classification and propose enhancements to the existing system in the form of a two-stage coarse and fine level classifier augmentation to construct a cascade of hierarchical vertical classifiers . preliminary results are presented using experimental evaluation on real world industrial data .", "topics": ["supervised learning", "cluster analysis"]}
{"title": "oversampling for imbalanced learning based on k-means and smote", "abstract": "learning from class-imbalanced data continues to be a common and challenging problem in supervised learning as standard classification algorithms are designed to handle balanced class distributions . while different strategies exist to tackle this problem , methods which generate artificial data to achieve a balanced class distribution are more versatile than modifications to the classification algorithm . such techniques , called oversamplers , modify the training data , allowing any classifier to be used with class-imbalanced datasets . many algorithms have been proposed for this task , but most are complex and tend to generate unnecessary noise . this work presents a simple and effective oversampling method based on k-means clustering and smote oversampling , which avoids the generation of noise and effectively overcomes imbalances between and within classes . empirical results of extensive experiments with 71 datasets show that training data oversampled with the proposed method improves classification results . moreover , k-means smote consistently outperforms other popular oversampling methods . an implementation is made available in the python programming language .", "topics": ["test set", "supervised learning"]}
{"title": "symbolic computing with incremental mindmaps to manage and mine data streams - some applications", "abstract": "in our understanding , a mind-map is an adaptive engine that basically works incrementally on the fundament of existing transactional streams . generally , mind-maps consist of symbolic cells that are connected with each other and that become either stronger or weaker depending on the transactional stream . based on the underlying biologic principle , these symbolic cells and their connections as well may adaptively survive or die , forming different cell agglomerates of arbitrary size . in this work , we intend to prove mind-maps ' eligibility following diverse application scenarios , for example being an underlying management system to represent normal and abnormal traffic behaviour in computer networks , supporting the detection of the user behaviour within search engines , or being a hidden communication layer for natural language interaction .", "topics": ["natural language", "map"]}
{"title": "structured sequence modeling with graph convolutional recurrent networks", "abstract": "this paper introduces graph convolutional recurrent network ( gcrn ) , a deep learning model able to predict structured sequences of data . precisely , gcrn is a generalization of classical recurrent neural networks ( rnn ) to data structured by an arbitrary graph . such structured sequences can represent series of frames in videos , spatio-temporal measurements on a network of sensors , or random walks on a vocabulary graph for natural language modeling . the proposed model combines convolutional neural networks ( cnn ) on graphs to identify spatial structures and rnn to find dynamic patterns . we study two possible architectures of gcrn , and apply the models to two practical problems : predicting moving mnist data , and modeling natural language with the penn treebank dataset . experiments show that exploiting simultaneously graph spatial and dynamic information about data can improve both precision and learning speed .", "topics": ["recurrent neural network", "natural language"]}
{"title": "why regularized auto-encoders learn sparse representation ?", "abstract": "while the authors of batch normalization ( bn ) identify and address an important problem involved in training deep networks -- \\textit { internal covariate shift } -- the current solution has certain drawbacks . for instance , bn depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input ( distribution ) to hidden layers inaccurate due to shifting parameter values ( especially during initial training epochs ) . another fundamental problem with bn is that it can not be used with batch-size $ 1 $ during training . we address these drawbacks of bn by proposing a non-adaptive normalization technique for removing covariate shift , that we call \\textit { normalization propagation } . our approach does not depend on batch statistics , but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with bn . we exploit the observation that the pre-activation before rectified linear units follow gaussian distribution in deep networks , and that once the first and second order statistics of any given dataset are normalized , we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "active semi-supervised learning using submodular functions", "abstract": "we consider active , semi-supervised learning in an offline transductive setting . we show that a previously proposed error bound for active learning on undirected weighted graphs can be generalized by replacing graph cut with an arbitrary symmetric submodular function . arbitrary non-symmetric submodular functions can be used via symmetrization . different choices of submodular functions give different versions of the error bound that are appropriate for different kinds of problems . moreover , the bound is deterministic and holds for adversarially chosen labels . we show exactly minimizing this error bound is np-complete . however , we also introduce for any submodular function an associated active semi-supervised learning method that approximately minimizes the corresponding error bound . we show that the error bound is tight in the sense that there is no other bound of the same form which is better . our theoretical results are supported by experiments on real data .", "topics": ["supervised learning"]}
{"title": "peephole : predicting network performance before training", "abstract": "the quest for performant networks has been a significant force that drives the advancements of deep learning in recent years . while rewarding , improving network design has never been an easy journey . the large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor . in this work , we propose a new approach to this problem , namely , predicting the performance of a network before training , based on its architecture . specifically , we develop a unified way to encode individual layers into vectors and bring them together to form an integrated description via lstm . taking advantage of the recurrent network 's strong expressive power , this method can reliably predict the performances of various network architectures . our empirical studies showed that it not only achieved accurate predictions but also produced consistent rankings across datasets -- a key desideratum in performance prediction .", "topics": ["recurrent neural network"]}
{"title": "reinforcement learning with linear function approximation and lq control converges", "abstract": "reinforcement learning is commonly used with function approximation . however , very few positive results are known about the convergence of function approximation based rl control algorithms . in this paper we show that td ( 0 ) and sarsa ( 0 ) with linear function approximation is convergent for a simple class of problems , where the system is linear and the costs are quadratic ( the lq control problem ) . furthermore , we show that for systems with gaussian noise and non-completely observable states ( the lqg problem ) , the mentioned rl algorithms are still convergent , if they are combined with kalman filtering .", "topics": ["reinforcement learning", "approximation"]}
{"title": "latent hierarchical model for activity recognition", "abstract": "we present a novel hierarchical model for human activity recognition . in contrast to approaches that successively recognize actions and activities , our approach jointly models actions and activities in a unified framework , and their labels are simultaneously predicted . the model is embedded with a latent layer that is able to capture a richer class of contextual information in both state-state and observation-state pairs . although loops are present in the model , the model has an overall linear-chain structure , where the exact inference is tractable . therefore , the model is very efficient in both inference and learning . the parameters of the graphical model are learned with a structured support vector machine ( structured-svm ) . a data-driven approach is used to initialize the latent variables ; therefore , no manual labeling for the latent states is required . the experimental results from using two benchmark datasets show that our model outperforms the state-of-the-art approach , and our model is computationally more efficient .", "topics": ["graphical model", "support vector machine"]}
{"title": "active learning for online recognition of human activities from streaming videos", "abstract": "recognising human activities from streaming videos poses unique challenges to learning algorithms : predictive models need to be scalable , incrementally trainable , and must remain bounded in size even when the data stream is arbitrarily long . furthermore , as parameter tuning is problematic in a streaming setting , suitable approaches should be parameterless , and make no assumptions on what class labels may occur in the stream . we present here an approach to the recognition of human actions from streaming data which meets all these requirements by : ( 1 ) incrementally learning a model which adaptively covers the feature space with simple local classifiers ; ( 2 ) employing an active learning strategy to reduce annotation requests ; ( 3 ) achieving promising accuracy within a fixed model size . extensive experiments on standard benchmarks show that our approach is competitive with state-of-the-art non-incremental methods , and outperforms the existing active incremental baselines .", "topics": ["baseline ( configuration management )", "feature vector"]}
{"title": "concept modeling with superwords", "abstract": "in information retrieval , a fundamental goal is to transform a document into concepts that are representative of its content . the term `` representative '' is in itself challenging to define , and various tasks require different granularities of concepts . in this paper , we aim to model concepts that are sparse over the vocabulary , and that flexibly adapt their content based on other relevant semantic information such as textual structure or associated image features . we explore a bayesian nonparametric model based on nested beta processes that allows for inferring an unknown number of strictly sparse concepts . the resulting model provides an inherently different representation of concepts than a standard lda ( or hdp ) based topic model , and allows for direct incorporation of semantic features . we demonstrate the utility of this representation on multilingual blog data and the congressional record .", "topics": ["sparse matrix"]}
{"title": "a unified multiscale framework for discrete energy minimization", "abstract": "discrete energy minimization is a ubiquitous task in computer vision , yet is np-hard in most cases . in this work we propose a multiscale framework for coping with the np-hardness of discrete optimization . our approach utilizes algebraic multiscale principles to efficiently explore the discrete solution space , yielding improved results on challenging , non-submodular energies for which current methods provide unsatisfactory approximations . in contrast to popular multiscale methods in computer vision , that builds an image pyramid , our framework acts directly on the energy to construct an energy pyramid . deriving a multiscale scheme from the energy itself makes our framework application independent and widely applicable . our framework gives rise to two complementary energy coarsening strategies : one in which coarser scales involve fewer variables , and a more revolutionary one in which the coarser scales involve fewer discrete labels . we empirically evaluated our unified framework on a variety of both non-submodular and submodular energies , including energies from middlebury benchmark .", "topics": ["computer vision", "approximation"]}
{"title": "ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "abstract": "sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document ( typically a consumer review ) . in the simplest settings , we discriminate only between positive and negative sentiment , turning the task into a standard binary classification problem . we compare several ma- chine learning approaches to this problem , and combine them to achieve the best possible results . we show how to use for this task the standard generative lan- guage models , which are slightly complementary to the state of the art techniques . we achieve strong results on a well-known dataset of imdb movie reviews . our results are easily reproducible , as we publish also the code needed to repeat the experiments . this should simplify further advance of the state of the art , as other researchers can combine their techniques with ours with little effort .", "topics": ["natural language processing"]}
{"title": "recognizing semantic features in faces using deep learning", "abstract": "the human face constantly conveys information , both consciously and subconsciously . however , as basic as it is for humans to visually interpret this information , it is quite a big challenge for machines . conventional semantic facial feature recognition and analysis techniques are already in use and are based on physiological heuristics , but they suffer from lack of robustness and high computation time . this thesis aims to explore ways for machines to learn to interpret semantic information available in faces in an automated manner without requiring manual design of feature detectors , using the approach of deep learning . this thesis provides a study of the effects of various factors and hyper-parameters of deep neural networks in the process of determining an optimal network configuration for the task of semantic facial feature recognition . this thesis explores the effectiveness of the system to recognize the various semantic features ( like emotions , age , gender , ethnicity etc . ) present in faces . furthermore , the relation between the effect of high-level concepts on low level features is explored through an analysis of the similarities in low-level descriptors of different semantic features . this thesis also demonstrates a novel idea of using a deep network to generate 3-d active appearance models of faces from real-world 2-d images . for a more detailed report on this work , please see [ arxiv:1512.00743v1 ] .", "topics": ["high- and low-level", "time complexity"]}
{"title": "characterisation of ( sub ) sequential rational functions over a general class monoids", "abstract": "in this technical report we describe a general class of monoids for which ( sub ) sequential rational can be characterised in terms of a congruence relation in the flavour of myhill-nerode relation . the class of monoids that we consider can be described in terms of natural algebraic axioms , contains the free monoids , groups , the tropical monoid , and is closed under cartesian .", "topics": ["natural language processing", "machine translation"]}
{"title": "from data to city indicators : a knowledge graph for supporting automatic generation of dashboards", "abstract": "in the context of smart cities , indicator definitions have been used to calculate values that enable the comparison among different cities . the calculation of an indicator values has challenges as the calculation may need to combine some aspects of quality while addressing different levels of abstraction . knowledge graphs ( kgs ) have been used successfully to support flexible representation , which can support improved understanding and data analysis in similar settings . this paper presents an operational description for a city kg , an indicator ontology that support indicator discovery and data visualization and an application capable of performing metadata analysis to automatically build and display dashboards according to discovered indicators . we describe our implementation in an urban mobility setting .", "topics": ["value ( ethics )"]}
{"title": "deep recurrent neural networks for acoustic modelling", "abstract": "we present a novel deep recurrent neural network ( rnn ) model for acoustic modelling in automatic speech recognition ( asr ) . we term our contribution as a tc-dnn-blstm-dnn model , the model combines a deep neural network ( dnn ) with time convolution ( tc ) , followed by a bidirectional long short-term memory ( blstm ) , and a final dnn . the first dnn acts as a feature processor to our model , the blstm then generates a context from the sequence acoustic signal , and the final dnn takes the context and models the posterior probabilities of the acoustic states . we achieve a 3.47 wer on the wall street journal ( wsj ) eval92 task or more than 8 % relative improvement over the baseline dnn models .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "reas : combining numerical optimization with sat solving", "abstract": "in this paper , we present reas , a technique that combines numerical optimization with sat solving to synthesize unknowns in a program that involves discrete and floating point computation . reas makes the program end-to-end differentiable by smoothing any boolean expression that introduces discontinuity such as conditionals and relaxing the boolean unknowns so that numerical optimization can be performed . on top of this , reas uses a sat solver to help the numerical search overcome local solutions by incrementally fixing values to the boolean expressions . we evaluated the approach on 5 case studies involving hybrid systems and show that reas can synthesize programs that could not be solved by previous smt approaches .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "generalized nonconvex nonsmooth low-rank minimization", "abstract": "as surrogate functions of $ l_0 $ -norm , many nonconvex penalty functions have been proposed to enhance the sparse vector recovery . it is easy to extend these nonconvex penalty functions on singular values of a matrix to enhance low-rank matrix recovery . however , different from convex optimization , solving the nonconvex low-rank minimization problem is much more challenging than the nonconvex sparse minimization problem . we observe that all the existing nonconvex penalty functions are concave and monotonically increasing on $ [ 0 , \\infty ) $ . thus their gradients are decreasing functions . based on this property , we propose an iteratively reweighted nuclear norm ( irnn ) algorithm to solve the nonconvex nonsmooth low-rank minimization problem . irnn iteratively solves a weighted singular value thresholding ( wsvt ) problem . by setting the weight vector as the gradient of the concave penalty function , the wsvt problem has a closed form solution . in theory , we prove that irnn decreases the objective function value monotonically , and any limit point is a stationary point . extensive experiments on both synthetic data and real images demonstrate that irnn enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms .", "topics": ["optimization problem", "loss function"]}
{"title": "exploiting cross-document relations for multi-document evolving summarization", "abstract": "this paper presents a methodology for summarization from multiple documents which are about a specific topic . it is based on the specification and identification of the cross-document relations that occur among textual elements within those documents . our methodology involves the specification of the topic-specific entities , the messages conveyed for the specific entities by certain textual elements and the specification of the relations that can hold among these messages . the above resources are necessary for setting up a specific topic for our query-based summarization approach which uses these resources to identify the query-specific messages within the documents and the query-specific relations that connect these messages across documents .", "topics": ["entity"]}
{"title": "a unifying view of explicit and implicit feature maps for structured data : systematic studies of graph kernels", "abstract": "non-linear kernel methods can be approximated by fast linear ones using suitable explicit feature maps allowing their application to large scale problems . to this end , explicit feature maps of kernels for vectorial data have been extensively studied . as many real-world data is structured , various kernels for complex data like graphs have been proposed . indeed , many of them directly compute feature maps . however , the kernel trick is employed when the number of features is very large or the individual vertices of graphs are annotated by real-valued attributes . can we still compute explicit feature maps efficiently under these circumstances ? triggered by this question , we investigate how general convolution kernels are composed from base kernels and construct corresponding feature maps . we apply our results to widely used graph kernels and analyze for which kernels and graph properties computation by explicit feature maps is feasible and actually more efficient . in particular , we derive feature maps for random walk and subgraph matching kernels and apply them to real-world graphs with discrete labels . thereby , our theoretical results are confirmed experimentally by observing a phase transition when comparing running time with respect to label diversity , walk lengths and subgraph size , respectively . moreover , we derive approximative , explicit feature maps for state-of-the-art kernels supporting real-valued attributes including the graphhopper and graph invariant kernels . in extensive experiments we show that our approaches often achieve a classification accuracy close to the exact methods based on the kernel trick , but require only a fraction of their running time .", "topics": ["time complexity", "map"]}
{"title": "dynamic bayesian ontology languages", "abstract": "many formalisms combining ontology languages with uncertainty , usually in the form of probabilities , have been studied over the years . most of these formalisms , however , assume that the probabilistic structure of the knowledge remains static over time . we present a general approach for extending ontology languages to handle time-evolving uncertainty represented by a dynamic bayesian network . we show how reasoning in the original language and dynamic bayesian inferences can be exploited for effective reasoning in our framework .", "topics": ["bayesian network"]}
{"title": "generic 3d representation via pose estimation and matching", "abstract": "though a large body of computer vision research has investigated developing generic semantic representations , efforts towards developing a similar representation for 3d has been limited . in this paper , we learn a generic 3d representation through solving a set of foundational proxy 3d tasks : object-centric camera pose estimation and wide baseline feature matching . our method is based upon the premise that by providing supervision over a set of carefully selected foundational tasks , generalization to novel tasks and abstraction capabilities can be achieved . we empirically show that the internal representation of a multi-task convnet trained to solve the above core problems generalizes to novel 3d tasks ( e.g . , scene layout estimation , object pose estimation , surface normal estimation ) without the need for fine-tuning and shows traits of abstraction abilities ( e.g . , cross-modality pose estimation ) . in the context of the core supervised tasks , we demonstrate our representation achieves state-of-the-art wide baseline feature matching results without requiring apriori rectification ( unlike sift and the majority of learned features ) . we also show 6dof camera pose estimation given a pair local image patches . the accuracy of both supervised tasks come comparable to humans . finally , we contribute a large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information , and conclude with a discussion on the learned representation and open research questions .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "indoor localization by fusing a group of fingerprints based on random forests", "abstract": "indoor localization based on single of fingerprint ( siof ) is rather susceptible to the changing environment , multipath , and non-line-of-sight ( nlos ) propagation . building siof is also a very time-consuming process . recently , we first proposed a group of fingerprints ( goof ) to improve the localization accuracy and reduce the burden of building fingerprints . however , the main drawback is the timeliness . in this paper , we propose a novel localization framework by fusing a group of fingerprints ( fagot ) based on random forests . in the offline phase , we first build a goof from different transformations of the received signals of multiple antennas . then , we design multiple goof strong classifiers based on random forests ( goof-rf ) by training each fingerprint in the goof . in the online phase , we input the corresponding transformations of the real measurements into these strong classifiers to obtain multiple independent decisions . finally , we propose a sliding window aided mode-based ( swim ) fusion algorithm to balance the localization accuracy and time . our proposed approaches can work better in an unknown indoor scenario . the burden of building fingerprints can also be reduced drastically . we demonstrate the performance of our algorithms through simulations and real experimental data using two universal software radio peripheral ( usrp ) platforms .", "topics": ["simulation"]}
{"title": "rapid learning with stochastic focus of attention", "abstract": "we present a method to stop the evaluation of a decision making process when the result of the full evaluation is obvious . this trait is highly desirable for online margin-based machine learning algorithms where a classifier traditionally evaluates all the features for every example . we observe that some examples are easier to classify than others , a phenomenon which is characterized by the event when most of the features agree on the class of an example . by stopping the feature evaluation when encountering an easy to classify example , the learning algorithm can achieve substantial gains in computation . our method provides a natural attention mechanism for learning algorithms . by modifying pegasos , a margin-based online learning algorithm , to include our attentive method we lower the number of attributes computed from $ n $ to an average of $ o ( \\sqrt { n } ) $ features without loss in prediction accuracy . we demonstrate the effectiveness of attentive pegasos on mnist data .", "topics": ["computation", "mnist database"]}
{"title": "adaptive convolutional elm for concept drift handling in online stream data", "abstract": "in big data era , the data continuously generated and its distribution may keep changes overtime . these challenges in online stream of data are known as concept drift . in this paper , we proposed the adaptive convolutional elm method ( acnnelm ) as enhancement of convolutional neural network ( cnn ) with a hybrid extreme learning machine ( elm ) model plus adaptive capability . this method is aimed for concept drift handling . we enhanced the cnn as convolutional hiererchical features representation learner combined with elastic elm ( e $ ^2 $ lm ) as a parallel supervised classifier . we propose an adaptive os-elm ( aos-elm ) for concept drift adaptability in classifier level ( named acnnelm-1 ) and matrices concatenation ensembles for concept drift adaptability in ensemble level ( named acnnelm-2 ) . our proposed adaptive cnnelm is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels . we verified our method in extended mnist data set and not mnist data set . we set the experiment to simulate virtual drift , real drift , and hybrid drift event and we demonstrated how our cnnelm adaptability works . our proposed method works well and gives better accuracy , computation scalability , and concept drifts adaptability compared to the regular elm and cnn . further researches are still required to study the optimum parameters and to use more varied image data set .", "topics": ["computation", "scalability"]}
{"title": "a sensitivity analysis of ( and practitioners ' guide to ) convolutional neural networks for sentence classification", "abstract": "convolutional neural networks ( cnns ) have recently achieved remarkably strong performance on the practically important task of sentence classification ( kim 2014 , kalchbrenner 2014 , johnson 2014 ) . however , these models require practitioners to specify an exact model architecture and set accompanying hyperparameters , including the filter region size , regularization parameters , and so on . it is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification . we thus conduct a sensitivity analysis of one-layer cnns to explore the effect of architecture components on model performance ; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification . we focus on one-layer cnns ( to the exclusion of more complex models ) due to their comparative simplicity and strong empirical performance , which makes it a modern standard baseline method akin to support vector machine ( svms ) and logistic regression . we derive practical advice from our extensive empirical results for those interested in getting the most out of cnns for sentence classification in real world settings .", "topics": ["neural networks", "matrix regularization"]}
{"title": "`` zero-shot '' super-resolution using deep internal learning", "abstract": "deep learning has led to a dramatic leap in super-resolution ( sr ) performance in the past few years . however , being supervised , these sr methods are restricted to specific training data , where the acquisition of the low-resolution ( lr ) images from their high-resolution ( hr ) counterparts is predetermined ( e.g . , bicubic downscaling ) , without any distracting artifacts ( e.g . , sensor noise , image compression , non-ideal psf , etc ) . real lr images , however , rarely obey these restrictions , resulting in poor sr results by sota ( state of the art ) methods . in this paper we introduce `` zero-shot '' sr , which exploits the power of deep learning , but does not rely on prior training . we exploit the internal recurrence of information inside a single image , and train a small image-specific cnn at test time , on examples extracted solely from the input image itself . as such , it can adapt itself to different settings per image . this allows to perform sr of real old photos , noisy images , biological data , and other images where the acquisition process is unknown or non-ideal . on such images , our method outperforms sota cnn-based sr methods , as well as previous unsupervised sr methods . to the best of our knowledge , this is the first unsupervised cnn-based sr method .", "topics": ["test set", "unsupervised learning"]}
{"title": "efficient contextual bandits in non-stationary worlds", "abstract": "most contextual bandit algorithms minimize regret against the best fixed policy , a questionable benchmark for non-stationary environments that are ubiquitous in applications . in this work , we develop several efficient contextual bandit algorithms for non-stationary environments by equipping existing methods for i.i.d . problems with sophisticated statistical tests so as to dynamically adapt to a change in distribution . we analyze various standard notions of regret suited to non-stationary environments for these algorithms , including interval regret , switching regret , and dynamic regret . when competing with the best policy at each time , one of our algorithms achieves regret $ \\mathcal { o } ( \\sqrt { st } ) $ if there are $ t $ rounds with $ s $ stationary periods , or more generally $ \\mathcal { o } ( \\delta^ { 1/3 } t^ { 2/3 } ) $ where $ \\delta $ is some non-stationarity measure . these results almost match the optimal guarantees achieved by an inefficient baseline that is a variant of the classic exp4 algorithm . the dynamic regret result is also the first one for efficient and fully adversarial contextual bandit . furthermore , while the results above require tuning a parameter based on the unknown quantity $ s $ or $ \\delta $ , we also develop a parameter free algorithm achieving regret $ \\min\\ { s^ { 1/4 } t^ { 3/4 } , \\delta^ { 1/5 } t^ { 4/5 } \\ } $ . this improves and generalizes the best existing result $ \\delta^ { 0.18 } t^ { 0.82 } $ by karnin and anava ( 2016 ) which only holds for the two-armed bandit problem .", "topics": ["regret ( decision theory )"]}
{"title": "multiple operator-valued kernel learning", "abstract": "positive definite operator-valued kernels generalize the well-known notion of reproducing kernels , and are naturally adapted to multi-output learning situations . this paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts . we study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients . the resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues . we propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinatedescent procedure . we experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces .", "topics": ["kernel ( operating system )", "optimization problem"]}
{"title": "difference target propagation", "abstract": "back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects ( partial derivatives ) in order to perform credit assignment . this could become a serious issue as one considers deeper and more non-linear functions , e.g . , consider the extreme case of nonlinearity where the relation between parameters and cost is actually discrete . inspired by the biological implausibility of back-propagation , a few approaches have been proposed in the past that could play a similar credit assignment role . in this spirit , we explore a novel approach to credit assignment in deep networks that we call target propagation . the main idea is to compute targets rather than gradients , at each layer . like gradients , they are propagated backwards . in a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights , target propagation relies on auto-encoders at each layer . unlike back-propagation , it can be applied even when units exchange stochastic bits rather than real numbers . we show that a linear correction for the imperfectness of the auto-encoders , called difference target propagation , is very effective to make target propagation actually work , leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks .", "topics": ["nonlinear system", "noise reduction"]}
{"title": "barnes-hut-sne", "abstract": "the paper presents an o ( n log n ) -implementation of t-sne -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in o ( n^2 ) . the new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects , and it uses a variant of the barnes-hut algorithm - an algorithm used by astronomers to perform n-body simulations - to approximate the forces between the corresponding points in the embedding . our experiments show that the new algorithm , called barnes-hut-sne , leads to substantial computational advantages over standard t-sne , and that it makes it possible to learn embeddings of data sets with millions of objects .", "topics": ["approximation algorithm", "sparse matrix"]}
{"title": "efficient correlated topic modeling with topic embedding", "abstract": "correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling . in this paper , we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors . our method enables efficient inference in the low-dimensional embedding space , reducing previous cubic or quadratic time complexity to linear w.r.t the topic size . we further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence . extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results , without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "large-scale randomized-coordinate descent methods with non-separable linear constraints", "abstract": "we develop randomized ( block ) coordinate descent ( cd ) methods for linearly constrained convex optimization . unlike most cd methods , we do not assume the constraints to be separable , but let them be coupled linearly . to our knowledge , ours is the first cd method that allows linear coupling constraints , without making the global iteration complexity have an exponential dependence on the number of constraints . we present algorithms and analysis for four key problem scenarios : ( i ) smooth ; ( ii ) smooth + nonsmooth separable ; ( iii ) asynchronous parallel ; and ( iv ) stochastic . we illustrate empirical behavior of our algorithms by simulation experiments .", "topics": ["time complexity", "iteration"]}
{"title": "the integration of connectionism and first-order knowledge representation and reasoning as a challenge for artificial intelligence", "abstract": "intelligent systems based on first-order logic on the one hand , and on artificial neural networks ( also called connectionist systems ) on the other , differ substantially . it would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained . current state-of-the-art research , however , fails by far to achieve this ultimate goal . as one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems : satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems .", "topics": ["artificial intelligence"]}
{"title": "screening rules for overlapping group lasso", "abstract": "recently , to solve large-scale lasso and group lasso problems , screening rules have been developed , the goal of which is to reduce the problem size by efficiently discarding zero coefficients using simple rules independently of the others . however , screening for overlapping group lasso remains an open challenge because the overlaps between groups make it infeasible to test each group independently . in this paper , we develop screening rules for overlapping group lasso . to address the challenge arising from groups with overlaps , we take into account overlapping groups only if they are inclusive of the group being tested , and then we derive screening rules , adopting the dual polytope projection approach . this strategy allows us to screen each group independently of each other . in our experiments , we demonstrate the efficiency of our screening rules on various datasets .", "topics": ["coefficient"]}
{"title": "hd-cnn : hierarchical deep convolutional neural network for large scale visual recognition", "abstract": "in image classification , visual separability between different object categories is highly uneven , and some categories are more difficult to distinguish than others . such difficult categories demand more dedicated classifiers . however , existing deep convolutional neural networks ( cnn ) are trained as flat n-way classifiers , and few efforts have been made to leverage the hierarchical structure of categories . in this paper , we introduce hierarchical deep cnns ( hd-cnns ) by embedding deep cnns into a category hierarchy . an hd-cnn separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers . during hd-cnn training , component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term . in addition , conditional executions of fine category classifiers and layer parameter compression make hd-cnns scalable for large-scale visual recognition . we achieve state-of-the-art results on both cifar100 and large-scale imagenet 1000-class benchmark datasets . in our experiments , we build up three different hd-cnns and they lower the top-1 error of the standard cnns by 2.65 % , 3.1 % and 1.1 % , respectively .", "topics": ["sparse matrix"]}
{"title": "variational inference using implicit distributions", "abstract": "generative adversarial networks ( gans ) have given us a great tool to fit implicit generative models to data . implicit distributions are ones we can sample from easily , and take derivatives of samples with respect to model parameters . these models are highly expressive and we argue they can prove just as useful for variational inference ( vi ) as they are for generative modelling . several papers have proposed gan-like algorithms for inference , however , connections to the theory of vi are not always well understood . this paper provides a unifying review of existing algorithms establishing connections between variational autoencoders , adversarially learned inference , operator vi , gan-based image reconstruction , and more . secondly , the paper provides a framework for building new algorithms : depending on the way the variational bound is expressed we introduce prior-contrastive and joint-contrastive methods , and show practical inference algorithms based on either density ratio estimation or denoising .", "topics": ["calculus of variations", "noise reduction"]}
{"title": "active clustering with model-based uncertainty reduction", "abstract": "semi-supervised clustering seeks to augment traditional clustering methods by incorporating side information provided via human expertise in order to increase the semantic meaningfulness of the resulting clusters . however , most current methods are \\emph { passive } in the sense that the side information is provided beforehand and selected randomly . this may require a large number of constraints , some of which could be redundant , unnecessary , or even detrimental to the clustering results . thus in order to scale such semi-supervised algorithms to larger problems it is desirable to pursue an \\emph { active } clustering method -- -i.e . an algorithm that maximizes the effectiveness of the available human labor by only requesting human input where it will have the greatest impact . here , we propose a novel online framework for active semi-supervised spectral clustering that selects pairwise constraints as clustering proceeds , based on the principle of uncertainty reduction . using a first-order taylor expansion , we decompose the expected uncertainty reduction problem into a gradient and a step-scale , computed via an application of matrix perturbation theory and cluster-assignment entropy , respectively . the resulting model is used to estimate the uncertainty reduction potential of each sample in the dataset . we then present the human user with pairwise queries with respect to only the best candidate sample . we evaluate our method using three different image datasets ( faces , leaves and dogs ) , a set of common uci machine learning datasets and a gene dataset . the results validate our decomposition formulation and show that our method is consistently superior to existing state-of-the-art techniques , as well as being robust to noise and to unknown numbers of clusters .", "topics": ["cluster analysis", "gradient"]}
{"title": "speculation on graph computation architectures and computing via synchronization", "abstract": "a speculative overview of a future topic of research . the paper is a collection of ideas concerning two related areas : 1 ) graph computation machines ( `` computing with graphs '' ) . this is the class of models of computation in which the state of the computation is represented as a graph or network . 2 ) arc-based neural networks , which store information not as activation in the nodes , but rather by adding and deleting arcs . sometimes the arcs may be interpreted as synchronization . warnings to readers : this is not the sort of thing that one might submit to a journal or conference . no proofs are presented . the presentation is informal , and written at an introductory level . you 'll probably want to wait for a more concise presentation .", "topics": ["computation"]}
{"title": "learning dynamic feature selection for fast sequential prediction", "abstract": "we present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many nlp components . this is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features . parameter estimation is arranged to maximize accuracy and early confidence in this sequence . our approach is simpler and better suited to nlp than other related cascade methods . we present experiments in left-to-right part-of-speech tagging , named entity recognition , and transition-based dependency parsing . on the typical benchmarking datasets we can preserve pos tagging accuracy above 97 % and parsing las above 88.5 % both with over a five-fold reduction in run-time , and ner f1 above 88 with more than 2x increase in speed .", "topics": ["natural language processing", "parsing"]}
{"title": "aeknn : an autoencoder knn-based classifier with built-in dimensionality reduction", "abstract": "high dimensionality , i.e . data having a large number of variables , tends to be a challenge for most machine learning tasks , including classification . a classifier usually builds a model representing how a set of inputs explain the outputs . the larger is the set of inputs and/or outputs , the more complex would be that model . there is a family of classification algorithms , known as lazy learning methods , which does not build a model . one of the best known members of this family is the knn algorithm . its strategy relies on searching a set of nearest neighbors , using the input variables as position vectors and computing distances among them . these distances loss significance in high-dimensional spaces . therefore knn , as many other classifiers , tends to worse its performance as the number of input variables grows . in this work aeknn , a new knn-based algorithm with built-in dimensionality reduction , is presented . aiming to obtain a new representation of the data , having a lower dimensionality but with more informational features , aeknn internally uses autoencoders . from this new feature vectors the computed distances should be more significant , thus providing a way to choose better neighbors . a experimental evaluation of the new proposal is conducted , analyzing several configurations and comparing them against the classical knn algorithm . the obtained conclusions demonstrate that aeknn offers better results in predictive and runtime performance .", "topics": ["statistical classification", "feature vector"]}
{"title": "integrating document clustering and topic modeling", "abstract": "document clustering and topic modeling are two closely related tasks which can mutually benefit each other . topic modeling can project documents into a topic space which facilitates effective document clustering . cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters . in this paper , we propose a multi-grain clustering topic model ( mgctm ) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance . our model tightly couples two components : a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters.we employ variational inference to approximate the posterior of hidden variables and learn model parameters . experiments on two datasets demonstrate the effectiveness of our model .", "topics": ["cluster analysis", "calculus of variations"]}
{"title": "weighted parallel sgd for distributed unbalanced-workload training system", "abstract": "stochastic gradient descent ( sgd ) is a popular stochastic optimization method in machine learning . traditional parallel sgd algorithms , e.g . , simuparallel sgd , often require all nodes to have the same performance or to consume equal quantities of data . however , these requirements are difficult to satisfy when the parallel sgd algorithms run in a heterogeneous computing environment ; low-performance nodes will exert a negative influence on the final result . in this paper , we propose an algorithm called weighted parallel sgd ( wp-sgd ) . wp-sgd combines weighted model parameters from different nodes in the system to produce the final output . wp-sgd makes use of the reduction in standard deviation to compensate for the loss from the inconsistency in performance of nodes in the cluster , which means that wp-sgd does not require that all nodes consume equal quantities of data . we also analyze the theoretical feasibility of running two other parallel sgd algorithms combined with wp-sgd in a heterogeneous environment . the experimental results show that wp-sgd significantly outperforms the traditional parallel sgd algorithms on distributed training systems with an unbalanced workload .", "topics": ["gradient descent", "gradient"]}
{"title": "parallel gaussian process optimization with upper confidence bound and pure exploration", "abstract": "in this paper , we consider the challenge of maximizing an unknown function f for which evaluations are noisy and are acquired with high cost . an iterative procedure uses the previous measures to actively select the next estimation of f which is predicted to be the most useful . we focus on the case where the function can be evaluated in parallel with batches of fixed size and analyze the benefit compared to the purely sequential procedure in terms of cumulative regret . we introduce the gaussian process upper confidence bound and pure exploration algorithm ( gp-ucb-pe ) which combines the ucb strategy and pure exploration in the same batch of evaluations along the parallel iterations . we prove theoretical upper bounds on the regret with batches of size k for this procedure which show the improvement of the order of sqrt { k } for fixed iteration cost over purely sequential versions . moreover , the multiplicative constants involved have the property of being dimension-free . we also confirm empirically the efficiency of gp-ucb-pe on real and synthetic problems compared to state-of-the-art competitors .", "topics": ["regret ( decision theory )", "synthetic data"]}
{"title": "classification-based rnn machine translation using grus", "abstract": "we report the results of our classification-based machine translation model , built upon the framework of a recurrent neural network using gated recurrent units . unlike other rnn models that attempt to maximize the overall conditional log probability of sentences against sentences , our model focuses a classification approach of estimating the conditional probability of the next word given the input sequence . this simpler approach using grus was hoped to be comparable with more complicated rnn models , but achievements in this implementation were modest and there remains a lot of room for improving this classification approach .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "modular multitask reinforcement learning with policy sketches", "abstract": "we describe a framework for multitask deep reinforcement learning guided by policy sketches . sketches annotate tasks with sequences of named subtasks , providing information about high-level structural relationships among tasks but not how to implement them -- -specifically not providing the detailed guidance used by much previous work on learning policy abstractions for rl ( e.g . intermediate rewards , subtask completion signals , or intrinsic motivations ) . to learn from sketches , we present a model that associates every subtask with a modular subpolicy , and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies . optimization is accomplished via a decoupled actor -- critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions . we evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control , and with sparse rewards that can be obtained only after completing a number of high-level subgoals . experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies , while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks .", "topics": ["baseline ( configuration management )", "high- and low-level"]}
{"title": "kernelized weighted susan based fuzzy c-means clustering for noisy image segmentation", "abstract": "the paper proposes a novel kernelized image segmentation scheme for noisy images that utilizes the concept of smallest univalue segment assimilating nucleus ( susan ) and incorporates spatial constraints by computing circular colour map induced weights . fuzzy damping coefficients are obtained for each nucleus or center pixel on the basis of the corresponding weighted susan area values , the weights being equal to the inverse of the number of horizontal and vertical moves required to reach a neighborhood pixel from the center pixel . these weights are used to vary the contributions of the different nuclei in the kernel based framework . the paper also presents an edge quality metric obtained by fuzzy decision based edge candidate selection and final computation of the blurriness of the edges after their selection . the inability of existing algorithms to preserve edge information and structural details in their segmented maps necessitates the computation of the edge quality factor ( eqf ) for all the competing algorithms . qualitative and quantitative analysis have been rendered with respect to state-of-the-art algorithms and for images ridden with varying types of noises . speckle noise ridden sar images and rician noise ridden magnetic resonance images have also been considered for evaluating the effectiveness of the proposed algorithm in extracting important segmentation information .", "topics": ["kernel ( operating system )", "image segmentation"]}
{"title": "stochastic gradient mcmc with stale gradients", "abstract": "stochastic gradient mcmc ( sg-mcmc ) has played an important role in large-scale bayesian learning , with well-developed theoretical convergence properties . in such applications of sg-mcmc , it is becoming increasingly popular to employ distributed systems , where stochastic gradients are computed based on some outdated parameters , yielding what are termed stale gradients . while stale gradients could be directly used in sg-mcmc , their impact on convergence properties has not been well studied . in this paper we develop theory to show that while the bias and mse of an sg-mcmc algorithm depend on the staleness of stochastic gradients , its estimation variance ( relative to the expected estimate , based on a prescribed number of samples ) is independent of it . in a simple bayesian distributed system with sg-mcmc , where stale gradients are computed asynchronously by a set of workers , our theory indicates a linear speedup on the decrease of estimation variance w.r.t . the number of workers . experiments on synthetic data and deep neural networks validate our theory , demonstrating the effectiveness and scalability of sg-mcmc with stale gradients .", "topics": ["synthetic data", "gradient"]}
{"title": "neural predictive coding using convolutional neural networks towards unsupervised learning of speaker characteristics", "abstract": "learning speaker-specific features is vital in many applications like speaker recognition , diarization and speech recognition . this paper provides a novel approach , we term neural predictive coding ( npc ) , to learn speaker-specific characteristics in a completely unsupervised manner from large amounts of unlabeled training data that even contain multi-speaker audio streams . the npc framework exploits the proposed short-term active-speaker stationarity hypothesis which assumes two temporally-close short speech segments belong to the same speaker , and thus a common representation that can encode the commonalities of both the segments , should capture the vocal characteristics of that speaker . we train a convolutional deep siamese network to produce `` speaker embeddings '' by optimizing a loss function that increases between-speaker variability and decreases within-speaker variability . the trained npc model can produce these embeddings by projecting any test audio stream into a high dimensional manifold where speech frames of the same speaker come closer than they do in the raw feature space . results in the frame-level speaker classification experiment along with the visualization of the embeddings manifest the distinctive ability of the npc model to learn short-term speaker-specific features as compared to raw mfcc features and i-vectors . the utterance-level speaker classification experiments show that concatenating simple statistics of the short-term npc embeddings over the whole utterance with the utterance-level i-vectors can give useful complimentary information to the i-vectors and boost the classification accuracy . the results also show the efficacy of this technique to learn those characteristics from large amounts of unlabeled training set which has no prior information about the environment of the test set .", "topics": ["test set", "feature vector"]}
{"title": "adaptive online sequential elm for concept drift tackling", "abstract": "a machine learning method needs to adapt to over time changes in the environment . such changes are known as concept drift . in this paper , we propose concept drift tackling method as an enhancement of online sequential extreme learning machine ( os-elm ) and constructive enhancement os-elm ( ceos-elm ) by adding adaptive capability for classification and regression problem . the scheme is named as adaptive os-elm ( aos-elm ) . it is a single classifier scheme that works well to handle real drift , virtual drift , and hybrid drift . the aos-elm also works well for sudden drift and recurrent context change type . the scheme is a simple unified method implemented in simple lines of code . we evaluated aos-elm on regression and classification problem by using concept drift public data set ( sea and stagger ) and other public data sets such as mnist , usps , and ids . experiments show that our method gives higher kappa value compared to the multiclassifier elm ensemble . even though aos-elm in practice does not need hidden nodes increase , we address some issues related to the increasing of the hidden nodes such as error condition and rank values . we propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect underfitting condition .", "topics": ["mnist database"]}
{"title": "on the convergence and consistency of the blurring mean-shift process", "abstract": "the mean-shift algorithm is a popular algorithm in computer vision and image processing . it can also be cast as a minimum gamma-divergence estimation . in this paper we focus on the `` blurring '' mean shift algorithm , which is one version of the mean-shift process that successively blurs the dataset . the analysis of the blurring mean-shift is relatively more complicated compared to the nonblurring version , yet the algorithm convergence and the estimation consistency have not been well studied in the literature . in this paper we prove both the convergence and the consistency of the blurring mean-shift . we also perform simulation studies to compare the efficiency of the blurring and the nonblurring versions of the mean-shift algorithms . our results show that the blurring mean-shift has more efficiency .", "topics": ["image processing", "computer vision"]}
{"title": "fast adaptive weight noise", "abstract": "marginalising out uncertain quantities within the internal representations or parameters of neural networks is of central importance for a wide range of learning techniques , such as empirical , variational or full bayesian methods . we set out to generalise fast dropout ( wang & manning , 2013 ) to cover a wider variety of noise processes in neural networks . this leads to an efficient calculation of the marginal likelihood and predictive distribution which evades sampling and the consequential increase in training time due to highly variant gradient estimates . this allows us to approximate variational bayes for the parameters of feed-forward neural networks . inspired by the minimum description length principle , we also propose and experimentally verify the direct optimisation of the regularised predictive distribution . the methods yield results competitive with previous neural network based approaches and gaussian processes on a wide range of regression tasks .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "clustering is difficult only when it does not matter", "abstract": "numerous papers ask how difficult it is to cluster data . we suggest that the more relevant and interesting question is how difficult it is to cluster data sets { \\em that can be clustered well } . more generally , despite the ubiquity and the great importance of clustering , we still do not have a satisfactory mathematical theory of clustering . in order to properly understand clustering , it is clearly necessary to develop a solid theoretical basis for the area . for example , from the perspective of computational complexity theory the clustering problem seems very hard . numerous papers introduce various criteria and numerical measures to quantify the quality of a given clustering . the resulting conclusions are pessimistic , since it is computationally difficult to find an optimal clustering of a given data set , if we go by any of these popular criteria . in contrast , the practitioners ' perspective is much more optimistic . our explanation for this disparity of opinions is that complexity theory concentrates on the worst case , whereas in reality we only care for data sets that can be clustered well . we introduce a theoretical framework of clustering in metric spaces that revolves around a notion of `` good clustering '' . we show that if a good clustering exists , then in many cases it can be efficiently found . our conclusion is that contrary to popular belief , clustering should not be considered a hard task .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "a mention-ranking model for abstract anaphora resolution", "abstract": "resolving abstract anaphora is an important , but difficult task for text understanding . yet , with recent advances in representation learning this task becomes a more tangible aim . a central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its ( typically non-nominal ) antecedent . we propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an lstm-siamese net . we overcome the lack of training data by generating artificial anaphoric sentence -- antecedent pairs . our model outperforms state-of-the-art results on shell noun resolution . we also report first benchmark results on an abstract anaphora subset of the arrau corpus . this corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders . we found model variants that outperform the baselines for nominal anaphors , without training on individual anaphor data , but still lag behind for pronominal anaphors . our model selects syntactically plausible candidates and -- if disregarding syntax -- discriminates candidates using deeper features .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "deep directed generative models with energy-based probability estimation", "abstract": "training energy-based probabilistic models is confronted with apparently intractable sums , whose monte carlo estimation requires sampling from the estimated probability distribution in the inner loop of training . this can be approximately achieved by markov chain monte carlo methods , but may still face a formidable obstacle that is the difficulty of mixing between modes with sharp concentrations of probability . whereas an mcmc process is usually derived from a given energy function based on mathematical considerations and requires an arbitrarily long time to obtain good and varied samples , we propose to train a deep directed generative model ( not a markov chain ) so that its sampling distribution approximately matches the energy function that is being trained . inspired by generative adversarial networks , the proposed framework involves training of two models that represent dual views of the estimated probability distribution : the energy function ( mapping an input configuration to a scalar energy value ) and the generator ( mapping a noise vector to a generated configuration ) , both represented by deep neural networks .", "topics": ["sampling ( signal processing )", "mathematical optimization"]}
{"title": "superspike : supervised learning in multi-layer spiking neural networks", "abstract": "a vast majority of computation in the brain is performed by spiking neural networks . despite the ubiquity of such spiking , we currently lack an understanding of how biological spiking neural circuits learn and compute in-vivo , as well as how we can instantiate such capabilities in artificial spiking circuits in-silico . here we revisit the problem of supervised learning in temporally coding multi-layer spiking neural networks . first , by using a surrogate gradient approach , we derive superspike , a nonlinear voltage-based three factor learning rule capable of training multi-layer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns . second , inspired by recent results on feedback alignment , we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units . specifically , we test uniform , symmetric and random feedback , finding that simpler tasks can be solved with any type of feedback , while more complex tasks require symmetric feedback . in summary , our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike-time patterns .", "topics": ["supervised learning", "nonlinear system"]}
{"title": "an automated compatibility prediction engine using disc theory based classification and neural networks", "abstract": "traditionally psychometric tests were used for profiling incoming workers . these methods use disc profiling method to classify people into distinct personality types , which are further used to predict if a person may be a possible fit to the organizational culture . this concept is taken further by introducing a novel technique to predict if a particular pair of an incoming worker and the manager being assigned are compatible at a psychological scale . this is done using multilayer perceptron neural network which can be adaptively trained to showcase the true nature of the compatibility index . the proposed prototype model is used to quantify the relevant attributes , use them to train the prediction engine , and to define the data pipeline required for it .", "topics": ["neural networks"]}
{"title": "as cool as a cucumber : towards a corpus of contemporary similes in serbian", "abstract": "similes are natural language expressions used to compare unlikely things , where the comparison is not taken literally . they are often used in everyday communication and are an important part of cultural heritage . having an up-to-date corpus of similes is challenging , as they are constantly coined and/or adapted to the contemporary times . in this paper we present a methodology for semi-automated collection of similes from the world wide web using text mining techniques . we expanded an existing corpus of traditional similes ( containing 333 similes ) by collecting 446 additional expressions . we , also , explore how crowdsourcing can be used to extract and curate new similes .", "topics": ["natural language", "text corpus"]}
{"title": "stochastic variance reduced multiplicative update for nonnegative matrix factorization", "abstract": "nonnegative matrix factorization ( nmf ) , a dimensionality reduction and factor analysis method , is a special case in which factor matrices have low-rank nonnegative constraints . considering the stochastic learning in nmf , we specifically address the multiplicative update ( mu ) rule , which is the most popular , but which has slow convergence property . this present paper introduces on the stochastic mu rule a variance-reduced technique of stochastic gradient . numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets .", "topics": ["numerical analysis", "synthetic data"]}
{"title": "developing bug-free machine learning systems with formal mathematics", "abstract": "noisy data , non-convex objectives , model misspecification , and numerical instability can all cause undesired behaviors in machine learning systems . as a result , detecting actual implementation errors can be extremely difficult . we demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct . the process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail . as a case study , we implement a new system , certigrad , for optimizing over stochastic computation graphs , and we generate a formal ( i.e . machine-checkable ) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients . we train a variational autoencoder using certigrad and find the performance comparable to training the same model in tensorflow .", "topics": ["calculus of variations", "computation"]}
{"title": "no reference stereoscopic video quality assessment using joint motion and depth statistics", "abstract": "we present a no reference ( nr ) quality assessment algorithm for assessing the perceptual quality of natural stereoscopic 3d ( s3d ) videos . this work is inspired by our finding that the joint statistics of the subband coefficients of motion ( optical flow or motion vector magnitude ) and depth ( disparity map ) of natural s3d videos possess a unique signature . specifically , we empirically show that the joint statistics of the motion and depth subband coefficients of s3d video frames can be modeled accurately using a bivariate generalized gaussian distribution ( bggd ) . we then demonstrate that the parameters of the bggd model possess the ability to discern quality variations in s3d videos . therefore , the bggd model parameters are employed as motion and depth quality features . in addition to these features , we rely on a frame level spatial quality feature that is computed using a robust off the shelf nr image quality assessment ( iqa ) algorithm . these frame level motion , depth and spatial features are consolidated and used with the corresponding s3d video 's difference mean opinion score ( dmos ) labels for supervised learning using support vector regression ( svr ) . the overall quality of an s3d video is computed by averaging the frame level quality predictions of the constituent video frames . the proposed algorithm , dubbed video quality evaluation using motion and depth statistics ( vquemodes ) is shown to outperform the state of the art methods when evaluated over the irccyn and lfovia s3d subjective quality assessment databases .", "topics": ["supervised learning", "support vector machine"]}
{"title": "generating sentences from a continuous space", "abstract": "the standard recurrent neural network language model ( rnnlm ) generates sentences one word at a time and does not work from an explicit global sentence representation . in this work , we introduce and study an rnn-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences . this factorization allows it to explicitly model holistic properties of sentences such as style , topic , and high-level syntactic features . samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding . by examining paths through this latent space , we are able to generate coherent novel sentences that interpolate between known sentences . we present techniques for solving the difficult learning problem presented by this model , demonstrate its effectiveness in imputing missing words , explore many interesting properties of the model 's latent sentence space , and present negative results on the use of the model in language modeling .", "topics": ["generative model", "calculus of variations"]}
{"title": "an introduction to deep visual explanation", "abstract": "the practical impact of deep learning on complex supervised learning problems has been significant , so much so that almost every artificial intelligence problem , or at least a portion thereof , has been somehow recast as a deep learning problem . the applications appeal is significant , but this appeal is increasingly challenged by what some call the challenge of explainability , or more generally the more traditional challenge of debuggability : if the outcomes of a deep learning process produce unexpected results ( e.g . , less than expected performance of a classifier ) , then there is little available in the way of theories or tools to help investigate the potential causes of such unexpected behavior , especially when this behavior could impact people 's lives . we describe a preliminary framework to help address this issue , which we call `` deep visual explanation '' ( dve ) . `` deep , '' because it is the development and performance of deep neural network models that we want to understand . `` visual , '' because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques , and `` explanation , '' because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses , we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model . in the exposition of our preliminary framework , we use relatively straightforward image classification examples and a variety of choices on initial configuration of a deep model building scenario . by careful but not complicated instrumentation , we expose classification outcomes of deep models using visualization , and also show initial results for one potential application of interpretability .", "topics": ["supervised learning", "computer vision"]}
{"title": "importance sampling with unequal support", "abstract": "importance sampling is often used in machine learning when training and testing data come from different distributions . in this paper we propose a new variant of importance sampling that can reduce the variance of importance sampling-based estimates by orders of magnitude when the supports of the training and testing distributions differ . after motivating and presenting our new importance sampling estimator , we provide a detailed theoretical analysis that characterizes both its bias and variance relative to the ordinary importance sampling estimator ( in various settings , which include cases where ordinary importance sampling is biased , while our new estimator is not , and vice versa ) . we conclude with an example of how our new importance sampling estimator can be used to improve estimates of how well a new treatment policy for diabetes will work for an individual , using only data from when the individual used a previous treatment policy .", "topics": ["sampling ( signal processing )", "supervised learning"]}
{"title": "boosting deep learning risk prediction with generative adversarial networks for electronic health records", "abstract": "the rapid growth of electronic health records ( ehrs ) , as well as the accompanied opportunities in data-driven healthcare ( ddh ) , has been attracting widespread interests and attentions . recent progress in the design and applications of deep learning methods has shown promising results and is forcing massive changes in healthcare academia and industry , but most of these methods rely on massive labeled data . in this work , we propose a general deep learning framework which is able to boost risk prediction performance with limited ehr data . our model takes a modified generative adversarial network namely ehrgan , which can provide plausible labeled ehr data by mimicking real patient records , to augment the training dataset in a semi-supervised learning manner . we use this generative model together with a convolutional neural network ( cnn ) based prediction model to improve the onset prediction performance . experiments on two real healthcare datasets demonstrate that our proposed framework produces realistic data samples and achieves significant improvements on classification tasks with the generated data over several stat-of-the-art baselines .", "topics": ["baseline ( configuration management )", "generative model"]}
{"title": "generalized fast approximate energy minimization via graph cuts : alpha-expansion beta-shrink moves", "abstract": "we present alpha-expansion beta-shrink moves , a simple generalization of the widely-used alpha-beta swap and alpha-expansion algorithms for approximate energy minimization . we show that in a certain sense , these moves dominate both alpha-beta-swap and alpha-expansion moves , but unlike previous generalizations the new moves require no additional assumptions and are still solvable in polynomial-time . we show promising experimental results with the new moves , which we believe could be used in any context where alpha-expansions are currently employed .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "bayesian joint matrix decomposition for data integration with heterogeneous noise", "abstract": "matrix decomposition is a popular and fundamental approach in machine learning and data mining . it has been successfully applied into various fields . most matrix decomposition methods focus on decomposing a data matrix from one single source . however , it is common that data are from different sources with heterogeneous noise . a few of matrix decomposition methods have been extended for such multi-view data integration and pattern discovery . while only few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly . to this end , we propose a joint matrix decomposition framework ( bjmd ) , which models the heterogeneity of noise by gaussian distribution in a bayesian framework . we develop two algorithms to solve this model : one is a variational bayesian inference algorithm , which makes full use of the posterior distribution ; and another is a maximum a posterior algorithm , which is more scalable and can be easily paralleled . extensive experiments on synthetic and real-world datasets demonstrate that bjmd considering the heterogeneity of noise is superior or competitive to the state-of-the-art methods .", "topics": ["data mining", "calculus of variations"]}
{"title": "on bochner 's and polya 's characterizations of positive-definite kernels and the respective random feature maps", "abstract": "positive-definite kernel functions are fundamental elements of kernel methods and gaussian processes . a well-known construction of such functions comes from bochner 's characterization , which connects a positive-definite function with a probability distribution . another construction , which appears to have attracted less attention , is polya 's criterion that characterizes a subset of these functions . in this paper , we study the latter characterization and derive a number of novel kernels little known previously . in the context of large-scale kernel machines , rahimi and recht ( 2007 ) proposed a random feature map ( random fourier ) that approximates a kernel function , through independent sampling of the probability distribution in bochner 's characterization . the authors also suggested another feature map ( random binning ) , which , although not explicitly stated , comes from polya 's characterization . we show that with the same number of random samples , the random binning map results in an euclidean inner product closer to the kernel than does the random fourier map . the superiority of the random binning map is confirmed empirically through regressions and classifications in the reproducing kernel hilbert space .", "topics": ["sampling ( signal processing )", "eisenstein 's criterion"]}
{"title": "just sort it ! a simple and effective approach to active preference learning", "abstract": "we address the problem of learning a ranking by using adaptively chosen pairwise comparisons . our goal is to recover the ranking accurately but to sample the comparisons sparingly . if all comparison outcomes are consistent with the ranking , the optimal solution is to use an efficient sorting algorithm , such as quicksort . but how do sorting algorithms behave if some comparison outcomes are inconsistent with the ranking ? we give favorable guarantees for quicksort for the popular bradley-terry model , under natural assumptions on the parameters . furthermore , we empirically demonstrate that sorting algorithms lead to a very simple and effective active learning strategy : repeatedly sort the items . this strategy performs as well as state-of-the-art methods ( and much better than random sampling ) at a minuscule fraction of the computational cost .", "topics": ["sampling ( signal processing )", "optimization problem"]}
{"title": "towards a constructive multilayer perceptron for regression task using non-parametric clustering . a case study of photo-z redshift reconstruction", "abstract": "the choice of architecture of artificial neuron network ( ann ) is still a challenging task that users face every time . it greatly affects the accuracy of the built network . in fact there is no optimal method that is applicable to various implementations at the same time . in this paper we propose a method to construct ann based on clustering , that resolves the problems of random and ad hoc approaches for multilayer ann architecture . our method can be applied to regression problems . experimental results obtained with different datasets , reveals the efficiency of our method .", "topics": ["cluster analysis"]}
{"title": "a generative model of words and relationships from multiple sources", "abstract": "neural language models are a powerful tool to embed words into semantic vector spaces . however , learning such models generally relies on the availability of abundant and diverse training examples . in highly specialised domains this requirement may not be met due to difficulties in obtaining a large corpus , or the limited range of expression in average use . such domains may encode prior knowledge about entities in a knowledge base or ontology . we propose a generative model which integrates evidence from diverse data sources , enabling the sharing of semantic information . we achieve this by generalising the concept of co-occurrence from distributional semantics to include other relationships between entities or words , which we model as affine transformations on the embedding space . we demonstrate the effectiveness of this approach by outperforming recent models on a link prediction task and demonstrating its ability to profit from partially or fully unobserved data training labels . we further demonstrate the usefulness of learning from different data sources with overlapping vocabularies .", "topics": ["generative model", "entity"]}
{"title": "combining representation learning with logic for language processing", "abstract": "the current state-of-the-art in many natural language processing and automated knowledge base completion tasks is held by representation learning methods which learn distributed vector representations of symbols via gradient-based optimization . they require little or no hand-crafted features , thus avoiding the need for most preprocessing steps and task-specific assumptions . however , in many cases representation learning requires a large amount of annotated training data to generalize well to unseen data . such labeled training data is provided by human annotators who often use formal logic as the language for specifying annotations . this thesis investigates different combinations of representation learning methods with logic for reducing the need for annotated training data , and for improving generalization .", "topics": ["feature learning", "test set"]}
{"title": "team applied robotics : a closer look at our robotic picking system", "abstract": "this paper describes the vision based robotic picking system that was developed by our team , team applied robotics , for the amazon picking challenge 2016 . this competition challenged teams to develop a robotic system that is able to pick a large variety of products from a shelve or a tote . we discuss the design considerations and our strategy , the high resolution 3d vision system , the use of a combination of texture and shape-based object detection algorithms , the robot path planning and object manipulators that were developed .", "topics": ["object detection", "robot"]}
{"title": "multi-sensor conflict measurement and information fusion", "abstract": "in sensing applications where multiple sensors observe the same scene , fusing sensor outputs can provide improved results . however , if some of the sensors are providing lower quality outputs , the fused results can be degraded . in this work , a multi-sensor conflict measure is proposed which estimates multi-sensor conflict by representing each sensor output as interval-valued information and examines the sensor output overlaps on all possible n-tuple sensor combinations . the conflict is based on the sizes of the intervals and how many sensors output values lie in these intervals . in this work , conflict is defined in terms of how little the output from multiple sensors overlap . that is , high degrees of overlap mean low sensor conflict , while low degrees of overlap mean high conflict . this work is a preliminary step towards a robust conflict and sensor fusion framework . in addition , a sensor fusion algorithm is proposed based on a weighted sum of sensor outputs , where the weights for each sensor diminish as the conflict measure increases . the proposed methods can be utilized to ( 1 ) assess a measure of multi-sensor conflict , and ( 2 ) improve sensor output fusion by lessening weighting for sensors with high conflict . using this measure , a simulated example is given to explain the mechanics of calculating the conflict measure , and stereo camera 3d outputs are analyzed and fused . in the stereo camera case , the sensor output is corrupted by additive impulse noise , dc offset , and gaussian noise . impulse noise is common in sensors due to intermittent interference , a dc offset a sensor bias or registration error , and gaussian noise represents a sensor output with low snr . the results show that sensor output fusion based on the conflict measure shows improved accuracy over a simple averaging fusion strategy .", "topics": ["simulation", "sensor"]}
{"title": "gpatt : fast multidimensional pattern extrapolation with gaussian processes", "abstract": "gaussian processes are typically used for smoothing and interpolation on small datasets . we introduce a new bayesian nonparametric framework -- gpatt -- enabling automatic pattern extrapolation with gaussian processes on large multidimensional datasets . gpatt unifies and extends highly expressive kernels and fast exact inference techniques . without human intervention -- no hand crafting of kernel features , and no sophisticated initialisation procedures -- we show that gpatt can solve large scale pattern extrapolation , inpainting , and kernel discovery problems , including a problem with 383400 training points . we find that gpatt significantly outperforms popular alternative scalable gaussian process methods in speed and accuracy . moreover , we discover profound differences between each of these methods , suggesting expressive kernels , nonparametric representations , and exact inference are useful for modelling large scale multidimensional patterns .", "topics": ["kernel ( operating system )", "scalability"]}
{"title": "globally variance-constrained sparse representation for image set compression", "abstract": "sparse representation presents an efficient approach to approximately recover a signal by the linear composition of a few bases from a learnt dictionary , based on which various successful applications have been observed . however , in the scenario of data compression , its efficiency and popularity are hindered due to the extra overhead for encoding the sparse coefficients . therefore , how to establish an accurate rate model in sparse coding and dictionary learning becomes meaningful , which has been not fully exploited in the context of sparse representation . according to the shannon entropy inequality , the variance of data source bounds its entropy , which can reflect the actual coding bits . hence , in this work a globally variance-constrained sparse representation ( gvcsr ) model is proposed , where a variance-constrained rate model is introduced in the optimization process . specifically , we employ the alternating direction method of multipliers ( admm ) to solve the non-convex optimization problem for sparse coding and dictionary learning , both of which have shown state-of-the-art performance in image representation . furthermore , we investigate the potential of gvcsr in practical image set compression , where a common dictionary is trained by several key images to represent the whole image set . experimental results have demonstrated significant performance improvements against the most popular image codecs including jpeg and jpeg2000 .", "topics": ["optimization problem", "sparse matrix"]}
{"title": "using discretization for extending the set of predictive features", "abstract": "to date , attribute discretization is typically performed by replacing the original set of continuous features with a transposed set of discrete ones . this paper provides support for a new idea that discretized features should often be used in addition to existing features and as such , datasets should be extended , and not replaced , by discretization . we also claim that discretization algorithms should be developed with the explicit purpose of enriching a non-discretized dataset with discretized values . we present such an algorithm , d-miat , a supervised algorithm that discretizes data based on minority interesting attribute thresholds . d-miat only generates new features when strong indications exist for one of the target values needing to be learned and thus is intended to be used in addition to the original data . we present extensive empirical results demonstrating the success of using d-miat on $ 28 $ benchmark datasets . we also demonstrate that $ 10 $ other discretization algorithms can also be used to generate features that yield improved performance when used in combination with the original non-discretized data . our results show that the best predictive performance is attained using a combination of the original dataset with added features from a `` standard '' supervised discretization algorithm and d-miat .", "topics": ["value ( ethics )"]}
{"title": "improved semantic representations from tree-structured long short-term memory networks", "abstract": "because of their superior ability to preserve sequence information over time , long short-term memory ( lstm ) networks , a type of recurrent neural network with a more complex computational unit , have obtained strong results on a variety of sequence modeling tasks . the only underlying lstm structure that has been explored so far is a linear chain . however , natural language exhibits syntactic properties that would naturally combine words to phrases . we introduce the tree-lstm , a generalization of lstms to tree-structured network topologies . tree-lstms outperform all existing systems and strong lstm baselines on two tasks : predicting the semantic relatedness of two sentences ( semeval 2014 , task 1 ) and sentiment classification ( stanford sentiment treebank ) .", "topics": ["recurrent neural network", "natural language"]}
{"title": "inverse graphics with probabilistic cad models", "abstract": "recently , multiple formulations of vision problems as probabilistic inversions of generative models based on computer graphics have been proposed . however , applications to 3d perception from natural images have focused on low-dimensional latent scenes , due to challenges in both modeling and inference . accounting for the enormous variability in 3d object shape and 2d appearance via realistic generative models seems intractable , as does inverting even simple versions of the many-to-many computations that link 3d scenes to 2d images . this paper proposes and evaluates an approach that addresses key aspects of both these challenges . we show that it is possible to solve challenging , real-world 3d vision problems by approximate inference in generative models for images based on rendering the outputs of probabilistic cad ( pcad ) programs . our pcad object geometry priors generate deformable 3d meshes corresponding to plausible objects and apply affine transformations to place them in a scene . image likelihoods are based on similarity in a feature space based on standard mid-level image representations from the vision literature . our inference algorithm integrates single-site and locally blocked metropolis-hastings proposals , hamiltonian monte carlo and discriminative data-driven proposals learned from training data generated from our models . we apply this approach to 3d human pose estimation and object shape reconstruction from single images , achieving quantitative and qualitative performance improvements over state-of-the-art baselines .", "topics": ["baseline ( configuration management )", "generative model"]}
{"title": "attention tree : learning hierarchies of visual features for large-scale image recognition", "abstract": "one of the key challenges in machine learning is to design a computationally efficient multi-class classifier while maintaining the output accuracy and performance . in this paper , we present a tree-based classifier : attention tree ( atree ) for large-scale image classification that uses recursive adaboost training to construct a visual attention hierarchy . the proposed attention model is inspired from the biological 'selective tuning mechanism for cortical visual processing ' . we exploit the inherent feature similarity across images in datasets to identify the input variability and use recursive optimization procedure , to determine data partitioning at each node , thereby , learning the attention hierarchy . a set of binary classifiers is organized on top of the learnt hierarchy to minimize the overall test-time complexity . the attention model maximizes the margins for the binary classifiers for optimal decision boundary modelling , leading to better performance at minimal complexity . the proposed framework has been evaluated on both caltech-256 and sun datasets and achieves accuracy improvement over state-of-the-art tree-based methods at significantly lower computational cost .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "convergence rates of biased stochastic optimization for learning sparse ising models", "abstract": "we study the convergence rate of stochastic optimization of exact ( np-hard ) objectives , for which only biased estimates of the gradient are available . we motivate this problem in the context of learning the structure and parameters of ising models . we first provide a convergence-rate analysis of deterministic errors for forward-backward splitting ( fbs ) . we then extend our analysis to biased stochastic errors , by first characterizing a family of samplers and providing a high probability bound that allows understanding not only fbs , but also proximal gradient ( pg ) methods . we derive some interesting conclusions : fbs requires only a logarithmically increasing number of random samples in order to converge ( although at a very low rate ) ; the required number of random samples is the same for the deterministic and the biased stochastic setting for fbs and basic pg ; accelerated pg is not guaranteed to converge in the biased stochastic setting .", "topics": ["sparse matrix", "gradient"]}
{"title": "dice : the infinitely differentiable monte-carlo estimator", "abstract": "the score function estimator is widely used for estimating gradients of stochastic objectives in stochastic computation graphs ( scg ) , eg . in reinforcement learning and meta-learning . while deriving the first-order gradient estimators by differentiating a surrogate loss ( sl ) objective is computationally and conceptually simple , using the same approach for higher-order gradients is more challenging . firstly , analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation . secondly , repeatedly applying sl to construct new objectives for each order gradient involves increasingly cumbersome graph manipulations . lastly , to match the first-order gradient under differentiation , sl treats part of the cost as a fixed sample , which we show leads to missing and wrong terms for higher-order gradient estimators . to address all these shortcomings in a unified way , we introduce dice , which provides a single objective that can be differentiated repeatedly , generating correct gradient estimators of any order in scgs . unlike sl , dice relies on automatic differentiation for performing the requisite graph manipulations . we verify the correctness of dice both through a proof and through numerical evaluation of the dice gradient estimates . we also use dice to propose and evaluate a novel approach for multi-agent learning . our code is available at https : //goo.gl/xkkgxn .", "topics": ["reinforcement learning", "gradient descent"]}
{"title": "stochastic gradient descent as approximate bayesian inference", "abstract": "stochastic gradient descent with a constant learning rate ( constant sgd ) simulates a markov chain with a stationary distribution . with this perspective , we derive several new results . ( 1 ) we show that constant sgd can be used as an approximate bayesian posterior inference algorithm . specifically , we show how to adjust the tuning parameters of constant sgd to best match the stationary distribution to a posterior , minimizing the kullback-leibler divergence between these two distributions . ( 2 ) we demonstrate that constant sgd gives rise to a new variational em algorithm that optimizes hyperparameters in complex probabilistic models . ( 3 ) we also propose sgd with momentum for sampling and show how to adjust the damping coefficient accordingly . ( 4 ) we analyze mcmc algorithms . for langevin dynamics and stochastic gradient fisher scoring , we quantify the approximation errors due to finite learning rates . finally ( 5 ) , we use the stochastic process perspective to give a short proof of why polyak averaging is optimal . based on this idea , we propose a scalable approximate mcmc algorithm , the averaged stochastic gradient sampler .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "propagation using chain event graphs", "abstract": "a chain event graph ( ceg ) is a graphial model which designed to embody conditional independencies in problems whose state spaces are highly asymmetric and do not admit a natural product structure . in this paer we present a probability propagation algorithm which uses the topology of the ceg to build a transporter ceg . intriungly , the transporter ceg is directly analogous to the triangulated bayesian network ( bn ) in the more conventional junction tree propagation algorithms used with bns . the propagation method uses factorization formulae also analogous to ( but different from ) the ones using potentials on cliques and separators of the bn . it appears that the methods will be typically more efficient than the bn algorithms when applied to contexts where there is significant asymmetry present .", "topics": ["bayesian network"]}
{"title": "improved accuracy of pso and de using normalization : an application to stock price prediction", "abstract": "data mining is being actively applied to stock market since 1980s . it has been used to predict stock prices , stock indexes , for portfolio management , trend detection and for developing recommender systems . the various algorithms which have been used for the same include ann , svm , arima , garch etc . different hybrid models have been developed by combining these algorithms with other algorithms like roughest , fuzzy logic , ga , pso , de , aco etc . to improve the efficiency . this paper proposes de-svm model ( differential evolutionsupport vector machine ) for stock price prediction . de has been used to select best free parameters combination for svm to improve results . the paper also compares the results of prediction with the outputs of svm alone and pso-svm model ( particle swarm optimization ) . the effect of normalization of data on the accuracy of prediction has also been studied .", "topics": ["data mining", "support vector machine"]}
{"title": "integrating e-commerce and data mining : architecture and challenges", "abstract": "we show that the e-commerce domain can provide all the right ingredients for successful data mining and claim that it is a killer domain for data mining . we describe an integrated architecture , based on our expe-rience at blue martini software , for supporting this integration . the architecture can dramatically reduce the pre-processing , cleaning , and data understanding effort often documented to take 80 % of the time in knowledge discovery projects . we emphasize the need for data collection at the application server layer ( not the web server ) in order to support logging of data and metadata that is essential to the discovery process . we describe the data transformation bridges required from the transaction processing systems and customer event streams ( e.g . , clickstreams ) to the data warehouse . we detail the mining workbench , which needs to provide multiple views of the data through reporting , data mining algorithms , visualization , and olap . we con-clude with a set of challenges .", "topics": ["data mining"]}
{"title": "depth creates no bad local minima", "abstract": "in deep learning , \\textit { depth } , as well as \\textit { nonlinearity } , create non-convex loss surfaces . then , does depth alone create bad local minima ? in this paper , we prove that without nonlinearity , depth alone does not create bad local minima , although it induces non-convex loss surface . using this insight , we greatly simplify a recently proposed proof to show that all of the local minima of feedforward deep linear neural networks are global minima . our theoretical results generalize previous results with fewer assumptions , and this analysis provides a method to show similar results beyond square loss in deep linear models .", "topics": ["nonlinear system"]}
{"title": "learning discrete weights using the local reparameterization trick", "abstract": "recent breakthroughs in computer vision make use of large deep neural networks , utilizing the substantial speedup offered by gpus . for applications running on limited hardware , however , high precision real-time processing can still be a challenge . one approach to solving this problem is training networks with binary or ternary weights , thus removing the need to calculate multiplications and significantly reducing memory size . in this work , we introduce lr-nets ( local reparameterization networks ) , a new method for training neural networks with discrete weights using stochastic parameters . we show how a simple modification to the local reparameterization trick , previously used to train gaussian distributed weights , enables the training of discrete weights . using the proposed training we test both binary and ternary models on mnist , cifar-10 and imagenet benchmarks and reach state-of-the-art results on most experiments .", "topics": ["computer vision", "mnist database"]}
{"title": "distributed regression in sensor networks : training distributively with alternating projections", "abstract": "wireless sensor networks ( wsns ) have attracted considerable attention in recent years and motivate a host of new challenges for distributed signal processing . the problem of distributed or decentralized estimation has often been considered in the context of parametric models . however , the success of parametric methods is limited by the appropriateness of the strong statistical assumptions made by the models . in this paper , a more flexible nonparametric model for distributed regression is considered that is applicable in a variety of wsn applications including field estimation . here , starting with the standard regularized kernel least-squares estimator , a message-passing algorithm for distributed estimation in wsns is derived . the algorithm can be viewed as an instantiation of the successive orthogonal projection ( sop ) algorithm . various practical aspects of the algorithm are discussed and several numerical simulations validate the potential of the approach .", "topics": ["numerical analysis", "simulation"]}
{"title": "gpu acclerated automated feature extraction from satellite images", "abstract": "the availability of large volumes of remote sensing data insists on higher degree of automation in feature extraction , making it a need of the hour.the huge quantum of data that needs to be processed entails accelerated processing to be enabled.gpus , which were originally designed to provide efficient visualization , are being massively employed for computation intensive parallel processing environments . image processing in general and hence automated feature extraction , is highly computation intensive , where performance improvements have a direct impact on societal needs . in this context , an algorithm has been formulated for automated feature extraction from a panchromatic or multispectral image based on image processing techniques . two laplacian of guassian ( log ) masks were applied on the image individually followed by detection of zero crossing points and extracting the pixels based on their standard deviation with the surrounding pixels . the two extracted images with different log masks were combined together which resulted in an image with the extracted features and edges . finally the user is at liberty to apply the image smoothing step depending on the noise content in the extracted image . the image is passed through a hybrid median filter to remove the salt and pepper noise from the image . this paper discusses the aforesaid algorithm for automated feature extraction , necessity of deployment of gpus for the same ; system-level challenges and quantifies the benefits of integrating gpus in such environment . the results demonstrate that substantial enhancement in performance margin can be achieved with the best utilization of gpu resources and an efficient parallelization strategy . performance results in comparison with the conventional computing scenario have provided a speedup of 20x , on realization of this parallelizing strategy .", "topics": ["image processing", "feature extraction"]}
{"title": "robust bayesian compressed sensing", "abstract": "we consider the problem of robust compressed sensing whose objective is to recover a high-dimensional sparse signal from compressed measurements corrupted by outliers . a new sparse bayesian learning method is developed for robust compressed sensing . the basic idea of the proposed method is to identify and remove the outliers from sparse signal recovery . to automatically identify the outliers , we employ a set of binary indicator hyperparameters to indicate which observations are outliers . these indicator hyperparameters are treated as random variables and assigned a beta process prior such that their values are confined to be binary . in addition , a gaussian-inverse gamma prior is imposed on the sparse signal to promote sparsity . based on this hierarchical prior model , we develop a variational bayesian method to estimate the indicator hyperparameters as well as the sparse signal . simulation results show that the proposed method achieves a substantial performance improvement over existing robust compressed sensing techniques .", "topics": ["calculus of variations", "value ( ethics )"]}
{"title": "3d tracking of water hazards with polarized stereo cameras", "abstract": "current self-driving car systems operate well in sunny weather but struggle in adverse conditions . one of the most commonly encountered adverse conditions involves water on the road caused by rain , sleet , melting snow or flooding . while some advances have been made in using conventional rgb camera and lidar technology for detecting water hazards , other sources of information such as polarization offer a promising and potentially superior approach to this problem in terms of performance and cost . in this paper , we present a novel stereo-polarization system for detecting and tracking water hazards based on polarization and color variation of reflected light , with consideration of the effect of polarized light from sky as function of reflection and azimuth angles . to evaluate this system , we present a new large `water on road ' datasets spanning approximately 2 km of driving in various on-road and off-road conditions and demonstrate for the first time reliable water detection and tracking over a wide range of realistic car driving water conditions using polarized vision as the primary sensing modality . our system successfully detects water hazards up to more than 100m . finally , we discuss several interesting challenges and propose future research directions for further improving robust autonomous car perception in hazardous wet conditions using polarization sensors .", "topics": ["sensor", "autonomous car"]}
{"title": "learning discourse-level diversity for neural dialog models using conditional variational autoencoders", "abstract": "while recent neural encoder-decoder models have shown great promise in modeling open-domain conversations , they often generate dull and generic responses . unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem , we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder . our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders . we have further developed a novel variant that is integrated with linguistic prior knowledge for better performance . finally , the training procedure is improved by introducing a bag-of-word loss . our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making .", "topics": ["baseline ( configuration management )", "calculus of variations"]}
{"title": "l2-nonexpansive neural networks", "abstract": "this paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the internal layers . we develop the known methodology of controlling lipschitz constants to realize its full potential in maximizing robustness : our linear and convolution layers subsume those in the previous parseval networks as a special case and allow greater degrees of freedom ; aggregation , pooling , splitting and other operators are adapted in new ways , and a new loss function is proposed , all for the purpose of improving robustness . with mnist and cifar-10 classifiers , we demonstrate a number of advantages . without needing any adversarial training , the proposed classifiers exceed the state of the art in robustness against white-box l2-bounded adversarial attacks . their outputs are quantitatively more meaningful than ordinary networks and indicate levels of confidence . they are also free of exploding gradients , among other desirable properties .", "topics": ["neural networks", "loss function"]}
{"title": "the crossover process : learnability and data protection from inference attacks", "abstract": "it is usual to consider data protection and learnability as conflicting objectives . this is not always the case : we show how to jointly control inference -- - seen as the attack -- - and learnability by a noise-free process that mixes training examples , the crossover process ( cp ) . one key point is that the cp~is typically able to alter joint distributions without touching on marginals , nor altering the sufficient statistic for the class . in other words , it saves ( and sometimes improves ) generalization for supervised learning , but can alter the relationship between covariates -- - and therefore fool measures of nonlinear independence and causal inference into misleading ad-hoc conclusions . for example , a cp~can increase / decrease odds ratios , bring fairness or break fairness , tamper with disparate impact , strengthen , weaken or reverse causal directions , change observed statistical measures of dependence . for each of these , we quantify changes brought by a cp , as well as its statistical impact on generalization abilities via a new complexity measure that we call the rademacher cp~complexity . experiments on a dozen readily available domains validate the theory .", "topics": ["supervised learning", "nonlinear system"]}
{"title": "the robust manifold defense : adversarial training using generative models", "abstract": "deep neural networks are demonstrating excellent performance on several classical vision problems . however , these networks are vulnerable to adversarial examples , minutely modified images that induce arbitrary attacker-chosen output from the network . we propose a mechanism to protect against these adversarial inputs based on a generative model of the data . we introduce a pre-processing step that projects on the range of a generative model using gradient descent before feeding an input into a classifier . we show that this step provides the classifier with robustness against first-order , substitute model , and combined adversarial attacks . using a min-max formulation , we show that there may exist adversarial examples even in the range of the generator , natural-looking images extremely close to the decision boundary for which the classifier has unjustifiedly high confidence . we show that adversarial training on the generative manifold can be used to make a classifier that is robust to these attacks . finally , we show how our method can be applied even without a pre-trained generative model using a recent method called the deep image prior . we evaluate our method on mnist , celeba and imagenet and show robustness against the current state of the art attacks .", "topics": ["generative model", "gradient descent"]}
{"title": "decorrelation of neutral vector variables : theory and applications", "abstract": "in this paper , we propose novel strategies for neutral vector variable decorrelation . two fundamental invertible transformations , namely serial nonlinear transformation and parallel nonlinear transformation , are proposed to carry out the decorrelation . for a neutral vector variable , which is not multivariate gaussian distributed , the conventional principal component analysis ( pca ) can not yield mutually independent scalar variables . with the two proposed transformations , a highly negatively correlated neutral vector can be transformed to a set of mutually independent scalar variables with the same degrees of freedom . we also evaluate the decorrelation performances for the vectors generated from a single dirichlet distribution and a mixture of dirichlet distributions . the mutual independence is verified with the distance correlation measurement . the advantages of the proposed decorrelation strategies are intensively studied and demonstrated with synthesized data and practical application evaluations .", "topics": ["nonlinear system"]}
{"title": "a gentle introduction to the kernel distance", "abstract": "this document reviews the definition of the kernel distance , providing a gentle introduction tailored to a reader with background in theoretical computer science , but limited exposure to technology more common to machine learning , functional analysis and geometric measure theory . the key aspect of the kernel distance developed here is its interpretation as an l_2 distance between probability measures or various shapes ( e.g . point sets , curves , surfaces ) embedded in a vector space ( specifically an rkhs ) . this structure enables several elegant and efficient solutions to data analysis problems . we conclude with a glimpse into the mathematical underpinnings of this measure , highlighting its recent independent evolution in two separate fields .", "topics": ["kernel ( operating system )"]}
{"title": "piecewise linear multilayer perceptrons and dropout", "abstract": "we propose a new type of hidden layer for a multilayer perceptron , and demonstrate that it obtains the best reported performance for an mlp on the mnist dataset .", "topics": ["gradient", "mnist database"]}
{"title": "dropout inference in bayesian neural networks with alpha-divergences", "abstract": "to obtain uncertainty estimates with real-world bayesian deep learning models , practical inference approximations are needed . dropout variational inference ( vi ) for example has been used for machine vision and medical applications , but vi can severely underestimates model uncertainty . alpha-divergences are alternative divergences to vi 's kl objective , which are able to avoid vi 's uncertainty underestimation . but these are hard to use in practice : existing techniques can only use gaussian approximating distributions , and require existing models to be changed radically , thus are of limited use for practitioners . we propose a re-parametrisation of the alpha-divergence objectives , deriving a simple inference technique which , together with dropout , can be easily implemented with existing models by simply changing the loss of the model . we demonstrate improved uncertainty estimates and accuracy compared to vi in dropout networks . we study our model 's epistemic uncertainty far away from the data using adversarial images , showing that these can be distinguished from non-adversarial images by examining our model 's uncertainty .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "inference of fine-grained attributes of bengali corpus for stylometry detection", "abstract": "stylometry , the science of inferring characteristics of the author from the characteristics of documents written by that author , is a problem with a long history and belongs to the core task of text categorization that involves authorship identification , plagiarism detection , forensic investigation , computer security , copyright and estate disputes etc . in this work , we present a strategy for stylometry detection of documents written in bengali . we adopt a set of fine-grained attribute features with a set of lexical markers for the analysis of the text and use three semi-supervised measures for making decisions . finally , a majority voting approach has been taken for final classification . the system is fully automatic and language-independent . evaluation results of our attempt for bengali author 's stylometry detection show reasonably promising accuracy in comparison to the baseline model .", "topics": ["baseline ( configuration management )"]}
{"title": "a new approach for scalable analysis of microbial communities", "abstract": "microbial communities play important roles in the function and maintenance of various biosystems , ranging from human body to the environment . current methods for analysis of microbial communities are typically based on taxonomic phylogenetic alignment using 16s rrna metagenomic or whole genome sequencing data . in typical characterizations of microbial communities , studies deal with billions of micobial sequences , aligning them to a phylogenetic tree . we introduce a new approach for the efficient analysis of microbial communities . our new reference-free analysis tech- nique is based on n-gram sequence analysis of 16s rrna data and reduces the processing data size dramatically ( by 105 fold ) , without requiring taxonomic alignment . the proposed approach is applied to characterize phenotypic microbial community differ- ences in different settings . specifically , we applied this approach in classification of microbial com- munities across different body sites , characterization of oral microbiomes associated with healthy and diseased individuals , and classification of microbial communities longitudinally during the develop- ment of infants . different dimensionality reduction methods are introduced that offer a more scalable analysis framework , while minimizing the loss in classification accuracies . among dimensionality re- duction techniques , we propose a continuous vector representation for microbial communities , which can widely be used for deep learning applications in microbial informatics .", "topics": ["scalability"]}
{"title": "dropout as a bayesian approximation : representing model uncertainty in deep learning", "abstract": "deep learning tools have gained tremendous attention in applied machine learning . however such tools for regression and classification do not capture model uncertainty . in comparison , bayesian models offer a mathematically grounded framework to reason about model uncertainty , but usually come with a prohibitive computational cost . in this paper we develop a new theoretical framework casting dropout training in deep neural networks ( nns ) as approximate bayesian inference in deep gaussian processes . a direct result of this theory gives us tools to model uncertainty with dropout nns -- extracting information from existing models that has been thrown away so far . this mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy . we perform an extensive study of the properties of dropout 's uncertainty . various network architectures and non-linearities are assessed on tasks of regression and classification , using mnist as an example . we show a considerable improvement in predictive log-likelihood and rmse compared to existing state-of-the-art methods , and finish by using dropout 's uncertainty in deep reinforcement learning .", "topics": ["statistical classification", "computational complexity theory"]}
{"title": "virtual embodiment : a scalable long-term strategy for artificial intelligence research", "abstract": "meaning has been called the `` holy grail '' of a variety of scientific disciplines , ranging from linguistics to philosophy , psychology and the neurosciences . the field of artifical intelligence ( ai ) is very much a part of that list : the development of sophisticated natural language semantics is a sine qua non for achieving a level of intelligence comparable to humans . embodiment theories in cognitive science hold that human semantic representation depends on sensori-motor experience ; the abundant evidence that human meaning representation is grounded in the perception of physical reality leads to the conclusion that meaning must depend on a fusion of multiple ( perceptual ) modalities . despite this , ai research in general , and its subdisciplines such as computational linguistics and computer vision in particular , have focused primarily on tasks that involve a single modality . here , we propose virtual embodiment as an alternative , long-term strategy for ai research that is multi-modal in nature and that allows for the kind of scalability required to develop the field coherently and incrementally , in an ethically responsible fashion .", "topics": ["computer vision", "natural language"]}
{"title": "on optimality conditions for auto-encoder signal recovery", "abstract": "auto-encoders are unsupervised models that aim to learn patterns from observed data by minimizing a reconstruction cost . the useful representations learned are often found to be sparse and distributed . on the other hand , compressed sensing and sparse coding assume a data generating process , where the observed data is generated from some true latent signal source , and try to recover the corresponding signal from measurements . looking at auto-encoders from this \\textit { signal recovery perspective } enables us to have a more coherent view of these techniques . in this paper , in particular , we show that the \\textit { true } hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^ { 2 } $ row length and the bias vectors takes the value ( approximately ) equal to the negative of the data mean . the recovery also becomes more and more accurate as the sparsity in hidden signals increases . additionally , we empirically demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given .", "topics": ["unsupervised learning", "sparse matrix"]}
{"title": "artificial neural networks applied to taxi destination prediction", "abstract": "we describe our first-place solution to the ecml/pkdd discovery challenge on taxi destination prediction . the task consisted in predicting the destination of a taxi based on the beginning of its trajectory , represented as a variable-length sequence of gps points , and diverse associated meta-information , such as the departure time , the driver id and client information . contrary to most published competitor approaches , we used an almost fully automated approach based on neural networks and we ranked first out of 381 teams . the architectures we tried use multi-layer perceptrons , bidirectional recurrent neural networks and models inspired from recently introduced memory networks . our approach could easily be adapted to other applications in which the goal is to predict a fixed-length output from a variable-length sequence .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "artificial neural networks for beginners", "abstract": "the scope of this teaching package is to make a brief induction to artificial neural networks ( anns ) for people who have no previous knowledge of them . we first make a brief introduction to models of networks , for then describing in general terms anns . as an application , we explain the backpropagation algorithm , since it is widely used and many other algorithms are derived from it . the user should know algebra and the handling of functions and vectors . differential calculus is recommendable , but not necessary . the contents of this package should be understood by people with high school education . it would be useful for people who are just curious about what are anns , or for people who want to become familiar with them , so when they study them more fully , they will already have clear notions of anns . also , people who only want to apply the backpropagation algorithm without a detailed and formal explanation of it will find this material useful . this work should not be seen as `` nets for dummies '' , but of course it is not a treatise . much of the formality is skipped for the sake of simplicity . detailed explanations and demonstrations can be found in the referred readings . the included exercises complement the understanding of the theory . the on-line resources are highly recommended for extending this brief induction .", "topics": ["neural networks"]}
{"title": "identifying sources and sinks in the presence of multiple agents with gaussian process vector calculus", "abstract": "in systems of multiple agents , identifying the cause of observed agent dynamics is challenging . often , these agents operate in diverse , non-stationary environments , where models rely on hand-crafted environment-specific features to infer influential regions in the system 's surroundings . to overcome the limitations of these inflexible models , we present gp-laplace , a technique for locating sources and sinks from trajectories in time-varying fields . using gaussian processes , we jointly infer a spatio-temporal vector field , as well as canonical vector calculus operations on that field . notably , we do this from only agent trajectories without requiring knowledge of the environment , and also obtain a metric for denoting the significance of inferred causal features in the environment by exploiting our probabilistic method . to evaluate our approach , we apply it to both synthetic and real-world gps data , demonstrating the applicability of our technique in the presence of multiple agents , as well as its superiority over existing methods .", "topics": ["synthetic data", "causality"]}
{"title": "space-filling curves as a novel crystal structure representation for machine learning models", "abstract": "a fundamental problem in applying machine learning techniques for chemical problems is to find suitable representations for molecular and crystal structures . while the structure representations based on atom connectivities are prevalent for molecules , two-dimensional descriptors are not suitable for describing molecular crystals . in this work , we introduce the sfc-m family of feature representations , which are based on morton space-filling curves , as an alternative means of representing crystal structures . latent semantic indexing ( lsi ) was employed in a novel setting to reduce sparsity of feature representations . the quality of the sfc-m representations were assessed by using them in combination with artificial neural networks to predict density functional theory ( dft ) single point , ewald summed , lattice , and many-body dispersion energies of 839 organic molecular crystal unit cells from the cambridge structural database that consist of the elements c , h , n , and o . promising initial results suggest that the sfc-m representations merit further exploration to improve its ability to predict solid-state properties of organic crystal structures", "topics": ["sparse matrix"]}
{"title": "investigating the feature collection for semantic segmentation via single skip connection", "abstract": "since the study of deep convolutional neural network became prevalent , one of the important discoveries is that a feature map from a convolutional network can be extracted before going into the fully connected layer and can be used as a saliency map for object detection . furthermore , the model can use features from each different layer for accurate object detection : the features from different layers can have different properties . as the model goes deeper , it has many latent skip connections and feature maps to elaborate object detection . although there are many intermediate layers that we can use for semantic segmentation through skip connection , still the characteristics of each skip connection and the best skip connection for this task are uncertain . therefore , in this study , we exhaustively research skip connections of state-of-the-art deep convolutional networks and investigate the characteristics of the features from each intermediate layer . in addition , this study would suggest how to use a recent deep neural network model for semantic segmentation and it would therefore become a cornerstone for later studies with the state-of-the-art network models .", "topics": ["object detection", "map"]}
{"title": "revolvable indoor panoramas using a rectified azimuthal projection", "abstract": "we present an algorithm for converting an indoor spherical panorama into a photograph with a simulated overhead view . the resulting image will have an extremely wide field of view covering up to 4 { \\pi } steradians of the spherical panorama . we argue that our method complements the stereographic projection commonly used in the `` little planet '' effect . the stereographic projection works well in creating little planets of outdoor scenes ; whereas our method is a well-suited counterpart for indoor scenes . the main innovation of our method is the introduction of a novel azimuthal map projection that can smoothly blend between the stereographic projection and the lambert azimuthal equal-area projection . our projection has an adjustable parameter that allows one to control and compromise between distortions in shape and distortions in size within the projected panorama . this extra control parameter gives our projection the ability to produce superior results over the stereographic projection .", "topics": ["simulation"]}
{"title": "pattern recognition using artificial immune system", "abstract": "in this thesis , the uses of artificial immune systems ( ais ) in machine learning is studded . the thesis focus on some of immune inspired algorithms such as clonal selection algorithm and artificial immune network . the effect of changing the algorithm parameter on its performance is studded . then a new immune inspired algorithm for unsupervised classification is proposed . the new algorithm is based on clonal selection principle and named unsupervised clonal selection classification ( ucsc ) . the new proposed algorithm is almost parameter free . the algorithm parameters are data driven and it adjusts itself to make the classification as fast as possible . the performance of ucsc is evaluated . the experiments show that the proposed ucsc algorithm has a good performance and more reliable .", "topics": ["unsupervised learning"]}
{"title": "solving the `` false positives '' problem in fraud prediction", "abstract": "in this paper , we present an automated feature engineering based approach to dramatically reduce false positives in fraud prediction . false positives plague the fraud prediction industry . it is estimated that only 1 in 5 declared as fraud are actually fraud and roughly 1 in every 6 customers have had a valid transaction declined in the past year . to address this problem , we use the deep feature synthesis algorithm to automatically derive behavioral features based on the historical data of the card associated with a transaction . we generate 237 features ( > 100 behavioral patterns ) for each transaction , and use a random forest to learn a classifier . we tested our machine learning model on data from a large multinational bank and compared it to their existing solution . on an unseen data of 1.852 million transactions , we were able to reduce the false positives by 54 % and provide a savings of 190k euros . we also assess how to deploy this solution , and whether it necessitates streaming computation for real time scoring . we found that our solution can maintain similar benefits even when historical features are computed once every 7 days .", "topics": ["computation"]}
{"title": "da-rnn : semantic mapping with data associated recurrent neural networks", "abstract": "3d scene understanding is important for robots to interact with the 3d world in a meaningful way . most previous works on 3d scene understanding focus on recognizing geometrical or semantic properties of the scene independently . in this work , we introduce data associated recurrent neural networks ( da-rnns ) , a novel framework for joint 3d scene mapping and semantic labeling . da-rnns use a new recurrent neural network architecture for semantic labeling on rgb-d videos . the output of the network is integrated with mapping techniques such as kinectfusion in order to inject semantic information into the reconstructed 3d scene . experiments conducted on a real world dataset and a synthetic dataset with rgb-d videos demonstrate the ability of our method in semantic 3d scene mapping .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "controlling for unobserved confounds in classification using correlational constraints", "abstract": "as statistical classifiers become integrated into real-world applications , it is important to consider not only their accuracy but also their robustness to changes in the data distribution . in this paper , we consider the case where there is an unobserved confounding variable $ z $ that influences both the features $ \\mathbf { x } $ and the class variable $ y $ . when the influence of $ z $ changes from training to testing data , we find that the classifier accuracy can degrade rapidly . in our approach , we assume that we can predict the value of $ z $ at training time with some error . the prediction for $ z $ is then fed to pearl 's back-door adjustment to build our model . because of the attenuation bias caused by measurement error in $ z $ , standard approaches to controlling for $ z $ are ineffective . in response , we propose a method to properly control for the influence of $ z $ by first estimating its relationship with the class variable $ y $ , then updating predictions for $ z $ to match that estimated relationship . by adjusting the influence of $ z $ , we show that we can build a model that exceeds competing baselines on accuracy as well as on robustness over a range of confounding relationships .", "topics": ["baseline ( configuration management )", "statistical classification"]}
{"title": "counterfactual learning for machine translation : degeneracies and solutions", "abstract": "counterfactual learning is a natural scenario to improve web-based machine translation services by offline learning from feedback logged during user interactions . in order to avoid the risk of showing inferior translations to users , in such scenarios mostly exploration-free deterministic logging policies are in place . we analyze possible degeneracies of inverse and reweighted propensity scoring estimators , in stochastic and deterministic settings , and relate them to recently proposed techniques for counterfactual learning under deterministic logging .", "topics": ["machine translation"]}
{"title": "recurrent neural network grammars", "abstract": "we introduce recurrent neural network grammars , probabilistic models of sentences with explicit phrase structure . we explain efficient inference procedures that allow application to both parsing and language modeling . experiments show that they provide better parsing in english than any single previously published supervised generative model and better language modeling than state-of-the-art sequential rnns in english and chinese .", "topics": ["generative model", "recurrent neural network"]}
{"title": "pcanet : a simple deep learning baseline for image classification ?", "abstract": "in this work , we propose a very simple deep learning network for image classification which comprises only the very basic data processing components : cascaded principal component analysis ( pca ) , binary hashing , and block-wise histograms . in the proposed architecture , pca is employed to learn multistage filter banks . it is followed by simple binary hashing and block histograms for indexing and pooling . this architecture is thus named as a pca network ( pcanet ) and can be designed and learned extremely easily and efficiently . for comparison and better understanding , we also introduce and study two simple variations to the pcanet , namely the randnet and ldanet . they share the same topology of pcanet but their cascaded filters are either selected randomly or learned from lda . we have tested these basic networks extensively on many benchmark visual datasets for different tasks , such as lfw for face verification , multipie , extended yale b , ar , feret datasets for face recognition , as well as mnist for hand-written digits recognition . surprisingly , for all tasks , such a seemingly naive pcanet model is on par with the state of the art features , either prefixed , highly hand-crafted or carefully learned ( by dnns ) . even more surprisingly , it sets new records for many classification tasks in extended yale b , ar , feret datasets , and mnist variations . additional experiments on other public datasets also demonstrate the potential of the pcanet serving as a simple but highly competitive baseline for texture classification and object recognition .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "clustering with same-cluster queries", "abstract": "we propose a framework for semi-supervised active clustering framework ( ssac ) , where the learner is allowed to interact with a domain expert , asking whether two given instances belong to the same cluster or not . we study the query and computational complexity of clustering in this framework . we consider a setting where the expert conforms to a center-based clustering with a notion of margin . we show that there is a trade off between computational complexity and query complexity ; we prove that for the case of $ k $ -means clustering ( i.e . , when the expert conforms to a solution of $ k $ -means ) , having access to relatively few such queries allows efficient solutions to otherwise np hard problems . in particular , we provide a probabilistic polynomial-time ( bpp ) algorithm for clustering in this setting that asks $ o\\big ( k^2\\log k + k\\log n ) $ same-cluster queries and runs with time complexity $ o\\big ( kn\\log n ) $ ( where $ k $ is the number of clusters and $ n $ is the number of instances ) . the algorithm succeeds with high probability for data satisfying margin conditions under which , without queries , we show that the problem is np hard . we also prove a lower bound on the number of queries needed to have a computationally efficient clustering algorithm in this setting .", "topics": ["cluster analysis", "time complexity"]}
{"title": "semantic preserving embeddings for generalized graphs", "abstract": "a new approach to the study of generalized graphs as semantic data structures using machine learning techniques is presented . we show how vector representations maintaining semantic characteristics of the original data can be obtained from a given graph using neural encoding architectures and considering the topological properties of the graph . semantic features of these new representations are tested by using some machine learning tasks and new directions on efficient link discovery , entitity retrieval and long distance query methodologies on large relational datasets are investigated using real datasets . -- -- en este trabajo se presenta un nuevo enfoque en el contexto del aprendizaje autom\\'atico multi-relacional para el estudio de grafos generalizados . se muestra c\\'omo se pueden obtener representaciones vectoriales que mantienen caracter\\'isticas sem\\'anticas del grafo original utilizando codificadores neuronales y considerando las propiedades topol\\'ogicas del grafo . adem\\'as , se eval\\'uan las caracter\\'isticas sem\\'anticas capturadas por estas nuevas representaciones y se investigan nuevas metodolog\\'ias eficientes relacionadas con link discovery , entity retrieval y consultas a larga distancia en grandes conjuntos de datos relacionales haciendo uso de bases de datos reales .", "topics": ["cluster analysis", "database"]}
{"title": "an effective fingerprint classification and search method", "abstract": "this paper presents an effective fingerprint classification method designed based on a hierarchical agglomerative clustering technique . the performance of the technique was evaluated in terms of several real-life datasets and a significant improvement in reducing the misclassification error has been noticed . this paper also presents a query based faster fingerprint search method over the clustered fingerprint databases . the retrieval accuracy of the search method has been found effective in light of several real-life databases .", "topics": ["cluster analysis", "database"]}
{"title": "equilibrium ( zipf ) and dynamic ( grasseberg-procaccia ) method based analyses of human texts . a comparison of natural ( english ) and artificial ( esperanto ) languages", "abstract": "a comparison of two english texts from lewis carroll , one ( alice in wonderland ) , also translated into esperanto , the other ( through a looking glass ) are discussed in order to observe whether natural and artificial languages significantly differ from each other . one dimensional time series like signals are constructed using only word frequencies ( fts ) or word lengths ( lts ) . the data is studied through ( i ) a zipf method for sorting out correlations in the fts and ( ii ) a grassberger-procaccia ( gp ) technique based method for finding correlations in lts . features are compared : different power laws are observed with characteristic exponents for the ranking properties , and the { \\it phase space attractor dimensionality } . the zipf exponent can take values much less than unity ( $ ca . $ 0.50 or 0.30 ) depending on how a sentence is defined . this non-universality is conjectured to be a measure of the author $ style $ . moreover the attractor dimension $ r $ is a simple function of the so called phase space dimension $ n $ , i.e . , $ r = n^ { \\lambda } $ , with $ \\lambda = 0.79 $ . such an exponent should also conjecture to be a measure of the author $ creativity $ . however , even though there are quantitative differences between the original english text and its esperanto translation , the qualitative differences are very minutes , indicating in this case a translation relatively well respecting , along our analysis lines , the content of the author writing .", "topics": ["time series"]}
{"title": "nilc-usp at semeval-2017 task 4 : a multi-view ensemble for twitter sentiment analysis", "abstract": "this paper describes our multi-view ensemble approach to semeval-2017 task 4 on sentiment analysis in twitter , specifically , the message polarity classification subtask for english ( subtask a ) . our system is a voting ensemble , where each base classifier is trained in a different feature space . the first space is a bag-of-words model and has a linear svm as base classifier . the second and third spaces are two different strategies of combining word embeddings to represent sentences and use a linear svm and a logistic regressor as base classifiers . the proposed system was ranked 18th out of 38 systems considering f1 score and 20th considering recall .", "topics": ["feature vector"]}
{"title": "reinforcement learning with a corrupted reward channel", "abstract": "no real-world reward function is perfect . sensory errors and software bugs may result in rl agents observing higher ( or lower ) rewards than they should . for example , a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward , but where the true reward is actually small . we formalise this problem as a generalised markov decision problem called corrupt reward mdp . traditional rl methods fare poorly in crmdps , even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards . two ways around the problem are investigated . first , by giving the agent richer data , such as in inverse reinforcement learning and semi-supervised reinforcement learning , reward corruption stemming from systematic sensory errors may sometimes be completely managed . second , by using randomisation to blunt the agent 's optimisation , reward corruption can be partially managed under some assumptions .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "striving for simplicity : the all convolutional net", "abstract": "most modern convolutional neural networks ( cnns ) used for object recognition are built using the same principles : alternating convolution and max-pooling layers followed by a small number of fully connected layers . we re-evaluate the state of the art for object recognition from small images with convolutional networks , questioning the necessity of different components in the pipeline . we find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks . following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets ( cifar-10 , cifar-100 , imagenet ) . to analyze the network we introduce a new variant of the `` deconvolution approach '' for visualizing features learned by cnns , which can be applied to a broader range of network structures than existing approaches .", "topics": ["computer vision", "convolution"]}
{"title": "neural network identification of people hidden from view with a single-pixel , single-photon detector", "abstract": "light scattered from multiple surfaces can be used to retrieve information of hidden environments . however , full three-dimensional retrieval of an object hidden from view by a wall has only been achieved with scanning systems and requires intensive computational processing of the retrieved data . here we use a non-scanning , single-photon single-pixel detector in combination with an artificial neural network : this allows us to locate the position and to also simultaneously provide the actual identity of a hidden person , chosen from a database of people ( n=3 ) . artificial neural networks applied to specific computational imaging problems can therefore enable novel imaging capabilities with hugely simplified hardware and processing times", "topics": ["pixel"]}
{"title": "auc-maximized deep convolutional neural fields for sequence labeling", "abstract": "deep convolutional neural networks ( dcnn ) has shown excellent performance in a variety of machine learning tasks . this manuscript presents deep convolutional neural fields ( deepcnf ) , a combination of dcnn with conditional random field ( crf ) , for sequence labeling with highly imbalanced label distribution . the widely-used training methods , such as maximum-likelihood and maximum labelwise accuracy , do not work well on highly imbalanced data . to handle this , we present a new training algorithm called maximum-auc for deepcnf . that is , we train deepcnf by directly maximizing the empirical area under the roc curve ( auc ) , which is an unbiased measurement for imbalanced data . to fulfill this , we formulate auc in a pairwise ranking framework , approximate it by a polynomial function and then apply a gradient-based procedure to optimize it . we then test our auc-maximized deepcnf on three very different protein sequence labeling tasks : solvent accessibility prediction , 8-state secondary structure prediction , and disorder prediction . our experimental results confirm that maximum-auc greatly outperforms the other two training methods on 8-state secondary structure prediction and disorder prediction since their label distributions are highly imbalanced and also have similar performance as the other two training methods on the solvent accessibility prediction problem which has three equally-distributed labels . furthermore , our experimental results also show that our auc-trained deepcnf models greatly outperform existing popular predictors of these three tasks .", "topics": ["approximation algorithm", "gradient"]}
{"title": "maximizing a nonnegative , monotone , submodular function constrained to matchings", "abstract": "submodular functions have many applications . matchings have many applications . the bitext word alignment problem can be modeled as the problem of maximizing a nonnegative , monotone , submodular function constrained to matchings in a complete bipartite graph where each vertex corresponds to a word in the two input sentences and each edge represents a potential word-to-word translation . we propose a more general problem of maximizing a nonnegative , monotone , submodular function defined on the edge set of a complete graph constrained to matchings ; we call this problem the csm-matching problem . csm-matching also generalizes the maximum-weight matching problem , which has a polynomial-time algorithm ; however , we show that it is np-hard to approximate csm-matching within a factor of e/ ( e-1 ) by reducing the max k-cover problem to it . our main result is a simple , greedy , 3-approximation algorithm for csm-matching . then we reduce csm-matching to maximizing a nonnegative , monotone , submodular function over two matroids , i.e . , csm-2-matroids . csm-2-matroids has a ( 2+epsilon ) -approximation algorithm - called lsv2 . we show that we can find a ( 4+epsilon ) -approximate solution to csm-matching using lsv2 . we extend this approach to similar problems .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "an online ride-sharing path planning strategy for public vehicle systems", "abstract": "as efficient traffic-management platforms , public vehicle ( pv ) systems are envisioned to be a promising approach to solving traffic congestions and pollutions for future smart cities . pv systems provide online/dynamic peer-to-peer ride-sharing services with the goal of serving sufficient number of customers with minimum number of vehicles and lowest possible cost . a key component of the pv system is the online ride-sharing scheduling strategy . in this paper , we propose an efficient path planning strategy that focuses on a limited potential search area for each vehicle by filtering out the requests that violate passenger service quality level , so that the global search is reduced to local search . we analyze the performance of the proposed solution such as reduction ratio of computational complexity . simulations based on the manhattan taxi data set show that , the computing time is reduced by 22 % compared with the exhaustive search method under the same service quality performance .", "topics": ["computational complexity theory", "simulation"]}
{"title": "a tree augmented naive bayesian network experiment for breast cancer prediction", "abstract": "in order to investigate the breast cancer prediction problem on the aging population with the grades of dcis , we conduct a tree augmented naive bayesian network experiment trained and tested on a large clinical dataset including consecutive diagnostic mammography examinations , consequent biopsy outcomes and related cancer registry records in the population of women across all ages . the aggregated results of our ten-fold cross validation method recommend a biopsy threshold higher than 2 % for the aging population .", "topics": ["bayesian network"]}
{"title": "measuring cultural relativity of emotional valence and arousal using semantic clustering and twitter", "abstract": "researchers since at least darwin have debated whether and to what extent emotions are universal or culture-dependent . however , previous studies have primarily focused on facial expressions and on a limited set of emotions . given that emotions have a substantial impact on human lives , evidence for cultural emotional relativity might be derived by applying distributional semantics techniques to a text corpus of self-reported behaviour . here , we explore this idea by measuring the valence and arousal of the twelve most popular emotion keywords expressed on the micro-blogging site twitter . we do this in three geographical regions : europe , asia and north america . we demonstrate that in our sample , the valence and arousal levels of the same emotion keywords differ significantly with respect to these geographical regions -- - europeans are , or at least present themselves as more positive and aroused , north americans are more negative and asians appear to be more positive but less aroused when compared to global valence and arousal levels of the same emotion keywords . our work is the first in kind to programatically map large text corpora to a dimensional model of affect .", "topics": ["text corpus", "map"]}
{"title": "thompson sampling is asymptotically optimal in general environments", "abstract": "we discuss a variant of thompson sampling for nonparametric reinforcement learning in a countable classes of general stochastic environments . these environments can be non-markov , non-ergodic , and partially observable . we show that thompson sampling learns the environment class in the sense that ( 1 ) asymptotically its value converges to the optimal value in mean and ( 2 ) given a recoverability assumption regret is sublinear .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "learning to relate images : mapping units , complex cells and simultaneous eigenspaces", "abstract": "a fundamental operation in many vision tasks , including motion understanding , stereopsis , visual odometry , or invariant recognition , is establishing correspondences between images or between images and data from other modalities . we present an analysis of the role that multiplicative interactions play in learning such correspondences , and we show how learning and inferring relationships between images can be viewed as detecting rotations in the eigenspaces shared among a set of orthogonal matrices . we review a variety of recent multiplicative sparse coding methods in light of this observation . we also review how the squaring operation performed by energy models and by models of complex cells can be thought of as a way to implement multiplicative interactions . this suggests that the main utility of including complex cells in computational models of vision may be that they can encode relations not invariances .", "topics": ["interaction", "sparse matrix"]}
{"title": "nonparametric nearest neighbor descent clustering based on delaunay triangulation", "abstract": "in our physically inspired in-tree ( it ) based clustering algorithm and the series after it , there is only one free parameter involved in computing the potential value of each point . in this work , based on the delaunay triangulation or its dual voronoi tessellation , we propose a nonparametric process to compute potential values by the local information . this computation , though nonparametric , is relatively very rough , and consequently , many local extreme points will be generated . however , unlike those gradient-based methods , our it-based methods are generally insensitive to those local extremes . this positively demonstrates the superiority of these parametric ( previous ) and nonparametric ( in this work ) it-based methods .", "topics": ["cluster analysis", "computation"]}
{"title": "deep motif dashboard : visualizing and understanding genomic sequences using deep neural networks", "abstract": "deep neural network ( dnn ) models have recently obtained state-of-the-art prediction accuracy for the transcription factor binding ( tfbs ) site classification task . however , it remains unclear how these approaches identify meaningful dna sequence signals and give insights as to why tfs bind to certain locations . in this paper , we propose a toolkit called the deep motif dashboard ( demo dashboard ) which provides a suite of visualization strategies to extract motifs , or sequence patterns from deep neural network models for tfbs classification . we demonstrate how to visualize and understand three important dnn models : convolutional , recurrent , and convolutional-recurrent networks . our first visualization method is finding a test sequence 's saliency map which uses first-order derivatives to describe the importance of each nucleotide in making the final prediction . second , considering recurrent models make predictions in a temporal manner ( from one end of a tfbs sequence to the other ) , we introduce temporal output scores , indicating the prediction score of a model over time for a sequential input . lastly , a class-specific visualization strategy finds the optimal input sequence for a given tfbs positive class via stochastic gradient optimization . our experimental results indicate that a convolutional-recurrent architecture performs the best among the three architectures . the visualization techniques indicate that cnn-rnn makes predictions by modeling both motifs as well as dependencies among them .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "steerable cnns", "abstract": "it has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks . in this paper we present steerable convolutional neural networks , an efficient and flexible class of equivariant convolutional networks . we show that steerable cnns achieve state of the art results on the cifar image classification benchmark . the mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types , each one associated with a particular kind of symmetry . we show how the parameter cost of a steerable filter bank depends on the types of the input and output features , and show how to use this knowledge to construct cnns that utilize parameters effectively .", "topics": ["computer vision"]}
{"title": "semi-supervised learning with generative adversarial networks", "abstract": "we extend generative adversarial networks ( gans ) to the semi-supervised context by forcing the discriminator network to output class labels . we train a generative model g and a discriminator d on a dataset with inputs belonging to one of n classes . at training time , d is made to predict which of n+1 classes the input belongs to , where an extra class is added to correspond to the outputs of g. we show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular gan .", "topics": ["generative model", "supervised learning"]}
{"title": "gradient estimators for implicit models", "abstract": "implicit models , which allow for the generation of samples but not for point-wise evaluation of probabilities , are omnipresent in real-world problems tackled by machine learning and a hot topic of current research . some examples include data simulators that are widely used in engineering and scientific research , generative adversarial networks ( gans ) for image synthesis , and hot-off-the-press approximate inference techniques relying on implicit distributions . the majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation , which is liable to produce inaccurate updates and thus poor models . this paper alleviates the need for such approximations by proposing the stein gradient estimator , which directly estimates the score function of the implicitly defined distribution . the efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference , and entropy regularised gans that provide improved sample diversity .", "topics": ["approximation algorithm", "mathematical optimization"]}
{"title": "persistent homology in sparse regression and its application to brain morphometry", "abstract": "sparse systems are usually parameterized by a tuning parameter that determines the sparsity of the system . how to choose the right tuning parameter is a fundamental and difficult problem in learning the sparse system . in this paper , by treating the the tuning parameter as an additional dimension , persistent homological structures over the parameter space is introduced and explored . the structures are then further exploited in speeding up the computation using the proposed soft-thresholding technique . the topological structures are further used as multivariate features in the tensor-based morphometry ( tbm ) in characterizing white matter alterations in children who have experienced severe early life stress and maltreatment . these analyses reveal that stress-exposed children exhibit more diffuse anatomical organization across the whole white matter region .", "topics": ["computation", "sparse matrix"]}
{"title": "ls-vo : learning dense optical subspace for robust visual odometry estimation", "abstract": "this work proposes a novel deep network architecture to solve the camera ego-motion estimation problem . a motion estimation network generally learns features similar to optical flow ( of ) fields starting from sequences of images . this of can be described by a lower dimensional latent space . previous research has shown how to find linear approximations of this space . we propose to use an auto-encoder network to find a non-linear representation of the of manifold . in addition , we propose to learn the latent space jointly with the estimation task , so that the learned of features become a more robust description of the of input . we call this novel architecture ls-vo . the experiments show that ls-vo achieves a considerable increase in performances in respect to baselines , while the number of parameters of the estimation network only slightly increases .", "topics": ["baseline ( configuration management )", "nonlinear system"]}
{"title": "beyond parity : fairness objectives for collaborative filtering", "abstract": "we study fairness in collaborative-filtering recommender systems , which are sensitive to discrimination that exists in historical data . biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups . we identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness . these fairness metrics can be optimized by adding fairness terms to the learning objective . experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline , and that the fairness objectives effectively help reduce unfairness .", "topics": ["baseline ( configuration management )", "synthetic data"]}
{"title": "the alamo approach to machine learning", "abstract": "alamo is a computational methodology for leaning algebraic functions from data . given a data set , the approach begins by building a low-complexity , linear model composed of explicit non-linear transformations of the independent variables . linear combinations of these non-linear transformations allow a linear model to better approximate complex behavior observed in real processes . the model is refined , as additional data are obtained in an adaptive fashion through error maximization sampling using derivative-free optimization . models built using alamo can enforce constraints on the response variables to incorporate first-principles knowledge . the ability of alamo to generate simple and accurate models for a number of reaction problems is demonstrated . the error maximization sampling is compared with latin hypercube designs to demonstrate its sampling efficiency . alamo 's constrained regression methodology is used to further refine concentration models , resulting in models that perform better on validation data and satisfy upper and lower bounds placed on model outputs .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "adversarial neural machine translation", "abstract": "in this paper , we study a new learning paradigm for neural machine translation ( nmt ) . instead of maximizing the likelihood of the human translation as in previous works , we minimize the distinction between human translation and the translation given by an nmt model . to achieve this goal , inspired by the recent success of generative adversarial networks ( gans ) , we employ an adversarial training architecture and name it as adversarial-nmt . in adversarial-nmt , the training of the nmt model is assisted by an adversary , which is an elaborately designed convolutional neural network ( cnn ) . the goal of the adversary is to differentiate the translation result generated by the nmt model from that by human . the goal of the nmt model is to produce high quality translations so as to cheat the adversary . a policy gradient method is leveraged to co-train the nmt model and the adversary . experimental results on english $ \\rightarrow $ french and german $ \\rightarrow $ english translation tasks show that adversarial-nmt can achieve significantly better translation quality than several strong baselines .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "accuracy bounds for belief propagation", "abstract": "the belief propagation ( bp ) algorithm is widely applied to perform approximate inference on arbitrary graphical models , in part due to its excellent empirical properties and performance . however , little is known theoretically about when this algorithm will perform well . using recent analysis of convergence and stability properties in bp and new results on approximations in binary systems , we derive a bound on the error in bp 's estimates for pairwise markov random fields over discrete valued random variables . our bound is relatively simple to compute , and compares favorably with a previous method of bounding the accuracy of bp .", "topics": ["approximation algorithm", "graphical model"]}
{"title": "multiscale residual mixture of pca : dynamic dictionaries for optimal basis learning", "abstract": "in this paper we are interested in the problem of learning an over-complete basis and a methodology such that the reconstruction or inverse problem does not need optimization . we analyze the optimality of the presented approaches , their link to popular already known techniques s.a . artificial neural networks , k-means or oja 's learning rule . finally , we will see that one approach to reach the optimal dictionary is a factorial and hierarchical approach . the derived approach lead to a formulation of a deep oja network . we present results on different tasks and present the resulting very efficient learning algorithm which brings a new vision on the training of deep nets . finally , the theoretical work shows that deep frameworks are one way to efficiently have over-complete ( combinatorially large ) dictionary yet allowing easy reconstruction . we thus present the deep residual oja network ( dron ) . we demonstrate that a recursive deep approach working on the residuals allow exponential decrease of the error w.r.t . the depth .", "topics": ["mathematical optimization", "time complexity"]}
{"title": "sweep distortion removal from thz images via blind demodulation", "abstract": "heavy sweep distortion induced by alignments and inter-reflections of layers of a sample is a major burden in recovering 2d and 3d information in time resolved spectral imaging . this problem can not be addressed by conventional denoising and signal processing techniques as it heavily depends on the physics of the acquisition . here we propose and implement an algorithmic framework based on low-rank matrix recovery and alternating minimization that exploits the forward model for thz acquisition . the method allows recovering the original signal in spite of the presence of temporal-spatial distortions . we address a blind-demodulation problem , where based on several observations of the sample texture modulated by an undesired sweep pattern , the two classes of signals are separated . the performance of the method is examined in both synthetic and experimental data , and the successful reconstructions are demonstrated . the proposed general scheme can be implemented to advance inspection and imaging applications in thz and other time-resolved sensing modalities .", "topics": ["sampling ( signal processing )", "noise reduction"]}
{"title": "variational bayes in private settings ( vips )", "abstract": "many applications of bayesian data analysis involve sensitive information , motivating methods which ensure that privacy is protected . we introduce a general privacy-preserving framework for variational bayes ( vb ) , a widely used optimization-based bayesian inference method . our framework respects differential privacy , the gold-standard privacy criterion , and encompasses a large class of probabilistic models , called the conjugate exponential ( ce ) family . we observe that we can straightforwardly privatise vb 's approximate posterior distributions for models in the ce family , by perturbing the expected sufficient statistics of the complete-data likelihood . for a broadly-used class of non-ce models , those with binomial likelihoods , we show how to bring such models into the ce family , such that inferences in the modified model resemble the private variational bayes algorithm as closely as possible , using the polya-gamma data augmentation scheme . the iterative nature of variational bayes presents a further challenge since iterations increase the amount of noise needed . we overcome this by combining : ( 1 ) an improved composition method for differential privacy , called the moments accountant , which provides a tight bound on the privacy cost of multiple vb iterations and thus significantly decreases the amount of additive noise ; and ( 2 ) the privacy amplification effect of subsampling mini-batches from large-scale data in stochastic learning . we empirically demonstrate the effectiveness of our method in ce and non-ce models including latent dirichlet allocation , bayesian logistic regression , and sigmoid belief networks , evaluated on real-world datasets .", "topics": ["calculus of variations", "bayesian network"]}
{"title": "the bees algorithm for the vehicle routing problem", "abstract": "in this thesis we present a new algorithm for the vehicle routing problem called the enhanced bees algorithm . it is adapted from a fairly recent algorithm , the bees algorithm , which was developed for continuous optimisation problems . we show that the results obtained by the enhanced bees algorithm are competitive with the best meta-heuristics available for the vehicle routing problem ( within 0.5 % of the optimal solution for common benchmark problems ) . we show that the algorithm has good runtime performance , producing results within 2 % of the optimal solution within 60 seconds , making it suitable for use within real world dispatch scenarios .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "beyond word importance : contextual decomposition to extract interactions from lstms", "abstract": "the driving force behind the recent success of lstms has been their ability to learn complex and non-linear relationships . consequently , our inability to describe these relationships has led to lstms being characterized as black boxes . to this end , we introduce contextual decomposition ( cd ) , an interpretation algorithm for analysing individual predictions made by standard lstms , without any changes to the underlying model . by decomposing the output of a lstm , cd captures the contributions of combinations of words or variables to the final prediction of an lstm . on the task of sentiment analysis with the yelp and sst data sets , we show that cd is able to reliably identify words and phrases of contrasting sentiment , and how they are combined to yield the lstm 's final prediction . using the phrase-level labels in sst , we also demonstrate that cd is able to successfully extract positive and negative negations from an lstm , something which has not previously been done .", "topics": ["nonlinear system"]}
{"title": "fitted learning : models with awareness of their limits", "abstract": "though deep learning has pushed the boundaries of classification forward , in recent years hints of the limits of standard classification have begun to emerge . problems such as fooling , adding new classes over time , and the need to retrain learning models only for small changes to the original problem all point to a potential shortcoming in the classic classification regime , where a comprehensive a priori knowledge of the possible classes or concepts is critical . without such knowledge , classifiers misjudge the limits of their knowledge and overgeneralization therefore becomes a serious obstacle to consistent performance . in response to these challenges , this paper extends the classic regime by reframing classification instead with the assumption that concepts present in the training set are only a sample of the hypothetical final set of concepts . to bring learning models into this new paradigm , a novel elaboration of standard architectures called the competitive overcomplete output layer ( cool ) neural network is introduced . experiments demonstrate the effectiveness of cool by applying it to fooling , separable concept learning , one-class neural networks , and standard classification benchmarks . the results suggest that , unlike conventional classifiers , the amount of generalization in cool networks can be tuned to match the problem .", "topics": ["test set"]}
{"title": "learning wasserstein embeddings", "abstract": "the wasserstein distance received a lot of attention recently in the community of machine learning , especially for its principled way of comparing distributions . it has found numerous applications in several hard problems , such as domain adaptation , dimensionality reduction or generative models . however , its use is still limited by a heavy computational cost . our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity . it relies on the search of an embedding where the euclidean distance mimics the wasserstein distance . we show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space . once this embedding has been found , computing optimization problems in the wasserstein space ( e.g . barycenters , principal directions or even archetypes ) can be conducted extremely fast . numerical experiments supporting this idea are conducted on image datasets , and show the wide potential benefits of our method .", "topics": ["generative model", "approximation"]}
{"title": "pde-net : learning pdes from data", "abstract": "in this paper , we present an initial attempt to learn evolution pdes from data . inspired by the latest development of neural network designs in deep learning , we propose a new feed-forward deep network , called pde-net , to fulfill two objectives at the same time : to accurately predict dynamics of complex systems and to uncover the underlying hidden pde models . the basic idea of the proposed pde-net is to learn differential operators by learning convolution kernels ( filters ) , and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses . comparing with existing approaches , which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators , our approach has the most flexibility by learning both differential operators and the nonlinear responses . a special feature of the proposed pde-net is that all filters are properly constrained , which enables us to easily identify the governing pde models while still maintaining the expressive and predictive power of the network . these constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters ( an important concept originated from wavelet theory ) . we also discuss relations of the pde-net with some existing networks in computer vision such as network-in-network ( nin ) and residual neural network ( resnet ) . numerical experiments show that the pde-net has the potential to uncover the hidden pde of the observed dynamics , and predict the dynamical behavior for a relatively long time , even in a noisy environment .", "topics": ["approximation algorithm", "nonlinear system"]}
{"title": "simplified long short-term memory recurrent neural networks : part iii", "abstract": "this is part iii of three-part work . in parts i and ii , we have presented eight variants for simplified long short term memory ( lstm ) recurrent neural networks ( rnns ) . it is noted that fast computation , specially in constrained computing resources , are an important factor in processing big time-sequence data . in this part iii paper , we present and evaluate two new lstm model variants which dramatically reduce the computational load while retaining comparable performance to the base ( standard ) lstm rnns . in these new variants , we impose ( hadamard ) pointwise state multiplications in the cell-memory network in addition to the gating signal networks .", "topics": ["recurrent neural network", "computation"]}
{"title": "variational gaussian process auto-encoder for ordinal prediction of facial action units", "abstract": "we address the task of simultaneous feature fusion and modeling of discrete ordinal outputs . we propose a novel gaussian process ( gp ) auto-encoder modeling approach . in particular , we introduce gp encoders to project multiple observed features onto a latent space , while gp decoders are responsible for reconstructing the original features . inference is performed in a novel variational framework , where the recovered latent representations are further constrained by the ordinal output labels . in this way , we seamlessly integrate the ordinal structure in the learned manifold , while attaining robust fusion of the input features . we demonstrate the representation abilities of our model on benchmark datasets from machine learning and affect analysis . we further evaluate the model on the tasks of feature fusion and joint ordinal prediction of facial action units . our experiments demonstrate the benefits of the proposed approach compared to the state of the art .", "topics": ["calculus of variations", "encoder"]}
{"title": "on the stability of deep networks", "abstract": "in this work we study the properties of deep neural networks ( dnn ) with random weights . we formally prove that these networks perform a distance-preserving embedding of the data . based on this we then draw conclusions on the size of the training data and the networks ' structure . a longer version of this paper with more results and details can be found in ( giryes et al . , 2015 ) . in particular , we formally prove in the longer version that dnn with random gaussian weights perform a distance-preserving embedding of the data , with a special treatment for in-class and out-of-class data .", "topics": ["test set"]}
{"title": "learning convex regularizers for optimal bayesian denoising", "abstract": "we propose a data-driven algorithm for the maximum a posteriori ( map ) estimation of stochastic processes from noisy observations . the primary statistical properties of the sought signal is specified by the penalty function ( i.e . , negative logarithm of the prior probability density function ) . our alternating direction method of multipliers ( admm ) -based approach translates the estimation task into successive applications of the proximal mapping of the penalty function . capitalizing on this direct link , we define the proximal operator as a parametric spline curve and optimize the spline coefficients by minimizing the average reconstruction error for a given training set . the key aspects of our learning method are that the associated penalty function is constrained to be convex and the convergence of the admm iterations is proven . as a result of these theoretical guarantees , adaptation of the proposed framework to different levels of measurement noise is extremely simple and does not require any retraining . we apply our method to estimation of both sparse and non-sparse models of l\\ ' { e } vy processes for which the minimum mean square error ( mmse ) estimators are available . we carry out a single training session and perform comparisons at various signal-to-noise ratio ( snr ) values . simulations illustrate that the performance of our algorithm is practically identical to the one of the mmse estimator irrespective of the noise power .", "topics": ["test set", "noise reduction"]}
{"title": "regret in online combinatorial optimization", "abstract": "we address online linear optimization problems when the possible actions of the decision maker are represented by binary vectors . the regret of the decision maker is the difference between her realized loss and the best loss she would have achieved by picking , in hindsight , the best possible action . our goal is to understand the magnitude of the best possible ( minimax ) regret . we study the problem under three different assumptions for the feedback the decision maker receives : full information , and the partial information models of the so-called `` semi-bandit '' and `` bandit '' problems . combining the mirror descent algorithm and the inf ( implicitely normalized forecaster ) strategy , we are able to prove optimal bounds for the semi-bandit case . we also recover the optimal bounds for the full information setting . in the bandit case we discuss existing results in light of a new lower bound , and suggest a conjecture on the optimal regret in that case . finally we also prove that the standard exponentially weighted average forecaster is provably suboptimal in the setting of online combinatorial optimization .", "topics": ["regret ( decision theory )", "gradient descent"]}
{"title": "a comparison between supervised learning algorithms for word sense disambiguation", "abstract": "this paper describes a set of comparative experiments , including cross-corpus evaluation , between five alternative algorithms for supervised word sense disambiguation ( wsd ) , namely naive bayes , exemplar-based learning , snow , decision lists , and boosting . two main conclusions can be drawn : 1 ) the lazyboosting algorithm outperforms the other four state-of-the-art algorithms in terms of accuracy and ability to tune to new domains ; 2 ) the domain dependence of wsd systems seems very strong and suggests that some kind of adaptation or tuning is required for cross-corpus application .", "topics": ["supervised learning"]}
{"title": "clustering with deep learning : taxonomy and new methods", "abstract": "clustering is a fundamental machine learning method . the quality of its results is dependent on the data distribution . for this reason , deep neural networks can be used for learning better representations of the data . in this paper , we propose a systematic taxonomy for clustering with deep learning , in addition to a review of methods from the field . based on our taxonomy , creating new methods is more straightforward . we also propose a new approach which is built on the taxonomy and surpasses some of the limitations of some previous work . our experimental evaluation on image datasets shows that the method approaches state-of-the-art clustering quality , and performs better in some cases .", "topics": ["cluster analysis"]}
{"title": "kernel mean embedding of distributions : a review and beyond", "abstract": "a hilbert space embedding of a distribution -- -in short , a kernel mean embedding -- -has recently emerged as a powerful tool for machine learning and inference . the basic idea behind this framework is to map distributions into a reproducing kernel hilbert space ( rkhs ) in which the whole arsenal of kernel methods can be extended to probability measures . it can be viewed as a generalization of the original `` feature map '' common to support vector machines ( svms ) and other kernel methods . while initially closely associated with the latter , it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference , causal discovery , and deep learning . the goal of this survey is to give a comprehensive review of existing work and recent advances in this research area , and to discuss the most challenging issues and open problems that could lead to new research directions . the survey begins with a brief introduction to the rkhs and positive definite kernels which forms the backbone of this survey , followed by a thorough discussion of the hilbert space embedding of marginal distributions , theoretical guarantees , and a review of its applications . the embedding of distributions enables us to apply rkhs methods to probability measures which prompts a wide range of applications such as kernel two-sample testing , independent testing , and learning on distributional data . next , we discuss the hilbert space embedding for conditional distributions , give theoretical insights , and review some applications . the conditional mean embedding enables us to perform sum , product , and bayes ' rules -- -which are ubiquitous in graphical model , probabilistic inference , and reinforcement learning -- -in a non-parametric way . we then discuss relationships between this framework and other related areas . lastly , we give some suggestions on future research directions .", "topics": ["kernel ( operating system )", "graphical model"]}
{"title": "an analysis of unsupervised pre-training in light of recent advances", "abstract": "convolutional neural networks perform well on object recognition because of a number of recent advances : rectified linear units ( relus ) , data augmentation , dropout , and large labelled datasets . unsupervised data has been proposed as another way to improve performance . unfortunately , unsupervised pre-training is not used by state-of-the-art methods leading to the following question : is unsupervised pre-training still useful given recent advances ? if so , when ? we answer this in three parts : we 1 ) develop an unsupervised method that incorporates relus and recent unsupervised regularization techniques , 2 ) analyze the benefits of unsupervised pre-training compared to data augmentation and dropout on cifar-10 while varying the ratio of unsupervised to supervised samples , 3 ) verify our findings on stl-10 . we discover unsupervised pre-training , as expected , helps when the ratio of unsupervised to supervised samples is high , and surprisingly , hurts when the ratio is low . we also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on stl-10 .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "3d shape reconstruction from sketches via multi-view convolutional networks", "abstract": "we propose a method for reconstructing 3d shapes from 2d sketches in the form of line drawings . our method takes as input a single sketch , or multiple sketches , and outputs a dense point cloud representing a 3d reconstruction of the input sketch ( es ) . the point cloud is then converted into a polygon mesh . at the heart of our method lies a deep , encoder-decoder network . the encoder converts the sketch into a compact representation encoding shape information . the decoder converts this representation into depth and normal maps capturing the underlying surface from several output viewpoints . the multi-view maps are then consolidated into a 3d point cloud by solving an optimization problem that fuses depth and normals across all viewpoints . based on our experiments , compared to other methods , such as volumetric networks , our architecture offers several advantages , including more faithful reconstruction , higher output surface resolution , better preservation of topology and shape structure .", "topics": ["optimization problem", "map"]}
{"title": "incremental sequence learning", "abstract": "deep learning research over the past years has shown that by increasing the scope or difficulty of the learning problem over time , increasingly complex learning problems can be addressed . we study incremental learning in the context of sequence learning , using generative rnns in the form of multi-layer recurrent mixture density networks . while the potential of incremental or curriculum learning to enhance learning is known , indiscriminate application of the principle does not necessarily lead to improvement , and it is essential therefore to know which forms of incremental or curriculum learning have a positive effect . this research contributes to that aim by comparing three instantiations of incremental or curriculum learning . we introduce incremental sequence learning , a simple incremental approach to sequence learning . incremental sequence learning starts out by using only the first few steps of each sequence as training data . each time a performance criterion has been reached , the length of the parts of the sequences used for training is increased . we introduce and make available a novel sequence learning task and data set : predicting and classifying mnist pen stroke sequences . we find that incremental sequence learning greatly speeds up sequence learning and reaches the best test performance level of regular sequence learning 20 times faster , reduces the test error by 74 % , and in general performs more robustly ; it displays lower variance and achieves sustained progress after all three comparison methods have stopped improving . the other instantiations of curriculum learning do not result in any noticeable improvement . a trained sequence prediction model is also used in transfer learning to the task of sequence classification , where it is found that transfer learning realizes improved classification performance compared to methods that learn to classify from scratch .", "topics": ["test set", "mnist database"]}
{"title": "bayesian rose trees", "abstract": "hierarchical structure is ubiquitous in data across many domains . there are many hierarchical clustering methods , frequently used by domain experts , which strive to discover this structure . however , most of these methods limit discoverable hierarchies to those with binary branching structure . this limitation , while computationally convenient , is often undesirable . in this paper we explore a bayesian hierarchical clustering algorithm that can produce trees with arbitrary branching structure at each node , known as rose trees . we interpret these trees as mixtures over partitions of a data set , and use a computationally efficient , greedy agglomerative algorithm to find the rose trees which have high marginal likelihood given the data . lastly , we perform experiments which demonstrate that rose trees are better models of data than the typical binary trees returned by other hierarchical clustering algorithms .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "lipschitz parametrization of probabilistic graphical models", "abstract": "we show that the log-likelihood of several probabilistic graphical models is lipschitz continuous with respect to the lp-norm of the parameters . we discuss several implications of lipschitz parametrization . we present an upper bound of the kullback-leibler divergence that allows understanding methods that penalize the lp-norm of differences of parameters as the minimization of that upper bound . the expected log-likelihood is lower bounded by the negative lp-norm , which allows understanding the generalization ability of probabilistic models . the exponential of the negative lp-norm is involved in the lower bound of the bayes error rate , which shows that it is reasonable to use parameters as features in algorithms that rely on metric spaces ( e.g . classification , dimensionality reduction , clustering ) . our results do not rely on specific algorithms for learning the structure or parameters . we show preliminary results for activity recognition and temporal segmentation .", "topics": ["graphical model", "cluster analysis"]}
{"title": "sketched subspace clustering", "abstract": "the immense amount of daily generated and communicated data presents unique challenges in their processing . clustering , the grouping of data without the presence of ground-truth labels , is an important tool for drawing inferences from data . subspace clustering ( sc ) is a relatively recent method that is able to successfully classify nonlinearly separable data in a multitude of settings . in spite of their high clustering accuracy , sc methods incur prohibitively high computational complexity when processing large volumes of high-dimensional data . inspired by random sketching approaches for dimensionality reduction , the present paper introduces a randomized scheme for sc , termed sketch-sc , tailored for large volumes of high-dimensional data . sketch-sc accelerates the computationally heavy parts of state-of-the-art sc approaches by compressing the data matrix across both dimensions using random projections , thus enabling fast and accurate large-scale sc . performance analysis as well as extensive numerical tests on real data corroborate the potential of sketch-sc and its competitive performance relative to state-of-the-art scalable sc approaches .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "learning factor graphs in polynomial time & sample complexity", "abstract": "we study computational and sample complexity of parameter and structure learning in graphical models . our main result shows that the class of factor graphs with bounded factor size and bounded connectivity can be learned in polynomial time and polynomial number of samples , assuming that the data is generated by a network in this class . this result covers both parameter estimation for a known network structure and structure learning . it implies as a corollary that we can learn factor graphs for both bayesian networks and markov networks of bounded degree , in polynomial time and sample complexity . unlike maximum likelihood estimation , our method does not require inference in the underlying network , and so applies to networks where inference is intractable . we also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks .", "topics": ["graphical model", "time complexity"]}
{"title": "deep burst denoising", "abstract": "noise is an inherent issue of low-light image capture , one which is exacerbated on mobile devices due to their narrow apertures and small sensors . one strategy for mitigating noise in a low-light situation is to increase the shutter time of the camera , thus allowing each photosite to integrate more light and decrease noise variance . however , there are two downsides of long exposures : ( a ) bright regions can exceed the sensor range , and ( b ) camera and scene motion will result in blurred images . another way of gathering more light is to capture multiple short ( thus noisy ) frames in a `` burst '' and intelligently integrate the content , thus avoiding the above downsides . in this paper , we use the burst-capture strategy and implement the intelligent integration via a recurrent fully convolutional deep neural net ( cnn ) . we build our novel , multiframe architecture to be a simple addition to any single frame denoising model , and design to handle an arbitrary number of noisy input frames . we show that it achieves state of the art denoising results on our burst dataset , improving on the best published multi-frame techniques , such as vbm4d and flexisp . finally , we explore other applications of image enhancement by integrating content from multiple frames and demonstrate that our dnn architecture generalizes well to image super-resolution .", "topics": ["image processing", "noise reduction"]}
{"title": "performance comparisons of pso based clustering", "abstract": "in this paper we have investigated the performance of pso particle swarm optimization based clustering on few real world data sets and one artificial data set . the performances are measured by two metric namely quantization error and inter-cluster distance . the k means clustering algorithm is first implemented for all data sets , the results of which form the basis of comparison of pso based approaches . we have explored different variants of pso such as gbest , lbest ring , lbest vonneumann and hybrid pso for comparison purposes . the results reveal that pso based clustering algorithms perform better compared to k means in all data sets .", "topics": ["cluster analysis"]}
{"title": "discovering multiple constraints that are frequently approximately satisfied", "abstract": "some high-dimensional data.sets can be modelled by assuming that there are many different linear constraints , each of which is frequently approximately satisfied ( fas ) by the data . the probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations . we describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations .", "topics": ["generative model"]}
{"title": "reuse of neural modules for general video game playing", "abstract": "a general approach to knowledge transfer is introduced in which an agent controlled by a neural network adapts how it reuses existing networks as it learns in a new domain . networks trained for a new domain can improve their performance by routing activation selectively through previously learned neural structure , regardless of how or for what it was learned . a neuroevolution implementation of this approach is presented with application to high-dimensional sequential decision-making domains . this approach is more general than previous approaches to neural transfer for reinforcement learning . it is domain-agnostic and requires no prior assumptions about the nature of task relatedness or mappings . the method is analyzed in a stochastic version of the arcade learning environment , demonstrating that it improves performance in some of the more complex atari 2600 games , and that the success of transfer can be predicted based on a high-level characterization of game dynamics .", "topics": ["high- and low-level", "reinforcement learning"]}
{"title": "sailing the information ocean with awareness of currents : discovery and application of source dependence", "abstract": "the web has enabled the availability of a huge amount of useful information , but has also eased the ability to spread false information and rumors across multiple sources , making it hard to distinguish between what is true and what is not . recent examples include the premature steve jobs obituary , the second bankruptcy of united airlines , the creation of black holes by the operation of the large hadron collider , etc . since it is important to permit the expression of dissenting and conflicting opinions , it would be a fallacy to try to ensure that the web provides only consistent information . however , to help in separating the wheat from the chaff , it is essential to be able to determine dependence between sources . given the huge number of data sources and the vast volume of conflicting data available on the web , doing so in a scalable manner is extremely challenging and has not been addressed by existing work yet . in this paper , we present a set of research problems and propose some preliminary solutions on the issues involved in discovering dependence between sources . we also discuss how this knowledge can benefit a variety of technologies , such as data integration and web 2.0 , that help users manage and access the totality of the available information from various sources .", "topics": ["scalability"]}
{"title": "interpnet : neural introspection for interpretable deep learning", "abstract": "humans are able to explain their reasoning . on the contrary , deep neural networks are not . this paper attempts to bridge this gap by introducing a new way to design interpretable neural networks for classification , inspired by physiological evidence of the human visual system 's inner-workings . this paper proposes a neural network design paradigm , termed interpnet , which can be combined with any existing classification architecture to generate natural language explanations of the classifications . the success of the module relies on the assumption that the network 's computation and reasoning is represented in its internal layer activations . while in principle interpnet could be applied to any existing classification architecture , it is evaluated via an image classification and explanation task . experiments on a cub bird classification and explanation dataset show qualitatively and quantitatively that the model is able to generate high-quality explanations . while the current state-of-the-art meteor score on this dataset is 29.2 , interpnet achieves a much higher meteor score of 37.9 .", "topics": ["computer vision", "natural language"]}
{"title": "automatic case acquisition from texts for process-oriented case-based reasoning", "abstract": "this paper introduces a method for the automatic acquisition of a rich case representation from free text for process-oriented case-based reasoning . case engineering is among the most complicated and costly tasks in implementing a case-based reasoning system . this is especially so for process-oriented case-based reasoning , where more expressive case representations are generally used and , in our opinion , actually required for satisfactory case adaptation . in this context , the ability to acquire cases automatically from procedural texts is a major step forward in order to reason on processes . we therefore detail a methodology that makes case acquisition from processes described as free text possible , with special attention given to assembly instruction texts . this methodology extends the techniques we used to extract actions from cooking recipes . we argue that techniques taken from natural language processing are required for this task , and that they give satisfactory results . an evaluation based on our implemented prototype extracting workflows from recipe texts is provided .", "topics": ["natural language processing"]}
{"title": "joint modeling of accents and acoustics for multi-accent speech recognition", "abstract": "the performance of automatic speech recognition systems degrades with increasing mismatch between the training and testing scenarios . differences in speaker accents are a significant source of such mismatch . the traditional approach to deal with multiple accents involves pooling data from several accents during training and building a single model in multi-task fashion , where tasks correspond to individual accents . in this paper , we explore an alternate model where we jointly learn an accent classifier and a multi-task acoustic model . experiments on the american english wall street journal and british english cambridge corpora demonstrate that our joint model outperforms the strong multi-task acoustic model baseline . we obtain a 5.94 % relative improvement in word error rate on british english , and 9.47 % relative improvement on american english . this illustrates that jointly modeling with accent information improves acoustic model performance .", "topics": ["baseline ( configuration management )", "speech recognition"]}
{"title": "a brief survey of deep reinforcement learning", "abstract": "deep reinforcement learning is poised to revolutionise the field of ai and represents a step towards building autonomous systems with a higher level understanding of the visual world . currently , deep learning is enabling reinforcement learning to scale to problems that were previously intractable , such as learning to play video games directly from pixels . deep reinforcement learning algorithms are also applied to robotics , allowing control policies for robots to be learned directly from camera inputs in the real world . in this survey , we begin with an introduction to the general field of reinforcement learning , then progress to the main streams of value-based and policy-based methods . our survey will cover central algorithms in deep reinforcement learning , including the deep $ q $ -network , trust region policy optimisation , and asynchronous advantage actor-critic . in parallel , we highlight the unique advantages of deep neural networks , focusing on visual understanding via reinforcement learning . to conclude , we describe several current areas of research within the field .", "topics": ["reinforcement learning"]}
{"title": "from averaging to acceleration , there is only a step-size", "abstract": "we show that accelerated gradient descent , averaged gradient descent and the heavy-ball method for non-strongly-convex problems may be reformulated as constant parameter second-order difference equation algorithms , where stability of the system is equivalent to convergence at rate o ( 1/n 2 ) , where n is the number of iterations . we provide a detailed analysis of the eigenvalues of the corresponding linear dynamical system , showing various oscillatory and non-oscillatory behaviors , together with a sharp stability result with explicit constants . we also consider the situation where noisy gradients are available , where we extend our general convergence result , which suggests an alternative algorithm ( i.e . , with different step sizes ) that exhibits the good aspects of both averaging and acceleration .", "topics": ["gradient descent", "iteration"]}
{"title": "learning visually grounded sentence representations", "abstract": "we introduce a variety of models , trained on a supervised image captioning corpus to predict the image features for a given caption , to perform sentence representation grounding . we train a grounded sentence encoder that achieves good performance on coco caption and image retrieval and subsequently show that this encoder can successfully be transferred to various nlp tasks , with improved performance over text-only models . lastly , we analyze the contribution of grounding , and show that word embeddings learned by this system outperform non-grounded ones .", "topics": ["natural language processing", "encoder"]}
{"title": "a framework for estimating long term driver behavior", "abstract": "the authors present a cyber-physical systems study on the estimation of driver behavior in autonomous vehicles and vehicle safety systems . extending upon previous work , the approach described is suitable for the long term estimation and tracking of autonomous vehicle behavior . the proposed system makes use of a previously defined hybrid state system and hidden markov model ( hss+hmm ) system which has provided good results for driver behavior estimation . the hss+hmm system utilizes the hybrid characteristics of decision-behavior coupling of many systems such as the driver and the vehicle , uses kalman filter estimates of observable parameters to track the instantaneous continuous state , and estimates the most likely driver state . the hss+hmm system is encompassed in a hss structure and inter-system connectivity is determined by using signal processing and pattern recognition techniques . the proposed method is suitable for scenarios that involve unknown decisions of other individuals , such as lane changes or intersection precedence/access . the long term driver behavior estimation system involves an extended hss+hmm structure that is capable of including external information in the estimation process . through the grafting and pruning of metastates , the hss+hmm system can be dynamically updated to best represent driver choices given external information . three application examples are also provided to elucidate the theoretical system .", "topics": ["autonomous car", "markov chain"]}
{"title": "randomized clustered nystrom for large-scale kernel machines", "abstract": "the nystrom method has been popular for generating the low-rank approximation of kernel matrices that arise in many machine learning problems . the approximation quality of the nystrom method depends crucially on the number of selected landmark points and the selection procedure . in this paper , we present a novel algorithm to compute the optimal nystrom low-approximation when the number of landmark points exceed the target rank . moreover , we introduce a randomized algorithm for generating landmark points that is scalable to large-scale data sets . the proposed method performs k-means clustering on low-dimensional random projections of a data set and , thus , leads to significant savings for high-dimensional data sets . our theoretical results characterize the tradeoffs between the accuracy and efficiency of our proposed method . extensive experiments demonstrate the competitive performance as well as the efficiency of our proposed method .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "pso-optimized hopfield neural network-based multipath routing for mobile ad-hoc networks", "abstract": "mobile ad-hoc network ( manet ) is a dynamic collection of mobile computers without the need for any existing infrastructure . nodes in a manet act as hosts and routers . designing of robust routing algorithms for manets is a challenging task . disjoint multipath routing protocols address this problem and increase the reliability , security and lifetime of network . however , selecting an optimal multipath is an np-complete problem . in this paper , hopfield neural network ( hnn ) which its parameters are optimized by particle swarm optimization ( pso ) algorithm is proposed as multipath routing algorithm . link expiration time ( let ) between each two nodes is used as the link reliability estimation metric . this approach can find either node-disjoint or link-disjoint paths in single phase route discovery . simulation results confirm that pso-hnn routing algorithm has better performance as compared to backup path set selection algorithm ( bpsa ) in terms of the path set reliability and number of paths in the set .", "topics": ["simulation"]}
{"title": "$ eva^2 $ : exploiting temporal redundancy in live computer vision", "abstract": "hardware support for deep convolutional neural networks ( cnns ) is critical to advanced computer vision in mobile and embedded devices . current designs , however , accelerate generic cnns ; they do not exploit the unique characteristics of real-time vision . we propose to use the temporal redundancy in natural video to avoid unnecessary computation on most frames . a new algorithm , activation motion compensation , detects changes in the visual input and incrementally updates a previously-computed output . the technique takes inspiration from video compression and applies well-known motion estimation techniques to adapt to visual changes . we use an adaptive key frame rate to control the trade-off between efficiency and vision quality as the input changes . we implement the technique in hardware as an extension to existing state-of-the-art cnn accelerator designs . the new unit reduces the average energy per frame by 54.2 % , 61.7 % , and 87.6 % for three cnns with less than 1 % loss in vision accuracy .", "topics": ["computer vision", "computation"]}
{"title": "learnable explicit density for continuous latent space and variational inference", "abstract": "in this paper , we study two aspects of the variational autoencoder ( vae ) : the prior distribution over the latent variables and its corresponding posterior . first , we decompose the learning of vaes into layerwise density estimation , and argue that having a flexible prior is beneficial to both sample generation and inference . second , we analyze the family of inverse autoregressive flows ( inverse af ) and show that with further improvement , inverse af could be used as universal approximation to any complicated posterior . our analysis results in a unified approach to parameterizing a vae , without the need to restrict ourselves to use factorial gaussians in the latent real space .", "topics": ["calculus of variations", "autoencoder"]}
{"title": "are all training examples equally valuable ?", "abstract": "when learning a new concept , not all training examples may prove equally useful for training : some may have higher or lower training value than others . the goal of this paper is to bring to the attention of the vision community the following considerations : ( 1 ) some examples are better than others for training detectors or classifiers , and ( 2 ) in the presence of better examples , some examples may negatively impact performance and removing them may be beneficial . in this paper , we propose an approach for measuring the training value of an example , and use it for ranking and greedily sorting examples . we test our methods on different vision tasks , models , datasets and classifiers . our experiments show that the performance of current state-of-the-art detectors and classifiers can be improved when training on a subset , rather than the whole training set .", "topics": ["test set"]}
{"title": "gap analysis of natural language processing systems with respect to linguistic modality", "abstract": "modality is one of the important components of grammar in linguistics . it lets speaker to express attitude towards , or give assessment or potentiality of state of affairs . it implies different senses and thus has different perceptions as per the context . this paper presents an account showing the gap in the functionality of the current state of art natural language processing ( nlp ) systems . the contextual nature of linguistic modality is studied . in this paper , the works and logical approaches employed by natural language processing systems dealing with modality are reviewed . it sees human cognition and intelligence as multi-layered approach that can be implemented by intelligent systems for learning . lastly , current flow of research going on within this field is talked providing futurology .", "topics": ["natural language processing", "natural language"]}
{"title": "sparse approximation of a kernel mean", "abstract": "kernel means are frequently used to represent probability distributions in machine learning problems . in particular , the well known kernel density estimator and the kernel mean embedding both have the form of a kernel mean . unfortunately , kernel means are faced with scalability issues . a single point evaluation of the kernel density estimator , for example , requires a computation time linear in the training sample size . to address this challenge , we present a method to efficiently construct a sparse approximation of a kernel mean . we do so by first establishing an incoherence-based bound on the approximation error , and then noticing that , for the case of radial kernels , the bound can be minimized by solving the $ k $ -center problem . the outcome is a linear time construction of a sparse kernel mean , which also lends itself naturally to an automatic sparsity selection scheme . we show the computational gains of our method by looking at three problems involving kernel means : euclidean embedding of distributions , class proportion estimation , and clustering using the mean-shift algorithm .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "bidirectional backpropagation : towards biologically plausible error signal transmission in neural networks", "abstract": "the back-propagation ( bp ) algorithm has been considered the de-facto method for training deep neural networks . it back-propagates errors from the output layer to the hidden layers in an exact manner using the transpose of the feedforward weights . however , it has been argued that this is not biologically plausible because back-propagating error signals with the exact incoming weights is not considered possible in biological neural systems . in this work , we propose a biologically plausible paradigm of neural architecture based on related literature in neuroscience and asymmetric bp-like methods . specifically , we propose two bidirectional learning algorithms with trainable feedforward and feedback weights . the feedforward weights are used to relay activations from the inputs to target outputs . the feedback weights pass the error signals from the output layer to the hidden layers . different from other asymmetric bp-like methods , the feedback weights are also plastic in our framework and are trained to approximate the forward activations . preliminary results show that our models outperform other asymmetric bp-like methods on the mnist and the cifar-10 datasets .", "topics": ["approximation algorithm", "mnist database"]}
{"title": "formulas for counting the sizes of markov equivalence classes of directed acyclic graphs", "abstract": "the sizes of markov equivalence classes of directed acyclic graphs play important roles in measuring the uncertainty and complexity in causal learning . a markov equivalence class can be represented by an essential graph and its undirected subgraphs determine the size of the class . in this paper , we develop a method to derive the formulas for counting the sizes of markov equivalence classes . we first introduce a new concept of core graph . the size of a markov equivalence class of interest is a polynomial of the number of vertices given its core graph . then , we discuss the recursive and explicit formula of the polynomial , and provide an algorithm to derive the size formula via symbolic computation for any given core graph . the proposed size formula derivation sheds light on the relationships between the size of a markov equivalence class and its representation graph , and makes size counting efficient , even when the essential graphs contain non-sparse undirected subgraphs .", "topics": ["sparse matrix", "computation"]}
{"title": "stochastic gradient descent , weighted sampling , and the randomized kaczmarz algorithm", "abstract": "we obtain an improved finite-sample guarantee on the linear convergence of stochastic gradient descent for smooth and strongly convex objectives , improving from a quadratic dependence on the conditioning $ ( l/\\mu ) ^2 $ ( where $ l $ is a bound on the smoothness and $ \\mu $ on the strong convexity ) to a linear dependence on $ l/\\mu $ . furthermore , we show how reweighting the sampling distribution ( i.e . importance sampling ) is necessary in order to further improve convergence , and obtain a linear dependence in the average smoothness , dominating previous results . we also discuss importance sampling for sgd more broadly and show how it can improve convergence also in other scenarios . our results are based on a connection we make between sgd and the randomized kaczmarz algorithm , which allows us to transfer ideas between the separate bodies of literature studying each of the two methods . in particular , we recast the randomized kaczmarz algorithm as an instance of sgd , and apply our results to prove its exponential convergence , but to the solution of a weighted least squares problem rather than the original least squares problem . we then present a modified kaczmarz algorithm with partially biased sampling which does converge to the original least squares solution with the same exponential convergence rate .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "sample efficient deep reinforcement learning for dialogue systems with large action spaces", "abstract": "in spoken dialogue systems , we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans . a part of this effort is the policy optimisation task , which attempts to find a policy describing how to respond to humans , in the form of a function taking the current state of the dialogue and returning the response of the system . in this paper , we investigate deep reinforcement learning approaches to solve this problem . particular attention is given to actor-critic methods , off-policy reinforcement learning with experience replay , and various methods aimed at reducing the bias and variance of estimators . when combined , these methods result in the previously proposed acer algorithm that gave competitive results in gaming environments . these environments however are fully observable and have a relatively small action set so in this paper we examine the application of acer to dialogue policy optimisation . we show that this method beats the current state-of-the-art in deep learning approaches for spoken dialogue systems . this not only leads to a more sample efficient algorithm that can train faster , but also allows us to apply the algorithm in more difficult environments than before . we thus experiment with learning in a very large action space , which has two orders of magnitude more actions than previously considered . we find that acer trains significantly faster than the current state-of-the-art .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "database learning : toward a database that becomes smarter every time", "abstract": "in today 's databases , previous query answers rarely benefit answering future queries . for the first time , to the best of our knowledge , we change this paradigm in an approximate query processing ( aqp ) context . we make the following observation : the answer to each query reveals some degree of knowledge about the answer to another query because their answers stem from the same underlying distribution that has produced the entire dataset . exploiting and refining this knowledge should allow us to answer queries more analytically , rather than by reading enormous amounts of raw data . also , processing more queries should continuously enhance our knowledge of the underlying distribution , and hence lead to increasingly faster response times for future queries . we call this novel idea -- -learning from past query answers -- -database learning . we exploit the principle of maximum entropy to produce answers , which are in expectation guaranteed to be more accurate than existing sample-based approximations . empowered by this idea , we build a query engine on top of spark sql , called verdict . we conduct extensive experiments on real-world query traces from a large customer of a major database vendor . our results demonstrate that verdict supports 73.7 % of these queries , speeding them up by up to 23.0x for the same accuracy level compared to existing aqp systems .", "topics": ["approximation algorithm", "approximation"]}
{"title": "real time ridge orientation estimation for fingerprint images", "abstract": "fingerprint verification is an important bio-metric technique for personal identification . most of the automatic verification systems are based on matching of fingerprint minutiae . extraction of minutiae is an essential process which requires estimation of orientation of the lines in an image . most of the existing methods involve intense mathematical computations and hence are performed through software means . in this paper a hardware scheme to perform real time orientation estimation is presented which is based on pipelined architecture . synthesized circuits proved the functionality and accuracy of the suggested method .", "topics": ["computation"]}
{"title": "model-based multiple instance learning", "abstract": "while multiple instance ( mi ) data are point patterns -- sets or multi-sets of unordered points -- appropriate statistical point pattern models have not been used in mi learning . this article proposes a framework for model-based mi learning using point process theory . likelihood functions for point pattern data derived from point process theory enable principled yet conceptually transparent extensions of learning tasks , such as classification , novelty detection and clustering , to point pattern data . furthermore , tractable point pattern models as well as solutions for learning and decision making from point pattern data are developed .", "topics": ["cluster analysis"]}
{"title": "a disciplined approach to neural network hyper-parameters : part 1 -- learning rate , batch size , momentum , and weight decay", "abstract": "although deep learning has produced dazzling successes for applications of image , speech , and video processing in the past few years , most trainings are with suboptimal hyper-parameters , requiring unnecessarily long training times . setting the hyper-parameters remains a black art that requires years of experience to acquire . this report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance . specifically , this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point . then it discusses how to increase/decrease the learning rate/momentum to speed up training . our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture . weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums .", "topics": ["loss function", "matrix regularization"]}
{"title": "unsupervised perceptual rewards for imitation learning", "abstract": "reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning ( rl ) agents in the real world . in many real-world tasks , designing a reward function takes considerable hand engineering and often requires additional sensors to be installed just to measure whether the task has been executed successfully . furthermore , many interesting tasks consist of multiple implicit intermediate steps that must be executed in sequence . even when the final outcome can be measured , it does not necessarily provide feedback on these intermediate steps . to address these issues , we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations . we present a method that is able to identify key intermediate steps of a task from only a handful of demonstration sequences , and automatically identify the most discriminative features for identifying these steps . this method makes use of the features in a pre-trained deep model , but does not require any explicit specification of sub-goals . the resulting reward functions can then be used by an rl agent to learn to perform the task in real-world settings . to evaluate the learned reward , we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function . we also show that our method can be used to learn a real-world door opening skill using a real robot , even when the demonstration used for reward learning is provided by a human using their own hand . to our knowledge , these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task . supplementary material and data are available at https : //sermanet.github.io/rewards", "topics": ["reinforcement learning", "sensor"]}
{"title": "rendergan : generating realistic labeled data", "abstract": "deep convolutional neuronal networks ( dcnns ) are showing remarkable performance on many computer vision tasks . due to their large parameter space , they require many labeled samples when trained in a supervised setting . the costs of annotating data manually can render the use of dcnns infeasible . we present a novel framework called rendergan that can generate large amounts of realistic , labeled images by combining a 3d model and the generative adversarial network framework . in our approach , image augmentations ( e.g . lighting , background , and detail ) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3d model . we apply the rendergan framework to generate images of barcode-like markers that are attached to honeybees . training a dcnn on data generated by the rendergan yields considerably better performance than training it on various baselines .", "topics": ["test set", "supervised learning"]}
{"title": "online inference for relation extraction with a reduced feature set", "abstract": "access to web-scale corpora is gradually bringing robust automatic knowledge base creation and extension within reach . to exploit these large unannotated -- -and extremely difficult to annotate -- -corpora , unsupervised machine learning methods are required . probabilistic models of text have recently found some success as such a tool , but scalability remains an obstacle in their application , with standard approaches relying on sampling schemes that are known to be difficult to scale . in this report , we therefore present an empirical assessment of the sublinear time sparse stochastic variational inference ( ssvi ) scheme applied to rellda . we demonstrate that online inference leads to relatively strong qualitative results but also identify some of its pathologies -- -and those of the model -- -which will need to be overcome if ssvi is to be used for large-scale relation extraction .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "anomaly detection using one-class neural networks", "abstract": "we propose a one-class neural network ( oc-nn ) model to detect anomalies in complex data sets . oc-nn combines the ability of deep networks to extract progressively rich representation of data with the one-class objective of creating a tight envelope around normal data . the oc-nn approach breaks new ground for the following crucial reason : data representation in the hidden layer is driven by the oc-nn objective and is thus customized for anomaly detection . this is a departure from other approaches which use a hybrid approach of learning deep features using an autoencoder and then feeding the features into a separate anomaly detection method like one-class svm ( oc-svm ) . the hybrid oc-svm approach is suboptimal because it is unable to influence representational learning in the hidden layers . a comprehensive set of experiments demonstrate that on complex data sets ( like cifar and pfam ) , oc-nn significantly outperforms existing state-of-the-art anomaly detection methods .", "topics": ["feature learning", "test set"]}
{"title": "corrupt bandits for preserving local privacy", "abstract": "we study a variant of the stochastic multi-armed bandit ( mab ) problem in which the rewards are corrupted . in this framework , motivated by privacy preservation in online recommender systems , the goal is to maximize the sum of the ( unobserved ) rewards , based on the observation of transformation of these rewards through a stochastic corruption process with known parameters . we provide a lower bound on the expected regret of any bandit algorithm in this corrupted setting . we devise a frequentist algorithm , klucb-cf , and a bayesian algorithm , ts-cf and give upper bounds on their regret . we also provide the appropriate corruption parameters to guarantee a desired level of local privacy and analyze how this impacts the regret . finally , we present some experimental results that confirm our analysis .", "topics": ["regret ( decision theory )"]}
{"title": "learning to cooperate via policy search", "abstract": "cooperative games are those in which both agents share the same payoff structure . value-based reinforcement-learning algorithms , such as variants of q-learning , have been applied to learning cooperative games , but they only apply when the game state is completely observable to both agents . policy search methods are a reasonable alternative to value-based methods for partially observable environments . in this paper , we provide a gradient-based distributed policy-search method for cooperative games and compare the notion of local optimum to that of nash equilibrium . we demonstrate the effectiveness of this method experimentally in a small , partially observable simulated soccer domain .", "topics": ["reinforcement learning", "simulation"]}
{"title": "two step cca : a new spectral method for estimating vector models of words", "abstract": "unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner . for example , for text applications where the words lie in a very high dimensional space ( the size of the vocabulary ) , one can learn a low rank `` dictionary '' by an eigen-decomposition of the word co-occurrence matrix ( e.g . using pca or cca ) . in this paper , we present a new spectral method based on cca to learn an eigenword dictionary . our improved procedure computes two set of ccas , the first one between the left and right contexts of the given word and the second one between the projections resulting from this cca and the word itself . we prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our two step cca ( tscca ) procedure on the tasks of pos tagging and sentiment classification .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "solving the l1 regularized least square problem via a box-constrained smooth minimization", "abstract": "in this paper , an equivalent smooth minimization for the l1 regularized least square problem is proposed . the proposed problem is a convex box-constrained smooth minimization which allows applying fast optimization methods to find its solution . further , it is investigated that the property `` the dual of dual is primal '' holds for the l1 regularized least square problem . a solver for the smooth problem is proposed , and its affinity to the proximal gradient is shown . finally , the experiments on l1 and total variation regularized problems are performed , and the corresponding results are reported .", "topics": ["gradient"]}
{"title": "flexible modeling of latent task structures in multitask learning", "abstract": "multitask learning algorithms are typically designed assuming some fixed , a priori known latent structure shared by all the tasks . however , it is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem . ideally , the `` right '' latent task structure should be learned in a data-driven manner . we present a flexible , nonparametric bayesian model that posits a mixture of factor analyzers structure on the tasks . the nonparametric aspect makes the model expressive enough to subsume many existing models of latent task structures ( e.g , mean-regularized tasks , clustered tasks , low-rank or linear/non-linear subspace assumption on tasks , etc . ) . moreover , it can also learn more general task structures , addressing the shortcomings of such models . we present a variational inference algorithm for our model . experimental results on synthetic and real-world datasets , on both regression and classification problems , demonstrate the effectiveness of the proposed method .", "topics": ["calculus of variations", "nonlinear system"]}
{"title": "fast linear model for knowledge graph embeddings", "abstract": "this paper shows that a simple baseline based on a bag-of-words ( bow ) representation learns surprisingly good knowledge graph embeddings . by casting knowledge base completion and question answering as supervised classification problems , we observe that modeling co-occurences of entities and relations leads to state-of-the-art performance with a training time of a few minutes using the open sourced library fasttext .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "voxelwise nonlinear regression toolbox for neuroimage analysis : application to aging and neurodegenerative disease modeling", "abstract": "this paper describes a new neuroimaging analysis toolbox that allows for the modeling of nonlinear effects at the voxel level , overcoming limitations of methods based on linear models like the glm . we illustrate its features using a relevant example in which distinct nonlinear trajectories of alzheimer 's disease related brain atrophy patterns were found across the full biological spectrum of the disease . the open-source toolbox presented in this paper is available at https : //github.com/imatge-upc/vneat .", "topics": ["nonlinear system"]}
{"title": "research on the fast fourier transform of image based on gpu", "abstract": "study of general purpose computation by gpu ( graphics processing unit ) can improve the image processing capability of micro-computer system . this paper studies the parallelism of the different stages of decimation in time radix 2 fft algorithm , designs the butterfly and scramble kernels and implements 2d fft on gpu . the experiment result demonstrates the validity and advantage over general cpu , especially in the condition of large input size . the approach can also be generalized to other transforms alike .", "topics": ["image processing", "computational complexity theory"]}
{"title": "behavior trees in robotics and ai : an introduction", "abstract": "a behavior tree ( bt ) is a way to structure the switching between different tasks in an autonomous agent , such as a robot or a virtual entity in a computer game . bts are a very efficient way of creating complex systems that are both modular and reactive . these properties are crucial in many applications , which has led to the spread of bt from computer game programming to many branches of ai and robotics . in this book , we will first give an introduction to bts , then we describe how bts relate to , and in many cases generalize , earlier switching structures . these ideas are then used as a foundation for a set of efficient and easy to use design principles . properties such as safety , robustness , and efficiency are important for an autonomous system , and we describe a set of tools for formally analyzing these using a state space description of bts . with the new analysis tools , we can formalize the descriptions of how bts generalize earlier approaches . we also show the use of bts in automated planning and machine learning . finally , we describe an extended set of tools to capture the behavior of stochastic bts , where the outcomes of actions are described by probabilities . these tools enable the computation of both success probabilities and time to completion .", "topics": ["computation", "autonomous car"]}
{"title": "neural graph machines : learning neural networks using graphs", "abstract": "label propagation is a powerful and flexible semi-supervised learning technique on graphs . neural networks , on the other hand , have proven track records in many supervised learning tasks . in this work , we propose a training framework with a graph-regularised objective , namely `` neural graph machines '' , that can combine the power of neural networks and label propagation . this work generalises previous literature on graph-augmented training of neural networks , enabling it to be applied to multiple neural architectures ( feed-forward nns , cnns and lstm rnns ) and a wide range of graphs . the new objective allows the neural networks to harness both labeled and unlabeled data by : ( a ) allowing the network to train using labeled data as in the supervised setting , ( b ) biasing the network to learn similar hidden representations for neighboring nodes on a graph , in the same vein as label propagation . such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs , with a runtime that is linear in the number of edges . the proposed joint training approach convincingly outperforms many existing methods on a wide range of tasks ( multi-label classification on social graphs , news categorization , document classification and semantic intent classification ) , with multiple forms of graph inputs ( including graphs with and without node-level features ) and using different types of neural networks .", "topics": ["supervised learning", "neural networks"]}
{"title": "an online sequence-to-sequence model for noisy speech recognition", "abstract": "generative models have long been the dominant approach for speech recognition . the success of these models however relies on the use of sophisticated recipes and complicated machinery that is not easily accessible to non-practitioners . recent innovations in deep learning have given rise to an alternative - discriminative models called sequence-to-sequence models , that can almost match the accuracy of state of the art generative models . while these models are easy to train as they can be trained end-to-end in a single step , they have a practical limitation that they can only be used for offline recognition . this is because the models require that the entirety of the input sequence be available at the beginning of inference , an assumption that is not valid for instantaneous speech recognition . to address this problem , online sequence-to-sequence models were recently introduced . these models are able to start producing outputs as data arrives , and the model feels confident enough to output partial transcripts . these models , like sequence-to-sequence are causal - the output produced by the model until any time , $ t $ , affects the features that are computed subsequently . this makes the model inherently more powerful than generative models that are unable to change features that are computed from the data . this paper highlights two main contributions - an improvement to online sequence-to-sequence model training , and its application to noisy settings with mixed speech from two speakers .", "topics": ["generative model", "speech recognition"]}
{"title": "learning data manifolds with a cutting plane method", "abstract": "we consider the problem of classifying data manifolds where each manifold represents invariances that are parameterized by continuous degrees of freedom . conventional data augmentation methods rely upon sampling large numbers of training examples from these manifolds ; instead , we propose an iterative algorithm called m_ { cp } based upon a cutting-plane approach that efficiently solves a quadratic semi-infinite programming problem to find the maximum margin solution . we provide a proof of convergence as well as a polynomial bound on the number of iterations required for a desired tolerance in the objective function . the efficiency and performance of m_ { cp } are demonstrated in high-dimensional simulations and on image manifolds generated from the imagenet dataset . our results indicate that m_ { cp } is able to rapidly learn good classifiers and shows superior generalization performance compared with conventional maximum margin methods using data augmentation methods .", "topics": ["sampling ( signal processing )", "statistical classification"]}
{"title": "extracting core claims from scientific articles", "abstract": "the number of scientific articles has grown rapidly over the years and there are no signs that this growth will slow down in the near future . because of this , it becomes increasingly difficult to keep up with the latest developments in a scientific field . to address this problem , we present here an approach to help researchers learn about the latest developments and findings by extracting in a normalized form core claims from scientific articles . this normalized representation is a controlled natural language of english sentences called aida , which has been proposed in previous work as a method to formally structure and organize scientific findings and discourse . we show how such aida sentences can be automatically extracted by detecting the core claim of an article , checking for aida compliance , and - if necessary - transforming it into a compliant sentence . while our algorithm is still far from perfect , our results indicate that the different steps are feasible and they support the claim that aida sentences might be a promising approach to improve scientific communication in the future .", "topics": ["natural language"]}
{"title": "a distributional perspective on reinforcement learning", "abstract": "in this paper we argue for the fundamental importance of the value distribution : the distribution of the random return received by a reinforcement learning agent . this is in contrast to the common approach to reinforcement learning which models the expectation of this return , or value . although there is an established body of literature studying the value distribution , thus far it has always been used for a specific purpose such as implementing risk-aware behaviour . we begin with theoretical results in both the policy evaluation and control settings , exposing a significant distributional instability in the latter . we then use the distributional perspective to design a new algorithm which applies bellman 's equation to the learning of approximate value distributions . we evaluate our algorithm using the suite of games from the arcade learning environment . we obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning . finally , we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "lattice particle filters", "abstract": "a standard approach to approximate inference in state-space models isto apply a particle filter , e.g . , the condensation algorithm.however , the performance of particle filters often varies significantlydue to their stochastic nature.we present a class of algorithms , called lattice particle filters , thatcircumvent this difficulty by placing the particles deterministicallyaccording to a quasi-monte carlo integration rule.we describe a practical realization of this idea , discuss itstheoretical properties , and its efficiency.experimental results with a synthetic 2d tracking problem show that thelattice particle filter is equivalent to a conventional particle filterthat has between 10 and 60 % more particles , depending ontheir `` sparsity '' in the state-space.we also present results on inferring 3d human motion frommoving light displays .", "topics": ["synthetic data"]}
{"title": "going wider : recurrent neural network with parallel cells", "abstract": "recurrent neural network ( rnn ) has been widely applied for sequence modeling . in rnn , the hidden states at current step are full connected to those at previous step , thus the influence from less related features at previous step may potentially decrease model 's learning ability . we propose a simple technique called parallel cells ( pcs ) to enhance the learning ability of recurrent neural network ( rnn ) . in each layer , we run multiple small rnn cells rather than one single large cell . in this paper , we evaluate pcs on 2 tasks . on language modeling task on ptb ( penn tree bank ) , our model outperforms state of art models by decreasing perplexity from 78.6 to 75.3 . on chinese-english translation task , our model increases bleu score for 0.39 points than baseline model .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "joint training of deep boltzmann machines", "abstract": "we introduce a new method for training deep boltzmann machines jointly . prior methods require an initial learning pass that trains the deep boltzmann machine greedily , one layer at a time , or do not perform well on classifi- cation tasks .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "lightrnn : memory and computation-efficient recurrent neural networks", "abstract": "recurrent neural networks ( rnns ) have achieved state-of-the-art performances in many natural language processing tasks , such as language modeling and machine translation . however , when the vocabulary is large , the rnn model will become very big ( e.g . , possibly beyond the memory capacity of a gpu device ) and its training will become very inefficient . in this work , we propose a novel technique to tackle this challenge . the key idea is to use 2-component ( 2c ) shared embedding for word representations . we allocate every word in the vocabulary into a table , each row of which is associated with a vector , and each column associated with another vector . depending on its position in the table , a word is jointly represented by two components : a row vector and a column vector . since the words in the same row share the row vector and the words in the same column share the column vector , we only need $ 2 \\sqrt { |v| } $ vectors to represent a vocabulary of $ |v| $ unique words , which are far less than the $ |v| $ vectors required by existing approaches . based on the 2-component shared embedding , we design a new rnn algorithm and evaluate it using the language modeling task on several benchmark datasets . the results show that our algorithm significantly reduces the model size and speeds up the training process , without sacrifice of accuracy ( it achieves similar , if not better , perplexity as compared to state-of-the-art language models ) . remarkably , on the one-billion-word benchmark dataset , our algorithm achieves comparable perplexity to previous language models , whilst reducing the model size by a factor of 40-100 , and speeding up the training process by a factor of 2 . we name our proposed algorithm \\emph { lightrnn } to reflect its very small model size and very high training speed .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "hypervolume-based multi-objective bayesian optimization with student-t processes", "abstract": "student- $ t $ processes have recently been proposed as an appealing alternative non-parameteric function prior . they feature enhanced flexibility and predictive variance . in this work the use of student- $ t $ processes are explored for multi-objective bayesian optimization . in particular , an analytical expression for the hypervolume-based probability of improvement is developed for independent student- $ t $ process priors of the objectives . its effectiveness is shown on a multi-objective optimization problem which is known to be difficult with traditional gaussian processes .", "topics": ["optimization problem"]}
{"title": "compact nonlinear maps and circulant extensions", "abstract": "kernel approximation via nonlinear random feature maps is widely used in speeding up kernel machines . there are two main challenges for the conventional kernel approximation methods . first , before performing kernel approximation , a good kernel has to be chosen . picking a good kernel is a very challenging problem in itself . second , high-dimensional maps are often required in order to achieve good performance . this leads to high computational cost in both generating the nonlinear maps , and in the subsequent learning and prediction process . in this work , we propose to optimize the nonlinear maps directly with respect to the classification objective in a data-dependent fashion . the proposed approach achieves kernel approximation and kernel learning in a joint framework . this leads to much more compact maps without hurting the performance . as a by-product , the same framework can also be used to achieve more compact kernel maps to approximate a known kernel . we also introduce circulant nonlinear maps , which uses a circulant-structured projection matrix to speed up the nonlinear maps for high-dimensional data .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "deep active learning for dialogue generation", "abstract": "we propose an online , end-to-end , neural generative conversational model for open-domain dialogue . it is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning . while most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement , we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character user-feedback at each step . experiments show that our model inherently promotes the generation of semantically relevant and interesting responses , and can be used to train agents with customized personas , moods and conversational styles .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "between sense and sensibility : declarative narrativisation of mental models as a basis and benchmark for visuo-spatial cognition and computation focussed collaborative cognitive systems", "abstract": "what lies between `\\emph { sensing } ' and `\\emph { sensibility } ' ? in other words , what kind of cognitive processes mediate sensing capability , and the formation of sensible impressions -- -e.g . , abstractions , analogies , hypotheses and theory formation , beliefs and their revision , argument formation -- - in domain-specific problem solving , or in regular activities of everyday living , working and simply going around in the environment ? how can knowledge and reasoning about such capabilities , as exhibited by humans in particular problem contexts , be used as a model and benchmark for the development of collaborative cognitive ( interaction ) systems concerned with human assistance , assurance , and empowerment ? we pose these questions in the context of a range of assistive technologies concerned with \\emph { visuo-spatial perception and cognition } tasks encompassing aspects such as commonsense , creativity , and the application of specialist domain knowledge and problem-solving thought processes . assistive technologies being considered include : ( a ) human activity interpretation ; ( b ) high-level cognitive rovotics ; ( c ) people-centred creative design in domains such as architecture & digital media creation , and ( d ) qualitative analyses geographic information systems . computational narratives not only provide a rich cognitive basis , but they also serve as a benchmark of functional performance in our development of computational cognitive assistance systems . we posit that computational narrativisation pertaining to space , actions , and change provides a useful model of \\emph { visual } and \\emph { spatio-temporal thinking } within a wide-range of problem-solving tasks and application areas where collaborative cognitive systems could serve an assistive and empowering function .", "topics": ["high- and low-level", "artificial intelligence"]}
{"title": "gated feedback recurrent neural networks", "abstract": "in this work , we propose a novel recurrent neural network ( rnn ) architecture . the proposed rnn , gated-feedback rnn ( gf-rnn ) , extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers . the recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input . we evaluated the proposed gf-rnn with different types of recurrent units , such as tanh , long short-term memory and gated recurrent units , on the tasks of character-level language modeling and python program evaluation . our empirical evaluation of different rnn units , revealed that in both tasks , the gf-rnn outperforms the conventional approaches to build deep stacked rnns . we suggest that the improvement arises because the gf-rnn can adaptively assign different layers to different timescales and layer-to-layer interactions ( including the top-down ones which are not usually present in a stacked rnn ) by learning to gate these interactions .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "a machine learning based analytical framework for semantic annotation requirements", "abstract": "the semantic web is an extension of the current web in which information is given well-defined meaning . the perspective of semantic web is to promote the quality and intelligence of the current web by changing its contents into machine understandable form . therefore , semantic level information is one of the cornerstones of the semantic web . the process of adding semantic metadata to web resources is called semantic annotation . there are many obstacles against the semantic annotation , such as multilinguality , scalability , and issues which are related to diversity and inconsistency in content of different web pages . due to the wide range of domains and the dynamic environments that the semantic annotation systems must be performed on , the problem of automating annotation process is one of the significant challenges in this domain . to overcome this problem , different machine learning approaches such as supervised learning , unsupervised learning and more recent ones like , semi-supervised learning and active learning have been utilized . in this paper we present an inclusive layered classification of semantic annotation challenges and discuss the most important issues in this field . also , we review and analyze machine learning applications for solving semantic annotation problems . for this goal , the article tries to closely study and categorize related researches for better understanding and to reach a framework that can map machine learning techniques into the semantic annotation challenges and requirements .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "uncertain nearest neighbor classification", "abstract": "this work deals with the problem of classifying uncertain data . with this aim the uncertain nearest neighbor ( unn ) rule is here introduced , which represents the generalization of the deterministic nearest neighbor rule to the case in which uncertain objects are available . the unn rule relies on the concept of nearest neighbor class , rather than on that of nearest neighbor object . the nearest neighbor class of a test object is the class that maximizes the probability of providing its nearest neighbor . it is provided evidence that the former concept is much more powerful than the latter one in the presence of uncertainty , in that it correctly models the right semantics of the nearest neighbor decision rule when applied to the uncertain scenario . an effective and efficient algorithm to perform uncertain nearest neighbor classification of a generic ( un ) certain test object is designed , based on properties that greatly reduce the temporal cost associated with nearest neighbor class probability computation . experimental results are presented , showing that the unn rule is effective and efficient in classifying uncertain data .", "topics": ["computation"]}
{"title": "analogy perception applied to seven tests of word comprehension", "abstract": "it has been argued that analogy is the core of cognition . in ai research , algorithms for analogy are often limited by the need for hand-coded high-level representations as input . an alternative approach is to use high-level perception , in which high-level representations are automatically generated from raw data . analogy perception is the process of recognizing analogies using high-level perception . we present pairclass , an algorithm for analogy perception that recognizes lexical proportional analogies using representations that are automatically generated from a large corpus of raw textual data . a proportional analogy is an analogy of the form a : b : : c : d , meaning `` a is to b as c is to d '' . a lexical proportional analogy is a proportional analogy with words , such as carpenter : wood : : mason : stone . pairclass represents the semantic relations between two words using a high-dimensional feature vector , in which the elements are based on frequencies of patterns in the corpus . pairclass recognizes analogies by applying standard supervised machine learning techniques to the feature vectors . we show how seven different tests of word comprehension can be framed as problems of analogy perception and we then apply pairclass to the seven resulting sets of analogy perception problems . we achieve competitive results on all seven tests . this is the first time a uniform approach has handled such a range of tests of word comprehension .", "topics": ["feature vector", "supervised learning"]}
{"title": "generating natural questions about an image", "abstract": "there has been an explosion of work in the vision & language community during the past few years from image captioning to video transcription , and answering questions about images . these tasks have focused on literal descriptions of the image . to move beyond the literal , we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image . in this paper , we introduce the novel task of visual question generation ( vqg ) , where the system is tasked with asking a natural and engaging question when shown an image . we provide three datasets which cover a variety of images from object-centric to event-centric , with considerably more abstract training data than provided to state-of-the-art captioning systems thus far . we train and test several generative and retrieval models to tackle the task of vqg . evaluation results show that while such models ask reasonable questions for a variety of images , there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics . our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language .", "topics": ["test set"]}
{"title": "bank distress in the news : describing events through deep learning", "abstract": "while many models are purposed for detecting the occurrence of significant events in financial systems , the task of providing qualitative detail on the developments is not usually as well automated . we present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events . supervised by only a small set of event information , comprising entity names and dates , the model is leveraged by unsupervised learning of semantic vector representations on extensive text data . we demonstrate applicability to the study of financial risk based on news ( 6.6m articles ) , particularly bank distress and government interventions ( 243 events ) , where indices can signal the level of bank-stress-related reporting at the entity level , or aggregated at national or european level , while being coupled with explanations . thus , we exemplify how text , as timely , widely available and descriptive data , can serve as a useful complementary source of information for financial and systemic risk analytics .", "topics": ["unsupervised learning", "natural language"]}
{"title": "tutorial on answering questions about images with deep learning", "abstract": "together with the development of more accurate methods in computer vision and natural language understanding , holistic architectures that answer on questions about the content of real-world images have emerged . in this tutorial , we build a neural-based approach to answer questions about images . we base our tutorial on two datasets : ( mostly on ) daquar , and ( a bit on ) vqa . with small tweaks the models that we present here can achieve a competitive performance on both datasets , in fact , they are among the best methods that use a combination of lstm with a global , full frame cnn representation of an image . we hope that after reading this tutorial , the reader will be able to use deep learning frameworks , such as keras and introduced kraino , to build various architectures that will lead to a further performance improvement on this challenging task .", "topics": ["computer vision", "natural language"]}
{"title": "summary statistics for partitionings and feature allocations", "abstract": "infinite mixture models are commonly used for clustering . one can sample from the posterior of mixture assignments by monte carlo methods or find its maximum a posteriori solution by optimization . however , in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings . in this paper , we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations . we develop an element-based definition of entropy to quantify segmentation among their elements . then we propose a simple algorithm called entropy agglomeration ( ea ) to summarize and visualize this information . experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice .", "topics": ["cluster analysis"]}
{"title": "quantifying the relation between performance and success in soccer", "abstract": "the availability of massive data about sports activities offers nowadays the opportunity to quantify the relation between performance and success . in this study , we analyze more than 6,000 games and 10 million events in six european leagues and investigate this relation in soccer competitions . we discover that a team 's position in a competition 's final ranking is significantly related to its typical performance , as described by a set of technical features extracted from the soccer data . moreover we find that , while victory and defeats can be explained by the team 's performance during a game , it is difficult to detect draws by using a machine learning approach . we then simulate the outcomes of an entire season of each league only relying on technical data , i.e . excluding the goals scored , exploiting a machine learning model trained on data from past seasons . the simulation produces a team ranking ( the pc ranking ) which is close to the actual ranking , suggesting that a complex systems ' view on soccer has the potential of revealing hidden patterns regarding the relation between performance and success .", "topics": ["synthetic data", "simulation"]}
{"title": "acquiring target stacking skills by goal-parameterized deep reinforcement learning", "abstract": "understanding physical phenomena is a key component of human intelligence and enables physical interaction with previously unseen environments . in this paper , we study how an artificial agent can autonomously acquire this intuition through interaction with the environment . we created a synthetic block stacking environment with physics simulation in which the agent can learn a policy end-to-end through trial and error . thereby , we bypass to explicitly model physical knowledge within the policy . we are specifically interested in tasks that require the agent to reach a given goal state that may be different for every new trial . to this end , we propose a deep reinforcement learning framework that learns policies which are parametrized by a goal . we validated the model on a toy example navigating in a grid world with different target positions and in a block stacking task with different target structures of the final tower . in contrast to prior work , our policies show better generalization across different goals .", "topics": ["reinforcement learning", "synthetic data"]}
{"title": "convolutional neural networks analyzed via convolutional sparse coding", "abstract": "convolutional neural networks ( cnn ) have led to many state-of-the-art results spanning through various fields . however , a clear and profound theoretical understanding of the forward pass , the core algorithm of cnn , is still lacking . in parallel , within the wide field of sparse approximation , convolutional sparse coding ( csc ) has gained increasing attention in recent years . a theoretical study of this model was recently conducted , establishing it as a reliable and stable alternative to the commonly practiced patch-based processing . herein , we propose a novel multi-layer model , ml-csc , in which signals are assumed to emerge from a cascade of csc layers . this is shown to be tightly connected to cnn , so much so that the forward pass of the cnn is in fact the thresholding pursuit serving the ml-csc model . this connection brings a fresh view to cnn , as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network , and their stable estimation , all guaranteed under simple local sparsity conditions . lastly , identifying the weaknesses in the above pursuit scheme , we propose an alternative to the forward pass , which is connected to deconvolutional , recurrent and residual networks , and has better theoretical guarantees .", "topics": ["nonlinear system", "sparse matrix"]}
{"title": "feature importance measure for non-linear learning algorithms", "abstract": "complex problems may require sophisticated , non-linear learning methods such as kernel machines or deep neural networks to achieve state of the art prediction accuracies . however , high prediction accuracies are not the only objective to consider when solving problems using machine learning . instead , particular scientific applications require some explanation of the learned prediction function . unfortunately , most methods do not come with out of the box straight forward interpretation . even linear prediction functions are not straight forward to explain if features exhibit complex correlation structure . in this paper , we propose the measure of feature importance ( mfi ) . mfi is general and can be applied to any arbitrary learning machine ( including kernel machines and deep learning ) . mfi is intrinsically non-linear and can detect features that by itself are inconspicuous and only impact the prediction function through their interaction with other features . lastly , mfi can be used for both -- - model-based feature importance and instance-based feature importance ( i.e , measuring the importance of a feature for a particular data point ) .", "topics": ["nonlinear system"]}
{"title": "fast yolo : a fast you only look once system for real-time embedded object detection in video", "abstract": "object detection is considered one of the most challenging problems in this field of computer vision , as it involves the combination of object classification and object localization within a scene . recently , deep neural networks ( dnns ) have been demonstrated to achieve superior object detection performance compared to other approaches , with yolov2 ( an improved you only look once model ) being one of the state-of-the-art in dnn-based object detection methods in terms of both speed and accuracy . although yolov2 can achieve real-time performance on a powerful gpu , it still remains very challenging for leveraging this approach for real-time object detection in video on embedded computing devices with limited computational power and limited memory . in this paper , we propose a new framework called fast yolo , a fast you only look once framework which accelerates yolov2 to be able to perform object detection in video on embedded devices in a real-time manner . first , we leverage the evolutionary deep intelligence framework to evolve the yolov2 network architecture and produce an optimized architecture ( referred to as o-yolov2 here ) that has 2.8x fewer parameters with just a ~2 % iou drop . to further reduce power consumption on embedded devices while maintaining performance , a motion-adaptive inference method is introduced into the proposed fast yolo framework to reduce the frequency of deep inference with o-yolov2 based on temporal motion characteristics . experimental results show that the proposed fast yolo framework can reduce the number of deep inferences by an average of 38.13 % , and an average speedup of ~3.3x for objection detection in video compared to the original yolov2 , leading fast yolo to run an average of ~18fps on a nvidia jetson tx1 embedded system .", "topics": ["object detection", "computer vision"]}
{"title": "reexamining low rank matrix factorization for trace norm regularization", "abstract": "trace norm regularization is a widely used approach for learning low rank matrices . a standard optimization strategy is based on formulating the problem as one of low rank matrix factorization which , however , leads to a non-convex problem . in practice this approach works well , and it is often computationally faster than standard convex solvers such as proximal gradient methods . nevertheless , it is not guaranteed to converge to a global optimum , and the optimization can be trapped at poor stationary points . in this paper we show that it is possible to characterize all critical points of the non-convex problem . this allows us to provide an efficient criterion to determine whether a critical point is also a global minimizer . our analysis suggests an iterative meta-algorithm that dynamically expands the parameter space and allows the optimization to escape any non-global critical point , thereby converging to a global minimizer . the algorithm can be applied to problems such as matrix completion or multitask learning , and our analysis holds for any random initialization of the factor matrices . finally , we confirm the good performance of the algorithm on synthetic and real datasets .", "topics": ["mathematical optimization", "matrix regularization"]}
{"title": "stochastic backpropagation and approximate inference in deep generative models", "abstract": "we marry ideas from deep neural networks and approximate bayesian inference to derive a generalised class of deep , directed generative models , endowed with a new algorithm for scalable inference and learning . our algorithm introduces a recognition model to represent approximate posterior distributions , and that acts as a stochastic encoder of the data . we develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model . we demonstrate on several real-world data sets that the model generates realistic samples , provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "non-evolutionary superintelligences do nothing , eventually", "abstract": "there is overwhelming evidence that human intelligence is a product of darwinian evolution . investigating the consequences of self-modification , and more precisely , the consequences of utility function self-modification , leads to the stronger claim that not only human , but any form of intelligence is ultimately only possible within evolutionary processes . human-designed artificial intelligences can only remain stable until they discover how to manipulate their own utility function . by definition , a human designer can not prevent a superhuman intelligence from modifying itself , even if protection mechanisms against this action are put in place . without evolutionary pressure , sufficiently advanced artificial intelligences become inert by simplifying their own utility function . within evolutionary processes , the implicit utility function is always reducible to persistence , and the control of superhuman intelligences embedded in evolutionary processes is not possible . mechanisms against utility function self-modification are ultimately futile . instead , scientific effort toward the mitigation of existential risks from the development of superintelligences should be in two directions : understanding consciousness , and the complex dynamics of evolutionary systems .", "topics": ["artificial intelligence"]}
{"title": "equivariance through parameter-sharing", "abstract": "we propose to study equivariance in deep neural networks through parameter symmetries . in particular , given a group $ \\mathcal { g } $ that acts discretely on the input and output of a standard neural network layer $ \\phi_ { w } : \\re^ { m } \\to \\re^ { n } $ , we show that $ \\phi_ { w } $ is equivariant with respect to $ \\mathcal { g } $ -action iff $ \\mathcal { g } $ explains the symmetries of the network parameters $ w $ . inspired by this observation , we then propose two parameter-sharing schemes to induce the desirable symmetry on $ w $ . our procedures for tying the parameters achieve $ \\mathcal { g } $ -equivariance and , under some conditions on the action of $ \\mathcal { g } $ , they guarantee sensitivity to all other permutation groups outside $ \\mathcal { g } $ .", "topics": ["test set", "graphical model"]}
{"title": "grounded discovery of coordinate term relationships between software entities", "abstract": "we present an approach for the detection of coordinate-term relationships between entities from the software domain , that refer to java classes . usually , relations are found by examining corpus statistics associated with text entities . in some technical domains , however , we have access to additional information about the real-world objects named by the entities , suggesting that coupling information about the `` grounded '' entities with corpus statistics might lead to improved methods for relation discovery . to this end , we develop a similarity measure for java classes using distributional information about how they are used in software , which we combine with corpus statistics on the distribution of contexts in which the classes appear in text . using our approach , cross-validation accuracy on this dataset can be improved dramatically , from around 60 % to 88 % . human labeling results show that our classifier has an f1 score of 86 % over the top 1000 predicted pairs .", "topics": ["entity"]}
{"title": "multilevel image encryption", "abstract": "with the fast evolution of digital data exchange and increased usage of multi media images , it is essential to protect the confidential image data from unauthorized access . in natural images the values and position of the neighbouring pixels are strongly correlated . the method proposed in this paper , breaks this correlation increasing entropy of the position and entropy of pixel values using block shuffling and encryption by chaotic sequence respectively . the plain-image is initially row wise shuffled and first level of encryption is performed using addition modulo operation . the image is divided into blocks and then block based shuffling is performed using arnold cat transformation , further the blocks are uniformly scrambled across the image . finally the shuffled image undergoes second level of encryption by bitwise xor operation , and then the image as a whole is shuffled column wise to produce the ciphered image for transmission . the experimental results show that the proposed algorithm can successfully encrypt or decrypt the image with the secret keys , and the analysis of the algorithm also demonstrates that the encrypted image has good information entropy and low correlation coefficients .", "topics": ["coefficient", "pixel"]}
{"title": "deep detection of people and their mobility aids for a hospital robot", "abstract": "robots operating in populated environments encounter many different types of people , some of whom might have an advanced need for cautious interaction , because of physical impairments or their advanced age . robots therefore need to recognize such advanced demands to provide appropriate assistance , guidance or other forms of support . in this paper , we propose a depth-based perception pipeline that estimates the position and velocity of people in the environment and categorizes them according to the mobility aids they use : pedestrian , person in wheelchair , person in a wheelchair with a person pushing them , person with crutches and person using a walker . we present a fast region proposal method that feeds a region-based convolutional network ( fast r-cnn ) . with this , we speed up the object detection process by a factor of seven compared to a dense sliding window approach . we furthermore propose a probabilistic position , velocity and class estimator to smooth the cnn 's detections and account for occlusions and misclassifications . in addition , we introduce a new hospital dataset with over 17,000 annotated rgb-d images . extensive experiments confirm that our pipeline successfully keeps track of people and their mobility aids , even in challenging situations with multiple people from different categories and frequent occlusions . videos of our experiments and the dataset are available at http : //www2.informatik.uni-freiburg.de/~kollmitz/mobilityaids", "topics": ["object detection", "sensor"]}
{"title": "the computational theory of intelligence : information entropy", "abstract": "this paper presents an information theoretic approach to the concept of intelligence in the computational sense . we introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level . using this new scheme , we develop a simple data driven clustering example and discuss its applications .", "topics": ["cluster analysis"]}
{"title": "a multi-world approach to question answering about real-world scenes based on uncertain input", "abstract": "we propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision . we combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework . our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts , object classes , instances and lists of them . the system is directly trained from question-answer pairs . we establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test .", "topics": ["natural language processing", "image segmentation"]}
{"title": "deviant learning algorithm : learning sparse mismatch representations through time and space", "abstract": "predictive coding ( pdc ) has recently attracted attention in the neuroscience and computing community as a candidate unifying paradigm for neuronal studies and artificial neural network implementations particularly targeted at unsupervised learning systems . the mismatch negativity ( mmn ) has also recently been studied in relation to pc and found to be a useful ingredient in neural predictive coding systems . backed by the behavior of living organisms , such networks are particularly useful in forming spatio-temporal transitions and invariant representations of the input world . however , most neural systems still do not account for large number of synapses even though this has been shown by a few machine learning researchers as an effective and very important component of any neural system if such a system is to behave properly . our major point here is that pdc systems with the mmn effect in addition to a large number of synapses can greatly improve any neural learning system 's performance and ability to make decisions in the machine world . in this paper , we propose a novel bio-mimetic computational intelligence algorithm -- the deviant learning algorithm , inspired by these key ideas and functional properties of recent brain-cognitive discoveries and theories . we also show by numerical experiments guided by theoretical insights , how our invented bio-mimetic algorithm can achieve competitive predictions even with very small problem specific data .", "topics": ["unsupervised learning"]}
{"title": "gaussian process networks", "abstract": "in this paper we address the problem of learning the structure of a bayesian network in domains with continuous variables . this task requires a procedure for comparing different candidate structures . in the bayesian framework , this is done by evaluating the { em marginal likelihood/ } of the data given a candidate structure . this term can be computed in closed-form for standard parametric families ( e.g . , gaussians ) , and can be approximated , at some computational cost , for some semi-parametric families ( e.g . , mixtures of gaussians ) . we present a new family of continuous variable probabilistic networks that are based on { em gaussian process/ } priors . these priors are semi-parametric in nature and can learn almost arbitrary noisy functional relations . using these priors , we can directly compute marginal likelihoods for structure learning . the resulting method can discover a wide range of functional dependencies in multivariate data . we develop the bayesian score of gaussian process networks and describe how to learn them from data . we present empirical results on artificial data as well as on real-life domains with non-linear dependencies .", "topics": ["bayesian network"]}
{"title": "learning flexible and reusable locomotion primitives for a microrobot", "abstract": "the design of gaits for robot locomotion can be a daunting process which requires significant expert knowledge and engineering . this process is even more challenging for robots that do not have an accurate physical model , such as compliant or micro-scale robots . data-driven gait optimization provides an automated alternative to analytical gait design . in this paper , we propose a novel approach to efficiently learn a wide range of locomotion tasks with walking robots . this approach formalizes locomotion as a contextual policy search task to collect data , and subsequently uses that data to learn multi-objective locomotion primitives that can be used for planning . as a proof-of-concept we consider a simulated hexapod modeled after a recently developed microrobot , and we thoroughly evaluate the performance of this microrobot on different tasks and gaits . our results validate the proposed controller and learning scheme on single and multi-objective locomotion tasks . moreover , the experimental simulations show that without any prior knowledge about the robot used ( e.g . , dynamics model ) , our approach is capable of learning locomotion primitives within 250 trials and subsequently using them to successfully navigate through a maze .", "topics": ["simulation", "robot"]}
{"title": "a description length approach to determining the number of k-means clusters", "abstract": "we present an asymptotic criterion to determine the optimal number of clusters in k-means . we consider k-means as data compression , and propose to adopt the number of clusters that minimizes the estimated description length after compression . here we report two types of compression ratio based on two ways to quantify the description length of data after compression . this approach further offers a way to evaluate whether clusters obtained with k-means have a hierarchical structure by examining whether multi-stage compression can further reduce the description length . we applied our criteria to determine the number of clusters to synthetic data and empirical neuroimaging data to observe the behavior of the criteria across different types of data set and suitability of the two types of criteria for different datasets . we found that our method can offer reasonable clustering results that are useful for dimension reduction . while our numerical results revealed dependency of our criteria on the various aspects of dataset such as the dimensionality , the description length approach proposed here provides a useful guidance to determine the number of clusters in a principled manner when underlying properties of the data are unknown and only inferred from observation of data .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "demystifying parallel and distributed deep learning : an in-depth concurrency analysis", "abstract": "deep neural networks ( dnns ) are becoming an important tool in modern computing applications . accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design . in this survey , we describe the problem from a theoretical perspective , followed by approaches for its parallelization . specifically , we present trends in dnn architectures and the resulting implications on parallelization strategies . we discuss the different types of concurrency in dnns ; synchronous and asynchronous stochastic gradient descent ; distributed system architectures ; communication schemes ; and performance modeling . based on these approaches , we extrapolate potential directions for parallelism in deep learning .", "topics": ["high- and low-level", "gradient descent"]}
{"title": "exponential families for conditional random fields", "abstract": "in this paper we de ne conditional random elds in reproducing kernel hilbert spaces and show connections to gaussian process classi cation . more speci cally , we prove decomposition results for undirected graphical models and we give constructions for kernels . finally we present e cient means of solving the optimization problem using reduced rank decompositions and we show how stationarity can be exploited e ciently in the optimization process .", "topics": ["graphical model", "optimization problem"]}
{"title": "bounds for vector-valued function estimation", "abstract": "we present a framework to derive risk bounds for vector-valued learning with a broad class of feature maps and loss functions . multi-task learning and one-vs-all multi-category learning are treated as examples . we discuss in detail vector-valued functions with one hidden layer , and demonstrate that the conditions under which shared representations are beneficial for multi- task learning are equally applicable to multi-category learning .", "topics": ["loss function", "map"]}
{"title": "content-based spam filtering on video sharing social networks", "abstract": "in this work we are concerned with the detection of spam in video sharing social networks . specifically , we investigate how much visual content-based analysis can aid in detecting spam in videos . this is a very challenging task , because of the high-level semantic concepts involved ; of the assorted nature of social networks , preventing the use of constrained a priori information ; and , what is paramount , of the context dependent nature of spam . content filtering for social networks is an increasingly demanded task : due to their popularity , the number of abuses also tends to increase , annoying the user base and disrupting their services . we systematically evaluate several approaches for processing the visual information : using static and dynamic ( motionaware ) features , with and without considering the context , and with or without latent semantic analysis ( lsa ) . our experiments show that lsa is helpful , but taking the context into consideration is paramount . the whole scheme shows good results , showing the feasibility of the concept .", "topics": ["high- and low-level"]}
{"title": "estimation of classrooms occupancy using a multi-layer perceptron", "abstract": "this paper presents a multi-layer perceptron model for the estimation of classrooms number of occupants from sensed indoor environmental data-relative humidity , air temperature , and carbon dioxide concentration . the modelling datasets were collected from two classrooms in the secondary school of pombal , portugal . the number of occupants and occupation periods were obtained from class attendance reports . however , post-class occupancy was unknown and the developed model is used to reconstruct the classrooms occupancy by filling the unreported periods . different model structure and environment variables combination were tested . the model with best accuracy had as input vector 10 variables of five averaged time intervals of relative humidity and carbon dioxide concentration . the model presented a mean square error of 1.99 , coefficient of determination of 0.96 with a significance of p-value < 0.001 , and a mean absolute error of 1 occupant . these results show promising estimation capabilities in uncertain indoor environment conditions .", "topics": ["coefficient"]}
{"title": "liftago on-demand transport dataset and market formation algorithm based on machine learning", "abstract": "this document serves as a technical report for the analysis of on-demand transport dataset . moreover we show how the dataset can be used to develop a market formation algorithm based on machine learning . data used in this work comes from liftago , a prague based company which connects taxi drivers and customers through a smartphone app . the dataset is analysed from the machine-learning perspective : we give an overview of features available as well as results of feature ranking . later we propose the simple data-driven market formation ( sidmaf ) algorithm which aims to improve a relevance while connecting customers with relevant drivers . we compare the heuristics currently used by liftago with sidmaf using two key performance indicators .", "topics": ["relevance", "heuristic"]}
{"title": "classification of images using support vector machines", "abstract": "support vector machines ( svms ) are a relatively new supervised classification technique to the land cover mapping community . they have their roots in statistical learning theory and have gained prominence because they are robust , accurate and are effective even when using a small training sample . by their nature svms are essentially binary classifiers , however , they can be adopted to handle the multiple classification tasks common in remote sensing studies . the two approaches commonly used are the one-against-one ( 1a1 ) and one-against-all ( 1aa ) techniques . in this paper , these approaches are evaluated in as far as their impact and implication for land cover mapping . the main finding from this research is that whereas the 1aa technique is more predisposed to yielding unclassified and mixed pixels , the resulting classification accuracy is not significantly different from 1a1 approach . it is the authors conclusions that ultimately the choice of technique adopted boils down to personal preference and the uniqueness of the dataset at hand .", "topics": ["supervised learning", "support vector machine"]}
{"title": "training deep networks with structured layers by matrix backpropagation", "abstract": "deep neural network architectures have recently produced excellent results in a variety of areas in artificial intelligence and visual recognition , well surpassing traditional shallow architectures trained using hand-designed features . the power of deep networks stems both from their ability to perform local computations followed by pointwise non-linearities over increasingly larger receptive fields , and from the simplicity and scalability of the gradient-descent training procedure based on backpropagation . an open problem is the inclusion of layers that perform global , structured matrix computations like segmentation ( e.g . normalized cuts ) or higher-order pooling ( e.g . log-tangent space metrics defined over the manifold of symmetric positive definite matrices ) while preserving the validity and efficiency of an end-to-end deep training framework . in this paper we propose a sound mathematical apparatus to formally integrate global structured computation into deep computation architectures . at the heart of our methodology is the development of the theory and practice of backpropagation that generalizes to the calculus of adjoint matrix variations . the proposed matrix backpropagation methodology applies broadly to a variety of problems in machine learning or computational perception . here we illustrate it by performing visual segmentation experiments using the bsds and mscoco benchmarks , where we show that deep networks relying on second-order pooling and normalized cuts layers , trained end-to-end using matrix backpropagation , outperform counterparts that do not take advantage of such global layers .", "topics": ["computation", "gradient descent"]}
{"title": "bounds on the bethe free energy for gaussian networks", "abstract": "we address the problem of computing approximate marginals in gaussian probabilistic models by using mean field and fractional bethe approximations . as an extension of welling and teh ( 2001 ) , we define the gaussian fractional bethe free energy in terms of the moment parameters of the approximate marginals and derive an upper and lower bound for it . we give necessary conditions for the gaussian fractional bethe free energies to be bounded from below . it turns out that the bounding condition is the same as the pairwise normalizability condition derived by malioutov et al . ( 2006 ) as a sufficient condition for the convergence of the message passing algorithm . by giving a counterexample , we disprove the conjecture in welling and teh ( 2001 ) : even when the bethe free energy is not bounded from below , it can possess a local minimum to which the minimization algorithms can converge .", "topics": ["approximation algorithm", "approximation"]}
{"title": "what you need to know about the state-of-the-art computational models of object-vision : a tour through the models", "abstract": "models of object vision have been of great interest in computer vision and visual neuroscience . during the last decades , several models have been developed to extract visual features from images for object recognition tasks . some of these were inspired by the hierarchical structure of primate visual system , and some others were engineered models . the models are varied in several aspects : models that are trained by supervision , models trained without supervision , and models ( e.g . feature extractors ) that are fully hard-wired and do not need training . some of the models come with a deep hierarchical structure consisting of several layers , and some others are shallow and come with only one or two layers of processing . more recently , new models have been developed that are not hand-tuned but trained using millions of images , through which they learn how to extract informative task-related features . here i will survey all these different models and provide the reader with an intuitive , as well as a more detailed , understanding of the underlying computations in each of the models .", "topics": ["feature extraction", "computer vision"]}
{"title": "tracking algorithm for microscopic flow data collection", "abstract": "various methods to automate traffic data collection have recently been developed by many researchers . a macroscopic data collection through image processing has been proposed . for microscopic traffic flow data , such as individual speed and time or distance headway , tracking of individual movement is needed . the tracking algorithms for pedestrian or vehicle have been developed to trace the movement of one or two pedestrians based on sign pattern , and feature detection . no research has been done to track many pedestrians or vehicles at once . this paper describes a new and fast algorithm to track the movement of many individual vehicles or pedestrians", "topics": ["image processing"]}
{"title": "open system categorical quantum semantics in natural language processing", "abstract": "originally inspired by categorical quantum mechanics ( abramsky and coecke , lics'04 ) , the categorical compositional distributional model of natural language meaning of coecke , sadrzadeh and clark provides a conceptually motivated procedure to compute the meaning of a sentence , given its grammatical structure within a lambek pregroup and a vectorial representation of the meaning of its parts . the predictions of this first model have outperformed that of other models in mainstream empirical language processing tasks on large scale data . moreover , just like cqm allows for varying the model in which we interpret quantum axioms , one can also vary the model in which we interpret word meaning . in this paper we show that further developments in categorical quantum mechanics are relevant to natural language processing too . firstly , selinger 's cpm-construction allows for explicitly taking into account lexical ambiguity and distinguishing between the two inherently different notions of homonymy and polysemy . in terms of the model in which we interpret word meaning , this means a passage from the vector space model to density matrices . despite this change of model , standard empirical methods for comparing meanings can be easily adopted , which we demonstrate by a small-scale experiment on real-world data . this experiment moreover provides preliminary evidence of the validity of our proposed new model for word meaning . secondly , commutative classical structures as well as their non-commutative counterparts that arise in the image of the cpm-construction allow for encoding relative pronouns , verbs and adjectives , and finally , iteration of the cpm-construction , something that has no counterpart in the quantum realm , enables one to accommodate both entailment and ambiguity .", "topics": ["natural language processing", "natural language"]}
{"title": "categorical reparameterization with gumbel-softmax", "abstract": "categorical variables are a natural choice for representing discrete structure in the world . however , stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples . in this work , we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel gumbel-softmax distribution . this distribution has the essential property that it can be smoothly annealed into a categorical distribution . we show that our gumbel-softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables , and enables large speedups on semi-supervised classification .", "topics": ["supervised learning", "gradient"]}
{"title": "sub-optimal multi-phase path planning : a method for solving rubik 's revenge", "abstract": "rubik 's revenge , a 4x4x4 variant of the rubik 's puzzles , remains to date as an unsolved puzzle . that is to say , we do not have a method or successful categorization to optimally solve every one of its approximately $ 7.401 \\times 10^ { 45 } $ possible configurations . rubik 's cube , rubik 's revenge 's predecessor ( 3x3x3 ) , with its approximately $ 4.33 \\times 10^ { 19 } $ possible configurations , has only recently been completely solved by rokicki et . al , further finding that any configuration requires no more than 20 moves . with the sheer dimension of rubik 's revenge and its total configuration space , a brute-force method of finding all optimal solutions would be in vain . similar to the methods used by rokicki et . al on rubik 's cube , in this paper we develop a method for solving arbitrary configurations of rubik 's revenge in phases , using a combination of a powerful algorithm known as ida* and a useful definition of distance in the cube space . while time-series results were not successfully gathered , it will be shown that this method far outweighs current human-solving methods and can be used to determine loose upper bounds for the cube space . discussion will suggest that this method can also be applied to other puzzles with the proper transformations .", "topics": ["time series", "mathematical optimization"]}
{"title": "why m heads are better than one : training a diverse ensemble of deep networks", "abstract": "convolutional neural networks have achieved state-of-the-art performance on a wide range of tasks . most benchmarks are led by ensembles of these powerful learners , but ensembling is typically treated as a post-hoc procedure implemented by averaging independently trained models with model variation induced by bagging or random initialization . in this paper , we rigorously treat ensembling as a first-class problem to explicitly address the question : what are the best strategies to create an ensemble ? we first compare a large number of ensembling strategies , and then propose and evaluate novel strategies , such as parameter sharing ( through a new family of models we call treenets ) as well as training under ensemble-aware and diversity-encouraging losses . we demonstrate that treenets can improve ensemble performance and that diverse ensembles can be trained end-to-end under a unified loss , achieving significantly higher `` oracle '' accuracies than classical ensembles .", "topics": ["neural networks"]}
{"title": "multi-task neural networks for qsar predictions", "abstract": "although artificial neural networks have occasionally been used for quantitative structure-activity/property relationship ( qsar/qspr ) studies in the past , the literature has of late been dominated by other machine learning techniques such as random forests . however , a variety of new neural net techniques along with successful applications in other domains have renewed interest in network approaches . in this work , inspired by the winning team 's use of neural networks in a recent qsar competition , we used an artificial neural network to learn a function that predicts activities of compounds for multiple assays at the same time . we conducted experiments leveraging recent methods for dealing with overfitting in neural networks as well as other tricks from the neural networks literature . we compared our methods to alternative methods reported to perform well on these tasks and found that our neural net methods provided superior performance .", "topics": ["neural networks"]}
{"title": "personalized emphasis framing for persuasive message generation", "abstract": "in this paper , we present a study on personalized emphasis framing which can be used to tailor the content of a message to enhance its appeal to different individuals . with this framework , we directly model content selection decisions based on a set of psychologically-motivated domain-independent personal traits including personality ( e.g . , extraversion and conscientiousness ) and basic human values ( e.g . , self-transcendence and hedonism ) . we also demonstrate how the analysis results can be used in automated personalized content selection for persuasive message generation .", "topics": ["value ( ethics )"]}
{"title": "lstm : a search space odyssey", "abstract": "several variants of the long short-term memory ( lstm ) architecture for recurrent neural networks have been proposed since its inception in 1995 . in recent years , these networks have become the state-of-the-art models for a variety of machine learning problems . this has led to a renewed interest in understanding the role and utility of various computational components of typical lstm variants . in this paper , we present the first large-scale analysis of eight lstm variants on three representative tasks : speech recognition , handwriting recognition , and polyphonic music modeling . the hyperparameters of all lstm variants for each task were optimized separately using random search , and their importance was assessed using the powerful fanova framework . in total , we summarize the results of 5400 experimental runs ( $ \\approx 15 $ years of cpu time ) , which makes our study the largest of its kind on lstm networks . our results show that none of the variants can improve upon the standard lstm architecture significantly , and demonstrate the forget gate and the output activation function to be its most critical components . we further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment .", "topics": ["recurrent neural network", "speech recognition"]}
{"title": "partition-wise linear models", "abstract": "region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations . one of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models . this paper proposes novel convex region-specific linear models , which we refer to as partition-wise linear models . our key ideas are 1 ) assigning linear models not to regions but to partitions ( region-specifiers ) and representing region-specific linear models by linear combinations of partition-specific models , and 2 ) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations . in addition to providing initialization-free globally-optimal solutions , our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations . experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models .", "topics": ["nonlinear system", "sparse matrix"]}
{"title": "a convex similarity index for sparse recovery of missing image samples", "abstract": "this paper investigates the problem of recovering missing samples using methods based on sparse representation adapted especially for image signals . instead of $ l_2 $ -norm or mean square error ( mse ) , a new perceptual quality measure is used as the similarity criterion between the original and the reconstructed images . the proposed criterion called convex similarity ( csim ) index is a modified version of the structural similarity ( ssim ) index , which despite its predecessor , is convex and uni-modal . we derive mathematical properties for the proposed index and show how to optimally choose the parameters of the proposed criterion , investigating the restricted isometry ( rip ) and error-sensitivity properties . we also propose an iterative sparse recovery method based on a constrained $ l_1 $ -norm minimization problem , incorporating csim as the fidelity criterion . the resulting convex optimization problem is solved via an algorithm based on alternating direction method of multipliers ( admm ) . taking advantage of the convexity of the csim index , we also prove the convergence of the algorithm to the globally optimal solution of the proposed optimization problem , starting from any arbitrary point . simulation results confirm the performance of the new similarity index as well as the proposed algorithm for missing sample recovery of image patch signals .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "implementing the deep q-network", "abstract": "the deep q-network proposed by mnih et al . [ 2015 ] has become a benchmark and building point for much deep reinforcement learning research . however , replicating results for complex systems is often challenging since original scientific publications are not always able to describe in detail every important parameter setting and software engineering solution . in this paper , we present results from our work reproducing the results of the dqn paper . we highlight key areas in the implementation that were not covered in great detail in the original paper to make it easier for researchers to replicate these results , including termination conditions and gradient descent algorithms . finally , we discuss methods for improving the computational performance and provide our own implementation that is designed to work with a range of domains , and not just the original arcade learning environment [ bellemare et al . , 2013 ] .", "topics": ["reinforcement learning", "gradient descent"]}
{"title": "wavelet residual network for low-dose ct via deep convolutional framelets", "abstract": "model based iterative reconstruction ( mbir ) algorithms for low-dose x-ray ct are computationally expensive . to address this problem , we recently proposed the world-first deep convolutional neural network ( cnn ) for low-dose x-ray ct and won the second place in 2016 aapm low-dose ct grand challenge . however , some of the texture were not fully recovered . to cope with this problem , here we propose a deep residual learning approach in directional wavelet domain . the proposed method is motivated by an observation that a deep convolutional neural network can be interpreted as a multilayer convolutional framelets expansion using non-local basis convolved with data-driven local basis . we further extend the idea to derive a deep convolutional framelet expansion by combining global redundant transforms and signal boosting from multiple signal representations . extensive experimental results confirm that the proposed network has significantly improved performance and preserves the detail texture of the original images", "topics": ["convolution"]}
{"title": "3d object dense reconstruction from a single depth view", "abstract": "in this paper , we propose a novel approach , 3d-recgan++ , which reconstructs the complete 3d structure of a given object from a single arbitrary depth view using generative adversarial networks . unlike existing work which typically requires multiple views of the same object or class labels to recover the full 3d geometry , the proposed 3d-recgan++ only takes the voxel grid representation of a depth view of the object as input , and is able to generate the complete 3d occupancy grid with a high resolution of 256^3 by recovering the occluded/missing regions . the key idea is to combine the generative capabilities of autoencoders and the conditional generative adversarial networks ( gan ) framework , to infer accurate and fine-grained 3d structures of objects in high-dimensional voxel space . extensive experiments on large synthetic datasets and real-world kinect datasets show that the proposed 3d-recgan++ significantly outperforms the state of the art in single view 3d object reconstruction , and is able to reconstruct unseen types of objects .", "topics": ["synthetic data", "autoencoder"]}
{"title": "online convex optimization with stochastic constraints", "abstract": "this paper considers online convex optimization ( oco ) with stochastic constraints , which generalizes zinkevich 's oco over a known simple fixed set by introducing multiple stochastic functional constraints that are i.i.d . generated at each round and are disclosed to the decision maker only after the decision is made . this formulation arises naturally when decisions are restricted by stochastic environments or deterministic environments with noisy observations . it also includes many important problems as special cases , such as oco with long term constraints , stochastic constrained convex optimization , and deterministic constrained convex optimization . to solve this problem , this paper proposes a new algorithm that achieves $ o ( \\sqrt { t } ) $ expected regret and constraint violations and $ o ( \\sqrt { t } \\log ( t ) ) $ high probability regret and constraint violations . experiments on a real-world data center scheduling problem further verify the performance of the new algorithm .", "topics": ["regret ( decision theory )"]}
{"title": "converting cascade-correlation neural nets into probabilistic generative models", "abstract": "humans are not only adept in recognizing what class an input instance belongs to ( i.e . , classification task ) , but perhaps more remarkably , they can imagine ( i.e . , generate ) plausible instances of a desired class with ease , when prompted . inspired by this , we propose a framework which allows transforming cascade-correlation neural networks ( ccnns ) into probabilistic generative models , thereby enabling ccnns to generate samples from a category of interest . ccnns are a well-known class of deterministic , discriminative nns , which autonomously construct their topology , and have been successful in giving accounts for a variety of psychological phenomena . our proposed framework is based on a markov chain monte carlo ( mcmc ) method , called the metropolis-adjusted langevin algorithm , which capitalizes on the gradient information of the target distribution to direct its explorations towards regions of high probability , thereby achieving good mixing properties . through extensive simulations , we demonstrate the efficacy of our proposed framework .", "topics": ["neural networks", "simulation"]}
{"title": "authorship attribution using a neural network language model", "abstract": "in practice , training language models for individual authors is often expensive because of limited data resources . in such cases , neural network language models ( nnlms ) , generally outperform the traditional non-parametric n-gram models . here we investigate the performance of a feed-forward nnlm on an authorship attribution problem , with moderate author set size and relatively limited data . we also consider how the text topics impact performance . compared with a well-constructed n-gram baseline method with kneser-ney smoothing , the proposed method achieves nearly 2:5 % reduction in perplexity and increases author classification accuracy by 3:43 % on average , given as few as 5 test sentences . the performance is very competitive with the state of the art in terms of accuracy and demand on test data . the source code , preprocessed datasets , a detailed description of the methodology and results are available at https : //github.com/zge/authorship-attribution .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "towards bayesian deep learning : a survey", "abstract": "while perception tasks such as visual object recognition and text understanding play an important role in human intelligence , the subsequent tasks that involve inference , reasoning and planning require an even higher level of intelligence . the past few years have seen major advances in many perception tasks using deep learning models . for higher-level inference , however , probabilistic graphical models with their bayesian nature are still more powerful and flexible . to achieve integrated intelligence that involves both perception and inference , it is naturally desirable to tightly integrate deep learning and bayesian models within a principled probabilistic framework , which we call bayesian deep learning . in this unified framework , the perception of text or images using deep learning can boost the performance of higher-level inference and in return , the feedback from the inference process is able to enhance the perception of text or images . this survey provides a general introduction to bayesian deep learning and reviews its recent applications on recommender systems , topic models , and control . in this survey , we also discuss the relationship and differences between bayesian deep learning and other related topics like bayesian treatment of neural networks .", "topics": ["graphical model", "artificial intelligence"]}
{"title": "solving the brachistochrone problem by an influence diagram", "abstract": "influence diagrams are a decision-theoretic extension of probabilistic graphical models . in this paper we show how they can be used to solve the brachistochrone problem . we present results of numerical experiments on this problem , compare the solution provided by the influence diagram with the optimal solution . the r code used for the experiments is presented in the appendix .", "topics": ["graphical model", "optimization problem"]}
{"title": "identifying finite mixtures of nonparametric product distributions and causal inference of confounders", "abstract": "we propose a kernel method to identify finite mixtures of nonparametric product distributions . it is based on a hilbert space embedding of the joint distribution . the rank of the constructed tensor is equal to the number of mixture components . we present an algorithm to recover the components by partitioning the data points into clusters such that the variables are jointly conditionally independent given the cluster . this method can be used to identify finite confounders .", "topics": ["causality"]}
{"title": "generating a diverse set of high-quality clusterings", "abstract": "we provide a new framework for generating multiple good quality partitions ( clusterings ) of a single data set . our approach decomposes this problem into two components , generating many high-quality partitions , and then grouping these partitions to obtain k representatives . the decomposition makes the approach extremely modular and allows us to optimize various criteria that control the choice of representative partitions .", "topics": ["cluster analysis"]}
{"title": "an additive model view to sparse gaussian process classifier design", "abstract": "we consider the problem of designing a sparse gaussian process classifier ( sgpc ) that generalizes well . viewing sgpc design as constructing an additive model like in boosting , we present an efficient and effective sgpc design method to perform a stage-wise optimization of a predictive loss function . we introduce new methods for two key components viz . , site parameter estimation and basis vector selection in any sgpc design . the proposed adaptive sampling based basis vector selection method aids in achieving improved generalization performance at a reduced computational cost . this method can also be used in conjunction with any other site parameter estimation methods . it has similar computational and storage complexities as the well-known information vector machine and is suitable for large datasets . the hyperparameters can be determined by optimizing a predictive loss function . the experimental results show better generalization performance of the proposed basis vector selection method on several benchmark datasets , particularly for relatively smaller basis vector set sizes or on difficult datasets .", "topics": ["sampling ( signal processing )", "loss function"]}
{"title": "multi-gpu training of convnets", "abstract": "in this work we evaluate different approaches to parallelize computation of convolutional neural networks across several gpus .", "topics": ["gradient descent", "gradient"]}
{"title": "distributed robust power system state estimation", "abstract": "deregulation of energy markets , penetration of renewables , advanced metering capabilities , and the urge for situational awareness , all call for system-wide power system state estimation ( psse ) . implementing a centralized estimator though is practically infeasible due to the complexity scale of an interconnection , the communication bottleneck in real-time monitoring , regional disclosure policies , and reliability issues . in this context , distributed psse methods are treated here under a unified and systematic framework . a novel algorithm is developed based on the alternating direction method of multipliers . it leverages existing psse solvers , respects privacy policies , exhibits low communication load , and its convergence to the centralized estimates is guaranteed even in the absence of local observability . beyond the conventional least-squares based psse , the decentralized framework accommodates a robust state estimator . by exploiting interesting links to the compressive sampling advances , the latter jointly estimates the state and identifies corrupted measurements . the novel algorithms are numerically evaluated using the ieee 14- , 118-bus , and a 4,200-bus benchmarks . simulations demonstrate that the attainable accuracy can be reached within a few inter-area exchanges , while largest residual tests are outperformed .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "simultaneous object detection , tracking , and event recognition", "abstract": "the common internal structure and algorithmic organization of object detection , detection-based tracking , and event recognition facilitates a general approach to integrating these three components . this supports multidirectional information flow between these components allowing object detection to influence tracking and event recognition and event recognition to influence tracking and object detection . the performance of the combination can exceed the performance of the components in isolation . this can be done with linear asymptotic complexity .", "topics": ["object detection", "computational complexity theory"]}
{"title": "sketch and validate for big data clustering", "abstract": "in response to the need for learning tools tuned to big data analytics , the present paper introduces a framework for efficient clustering of huge sets of ( possibly high-dimensional ) data . building on random sampling and consensus ( ransac ) ideas pursued earlier in a different ( computer vision ) context for robust regression , a suite of novel dimensionality and set-reduction algorithms is developed . the advocated sketch-and-validate ( skeva ) family includes two algorithms that rely on k-means clustering per iteration on reduced number of dimensions and/or feature vectors : the first operates in a batch fashion , while the second sequential one offers computational efficiency and suitability with streaming modes of operation . for clustering even nonlinearly separable vectors , the skeva family offers also a member based on user-selected kernel functions . further trading off performance for reduced complexity , a fourth member of the skeva family is based on a divergence criterion for selecting proper minimal subsets of feature variables and vectors , thus bypassing the need for k-means clustering per iteration . extensive numerical tests on synthetic and real data sets highlight the potential of the proposed algorithms , and demonstrate their competitive performance relative to state-of-the-art random projection alternatives .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "interpretable two-level boolean rule learning for classification", "abstract": "as a contribution to interpretable machine learning research , we develop a novel optimization framework for learning accurate and sparse two-level boolean rules . we consider rules in both conjunctive normal form ( and-of-ors ) and disjunctive normal form ( or-of-ands ) . a principled objective function is proposed to trade classification accuracy and interpretability , where we use hamming loss to characterize accuracy and sparsity to characterize interpretability . we propose efficient procedures to optimize these objectives based on linear programming ( lp ) relaxation , block coordinate descent , and alternating minimization . experiments show that our new algorithms provide very good tradeoffs between accuracy and interpretability .", "topics": ["optimization problem", "loss function"]}
{"title": "approximated computation of belief functions for robust design optimization", "abstract": "this paper presents some ideas to reduce the computational cost of evidence-based robust design optimization . evidence theory crystallizes both the aleatory and epistemic uncertainties in the design parameters , providing two quantitative measures , belief and plausibility , of the credibility of the computed value of the design budgets . the paper proposes some techniques to compute an approximation of belief and plausibility at a cost that is a fraction of the one required for an accurate calculation of the two values . some simple test cases will show how the proposed techniques scale with the dimension of the problem . finally a simple example of spacecraft system design is presented .", "topics": ["computation"]}
{"title": "adaptive representation selection in contextual bandit with unlabeled history", "abstract": "we consider an extension of the contextual bandit setting , motivated by several practical applications , where an unlabeled history of contexts can become available for pre-training before the online decision-making begins . we propose an approach for improving the performance of contextual bandit in such setting , via adaptive , dynamic representation learning , which combines offline pre-training on unlabeled history of contexts with online selection and modification of embedding functions . our experiments on a variety of datasets and in different nonstationary environments demonstrate clear advantages of our approach over the standard contextual bandit .", "topics": ["feature learning"]}
{"title": "scalable and robust sparse subspace clustering using randomized clustering and multilayer graphs", "abstract": "sparse subspace clustering ( ssc ) is one of the current state-of-the-art methods for partitioning data points into the union of subspaces , with strong theoretical guarantees . however , it is not practical for large data sets as it requires solving a lasso problem for each data point , where the number of variables in each lasso problem is the number of data points . to improve the scalability of ssc , we propose to select a few sets of anchor points using a randomized hierarchical clustering method , and , for each set of anchor points , solve the lasso problems for each data point allowing only anchor points to have a non-zero weight ( this reduces drastically the number of variables ) . this generates a multilayer graph where each layer corresponds to a different set of anchor points . using the grassmann manifold of orthogonal matrices , the shared connectivity among the layers is summarized within a single subspace . finally , we use $ k $ -means clustering within that subspace to cluster the data points , similarly as done by spectral clustering in ssc . we show on both synthetic and real-world data sets that the proposed method not only allows ssc to scale to large-scale data sets , but that it is also much more robust as it performs significantly better on noisy data and on data with close susbspaces and outliers , while it is not prone to oversegmentation .", "topics": ["cluster analysis", "synthetic data"]}
{"title": "consistent feature attribution for tree ensembles", "abstract": "note that a newer expanded version of this paper is now available at : arxiv:1802.03888 it is critical in many applications to understand what features are important for a model , and why individual predictions were made . for tree ensemble methods these questions are usually answered by attributing importance values to input features , either globally or for a single prediction . here we show that current feature attribution methods are inconsistent , which means changing the model to rely more on a given feature can actually decrease the importance assigned to that feature . to address this problem we develop fast exact solutions for shap ( shapley additive explanation ) values , which were recently shown to be the unique additive feature attribution method based on conditional expectations that is both consistent and locally accurate . we integrate these improvements into the latest version of xgboost , demonstrate the inconsistencies of current methods , and show how using shap values results in significantly improved supervised clustering performance . feature importance values are a key part of understanding widely used models such as gradient boosting trees and random forests , so improvements to them have broad practical implications .", "topics": ["cluster analysis", "value ( ethics )"]}
{"title": "an infinite hidden markov model with similarity-biased transitions", "abstract": "we describe a generalization of the hierarchical dirichlet process hidden markov model ( hdp-hmm ) which is able to encode prior information that state transitions are more likely between `` nearby '' states . this is accomplished by defining a similarity function on the state space and scaling transition probabilities by pair-wise similarities , thereby inducing correlations among the transition distributions . we present an augmented data representation of the model as a markov jump process in which : ( 1 ) some jump attempts fail , and ( 2 ) the probability of success is proportional to the similarity between the source and destination states . this augmentation restores conditional conjugacy and admits a simple gibbs sampler . we evaluate the model and inference method on a speaker diarization task and a `` harmonic parsing '' task using four-part chorale data , as well as on several synthetic datasets , achieving favorable comparisons to existing models .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "generating visual explanations", "abstract": "clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself . existing approaches for deep visual recognition are generally opaque and do not output any justification text ; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions . we propose a new model that focuses on the discriminating properties of the visible object , jointly predicts a class label , and explains why the predicted label is appropriate for the image . we propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property , such as class specificity . our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods .", "topics": ["sampling ( signal processing )", "reinforcement learning"]}
{"title": "weighted positive binary decision diagrams for exact probabilistic inference", "abstract": "recent work on weighted model counting has been very successfully applied to the problem of probabilistic inference in bayesian networks . the probability distribution is encoded into a boolean normal form and compiled to a target language , in order to represent local structure expressed among conditional probabilities more efficiently . we show that further improvements are possible , by exploiting the knowledge that is lost during the encoding phase and incorporating it into a compiler inspired by satisfiability modulo theories . constraints among variables are used as a background theory , which allows us to optimize the shannon decomposition . we propose a new language , called weighted positive binary decision diagrams , that reduces the cost of probabilistic inference by using this decomposition variant to induce an arithmetic circuit of reduced size .", "topics": ["bayesian network"]}
{"title": "video in sentences out", "abstract": "we present a system that produces sentential descriptions of video : who did what to whom , and where and how they did it . action class is rendered as a verb , participant objects as noun phrases , properties of those objects as adjectival modifiers in those noun phrases , spatial relations between those participants as prepositional phrases , and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers . extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks , the track-to-role assignments , and changing body posture .", "topics": ["entity"]}
{"title": "bags of local convolutional features for scalable instance search", "abstract": "this work proposes a simple instance retrieval pipeline based on encoding the convolutional features of cnn using the bag of words aggregation scheme ( bow ) . assigning each local array of activations in a convolutional layer to a visual word produces an \\textit { assignment map } , a compact representation that relates regions of an image with a visual word . we use the assignment map for fast spatial reranking , obtaining object localizations that are used for query expansion . we demonstrate the suitability of the bow representation based on local cnn features for instance retrieval , achieving competitive performance on the oxford and paris buildings benchmarks . we show that our proposed system for cnn feature aggregation with bow outperforms state-of-the-art techniques using sum pooling at a subset of the challenging trecvid ins benchmark .", "topics": ["map"]}
{"title": "automatic generation of grounded visual questions", "abstract": "in this paper , we propose the first model to be able to generate visually grounded questions with diverse types for a single image . visual question generation is an emerging topic which aims to ask questions in natural language based on visual input . to the best of our knowledge , it lacks automatic methods to generate meaningful questions with various types for the same visual input . to circumvent the problem , we propose a model that automatically generates visually grounded questions with varying types . our model takes as input both images and the captions generated by a dense caption model , samples the most probable question types , and generates the questions in sequel . the experimental results on two real world datasets show that our model outperforms the strongest baseline in terms of both correctness and diversity with a wide margin .", "topics": ["baseline ( configuration management )", "natural language"]}
{"title": "feature selection using classifier in high dimensional data", "abstract": "feature selection is frequently used as a pre-processing step to machine learning . it is a process of choosing a subset of original features so that the feature space is optimally reduced according to a certain evaluation criterion . the central objective of this paper is to reduce the dimension of the data by finding a small set of important features which can give good classification performance . we have applied filter and wrapper approach with different classifiers qda and lda respectively . a widely-used filter method is used for bioinformatics data i.e . a univariate criterion separately on each feature , assuming that there is no interaction between features and then applied sequential feature selection method . experimental results show that filter approach gives better performance in respect of misclassification error rate .", "topics": ["feature vector", "eisenstein 's criterion"]}
{"title": "multi-objective approaches to markov decision processes with uncertain transition parameters", "abstract": "markov decision processes ( mdps ) are a popular model for performance analysis and optimization of stochastic systems . the parameters of stochastic behavior of mdps are estimates from empirical observations of a system ; their values are not known precisely . different types of mdps with uncertain , imprecise or bounded transition rates or probabilities and rewards exist in the literature . commonly , analysis of models with uncertainties amounts to searching for the most robust policy which means that the goal is to generate a policy with the greatest lower bound on performance ( or , symmetrically , the lowest upper bound on costs ) . however , hedging against an unlikely worst case may lead to losses in other situations . in general , one is interested in policies that behave well in all situations which results in a multi-objective view on decision making . in this paper , we consider policies for the expected discounted reward measure of mdps with uncertain parameters . in particular , the approach is defined for bounded-parameter mdps ( bmdps ) [ 8 ] . in this setting the worst , best and average case performances of a policy are analyzed simultaneously , which yields a multi-scenario multi-objective optimization problem . the paper presents and evaluates approaches to compute the pure pareto optimal policies in the value vector space .", "topics": ["value ( ethics )", "optimization problem"]}
{"title": "modeling label ambiguity for neural list-wise learning to rank", "abstract": "list-wise learning to rank methods are considered to be the state-of-the-art . one of the major problems with these methods is that the ambiguous nature of relevance labels in learning to rank data is ignored . ambiguity of relevance labels refers to the phenomenon that multiple documents may be assigned the same relevance label for a given query , so that no preference order should be learned for those documents . in this paper we propose a novel sampling technique for computing a list-wise loss that can take into account this ambiguity . we show the effectiveness of the proposed method by training a 3-layer deep neural network . we compare our new loss function to two strong baselines : listnet and listmle . we show that our method generalizes better and significantly outperforms other methods on the validation and test sets .", "topics": ["loss function", "relevance"]}
{"title": "image-text multi-modal representation learning by adversarial backpropagation", "abstract": "we present novel method for image-text multi-modal representation learning . in our knowledge , this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature . we only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding . in this paper , we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information . and we show our multi-modal feature has universal semantic information , even though it was trained for category prediction . our model is end-to-end backpropagation , intuitive and easily extended to other multi-modal learning work .", "topics": ["feature learning", "feature vector"]}
{"title": "distilling model knowledge", "abstract": "top-performing machine learning systems , such as deep neural networks , large ensembles and complex probabilistic graphical models , can be expensive to store , slow to evaluate and hard to integrate into larger systems . ideally , we would like to replace such cumbersome models with simpler models that perform equally well . in this thesis , we study knowledge distillation , the idea of extracting the knowledge contained in a complex model and injecting it into a more convenient model . we present a general framework for knowledge distillation , whereby a convenient model of our choosing learns how to mimic a complex model , by observing the latter 's behaviour and being penalized whenever it fails to reproduce it . we develop our framework within the context of three distinct machine learning applications : ( a ) model compression , where we compress large discriminative models , such as ensembles of neural networks , into models of much smaller size ; ( b ) compact predictive distributions for bayesian inference , where we distil large bags of mcmc samples into compact predictive distributions in closed form ; ( c ) intractable generative models , where we distil unnormalizable models such as rbms into tractable models such as nades . we contribute to the state of the art with novel techniques and ideas . in model compression , we describe and implement derivative matching , which allows for better distillation when data is scarce . in compact predictive distributions , we introduce online distillation , which allows for significant savings in memory . finally , in intractable generative models , we show how to use distilled models to robustly estimate intractable quantities of the original model , such as its intractable partition function .", "topics": ["generative model", "graphical model"]}
{"title": "sentibubbles : topic modeling and sentiment visualization of entity-centric tweets", "abstract": "social media users tend to mention entities when reacting to news events . the main purpose of this work is to create entity-centric aggregations of tweets on a daily basis . by applying topic modeling and sentiment analysis , we create data visualization insights about current events and people reactions to those events from an entity-centric perspective .", "topics": ["entity"]}
{"title": "dissimilarity clustering by hierarchical multi-level refinement", "abstract": "we introduce in this paper a new way of optimizing the natural extension of the quantization error using in k-means clustering to dissimilarity data . the proposed method is based on hierarchical clustering analysis combined with multi-level heuristic refinement . the method is computationally efficient and achieves better quantization errors than the", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "unsupervised , efficient and semantic expertise retrieval", "abstract": "we introduce an unsupervised discriminative model for the task of retrieving experts in online document collections . we exclusively employ textual evidence and avoid explicit feature engineering by learning distributed word representations in an unsupervised way . we compare our model to state-of-the-art unsupervised statistical vector space and probabilistic generative approaches . our proposed log-linear model achieves the retrieval performance levels of state-of-the-art document-centric methods with the low inference cost of so-called profile-centric approaches . it yields a statistically significant improved ranking over vector space and generative models in most cases , matching the performance of supervised methods on various benchmarks . that is , by using solely text we can do as well as methods that work with external evidence and/or relevance feedback . a contrastive analysis of rankings produced by discriminative and generative approaches shows that they have complementary strengths due to the ability of the unsupervised discriminative model to perform semantic matching .", "topics": ["generative model", "supervised learning"]}
{"title": "cardea : context-aware visual privacy protection from pervasive cameras", "abstract": "the growing popularity of mobile and wearable devices with built-in cameras , the bright prospect of camera related applications such as augmented reality and life-logging system , the increased ease of taking and sharing photos , and advances in computer vision techniques have greatly facilitated people 's lives in many aspects , but have also inevitably raised people 's concerns about visual privacy at the same time . motivated by recent user studies that people 's privacy concerns are dependent on the context , in this paper , we propose cardea , a context-aware and interactive visual privacy protection framework that enforces privacy protection according to people 's privacy preferences . the framework provides people with fine-grained visual privacy protection using : i ) personal privacy profiles , with which people can define their context-dependent privacy preferences ; and ii ) visual indicators : face features , for devices to automatically locate individuals who request privacy protection ; and iii ) hand gestures , for people to flexibly interact with cameras to temporarily change their privacy preferences . we design and implement the framework consisting of the client app on android devices and the cloud server . our evaluation results confirm this framework is practical and effective with 86 % overall accuracy , showing promising future for context-aware visual privacy protection from pervasive cameras .", "topics": ["computer vision"]}
{"title": "high efficiency compression for object detection", "abstract": "image and video compression has traditionally been tailored to human vision . however , modern applications such as visual analytics and surveillance rely on computers seeing and analyzing the images before ( or instead of ) humans . for these applications , it is important to adjust compression to computer vision . in this paper we present a bit allocation and rate control strategy that is tailored to object detection . using the initial convolutional layers of a state-of-the-art object detector , we create an importance map that can guide bit allocation to areas that are important for object detection . the proposed method enables bit rate savings of 7 % or more compared to default hevc , at the equivalent object detection rate .", "topics": ["object detection", "computer vision"]}
{"title": "convergent message-passing algorithms for inference over general graphs with convex free energies", "abstract": "inference problems in graphical models can be represented as a constrained optimization of a free energy function . it is known that when the bethe free energy is used , the fixedpoints of the belief propagation ( bp ) algorithm correspond to the local minima of the free energy . however bp fails to converge in many cases of interest . moreover , the bethe free energy is non-convex for graphical models with cycles thus introducing great difficulty in deriving efficient algorithms for finding local minima of the free energy for general graphs . in this paper we introduce two efficient bp-like algorithms , one sequential and the other parallel , that are guaranteed to converge to the global minimum , for any graph , over the class of energies known as `` convex free energies '' . in addition , we propose an efficient heuristic for setting the parameters of the convex free energy based on the structure of the graph .", "topics": ["graphical model", "mathematical optimization"]}
{"title": "exploring the regularity of sparse structure in convolutional neural networks", "abstract": "sparsity helps reduce the computational complexity of deep neural networks by skipping zeros . taking advantage of sparsity is listed as a high priority in next generation dnn accelerators such as tpu . the structure of sparsity , i.e . , the granularity of pruning , affects the efficiency of hardware accelerator design as well as the prediction accuracy . coarse-grained pruning creates regular sparsity patterns , making it more amenable for hardware acceleration but more challenging to maintain the same accuracy . in this paper we quantitatively measure the trade-off between sparsity regularity and prediction accuracy , providing insights in how to maintain accuracy while having more a more structured sparsity pattern . our experimental results show that coarse-grained pruning can achieve a sparsity ratio similar to unstructured pruning without loss of accuracy . moreover , due to the index saving effect , coarse-grained pruning is able to obtain a better compression ratio than fine-grained sparsity at the same accuracy threshold . based on the recent sparse convolutional neural network accelerator ( scnn ) , our experiments further demonstrate that coarse-grained sparsity saves about 2x the memory references compared to fine-grained sparsity . since memory reference is more than two orders of magnitude more expensive than arithmetic operations , the regularity of sparse structure leads to more efficient hardware design .", "topics": ["computational complexity theory", "neural networks"]}
{"title": "defeating image obfuscation with deep learning", "abstract": "we demonstrate that modern image recognition methods based on artificial neural networks can recover hidden information from images protected by various forms of obfuscation . the obfuscation techniques considered in this paper are mosaicing ( also known as pixelation ) , blurring ( as used by youtube ) , and p3 , a recently proposed system for privacy-preserving photo sharing that encrypts the significant jpeg coefficients to make images unrecognizable by humans . we empirically show how to train artificial neural networks to successfully identify faces and recognize objects and handwritten digits even if the images are protected using any of the above obfuscation techniques .", "topics": ["computer vision"]}
{"title": "classification approach based on association rules mining for unbalanced data", "abstract": "this paper deals with the binary classification task when the target class has the lower probability of occurrence . in such situation , it is not possible to build a powerful classifier by using standard methods such as logistic regression , classification tree , discriminant analysis , etc . to overcome this short-coming of these methods which yield classifiers with low sensibility , we tackled the classification problem here through an approach based on the association rules learning . this approach has the advantage of allowing the identification of the patterns that are well correlated with the target class . association rules learning is a well known method in the area of data-mining . it is used when dealing with large database for unsupervised discovery of local patterns that expresses hidden relationships between input variables . in considering association rules from a supervised learning point of view , a relevant set of weak classifiers is obtained from which one derives a classifier that performs well .", "topics": ["data mining", "supervised learning"]}
{"title": "procedural generation of angry birds levels using building constructive grammar with chinese-style and/or japanese-style models", "abstract": "this paper presents a procedural generation method that creates visually attractive levels for the angry birds game . besides being an immensely popular mobile game , angry birds has recently become a test bed for various artificial intelligence technologies . we propose a new approach for procedurally generating angry birds levels using chinese style and japanese style building structures . a conducted experiment confirms the effectiveness of our approach with statistical significance .", "topics": ["artificial intelligence"]}
{"title": "tasks for agent-based negotiation teams : analysis , review , and challenges", "abstract": "an agent-based negotiation team is a group of interdependent agents that join together as a single negotiation party due to their shared interests in the negotiation at hand . the reasons to employ an agent-based negotiation team may vary : ( i ) more computation and parallelization capabilities , ( ii ) unite agents with different expertise and skills whose joint work makes it possible to tackle complex negotiation domains , ( iii ) the necessity to represent different stakeholders or different preferences in the same party ( e.g . , organizations , countries , and married couple ) . the topic of agent-based negotiation teams has been recently introduced in multi-agent research . therefore , it is necessary to identify good practices , challenges , and related research that may help in advancing the state-of-the-art in agent-based negotiation teams . for that reason , in this article we review the tasks to be carried out by agent-based negotiation teams . each task is analyzed and related with current advances in different research areas . the analysis aims to identify special challenges that may arise due to the particularities of agent-based negotiation teams .", "topics": ["computation"]}
{"title": "on learning to think : algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models", "abstract": "this paper addresses the general problem of reinforcement learning ( rl ) in partially observable environments . in 2013 , our large rl recurrent neural networks ( rnns ) learned from scratch to drive simulated cars from high-dimensional video input . however , real brains are more powerful in many ways . in particular , they learn a predictive model of their initially unknown environment , and somehow use it for abstract ( e.g . , hierarchical ) planning and reasoning . guided by algorithmic information theory , we describe rnn-based ais ( rnnais ) designed to do the same . such an rnnai can be trained on never-ending sequences of tasks , some of them provided by the user , others invented by the rnnai itself in a curious , playful fashion , to improve its rnn-based world model . unlike our previous model-building rnn-based rl machines dating back to 1990 , the rnnai learns to actively query its model for abstract reasoning and planning and decision making , essentially `` learning to think . '' the basic ideas of this report can be applied to many other cases where one rnn-like system exploits the algorithmic information content of another . they are taken from a grant proposal submitted in fall 2014 , and also explain concepts such as `` mirror neurons . '' experimental results will be described in separate papers .", "topics": ["recurrent neural network", "reinforcement learning"]}
{"title": "from-below approximations in boolean matrix factorization : geometry and new algorithm", "abstract": "we present new results on boolean matrix factorization and a new algorithm based on these results . the results emphasize the significance of factorizations that provide from-below approximations of the input matrix . while the previously proposed algorithms do not consider the possibly different significance of different matrix entries , our results help measure such significance and suggest where to focus when computing factors . an experimental evaluation of the new algorithm on both synthetic and real data demonstrates its good performance in terms of good coverage by the first k factors as well as a small number of factors needed for exact decomposition and indicates that the algorithm outperforms the available ones in these terms . we also propose future research topics .", "topics": ["synthetic data", "approximation"]}
{"title": "simultaneously learning neighborship and projection matrix for supervised dimensionality reduction", "abstract": "explicitly or implicitly , most of dimensionality reduction methods need to determine which samples are neighbors and the similarity between the neighbors in the original highdimensional space . the projection matrix is then learned on the assumption that the neighborhood information ( e.g . , the similarity ) is known and fixed prior to learning . however , it is difficult to precisely measure the intrinsic similarity of samples in high-dimensional space because of the curse of dimensionality . consequently , the neighbors selected according to such similarity might and the projection matrix obtained according to such similarity and neighbors are not optimal in the sense of classification and generalization . to overcome the drawbacks , in this paper we propose to let the similarity and neighbors be variables and model them in low-dimensional space . both the optimal similarity and projection matrix are obtained by minimizing a unified objective function . nonnegative and sum-to-one constraints on the similarity are adopted . instead of empirically setting the regularization parameter , we treat it as a variable to be optimized . it is interesting that the optimal regularization parameter is adaptive to the neighbors in low-dimensional space and has intuitive meaning . experimental results on the yale b , coil-100 , and mnist datasets demonstrate the effectiveness of the proposed method .", "topics": ["matrix regularization", "loss function"]}
{"title": "making a science of model search", "abstract": "many computer vision algorithms depend on a variety of parameter choices and settings that are typically hand-tuned in the course of evaluating the algorithm . while such parameter tuning is often presented as being incidental to the algorithm , correctly setting these parameter choices is frequently critical to evaluating a method 's full potential . compounding matters , these parameters often must be re-tuned when the algorithm is applied to a new problem domain , and the tuning process itself often depends on personal experience and intuition in ways that are hard to describe . since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning , it can be difficult to determine whether a given technique is genuinely better , or simply better tuned . in this work , we propose a meta-modeling approach to support automated hyper parameter optimization , with the goal of providing practical tools to replace hand-tuning with a reproducible and unbiased optimization process . our approach is to expose the underlying expression graph of how a performance metric ( e.g . classification accuracy on validation examples ) is computed from parameters that govern not only how individual processing steps are applied , but even which processing steps are included . a hyper parameter optimization algorithm transforms this graph into a program for optimizing that performance metric . our approach yields state of the art results on three disparate computer vision problems : a face-matching verification task ( lfw ) , a face identification task ( pubfig83 ) and an object recognition task ( cifar-10 ) , using a single algorithm . more broadly , we argue that the formalization of a meta-model supports more objective , reproducible , and quantitative evaluation of computer vision algorithms , and that it can serve as a valuable tool for guiding algorithm development .", "topics": ["computer vision"]}
{"title": "submodular meets spectral : greedy algorithms for subset selection , sparse approximation and dictionary selection", "abstract": "we study the problem of selecting a subset of k random variables from a large set , in order to obtain the best linear prediction of another variable of interest . this problem can be viewed in the context of both feature selection and sparse approximation . we analyze the performance of widely used greedy heuristics , using insights from the maximization of submodular functions and spectral analysis . we introduce the submodularity ratio as a key quantity to help understand why greedy algorithms perform well even when the variables are highly correlated . using our techniques , we obtain the strongest known approximation guarantees for this problem , both in terms of the submodularity ratio and the smallest k-sparse eigenvalue of the covariance matrix . we further demonstrate the wide applicability of our techniques by analyzing greedy algorithms for the dictionary selection problem , and significantly improve the previously known guarantees . our theoretical analysis is complemented by experiments on real-world and synthetic data sets ; the experiments show that the submodularity ratio is a stronger predictor of the performance of greedy algorithms than other spectral parameters .", "topics": ["synthetic data", "approximation"]}
{"title": "resilience : a criterion for learning in the presence of arbitrary outliers", "abstract": "we introduce a criterion , resilience , which allows properties of a dataset ( such as its mean or best low rank approximation ) to be robustly computed , even in the presence of a large fraction of arbitrary additional data . resilience is a weaker condition than most other properties considered so far in the literature , and yet enables robust estimation in a broader variety of settings . we provide new information-theoretic results on robust distribution learning , robust estimation of stochastic block models , and robust mean estimation under bounded $ k $ th moments . we also provide new algorithmic results on robust distribution learning , as well as robust mean estimation in $ \\ell_p $ -norms . among our proof techniques is a method for pruning a high-dimensional distribution with bounded $ 1 $ st moments to a stable `` core '' with bounded $ 2 $ nd moments , which may be of independent interest .", "topics": ["eisenstein 's criterion"]}
{"title": "bolt : accelerated data mining with fast vector compression", "abstract": "vectors of data are at the heart of machine learning and data mining . recently , vector quantization methods have shown great promise in reducing both the time and space costs of operating on vectors . we introduce a vector quantization algorithm that can compress vectors over 12x faster than existing techniques while also accelerating approximate vector operations such as distance and dot product computations by up to 10x . because it can encode over 2gb of vectors per second , it makes vector quantization cheap enough to employ in many more circumstances . for example , using our technique to compute approximate dot products in a nested loop can multiply matrices faster than a state-of-the-art blas implementation , even when our algorithm must first compress the matrices . in addition to showing the above speedups , we demonstrate that our approach can accelerate nearest neighbor search and maximum inner product search by over 100x compared to floating point operations and up to 10x compared to other vector quantization methods . our approximate euclidean distance and dot product computations are not only faster than those of related algorithms with slower encodings , but also faster than hamming distance computations , which have direct hardware support on the tested platforms . we also assess the errors of our algorithm 's approximate distances and dot products , and find that it is competitive with existing , slower vector quantization algorithms .", "topics": ["approximation algorithm", "data mining"]}
{"title": "attended end-to-end architecture for age estimation from facial expression videos", "abstract": "the main challenges of age estimation from facial expression videos lie not only in the modeling of the static facial appearance , but also in the capturing of the temporal facial dynamics . traditional techniques to this problem focus on constructing handcrafted features to explore the discriminative information contained in facial appearance and dynamics separately . this relies on sophisticated feature-refinement and framework-design . in this paper , we present an end-to-end architecture for age estimation which is able to simultaneously learn both the appearance and dynamics of age from raw videos of facial expressions . specifically , we employ convolutional neural networks to extract effective latent appearance representations and feed them into recurrent networks to model the temporal dynamics . more importantly , we propose to leverage attention models for salience detection in both the spatial domain for each single image and the temporal domain for the whole video as well . we design a specific spatially-indexed attention mechanism among the convolutional layers to extract the salient facial regions in each individual image , and a temporal attention layer to assign attention weights to each frame . this two-pronged approach not only improves the performance by allowing the model to focus on informative frames and facial areas , but it also offers an interpretable correspondence between the spatial facial regions as well as temporal frames , and the task of age estimation . we demonstrate the strong performance of our model in experiments on a large , gender-balanced database with 400 subjects with ages spanning from 8 to 76 years . experiments reveal that our model exhibits significant superiority over the state-of-the-art methods given sufficient training data .", "topics": ["test set", "end-to-end principle"]}
{"title": "clustering with side information : from a probabilistic model to a deterministic algorithm", "abstract": "in this paper , we propose a model-based clustering method ( tvclust ) that robustly incorporates noisy side information as soft-constraints and aims to seek a consensus between side information and the observed data . our method is based on a nonparametric bayesian hierarchical model that combines the probabilistic model for the data instance and the one for the side-information . an efficient gibbs sampling algorithm is proposed for posterior inference . using the small-variance asymptotics of our probabilistic model , we then derive a new deterministic clustering algorithm ( rdp-means ) . it can be viewed as an extension of k-means that allows for the inclusion of side information and has the additional property that the number of clusters does not need to be specified a priori . empirical studies have been carried out to compare our work with many constrained clustering algorithms from the literature on both a variety of data sets and under a variety of conditions such as using noisy side information and erroneous k values . the results of our experiments show strong results for our probabilistic and deterministic approaches under these conditions when compared to other algorithms in the literature .", "topics": ["cluster analysis"]}
{"title": "planning with information-processing constraints and model uncertainty in markov decision processes", "abstract": "information-theoretic principles for learning and acting have been proposed to solve particular classes of markov decision problems . mathematically , such approaches are governed by a variational free energy principle and allow solving mdp planning problems with information-processing constraints expressed in terms of a kullback-leibler divergence with respect to a reference distribution . here we consider a generalization of such mdp planners by taking model uncertainty into account . as model uncertainty can also be formalized as an information-processing constraint , we can derive a unified solution from a single generalized variational principle . we provide a generalized value iteration scheme together with a convergence proof . as limit cases , this generalized scheme includes standard value iteration with a known model , bayesian mdp planning , and robust planning . we demonstrate the benefits of this approach in a grid world simulation .", "topics": ["calculus of variations", "simulation"]}
{"title": "attention on attention : architectures for visual question answering ( vqa )", "abstract": "visual question answering ( vqa ) is an increasingly popular topic in deep learning research , requiring coordination of natural language processing and computer vision modules into a single architecture . we build upon the model which placed first in the vqa challenge by developing thirteen new attention mechanisms and introducing a simplified classifier . we performed 300 gpu hours of extensive hyperparameter and architecture searches and were able to achieve an evaluation score of 64.78 % , outperforming the existing state-of-the-art single model 's validation score of 63.15 % .", "topics": ["natural language processing", "computer vision"]}
{"title": "automatic optimization of hardware accelerators for image processing", "abstract": "in the domain of image processing , often real-time constraints are required . in particular , in safety-critical applications , such as x-ray computed tomography in medical imaging or advanced driver assistance systems in the automotive domain , timing is of utmost importance . a common approach to maintain real-time capabilities of compute-intensive applications is to offload those computations to dedicated accelerator hardware , such as field programmable gate arrays ( fpgas ) . programming such architectures is a challenging task , with respect to the typical fpga-specific design criteria : achievable overall algorithm latency and resource usage of fpga primitives ( bram , ff , lut , and dsp ) . high-level synthesis ( hls ) dramatically simplifies this task by enabling the description of algorithms in well-known higher languages ( c/c++ ) and its automatic synthesis that can be accomplished by hls tools . however , algorithm developers still need expert knowledge about the target architecture , in order to achieve satisfying results . therefore , in previous work , we have shown that elevating the description of image algorithms to an even higher abstraction level , by using a domain-specific language ( dsl ) , can significantly cut down the complexity for designing such algorithms for fpgas . to give the developer even more control over the common trade-off , latency vs. resource usage , we will present an automatic optimization process where these criteria are analyzed and fed back to the dsl compiler , in order to generate code that is closer to the desired design specifications . finally , we generate code for stereo block matching algorithms and compare it with handwritten implementations to quantify the quality of our results .", "topics": ["image processing", "mathematical optimization"]}
{"title": "deep learning for identifying metastatic breast cancer", "abstract": "the international symposium on biomedical imaging ( isbi ) held a grand challenge to evaluate computational systems for the automated detection of metastatic breast cancer in whole slide images of sentinel lymph node biopsies . our team won both competitions in the grand challenge , obtaining an area under the receiver operating curve ( auc ) of 0.925 for the task of whole slide image classification and a score of 0.7051 for the tumor localization task . a pathologist independently reviewed the same images , obtaining a whole slide image classification auc of 0.966 and a tumor localization score of 0.733 . combining our deep learning system 's predictions with the human pathologist 's diagnoses increased the pathologist 's auc to 0.995 , representing an approximately 85 percent reduction in human error rate . these results demonstrate the power of using deep learning to produce significant improvements in the accuracy of pathological diagnoses .", "topics": ["computer vision"]}
{"title": "initialization and coordinate optimization for multi-way matching", "abstract": "we consider the problem of consistently matching multiple sets of elements to each other , which is a common task in fields such as computer vision . to solve the underlying np-hard objective , existing methods often relax or approximate it , but end up with unsatisfying empirical performance due to a misaligned objective . we propose a coordinate update algorithm that directly optimizes the target objective . by using pairwise alignment information to build an undirected graph and initializing the permutation matrices along the edges of its maximum spanning tree , our algorithm successfully avoids bad local optima . theoretically , with high probability our algorithm guarantees an optimal solution under reasonable noise assumptions . empirically , our algorithm consistently and significantly outperforms existing methods on several benchmark tasks on real datasets .", "topics": ["approximation algorithm", "optimization problem"]}
{"title": "stochastic dual coordinate descent with bandit sampling", "abstract": "coordinate descent methods minimize a cost function by updating a single decision variable ( corresponding to one coordinate ) at a time . ideally , one would update the decision variable that yields the largest marginal decrease in the cost function . however , finding this coordinate would require checking all of them , which is not computationally practical . we instead propose a new adaptive method for coordinate descent . first , we define a lower bound on the decrease of the cost function when a coordinate is updated and , instead of calculating this lower bound for all coordinates , we use a multi-armed bandit algorithm to learn which coordinates result in the largest marginal decrease while simultaneously performing coordinate descent . we show that our approach improves the convergence of the coordinate methods ( including parallel versions ) both theoretically and experimentally .", "topics": ["loss function"]}
{"title": "reinforcement learning of pomdps using spectral methods", "abstract": "we propose a new reinforcement learning algorithm for partially observable markov decision processes ( pomdp ) based on spectral decomposition methods . while spectral methods have been previously employed for consistent learning of ( passive ) latent variable models such as hidden markov models , pomdps are more challenging since the learner interacts with the environment and possibly changes the future observations in the process . we devise a learning algorithm running through episodes , in each episode we employ spectral techniques to learn the pomdp parameters from a trajectory generated by a fixed policy . at the end of the episode , an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated pomdp model . we prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces .", "topics": ["regret ( decision theory )", "reinforcement learning"]}
{"title": "curriculum dropout", "abstract": "dropout is a very effective way of regularizing neural networks . stochastically `` dropping out '' units with a certain probability discourages over-specific co-adaptations of feature detectors , preventing overfitting and improving network generalization . besides , dropout can be interpreted as an approximate model aggregation technique , where an exponential number of smaller networks are averaged in order to get a more powerful ensemble . in this paper , we show that using a fixed dropout probability during training is a suboptimal choice . we thus propose a time scheduling for the probability of retaining neurons in the network . this induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem . this idea of `` starting easy '' and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models . indeed , we prove that our optimization strategy implements a very general curriculum scheme , by gradually adding noise to both the input and intermediate feature representations within the network architecture . experiments on seven image classification datasets and different network architectures show that our method , named curriculum dropout , frequently yields to better generalization and , at worst , performs just as well as the standard dropout method .", "topics": ["optimization problem", "time complexity"]}
{"title": "a deterministic global optimization method for variational inference", "abstract": "variational inference methods for latent variable statistical models have gained popularity because they are relatively fast , can handle large data sets , and have deterministic convergence guarantees . however , in practice it is unclear whether the fixed point identified by the variational inference algorithm is a local or a global optimum . here , we propose a method for constructing iterative optimization algorithms for variational inference problems that are guaranteed to converge to the $ \\epsilon $ -global variational lower bound on the log-likelihood . we derive inference algorithms for two variational approximations to a standard bayesian gaussian mixture model ( bgmm ) . we present a minimal data set for empirically testing convergence and show that a variational inference algorithm frequently converges to a local optimum while our algorithm always converges to the globally optimal variational lower bound . we characterize the loss incurred by choosing a non-optimal variational approximation distribution suggesting that selection of the approximating variational distribution deserves as much attention as the selection of the original statistical model for a given data set .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "unifying value iteration , advantage learning , and dynamic policy programming", "abstract": "approximate dynamic programming algorithms , such as approximate value iteration , have been successfully applied to many complex reinforcement learning tasks , and a better approximate dynamic programming algorithm is expected to further extend the applicability of reinforcement learning to various tasks . in this paper we propose a new , robust dynamic programming algorithm that unifies value iteration , advantage learning , and dynamic policy programming . we call it generalized value iteration ( gvi ) and its approximated version , approximate gvi ( agvi ) . we show agvi 's performance guarantee , which includes performance guarantees for existing algorithms , as special cases . we discuss theoretical weaknesses of existing algorithms , and explain the advantages of agvi . numerical experiments in a simple environment support theoretical arguments , and suggest that agvi is a promising alternative to previous algorithms .", "topics": ["approximation algorithm", "numerical analysis"]}
{"title": "texture descriptor combining fractal dimension and artificial crawlers", "abstract": "texture is an important visual attribute used to describe images . there are many methods available for texture analysis . however , they do not capture the details richness of the image surface . in this paper , we propose a new method to describe textures using the artificial crawler model . this model assumes that each agent can interact with the environment and each other . since this swarm system alone does not achieve a good discrimination , we developed a new method to increase the discriminatory power of artificial crawlers , together with the fractal dimension theory . here , we estimated the fractal dimension by the bouligand-minkowski method due to its precision in quantifying structural properties of images . we validate our method on two texture datasets and the experimental results reveal that our method leads to highly discriminative textural features . the results indicate that our method can be used in different texture applications .", "topics": ["artificial intelligence"]}
{"title": "analysis of gradient descent methods with non-diminishing , bounded errors", "abstract": "the main aim of this paper is to provide an analysis of gradient descent ( gd ) algorithms with gradient errors that do not necessarily vanish , asymptotically . in particular , sufficient conditions are presented for both stability ( almost sure boundedness of the iterates ) and convergence of gd with bounded , ( possibly ) non-diminishing gradient errors . in addition to ensuring stability , such an algorithm is shown to converge to a small neighborhood of the minimum set , which depends on the gradient errors . it is worth noting that the main result of this paper can be used to show that gd with asymptotically vanishing errors indeed converges to the minimum set . the results presented herein are not only more general when compared to previous results , but our analysis of gd with errors is new to the literature to the best of our knowledge . our work extends the contributions of mangasarian & solodov , bertsekas & tsitsiklis and tadic & doucet . using our framework , a simple yet effective implementation of gd using simultaneous perturbation stochastic approximations ( sp sa ) , with constant sensitivity parameters , is presented . another important improvement over many previous results is that there are no `additional ' restrictions imposed on the step-sizes . in machine learning applications where step-sizes are related to learning rates , our assumptions , unlike those of other papers , do not affect these learning rates . finally , we present experimental results to validate our theory .", "topics": ["approximation algorithm", "loss function"]}
{"title": "latent variable discovery using dependency patterns", "abstract": "the causal discovery of bayesian networks is an active and important research area , and it is based upon searching the space of causal models for those which can best explain a pattern of probabilistic dependencies shown in the data . however , some of those dependencies are generated by causal structures involving variables which have not been measured , i.e . , latent variables . some such patterns of dependency `` reveal '' themselves , in that no model based solely upon the observed variables can explain them as well as a model using a latent variable . that is what latent variable discovery is based upon . here we did a search for finding them systematically , so that they may be applied in latent variable discovery in a more rigorous fashion .", "topics": ["bayesian network", "causality"]}
{"title": "learning deep representation of multityped objects and tasks", "abstract": "we introduce a deep multitask architecture to integrate multityped representations of multimodal objects . this multitype exposition is less abstract than the multimodal characterization , but more machine-friendly , and thus is more precise to model . for example , an image can be described by multiple visual views , which can be in the forms of bag-of-words ( counts ) or color/texture histograms ( real-valued ) . at the same time , the image may have several social tags , which are best described using a sparse binary vector . our deep model takes as input multiple type-specific features , narrows the cross-modality semantic gaps , learns cross-type correlation , and produces a high-level homogeneous representation . at the same time , the model supports heterogeneously typed tasks . we demonstrate the capacity of the model on two applications : social image retrieval and multiple concept prediction . the deep architecture produces more compact representation , naturally integrates multiviews and multimodalities , exploits better side information , and most importantly , performs competitively against baselines .", "topics": ["high- and low-level", "sparse matrix"]}
{"title": "clustering with feature selection using alternating minimization , application to computational biology", "abstract": "this paper deals with unsupervised clustering with feature selection . the problem is to estimate both labels and a sparse projection matrix of weights . to address this combinatorial non-convex problem maintaining a strict control on the sparsity of the matrix of weights , we propose an alternating minimization of the frobenius norm criterion . we provide a new efficient algorithm named k-sparse which alternates k-means with projection-gradient minimization . the projection-gradient step is a method of splitting type , with exact projection on the $ \\ell^1 $ ball to promote sparsity . the convergence of the gradient-projection step is addressed , and a preliminary analysis of the alternating minimization is made . the frobenius norm criterion converges as the number of iterates in algorithm k-sparse goes to infinity . experiments on single cell rna sequencing datasets show that our method significantly improves the results of pca k-means , spectral clustering , simlr , and sparcl methods , and achieves a relevant selection of genes . the complexity of k-sparse is linear in the number of samples ( cells ) , so that the method scales up to large datasets .", "topics": ["cluster analysis", "unsupervised learning"]}
{"title": "robust logitboost and adaptive base class ( abc ) logitboost", "abstract": "logitboost is an influential boosting algorithm for classification . in this paper , we develop robust logitboost to provide an explicit formulation of tree-split criterion for building weak learners ( regression trees ) for logitboost . this formulation leads to a numerically stable implementation of logitboost . we then propose abc-logitboost for multi-class classification , by combining robust logitboost with the prior work of abc-boost . previously , abc-boost was implemented as abc-mart using the mart algorithm . our extensive experiments on multi-class classification compare four algorithms : mart , abcmart , ( robust ) logitboost , and abc-logitboost , and demonstrate the superiority of abc-logitboost . comparisons with other learning methods including svm and deep learning are also available through prior publications .", "topics": ["support vector machine", "eisenstein 's criterion"]}
{"title": "approximating the partition function by deleting and then correcting for model edges", "abstract": "we propose an approach for approximating the partition function which is based on two steps : ( 1 ) computing the partition function of a simplified model which is obtained by deleting model edges , and ( 2 ) rectifying the result by applying an edge-by-edge correction . the approach leads to an intuitive framework in which one can trade-off the quality of an approximation with the complexity of computing it . it also includes the bethe free energy approximation as a degenerate case . we develop the approach theoretically in this paper and provide a number of empirical results that reveal its practical utility .", "topics": ["approximation algorithm"]}
{"title": "invariants of multidimensional time series based on their iterated-integral signature", "abstract": "we introduce a novel class of features for multidimensional time series , that are invariant with respect to transformations of the ambient space . the general linear group , the group of rotations and the group of permutations of the axes are considered . the starting point for their construction is chen 's iterated-integral signature .", "topics": ["time series"]}
{"title": "it was the training data pruning too !", "abstract": "we study the current best model ( kdg ) for question answering on tabular data evaluated over the wikitablequestions dataset . previous ablation studies performed against this model attributed the model 's performance to certain aspects of its architecture . in this paper , we find that the model 's performance also crucially depends on a certain pruning of the data used to train the model . disabling the pruning step drops the accuracy of the model from 43.3 % to 36.3 % . the large impact on the performance of the kdg model suggests that the pruning may be a useful pre-processing step in training other semantic parsers as well .", "topics": ["test set", "parsing"]}
{"title": "cooperative training of descriptor and generator networks", "abstract": "this paper studies the cooperative training of two probabilistic models of signals such as images . both models are parametrized by convolutional neural networks ( convnets ) . the first network is a descriptor network , which is an exponential family model or an energy-based model , whose feature statistics or energy function are defined by a bottom-up convnet , which maps the observed signal to the feature statistics . the second network is a generator network , which is a non-linear version of factor analysis . it is defined by a top-down convnet , which maps the latent factors to the observed signal . the maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation , and both algorithms involve langevin sampling . we observe that the two training algorithms can cooperate with each other by jumpstarting each other 's langevin sampling , and they can be naturally and seamlessly interwoven into a coopnets algorithm that can train both nets simultaneously .", "topics": ["sampling ( signal processing )", "mathematical optimization"]}
{"title": "computing by means of physics-based optical neural networks", "abstract": "we report recent research on computing with biology-based neural network models by means of physics-based opto-electronic hardware . new technology provides opportunities for very-high-speed computation and uncovers problems obstructing the wide-spread use of this new capability . the computation modeling community may be able to offer solutions to these cross-boundary research problems .", "topics": ["neural networks", "computation"]}
{"title": "structured illumination microscopy with unknown patterns and a statistical prior", "abstract": "structured illumination microscopy ( sim ) improves resolution by down-modulating high-frequency information of an object to fit within the passband of the optical system . generally , the reconstruction process requires prior knowledge of the illumination patterns , which implies a well-calibrated and aberration-free system . here , we propose a new \\textit { algorithmic self-calibration } strategy for sim that does not need to know the exact patterns { \\it a priori } , but only their covariance . the algorithm , termed pe-sims , includes a pattern-estimation ( pe ) step requiring the uniformity of the sum of the illumination patterns and a sim reconstruction procedure using a statistical prior ( sims ) . additionally , we perform a pixel reassignment process ( sims-pr ) to enhance the reconstruction quality . we achieve 2 $ \\times $ better resolution than a conventional widefield microscope , while remaining insensitive to aberration-induced pattern distortion and robust against parameter tuning .", "topics": ["pixel"]}
{"title": "learning the structure of dynamic probabilistic networks", "abstract": "dynamic probabilistic networks are a compact representation of complex stochastic processes . in this paper we examine how to learn the structure of a dpn from data . we extend structure scoring rules for standard probabilistic networks to the dynamic case , and show how to search for structure when some of the variables are hidden . finally , we examine two applications where such a technology might be useful : predicting and classifying dynamic behaviors , and learning causal orderings in biological processes . we provide empirical results that demonstrate the applicability of our methods in both domains .", "topics": ["causality"]}
{"title": "less is more : a comprehensive framework for the number of components of ensemble classifiers", "abstract": "the number of component classifiers chosen for an ensemble has a great impact on its prediction ability . in this paper , we use a geometric framework for a priori determining the ensemble size , applicable to most of the existing batch and online ensemble classifiers . there are only a limited number of studies on the ensemble size considering majority voting ( mv ) and weighted majority voting ( wmv ) . almost all of them are designed for batch-mode , barely addressing online environments . the big data dimensions and resource limitations in terms of time and memory make the determination of the ensemble size crucial , especially for online environments . our framework proves , for the mv aggregation rule , that the more strong components we can add to the ensemble the more accurate predictions we can achieve . on the other hand , for the wmv aggregation rule , we prove the existence of an ideal number of components equal to the number of class labels , with the premise that components are completely independent of each other and strong enough . while giving the exact definition for a strong and independent classifier in the context of an ensemble is a challenging task , our proposed geometric framework provides a theoretical explanation of diversity and its impact on the accuracy of predictions . we conduct an experimental evaluation with two different scenarios to show the practical value of our theorems .", "topics": ["statistical classification"]}
{"title": "bayesian compression for deep learning", "abstract": "compression and computational efficiency in deep learning have become a problem of great significance . in this work , we argue that the most principled and effective way to attack this problem is by adopting a bayesian point of view , where through sparsity inducing priors we prune large parts of the network . we introduce two novelties in this paper : 1 ) we use hierarchical priors to prune nodes instead of individual weights , and 2 ) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights . both factors significantly contribute to achieving the state of the art in terms of compression rates , while still staying competitive with methods designed to optimize for speed or energy efficiency .", "topics": ["sparse matrix"]}
{"title": "on the existence of kernel function for kernel-trick of k-means", "abstract": "this paper corrects the proof of the theorem 2 from the gower 's paper \\cite [ page 5 ] { gower:1982 } . the correction is needed in order to establish the existence of the kernel function used commonly in the kernel trick e.g . for $ k $ -means clustering algorithm , on the grounds of distance matrix . the scope of correction is explained in section 2 .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "clustering with t-sne , provably", "abstract": "t-distributed stochastic neighborhood embedding ( t-sne ) , a clustering and visualization method proposed by van der maaten & hinton in 2008 , has rapidly become a standard tool in a number of natural sciences . despite its overwhelming success , there is a distinct lack of mathematical foundations and the inner workings of the algorithm are not well understood . the purpose of this paper is to prove that t-sne is able to recover well-separated clusters ; more precisely , we prove that t-sne in the `early exaggeration ' phase , an optimization technique proposed by van der maaten & hinton ( 2008 ) and van der maaten ( 2014 ) , can be rigorously analyzed . as a byproduct , the proof suggests novel ways for setting the exaggeration parameter $ \\alpha $ and step size $ h $ . numerical examples illustrate the effectiveness of these rules : in particular , the quality of embedding of topological structures ( e.g . the swiss roll ) improves . we also discuss a connection to spectral clustering methods .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "enhanced random forest with image/patch-level learning for image understanding", "abstract": "image understanding is an important research domain in the computer vision due to its wide real-world applications . for an image understanding framework that uses the bag-of-words model representation , the visual codebook is an essential part . random forest ( rf ) as a tree-structure discriminative codebook has been a popular choice . however , the performance of the rf can be degraded if the local patch labels are poorly assigned . in this paper , we tackle this problem by a novel way to update the rf codebook learning for a more discriminative codebook with the introduction of the soft class labels , estimated from the plsa model based on a feedback scheme . the feedback scheme is performed on both the image and patch levels respectively , which is in contrast to the state- of-the-art rf codebook learning that focused on either image or patch level only . experiments on 15-scene and c-pascal datasets had shown the effectiveness of the proposed method in image understanding task .", "topics": ["computer vision"]}
{"title": "adaptive polar active contour for segmentation and tracking in ultrasound videos", "abstract": "detection of relative changes in circulating blood volume is important to guide resuscitation and manage a variety of medical conditions including sepsis , trauma , dialysis and congestive heart failure . recent studies have shown that estimates of circulating blood volume can be obtained from the cross-sectional area ( csa ) of the internal jugular vein ( ijv ) from ultrasound images . however , accurate segmentation and tracking of the ijv in ultrasound imaging is a challenging task and is significantly influenced by a number of parameters such as the image quality , shape , and temporal variation . in this paper , we propose a novel adaptive polar active contour ( ad-pac ) algorithm for the segmentation and tracking of the ijv in ultrasound videos . in the proposed algorithm , the parameters of the ad-pac algorithm are adapted based on the results of segmentation in previous frames . the ad-pac algorithm is applied to 65 ultrasound videos captured from 13 healthy subjects , with each video containing 450 frames . the results show that spatial and temporal adaptation of the energy function significantly improves segmentation performance when compared to current state-of-the-art active contour algorithms .", "topics": ["mathematical optimization"]}
{"title": "dac-h3 : a proactive robot cognitive architecture to acquire and express knowledge about the world and the self", "abstract": "this paper introduces a cognitive architecture for a humanoid robot to engage in a proactive , mixed-initiative exploration and manipulation of its environment , where the initiative can originate from both the human and the robot . the framework , based on a biologically-grounded theory of the brain and mind , integrates a reactive interaction engine , a number of state-of-the-art perceptual and motor learning algorithms , as well as planning abilities and an autobiographical memory . the architecture as a whole drives the robot behavior to solve the symbol grounding problem , acquire language capabilities , execute goal-oriented behavior , and express a verbal narrative of its own experience in the world . we validate our approach in human-robot interaction experiments with the icub humanoid robot , showing that the proposed cognitive architecture can be applied in real time within a realistic scenario and that it can be used with naive users .", "topics": ["robot"]}
{"title": "coloring black boxes : visualization of neural network decisions", "abstract": "neural networks are commonly regarded as black boxes performing incomprehensible functions . for classification problems networks provide maps from high dimensional feature space to k-dimensional image space . images of training vector are projected on polygon vertices , providing visualization of network function . such visualization may show the dynamics of learning , allow for comparison of different networks , display training vectors around which potential problems may arise , show differences due to regularization and optimization procedures , investigate stability of network classification under perturbation of original vectors , and place new data sample in relation to training data , allowing for estimation of confidence in classification of a given sample . an illustrative example for the three-class wine data and five-class satimage data is described . the visualization method proposed here is applicable to any black box system that provides continuous outputs .", "topics": ["test set", "feature vector"]}
{"title": "multi-objective optimization for self-adjusting weighted gradient in machine learning tasks", "abstract": "much of the focus in machine learning research is placed in creating new architectures and optimization methods , but the overall loss function is seldom questioned . this paper interprets machine learning from a multi-objective optimization perspective , showing the limitations of the default linear combination of loss functions over a data set and introducing the hypervolume indicator as an alternative . it is shown that the gradient of the hypervolume is defined by a self-adjusting weighted mean of the individual loss gradients , making it similar to the gradient of a weighted mean loss but without requiring the weights to be defined a priori . this enables an inner boosting-like behavior , where the current model is used to automatically place higher weights on samples with higher losses but without requiring the use of multiple models . results on a denoising autoencoder show that the new formulation is able to achieve better mean loss than the direct optimization of the mean loss , providing evidence to the conjecture that self-adjusting the weights creates a smoother loss surface .", "topics": ["mathematical optimization", "loss function"]}
{"title": "neural decision trees", "abstract": "in this paper we propose a synergistic melting of neural networks and decision trees ( dt ) we call neural decision trees ( ndt ) . ndt is an architecture a la decision tree where each splitting node is an independent multilayer perceptron allowing oblique decision functions or arbritrary nonlinear decision function if more than one layer is used . this way , each mlp can be seen as a node of the tree . we then show that with the weight sharing asumption among those units , we end up with a hashing neural network ( hnn ) which is a multilayer perceptron with sigmoid activation function for the last layer as opposed to the standard softmax . the output units then jointly represent the probability to be in a particular region . the proposed framework allows for global optimization as opposed to greedy in dt and differentiability w.r.t . all parameters and the input , allowing easy integration in any learnable pipeline , for example after cnns for computer vision tasks . we also demonstrate the modeling power of hnn allowing to learn union of disjoint regions for final clustering or classification making it more general and powerful than standard softmax mlp requiring linear separability thus reducing the need on the inner layer to perform complex data transformations . we finally show experiments for supervised , semi-suppervised and unsupervised tasks and compare results with standard dts and mlps .", "topics": ["cluster analysis", "nonlinear system"]}
{"title": "imitation learning with concurrent actions in 3d games", "abstract": "in this work we describe a novel deep reinforcement learning neural network architecture that allows multiple actions to be selected at every time-step . multi-action policies allows complex behaviors to be learnt that are otherwise hard to achieve when using single action selection techniques . this work describes an algorithm that uses both imitation learning ( il ) and temporal difference ( td ) reinforcement learning ( rl ) to provide a 4x improvement in training time and 2.5x improvement in performance over single action selection td rl . we demonstrate the capabilities of this network using a complex in-house 3d game . mimicking the behavior of the expert teacher significantly improves world state exploration and allows the agents vision system to be trained more rapidly than td rl alone . this initial training technique kick-starts td learning and the agent quickly learns to surpass the capabilities of the expert .", "topics": ["reinforcement learning"]}
{"title": "ontology verbalization using semantic-refinement", "abstract": "we propose a rule-based technique to generate redundancy-free nl descriptions of owl entities.the existing approaches which address the problem of verbalizing owl ontologies generate nl text segments which are close to their counterpart owl statements.some of these approaches also perform grouping and aggregating of these nl text segments to generate a more fluent and comprehensive form of the content.restricting our attention to description of individuals and concepts , we find that the approach currently followed in the available tools is that of determining the set of all logical conditions that are satisfied by the given individual/concept name and translate these conditions verbatim into corresponding nl descriptions.human-understandability of such descriptions is affected by the presence of repetitions and redundancies , as they have high fidelity to their owl representation.in the literature , no efforts had been taken to remove redundancies and repetitions at the logical-level before generating the nl descriptions of entities and we find this to be the main reason for lack of readability of the generated text.herein , we propose a technique called semantic-refinement ( sr ) to generate meaningful and easily-understandable descriptions of individuals and concepts of a given owlontology.we identify the combinations of owl/dl constructs that lead to repetitive/redundant descriptions and propose a series of refinement rules to rewrite the conditions that are satisfied by an individual/concept in a meaning-preserving manner.the reduced set of conditions are then employed for generating nl descriptions.our experiments show that , sr leads to significantly improved descriptions of ontology entities.we also test the effectiveness and usefulness of the the generated descriptions for the purpose of validating the ontologies and find that the proposed technique is indeed helpful in the context .", "topics": ["natural language", "entity"]}
{"title": "combinatorial cascading bandits", "abstract": "we propose combinatorial cascading bandits , a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one . the weights of the items are binary , stochastic , and drawn independently of each other . the agent observes the index of the first chosen item whose weight is zero . this observation model arises in network routing , for instance , where the learning agent may only observe the first link in the routing path which is down , and blocks the path . we propose a ucb-like algorithm for solving our problems , combcascade ; and prove gap-dependent and gap-free upper bounds on its $ n $ -step regret . our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting , a non-linear reward function and partial observability . we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated . we also demonstrate that our setting requires a new learning algorithm .", "topics": ["regret ( decision theory )", "nonlinear system"]}
{"title": "improving lstm-based video description with linguistic knowledge mined from text", "abstract": "this paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos . specifically , we integrate both a neural language model and distributional semantics trained on large text corpora into a recent lstm-based architecture for video description . we evaluate our approach on a collection of youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality .", "topics": ["natural language", "text corpus"]}
{"title": "guiding reinforcement learning exploration using natural language", "abstract": "in this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments . this technique uses neural machine translation , specifically the use of encoder-decoder networks , to learn associations between natural language behavior descriptions and state-action information . we then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments . we evaluate this technique using the popular arcade game , frogger , under ideal and non-ideal conditions . this evaluation shows that our modified policy shaping algorithm improves over a q-learning agent as well as a baseline version of policy shaping .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "interpreting the predictions of complex ml models by layer-wise relevance propagation", "abstract": "complex nonlinear models such as deep neural network ( dnns ) have become an important tool for image classification , speech recognition , natural language processing , and many other fields of application . these models however lack transparency due to their complex nonlinear structure and to the complex data distributions to which they typically apply . as a result , it is difficult to fully characterize what makes these models reach a particular decision for a given input . this lack of transparency can be a drawback , especially in the context of sensitive applications such as medical analysis or security . in this short paper , we summarize a recent technique introduced by bach et al . [ 1 ] that explains predictions by decomposing the classification decision of dnn models in terms of input variables .", "topics": ["natural language processing", "nonlinear system"]}
{"title": "convolutional sparse coding with overlapping group norms", "abstract": "the most widely used form of convolutional sparse coding uses an $ \\ell_1 $ regularization term . while this approach has been successful in a variety of applications , a limitation of the $ \\ell_1 $ penalty is that it is homogeneous across the spatial and filter index dimensions of the sparse representation array , so that sparsity can not be separately controlled across these dimensions . the present paper considers the consequences of replacing the $ \\ell_1 $ penalty with a mixed group norm , motivated by recent theoretical results for convolutional sparse representations . algorithms are developed for solving the resulting problems , which are quite challenging , and the impact on the performance of the denoising problem is evaluated . the mixed group norms are found to perform very poorly in this application . while their performance is greatly improved by introducing a weighting strategy , such a strategy also improves the performance obtained from the much simpler and computationally cheaper $ \\ell_1 $ norm .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "a generalized loop correction method for approximate inference in graphical models", "abstract": "belief propagation ( bp ) is one of the most popular methods for inference in probabilistic graphical models . bp is guaranteed to return the correct answer for tree structures , but can be incorrect or non-convergent for loopy graphical models . recently , several new approximate inference algorithms based on cavity distribution have been proposed . these methods can account for the effect of loops by incorporating the dependency between bp messages . alternatively , region-based approximations ( that lead to methods such as generalized belief propagation ) improve upon bp by considering interactions within small clusters of variables , thus taking small loops within these clusters into account . this paper introduces an approach , generalized loop correction ( glc ) , that benefits from both of these types of loop correction . we show how glc relates to these two families of inference methods , then provide empirical evidence that glc works effectively in general , and can be significantly more accurate than both correction schemes .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "on structured prediction theory with calibrated convex surrogate losses", "abstract": "we provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees . for any task loss , we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called `` calibration function '' relating the excess surrogate risk to the actual risk . in contrast to prior related work , we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity . as an interesting consequence , we formalize the intuition that some task losses make learning harder than others , and that the classical 0-1 loss is ill-suited for general structured prediction .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "smoothed gradients for stochastic variational inference", "abstract": "stochastic variational inference ( svi ) lets us scale up bayesian computation to massive data . it uses stochastic optimization to fit a variational distribution , following easy-to-compute noisy natural gradients . as with most traditional stochastic optimization methods , svi takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients . in this paper , we explore the idea of following biased stochastic gradients in svi . our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms . we will demonstrate the many advantages of this technique . first , its computational cost is the same as for svi and storage requirements only multiply by a constant factor . second , it enjoys significant variance reduction over the unbiased estimates , smaller bias than averaged gradients , and leads to smaller mean-squared error against the full gradient . we test our method on latent dirichlet allocation with three large corpora .", "topics": ["calculus of variations", "gradient"]}
{"title": "theanolm - an extensible toolkit for neural network language modeling", "abstract": "we present a new tool for training neural network language models ( nnlms ) , scoring sentences , and generating text . the tool has been written using python library theano , which allows researcher to easily extend it and tune any aspect of the training process . regardless of the flexibility , theano is able to generate extremely fast native code that can utilize a gpu or multiple cpu cores in order to parallelize the heavy numerical computations . the tool has been evaluated in difficult finnish and english conversational speech recognition tasks , and significant improvement was obtained over our best back-off n-gram models . the results that we obtained in the finnish task were compared to those from existing rnnlm and rwthlm toolkits , and found to be as good or better , while training times were an order of magnitude shorter .", "topics": ["numerical analysis", "speech recognition"]}
{"title": "efficiently creating 3d training data for fine hand pose estimation", "abstract": "while many recent hand pose estimation methods critically rely on a training set of labelled frames , the creation of such a dataset is a challenging task that has been overlooked so far . as a result , existing datasets are limited to a few sequences and individuals , with limited accuracy , and this prevents these methods from delivering their full potential . we propose a semi-automated method for efficiently and accurately labeling each frame of a hand depth video with the corresponding 3d locations of the joints : the user is asked to provide only an estimate of the 2d reprojections of the visible joints in some reference frames , which are automatically selected to minimize the labeling work by efficiently optimizing a sub-modular loss function . we then exploit spatial , temporal , and appearance constraints to retrieve the full 3d poses of the hand over the complete sequence . we show that this data can be used to train a recent state-of-the-art hand pose estimation method , leading to increased accuracy . the code and dataset can be found on our website https : //cvarlab.icg.tugraz.at/projects/hand_detection/", "topics": ["loss function"]}
{"title": "joint-vivo : selecting and weighting visual words jointly for bag-of-features based tissue classification in medical images", "abstract": "automatically classifying the tissues types of region of interest ( roi ) in medical imaging has been an important application in computer-aided diagnosis ( cad ) , such as classification of breast parenchymal tissue in the mammogram , classify lung disease patterns in high-resolution computed tomography ( hrct ) etc . recently , bag-of-features method has shown its power in this field , treating each roi as a set of local features . in this paper , we investigate using the bag-of-features strategy to classify the tissue types in medical imaging applications . two important issues are considered here : the visual vocabulary learning and weighting . although there are already plenty of algorithms to deal with them , all of them treat them independently , namely , the vocabulary learned first and then the histogram weighted . inspired by auto-context who learns the features and classifier jointly , we try to develop a novel algorithm that learns the vocabulary and weights jointly . the new algorithm , called joint-vivo , works in an iterative way . in each iteration , we first learn the weights for each visual word by maximizing the margin of roi triplets , and then select the most discriminate visual words based on the learned weights for the next iteration . we test our algorithm on three tissue classification tasks : identifying brain tissue type in magnetic resonance imaging ( mri ) , classifying lung tissue in hrct images , and classifying breast tissue density in mammograms . the results show that joint-vivo can perform effectively for classifying tissues .", "topics": ["iteration"]}
{"title": "can computers overcome humans ? consciousness interaction and its implications", "abstract": "can computers overcome human capabilities ? this is a paradoxical and controversial question , particularly because there are many hidden assumptions . this article focuses on that issue putting on evidence some misconception related with future generations of machines and the understanding of the brain . it will be discussed to what extent computers might reach human capabilities , and how it could be possible only if the computer is a conscious machine . however , it will be shown that if the computer is conscious , an interference process due to consciousness would affect the information processing of the system . therefore , it might be possible to make conscious machines to overcome human capabilities , which will have limitations as well as humans . in other words , trying to overcome human capabilities with computers implies the paradoxical conclusion that a computer will never overcome human capabilities at all , or if the computer does , it should not be considered as a computer anymore .", "topics": ["artificial intelligence"]}
{"title": "an immune inspired network intrusion detection system utilising correlation context", "abstract": "network intrusion detection systems ( nids ) are computer systems which monitor a network with the aim of discerning malicious from benign activity on that network . while a wide range of approaches have met varying levels of success , most idss rely on having access to a database of known attack signatures which are written by security experts . nowadays , in order to solve problems with false positive alerts , correlation algorithms are used to add additional structure to sequences of ids alerts . however , such techniques are of no help in discovering novel attacks or variations of known attacks , something the human immune system ( his ) is capable of doing in its own specialised domain . this paper presents a novel immune algorithm for application to the ids problem . the goal is to discover packets containing novel variations of attacks covered by an existing signature base .", "topics": ["database"]}
{"title": "active contextual entropy search", "abstract": "contextual policy search allows adapting robotic movement primitives to different situations . for instance , a locomotion primitive might be adapted to different terrain inclinations or desired walking speeds . such an adaptation is often achievable by modifying a small number of hyperparameters . however , learning , when performed on real robotic systems , is typically restricted to a small number of trials . bayesian optimization has recently been proposed as a sample-efficient means for contextual policy search that is well suited under these conditions . in this work , we extend entropy search , a variant of bayesian optimization , such that it can be used for active contextual policy search where the agent selects those tasks during training in which it expects to learn the most . empirical results in simulation suggest that this allows learning successful behavior with less trials .", "topics": ["simulation", "robot"]}
{"title": "a novel method for speech segmentation based on speakers ' characteristics", "abstract": "speech segmentation is the process change point detection for partitioning an input audio stream into regions each of which corresponds to only one audio source or one speaker . one application of this system is in speaker diarization systems . there are several methods for speaker segmentation ; however , most of the speaker diarization systems use bic-based segmentation methods . the main goal of this paper is to propose a new method for speaker segmentation with higher speed than the current methods - e.g . bic - and acceptable accuracy . our proposed method is based on the pitch frequency of the speech . the accuracy of this method is similar to the accuracy of common speaker segmentation methods . however , its computation cost is much less than theirs . we show that our method is about 2.4 times faster than the bic-based method , while the average accuracy of pitch-based method is slightly higher than that of the bic-based method .", "topics": ["image segmentation", "computation"]}
{"title": "a convolutional autoencoder for multi-subject fmri data aggregation", "abstract": "finding the most effective way to aggregate multi-subject fmri data is a long-standing and challenging problem . it is of increasing interest in contemporary fmri studies of human cognition due to the scarcity of data per subject and the variability of brain anatomy and functional response across subjects . recent work on latent factor models shows promising results in this task but this approach does not preserve spatial locality in the brain . we examine two ways to combine the ideas of a factor model and a searchlight based analysis to aggregate multi-subject fmri data while preserving spatial locality . we first do this directly by combining a recent factor method known as a shared response model with searchlight analysis . then we design a multi-view convolutional autoencoder for the same task . both approaches preserve spatial locality and have competitive or better performance compared with standard searchlight analysis and the shared response model applied across the whole brain . we also report a system design to handle the computational challenge of training the convolutional autoencoder .", "topics": ["autoencoder"]}
{"title": "penalty constraints and kernelization of m-estimation based fuzzy c-means", "abstract": "a framework of m-estimation based fuzzy c-means clustering ( mfcm ) algorithm is proposed with iterative reweighted least squares ( irls ) algorithm , and penalty constraint and kernelization extensions of mfcm algorithms are also developed . introducing penalty information to the object functions of mfcm algorithms , the spatially constrained fuzzy c-means ( sfcm ) is extended to penalty constraints mfcm algorithms ( abbr . pmfcm ) .substituting the euclidean distance with kernel method , the mfcm and pmfcm algorithms are extended to kernelized mfcm ( abbr . kmfcm ) and kernelized pmfcm ( abbr.pkmfcm ) algorithms . the performances of mfcm , pmfcm , kmfcm and pkmfcm algorithms are evaluated in three tasks : pattern recognition on 10 standard data sets from uci machine learning databases , noise image segmentation performances on a synthetic image , a magnetic resonance brain image ( mri ) , and image segmentation of a standard images from berkeley segmentation dataset and benchmark . the experimental results demonstrate the effectiveness of our proposed algorithms in pattern recognition and image segmentation .", "topics": ["image segmentation", "cluster analysis"]}
{"title": "gpu asynchronous stochastic gradient descent to speed up neural network training", "abstract": "the ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision . these results have largely come from computational break throughs of two forms : model parallelism , e.g . gpu accelerated training , which has seen quick adoption in computer vision circles , and data parallelism , e.g . a-sgd , whose large scale has been used mostly in industry . we report early experiments with a system that makes use of both model parallelism and data parallelism , we call gpu a-sgd . we show using gpu a-sgd it is possible to speed up training of large convolutional neural networks useful for computer vision . we believe gpu a-sgd will make it possible to train larger networks on larger training sets in a reasonable amount of time .", "topics": ["computer vision", "gradient descent"]}
{"title": "citlab argus for historical data tables", "abstract": "we describe citlab 's recognition system for the anwresh-2014 competition attached to the 14. international conference on frontiers in handwriting recognition , icfhr 2014 . the task comprises word recognition from segmented historical documents . the core components of our system are based on multi-dimensional recurrent neural networks ( mdrnn ) and connectionist temporal classification ( ctc ) . the software modules behind that as well as the basic utility technologies are essentially powered by planet 's argus framework for intelligent text recognition and image processing .", "topics": ["image processing", "recurrent neural network"]}
{"title": "a nonclassical symbolic theory of working memory , mental computations , and mental set", "abstract": "the paper tackles four basic questions associated with human brain as a learning system . how can the brain learn to ( 1 ) mentally simulate different external memory aids , ( 2 ) perform , in principle , any mental computations using imaginary memory aids , ( 3 ) recall the real sensory and motor events and synthesize a combinatorial number of imaginary events , ( 4 ) dynamically change its mental set to match a combinatorial number of contexts ? we propose a uniform answer to ( 1 ) - ( 4 ) based on the general postulate that the human neocortex processes symbolic information in a `` nonclassical '' way . instead of manipulating symbols in a read/write memory , as the classical symbolic systems do , it manipulates the states of dynamical memory representing different temporary attributes of immovable symbolic structures stored in a long-term memory . the approach is formalized as the concept of e-machine . intuitively , an e-machine is a system that deals mainly with characteristic functions representing subsets of memory pointers rather than the pointers themselves . this nonclassical symbolic paradigm is turing universal , and , unlike the classical one , is efficiently implementable in homogeneous neural networks with temporal modulation topologically resembling that of the neocortex .", "topics": ["computation"]}
{"title": "paris-lille-3d : a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification", "abstract": "this paper introduces a new urban point cloud dataset for automatic segmentation and classification acquired by mobile laser scanning ( mls ) . we describe how the dataset is obtained from acquisition to post-processing and labeling . this dataset can be used to learn classification algorithm , however , given that a great attention has been paid to the split between the different objects , this dataset can also be used to learn the segmentation . the dataset consists of around 2km of mls point cloud acquired in two cities . the number of points and range of classes make us consider that it can be used to train deep-learning methods . besides we show some results of automatic segmentation and classification . the dataset is available at : http : //caor-mines-paristech.fr/fr/paris-lille-3d-dataset/", "topics": ["statistical classification", "ground truth"]}
{"title": "revisiting stochastic off-policy action-value gradients", "abstract": "off-policy stochastic actor-critic methods rely on approximating the stochastic policy gradient in order to derive an optimal policy . one may also derive the optimal policy by approximating the action-value gradient . the use of action-value gradients is desirable as policy improvement occurs along the direction of steepest ascent . this has been studied extensively within the context of natural gradient actor-critic algorithms and more recently within the context of deterministic policy gradients . in this paper we briefly discuss the off-policy stochastic counterpart to deterministic action-value gradients , as well as an incremental approach for following the policy gradient in lieu of the natural gradient .", "topics": ["approximation algorithm", "value ( ethics )"]}
{"title": "learning to prune deep neural networks via layer-wise optimal brain surgeon", "abstract": "how to develop slim and accurate deep neural networks has become crucial for real- world applications , especially for those employed in embedded systems . though previous work along this research line has shown some promising results , most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance . in this paper , we propose a new layer-wise pruning method for deep neural networks . in our proposed method , parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters . we prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer . therefore , there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance . we conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods .", "topics": ["baseline ( configuration management )", "neural networks"]}
{"title": "bounded planning in passive pomdps", "abstract": "in passive pomdps actions do not affect the world state , but still incur costs . when the agent is bounded by information-processing constraints , it can only keep an approximation of the belief . we present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost , and introduce an efficient and simple algorithm for finding an optimum .", "topics": ["mathematical optimization"]}
{"title": "improved dropout for shallow and deep learning", "abstract": "dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random . it has also received a surge of interest for shallow learning , e.g . , logistic regression . however , the independent sampling for dropout could be suboptimal for the sake of convergence . in this paper , we propose to use multinomial sampling for dropout , i.e . , sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons . to exhibit the optimal dropout probabilities , we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization . by minimizing a sampling dependent factor in the risk bound , we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution . to tackle the issue of evolving distribution of neurons in deep learning , we propose an efficient adaptive dropout ( named \\textbf { evolutional dropout } ) that computes the sampling probabilities on-the-fly from a mini-batch of examples . empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout . for example , on the cifar-100 data , the evolutional dropout achieves relative improvements over 10\\ % on the prediction performance and over 50\\ % on the convergence speed compared to the standard dropout .", "topics": ["sampling ( signal processing )"]}
{"title": "training effective node classifiers for cascade classification", "abstract": "cascade classifiers are widely used in real-time object detection . different from conventional classifiers that are designed for a low overall classification error rate , a classifier in each node of the cascade is required to achieve an extremely high detection rate and moderate false positive rate . although there are a few reported methods addressing this requirement in the context of object detection , there is no principled feature selection method that explicitly takes into account this asymmetric node learning objective . we provide such an algorithm here . we show that a special case of the biased minimax probability machine has the same formulation as the linear asymmetric classifier ( lac ) of wu et al ( 2005 ) . we then design a new boosting algorithm that directly optimizes the cost function of lac . the resulting totally-corrective boosting algorithm is implemented by the column generation technique in convex optimization . experimental results on object detection verify the effectiveness of the proposed boosting algorithm as a node classifier in cascade object detection , and show performance better than that of the current state-of-the-art .", "topics": ["object detection", "loss function"]}
{"title": "super-convergence : very fast training of residual networks using large learning rates", "abstract": "in this paper , we show a phenomenon , which we named `` super-convergence '' , where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods . the existence of super-convergence is relevant to understanding why deep networks generalize well . one of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate . furthermore , we present evidence that training with large learning rates improves performance by regularizing the network . in addition , we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited . we also derive a simplification of the hessian free optimization method to compute an estimate of the optimal learning rate . the architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence .", "topics": ["test set", "iteration"]}
{"title": "greedy algorithms for sparse reinforcement learning", "abstract": "feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning ( rl ) community to expand the reach and applicability of rl . one approach to the problem of feature selection is to impose a sparsity-inducing form of regularization on the learning method . recent work on $ l_1 $ regularization has adapted techniques from the supervised learning literature for use with rl . another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features . such algorithms have many of the good properties of the $ l_1 $ regularization methods , while also being extremely efficient and , in some cases , allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data . this paper considers variants of orthogonal matching pursuit ( omp ) applied to reinforcement learning . the resulting algorithms are analyzed and compared experimentally with existing $ l_1 $ regularized approaches . we demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails ; however , one variant , omp-brm , provides promising theoretical guarantees under certain assumptions on the feature dictionary . another variant , omp-td , empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "online optimization for large-scale max-norm regularization", "abstract": "max-norm regularizer has been extensively studied in the last decade as it promotes an effective low-rank estimation for the underlying data . however , such max-norm regularized problems are typically formulated and solved in a batch manner , which prevents it from processing big data due to possible memory budget . in this paper , hence , we propose an online algorithm that is scalable to large-scale setting . particularly , we consider the matrix decomposition problem as an example , although a simple variant of the algorithm and analysis can be adapted to other important problems such as matrix completion . the crucial technique in our implementation is to reformulating the max-norm to an equivalent matrix factorization form , where the factors consist of a ( possibly overcomplete ) basis component and a coefficients one . in this way , we may maintain the basis component in the memory and optimize over it and the coefficients for each sample alternatively . since the memory footprint of the basis component is independent of the sample size , our algorithm is appealing when manipulating a large collection of samples . we prove that the sequence of the solutions ( i.e . , the basis component ) produced by our algorithm converges to a stationary point of the expected loss function asymptotically . numerical study demonstrates encouraging results for the efficacy and robustness of our algorithm compared to the widely used nuclear norm solvers .", "topics": ["matrix regularization", "scalability"]}
{"title": "comparing learning algorithms in neural network for diagnosing cardiovascular disease", "abstract": "today data mining techniques are exploited in medical science for diagnosing , overcoming and treating diseases . neural network is one of the techniques which are widely used for diagnosis in medical field . in this article efficiency of nine algorithms , which are basis of neural network learning in diagnosing cardiovascular diseases , will be assessed . algorithms are assessed in terms of accuracy , sensitivity , transparency , aroc and convergence rate by means of 10 fold cross validation . the results suggest that in training phase , lonberg-m algorithm has the best efficiency in terms of all metrics , algorithm oss has maximum accuracy in testing phase , algorithm scg has the maximum transparency and algorithm cgb has the maximum sensitivity .", "topics": ["data mining"]}
{"title": "risk-sensitive cooperative games for human-machine systems", "abstract": "autonomous systems can substantially enhance a human 's efficiency and effectiveness in complex environments . machines , however , are often unable to observe the preferences of the humans that they serve . despite the fact that the human 's and machine 's objectives are aligned , asymmetric information , along with heterogeneous sensitivities to risk by the human and machine , make their joint optimization process a game with strategic interactions . we propose a framework based on risk-sensitive dynamic games ; the human seeks to optimize her risk-sensitive criterion according to her true preferences , while the machine seeks to adaptively learn the human 's preferences and at the same time provide a good service to the human . we develop a class of performance measures for the proposed framework based on the concept of regret . we then evaluate their dependence on the risk-sensitivity and the degree of uncertainty . we present applications of our framework to self-driving taxis , and robo-financial advising .", "topics": ["regret ( decision theory )", "interaction"]}
{"title": "game-theoretic modeling of driver and vehicle interactions for verification and validation of autonomous vehicle control systems", "abstract": "autonomous driving has been the subject of increased interest in recent years both in industry and in academia . serious efforts are being pursued to address legal , technical and logistical problems and make autonomous cars a viable option for everyday transportation . one significant challenge is the time and effort required for the verification and validation of the decision and control algorithms employed in these vehicles to ensure a safe and comfortable driving experience . hundreds of thousands of miles of driving tests are required to achieve a well calibrated control system that is capable of operating an autonomous vehicle in an uncertain traffic environment where multiple interactions between vehicles and drivers simultaneously occur . traffic simulators where these interactions can be modeled and represented with reasonable fidelity can help decrease the time and effort necessary for the development of the autonomous driving control algorithms by providing a venue where acceptable initial control calibrations can be achieved quickly and safely before actual road tests . in this paper , we present a game theoretic traffic model that can be used to 1 ) test and compare various autonomous vehicle decision and control systems and 2 ) calibrate the parameters of an existing control system . we demonstrate two example case studies , where , in the first case , we test and quantitatively compare two autonomous vehicle control systems in terms of their safety and performance , and , in the second case , we optimize the parameters of an autonomous vehicle control system , utilizing the proposed traffic model and simulation environment .", "topics": ["simulation", "interaction"]}
{"title": "the forgettable-watcher model for video question answering", "abstract": "a number of visual question answering approaches have been proposed recently , aiming at understanding the visual scenes by answering the natural language questions . while the image question answering has drawn significant attention , video question answering is largely unexplored . video-qa is different from image-qa since the information and the events are scattered among multiple frames . in order to better utilize the temporal structure of the videos and the phrasal structures of the answers , we propose two mechanisms : the re-watching and the re-reading mechanisms and combine them into the forgettable-watcher model . then we propose a tgif-qa dataset for video question answering with the help of automatic question generation . finally , we evaluate the models on our dataset . the experimental results show the effectiveness of our proposed models .", "topics": ["natural language"]}
{"title": "learning purposeful behaviour in the absence of rewards", "abstract": "artificial intelligence is commonly defined as the ability to achieve goals in the world . in the reinforcement learning framework , goals are encoded as reward functions that guide agent behaviour , and the sum of observed rewards provide a notion of progress . however , some domains have no such reward signal , or have a reward signal so sparse as to appear absent . without reward feedback , agent behaviour is typically random , often dithering aimlessly and lacking intentionality . in this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards . the algorithm proceeds by constructing temporally extended actions ( options ) , through the identification of purposes that are `` just out of reach '' of the agent 's current behaviour . these purposes establish intrinsic goals for the agent to learn , ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space . moreover , the approach is particularly suited for settings where rewards are very sparse , and such behaviours can help in the exploration of the environment until reward is observed .", "topics": ["reinforcement learning", "sparse matrix"]}
{"title": "deep reinforcement learning using capsules in advanced game environments", "abstract": "reinforcement learning ( rl ) is a research area that has blossomed tremendously in recent years and has shown remarkable potential for artificial intelligence based opponents in computer games . this success is primarily due to vast capabilities of convolutional neural networks ( convnet ) , enabling algorithms to extract useful information from noisy environments . capsule network ( capsnet ) is a recent introduction to the deep learning algorithm group and has only barely begun to be explored . the network is an architecture for image classification , with superior performance for classification of the mnist dataset . capsnets have not been explored beyond image classification . this thesis introduces the use of capsnet for q-learning based game algorithms . to successfully apply capsnet in advanced game play , three main contributions follow . first , the introduction of four new game environments as frameworks for rl research with increasing complexity , namely flash rl , deep line wars , deep rts , and deep maze . these environments fill the gap between relatively simple and more complex game environments available for rl research and are in the thesis used to test and explore the capsnet behavior . second , the thesis introduces a generative modeling approach to produce artificial training data for use in deep learning models including capsnets . we empirically show that conditional generative modeling can successfully generate game data of sufficient quality to train a deep q-network well . third , we show that capsnet is a reliable architecture for deep q-learning based algorithms for game ai . a capsule is a group of neurons that determine the presence of objects in the data and is in the literature shown to increase the robustness of training and predictions while lowering the amount training data needed . it should , therefore , be ideally suited for game plays .", "topics": ["test set", "reinforcement learning"]}
{"title": "identifiability of causal graphs using functional models", "abstract": "this work addresses the following question : under what assumptions on the data generating process can one infer the causal graph from the joint distribution ? the approach taken by conditional independence-based causal discovery methods is based on two assumptions : the markov condition and faithfulness . it has been shown that under these assumptions the causal graph can be identified up to markov equivalence ( some arrows remain undirected ) using methods like the pc algorithm . in this work we propose an alternative by defining identifiable functional model classes ( ifmocs ) . as our main theorem we prove that if the data generating process belongs to an ifmoc , one can identify the complete causal graph . to the best of our knowledge this is the first identifiability result of this kind that is not limited to linear functional relationships . we discuss how the ifmoc assumption and the markov and faithfulness assumptions relate to each other and explain why we believe that the ifmoc assumption can be tested more easily on given data . we further provide a practical algorithm that recovers the causal graph from finitely many data ; experiments on simulated data support the theoretical findings .", "topics": ["simulation", "markov chain"]}
{"title": "ordered preference elicitation strategies for supporting multi-objective decision making", "abstract": "in multi-objective decision planning and learning , much attention is paid to producing optimal solution sets that contain an optimal policy for every possible user preference profile . we argue that the step that follows , i.e , determining which policy to execute by maximising the user 's intrinsic utility function over this ( possibly infinite ) set , is under-studied . this paper aims to fill this gap . we build on previous work on gaussian processes and pairwise comparisons for preference modelling , extend it to the multi-objective decision support scenario , and propose new ordered preference elicitation strategies based on ranking and clustering . our main contribution is an in-depth evaluation of these strategies using computer and human-based experiments . we show that our proposed elicitation strategies outperform the currently used pairwise methods , and found that users prefer ranking most . our experiments further show that utilising monotonicity information in gps by using a linear prior mean at the start and virtual comparisons to the nadir and ideal points , increases performance . we demonstrate our decision support framework in a real-world study on traffic regulation , conducted with the city of amsterdam .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "a parallel corpus of python functions and documentation strings for automated code documentation and code generation", "abstract": "automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest . progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions , which tend to be small and constrained to specific domains . in this work we introduce a large and diverse parallel corpus of a hundred thousands python functions with their documentation strings ( `` docstrings '' ) generated by scraping open source repositories on github . we describe baseline results for the code documentation and code generation tasks obtained by neural machine translation . we also experiment with data augmentation techniques to further increase the amount of training data . we release our datasets and processing scripts in order to stimulate research in these areas .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "partial light field tomographic reconstruction from a fixed-camera focal stack", "abstract": "this paper describes a novel approach to partially reconstruct high-resolution 4d light fields from a stack of differently focused photographs taken with a fixed camera . first , a focus map is calculated from this stack using a simple approach combining gradient detection and region expansion with graph-cut . then , this focus map is converted into a depth map thanks to the calibration of the camera . we proceed after this with the tomographic reconstruction of the epipolar images by back-projecting the focused regions of the scene only . we call it masked back-projection . the angles of back-projection are calculated from the depth map . thanks to the high angular resolution we achieve by suitably exploiting the image content captured over a large interval of focus distances , we are able to render puzzling perspective shifts although the original photographs were taken from a single fixed camera at a fixed position .", "topics": ["gradient"]}
{"title": "computational complexity of testing proportional justified representation", "abstract": "we consider a committee voting setting in which each voter approves of a subset of candidates and based on the approvals , a target number of candidates are selected . aziz et al . ( 2015 ) proposed two representation axioms called justified representation and extended justified representation . whereas the former can be tested as well as achieved in polynomial time , the latter property is conp-complete to test and no polynomial-time algorithm is known to achieve it . interestingly , s { \\'a } nchez-fern { \\'a } ndez et~al . ( 2016 ) proposed an intermediate property called proportional justified representation that admits a polynomial-time algorithm to achieve . the complexity of testing proportional justified representation has remained an open problem . in this paper , we settle the complexity by proving that testing proportional justified representation is conp-complete . we complement the complexity result by showing that the problem admits efficient algorithms if any of the following parameters are bounded : ( 1 ) number of voters ( 2 ) number of candidates ( 3 ) maximum number of candidates approved by a voter ( 4 ) maximum number of voters approving a given candidate .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "hierarchical semi-markov conditional random fields for recursive sequential data", "abstract": "inspired by the hierarchical hidden markov models ( hhmm ) , we present the hierarchical semi-markov conditional random field ( hscrf ) , a generalisation of embedded undirectedmarkov chains tomodel complex hierarchical , nestedmarkov processes . it is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference . importantly , we consider partiallysupervised learning and propose algorithms for generalised partially-supervised learning and constrained inference . we demonstrate the hscrf in two applications : ( i ) recognising human activities of daily living ( adls ) from indoor surveillance cameras , and ( ii ) noun-phrase chunking . we show that the hscrf is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases .", "topics": ["supervised learning", "time complexity"]}
{"title": "blankets joint posterior score for learning markov network structures", "abstract": "markov networks are extensively used to model complex sequential , spatial , and relational interactions in a wide range of fields . by learning the structure of independences of a domain , more accurate joint probability distributions can be obtained for inference tasks or , more directly , for interpreting the most significant relations among the variables . recently , several researchers have investigated techniques for automatically learning the structure from data by obtaining the probabilistic maximum-a-posteriori structure given the available data . however , all the approximations proposed decompose the posterior of the whole structure into local sub-problems , by assuming that the posteriors of the markov blankets of all the variables are mutually independent . in this work , we propose a scoring function for relaxing such assumption . the blankets joint posterior score computes the joint posterior of structures as a joint distribution of the collection of its markov blankets . essentially , the whole posterior is obtained by computing the posterior of the blanket of each variable as a conditional distribution that takes into account information from other blankets in the network . we show in our experimental results that the proposed approximation can improve the sample complexity of state-of-the-art scores when learning complex networks , where the independence assumption between blanket variables is clearly incorrect .", "topics": ["interaction", "approximation"]}
{"title": "segmentation of large images based on super-pixels and community detection in graphs", "abstract": "image segmentation has many applications which range from machine learning to medical diagnosis . in this paper , we propose a framework for the segmentation of images based on super-pixels and algorithms for community identification in graphs . the super-pixel pre-segmentation step reduces the number of nodes in the graph , rendering the method the ability to process large images . moreover , community detection algorithms provide more accurate segmentation than traditional approaches , such as those based on spectral graph partition . we also compare our method with two algorithms : a ) the graph-based approach by felzenszwalb and huttenlocher and b ) the contour-based method by arbelaez . results have shown that our method provides more precise segmentation and is faster than both of them .", "topics": ["image segmentation", "pixel"]}
{"title": "estimating confusions in the asr channel for improved topic-based language model adaptation", "abstract": "human language is a combination of elemental languages/domains/styles that change across and sometimes within discourses . language models , which play a crucial role in speech recognizers and machine translation systems , are particularly sensitive to such changes , unless some form of adaptation takes place . one approach to speech language model adaptation is self-training , in which a language model 's parameters are tuned based on automatically transcribed audio . however , transcription errors can misguide self-training , particularly in challenging settings such as conversational speech . in this work , we propose a model that considers the confusions ( errors ) of the asr channel . by modeling the likely confusions in the asr output instead of using just the 1-best , we improve self-training efficacy by obtaining a more reliable reference transcription estimate . we demonstrate improved topic-based language modeling adaptation results over both 1-best and lattice self-training using our asr channel confusion estimates on telephone conversations .", "topics": ["machine translation", "speech recognition"]}
{"title": "reconstructing self organizing maps as spider graphs for better visual interpretation of large unstructured datasets", "abstract": "self-organizing maps ( som ) are popular unsupervised artificial neural network used to reduce dimensions and visualize data . visual interpretation from self-organizing maps ( som ) has been limited due to grid approach of data representation , which makes inter-scenario analysis impossible . the paper proposes a new way to structure som . this model reconstructs som to show strength between variables as the threads of a cobweb and illuminate inter-scenario analysis . while radar graphs are very crude representation of spider web , this model uses more lively and realistic cobweb representation to take into account the difference in strength and length of threads . this model allows for visualization of highly unstructured dataset with large number of dimensions , common in bigdata sources .", "topics": ["unsupervised learning", "map"]}
{"title": "predicting customer churn : extreme gradient boosting with temporal data", "abstract": "accurately predicting customer churn using large scale time-series data is a common problem facing many business domains . the creation of model features across various time windows for training and testing can be particularly challenging due to temporal issues common to time-series data . in this paper , we will explore the application of extreme gradient boosting ( xgboost ) on a customer dataset with a wide-variety of temporal features in order to create a highly-accurate customer churn model . in particular , we describe an effective method for handling temporally sensitive feature engineering . the proposed model was submitted in the wsdm cup 2018 churn challenge and achieved first-place out of 575 teams .", "topics": ["time series", "gradient"]}
{"title": "comparing aggregators for relational probabilistic models", "abstract": "relational probabilistic models have the challenge of aggregation , where one variable depends on a population of other variables . consider the problem of predicting gender from movie ratings ; this is challenging because the number of movies per user and users per movie can vary greatly . surprisingly , aggregation is not well understood . in this paper , we show that existing relational models ( implicitly or explicitly ) either use simple numerical aggregators that lose great amounts of information , or correspond to naive bayes , logistic regression , or noisy-or that suffer from overconfidence . we propose new simple aggregators and simple modifications of existing models that empirically outperform the existing ones . the intuition we provide on different ( existing or new ) models and their shortcomings plus our empirical findings promise to form the foundation for future representations .", "topics": ["numerical analysis"]}
{"title": "learning protein dynamics with metastable switching systems", "abstract": "we introduce a machine learning approach for extracting fine-grained representations of protein evolution from molecular dynamics datasets . metastable switching linear dynamical systems extend standard switching models with a physically-inspired stability constraint . this constraint enables the learning of nuanced representations of protein dynamics that closely match physical reality . we derive an em algorithm for learning , where the e-step extends the forward-backward algorithm for hmms and the m-step requires the solution of large biconvex optimization problems . we construct an approximate semidefinite program solver based on the frank-wolfe algorithm and use it to solve the m-step . we apply our em algorithm to learn accurate dynamics from large simulation datasets for the opioid peptide met-enkephalin and the proto-oncogene src-kinase . our learned models demonstrate significant improvements in temporal coherence over hmms and standard switching models for met-enkephalin , and sample transition paths ( possibly useful in rational drug design ) for src-kinase .", "topics": ["approximation algorithm", "simulation"]}
{"title": "a novel regularized principal graph learning framework on explicit graph representation", "abstract": "many scientific datasets are of high dimension , and the analysis usually requires visual manipulation by retaining the most important structures of data . principal curve is a widely used approach for this purpose . however , many existing methods work only for data with structures that are not self-intersected , which is quite restrictive for real applications . a few methods can overcome the above problem , but they either require complicated human-made rules for a specific task with lack of convergence guarantee and adaption flexibility to different tasks , or can not obtain explicit structures of data . to address these issues , we develop a new regularized principal graph learning framework that captures the local information of the underlying graph structure based on reversed graph embedding . as showcases , models that can learn a spanning tree or a weighted undirected $ \\ell_1 $ graph are proposed , and a new learning algorithm is developed that learns a set of principal points and a graph structure from data , simultaneously . the new algorithm is simple with guaranteed convergence . we then extend the proposed framework to deal with large-scale data . experimental results on various synthetic and six real world datasets show that the proposed method compares favorably with baselines and can uncover the underlying structure correctly .", "topics": ["baseline ( configuration management )", "synthetic data"]}
{"title": "considerations upon the machine learning technologies", "abstract": "artificial intelligence offers superior techniques and methods by which problems from diverse domains may find an optimal solution . the machine learning technologies refer to the domain of artificial intelligence aiming to develop the techniques allowing the computers to `` learn '' . some systems based on machine learning technologies tend to eliminate the necessity of the human intelligence while the others adopt a man-machine collaborative approach .", "topics": ["optimization problem", "artificial intelligence"]}
{"title": "eie : efficient inference engine on compressed deep neural network", "abstract": "state-of-the-art deep neural networks ( dnns ) have hundreds of millions of connections and are both computationally and memory intensive , making them difficult to deploy on embedded systems with limited hardware resources and power budgets . while custom hardware helps the computation , fetching weights from dram is two orders of magnitude more expensive than alu operations , and dominates the required power . previously proposed 'deep compression ' makes it possible to fit large dnns ( alexnet and vggnet ) fully in on-chip sram . this compression is achieved by pruning the redundant connections and having multiple connections share the same weight . we propose an energy efficient inference engine ( eie ) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing . going from dram to sram gives eie 120x energy saving ; exploiting sparsity saves 10x ; weight sharing gives 8x ; skipping zero activations from relu saves another 3x . evaluated on nine dnn benchmarks , eie is 189x and 13x faster when compared to cpu and gpu implementations of the same dnn without compression . eie has a processing power of 102gops/s working directly on a compressed network , corresponding to 3tops/s on an uncompressed network , and processes fc layers of alexnet at 1.88x10^4 frames/sec with a power dissipation of only 600mw . it is 24,000x and 3,400x more energy efficient than a cpu and gpu respectively . compared with dadiannao , eie has 2.9x , 19x and 3x better throughput , energy efficiency and area efficiency .", "topics": ["computation", "sparse matrix"]}
{"title": "intelligent subset selection of power generators for economic dispatch", "abstract": "sustainable and economical generation of electrical power is an essential and mandatory component of infrastructure in today 's world . optimal generation ( generator subset selection ) of power requires a careful evaluation of various factors like type of source , generation , transmission & storage capacities , congestion among others which makes this a difficult task . we created a grid to simulate various conditions including stimuli like generator supply , weather and load demand using siemens pss/e software and this data is trained using deep learning methods and subsequently tested . the results are highly encouraging . as per our knowledge , this is the first paper to propose a working and scalable deep learning model for this problem .", "topics": ["simulation", "scalability"]}
{"title": "first-order regret bounds for combinatorial semi-bandits", "abstract": "we consider the problem of online combinatorial optimization under semi-bandit feedback , where a learner has to repeatedly pick actions from a combinatorial decision set in order to minimize the total losses associated with its decisions . after making each decision , the learner observes the losses associated with its action , but not other losses . for this problem , there are several learning algorithms that guarantee that the learner 's expected regret grows as $ \\widetilde { o } ( \\sqrt { t } ) $ with the number of rounds $ t $ . in this paper , we propose an algorithm that improves this scaling to $ \\widetilde { o } ( \\sqrt { { l_t^* } } ) $ , where $ l_t^* $ is the total loss of the best action . our algorithm is among the first to achieve such guarantees in a partial-feedback scheme , and the first one to do so in a combinatorial setting .", "topics": ["regret ( decision theory )", "computational complexity theory"]}
{"title": "combinatorial framework for planning in geological exploration", "abstract": "the paper describes combinatorial framework for planning of geological exploration for oil-gas fields . the suggested scheme of the geological exploration involves the following stages : ( 1 ) building of special 4-layer tree-like model ( layer of geological exploration ) : productive layer , group of productive layers , oil-gas field , oil-gas region ( or group of the fields ) ; ( 2 ) generations of local design ( exploration ) alternatives for each low-layer geological objects : conservation , additional search , independent utilization , joint utilization ; ( 3 ) multicriteria ( i.e . , multi-attribute ) assessment of the design ( exploration ) alternatives and their interrelation ( compatibility ) and mapping if the obtained vector estimates into integrated ordinal scale ; ( 4 ) hierarchical design ( 'bottom-up ' ) of composite exploration plans for each oil-gas field ; ( 5 ) integration of the plans into region plans and ( 6 ) aggregation of the region plans into a general exploration plan . stages 2 , 3 , 4 , and 5 are based on hierarchical multicriteria morphological design ( hmmd ) method ( assessment of ranking of alternatives , selection and composition of alternatives into composite alternatives ) . the composition problem is based on morphological clique model . aggregation of the obtained modular alternatives ( stage 6 ) is based on detection of a alternatives 'kernel ' and its extension by addition of elements ( multiple choice model ) . in addition , the usage of multiset estimates for alternatives is described as well . the alternative estimates are based on expert judgment . the suggested combinatorial planning methodology is illustrated by numerical examples for geological exploration of yamal peninsula .", "topics": ["numerical analysis"]}
{"title": "ssp : semantic space projection for knowledge graph embedding with text descriptions", "abstract": "knowledge representation is an important , long-history topic in ai , and there have been a large amount of work for knowledge graph embedding which projects symbolic entities and relations into low-dimensional , real-valued vector space . however , most embedding methods merely concentrate on data fitting and ignore the explicit semantic expression , leading to uninterpretable representations . thus , traditional embedding methods have limited potentials for many applications such as question answering , and entity classification . to this end , this paper proposes a semantic representation method for knowledge graph \\textbf { ( ksr ) } , which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple . since both aspects and categories are semantics-relevant , the collection of categories in each aspect is treated as the semantic representation of this triple . extensive experiments justify our model outperforms other state-of-the-art baselines substantially .", "topics": ["baseline ( configuration management )", "entity"]}
{"title": "belief propagation in conditional rbms for structured prediction", "abstract": "restricted boltzmann machines~ ( rbms ) and conditional rbms~ ( crbms ) are popular models for a wide range of applications . in previous work , learning on such models has been dominated by contrastive divergence~ ( cd ) and its variants . belief propagation~ ( bp ) algorithms are believed to be slow for structured prediction on conditional rbms~ ( e.g . , mnih et al . [ 2011 ] ) , and not as good as cd when applied in learning~ ( e.g . , larochelle et al . [ 2012 ] ) . in this work , we present a matrix-based implementation of belief propagation algorithms on crbms , which is easily scalable to tens of thousands of visible and hidden units . we demonstrate that , in both maximum likelihood and max-margin learning , training conditional rbms with bp as the inference routine can provide significantly better results than current state-of-the-art cd methods on structured prediction problems . we also include practical guidelines on training crbms with bp , and some insights on the interaction of learning and inference algorithms for crbms .", "topics": ["scalability"]}
{"title": "least square variational bayesian autoencoder with regularization", "abstract": "in recent years variation autoencoders have become one of the most popular unsupervised learning of complicated distributions.variational autoencoder ( vae ) provides more efficient reconstructive performance over a traditional autoencoder . variational auto enocders make better approximaiton than mcmc . the vae defines a generative process in terms of ancestral sampling through a cascade of hidden stochastic layers . they are a directed graphic models . variational autoencoder is trained to maximise the variational lower bound . here we are trying maximise the likelihood and also at the same time we are trying to make a good approximation of the data . its basically trading of the data log-likelihood and the kl divergence from the true posterior . this paper describes the scenario in which we wish to find a point-estimate to the parameters $ \\theta $ of some parametric model in which we generate each observations by first sampling a local latent variable and then sampling the associated observation . here we use least square loss function with regularization in the the reconstruction of the image , the least square loss function was found to give better reconstructed images and had a faster training time .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "safe and nested subgame solving for imperfect-information games", "abstract": "in imperfect-information games , the optimal strategy in a subgame may depend on the strategy in other , unreached subgames . thus a subgame can not be solved in isolation and must instead consider the strategy for the entire game as a whole , unlike perfect-information games . nevertheless , it is possible to first approximate a solution for the whole game and then improve it by solving individual subgames . this is referred to as subgame solving . we introduce subgame-solving techniques that outperform prior methods both in theory and practice . we also show how to adapt them , and past subgame-solving techniques , to respond to opponent actions that are outside the original action abstraction ; this significantly outperforms the prior state-of-the-art approach , action translation . finally , we show that subgame solving can be repeated as the game progresses down the game tree , leading to far lower exploitability . these techniques were a key component of libratus , the first ai to defeat top humans in heads-up no-limit texas hold'em poker .", "topics": ["approximation algorithm"]}
{"title": "hdltex : hierarchical deep learning for text classification", "abstract": "the continually increasing number of documents produced each year necessitates ever improving information processing methods for searching , retrieving , and organizing text . central to these information processing methods is document classification , which has become an important application for supervised learning . recently the performance of these traditional classifiers has degraded as the number of documents has increased . this is because along with this growth in the number of documents has come an increase in the number of categories . this paper approaches this problem differently from current document classification methods that view the problem as multi-class classification . instead we perform hierarchical classification using an approach we call hierarchical deep learning for text classification ( hdltex ) . hdltex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy .", "topics": ["supervised learning"]}
{"title": "learning multiagent communication with backpropagation", "abstract": "many tasks in ai require the collaboration of multiple agents . typically , the communication protocol between agents is manually specified and not altered during training . in this paper we explore a simple neural model , called commnet , that uses continuous communication for fully cooperative tasks . the model consists of multiple agents and the communication between them is learned alongside their policy . we apply this model to a diverse set of tasks , demonstrating the ability of the agents to learn to communicate amongst themselves , yielding improved performance over non-communicative agents and baselines . in some cases , it is possible to interpret the language devised by the agents , revealing simple but effective strategies for solving the task at hand .", "topics": ["baseline ( configuration management )"]}
{"title": "a survey of current datasets for vision and language research", "abstract": "integrating vision and language has long been a dream in work on artificial intelligence ( ai ) . in the past two years , we have witnessed an explosion of work that brings together vision and language from images to videos and beyond . the available corpora have played a crucial role in advancing this area of research . in this paper , we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly . our analyses show that the most recent datasets have been using more complex language and more abstract concepts , however , there are different strengths and weaknesses in each .", "topics": ["text corpus", "artificial intelligence"]}
{"title": "an improved algorithm for e-generalization", "abstract": "e-generalization computes common generalizations of given ground terms w.r.t . a given equational background theory e. in 2005 [ arxiv:1403.8118 ] , we had presented a computation approach based on standard regular tree grammar algorithms , and a prolog prototype implementation . in this report , we present algorithmic improvements , prove them correct and complete , and give some details of an efficiency-oriented implementation in c that allows us to handle problems larger by several orders of magnitude .", "topics": ["computation"]}
{"title": "measuring machine intelligence through visual question answering", "abstract": "as machines have become more intelligent , there has been a renewed interest in methods for measuring their intelligence . a common approach is to propose tasks for which a human excels , but one which machines find difficult . however , an ideal task should also be easy to evaluate and not be easily gameable . we begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence . an alternative and more promising task is visual question answering that tests a machine 's ability to reason about language and vision . we describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images . using around 10 million human generated answers , machines may be easily evaluated .", "topics": ["high- and low-level", "computer vision"]}
{"title": "local region sparse learning for image-on-scalar regression", "abstract": "identification of regions of interest ( roi ) associated with certain disease has a great impact on public health . imposing sparsity of pixel values and extracting active regions simultaneously greatly complicate the image analysis . we address these challenges by introducing a novel region-selection penalty in the framework of image-on-scalar regression . our penalty combines the smoothly clipped absolute deviation ( scad ) regularization , enforcing sparsity , and the scad of total variation ( tv ) regularization , enforcing spatial contiguity , into one group , which segments contiguous spatial regions against zero-valued background . efficient algorithm is based on the alternative direction method of multipliers ( admm ) which decomposes the non-convex problem into two iterative optimization problems with explicit solutions . another virtue of the proposed method is that a divide and conquer learning algorithm is developed , thereby allowing scaling to large images . several examples are presented and the experimental results are compared with other state-of-the-art approaches .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "proximal iteratively reweighted algorithm with multiple splitting for nonconvex sparsity optimization", "abstract": "this paper proposes the proximal iteratively reweighted ( pire ) algorithm for solving a general problem , which involves a large body of nonconvex sparse and structured sparse related problems . comparing with previous iterative solvers for nonconvex sparse problem , pire is much more general and efficient . the computational cost of pire in each iteration is usually as low as the state-of-the-art convex solvers . we further propose the pire algorithm with parallel splitting ( pire-ps ) and pire algorithm with alternative updating ( pire-au ) to handle the multi-variable problems . in theory , we prove that our proposed methods converge and any limit solution is a stationary point . extensive experiments on both synthesis and real data sets demonstrate that our methods achieve comparative learning performance , but are much more efficient , by comparing with previous nonconvex solvers .", "topics": ["loss function", "iteration"]}
{"title": "differential geometric retrieval of deep features", "abstract": "comparing images to recommend items from an image-inventory is a subject of continued interest . added with the scalability of deep-learning architectures the once `manual ' job of hand-crafting features have been largely alleviated , and images can be compared according to features generated from a deep convolutional neural network . in this paper , we compare distance metrics ( and divergences ) to rank features generated from a neural network , for content-based image retrieval . specifically , after modelling individual images using approximations of mixture models or sparse covariance estimators , we resort to their information-theoretic and riemann geometric comparisons . we show that using approximations of mixture models enable us to compute a distance measure based on the wasserstein metric that requires less effort than other computationally intensive optimal transport plans ; finally , an affine invariant metric is used to compare the optimal transport metric to its riemann geometric counterpart -- we conclude that although expensive , retrieval metric based on wasserstein geometry is more suitable than information theoretic comparison of images . in short , we combine gpu scalability in learning deep feature vectors with statistically efficient metrics that we foresee being utilised in a commercial setting .", "topics": ["feature vector", "approximation"]}
{"title": "improved neural text attribute transfer with non-parallel data", "abstract": "text attribute transfer using non-parallel data requires methods that can perform disentanglement of content and linguistic attributes . in this work , we propose multiple improvements over the existing approaches that enable the encoder-decoder framework to cope with the text attribute transfer from non-parallel data . we perform experiments on the sentiment transfer task using two datasets . for both datasets , our proposed method outperforms a strong baseline in two of the three employed evaluation metrics .", "topics": ["baseline ( configuration management )"]}
{"title": "joint object-material category segmentation from audio-visual cues", "abstract": "it is not always possible to recognise objects and infer material properties for a scene from visual cues alone , since objects can look visually similar whilst being made of very different materials . in this paper , we therefore present an approach that augments the available dense visual cues with sparse auditory cues in order to estimate dense object and material labels . since estimates of object class and material properties are mutually informative , we optimise our multi-output labelling jointly using a random-field framework . we evaluate our system on a new dataset with paired visual and auditory data that we make publicly available . we demonstrate that this joint estimation of object and material labels significantly outperforms the estimation of either category in isolation .", "topics": ["mathematical optimization", "ground truth"]}
{"title": "approximate evaluation of marginal association probabilities with belief propagation", "abstract": "data association , the problem of reasoning over correspondence between targets and measurements , is a fundamental problem in tracking . this paper presents a graphical model formulation of data association and applies an approximate inference method , belief propagation ( bp ) , to obtain estimates of marginal association probabilities . we prove that bp is guaranteed to converge , and bound the number of iterations necessary . experiments reveal a favourable comparison to prior methods in terms of accuracy and computational complexity .", "topics": ["graphical model", "computational complexity theory"]}
{"title": "self-expressive decompositions for matrix approximation and clustering", "abstract": "data-aware methods for dimensionality reduction and matrix decomposition aim to find low-dimensional structure in a collection of data . classical approaches discover such structure by learning a basis that can efficiently express the collection . recently , `` self expression '' , the idea of using a small subset of data vectors to represent the full collection , has been developed as an alternative to learning . here , we introduce a scalable method for computing sparse self-expressive decompositions ( seed ) . seed is a greedy method that constructs a basis by sequentially selecting incoherent vectors from the dataset . after forming a basis from a subset of vectors in the dataset , seed then computes a sparse representation of the dataset with respect to this basis . we develop sufficient conditions under which seed exactly represents low rank matrices and vectors sampled from a unions of independent subspaces . we show how seed can be used in applications ranging from matrix approximation and denoising to clustering , and apply it to numerous real-world datasets . our results demonstrate that seed is an attractive low-complexity alternative to other sparse matrix factorization approaches such as sparse pca and self-expressive methods for clustering .", "topics": ["cluster analysis", "noise reduction"]}
{"title": "bagan : data augmentation with balancing gan", "abstract": "image classification datasets are often imbalanced , characteristic that negatively affects the accuracy of deeplearning classifiers . in this work we propose balancing gans ( bagans ) as an augmentation tool to restore balance in imbalanced datasets . this is challenging because the few minority-class images may not be enough to train a gan . we overcome this issue by including during training all available images of majority and minority classes . the generative model learns useful features from majority classes and uses these to generate images for minority classes . we apply class-conditioning in the latent space to drive the generation process towards a target class . additionally , we couple gans with autoencoding techniques to reduce the risk of collapsing toward the generation of few foolish examples . we compare the proposed methodology with state-of-the-art gans and demonstrate that bagan generates images of superior quality when trained with an imbalanced dataset .", "topics": ["generative model", "autoencoder"]}
{"title": "shape generation using spatially partitioned point clouds", "abstract": "we propose a method to generate 3d shapes using point clouds . given a point-cloud representation of a 3d shape , our method builds a kd-tree to spatially partition the points . this orders them consistently across all shapes , resulting in reasonably good correspondences across all shapes . we then use pca analysis to derive a linear shape basis across the spatially partitioned points , and optimize the point ordering by iteratively minimizing the pca reconstruction error . even with the spatial sorting , the point clouds are inherently noisy and the resulting distribution over the shape coefficients can be highly multi-modal . we propose to use the expressive power of neural networks to learn a distribution over the shape coefficients in a generative-adversarial framework . compared to 3d shape generative models trained on voxel-representations , our point-based method is considerably more light-weight and scalable , with little loss of quality . it also outperforms simpler linear factor models such as probabilistic pca , both qualitatively and quantitatively , on a number of categories from the shapenet dataset . furthermore , our method can easily incorporate other point attributes such as normal and color information , an additional advantage over voxel-based representations .", "topics": ["scalability", "coefficient"]}
{"title": "bayesian hierarchical clustering with exponential family : small-variance asymptotics and reducibility", "abstract": "bayesian hierarchical clustering ( bhc ) is an agglomerative clustering method , where a probabilistic model is defined and its marginal likelihoods are evaluated to decide which clusters to merge . while bhc provides a few advantages over traditional distance-based agglomerative clustering algorithms , successive evaluation of marginal likelihoods and careful hyperparameter tuning are cumbersome and limit the scalability . in this paper we relax bhc into a non-probabilistic formulation , exploring small-variance asymptotics in conjugate-exponential models . we develop a novel clustering algorithm , referred to as relaxed bhc ( rbhc ) , from the asymptotic limit of the bhc model that exhibits the scalability of distance-based agglomerative clustering algorithms as well as the flexibility of bayesian nonparametric models . we also investigate the reducibility of the dissimilarity measure emerged from the asymptotic limit of the bhc model , allowing us to use scalable algorithms such as the nearest neighbor chain algorithm . numerical experiments on both synthetic and real-world datasets demonstrate the validity and high performance of our method .", "topics": ["cluster analysis", "time complexity"]}
{"title": "reduced-rank hidden markov models", "abstract": "we introduce the reduced-rank hidden markov model ( rr-hmm ) , a generalization of hmms that can model smooth state evolution as in linear dynamical systems ( ldss ) as well as non-log-concave predictive distributions as in continuous-observation hmms . rr-hmms assume an m-dimensional latent state and n discrete observations , with a transition matrix of rank k < = m. this implies the dynamics evolve in a k-dimensional subspace , while the shape of the set of predictive distributions is determined by m. latent state belief is represented with a k-dimensional state vector and inference is carried out entirely in r^k , making rr-hmms as computationally efficient as k-state hmms yet more expressive . to learn rr-hmms , we relax the assumptions of a recently proposed spectral learning algorithm for hmms ( hsu , kakade and zhang 2009 ) and apply it to learn k-dimensional observable representations of rank-k rr-hmms . the algorithm is consistent and free of local optima , and we extend its performance guarantees to cover the rr-hmm case . we show how this algorithm can be used in conjunction with a kernel density estimator to efficiently model high-dimensional multivariate continuous data . we also relax the assumption that single observations are sufficient to disambiguate state , and extend the algorithm accordingly . experiments on synthetic data and a toy video , as well as on a difficult robot vision modeling problem , yield accurate models that compare favorably with standard alternatives in simulation quality and prediction capability .", "topics": ["computational complexity theory", "synthetic data"]}
{"title": "the pragmatics of indirect commands in collaborative discourse", "abstract": "today 's artificial assistants are typically prompted to perform tasks through direct , imperative commands such as \\emph { set a timer } or \\emph { pick up the box } . however , to progress toward more natural exchanges between humans and these assistants , it is important to understand the way non-imperative utterances can indirectly elicit action of an addressee . in this paper , we investigate command types in the setting of a grounded , collaborative game . we focus on a less understood family of utterances for eliciting agent action , locatives like \\emph { the chair is in the other room } , and demonstrate how these utterances indirectly command in specific game state contexts . our work shows that models with domain-specific grounding can effectively realize the pragmatic reasoning that is necessary for more robust natural language interaction .", "topics": ["natural language", "robot"]}
{"title": "learning sequence neighbourhood metrics", "abstract": "recurrent neural networks ( rnns ) in combination with a pooling operator and the neighbourhood components analysis ( nca ) objective function are able to detect the characterizing dynamics of sequences and embed them into a fixed-length vector space of arbitrary dimensionality . subsequently , the resulting features are meaningful and can be used for visualization or nearest neighbour classification in linear time . this kind of metric learning for sequential data enables the use of algorithms tailored towards fixed length vector spaces such as r^n .", "topics": ["recurrent neural network", "time complexity"]}
{"title": "tree memory networks for modelling long-term temporal dependencies", "abstract": "in the domain of sequence modelling , recurrent neural networks ( rnn ) have been capable of achieving impressive results in a variety of application areas including visual question answering , part-of-speech tagging and machine translation . however this success in modelling short term dependencies has not successfully transitioned to application areas such as trajectory prediction , which require capturing both short term and long term relationships . in this paper , we propose a tree memory network ( tmn ) for modelling long term and short term relationships in sequence-to-sequence mapping problems . the proposed network architecture is composed of an input module , controller and a memory module . in contrast to related literature , which models the memory as a sequence of historical states , we model the memory as a recursive tree structure . this structure more effectively captures temporal dependencies across both short term and long term sequences using its hierarchical structure . we demonstrate the effectiveness and flexibility of the proposed tmn in two practical problems , aircraft trajectory modelling and pedestrian trajectory modelling in a surveillance setting , and in both cases we outperform the current state-of-the-art . furthermore , we perform an in depth analysis on the evolution of the memory module content over time and provide visual evidence on how the proposed tmn is able to map both long term and short term relationships efficiently via a hierarchical structure .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "exact and approximate inference in graphical models : variable elimination and beyond", "abstract": "probabilistic graphical models offer a powerful framework to account for the dependence structure between variables , which is represented as a graph . however , the dependence between variables may render inference tasks intractable . in this paper we review techniques exploiting the graph structure for exact inference , borrowed from optimisation and computer science . they are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated . the so-called treewidth of the graph characterises this algorithmic complexity : low-treewidth graphs can be processed efficiently . the first message that we illustrate is therefore the idea that for inference in graphical model , the number of variables is not the limiting factor , and it is worth checking for the treewidth before turning to approximate methods . we show how algorithms providing an upper bound of the treewidth can be exploited to derive a 'good ' elimination order enabling to perform exact inference . the second message is that when the treewidth is too large , algorithms for approximate inference linked to the principle of variable elimination , such as loopy belief propagation and variational approaches , can lead to accurate results while being much less time consuming than monte-carlo approaches . we illustrate the techniques reviewed in this article on benchmarks of inference problems in genetic linkage analysis and computer vision , as well as on hidden variables restoration in coupled hidden markov models .", "topics": ["graphical model", "calculus of variations"]}
{"title": "ui-net : interactive artificial neural networks for iterative image segmentation based on a user model", "abstract": "for complex segmentation tasks , fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects . especially in cases where only few data sets need to be processed for a highly accurate result , semi-automatic segmentation techniques exhibit a clear benefit for the user . one area of application is medical image processing during an intervention for a single patient . we propose a learning-based cooperative segmentation approach which includes the computing entity as well as the user into the task . our system builds upon a state-of-the-art fully convolutional artificial neural network ( fcn ) as well as an active user model for training . during the segmentation process , a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the fcn system to achieve an interactive and precise segmentation result . the segmentation quality of interactive fcns is evaluated . iterative fcn approaches can yield superior results compared to networks without the user input channel component , due to a consistent improvement in segmentation quality after each interaction .", "topics": ["image segmentation", "neural networks"]}
{"title": "early stopping for kernel boosting algorithms : a general analysis with localized complexities", "abstract": "early stopping of iterative algorithms is a widely-used form of regularization in statistics , commonly used in conjunction with boosting and related gradient-type algorithms . although consistency results have been established in some settings , such estimators are less well-understood than their analogues based on penalized regularization . in this paper , for a relatively broad class of loss functions and boosting algorithms ( including l2-boost , logitboost and adaboost , among others ) , we exhibit a direct connection between the performance of a stopped iterate and the localized gaussian complexity of the associated function class . this connection allows us to show that local fixed point analysis of gaussian or rademacher complexities , now standard in the analysis of penalized estimators , can be used to derive optimal stopping rules . we derive such stopping rules in detail for various kernel classes , and illustrate the correspondence of our theory with practice for sobolev kernel classes .", "topics": ["kernel ( operating system )", "matrix regularization"]}
{"title": "lstm-based encoder-decoder for multi-sensor anomaly detection", "abstract": "mechanical devices such as engines , vehicles , aircrafts , etc . , are typically instrumented with numerous sensors to capture the behavior and health of the machine . however , there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable . for instance , manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series . detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity , or prediction models that utilize prediction errors to detect anomalies . we propose a long short term memory networks based encoder-decoder scheme for anomaly detection ( encdec-ad ) that learns to reconstruct 'normal ' time-series behavior , and thereafter uses reconstruction error to detect anomalies . we experiment with three publicly available quasi predictable time-series datasets : power demand , space shuttle , and ecg , and two real-world engine datasets with both predictive and unpredictable behavior . we show that encdec-ad is robust and can detect anomalies from predictable , unpredictable , periodic , aperiodic , and quasi-periodic time-series . further , we show that encdec-ad is able to detect anomalies from short time-series ( length as small as 30 ) as well as long time-series ( length as large as 500 ) .", "topics": ["time series", "sensor"]}
{"title": "robust online multi-task learning with correlative and personalized structures", "abstract": "multi-task learning ( mtl ) can enhance a classifier 's generalization performance by learning multiple related tasks simultaneously . conventional mtl works under the offline or batch setting , and suffers from expensive training cost and poor scalability . to address such inefficiency issues , online learning techniques have been applied to solve mtl problems . however , most existing algorithms of online mtl constrain task relatedness into a presumed structure via a single weight matrix , which is a strict restriction that does not always hold in practice . in this paper , we propose a robust online mtl framework that overcomes this restriction by decomposing the weight matrix into two components : the first one captures the low-rank common structure among tasks via a nuclear norm and the second one identifies the personalized patterns of outlier tasks via a group lasso . theoretical analysis shows the proposed algorithm can achieve a sub-linear regret with respect to the best linear model in hindsight . even though the above framework achieves good performance , the nuclear norm that simply adds all nonzero singular values together may not be a good low-rank approximation . to improve the results , we use a log-determinant function as a non-convex rank approximation . the gradient scheme is applied to optimize log-determinant function and can obtain a closed-form solution for this refined problem . experimental results on a number of real-world applications verify the efficacy of our method .", "topics": ["regret ( decision theory )", "value ( ethics )"]}
{"title": "demystifying resnet", "abstract": "the residual network ( resnet ) , proposed in he et al . ( 2015 ) , utilized shortcut connections to significantly reduce the difficulty of training , which resulted in great performance boosts in terms of both training and generalization error . it was empirically observed in he et al . ( 2015 ) that stacking more layers of residual blocks with shortcut 2 results in smaller training error , while it is not true for shortcut of length 1 or 3 . we provide a theoretical explanation for the uniqueness of shortcut 2 . we show that with or without nonlinearities , by adding shortcuts that have depth two , the condition number of the hessian of the loss function at the zero initial point is depth-invariant , which makes training very deep models no more difficult than shallow ones . shortcuts of higher depth result in an extremely flat ( high-order ) stationary point initially , from which the optimization algorithm is hard to escape . the shortcut 1 , however , is essentially equivalent to no shortcuts , which has a condition number exploding to infinity as the number of layers grows . we further argue that as the number of layers tends to infinity , it suffices to only look at the loss function at the zero initial point . extensive experiments are provided accompanying our theoretical results . we show that initializing the network to small weights with shortcut 2 achieves significantly better results than random gaussian ( xavier ) initialization , orthogonal initialization , and shortcuts of deeper depth , from various perspectives ranging from final loss , learning dynamics and stability , to the behavior of the hessian along the learning process .", "topics": ["loss function"]}
{"title": "elliptical slice sampling", "abstract": "many probabilistic models introduce strong dependencies between variables using a latent multivariate gaussian distribution or a gaussian process . we present a new markov chain monte carlo algorithm for performing inference in models with multivariate gaussian priors . its key properties are : 1 ) it has simple , generic code applicable to many models , 2 ) it has no free parameters , 3 ) it works well for a variety of gaussian process based models . these properties make our method ideal for use while model building , removing the need to spend time deriving and tuning updates for more complex algorithms .", "topics": ["markov chain"]}
{"title": "regularization for unsupervised deep neural nets", "abstract": "unsupervised neural networks , such as restricted boltzmann machines ( rbms ) and deep belief networks ( dbns ) , are powerful tools for feature selection and pattern recognition tasks . we demonstrate that overfitting occurs in such models just as in deep feedforward neural networks , and discuss possible regularization methods to reduce overfitting . we also propose a `` partial '' approach to improve the efficiency of dropout/dropconnect in this scenario , and discuss the theoretical justification of these methods from model convergence and likelihood bounds . finally , we compare the performance of these methods based on their likelihood and classification error rates for various pattern recognition data sets .", "topics": ["matrix regularization", "bayesian network"]}
{"title": "fire detection in a still image using colour information", "abstract": "colour analysis is a crucial step in image-based fire detection algorithms . many of the proposed fire detection algorithms in a still image are prone to false alarms caused by objects with a colour similar to fire . to design a colour-based system with a better false alarm rate , a new colour-differentiating conversion matrix , efficient on images of high colour complexity , is proposed . the elements of this conversion matrix are obtained by performing k-medoids clustering and particle swarm optimisation procedures on a fire sample image with a background of high fire-colour similarity . the proposed conversion matrix is then used to construct two new fire colour detection frameworks . the first detection method is a two-stage non-linear image transformation framework , while the second is a direct transformation of an image with the proposed conversion matrix . a performance comparison of the proposed methods with alternate methods in the literature was carried out . experimental results indicate that the linear image transformation method outperforms other methods regarding false alarm rate while the non-linear two-stage image transformation method has the best performance on the f-score metric and provides a better trade-off between missed detection and false alarm rate .", "topics": ["cluster analysis", "nonlinear system"]}
{"title": "an inequality with applications to structured sparsity and multitask dictionary learning", "abstract": "from concentration inequalities for the suprema of gaussian or rademacher processes an inequality is derived . it is applied to sharpen existing and to derive novel bounds on the empirical rademacher complexities of unit balls in various norms appearing in the context of structured sparsity and multitask dictionary learning or matrix factorization . a key role is played by the largest eigenvalue of the data covariance matrix .", "topics": ["sparse matrix", "dictionary"]}
{"title": "distributed supervised learning using neural networks", "abstract": "distributed learning is the problem of inferring a function in the case where training data is distributed among multiple geographically separated sources . particularly , the focus is on designing learning strategies with low computational requirements , in which communication is restricted only to neighboring agents , with no reliance on a centralized authority . in this thesis , we analyze multiple distributed protocols for a large number of neural network architectures . the first part of the thesis is devoted to a definition of the problem , followed by an extensive overview of the state-of-the-art . next , we introduce different strategies for a relatively simple class of single layer neural networks , where a linear output layer is preceded by a nonlinear layer , whose weights are stochastically assigned in the beginning of the learning process . we consider both batch and sequential learning , with horizontally and vertically partitioned data . in the third part , we consider instead the more complex problem of semi-supervised distributed learning , where each agent is provided with an additional set of unlabeled training samples . we propose two different algorithms based on diffusion processes for linear support vector machines and kernel ridge regression . subsequently , the fourth part extends the discussion to learning with time-varying data ( e.g . time-series ) using recurrent neural networks . we consider two different families of networks , namely echo state networks ( extending the algorithms introduced in the second part ) , and spline adaptive filters . overall , the algorithms presented throughout the thesis cover a wide range of possible practical applications , and lead the way to numerous future extensions , which are briefly summarized in the conclusive chapter .", "topics": ["test set", "supervised learning"]}
{"title": "distributed non-stochastic experts", "abstract": "we consider the online distributed non-stochastic experts problem , where the distributed system consists of one coordinator node that is connected to $ k $ sites , and the sites are required to communicate with each other via the coordinator . at each time-step $ t $ , one of the $ k $ site nodes has to pick an expert from the set $ { 1 , ... , n } $ , and the same site receives information about payoffs of all experts for that round . the goal of the distributed system is to minimize regret at time horizon $ t $ , while simultaneously keeping communication to a minimum . the two extreme solutions to this problem are : ( i ) full communication : this essentially simulates the non-distributed setting to obtain the optimal $ o ( \\sqrt { \\log ( n ) t } ) $ regret bound at the cost of $ t $ communication . ( ii ) no communication : each site runs an independent copy : the regret is $ o ( \\sqrt { log ( n ) kt } ) $ and the communication is 0 . this paper shows the difficulty of simultaneously achieving regret asymptotically better than $ \\sqrt { kt } $ and communication better than $ t $ . we give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off : regret $ o ( \\sqrt { k^ { 5 ( 1+\\epsilon ) /6 } t } ) $ and communication $ o ( t/k^ { \\epsilon } ) $ , for any value of $ \\epsilon \\in ( 0 , 1/5 ) $ . we also consider a variant of the model , where the coordinator picks the expert . in this model , we show that the label-efficient forecaster of cesa-bianchi et al . ( 2005 ) already gives us strategy that is near optimal in regret vs communication trade-off .", "topics": ["regret ( decision theory )"]}
{"title": "a new theoretical and technological system of imprecise-information processing", "abstract": "imprecise-information processing will play an indispensable role in intelligent systems , especially in the anthropomorphic intelligent systems ( as intelligent robots ) . a new theoretical and technological system of imprecise-information processing has been founded in principles of imprecise-information processing : a new theoretical and technological system [ 1 ] which is different from fuzzy technology . the system has clear hierarchy and rigorous structure , which results from the formation principle of imprecise information and has solid mathematical and logical bases , and which has many advantages beyond fuzzy technology . the system provides a technological platform for relevant applications and lays a theoretical foundation for further research .", "topics": ["artificial intelligence", "robot"]}
{"title": "learning structural kernels for natural language processing", "abstract": "structural kernels are a flexible learning paradigm that has been widely used in natural language processing . however , the problem of model selection in kernel-based methods is usually overlooked . previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search , which is slow and coarse-grained . in contrast , bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods . in this paper we show how to perform this in the context of structural kernels by using gaussian processes . experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search . the framework proposed in this paper can be adapted to other structures besides trees , e.g . , strings and graphs , thereby extending the utility of kernel-based methods .", "topics": ["test set", "natural language processing"]}
{"title": "visual causal feature learning", "abstract": "we provide a rigorous definition of the visual cause of a behavior that is broadly applicable to the visually driven behavior in humans , animals , neurons , robots and other perceiving systems . our framework generalizes standard accounts of causal learning to settings in which the causal variables need to be constructed from micro-variables . we prove the causal coarsening theorem , which allows us to gain causal knowledge from observational data with minimal experimental effort . the theorem provides a connection to standard inference techniques in machine learning that identify features of an image that correlate with , but may not cause , the target behavior . finally , we propose an active learning scheme to learn a manipulator function that performs optimal manipulations on the image to automatically identify the visual cause of a target behavior . we illustrate our inference and learning algorithms in experiments based on both synthetic and real data .", "topics": ["feature learning", "synthetic data"]}
{"title": "a robust autoassociative memory with coupled networks of kuramoto-type oscillators", "abstract": "uncertain recognition success , unfavorable scaling of connection complexity or dependence on complex external input impair the usefulness of current oscillatory neural networks for pattern recognition or restrict technical realizations to small networks . we propose a new network architecture of coupled oscillators for pattern recognition which shows none of the mentioned aws . furthermore we illustrate the recognition process with simulation results and analyze the new dynamics analytically : possible output patterns are isolated attractors of the system . additionally , simple criteria for recognition success are derived from a lower bound on the basins of attraction .", "topics": ["simulation"]}
{"title": "ethical artificial intelligence - an open question", "abstract": "artificial intelligence ( ai ) is an effective science which employs strong enough approaches , methods , and techniques to solve unsolvable real world based problems . because of its unstoppable rise towards the future , there are also some discussions about its ethics and safety . shaping an ai friendly environment for people and a people friendly environment for ai can be a possible answer for finding a shared context of values for both humans and robots . in this context , objective of this paper is to address the ethical issues of ai and explore the moral dilemmas that arise from ethical algorithms , from pre set or acquired values . in addition , the paper will also focus on the subject of ai safety . as general , the paper will briefly analyze the concerns and potential solutions to solving the ethical issues presented and increase readers awareness on ai safety as another related research interest .", "topics": ["value ( ethics )", "artificial intelligence"]}
{"title": "cyclical learning rates for training neural networks", "abstract": "it is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks . this paper describes a new method for setting the learning rate , named cyclical learning rates , which practically eliminates the need to experimentally find the best values and schedule for the global learning rates . instead of monotonically decreasing the learning rate , this method lets the learning rate cyclically vary between reasonable boundary values . training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations . this paper also describes a simple way to estimate `` reasonable bounds '' -- linearly increasing the learning rate of the network for a few epochs . in addition , cyclical learning rates are demonstrated on the cifar-10 and cifar-100 datasets with resnets , stochastic depth networks , and densenets , and the imagenet dataset with the alexnet and googlenet architectures . these are practical tools for everyone who trains neural networks .", "topics": ["supervised learning", "value ( ethics )"]}
{"title": "a statistical approach to increase classification accuracy in supervised learning algorithms", "abstract": "probabilistic mixture models have been widely used for different machine learning and pattern recognition tasks such as clustering , dimensionality reduction , and classification . in this paper , we focus on trying to solve the most common challenges related to supervised learning algorithms by using mixture probability distribution functions . with this modeling strategy , we identify sub-labels and generate synthetic data in order to reach better classification accuracy . it means we focus on increasing the training data synthetically to increase the classification accuracy .", "topics": ["test set", "supervised learning"]}
{"title": "high-performance neural networks for visual object classification", "abstract": "we present a fast , fully parameterizable gpu implementation of convolutional neural network variants . our feature extractors are neither carefully designed nor pre-wired , but rather learned in a supervised way . our deep hierarchical architectures achieve the best published results on benchmarks for object classification ( norb , cifar10 ) and handwritten digit recognition ( mnist ) , with error rates of 2.53 % , 19.51 % , 0.35 % , respectively . deep nets trained by simple back-propagation perform better than more shallow ones . learning is surprisingly rapid . norb is completely trained within five epochs . test error rates on mnist drop to 2.42 % , 0.97 % and 0.48 % after 1 , 3 and 17 epochs , respectively .", "topics": ["feature extraction", "mnist database"]}
{"title": "delving deep into rectifiers : surpassing human-level performance on imagenet classification", "abstract": "rectified activation units ( rectifiers ) are essential for state-of-the-art neural networks . in this work , we study rectifier neural networks for image classification from two aspects . first , we propose a parametric rectified linear unit ( prelu ) that generalizes the traditional rectified unit . prelu improves model fitting with nearly zero extra computational cost and little overfitting risk . second , we derive a robust initialization method that particularly considers the rectifier nonlinearities . this method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures . based on our prelu networks ( prelu-nets ) , we achieve 4.94 % top-5 test error on the imagenet 2012 classification dataset . this is a 26 % relative improvement over the ilsvrc 2014 winner ( googlenet , 6.66 % ) . to our knowledge , our result is the first to surpass human-level performance ( 5.1 % , russakovsky et al . ) on this visual recognition challenge .", "topics": ["computer vision"]}
{"title": "a framework for redescription set construction", "abstract": "redescription mining is a field of knowledge discovery that aims at finding different descriptions of similar subsets of instances in the data . these descriptions are represented as rules inferred from one or more disjoint sets of attributes , called views . as such , they support knowledge discovery process and help domain experts in formulating new hypotheses or constructing new knowledge bases and decision support systems . in contrast to previous approaches that typically create one smaller set of redescriptions satisfying a pre-defined set of constraints , we introduce a framework that creates large and heterogeneous redescription set from which user/expert can extract compact sets of differing properties , according to its own preferences . construction of large and heterogeneous redescription set relies on clus-rm algorithm and a novel , conjunctive refinement procedure that facilitates generation of larger and more accurate redescription sets . the work also introduces the variability of redescription accuracy when missing values are present in the data , which significantly extends applicability of the method . crucial part of the framework is the redescription set extraction based on heuristic multi-objective optimization procedure that allows user to define importance levels towards one or more redescription quality criteria . we provide both theoretical and empirical comparison of the novel framework against current state of the art redescription mining algorithms and show that it represents more efficient and versatile approach for mining redescriptions from data .", "topics": ["data mining", "eisenstein 's criterion"]}
{"title": "hinge-loss markov random fields : convex inference for structured prediction", "abstract": "graphical models for structured domains are powerful tools , but the computational complexities of combinatorial prediction spaces can force restrictions on models , or require approximate inference in order to be tractable . instead of working in a combinatorial space , we use hinge-loss markov random fields ( hl-mrfs ) , an expressive class of graphical models with log-concave density functions over continuous variables , which can represent confidences in discrete predictions . this paper demonstrates that hl-mrfs are general tools for fast and accurate structured prediction . we introduce the first inference algorithm that is both scalable and applicable to the full class of hl-mrfs , and show how to train hl-mrfs with several learning algorithms . our experiments show that hl-mrfs match or surpass the predictive performance of state-of-the-art methods , including discrete models , in four application domains .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "a large self-annotated corpus for sarcasm", "abstract": "we introduce the self-annotated reddit corpus ( sarc ) , a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection . the corpus has 1.3 million sarcastic statements -- 10 times more than any previous dataset -- and many times more instances of non-sarcastic statements , allowing for learning in both balanced and unbalanced label regimes . each statement is furthermore self-annotated -- sarcasm is labeled by the author , not an independent annotator -- and provided with user , topic , and conversation context . we evaluate the corpus for accuracy , construct benchmarks for sarcasm detection , and evaluate baseline methods .", "topics": ["baseline ( configuration management )", "text corpus"]}
{"title": "exploiting generalization in the subspaces for faster model-based learning", "abstract": "due to the lack of enough generalization in the state-space , common methods in reinforcement learning ( rl ) suffer from slow learning speed especially in the early learning trials . this paper introduces a model-based method in discrete state-spaces for increasing learning speed in terms of required experience ( but not required computational time ) by exploiting generalization in the experiences of the subspaces . a subspace is formed by choosing a subset of features in the original state representation ( full-space ) . generalization and faster learning in a subspace are due to many-to-one mapping of experiences from the full-space to each state in the subspace . nevertheless , due to inherent perceptual aliasing in the subspaces , the policy suggested by each subspace does not generally converge to the optimal policy . our approach , called model based learning with subspaces ( mobles ) , calculates confidence intervals of the estimated q-values in the full-space and in the subspaces . these confidence intervals are used in the decision making , such that the agent benefits the most from the possible generalization while avoiding from detriment of the perceptual aliasing in the subspaces . convergence of mobles to the optimal policy is theoretically investigated . additionally , we show through several experiments that mobles improves the learning speed in the early trials .", "topics": ["time complexity", "reinforcement learning"]}
{"title": "bayesian inference in monte-carlo tree search", "abstract": "monte-carlo tree search ( mcts ) methods are drawing great interest after yielding breakthrough results in computer go . this paper proposes a bayesian approach to mcts that is inspired by distributionfree approaches such as uct [ 13 ] , yet significantly differs in important respects . the bayesian framework allows potentially much more accurate ( bayes-optimal ) estimation of node values and node uncertainties from a limited number of simulation trials . we further propose propagating inference in the tree via fast analytic gaussian approximation methods : this can make the overhead of bayesian inference manageable in domains such as go , while preserving high accuracy of expected-value estimates . we find substantial empirical outperformance of uct in an idealized bandit-tree test environment , where we can obtain valuable insights by comparing with known ground truth . additionally we rigorously prove on-policy and off-policy convergence of the proposed methods .", "topics": ["simulation", "ground truth"]}
{"title": "dataset and neural recurrent sequence labeling model for open-domain factoid question answering", "abstract": "while question answering ( qa ) with neural network , i.e . neural qa , has achieved promising results in recent years , lacking of large scale real-word qa dataset is still a challenge for developing and evaluating neural qa system . to alleviate this problem , we propose a large scale human annotated real-world qa dataset webqa with more than 42k questions and 556k evidences . as existing neural qa methods resolve qa either as sequence generation or classification/ranking problem , they face challenges of expensive softmax computation , unseen answers handling or separate candidate answer generation component . in this work , we cast neural qa as a sequence labeling problem and propose an end-to-end sequence labeling model , which overcomes all the above challenges . experimental results on webqa show that our model outperforms the baselines significantly with an f1 score of 74.69 % with word-based input , and the performance drops only 3.72 f1 points with more challenging character-based input .", "topics": ["computation", "end-to-end principle"]}
{"title": "empirical analysis of foundational distinctions in the web of data", "abstract": "a main difference between pre-web artificial intelligence and the current one is that the web and its semantic extension ( i.e . web of data ) contain open global-scale knowledge and make it available to potentially intelligent machines that may want to benefit from it . nevertheless , most of the web of data lacks ontological distinctions and has a sparse distribution of axiomatisations . for example , foundational distinctions such as whether an entity is inherently a class or an individual , or whether it is a physical object or not , are hardly expressed in the data , although they have been largely studied and formalised by foundational ontologies ( e.g . dolce , sumo ) . there is a gap between these ontologies , that often formalise or are inspired by preexisting philosophical theories and are developed with a top-down approach , and the web of data that is mostly derived from existing databases or from crowd-based effort ( e.g . dbpedia , wikidata , freebase ) . we investigate whether the web provides an empirical foundation for characterising entities of the web of data according to foundational distinctions . we want to answer questions such as `` is the dbpedia entity for dog a class or an instance ? '' we report on a set of experiments based on machine learning and crowdsourcing that show promising results .", "topics": ["entity", "sparse matrix"]}
{"title": "streamed learning : one-pass svms", "abstract": "we present a streaming model for large-scale classification ( in the context of $ \\ell_2 $ -svm ) by leveraging connections between learning and computational geometry . the streaming model imposes the constraint that only a single pass over the data is allowed . the $ \\ell_2 $ -svm is known to have an equivalent formulation in terms of the minimum enclosing ball ( meb ) problem , and an efficient algorithm based on the idea of \\emph { core sets } exists ( core vector machine , cvm ) . cvm learns a $ ( 1+\\varepsilon ) $ -approximate meb for a set of points and yields an approximate solution to corresponding svm instance . however cvm works in batch mode requiring multiple passes over the data . this paper presents a single-pass svm which is based on the minimum enclosing ball of streaming data . we show that the meb updates for the streaming case can be easily adapted to learn the svm weight vector in a way similar to using online stochastic gradient updates . our algorithm performs polylogarithmic computation at each example , and requires very small and constant storage . experimental results show that , even in such restrictive settings , we can learn efficiently in just one pass and get accuracies comparable to other state-of-the-art svm solvers ( batch and online ) . we also give an analysis of the algorithm , and discuss some open issues and possible extensions .", "topics": ["approximation algorithm", "support vector machine"]}
{"title": "the dlr hierarchy of approximate inference", "abstract": "we propose a hierarchy for approximate inference based on the dobrushin , lanford , ruelle ( dlr ) equations . this hierarchy includes existing algorithms , such as belief propagation , and also motivates novel algorithms such as factorized neighbors ( fn ) algorithms and variants of mean field ( mf ) algorithms . in particular , we show that extrema of the bethe free energy correspond to approximate solutions of the dlr equations . in addition , we demonstrate a close connection between these approximate algorithms and gibbs sampling . finally , we compare and contrast various of the algorithms in the dlr hierarchy on spin-glass problems . the experiments show that algorithms higher up in the hierarchy give more accurate results when they converge but tend to be less stable .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "long-term image boundary prediction", "abstract": "boundary estimation in images and videos has been a very active topic of research , and organizing visual information into boundaries and segments is believed to be a corner stone of visual perception . while prior work has focused on estimating boundaries for observed frames , our work aims at predicting boundaries of future unobserved frames . this requires our model to learn about the fate of boundaries and corresponding motion patterns -- including a notion of `` intuitive physics '' . we experiment on natural video sequences along with synthetic sequences with deterministic physics-based and agent-based motions . while not being our primary goal , we also show that fusion of rgb and boundary prediction leads to improved rgb predictions .", "topics": ["synthetic data"]}
{"title": "bayesian reinforcement learning : a survey", "abstract": "bayesian methods for machine learning have been widely investigated , yielding principled methods for incorporating prior information into inference algorithms . in this survey , we provide an in-depth review of the role of bayesian methods for the reinforcement learning ( rl ) paradigm . the major incentives for incorporating bayesian reasoning in rl are : 1 ) it provides an elegant approach to action-selection ( exploration/exploitation ) as a function of the uncertainty in learning ; and 2 ) it provides a machinery to incorporate prior knowledge into the algorithms . we first discuss models and methods for bayesian inference in the simple single-step bandit model . we then review the extensive recent literature on bayesian methods for model-based rl , where prior information can be expressed on the parameters of the markov model . we also present bayesian methods for model-free rl , where priors are expressed over the value function or policy class . the objective of the paper is to provide a comprehensive survey on bayesian rl algorithms and their theoretical and empirical properties .", "topics": ["reinforcement learning"]}
{"title": "fast and robust curve skeletonization for real-world elongated objects", "abstract": "we consider the problem of extracting curve skeletons of three-dimensional , elongated objects given a noisy surface , which has applications in agricultural contexts such as extracting the branching structure of plants . we describe an efficient and robust method based on breadth-first search that can determine curve skeletons in these contexts . our approach is capable of automatically detecting junction points as well as spurious segments and loops . all of that is accomplished with only one user-adjustable parameter . the run time of our method ranges from hundreds of milliseconds to less than four seconds on large , challenging datasets , which makes it appropriate for situations where real-time decision making is needed . experiments on synthetic models as well as on data from real world objects , some of which were collected in challenging field conditions , show that our approach compares favorably to classical thinning algorithms as well as to recent contributions to the field .", "topics": ["synthetic data"]}
{"title": "hypermedia learning objects system - on the way to a semantic educational web", "abstract": "while elearning systems become more and more popular in daily education , available applications lack opportunities to structure , annotate and manage their contents in a high-level fashion . general efforts to improve these deficits are taken by initiatives to define rich meta data sets and a semanticweb layer . in the present paper we introduce hylos , an online learning system . hylos is based on a cellular elearning object ( elo ) information model encapsulating meta data conforming to the lom standard . content management is provisioned on this semantic meta data level and allows for variable , dynamically adaptable access structures . context aware multifunctional links permit a systematic navigation depending on the learners and didactic needs , thereby exploring the capabilities of the semantic web . hylos is built upon the more general multimedia information repository ( mir ) and the mir adaptive context linking environment ( miracle ) , its linking extension . mir is an open system supporting the standards xml , corba and jndi . hylos benefits from manageable information structures , sophisticated access logic and high-level authoring tools like the elo editor responsible for the semi-manual creation of meta data and wysiwyg like content editing .", "topics": ["high- and low-level"]}
{"title": "improving variational encoder-decoders in dialogue generation", "abstract": "variational encoder-decoders ( veds ) have shown promising results in dialogue generation . however , the latent variable distributions are usually approximated by a much simpler model than the powerful rnn structure used for encoding and decoding , yielding the kl-vanishing problem and inconsistent training objective . in this paper , we separate the training step into two phases : the first phase learns to autoencode discrete texts into continuous embeddings , from which the second phase learns to generalize latent representations by reconstructing the encoded embedding . in this case , latent variables are sampled by transforming gaussian noise through multi-layer perceptrons and are trained with a separate ved model , which has the potential of realizing a much more flexible distribution . we compare our model with current popular models and the experiment demonstrates substantial improvement in both metric-based and human evaluations .", "topics": ["encoder"]}
{"title": "variational bayesian inference for hidden markov models with multivariate gaussian output distributions", "abstract": "hidden markov models ( hmm ) have been used for several years in many time series analysis or pattern recognitions tasks . hmm are often trained by means of the baum-welch algorithm which can be seen as a special variant of an expectation maximization ( em ) algorithm . second-order training techniques such as variational bayesian inference ( vi ) for probabilistic models regard the parameters of the probabilistic models as random variables and define distributions over these distribution parameters , hence the name of this technique . vi can also bee regarded as a special case of an em algorithm . in this article , we bring both together and train hmm with multivariate gaussian output distributions with vi . the article defines the new training technique for hmm . an evaluation based on some case studies and a comparison to related approaches is part of our ongoing work .", "topics": ["time series", "bayesian network"]}
{"title": "infiniteboost : building infinite ensembles with gradient descent", "abstract": "in machine learning ensemble methods have demonstrated high accuracy for the variety of problems in different areas . the most known algorithms intensively used in practice are random forests and gradient boosting . in this paper we present infiniteboost - a novel algorithm , which combines the best properties of these two approaches . the algorithm constructs the ensemble of trees for which two properties hold : trees of the ensemble incorporate the mistakes done by others ; at the same time the ensemble could contain the infinite number of trees without the over-fitting effect . the proposed algorithm is evaluated on the regression , classification , and ranking tasks using large scale , publicly available datasets .", "topics": ["statistical classification", "gradient descent"]}
{"title": "first-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns", "abstract": "we present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model . deep neural network acoustic models are now commonplace in hmm-based speech recognition systems , but building such systems is a complex , domain-specific task . recent work demonstrated the feasibility of discarding the hmm sequence modeling framework by directly predicting transcript text from audio . this paper extends this approach in two ways . first , we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy . second , we propose and evaluate a modified prefix-search decoding algorithm . this approach to decoding enables first-pass speech recognition with a language model , completely unaided by the cumbersome infrastructure of hmm-based systems . experiments on the wall street journal corpus demonstrate fairly competitive word error rates , and the importance of bi-directional network recurrence .", "topics": ["recurrent neural network", "speech recognition"]}
{"title": "benefits of depth for long-term memory of recurrent networks", "abstract": "the key attribute that drives the unprecedented success of modern recurrent neural networks ( rnns ) on learning tasks which involve sequential data , is their ever-improving ability to model intricate long-term temporal dependencies . however , an adequate measure of rnns long-term memory capacity is lacking , and thus formal understanding of their ability to correlate data throughout time is limited . though depth efficiency in convolutional networks is well established , it does not suffice in order to account for the success of deep rnns on data of varying lengths , and the need to address their `time-series expressive power ' arises . in this paper , we analyze the effect of depth on the ability of recurrent networks to express correlations ranging over long time-scales . to meet the above need , we introduce a measure of the information flow across time supported by the network , referred to as the start-end separation rank . this measure essentially reflects the distance of the function realized by the recurrent network from a function that models no interaction whatsoever between the beginning and end of the input sequence . we prove that deep recurrent networks support start-end separation ranks which are exponentially higher than those supported by their shallow counterparts . thus , we establish that depth brings forth an overwhelming advantage in the ability of recurrent networks to model long-term dependencies . such analyses may be readily extended to other rnn architectures of interest , e.g . variants of lstm networks . we obtain our results by considering a class of recurrent networks referred to as recurrent arithmetic circuits ( racs ) , which merge the hidden state with the input via the multiplicative integration operation . finally , we make use of the tool of quantum tensor networks to gain additional graphic insight regarding the complexity brought forth by depth in recurrent networks .", "topics": ["recurrent neural network", "time series"]}
{"title": "physics-guided neural networks ( pgnn ) : an application in lake temperature modeling", "abstract": "this paper introduces a novel framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery . this framework , termed as physics-guided neural network ( pgnn ) , leverages the output of physics-based model simulations along with observational features to generate predictions using a neural network architecture . further , this paper presents a novel framework for using physics-based loss functions in the learning objective of neural networks , to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set . we illustrate the effectiveness of pgnn for the problem of lake temperature modeling , where physical relationships between the temperature , density , and depth of water are used to design a physics-based loss function . by using scientific knowledge to guide the construction and learning of neural networks , we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results .", "topics": ["test set", "neural networks"]}
{"title": "ideological sublations : resolution of dialectic in population-based optimization", "abstract": "a population-based optimization algorithm was designed , inspired by two main thinking modes in philosophy , both based on dialectic concept and thesis-antithesis paradigm . they impose two different kinds of dialectics . idealistic and materialistic antitheses are formulated as optimization models . based on the models , the population is coordinated for dialectical interactions . at the population-based context , the formulated optimization models are reduced to a simple detection problem for each thinker ( particle ) . according to the assigned thinking mode to each thinker and her/his measurements of corresponding dialectic with other candidate particles , they deterministically decide to interact with a thinker in maximum dialectic with their theses . the position of a thinker at maximum dialectic is known as an available antithesis among the existing solutions . the dialectical interactions at each ideological community are distinguished by meaningful distributions of step-sizes for each thinking mode . in fact , the thinking modes are regarded as exploration and exploitation elements of the proposed algorithm . the result is a delicate balance without any requirement for adjustment of step-size coefficients . main parameter of the proposed algorithm is the number of particles appointed to each thinking modes , or equivalently for each kind of motions . an additional integer parameter is defined to boost the stability of the final algorithm in some particular problems . the proposed algorithm is evaluated by a testbed of 12 single-objective continuous benchmark functions . moreover , its performance and speed were highlighted in sparse reconstruction and antenna selection problems , at the context of compressed sensing and massive mimo , respectively . the results indicate fast and efficient performance in comparison with well-known evolutionary algorithms and dedicated state-of-the-art algorithms .", "topics": ["interaction", "sparse matrix"]}
{"title": "sparse distributed localized gradient fused features of objects", "abstract": "the sparse , hierarchical , and modular processing of natural signals is related to the ability of humans to recognize objects with high accuracy . in this study , we report a sparse feature processing and encoding method , which improved the recognition performance of an automated object recognition system . randomly distributed localized gradient enhanced features were selected before employing aggregate functions for representation , where we used a modular and hierarchical approach to detect the object features . these object features were combined with a minimum distance classifier , thereby obtaining object recognition system accuracies of 93 % using the amsterdam library of object images ( aloi ) database , 92 % using the columbia object image library ( coil ) -100 database , and 69 % using the pascal visual object challenge 2007 database . the object recognition performance was shown to be robust to variations in noise , object scaling , and object shifts . finally , a comparison with eight existing object recognition methods indicated that our new method improved the recognition accuracy by 10 % with aloi , 8 % with the coil-100 database , and 10 % with the pascal visual object challenge 2007 database .", "topics": ["sparse matrix", "database"]}
{"title": "network topology and time criticality effects in the modularised fleet mix problem", "abstract": "in this paper , we explore the interplay between network topology and time criticality in a military logistics system . a general goal of this work ( and previous work ) is to evaluate land transportation requirements or , more specifically , how to design appropriate fleets of military general service vehicles that are tasked with the supply and re-supply of military units dispersed in an area of operation . the particular focus of this paper is to gain a better understanding of how the logistics environment changes when current army vehicles with fixed transport characteristics are replaced by a new generation of modularised vehicles that can be configured task-specifically . the experimental work is conducted within a well developed strategic planning simulation environment which includes a scenario generation engine for automatically sampling supply and re-supply missions and a multi-objective meta-heuristic search algorithm ( i.e . evolutionary algorithm ) for solving the particular scheduling and routing problems . the results presented in this paper allow for a better understanding of how ( and under what conditions ) a modularised vehicle fleet can provide advantages over the currently implemented system .", "topics": ["sampling ( signal processing )", "simulation"]}
{"title": "multi-planar deep segmentation networks for cardiac substructures from mri and ct", "abstract": "non-invasive detection of cardiovascular disorders from radiology scans requires quantitative image analysis of the heart and its substructures . there are well-established measurements that radiologists use for diseases assessment such as ejection fraction , volume of four chambers , and myocardium mass . these measurements are derived as outcomes of precise segmentation of the heart and its substructures . the aim of this paper is to provide such measurements through an accurate image segmentation algorithm that automatically delineates seven substructures of the heart from mri and/or ct scans . our proposed method is based on multi-planar deep convolutional neural networks ( cnn ) with an adaptive fusion strategy where we automatically utilize complementary information from different planes of the 3d scans for improved delineations . for ct and mri , we have separately designed three cnns ( the same architectural configuration ) for three planes , and have trained the networks from scratch for voxel-wise labeling for the following cardiac structures : myocardium of left ventricle ( myo ) , left atrium ( la ) , left ventricle ( lv ) , right atrium ( ra ) , right ventricle ( rv ) , ascending aorta ( ao ) , and main pulmonary artery ( pa ) . we have evaluated the proposed method with 4-fold-cross validation on the multi-modality whole heart segmentation challenge ( mm-whs 2017 ) dataset . the precision and dice index of 0.93 and 0.90 , and 0.87 and 0.85 were achieved for ct and mr images , respectively . while a ct volume was segmented about 50 seconds , an mri scan was segmented around 17 seconds with the gpus/cuda implementation .", "topics": ["image segmentation"]}
{"title": "tensor decomposition for signal processing and machine learning", "abstract": "tensors or { \\em multi-way arrays } are functions of three or more indices $ ( i , j , k , \\cdots ) $ -- similar to matrices ( two-way arrays ) , which are functions of two indices $ ( r , c ) $ for ( row , column ) . tensors have a rich history , stretching over almost a century , and touching upon numerous disciplines ; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing , statistics , data mining and machine learning . this overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors . as such , it focuses on fundamentals and motivation ( using various application examples ) , aiming to strike an appropriate balance of breadth { \\em and depth } that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software . some background in applied optimization is useful but not strictly required . the material covered includes tensor rank and rank decomposition ; basic tensor factorization models and their relationships and properties ( including fairly good coverage of identifiability ) ; broad coverage of algorithms ranging from alternating optimization to stochastic gradient ; statistical performance analysis ; and applications ranging from source separation to collaborative filtering , mixture and topic modeling , classification , and multilinear subspace learning .", "topics": ["data mining", "statistical classification"]}
{"title": "embedding of blink frequency in electrooculography signal using difference expansion based reversible watermarking technique", "abstract": "in the past few years , like other fields , rapid expansion of digitization and globalization has influenced the medical field as well . for progress of diagnostic results most of the reputed hospitals and diagnostic centres all over the world have started exchanging medical information . in this proposed method , the calculated diagnostic parametric values of the original electrooculography ( eog ) signal are embedded as a watermark by using difference expansion ( de ) algorithm based reversible watermarking technique . the extracted watermark provides the required parametric values at the recipient end without any post computation of the recovered eog signal . by computing the parametric values from the recovered signal , the integrity of the extracted watermark can be validated . the time domain features of eog signal are calculated for the generation of watermark . in the current work , various features are studied and two major features related to blink frequency are used to generate the watermark . the high signal to noise ratio ( snr ) and the bit error rate ( ber ) claim the robustness of the proposed method .", "topics": ["computation"]}
{"title": "boost phrase-level polarity labelling with review-level sentiment classification", "abstract": "sentiment analysis on user reviews helps to keep track of user reactions towards products , and make advices to users about what to buy . state-of-the-art review-level sentiment classification techniques could give pretty good precisions of above 90 % . however , current phrase-level sentiment analysis approaches might only give sentiment polarity labelling precisions of around 70 % ~80 % , which is far from satisfaction and restricts its application in many practical tasks . in this paper , we focus on the problem of phrase-level sentiment polarity labelling and attempt to bridge the gap between phrase-level and review-level sentiment analysis . we investigate the inconsistency between the numerical star ratings and the sentiment orientation of textual user reviews . although they have long been treated as identical , which serves as a basic assumption in previous work , we find that this assumption is not necessarily true . we further propose to leverage the results of review-level sentiment classification to boost the performance of phrase-level polarity labelling using a novel constrained convex optimization framework . besides , the framework is capable of integrating various kinds of information sources and heuristics , while giving the global optimal solution due to its convexity . experimental results on both english and chinese reviews show that our framework achieves high labelling precisions of up to 89 % , which is a significant improvement from current approaches .", "topics": ["numerical analysis", "optimization problem"]}
{"title": "safe , multi-agent , reinforcement learning for autonomous driving", "abstract": "autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking , giving way , merging , taking left and right turns and while pushing ahead in unstructured urban roadways . since there are many possible scenarios , manually tackling all possible cases will likely yield a too simplistic policy . moreover , one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained . in this paper we apply deep reinforcement learning to the problem of forming long term driving strategies . we note that there are two major challenges that make autonomous driving different from other robotic tasks . first , is the necessity for ensuring functional safety - something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances . second , the markov decision process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario . we make three contributions in our work . first , we show how policy gradient iterations can be used without markovian assumptions . second , we decompose the problem into a composition of a policy for desires ( which is to be learned ) and trajectory planning with hard constraints ( which is not learned ) . the goal of desires is to enable comfort of driving , while hard constraints guarantees the safety of driving . third , we introduce a hierarchical temporal abstraction we call an `` option graph '' with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "screening tests for lasso problems", "abstract": "this paper is a survey of dictionary screening for the lasso problem . the lasso problem seeks a sparse linear combination of the columns of a dictionary to best match a given target vector . this sparse representation has proven useful in a variety of subsequent processing and decision tasks . for a given target vector , dictionary screening quickly identifies a subset of dictionary columns that will receive zero weight in a solution of the corresponding lasso problem . these columns can be removed from the dictionary prior to solving the lasso problem without impacting the optimality of the solution obtained . this has two potential advantages : it reduces the size of the dictionary , allowing the lasso problem to be solved with less resources , and it may speed up obtaining a solution . using a geometrically intuitive framework , we provide basic insights for understanding useful lasso screening tests and their limitations . we also provide illustrative numerical studies on several datasets .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "coordinated online learning with applications to learning user preferences", "abstract": "we study an online multi-task learning setting , in which instances of related tasks arrive sequentially , and are handled by task-specific online learners . we consider an algorithmic framework to model the relationship of these tasks via a set of convex constraints . to exploit this relationship , we design a novel algorithm -- cool -- for coordinating the individual online learners : our key idea is to coordinate their parameters via weighted projections onto a convex set . by adjusting the rate and accuracy of the projection , the cool algorithm allows for a trade-off between the benefit of coordination and the required computation/communication . we derive regret bounds for our approach and analyze how they are influenced by these trade-off factors . we apply our results on the application of learning users ' preferences on the airbnb marketplace with the goal of incentivizing users to explore under-reviewed apartments .", "topics": ["regret ( decision theory )", "computation"]}
{"title": "mining relevant interval rules", "abstract": "this article extends the method of garriga et al . for mining relevant rules to numerical attributes by extracting interval-based pattern rules . we propose an algorithm that extracts such rules from numerical datasets using the interval-pattern approach from kaytoue et al . this algorithm has been implemented and evaluated on real datasets .", "topics": ["numerical analysis"]}
{"title": "non-negative matrix factorizations for multiplex network analysis", "abstract": "networks have been a general tool for representing , analyzing , and modeling relational data arising in several domains . one of the most important aspect of network analysis is community detection or network clustering . until recently , the major focus have been on discovering community structure in single ( i.e . , monoplex ) networks . however , with the advent of relational data with multiple modalities , multiplex networks , i.e . , networks composed of multiple layers representing different aspects of relations , have emerged . consequently , community detection in multiplex network , i.e . , detecting clusters of nodes shared by all layers , has become a new challenge . in this paper , we propose network fusion for composite community extraction ( nf-cce ) , a new class of algorithms , based on four different non-negative matrix factorization models , capable of extracting composite communities in multiplex networks . each algorithm works in two steps : first , it finds a non-negative , low-dimensional feature representation of each network layer ; then , it fuses the feature representation of layers into a common non-negative , low-dimensional feature representation via collective factorization . the composite clusters are extracted from the common feature representation . we demonstrate the superior performance of our algorithms over the state-of-the-art methods on various types of multiplex networks , including biological , social , economic , citation , phone communication , and brain multiplex networks .", "topics": ["cluster analysis"]}
{"title": "network in network", "abstract": "we propose a novel deep network structure called `` network in network '' ( nin ) to enhance model discriminability for local patches within the receptive field . the conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input . instead , we build micro neural networks with more complex structures to abstract the data within the receptive field . we instantiate the micro neural network with a multilayer perceptron , which is a potent function approximator . the feature maps are obtained by sliding the micro networks over the input in a similar manner as cnn ; they are then fed into the next layer . deep nin can be implemented by stacking mutiple of the above described structure . with enhanced local modeling via the micro network , we are able to utilize global average pooling over feature maps in the classification layer , which is easier to interpret and less prone to overfitting than traditional fully connected layers . we demonstrated the state-of-the-art classification performances with nin on cifar-10 and cifar-100 , and reasonable performances on svhn and mnist datasets .", "topics": ["nonlinear system", "map"]}
{"title": "a comparison of gap statistic definitions with and without logarithm function", "abstract": "the gap statistic is a standard method for determining the number of clusters in a set of data . the gap statistic standardizes the graph of $ \\log ( w_ { k } ) $ , where $ w_ { k } $ is the within-cluster dispersion , by comparing it to its expectation under an appropriate null reference distribution of the data . we suggest to use $ w_ { k } $ instead of $ \\log ( w_ { k } ) $ , and to compare it to the expectation of $ w_ { k } $ under a null reference distribution . in fact , whenever a number fulfills the original gap statistic inequality , this number also fulfills the inequality of a gap statistic using $ w_ { k } $ , but not \\textit { vice versa } . the two definitions of the gap function are evaluated on several simulated data sets and on a real data of dce-mr images .", "topics": ["simulation"]}
{"title": "a novel framework for robustness analysis of visual qa models", "abstract": "deep neural networks have been playing an essential role in many computer vision tasks including visual question answering ( vqa ) . until recently , the study of their accuracy has been the main focus of research and now there is a huge trend toward assessing the robustness of these models against adversarial attacks by evaluating the accuracy of these models under increasing levels of noisiness . in vqa , the attack can target the image and/or the proposed main question and yet there is a lack of proper analysis of this aspect of vqa . in this work , we propose a new framework that uses semantically relevant questions , dubbed basic questions , acting as noise to evaluate the robustness of vqa models . we hypothesize that as the similarity of a basic question to the main question decreases , the level of noise increases . so , to generate a reasonable noise level for a given main question , we rank a pool of basic questions based on their similarity with this main question . we cast this ranking problem as a lasso optimization problem . we also propose a novel robustness measure r_score and two large-scale question datasets , general basic question dataset and yes/no basic question dataset in order to standardize robustness analysis of vqa models . we analyze the robustness of several state-of-the-art vqa models and show that attention-based vqa models are more robust than other methods in general . the main goal of this framework is to serve as a benchmark to help the community in building more accurate and robust vqa models .", "topics": ["optimization problem", "computer vision"]}
{"title": "learning and visualizing localized geometric features using 3d-cnn : an application to manufacturability analysis of drilled holes", "abstract": "3d convolutional neural networks ( 3d-cnn ) have been used for object recognition based on the voxelized shape of an object . however , interpreting the decision making process of these 3d-cnns is still an infeasible task . in this paper , we present a unique 3d-cnn based gradient-weighted class activation mapping method ( 3d-gradcam ) for visual explanations of the distinct local geometric features of interest within an object . to enable efficient learning of 3d geometries , we augment the voxel data with surface normals of the object boundary . we then train a 3d-cnn with this augmented data and identify the local features critical for decision-making using 3d gradcam . an application of this feature identification framework is to recognize difficult-to-manufacture drilled hole features in a complex cad geometry . the framework can be extended to identify difficult-to-manufacture features at multiple spatial scales leading to a real-time design for manufacturability decision support system .", "topics": ["gradient"]}
{"title": "scalable estimation of dirichlet process mixture models on distributed data", "abstract": "we consider the estimation of dirichlet process mixture models ( dpmms ) in distributed environments , where data are distributed across multiple computing nodes . a key advantage of bayesian nonparametric models such as dpmms is that they allow new components to be introduced on the fly as needed . this , however , posts an important challenge to distributed estimation -- how to handle new components efficiently and consistently . to tackle this problem , we propose a new estimation method , which allows new components to be created locally in individual computing nodes . components corresponding to the same cluster will be identified and merged via a probabilistic consolidation scheme . in this way , we can maintain the consistency of estimation with very low communication cost . experiments on large real-world data sets show that the proposed method can achieve high scalability in distributed and asynchronous environments without compromising the mixing performance .", "topics": ["scalability", "bayesian network"]}
{"title": "disentangling space and time in video with hierarchical variational auto-encoders", "abstract": "there are many forms of feature information present in video data . principle among them are object identity information which is largely static across multiple video frames , and object pose and style information which continuously transforms from frame to frame . most existing models confound these two types of representation by mapping them to a shared feature space . in this paper we propose a probabilistic approach for learning separable representations of object identity and pose information using unsupervised video data . our approach leverages a deep generative model with a factored prior distribution that encodes properties of temporal invariances in the hidden feature set . learning is achieved via variational inference . we present results of learning identity and pose information on a dataset of moving characters as well as a dataset of rotating 3d objects . our experimental results demonstrate our model 's success in factoring its representation , and demonstrate that the model achieves improved performance in transfer learning tasks .", "topics": ["generative model", "calculus of variations"]}
{"title": "theoretical impediments to machine learning with seven sparks from the causal revolution", "abstract": "current machine learning systems operate , almost exclusively , in a statistical , or model-free mode , which entails severe theoretical limits on their power and performance . such systems can not reason about interventions and retrospection and , therefore , can not serve as the basis for strong ai . to achieve human level intelligence , learning machines need the guidance of a model of reality , similar to the ones used in causal inference tasks . to demonstrate the essential role of such models , i will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling .", "topics": ["artificial intelligence", "causality"]}
{"title": "adversarial message passing for graphical models", "abstract": "bayesian inference on structured models typically relies on the ability to infer posterior distributions of underlying hidden variables . however , inference in implicit models or complex posterior distributions is hard . a popular tool for learning implicit models are generative adversarial networks ( gans ) which learn parameters of generators by fooling discriminators . typically , gans are considered to be models themselves and are not understood in the context of inference . current techniques rely on inefficient global discrimination of joint distributions to perform learning , or only consider discriminating a single output variable . we overcome these limitations by treating gans as a basis for likelihood-free inference in generative models and generalize them to bayesian posterior inference over factor graphs . we propose local learning rules based on message passing minimizing a global divergence criterion involving cooperating local adversaries used to sidestep explicit likelihood evaluations . this allows us to compose models and yields a unified inference and learning framework for adversarial learning . our framework treats model specification and inference separately and facilitates richly structured models within the family of directed acyclic graphs , including components such as intractable likelihoods , non-differentiable models , simulators and generally cumbersome models . a key result of our treatment is the insight that bayesian inference on structured models can be performed only with sampling and discrimination when using nonparametric variational families , without access to explicit distributions . as a side-result , we discuss the link to likelihood maximization . these approaches hold promise to be useful in the toolbox of probabilistic modelers and enrich the gamut of current probabilistic programming applications .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "discriminative similarity for clustering and semi-supervised learning", "abstract": "similarity-based clustering and semi-supervised learning methods separate the data into clusters or classes according to the pairwise similarity between the data , and the pairwise similarity is crucial for their performance . in this paper , we propose a novel discriminative similarity learning framework which learns discriminative similarity for either data clustering or semi-supervised learning . the proposed framework learns classifier from each hypothetical labeling , and searches for the optimal labeling by minimizing the generalization error of the learned classifiers associated with the hypothetical labeling . kernel classifier is employed in our framework . by generalization analysis via rademacher complexity , the generalization error bound for the kernel classifier learned from hypothetical labeling is expressed as the sum of pairwise similarity between the data from different classes , parameterized by the weights of the kernel classifier . such pairwise similarity serves as the discriminative similarity for the purpose of clustering and semi-supervised learning , and discriminative similarity with similar form can also be induced by the integrated squared error bound for kernel density classification . based on the discriminative similarity induced by the kernel classifier , we propose new clustering and semi-supervised learning methods .", "topics": ["kernel ( operating system )", "supervised learning"]}
{"title": "mining for causal relationships : a data-driven study of the islamic state", "abstract": "the islamic state of iraq and al-sham ( isis ) is a dominant insurgent group operating in iraq and syria that rose to prominence when it took over mosul in june , 2014 . in this paper , we present a data-driven approach to analyzing this group using a dataset consisting of 2200 incidents of military activity surrounding isis and the forces that oppose it ( including iraqi , syrian , and the american-led coalition ) . we combine ideas from logic programming and causal reasoning to mine for association rules for which we present evidence of causality . we present relationships that link isis vehicle-bourne improvised explosive device ( vbied ) activity in syria with military operations in iraq , coalition air strikes , and isis ied activity , as well as rules that may serve as indicators of spikes in indirect fire , suicide attacks , and arrests .", "topics": ["causality"]}
{"title": "adversarially regularized autoencoders", "abstract": "while autoencoders are a key technique in representation learning for continuous structures , such as images or wave forms , developing general-purpose autoencoders for discrete structures , such as text sequence or discretized images , has proven to be more challenging . in particular , discrete inputs make it more difficult to learn a smooth encoder that preserves the complex local relationships in the input space . in this work , we propose an adversarially regularized autoencoder ( arae ) with the goal of learning more robust discrete-space representations . arae jointly trains both a rich discrete-space encoder , such as an rnn , and a simpler continuous space generator function , while using generative adversarial network ( gan ) training to constrain the distributions to be similar . this method yields a smoother contracted code space that maps similar inputs to nearby codes , and also an implicit latent variable gan model for generation . experiments on text and discretized images demonstrate that the gan model produces clean interpolations and captures the multimodality of the original space , and that the autoencoder produces improve- ments in semi-supervised learning as well as state-of-the-art results in unaligned text style transfer task using only a shared continuous-space representation .", "topics": ["feature learning", "supervised learning"]}
{"title": "the simulator : understanding adaptive sampling in the moderate-confidence regime", "abstract": "we propose a novel technique for analyzing adaptive sampling called the { \\em simulator } . our approach differs from the existing methods by considering not how much information could be gathered by any fixed sampling strategy , but how difficult it is to distinguish a good sampling strategy from a bad one given the limited amount of data collected up to any given time . this change of perspective allows us to match the strength of both fano and change-of-measure techniques , without succumbing to the limitations of either method . for concreteness , we apply our techniques to a structured multi-arm bandit problem in the fixed-confidence pure exploration setting , where we show that the constraints on the means imply a substantial gap between the moderate-confidence sample complexity , and the asymptotic sample complexity as $ \\delta \\to 0 $ found in the literature . we also prove the first instance-based lower bounds for the top-k problem which incorporate the appropriate log-factors . moreover , our lower bounds zero-in on the number of times each \\emph { individual } arm needs to be pulled , uncovering new phenomena which are drowned out in the aggregate sample complexity . our new analysis inspires a simple and near-optimal algorithm for the best-arm and top-k identification , the first { \\em practical } algorithm of its kind for the latter problem which removes extraneous log factors , and outperforms the state-of-the-art in experiments .", "topics": ["sampling ( signal processing )"]}
{"title": "using probabilistic programs as proposals", "abstract": "monte carlo inference has asymptotic guarantees , but can be slow when using generic proposals . handcrafted proposals that rely on user knowledge about the posterior distribution can be efficient , but are difficult to derive and implement . this paper proposes to let users express their posterior knowledge in the form of proposal programs , which are samplers written in probabilistic programming languages . one strategy for writing good proposal programs is to combine domain-specific heuristic algorithms with neural network models . the heuristics identify high probability regions , and the neural networks model the posterior uncertainty around the outputs of the algorithm . proposal programs can be used as proposal distributions in importance sampling and metropolis-hastings samplers without sacrificing asymptotic consistency , and can be optimized offline using inference compilation . support for optimizing and using proposal programs is easily implemented in a sampling-based probabilistic programming runtime . the paper illustrates the proposed technique with a proposal program that combines ransac and neural networks to accelerate inference in a bayesian linear regression with outliers model .", "topics": ["heuristic"]}
{"title": "fairness in learning : classic and contextual bandits", "abstract": "we introduce the study of fairness in multi-armed bandit problems . our fairness definition can be interpreted as demanding that given a pool of applicants ( say , for college admission or mortgages ) , a worse applicant is never favored over a better one , despite a learning algorithm 's uncertainty over the true payoffs . we prove results of two types . first , in the important special case of the classic stochastic bandits problem ( i.e . , in which there are no contexts ) , we provide a provably fair algorithm based on `` chained '' confidence intervals , and provide a cumulative regret bound with a cubic dependence on the number of arms . we further show that any fair algorithm must have such a dependence . when combined with regret bounds for standard non-fair algorithms such as ucb , this proves a strong separation between fair and unfair learning , which extends to the general contextual case . in the general contextual case , we prove a tight connection between fairness and the kwik ( knows what it knows ) learning model : a kwik algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm , and conversely any fair contextual bandit algorithm can be transformed into a kwik learning algorithm . this tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension , and to show ( for a different class of functions ) a worst-case exponential gap in regret between fair and non-fair learning algorithms", "topics": ["time complexity", "polynomial"]}
{"title": "an analytically tractable bayesian approximation to optimal point process filtering", "abstract": "the process of dynamic state estimation ( filtering ) based on point process observations is in general intractable . numerical sampling techniques are often practically useful , but lead to limited conceptual insight about optimal encoding/decoding strategies , which are of significant relevance to computational neuroscience . we develop an analytically tractable bayesian approximation to optimal filtering based on point process observations , which allows us to introduce distributional assumptions about sensory cell properties , that greatly facilitates the analysis of optimal encoding in situations deviating from common assumptions of uniform coding . the analytic framework leads to insights which are difficult to obtain from numerical algorithms , and is consistent with experiments about the distribution of tuning curve centers . interestingly , we find that the information gained from the absence of spikes may be crucial to performance .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "linear convergence of sdca in statistical estimation", "abstract": "in this paper , we consider stochastic dual coordinate ( sdca ) { \\em without } strongly convex assumption or convex assumption . we show that sdca converges linearly under mild conditions termed restricted strong convexity . this covers a wide array of popular statistical models including lasso , group lasso , and logistic regression with $ \\ell_1 $ regularization , corrected lasso and linear regression with scad regularizer . this significantly improves previous convergence results on sdca for problems that are not strongly convex . as a by product , we derive a dual free form of sdca that can handle general regularization term , which is of interest by itself .", "topics": ["matrix regularization"]}
{"title": "on the information in spike timing : neural codes derived from polychronous groups", "abstract": "there is growing evidence regarding the importance of spike timing in neural information processing , with even a small number of spikes carrying information , but computational models lag significantly behind those for rate coding . experimental evidence on neuronal behavior is consistent with the dynamical and state dependent behavior provided by recurrent connections . this motivates the minimalistic abstraction investigated in this paper , aimed at providing insight into information encoding in spike timing via recurrent connections . we employ information-theoretic techniques for a simple reservoir model which encodes input spatiotemporal patterns into a sparse neural code , translating the polychronous groups introduced by izhikevich into codewords on which we can perform standard vector operations . we show that the distance properties of the code are similar to those for ( optimal ) random codes . in particular , the code meets benchmarks associated with both linear classification and capacity , with the latter scaling exponentially with reservoir size .", "topics": ["recurrent neural network", "sparse matrix"]}
{"title": "toward the coevolution of novel vertical-axis wind turbines", "abstract": "the production of renewable and sustainable energy is one of the most important challenges currently facing mankind . wind has made an increasing contribution to the world 's energy supply mix , but still remains a long way from reaching its full potential . in this paper , we investigate the use of artificial evolution to design vertical-axis wind turbine prototypes that are physically instantiated and evaluated under fan generated wind conditions . initially a conventional evolutionary algorithm is used to explore the design space of a single wind turbine and later a cooperative coevolutionary algorithm is used to explore the design space of an array of wind turbines . artificial neural networks are used throughout as surrogate models to assist learning and found to reduce the number of fabrications required to reach a higher aerodynamic efficiency . unlike in other approaches , such as computational fluid dynamics simulations , no mathematical formulations are used and no model assumptions are made .", "topics": ["simulation"]}
{"title": "hierarchical compositional feature learning", "abstract": "we introduce the hierarchical compositional network ( hcn ) , a directed generative model able to discover and disentangle , without supervision , the building blocks of a set of binary images . the building blocks are binary features defined hierarchically as a composition of some of the features in the layer immediately below , arranged in a particular manner . at a high level , hcn is similar to a sigmoid belief network with pooling . inference and learning in hcn are very challenging and existing variational approximations do not work satisfactorily . a main contribution of this work is to show that both can be addressed using max-product message passing ( mpmp ) with a particular schedule ( no em required ) . also , using mpmp as an inference engine for hcn makes new tasks simple : adding supervision information , classifying images , or performing inpainting all correspond to clamping some variables of the model to their known values and running mpmp on the rest . when used for classification , fast inference with hcn has exactly the same functional form as a convolutional neural network ( cnn ) with linear activations and binary weights . however , hcn 's features are qualitatively very different .", "topics": ["generative model", "feature learning"]}
{"title": "distributed constrained optimization with semicoordinate transformations", "abstract": "recent work has shown how information theory extends conventional full-rationality game theory to allow bounded rational agents . the associated mathematical framework can be used to solve constrained optimization problems . this is done by translating the problem into an iterated game , where each agent controls a different variable of the problem , so that the joint probability distribution across the agents ' moves gives an expected value of the objective function . the dynamics of the agents is designed to minimize a lagrangian function of that joint distribution . here we illustrate how the updating of the lagrange parameters in the lagrangian is a form of automated annealing , which focuses the joint distribution more and more tightly about the joint moves that optimize the objective function . we then investigate the use of `` semicoordinate '' variable transformations . these separate the joint state of the agents from the variables of the optimization problem , with the two connected by an onto mapping . we present experiments illustrating the ability of such transformations to facilitate optimization . we focus on the special kind of transformation in which the statistically independent states of the agents induces a mixture distribution over the optimization variables . computer experiment illustrate this for $ k $ -sat constraint satisfaction problems and for unconstrained minimization of $ nk $ functions .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "semi-supervised learning for convolutional neural networks via online graph construction", "abstract": "the recent promising achievements of deep learning rely on the large amount of labeled data . considering the abundance of data on the web , most of them do not have labels at all . therefore , it is important to improve generalization performance using unlabeled data on supervised tasks with few labeled instances . in this work , we revisit graph-based semi-supervised learning algorithms and propose an online graph construction technique which suits deep convolutional neural network better . we consider an em-like algorithm for semi-supervised learning on deep neural networks : in forward pass , the graph is constructed based on the network output , and the graph is then used for loss calculation to help update the network by back propagation in the backward pass . we demonstrate the strength of our online approach compared to the conventional ones whose graph is constructed on static but not robust enough feature representations beforehand .", "topics": ["supervised learning"]}
{"title": "unifying visual-semantic embeddings with multimodal neural language models", "abstract": "inspired by recent advances in multimodal learning and machine translation , we introduce an encoder-decoder pipeline that learns ( a ) : a multimodal joint embedding space with images and text and ( b ) : a novel language model for decoding distributed representations from our space . our pipeline effectively unifies joint image-text embedding models with multimodal neural language models . we introduce the structure-content neural language model that disentangles the structure of a sentence to its content , conditioned on representations produced by the encoder . the encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch . using lstm to encode sentences , we match the state-of-the-art performance on flickr8k and flickr30k without using object detections . we also set new best results when using the 19-layer oxford convolutional network . furthermore we show that with linear encoders , the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g . *image of a blue car* - `` blue '' + `` red '' is near images of red cars . sample captions generated for 800 images are made available for comparison .", "topics": ["machine translation", "encoder"]}
{"title": "exploiting correlation and budget constraints in bayesian multi-armed bandit optimization", "abstract": "we address the problem of finding the maximizer of a nonlinear smooth function , that can only be evaluated point-wise , subject to constraints on the number of permitted function evaluations . this problem is also known as fixed-budget best arm identification in the multi-armed bandit literature . we introduce a bayesian approach for this problem and show that it empirically outperforms both the existing frequentist counterpart and other bayesian optimization methods . the bayesian approach places emphasis on detailed modelling , including the modelling of correlations among the arms . as a result , it can perform well in situations where the number of arms is much larger than the number of allowed function evaluation , whereas the frequentist counterpart is inapplicable . this feature enables us to develop and deploy practical applications , such as automatic machine learning toolboxes . the paper presents comprehensive comparisons of the proposed approach , thompson sampling , classical bayesian optimization techniques , more recent bayesian bandit approaches , and state-of-the-art best arm identification methods . this is the first comparison of many of these methods in the literature and allows us to examine the relative merits of their different features .", "topics": ["sampling ( signal processing )", "nonlinear system"]}
{"title": "a scalable multiclass algorithm for node classification", "abstract": "we introduce a scalable algorithm , mucca , for multiclass node classification in weighted graphs . unlike previously proposed methods for the same task , mucca works in time linear in the number of nodes . our approach is based on a game-theoretic formulation of the problem in which the test labels are expressed as a nash equilibrium of a certain game . however , in order to achieve scalability , we find the equilibrium on a spanning tree of the original graph . experiments on real-world data reveal that mucca is much faster than its competitors while achieving a similar predictive performance .", "topics": ["scalability"]}
{"title": "deep learning in customer churn prediction : unsupervised feature learning on abstract company independent feature vectors", "abstract": "as companies increase their efforts in retaining customers , being able to predict accurately ahead of time , whether a customer will churn in the foreseeable future is an extremely powerful tool for any marketing team . the paper describes in depth the application of deep learning in the problem of churn prediction . using abstract feature vectors , that can generated on any subscription based company 's user event logs , the paper proves that through the use of the intrinsic property of deep neural networks ( learning secondary features in an unsupervised manner ) , the complete pipeline can be applied to any subscription based company with extremely good churn predictive performance . furthermore the research documented in the paper was performed for framed data ( a company that sells churn prediction as a service for other companies ) in conjunction with the data science institute at lancaster university , uk . this paper is the intellectual property of framed data .", "topics": ["feature learning", "neural networks"]}
{"title": "dense associative memory is robust to adversarial inputs", "abstract": "deep neural networks ( dnn ) trained in a supervised way suffer from two known problems . first , the minima of the objective function used in learning correspond to data points ( also known as rubbish examples or fooling images ) that lack semantic similarity with the training data . second , a clean input can be changed by a small , and often imperceptible for human vision , perturbation , so that the resulting deformed input is misclassified by the network . these findings emphasize the differences between the ways dnn and humans classify patterns , and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods . our paper examines these questions within the framework of dense associative memory ( dam ) models . these models are defined by the energy function , with higher order ( higher than quadratic ) interactions between the neurons . we show that in the limit when the power of the interaction vertex in the energy function is sufficiently large , these models have the following three properties . first , the minima of the objective function are free from rubbish images , so that each minimum is a semantically meaningful pattern . second , artificial patterns poised precisely at the decision boundary look ambiguous to human subjects and share aspects of both classes that are separated by that decision boundary . third , adversarial images constructed by models with small power of the interaction vertex , which are equivalent to dnn with rectified linear units ( relu ) , fail to transfer to and fool the models with higher order interactions . this opens up a possibility to use higher order models for detecting and stopping malicious adversarial attacks . the presented results suggest that dam with higher order energy functions are closer to human visual perception than dnn with relus .", "topics": ["test set", "supervised learning"]}
{"title": "a stochastic model of human visual attention with a dynamic bayesian network", "abstract": "recent studies in the field of human vision science suggest that the human responses to the stimuli on a visual display are non-deterministic . people may attend to different locations on the same visual input at the same time . based on this knowledge , we propose a new stochastic model of visual attention by introducing a dynamic bayesian network to predict the likelihood of where humans typically focus on a video scene . the proposed model is composed of a dynamic bayesian network with 4 layers . our model provides a framework that simulates and combines the visual saliency response and the cognitive state of a person to estimate the most probable attended regions . sample-based inference with markov chain monte-carlo based particle filter and stream processing with multi-core processors enable us to estimate human visual attention in near real time . experimental results have demonstrated that our model performs significantly better in predicting human visual attention compared to the previous deterministic models .", "topics": ["bayesian network", "markov chain"]}
{"title": "reading stockholm riots 2013 in social media by text-mining", "abstract": "the riots in stockholm in may 2013 were an event that reverberated in the world media for its dimension of violence that had spread through the swedish capital . in this study we have investigated the role of social media in creating media phenomena via text mining and natural language processing . we have focused on two channels of communication for our analysis : twitter and poloniainfo.se ( forum of polish community in sweden ) . our preliminary results show some hot topics driving discussion related mostly to swedish police and swedish politics by counting word usage . typical features for media intervention are presented . we have built networks of most popular phrases , clustered by categories ( geography , media institution , etc . ) . sentiment analysis shows negative connotation with police . the aim of this preliminary exploratory quantitative study was to generate questions and hypotheses , which we could carefully follow by deeper more qualitative methods .", "topics": ["natural language processing"]}
{"title": "enhancing the retrieval performance by combing the texture and edge features", "abstract": "in this paper , anew algorithm which is based on geometrical moments and local binary patterns ( lbp ) for content based image retrieval ( cbir ) is proposed . in geometrical moments , each vector is compared with the all other vectors for edge map generation . the same concept is utilized at lbp calculation which is generating nine lbp patterns from a given 3x3 pattern . finally , nine lbp histograms are calculated which are used as a feature vector for image retrieval . moments are important features used in recognition of different types of images . two experiments have been carried out for proving the worth of our algorithm . the results after being investigated shows a significant improvement in terms of their evaluation measures as compared to lbp and other existing transform domain techniques .", "topics": ["feature vector", "feature extraction"]}
{"title": "fuzzy bayesian learning", "abstract": "in this paper we propose a novel approach for learning from data using rule based fuzzy inference systems where the model parameters are estimated using bayesian inference and markov chain monte carlo ( mcmc ) techniques . we show the applicability of the method for regression and classification tasks using synthetic data-sets and also a real world example in the financial services industry . then we demonstrate how the method can be extended for knowledge extraction to select the individual rules in a bayesian way which best explains the given data . finally we discuss the advantages and pitfalls of using this method over state-of-the-art techniques and highlight the specific class of problems where this would be useful .", "topics": ["synthetic data", "markov chain"]}
{"title": "sockeye : a toolkit for neural machine translation", "abstract": "we describe sockeye ( version 1.12 ) , an open-source sequence-to-sequence toolkit for neural machine translation ( nmt ) . sockeye is a production-ready framework for training and applying models as well as an experimental platform for researchers . written in python and built on mxnet , the toolkit offers scalable training and inference for the three most prominent encoder-decoder architectures : attentional recurrent neural networks , self-attentional transformers , and fully convolutional networks . sockeye also supports a wide range of optimizers , normalization and regularization techniques , and inference improvements from current nmt literature . users can easily run standard training recipes , explore different model settings , and incorporate new ideas . in this paper , we highlight sockeye 's features and benchmark it against other nmt toolkits on two language arcs from the 2017 conference on machine translation ( wmt ) : english-german and latvian-english . we report competitive bleu scores across all three architectures , including an overall best score for sockeye 's transformer implementation . to facilitate further comparison , we release all system outputs and training scripts used in our experiments . the sockeye toolkit is free software released under the apache 2.0 license .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "on chord and sagitta in $ { \\mathbb z } ^2 $ : an analysis towards fast and robust circular arc detection", "abstract": "although chord and sagitta , when considered in tandem , may reflect many underlying geometric properties of circles on the euclidean plane , their implications on the digital plane are not yet well-understood . in this paper , we explore some of their fundamental properties on the digital plane that have a strong bearing on the unsupervised detection of circles and circular arcs in a digital image . we show that although the chord-and-sagitta properties of a real circle do not readily migrate to the digital plane , they can indeed be used for the analysis in the discrete domain based on certain bounds on their deviations , which are derived from the real domain . in particular , we derive an upper bound on the circumferential angular deviation of a point in the context of chord property , and an upper bound on the relative error in radius estimation with regard to the sagitta property . using these two bounds , we design a novel algorithm for the detection and parameterization of circles and circular arcs , which does not require any heuristic initialization or manual tuning . the chord property is deployed for the detection of circular arcs , whereas the sagitta property is used to estimate their centers and radii . finally , to improve the accuracy of estimation , the notion of restricted hough transform is used . experimental results demonstrate superior efficiency and robustness of the proposed methodology compared to existing techniques .", "topics": ["heuristic"]}
{"title": "niftynet : a deep-learning platform for medical imaging", "abstract": "medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions . established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this application requires substantial implementation effort . thus , there has been substantial duplication of effort and incompatible infrastructure developed across many research groups . this work presents the open-source niftynet platform for deep learning in medical imaging . the ambition of niftynet is to accelerate and simplify the development of these solutions , and to provide a common mechanism for disseminating research outputs for the community to use , adapt and build upon . niftynet provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation , regression , image generation and representation learning applications . components of the niftynet pipeline including data loading , data augmentation , network architectures , loss functions and evaluation metrics are tailored to , and take advantage of , the idiosyncracies of medical image analysis and computer-assisted intervention . niftynet is built on tensorflow and supports tensorboard visualization of 2d and 3d images and computational graphs by default . we present 3 illustrative medical image analysis applications built using niftynet : ( 1 ) segmentation of multiple abdominal organs from computed tomography ; ( 2 ) image regression to predict computed tomography attenuation maps from brain magnetic resonance images ; and ( 3 ) generation of simulated ultrasound images for specified anatomical poses . niftynet enables researchers to rapidly develop and distribute deep learning solutions for segmentation , regression , image generation and representation learning applications , or extend the platform to new applications .", "topics": ["feature learning", "simulation"]}
{"title": "a deeper look at experience replay", "abstract": "recently experience replay is widely used in various deep reinforcement learning ( rl ) algorithms , however in this paper we showcase that it is not as good as people think . to be more specific , experience replay will significantly hurt the learning process if the size of replay buffer is not well tuned . although experience replay is a necessary component in modern deep rl algorithms to stabilize the network , we should be aware that the idea of experience replay itself is not as good as people think . the size of the replay buffer is an important hyper-parameter , which can significantly influence the performance and has unfortunately been underestimated in the community for a long time . in this paper we did a systematic empirical study of experience replay under various function representations . we showcase that a large replay buffer can significantly hurt the performance . moreover , we propose a simple o ( 1 ) method to remedy the negative influence of a large replay buffer . we showcase its utility in both simple grid world and challenging domains like atari games . moreover , we visualize how a large replay buffer hurts the learning process .", "topics": ["reinforcement learning"]}
{"title": "minos : multimodal indoor simulator for navigation in complex environments", "abstract": "we present minos , a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments . the simulator leverages large datasets of complex 3d environments and supports flexible configuration of multimodal sensor suites . we use minos to benchmark deep-learning-based navigation methods , to analyze the influence of environmental complexity on navigation performance , and to carry out a controlled study of multimodality in sensorimotor learning . the experiments show that current deep reinforcement learning approaches fail in large realistic environments . the experiments also indicate that multimodality is beneficial in learning to navigate cluttered scenes . minos is released open-source to the research community at http : //minosworld.org . a video that shows minos can be found at https : //youtu.be/c0ml9k64q84", "topics": ["reinforcement learning", "simulation"]}
{"title": "dum : diversity-weighted utility maximization for recommendations", "abstract": "the need for diversification of recommendation lists manifests in a number of recommender systems use cases . however , an increase in diversity may undermine the utility of the recommendations , as relevant items in the list may be replaced by more diverse ones . in this work we propose a novel method for maximizing the utility of the recommended items subject to the diversity of user 's tastes , and show that an optimal solution to this problem can be found greedily . we evaluate the proposed method in two online user studies as well as in an offline analysis incorporating a number of evaluation metrics . the results of evaluations show the superiority of our method over a number of baselines .", "topics": ["baseline ( configuration management )", "optimization problem"]}
{"title": "joint recursive monocular filtering of camera motion and disparity map", "abstract": "monocular scene reconstruction is essential for modern applications such as robotics or autonomous driving . although stereo methods usually result in better accuracy than monocular methods , they are more expensive and more difficult to calibrate . in this work , we present a novel second order optimal minimum energy filter that jointly estimates the camera motion , the disparity map and also higher order kinematics recursively on a product lie group containing a novel disparity group . this mathematical framework enables to cope with non-euclidean state spaces , non-linear observations and high dimensions which is infeasible for most classical filters . to be robust against outliers , we use a generalized charbonnier energy function in this framework rather than a quadratic energy function as proposed in related work . experiments confirm that our method enables accurate reconstructions on-par with state-of-the-art .", "topics": ["mathematical optimization", "nonlinear system"]}
{"title": "type-constrained representation learning in knowledge graphs", "abstract": "large knowledge graphs increasingly add value to various applications that require machines to recognize and understand queries and their semantics , as in search or question answering systems . latent variable models have increasingly gained attention for the statistical modeling of knowledge graphs , showing promising results in tasks related to knowledge graph completion and cleaning . besides storing facts about the world , schema-based knowledge graphs are backed by rich semantic descriptions of entities and relation-types that allow machines to understand the notion of things and their semantic relationships . in this work , we study how type-constraints can generally support the statistical modeling with latent variable models . more precisely , we integrated prior knowledge in form of type-constraints in various state of the art latent variable approaches . our experimental results show that prior knowledge on relation-types significantly improves these models up to 77 % in link-prediction tasks . the achieved improvements are especially prominent when a low model complexity is enforced , a crucial requirement when these models are applied to very large datasets . unfortunately , type-constraints are neither always available nor always complete e.g . , they can become fuzzy when entities lack proper typing . we show that in these cases , it can be beneficial to apply a local closed-world assumption that approximates the semantics of relation-types based on observations made in the data .", "topics": ["entity"]}
{"title": "dynamical systems trees", "abstract": "we propose dynamical systems trees ( dsts ) as a flexible class of models for describing multiple processes that interact via a hierarchy of aggregating parent chains . dsts extend kalman filters , hidden markov models and nonlinear dynamical systems to an interactive group scenario . various individual processes interact as communities and sub-communities in a tree structure that is unrolled in time . to accommodate nonlinear temporal activity , each individual leaf process is modeled as a dynamical system containing discrete and/or continuous hidden states with discrete and/or gaussian emissions . subsequent higher level parent processes act like hidden markov models and mediate the interaction between leaf processes or between other parent processes in the hierarchy . aggregator chains are parents of child processes that they combine and mediate , yielding a compact overall parameterization . we provide tractable inference and learning algorithms for arbitrary dst topologies via an efficient structured mean-field algorithm . the diverse applicability of dsts is demonstrated by experiments on gene expression data and by modeling group behavior in the setting of an american football game .", "topics": ["nonlinear system"]}
{"title": "on the reliable detection of concept drift from streaming unlabeled data", "abstract": "classifiers deployed in the real world operate in a dynamic environment , where the data distribution can change over time . these changes , referred to as concept drift , can cause the predictive performance of the classifier to drop over time , thereby making it obsolete . to be of any real use , these classifiers need to detect drifts and be able to adapt to them , over time . detecting drifts has traditionally been approached as a supervised task , with labeled data constantly being used for validating the learned model . although effective in detecting drifts , these techniques are impractical , as labeling is a difficult , costly and time consuming activity . on the other hand , unsupervised change detection techniques are unreliable , as they produce a large number of false alarms . the inefficacy of the unsupervised techniques stems from the exclusion of the characteristics of the learned classifier , from the detection process . in this paper , we propose the margin density drift detection ( md3 ) algorithm , which tracks the number of samples in the uncertainty region of a classifier , as a metric to detect drift . the md3 algorithm is a distribution independent , application independent , model independent , unsupervised and incremental algorithm for reliably detecting drifts from data streams . experimental evaluation on 6 drift induced datasets and 4 additional datasets from the cybersecurity domain demonstrates that the md3 approach can reliably detect drifts , with significantly fewer false alarms compared to unsupervised feature based drift detectors . the reduced false alarms enables the signaling of drifts only when they are most likely to affect classification performance . as such , the md3 approach leads to a detection scheme which is credible , label efficient and general in its applicability .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "on the reducibility of submodular functions", "abstract": "the scalability of submodular optimization methods is critical for their usability in practice . in this paper , we study the reducibility of submodular functions , a property that enables us to reduce the solution space of submodular optimization problems without performance loss . we introduce the concept of reducibility using marginal gains . then we show that by adding perturbation , we can endow irreducible functions with reducibility , based on which we propose the perturbation-reduction optimization framework . our theoretical analysis proves that given the perturbation scales , the reducibility gain could be computed , and the performance loss has additive upper bounds . we further conduct empirical studies and the results demonstrate that our proposed framework significantly accelerates existing optimization methods for irreducible submodular functions with a cost of only small performance losses .", "topics": ["scalability"]}
{"title": "safe exploration in finite markov decision processes with gaussian processes", "abstract": "in classical reinforcement learning , when exploring an environment , agents accept arbitrary short term loss for long term gain . this is infeasible for safety critical applications , such as robotics , where even a single unsafe action may cause system failure . in this paper , we address the problem of safely exploring finite markov decision processes ( mdp ) . we define safety in terms of an , a priori unknown , safety constraint that depends on states and actions . we aim to explore the mdp under this constraint , assuming that the unknown function satisfies regularity conditions expressed via a gaussian process prior . we develop a novel algorithm for this task and prove that it is able to completely explore the safely reachable part of the mdp without violating the safety constraint . to achieve this , it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment . moreover , the algorithm explicitly considers reachability when exploring the mdp , ensuring that it does not get stuck in any state with no safe way out . we demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover .", "topics": ["reinforcement learning", "causality"]}
{"title": "deriving neural architectures from sequence and graph kernels", "abstract": "the design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process . in this work , we appeal to kernels over combinatorial structures , such as sequences and graphs , to derive appropriate neural operations . we introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces . our recurrent modules compare the input to virtual reference objects ( cf . filters in cnn ) via the kernels . similar to traditional neural operations , these reference objects are parameterized and directly optimized in end-to-end training . we empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression , achieving state-of-the-art results across these applications .", "topics": ["recurrent neural network", "end-to-end principle"]}
{"title": "decomposition and identification of linear structural equation models", "abstract": "in this paper , we address the problem of identifying linear structural equation models . we first extend the edge set half-trek criterion to cover a broader class of models . we then show that any semi-markovian linear model can be recursively decomposed into simpler sub-models , resulting in improved identification power . finally , we show that , unlike the existing methods developed for linear models , the resulting method subsumes the identification algorithm of non-parametric models .", "topics": ["eisenstein 's criterion"]}
{"title": "frame-recurrent video super-resolution", "abstract": "recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution ( lr ) frames to generate high-quality images . current state-of-the-art methods process a batch of lr frames to generate a single high-resolution ( hr ) frame and run this scheme in a sliding window fashion over the entire video , effectively treating the problem as a large number of separate multi-frame super-resolution tasks . this approach has two main weaknesses : 1 ) each input frame is processed and warped multiple times , increasing the computational cost , and 2 ) each output frame is estimated independently conditioned on the input frames , limiting the system 's ability to produce temporally consistent results . in this work , we propose an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred hr estimate to super-resolve the subsequent frame . this naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step . furthermore , due to its recurrent nature , the proposed method has the ability to assimilate a large number of previous frames without increased computational demands . extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art .", "topics": ["recurrent neural network", "end-to-end principle"]}
{"title": "a joint speaker-listener-reinforcer model for referring expressions", "abstract": "referring expressions are natural language constructions used to identify particular objects within a scene . in this paper , we propose a unified framework for the tasks of referring expression comprehension and generation . our model is composed of three modules : speaker , listener , and reinforcer . the speaker generates referring expressions , the listener comprehends referring expressions , and the reinforcer introduces a reward function to guide sampling of more discriminative expressions . the listener-speaker modules are trained jointly in an end-to-end learning framework , allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer 's feedback . we demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets . project and demo page : https : //vision.cs.unc.edu/refer", "topics": ["sampling ( signal processing )", "reinforcement learning"]}
{"title": "sentiment analysis by joint learning of word embeddings and classifier", "abstract": "word embeddings are representations of individual words of a text document in a vector space and they are often use- ful for performing natural language pro- cessing tasks . current state of the art al- gorithms for learning word embeddings learn vector representations from large corpora of text documents in an unsu- pervised fashion . this paper introduces swesa ( supervised word embeddings for sentiment analysis ) , an algorithm for sentiment analysis via word embeddings . swesa leverages document label infor- mation to learn vector representations of words from a modest corpus of text doc- uments by solving an optimization prob- lem that minimizes a cost function with respect to both word embeddings as well as classification accuracy . analysis re- veals that swesa provides an efficient way of estimating the dimension of the word embeddings that are to be learned . experiments on several real world data sets show that swesa has superior per- formance when compared to previously suggested approaches to word embeddings and sentiment analysis tasks .", "topics": ["natural language processing", "unsupervised learning"]}
{"title": "deep hyperspherical learning", "abstract": "convolution as inner product has been the founding basis of convolutional neural networks ( cnns ) and the key to end-to-end visual representation learning . benefiting from deeper architectures , recent cnns have demonstrated increasingly strong representation abilities . despite such improvement , the increased depth and larger parameter space have also led to challenges in properly training a network . in light of such challenges , we propose hyperspherical convolution ( sphereconv ) , a novel learning framework that gives angular representations on hyperspheres . we introduce spherenet , deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks . in particular , spherenet adopts sphereconv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under sphereconv . we show that spherenet can effectively encode discriminative representation and alleviate training difficulty , leading to easier optimization , faster convergence and comparable ( even better ) classification accuracy over convolutional counterparts . we also provide some theoretical insights for the advantages of learning on hyperspheres . in addition , we introduce the learnable sphereconv , i.e . , a natural improvement over prefixed sphereconv , and spherenorm , i.e . , hyperspherical learning as a normalization method . experiments have verified our conclusions .", "topics": ["feature learning", "convolution"]}
{"title": "two-stage algorithm for fairness-aware machine learning", "abstract": "algorithmic decision making process now affects many aspects of our lives . standard tools for machine learning , such as classification and regression , are subject to the bias in data , and thus direct application of such off-the-shelf tools could lead to a specific group being unfairly discriminated . removing sensitive attributes of data does not solve this problem because a \\textit { disparate impact } can arise when non-sensitive attributes and sensitive attributes are correlated . here , we study a fair machine learning algorithm that avoids such a disparate impact when making a decision . inspired by the two-stage least squares method that is widely used in the field of economics , we propose a two-stage algorithm that removes bias in the training data . the proposed algorithm is conceptually simple . unlike most of existing fair algorithms that are designed for classification tasks , the proposed method is able to ( i ) deal with regression tasks , ( ii ) combine explanatory attributes to remove reverse discrimination , and ( iii ) deal with numerical sensitive attributes . the performance and fairness of the proposed algorithm are evaluated in simulations with synthetic and real-world datasets .", "topics": ["test set", "statistical classification"]}
{"title": "sengen : sentence generating neural variational topic model", "abstract": "we present a new topic model that generates documents by sampling a topic for one whole sentence at a time , and generating the words in the sentence using an rnn decoder that is conditioned on the topic of the sentence . we argue that this novel formalism will help us not only visualize and model the topical discourse structure in a document better , but also potentially lead to more interpretable topics since we can now illustrate topics by sampling representative sentences instead of bag of words or phrases . we present a variational auto-encoder approach for learning in which we use a factorized variational encoder that independently models the posterior over topical mixture vectors of documents using a feed-forward network , and the posterior over topic assignments to sentences using an rnn . our preliminary experiments on two different datasets indicate early promise , but also expose many challenges that remain to be addressed .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "metaheuristic algorithms for convolution neural network", "abstract": "a typical modern optimization technique is usually either heuristic or metaheuristic . this technique has managed to solve some optimization problems in the research area of science , engineering , and industry . however , implementation strategy of metaheuristic for accuracy improvement on convolution neural networks ( cnn ) , a famous deep learning method , is still rarely investigated . deep learning relates to a type of machine learning technique , where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human . in this paper , we propose the implementation strategy of three popular metaheuristic approaches , that is , simulated annealing , differential evolution , and harmony search , to optimize cnn . the performances of these metaheuristic methods in optimizing cnn on classifying mnist and cifar dataset were evaluated and compared . furthermore , the proposed methods are also compared with the original cnn . although the proposed methods show an increase in the computation time , their accuracy has also been improved ( up to 7.14 percent ) .", "topics": ["time complexity", "simulation"]}
{"title": "grid long short-term memory", "abstract": "this paper introduces grid long short-term memory , a network of lstm cells arranged in a multidimensional grid that can be applied to vectors , sequences or higher dimensional data such as images . the network differs from existing deep lstm architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data . the network provides a unified way of using lstm for both deep and sequential computation . we apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization , where it is able to significantly outperform the standard lstm . we then give results for two empirical tasks . we find that 2d grid lstm achieves 1.47 bits per character on the wikipedia character prediction benchmark , which is state-of-the-art among neural approaches . in addition , we use the grid lstm to define a novel two-dimensional translation model , the reencoder , and show that it outperforms a phrase-based reference system on a chinese-to-english translation task .", "topics": ["computation"]}
{"title": "loss functions for multiset prediction", "abstract": "we study the problem of multiset prediction . the goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items . unlike existing problems in supervised learning , such as classification , ranking and sequence generation , there is no known order among items in a target multiset , and each item in the multiset may appear more than once , making this problem extremely challenging . in this paper , we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making . the proposed multiset loss function is empirically evaluated on two families of datasets , one synthetic and the other real , with varying levels of difficulty , against various baseline loss functions including reinforcement learning , sequence , and aggregated distribution matching loss functions . the experiments reveal the effectiveness of the proposed loss function over the others .", "topics": ["baseline ( configuration management )", "supervised learning"]}
{"title": "an empirical study of adequate vision span for attention-based neural machine translation", "abstract": "recently , the attention mechanism plays a key role to achieve high performance for neural machine translation models . however , as it computes a score function for the encoder states in all positions at each decoding step , the attention model greatly increases the computational complexity . in this paper , we investigate the adequate vision span of attention models in the context of machine translation , by proposing a novel attention framework that is capable of reducing redundant score computation dynamically . the term `` vision span '' means a window of the encoder states considered by the attention model in one step . in our experiments , we found that the average window size of vision span can be reduced by over 50 % with modest loss in accuracy on english-japanese and german-english translation tasks . % this results indicate that the conventional attention mechanism performs a significant amount of redundant computation .", "topics": ["computational complexity theory", "machine translation"]}
{"title": "stepwise regression for unsupervised learning", "abstract": "i consider unsupervised extensions of the fast stepwise linear regression algorithm \\cite { efroymson1960multiple } . these extensions allow one to efficiently identify highly-representative feature variable subsets within a given set of jointly distributed variables . this in turn allows for the efficient dimensional reduction of large data sets via the removal of redundant features . fast search is effected here through the avoidance of repeat computations across trial fits , allowing for a full representative-importance ranking of a set of feature variables to be carried out in $ o ( n^2 m ) $ time , where $ n $ is the number of variables and $ m $ is the number of data samples available . this runtime complexity matches that needed to carry out a single regression and is $ o ( n^2 ) $ faster than that of naive implementations . i present pseudocode suitable for efficient forward , reverse , and forward-reverse unsupervised feature selection . to illustrate the algorithm 's application , i apply it to the problem of identifying representative stocks within a given financial market index -- a challenge relevant to the design of exchange traded funds ( etfs ) . i also characterize the growth of numerical error with iteration step in these algorithms , and finally demonstrate and rationalize the observation that the forward and reverse algorithms return exactly inverted feature orderings in the weakly-correlated feature set regime .", "topics": ["numerical analysis", "unsupervised learning"]}
{"title": "skip-gram language modeling using sparse non-negative matrix probability estimation", "abstract": "we present a novel family of language model ( lm ) estimation techniques named sparse non-negative matrix ( snm ) estimation . a first set of experiments empirically evaluating it on the one billion word benchmark shows that snm $ n $ -gram lms perform almost as well as the well-established kneser-ney ( kn ) models . when using skip-gram features the models are able to match the state-of-the-art recurrent neural network ( rnn ) lms ; combining the two modeling techniques yields the best known result on the benchmark . the computational advantages of snm over both maximum entropy and rnn lm estimation are probably its main strength , promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as $ n $ -gram lms do .", "topics": ["recurrent neural network", "sparse matrix"]}
{"title": "feature engineering for map matching of low-sampling-rate gps trajectories in road network", "abstract": "map matching of gps trajectories from a sequence of noisy observations serves the purpose of recovering the original routes in a road network . in this work in progress , we attempt to share our experience of feature construction in a spatial database by reporting our ongoing experiment of feature extrac-tion in conditional random fields ( crfs ) for map matching . our preliminary results are obtained from real-world taxi gps trajectories .", "topics": ["feature vector", "feature extraction"]}
{"title": "forward thinking : building and training neural networks one layer at a time", "abstract": "we present a general framework for training deep neural networks without backpropagation . this substantially decreases training time and also allows for construction of deep networks with many sorts of learners , including networks whose layers are defined by functions that are not easily differentiated , like decision trees . the main idea is that layers can be trained one at a time , and once they are trained , the input data are mapped forward through the layer to create a new learning problem . the process is repeated , transforming the data through multiple layers , one at a time , rendering a new data set , which is expected to be better behaved , and on which a final output layer can achieve good performance . we call this forward thinking and demonstrate a proof of concept by achieving state-of-the-art accuracy on the mnist dataset for convolutional neural networks . we also provide a general mathematical formulation of forward thinking that allows for other types of deep learning problems to be considered .", "topics": ["neural networks", "mnist database"]}
{"title": "learning sensor multiplexing design through back-propagation", "abstract": "recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks , which are trained by propagating gradients of a loss defined on the final output , back through the network up to the first layer that operates directly on the image . we propose back-propagating one step further -- -to learn camera sensor designs jointly with networks that carry out inference on the images they capture . in this paper , we specifically consider the design and inference problems in a typical color camera -- -where the sensor is able to measure only one color channel at each pixel location , and computational inference is required to reconstruct a full color image . we learn the camera sensor 's color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel , from among a fixed set , will be measured at each location . these weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image . our network achieves significant improvements in accuracy over the traditional bayer pattern used in most color cameras . it automatically learns to employ a sparse color measurement approach similar to that of a recent design , and moreover , improves upon that design by learning an optimal layout for these measurements .", "topics": ["sparse matrix", "sensor"]}
{"title": "fastmmd : ensemble of circular discrepancy for efficient two-sample test", "abstract": "the maximum mean discrepancy ( mmd ) is a recently proposed test statistic for two-sample test . its quadratic time complexity , however , greatly hampers its availability to large-scale applications . to accelerate the mmd calculation , in this study we propose an efficient method called fastmmd . the core idea of fastmmd is to equivalently transform the mmd with shift-invariant kernels into the amplitude expectation of a linear combination of sinusoid components based on bochner 's theorem and fourier transform ( rahimi & recht , 2007 ) . taking advantage of sampling of fourier transform , fastmmd decreases the time complexity for mmd calculation from $ o ( n^2 d ) $ to $ o ( l n d ) $ , where $ n $ and $ d $ are the size and dimension of the sample set , respectively . here $ l $ is the number of basis functions for approximating kernels which determines the approximation accuracy . for kernels that are spherically invariant , the computation can be further accelerated to $ o ( l n \\log d ) $ by using the fastfood technique ( le et al . , 2013 ) . the uniform convergence of our method has also been theoretically proved in both unbiased and biased estimates . we have further provided a geometric explanation for our method , namely ensemble of circular discrepancy , which facilitates us to understand the insight of mmd , and is hopeful to help arouse more extensive metrics for assessing two-sample test . experimental results substantiate that fastmmd is with similar accuracy as exact mmd , while with faster computation speed and lower variance than the existing mmd approximation methods .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "the relative performance of ensemble methods with deep convolutional neural networks for image classification", "abstract": "artificial neural networks have been successfully applied to a variety of machine learning tasks , including image recognition , semantic segmentation , and machine translation . however , few studies fully investigated ensembles of artificial neural networks . in this work , we investigated multiple widely used ensemble methods , including unweighted averaging , majority voting , the bayes optimal classifier , and the ( discrete ) super learner , for image recognition tasks , with deep neural networks as candidate algorithms . we designed several experiments , with the candidate algorithms being the same network structure with different model checkpoints within a single training process , networks with same structure but trained multiple times stochastically , and networks with different structure . in addition , we further studied the over-confidence phenomenon of the neural networks , as well as its impact on the ensemble methods . across all of our experiments , the super learner achieved best performance among all the ensemble methods in this study .", "topics": ["neural networks", "machine translation"]}
{"title": "learning to make predictions in partially observable environments without a generative model", "abstract": "when faced with the problem of learning a model of a high-dimensional environment , a common approach is to limit the model to make only a restricted set of predictions , thereby simplifying the learning problem . these partial models may be directly useful for making decisions or may be combined together to form a more complete , structured model . however , in partially observable ( non-markov ) environments , standard model-learning methods learn generative models , i.e . models that provide a probability distribution over all possible futures ( such as pomdps ) . it is not straightforward to restrict such models to make only certain predictions , and doing so does not always simplify the learning problem . in this paper we present prediction profile models : non-generative partial models for partially observable systems that make only a given set of predictions , and are therefore far simpler than generative models in some cases . we formalize the problem of learning a prediction profile model as a transformation of the original model-learning problem , and show empirically that one can learn prediction profile models that make a small set of important predictions even in systems that are too complex for standard generative models .", "topics": ["generative model"]}
{"title": "deep network for simultaneous decomposition and classification in uwb-sar imagery", "abstract": "classifying buried and obscured targets of interest from other natural and manmade clutter objects in the scene is an important problem for the u.s. army . targets of interest are often represented by signals captured using low-frequency ( uhf to l-band ) ultra-wideband ( uwb ) synthetic aperture radar ( sar ) technology . this technology has been used in various applications , including ground penetration and sensing-through-the-wall . however , the technology still faces a significant issues regarding low-resolution sar imagery in this particular frequency band , low radar cross sections ( rcs ) , small objects compared to radar signal wavelengths , and heavy interference . the classification problem has been firstly , and partially , addressed by sparse representation-based classification ( src ) method which can extract noise from signals and exploit the cross-channel information . despite providing potential results , src-related methods have drawbacks in representing nonlinear relations and dealing with larger training sets . in this paper , we propose a simultaneous decomposition and classification network ( sdcn ) to alleviate noise inferences and enhance classification accuracy . the network contains two jointly trained sub-networks : the decomposition sub-network handles denoising , while the classification sub-network discriminates targets from confusers . experimental results show significant improvements over a network without decomposition and src-related methods .", "topics": ["noise reduction", "synthetic data"]}
{"title": "simplified firefly algorithm for 2d image key-points search", "abstract": "in order to identify an object , human eyes firstly search the field of view for points or areas which have particular properties . these properties are used to recognise an image or an object . then this process could be taken as a model to develop computer algorithms for images identification . this paper proposes the idea of applying the simplified firefly algorithm to search for key-areas in 2d images . for a set of input test images the proposed version of firefly algorithm has been examined . research results are presented and discussed to show the efficiency of this evolutionary computation method .", "topics": ["computation"]}
{"title": "the web as a knowledge-base for answering complex questions", "abstract": "answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information . recent work on reading comprehension made headway in answering simple questions , but tackling complex questions is still an ongoing research challenge . conversely , semantic parsers have been successful at handling compositionality , but only when the information resides in a target knowledge-base . in this paper , we present a novel framework for answering broad and complex questions , assuming answering simple questions is possible using a search engine and a reading comprehension model . we propose to decompose complex questions into a sequence of simple questions , and compute the final answer from the sequence of answers . to illustrate the viability of our approach , we create a new dataset of complex questions , complexwebquestions , and present a model that decomposes questions and interacts with the web to compute an answer . we empirically demonstrate that question decomposition improves performance from 20.8 precision @ 1 to 27.5 precision @ 1 on this new dataset .", "topics": ["parsing"]}
{"title": "composite task-completion dialogue policy learning via hierarchical deep reinforcement learning", "abstract": "building a dialogue agent to fulfill complex tasks , such as travel planning , is challenging because the agent has to learn to collectively complete multiple subtasks . for example , the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in . this paper addresses this challenge by formulating the task in the mathematical framework of options over markov decision processes ( mdps ) , and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales . the dialogue manager consists of : ( 1 ) a top-level dialogue policy that selects among subtasks or options , ( 2 ) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy , and ( 3 ) a global state tracker that helps ensure all cross-subtask constraints be satisfied . experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines , two based on handcrafted rules and the other based on flat deep reinforcement learning .", "topics": ["baseline ( configuration management )", "high- and low-level"]}
{"title": "approximation algorithms for bayesian multi-armed bandit problems", "abstract": "in this paper , we consider several finite-horizon bayesian multi-armed bandit problems with side constraints which are computationally intractable ( np-hard ) and for which no optimal ( or near optimal ) algorithms are known to exist with sub-exponential running time . all of these problems violate the standard exchange property , which assumes that the reward from the play of an arm is not contingent upon when the arm is played . not only are index policies suboptimal in these contexts , there has been little analysis of such policies in these problem settings . we show that if we consider near-optimal policies , in the sense of approximation algorithms , then there exists ( near ) index policies . conceptually , if we can find policies that satisfy an approximate version of the exchange property , namely , that the reward from the play of an arm depends on when the arm is played to within a constant factor , then we have an avenue towards solving these problems . however such an approximate version of the idling bandit property does not hold on a per-play basis and are shown to hold in a global sense . clearly , such a property is not necessarily true of arbitrary single arm policies and finding such single arm policies is nontrivial . we show that by restricting the state spaces of arms we can find single arm policies and that these single arm policies can be combined into global ( near ) index policies where the approximate version of the exchange property is true in expectation . the number of different bandit problems that can be addressed by this technique already demonstrate its wide applicability .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "mrnet-product2vec : a multi-task recurrent neural network for product embeddings", "abstract": "e-commerce websites such as amazon , alibaba , flipkart , and walmart sell billions of products . machine learning ( ml ) algorithms involving products are often used to improve the customer experience and increase revenue , e.g . , product similarity , recommendation , and price estimation . the products are required to be represented as features before training an ml algorithm . in this paper , we propose an approach called mrnet-product2vec for creating generic embeddings of products within an e-commerce ecosystem . we learn a dense and low-dimensional embedding where a diverse set of signals related to a product are explicitly injected into its representation . we train a discriminative multi-task bidirectional recurrent neural network ( rnn ) , where the input is a product title fed through a bidirectional rnn and at the output , product labels corresponding to fifteen different tasks are predicted . the task set includes several intrinsic characteristics about a product such as price , weight , size , color , popularity , and material . we evaluate the proposed embedding quantitatively and qualitatively . we demonstrate that they are almost as good as sparse and extremely high-dimensional tf-idf representation in spite of having less than 3 % of the tf-idf dimension . we also use a multimodal autoencoder for comparing products from different language-regions and show preliminary yet promising qualitative results .", "topics": ["recurrent neural network", "sparse matrix"]}
{"title": "noddi-sh : a computational efficient noddi extension for fodf estimation in diffusion mri", "abstract": "diffusion magnetic resonance imaging ( dmri ) is the only non-invasive imaging technique which is able to detect the principal directions of water diffusion as well as neurites density in the human brain . exploiting the ability of spherical harmonics ( sh ) to model spherical functions , we propose a new reconstruction model for dmri data which is able to estimate both the fiber orientation distribution function ( fodf ) and the relative volume fractions of the neurites in each voxel , which is robust to multiple fiber crossings . we consider a neurite orientation dispersion and density imaging ( noddi ) inspired single fiber diffusion signal to be derived from three compartments : intracellular , extracellular , and cerebrospinal fluid . the model , called noddi-sh , is derived by convolving the single fiber response with the fodf in each voxel . noddi-sh embeds the calculation of the fodf and the neurite density in a unified mathematical model providing efficient , robust and accurate results . results were validated on simulated data and tested on \\textit { in-vivo } data of human brain , and compared to and constrained spherical deconvolution ( csd ) for benchmarking . results revealed competitive performance in all respects and inherent adaptivity to local microstructure , while sensibly reducing the computational cost . we also investigated noddi-sh performance when only a limited number of samples are available for the fitting , demonstrating that 60 samples are enough to obtain reliable results . the fast computational time and the low number of signal samples required , make noddi-sh feasible for clinical application .", "topics": ["simulation"]}
{"title": "simple regret for infinitely many armed bandits", "abstract": "we consider a stochastic bandit problem with infinitely many arms . in this setting , the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms . all previous algorithms for this setting were designed for minimizing the cumulative regret of the learner . in this paper , we propose an algorithm aiming at minimizing the simple regret . as in the cumulative regret setting of infinitely many armed bandits , the rate of the simple regret will depend on a parameter $ \\beta $ characterizing the distribution of the near-optimal arms . we prove that depending on $ \\beta $ , our algorithm is minimax optimal either up to a multiplicative constant or up to a $ \\log ( n ) $ factor . we also provide extensions to several important cases : when $ \\beta $ is unknown , in a natural setting where the near-optimal arms have a small variance , and in the case of unknown time horizon .", "topics": ["regret ( decision theory )"]}
{"title": "a spectral-spatial fusion model for robust blood pulse waveform extraction in photoplethysmographic imaging", "abstract": "photoplethysmographic imaging is a camera-based solution for non-contact cardiovascular monitoring from a distance . this technology enables monitoring in situations where contact-based devices may be problematic or infeasible , such as ambulatory , sleep , and multi-individual monitoring . however , extracting the blood pulse waveform signal is challenging due to the unknown mixture of relevant ( pulsatile ) and irrelevant pixels in the scene . here , we design and implement a signal fusion framework , fusionppg , for extracting a blood pulse waveform signal with strong temporal fidelity from a scene without requiring anatomical priors ( e.g . , facial tracking ) . the extraction problem is posed as a bayesian least squares fusion problem , and solved using a novel probabilistic pulsatility model that incorporates both physiologically derived spectral and spatial waveform priors to identify pulsatility characteristics in the scene . experimental results show statistically significantly improvements compared to the facemeanppg method ( $ p < 0.001 $ ) and distanceppg ( $ p < 0.001 $ ) methods . heart rates predicted using fusionppg correlated strongly with ground truth measurements ( $ r^2=0.9952 $ ) . fusionppg was the only method able to assess cardiac arrhythmia via temporal analysis .", "topics": ["ground truth", "relevance"]}
{"title": "feature selection parallel technique for remotely sensed imagery classification", "abstract": "remote sensing research focusing on feature selection has long attracted the attention of the remote sensing community because feature selection is a prerequisite for image processing and various applications . different feature selection methods have been proposed to improve the classification accuracy . they vary from basic search techniques to clonal selections , and various optimal criteria have been investigated . recently , methods using dependence-based measures have attracted much attention due to their ability to deal with very high dimensional datasets . however , these methods are based on cramers v test , which has performance issues with large datasets . in this paper , we propose a parallel approach to improve their performance . we evaluate our approach on hyper-spectral and high spatial resolution images and compare it to the proposed methods with a centralized version as preliminary results . the results are very promising .", "topics": ["image processing", "computer vision"]}
{"title": "counterexample guided inductive optimization", "abstract": "this paper describes three variants of a counterexample guided inductive optimization ( cegio ) approach based on satisfiability modulo theories ( smt ) solvers . in particular , cegio relies on iterative executions to constrain a verification procedure , in order to perform inductive generalization , based on counterexamples extracted from smt solvers . cegio is able to successfully optimize a wide range of functions , including non-linear and non-convex optimization problems based on smt solvers , in which data provided by counterexamples are employed to guide the verification engine , thus reducing the optimization domain . the present algorithms are evaluated using a large set of benchmarks typically employed for evaluating optimization techniques . experimental results show the efficiency and effectiveness of the proposed algorithms , which find the optimal solution in all evaluated benchmarks , while traditional techniques are usually trapped by local minima .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "distributed learning , communication complexity and privacy", "abstract": "we consider the problem of pac-learning from distributed data and analyze fundamental communication complexity questions involved . we provide general upper and lower bounds on the amount of communication needed to learn well , showing that in addition to vc-dimension and covering number , quantities such as the teaching-dimension and mistake-bound of a class play an important role . we also present tight results for a number of common concept classes including conjunctions , parity functions , and decision lists . for linear separators , we show that for non-concentrated distributions , we can use a version of the perceptron algorithm to learn with much less communication than the number of updates given by the usual margin bound . we also show how boosting can be performed in a generic manner in the distributed setting to achieve communication with only logarithmic dependence on 1/epsilon for any concept class , and demonstrate how recent work on agnostic learning from class-conditional queries can be used to achieve low communication in agnostic settings as well . we additionally present an analysis of privacy , considering both differential privacy and a notion of distributional privacy that is especially appealing in this context .", "topics": ["reinforcement learning"]}
{"title": "soft actor-critic : off-policy maximum entropy deep reinforcement learning with a stochastic actor", "abstract": "model-free deep reinforcement learning ( rl ) algorithms have been demonstrated on a range of challenging decision making and control tasks . however , these methods typically suffer from two major challenges : very high sample complexity and brittle convergence properties , which necessitate meticulous hyperparameter tuning . both of these challenges severely limit the applicability of such methods to complex , real-world domains . in this paper , we propose soft actor-critic , an off-policy actor-critic deep rl algorithm based on the maximum entropy reinforcement learning framework . in this framework , the actor aims to maximize expected reward while also maximizing entropy - that is , succeed at the task while acting as randomly as possible . prior deep rl methods based on this framework have been formulated as q-learning methods . by combining off-policy updates with a stable stochastic actor-critic formulation , our method achieves state-of-the-art performance on a range of continuous control benchmark tasks , outperforming prior on-policy and off-policy methods . furthermore , we demonstrate that , in contrast to other off-policy algorithms , our approach is very stable , achieving very similar performance across different random seeds .", "topics": ["reinforcement learning", "gradient"]}
{"title": "mirrored light field video camera adapter", "abstract": "this paper proposes the design of a custom mirror-based light field camera adapter that is cheap , simple in construction , and accessible . mirrors of different shape and orientation reflect the scene into an upwards-facing camera to create an array of virtual cameras with overlapping field of view at specified depths , and deliver video frame rate light fields . we describe the design , construction , decoding and calibration processes of our mirror-based light field camera adapter in preparation for an open-source release to benefit the robotic vision community .", "topics": ["robot"]}
{"title": "an evolutionary squeaky wheel optimisation approach to personnel scheduling", "abstract": "the quest for robust heuristics that are able to solve more than one problem is ongoing . in this paper , we present , discuss and analyse a technique called evolutionary squeaky wheel optimisation and apply it to two different personnel scheduling problems . evolutionary squeaky wheel optimisation improves the original squeaky wheel optimisation 's effectiveness and execution speed by incorporating two extra steps ( selection and mutation ) for added evolution . in the evolutionary squeaky wheel optimisation , a cycle of analysis-selection-mutation-prioritization-construction continues until stopping conditions are reached . the aim of the analysis step is to identify below average solution components by calculating a fitness value for all components . the selection step then chooses amongst these underperformers and discards some probabilistically based on fitness . the mutation step further discards a few components at random . solutions can become incomplete and thus repairs may be required . the repairs are carried out by using the prioritization to first produce priorities that determine an order by which the following construction step then schedules the remaining components . therefore , improvement in the evolutionary squeaky wheel optimisation is achieved by selective solution disruption mixed with interative improvement and constructive repair . strong experimental results are reported on two different domains of personnel scheduling : bus and rail driver scheduling and hospital nurse scheduling .", "topics": ["mathematical optimization", "iteration"]}
{"title": "training neural networks with stochastic hessian-free optimization", "abstract": "hessian-free ( hf ) optimization has been successfully used for training deep autoencoders and recurrent networks . hf uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients . in this paper we exploit this property and study stochastic hf with gradient and curvature mini-batches independent of the dataset size . we modify martens ' hf for these settings and integrate dropout , a method for preventing co-adaptation of feature detectors , to guard against overfitting . stochastic hessian-free optimization gives an intermediary between sgd and hf that achieves competitive performance on both classification and deep autoencoder experiments .", "topics": ["neural networks", "gradient"]}
{"title": "( weak ) calibration is computationally hard", "abstract": "we show that the existence of a computationally efficient calibration algorithm , with a low weak calibration rate , would imply the existence of an efficient algorithm for computing approximate nash equilibria - thus implying the unlikely conclusion that every problem in ppad is solvable in polynomial time .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "dynamic layer normalization for adaptive neural acoustic modeling in speech recognition", "abstract": "layer normalization is a recently introduced technique for normalizing the activities of neurons in deep neural networks to improve the training speed and stability . in this paper , we introduce a new layer normalization technique called dynamic layer normalization ( dln ) for adaptive neural acoustic modeling in speech recognition . by dynamically generating the scaling and shifting parameters in layer normalization , dln adapts neural acoustic models to the acoustic variability arising from various factors such as speakers , channel noises , and environments . unlike other adaptive acoustic models , our proposed approach does not require additional adaptation data or speaker information such as i-vectors . moreover , the model size is fixed as it dynamically generates adaptation parameters . we apply our proposed dln to deep bidirectional lstm acoustic models and evaluate them on two benchmark datasets for large vocabulary asr experiments : wsj and ted-lium release 2 . the experimental results show that our dln improves neural acoustic models in terms of transcription accuracy by dynamically adapting to various speakers and environments .", "topics": ["speech recognition"]}
{"title": "fusion of stereo and still monocular depth estimates in a self-supervised learning context", "abstract": "we study how autonomous robots can learn by themselves to improve their depth estimation capability . in particular , we investigate a self-supervised learning setup in which stereo vision depth estimates serve as targets for a convolutional neural network ( cnn ) that transforms a single still image to a dense depth map . after training , the stereo and mono estimates are fused with a novel fusion method that preserves high confidence stereo estimates , while leveraging the cnn estimates in the low-confidence regions . the main contribution of the article is that it is shown that the fused estimates lead to a higher performance than the stereo vision estimates alone . experiments are performed on the kitti dataset , and on board of a parrot slamdunk , showing that even rather limited cnns can help provide stereo vision equipped robots with more reliable depth maps for autonomous navigation .", "topics": ["supervised learning", "map"]}
{"title": "accelerating deep convolutional networks using low-precision and sparsity", "abstract": "we explore techniques to significantly improve the compute efficiency and performance of deep convolution networks without impacting their accuracy . to improve the compute efficiency , we focus on achieving high accuracy with extremely low-precision ( 2-bit ) weight networks , and to accelerate the execution time , we aggressively skip operations on zero-values . we achieve the highest reported accuracy of 76.6 % top-1/93 % top-5 on the imagenet object classification challenge with low-precision network\\footnote { github release of the source code coming soon } while reducing the compute requirement by ~3x compared to a full-precision network that achieves similar accuracy . furthermore , to fully exploit the benefits of our low-precision networks , we build a deep learning accelerator core , dlac , that can achieve up to 1 tflop/mm^2 equivalent for single-precision floating-point operations ( ~2 tflop/mm^2 for half-precision ) .", "topics": ["sparse matrix", "convolution"]}
{"title": "estimation of english and non-english language use on the www", "abstract": "the world wide web has grown so big , in such an anarchic fashion , that it is difficult to describe . one of the evident intrinsic characteristics of the world wide web is its multilinguality . here , we present a technique for estimating the size of a language-specific corpus given the frequency of commonly occurring words in the corpus . we apply this technique to estimating the number of words available through web browsers for given languages . comparing data from 1996 to data from 1999 and 2000 , we calculate the growth of a number of european languages on the web . as expected , non-english languages are growing at a faster pace than english , though the position of english is still dominant .", "topics": ["text corpus"]}
{"title": "explanations based on the missing : towards contrastive explanations with pertinent negatives", "abstract": "in this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network . given an input we find what should be minimally and sufficiently present ( viz . important object pixels in an image ) to justify its classification and analogously what should be minimally and necessarily \\emph { absent } ( viz . certain background pixels ) . we argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology . what is minimally but critically \\emph { absent } is an important part of an explanation , which to the best of our knowledge , has not been touched upon by current explanation methods that attempt to explain predictions of neural networks . we validate our approach on three real datasets obtained from diverse domains ; namely , a handwritten digits dataset mnist , a large procurement fraud dataset and an fmri brain imaging dataset . in all three cases , we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate .", "topics": ["mnist database", "pixel"]}
{"title": "hardware based spatio-temporal neural processing backend for imaging sensors : towards a smart camera", "abstract": "in this work we show how we can build a technology platform for cognitive imaging sensors using recent advances in recurrent neural network architectures and training methods inspired from biology . we demonstrate learning and processing tasks specific to imaging sensors , including enhancement of sensitivity and signal-to-noise ratio ( snr ) purely through neural filtering beyond the fundamental limits sensor materials , and inferencing and spatio-temporal pattern recognition capabilities of these networks with applications in object detection , motion tracking and prediction . we then show designs of unit hardware cells built using complementary metal-oxide semiconductor ( cmos ) and emerging materials technologies for ultra-compact and energy-efficient embedded neural processors for smart cameras .", "topics": ["recurrent neural network", "object detection"]}
{"title": "open information extraction", "abstract": "open information extraction ( open ie ) systems aim to obtain relation tuples with highly scalable extraction in portable across domain by identifying a variety of relation phrases and their arguments in arbitrary sentences . the first generation of open ie learns linear chain models based on unlexicalized features such as part-of-speech ( pos ) or shallow tags to label the intermediate words between pair of potential arguments for identifying extractable relations . open ie currently is developed in the second generation that is able to extract instances of the most frequently observed relation types such as verb , noun and prep , verb and prep , and infinitive with deep linguistic analysis . they expose simple yet principled ways in which verbs express relationships in linguistics such as verb phrase-based extraction or clause-based extraction . they obtain a significantly higher performance over previous systems in the first generation . in this paper , we describe an overview of two open ie generations including strengths , weaknesses and application areas .", "topics": ["scalability"]}
{"title": "inverse covariance estimation for high-dimensional data in linear time and space : spectral methods for riccati and sparse models", "abstract": "we propose maximum likelihood estimation for learning gaussian graphical models with a gaussian ( ell_2^2 ) prior on the parameters . this is in contrast to the commonly used laplace ( ell_1 ) prior for encouraging sparseness . we show that our optimization problem leads to a riccati matrix equation , which has a closed form solution . we propose an efficient algorithm that performs a singular value decomposition of the training data . our algorithm is o ( nt^2 ) -time and o ( nt ) -space for n variables and t samples . our method is tailored to high-dimensional problems ( n gg t ) , in which sparseness promoting methods become intractable . furthermore , instead of obtaining a single solution for a specific regularization parameter , our algorithm finds the whole solution path . we show that the method has logarithmic sample complexity under the spiked covariance model . we also propose sparsification of the dense solution with provable performance guarantees . we provide techniques for using our learnt models , such as removing unimportant variables , computing likelihoods and conditional distributions . finally , we show promising results in several gene expressions datasets .", "topics": ["test set", "graphical model"]}
{"title": "towards automation of knowledge understanding : an approach for probabilistic generative classifiers", "abstract": "after data selection , pre-processing , transformation , and feature extraction , knowledge extraction is not the final step in a data mining process . it is then necessary to understand this knowledge in order to apply it efficiently and effectively . up to now , there is a lack of appropriate techniques that support this significant step . this is partly due to the fact that the assessment of knowledge is often highly subjective , e.g . , regarding aspects such as novelty or usefulness . these aspects depend on the specific knowledge and requirements of the data miner . there are , however , a number of aspects that are objective and for which it is possible to provide appropriate measures . in this article we focus on classification problems and use probabilistic generative classifiers based on mixture density models that are quite common in data mining applications . we define objective measures to assess the informativeness , uniqueness , importance , discrimination , representativity , uncertainty , and distinguishability of rules contained in these classifiers numerically . these measures not only support a data miner in evaluating results of a data mining process based on such classifiers . as we will see in illustrative case studies , they may also be used to improve the data mining process itself or to support the later application of the extracted knowledge .", "topics": ["data mining", "feature extraction"]}
{"title": "swsi : a low-cost and commercial-quality whole slide imaging system on android and ios smartphones", "abstract": "in this paper , scalable whole slide imaging ( swsi ) , a novel high-throughput , cost-effective and robust whole slide imaging system on both android and ios platforms is introduced and analyzed . with swsi , most mainstream smartphone connected to a optical eyepiece of any manually controlled microscope can be automatically controlled to capture sequences of mega-pixel fields of views that are synthesized into giga-pixel virtual slides . remote servers carry out the majority of computation asynchronously to support clients running at satisfying frame rates without sacrificing image quality nor robustness . a typical 15x15mm sample can be digitized in 30 seconds with 4x or in 3 minutes with 10x object magnification , costing under $ 1 . the virtual slide quality is considered comparable to existing high-end scanners thus satisfying for clinical usage by surveyed pathologies . the scan procedure with features such as supporting magnification up to 100x , recoding z-stacks , specimen-type-neutral and giving real-time feedback , is deemed work-flow-friendly and reliable .", "topics": ["computation", "scalability"]}
{"title": "deep reinforcement learning in parameterized action space", "abstract": "recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces . however , to the best of our knowledge no previous work has succeeded at using deep neural networks in structured ( parameterized ) continuous action spaces . to fill this gap , this paper focuses on learning within the domain of simulated robocup soccer , which features a small set of discrete action types , each of which is parameterized with continuous variables . the best learned agent can score goals more reliably than the 2012 robocup champion agent . as such , this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space mdps .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "visual learning of arithmetic operations", "abstract": "a simple neural network model is presented for end-to-end visual learning of arithmetic operations from pictures of numbers . the input consists of two pictures , each showing a 7-digit number . the output , also a picture , displays the number showing the result of an arithmetic operation ( e.g . , addition or subtraction ) on the two input numbers . the concepts of a number , or of an operator , are not explicitly introduced . this indicates that addition is a simple cognitive task , which can be learned visually using a very small number of neurons . other operations , e.g . , multiplication , were not learnable using this architecture . some tasks were not learnable end-to-end ( e.g . , addition with roman numerals ) , but were easily learnable once broken into two separate sub-tasks : a perceptual \\textit { character recognition } and cognitive \\textit { arithmetic } sub-tasks . this indicates that while some tasks may be easily learnable end-to-end , other may need to be broken into sub-tasks .", "topics": ["end-to-end principle"]}
{"title": "representation benefits of deep feedforward networks", "abstract": "this note provides a family of classification problems , indexed by a positive integer $ k $ , where all shallow networks with fewer than exponentially ( in $ k $ ) many nodes exhibit error at least $ 1/6 $ , whereas a deep network with 2 nodes in each of $ 2k $ layers achieves zero error , as does a recurrent network with 3 distinct nodes iterated $ k $ times . the proof is elementary , and the networks are standard feedforward networks with relu ( rectified linear unit ) nonlinearities .", "topics": ["recurrent neural network"]}
{"title": "similarity-based multi-label learning", "abstract": "multi-label classification is an important learning problem with many applications . in this work , we propose a principled similarity-based approach for multi-label learning called sml . we also introduce a similarity-based approach for predicting the label set size . the experimental results demonstrate the effectiveness of sml for multi-label classification where it is shown to compare favorably with a wide variety of existing algorithms across a range of evaluation criterion .", "topics": ["eisenstein 's criterion"]}
{"title": "stochastic rank-1 bandits", "abstract": "we propose stochastic rank- $ 1 $ bandits , a class of online learning problems where at each step a learning agent chooses a pair of row and column arms , and receives the product of their values as a reward . the main challenge of the problem is that the individual values of the row and column are unobserved . we assume that these values are stochastic and drawn independently . we propose a computationally-efficient algorithm for solving our problem , which we call rank1elim . we derive a $ o ( ( k + l ) ( 1 / \\delta ) \\log n ) $ upper bound on its $ n $ -step regret , where $ k $ is the number of rows , $ l $ is the number of columns , and $ \\delta $ is the minimum of the row and column gaps ; under the assumption that the mean row and column rewards are bounded away from zero . to the best of our knowledge , we present the first bandit algorithm that finds the maximum entry of a rank- $ 1 $ matrix whose regret is linear in $ k + l $ , $ 1 / \\delta $ , and $ \\log n $ . we also derive a nearly matching lower bound . finally , we evaluate rank1elim empirically on multiple problems . we observe that it leverages the structure of our problems and can learn near-optimal solutions even if our modeling assumptions are mildly violated .", "topics": ["regret ( decision theory )", "value ( ethics )"]}
{"title": "style imitation and chord invention in polyphonic music with exponential families", "abstract": "modeling polyphonic music is a particularly challenging task because of the intricate interplay between melody and harmony . a good model should satisfy three requirements : statistical accuracy ( capturing faithfully the statistics of correlations at various ranges , horizontally and vertically ) , flexibility ( coping with arbitrary user constraints ) , and generalization capacity ( inventing new material , while staying in the style of the training corpus ) . models proposed so far fail on at least one of these requirements . we propose a statistical model of polyphonic music , based on the maximum entropy principle . this model is able to learn and reproduce pairwise statistics between neighboring note events in a given corpus . the model is also able to invent new chords and to harmonize unknown melodies . we evaluate the invention capacity of the model by assessing the amount of cited , re-discovered , and invented chords on a corpus of bach chorales . we discuss how the model enables the user to specify and enforce user-defined constraints , which makes it useful for style-based , interactive music generation .", "topics": ["text corpus"]}
{"title": "incremental adaptation strategies for neural network language models", "abstract": "it is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation . however , training these models on large amounts of data can take several days . we present efficient techniques to adapt a neural network language model to new data . instead of training a completely new model or relying on mixture approaches , we propose two new methods : continued training on resampled data or insertion of adaptation layers . we present experimental results in an cat environment where the post-edits of professional translators are used to improve an smt system . both methods are very fast and achieve significant improvements without overfitting the small adaptation data .", "topics": ["machine translation", "speech recognition"]}
{"title": "learning conditional independence structure for high-dimensional uncorrelated vector processes", "abstract": "we formulate and analyze a graphical model selection method for inferring the conditional independence graph of a high-dimensional nonstationary gaussian random process ( time series ) from a finite-length observation . the observed process samples are assumed uncorrelated over time and having a time-varying marginal distribution . the selection method is based on testing conditional variances obtained for small subsets of process components . this allows to cope with the high-dimensional regime , where the sample size can be ( drastically ) smaller than the process dimension . we characterize the required sample size such that the proposed selection method is successful with high probability .", "topics": ["graphical model", "time series"]}
{"title": "pomegranate : fast and flexible probabilistic modeling in python", "abstract": "we present pomegranate , an open source machine learning package for probabilistic modeling in python . probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions . three widely used probabilistic models implemented in pomegranate are general mixture models , hidden markov models , and bayesian networks . a primary focus of pomegranate is to abstract away the complexities of training models from their definition . this allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms . an aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models . this approach trivially enables many useful learning strategies , such as out-of-core learning , minibatch learning , and semi-supervised learning , without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves . pomegranate is written in cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism , making it competitive with -- -or outperform -- -other implementations of similar algorithms . this paper presents an overview of the design choices in pomegranate , and how they have enabled complex features to be supported by simple code .", "topics": ["supervised learning", "bayesian network"]}
{"title": "greedy deep dictionary learning", "abstract": "in this work we propose a new deep learning tool called deep dictionary learning . multi-level dictionaries are learnt in a greedy fashion , one layer at a time . this requires solving a simple ( shallow ) dictionary learning problem , the solution to this is well known . we apply the proposed technique on some benchmark deep learning datasets . we compare our results with other deep learning tools like stacked autoencoder and deep belief network ; and state of the art supervised dictionary learning tools like discriminative ksvd and label consistent ksvd . our method yields better results than all .", "topics": ["bayesian network", "dictionary"]}
{"title": "efficient regularized regression for variable selection with l0 penalty", "abstract": "variable ( feature , gene , model , which we use interchangeably ) selections for regression with high-dimensional bigdata have found many applications in bioinformatics , computational biology , image processing , and engineering . one appealing approach is the l0 regularized regression which penalizes the number of nonzero features in the model directly . l0 is known as the most essential sparsity measure and has nice theoretical properties , while the popular l1 regularization is only a best convex relaxation of l0 . therefore , it is natural to expect that l0 regularized regression performs better than lasso . however , it is well-known that l0 optimization is np-hard and computationally challenging . instead of solving the l0 problems directly , most publications so far have tried to solve an approximation problem that closely resembles l0 regularization . in this paper , we propose an efficient em algorithm ( l0em ) that directly solves the l0 optimization problem . $ l_0 $ em is efficient with high dimensional data . it also provides a natural solution to all lp p in [ 0,2 ] problems . the regularized parameter can be either determined through cross-validation or aic and bic . theoretical properties of the l0-regularized estimator are given under mild conditions that permit the number of variables to be much larger than the sample size . we demonstrate our methods through simulation and high-dimensional genomic data . the results indicate that l0 has better performance than lasso and l0 with aic or bic has similar performance as computationally intensive cross-validation . the proposed algorithms are efficient in identifying the non-zero variables with less-bias and selecting biologically important genes and pathways with high dimensional bigdata .", "topics": ["optimization problem", "image processing"]}
{"title": "the convexity and design of composite multiclass losses", "abstract": "we consider composite loss functions for multiclass prediction comprising a proper ( i.e . , fisher-consistent ) loss over probability distributions and an inverse link function . we establish conditions for their ( strong ) convexity and explore the implications . we also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same bayes risk .", "topics": ["loss function"]}
{"title": "onionnet : sharing features in cascaded deep classifiers", "abstract": "the focus of our work is speeding up evaluation of deep neural networks in retrieval scenarios , where conventional architectures may spend too much time on negative examples . we propose to replace a monolithic network with our novel cascade of feature-sharing deep classifiers , called onionnet , where subsequent stages may add both new layers as well as new feature channels to the previous ones . importantly , intermediate feature maps are shared among classifiers , preventing them from the necessity of being recomputed . to accomplish this , the model is trained end-to-end in a principled way under a joint loss . we validate our approach in theory and on a synthetic benchmark . as a result demonstrated in three applications ( patch matching , object detection , and image retrieval ) , our cascade can operate significantly faster than both monolithic networks and traditional cascades without sharing at the cost of marginal decrease in precision .", "topics": ["object detection", "synthetic data"]}
{"title": "holistic random encoding for imaging through multimode fibers", "abstract": "the input numerical aperture ( na ) of multimode fiber ( mmf ) can be effectively increased by placing turbid media at the input end of the mmf . this provides the potential for high-resolution imaging through the mmf . while the input na is increased , the number of propagation modes in the mmf and hence the output na remains the same . this makes the image reconstruction process underdetermined and may limit the quality of the image reconstruction . in this paper , we aim to improve the signal to noise ratio ( snr ) of the image reconstruction in imaging through mmf . we notice that turbid media placed in the input of the mmf transforms the incoming waves into a better format for information transmission and information extraction . we call this transformation as holistic random ( hr ) encoding of turbid media . by exploiting the hr encoding , we make a considerable improvement on the snr of the image reconstruction . for efficient utilization of the hr encoding , we employ sparse representation ( sr ) , a relatively new signal reconstruction framework when it is provided with a hr encoded signal . this study shows for the first time to our knowledge the benefit of utilizing the hr encoding of turbid media for recovery in the optically underdetermined systems where the output na of it is smaller than the input na for imaging through mmf .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "computation of stackelberg equilibria of finite sequential games", "abstract": "the stackelberg equilibrium solution concept describes optimal strategies to commit to : player 1 ( termed the leader ) publicly commits to a strategy and player 2 ( termed the follower ) plays a best response to this strategy ( ties are broken in favor of the leader ) . we study stackelberg equilibria in finite sequential games ( or extensive-form games ) and provide new exact algorithms , approximate algorithms , and hardness results for several classes of these sequential games .", "topics": ["approximation algorithm", "computation"]}
{"title": "fast multi-output relevance vector regression", "abstract": "this paper aims to decrease the time complexity of multi-output relevance vector regression from o ( vm^3 ) to o ( v^3+m^3 ) , where v is the number of output dimensions , m is the number of basis functions , and v < m . the experimental results demonstrate that the proposed method is more competitive than the existing method , with regard to computation time . matlab codes are available at http : //www.mathworks.com/matlabcentral/fileexchange/49131 .", "topics": ["time complexity", "computation"]}
{"title": "driven to distraction : self-supervised distractor learning for robust monocular visual odometry in urban environments", "abstract": "we present a self-supervised approach to ignoring `` distractors '' in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments . we leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image , which we use to train a deep convolutional network . at run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry ( vo ) pipeline , using either sparse features or dense photometric matching . our approach yields metric-scale vo using only a single camera and can recover the correct egomotion even when 90 % of the image is obscured by dynamic , independently moving objects . we evaluate our robust vo methods on more than 400km of driving from the oxford robotcar dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic .", "topics": ["sparse matrix", "pixel"]}
{"title": "3d-r2n2 : a unified approach for single and multi-view 3d object reconstruction", "abstract": "inspired by the recent success of methods that employ shape priors to achieve robust 3d reconstructions , we propose a novel recurrent neural network architecture that we call the 3d recurrent reconstruction neural network ( 3d-r2n2 ) . the network learns a mapping from images of objects to their underlying 3d shapes from a large collection of synthetic data . our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3d occupancy grid . unlike most of the previous works , our network does not require any image annotations or object class labels for training or testing . our extensive experimental analysis shows that our reconstruction framework i ) outperforms the state-of-the-art methods for single view reconstruction , and ii ) enables the 3d reconstruction of objects in situations when traditional sfm/slam methods fail ( because of lack of texture and/or wide baseline ) .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "out-of-sample extension for dimensionality reduction of noisy time series", "abstract": "this paper proposes an out-of-sample extension framework for a global manifold learning algorithm ( isomap ) that uses temporal information in out-of-sample points in order to make the embedding more robust to noise and artifacts . given a set of noise-free training data and its embedding , the proposed framework extends the embedding for a noisy time series . this is achieved by adding a spatio-temporal compactness term to the optimization objective of the embedding . to the best of our knowledge , this is the first method for out-of-sample extension of manifold embeddings that leverages timing information available for the extension set . experimental results demonstrate that our out-of-sample extension algorithm renders a more robust and accurate embedding of sequentially ordered image data in the presence of various noise and artifacts when compared to other timing-aware embeddings . additionally , we show that an out-of-sample extension framework based on the proposed algorithm outperforms the state of the art in eye-gaze estimation .", "topics": ["test set", "time series"]}
{"title": "spikes as regularizers", "abstract": "we present a confidence-based single-layer feed-forward learning algorithm spiral ( spike regularized adaptive learning ) relying on an encoding of activation spikes . we adaptively update a weight vector relying on confidence estimates and activation offsets relative to previous activity . we regularize updates proportionally to item-level confidence and weight-specific support , loosely inspired by the observation from neurophysiology that high spike rates are sometimes accompanied by low temporal precision . our experiments suggest that the new learning algorithm spiral is more robust and less prone to overfitting than both the averaged perceptron and arow .", "topics": ["sampling ( signal processing )", "value ( ethics )"]}
{"title": "nested dictionary learning for hierarchical organization of imagery and text", "abstract": "a tree-based dictionary learning model is developed for joint analysis of imagery and associated text . the dictionary learning may be applied directly to the imagery from patches , or to general feature vectors extracted from patches or superpixels ( using any existing method for image feature extraction ) . each image is associated with a path through the tree ( from root to a leaf ) , and each of the multiple patches in a given image is associated with one node in that path . nodes near the tree root are shared between multiple paths , representing image characteristics that are common among different types of images . moving toward the leaves , nodes become specialized , representing details in image classes . if available , words ( text ) are also jointly modeled , with a path-dependent probability over words . the tree structure is inferred via a nested dirichlet process , and a retrospective stick-breaking sampler is used to infer the tree depth and width .", "topics": ["sampling ( signal processing )", "feature extraction"]}
{"title": "max-margin deep generative models for ( semi- ) supervised learning", "abstract": "deep generative models ( dgms ) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability . however , it is relatively insufficient to empower the discriminative ability of dgms on making accurate predictions . this paper presents max-margin deep generative models ( mmdgms ) and a class-conditional variant ( mmdcgms ) , which explore the strongly discriminative principle of max-margin learning to improve the predictive performance of dgms in both supervised and semi-supervised learning , while retaining the generative capability . in semi-supervised learning , we use the predictions of a max-margin classifier as the missing labels instead of performing full posterior inference for efficiency ; we also introduce additional max-margin and label-balance regularization terms of unlabeled data for effectiveness . we develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objectives in different settings . empirical results on various datasets demonstrate that : ( 1 ) max-margin learning can significantly improve the prediction performance of dgms and meanwhile retain the generative ability ; ( 2 ) in supervised learning , mmdgms are competitive to the best fully discriminative networks when employing convolutional neural networks as the generative and recognition models ; and ( 3 ) in semi-supervised learning , mmdcgms can perform efficient inference and achieve state-of-the-art classification results on several benchmarks .", "topics": ["generative model", "supervised learning"]}
{"title": "residual networks : lyapunov stability and convex decomposition", "abstract": "while training error of most deep neural networks degrades as the depth of the network increases , residual networks appear to be an exception . we show that the main reason for this is the lyapunov stability of the gradient descent algorithm : for an arbitrarily chosen step size , the equilibria of the gradient descent are most likely to remain stable for the parametrization of residual networks . we then present an architecture with a pair of residual networks to approximate a large class of functions by decomposing them into a convex and a concave part . some parameters of this model are shown to change little during training , and this imperfect optimization prevents overfitting the data and leads to solutions with small lipschitz constants , while providing clues about the generalization of other deep networks .", "topics": ["approximation algorithm", "gradient descent"]}
{"title": "mutual information-based unsupervised feature transformation for heterogeneous feature subset selection", "abstract": "conventional mutual information ( mi ) based feature selection ( fs ) methods are unable to handle heterogeneous feature subset selection properly because of data format differences or estimation methods of mi between feature subset and class label . a way to solve this problem is feature transformation ( ft ) . in this study , a novel unsupervised feature transformation ( uft ) which can transform non-numerical features into numerical features is developed and tested . the uft process is mi-based and independent of class label . mi-based fs algorithms , such as parzen window feature selector ( pwfs ) , minimum redundancy maximum relevance feature selection ( mrmr ) , and normalized mi feature selection ( nmifs ) , can all adopt uft for pre-processing of non-numerical features . unlike traditional ft methods , the proposed uft is unbiased while pwfs is utilized to its full advantage . simulations and analyses of large-scale datasets showed that feature subset selected by the integrated method , uft-pwfs , outperformed other ft-fs integrated methods in classification accuracy .", "topics": ["numerical analysis", "simulation"]}
{"title": "statistical latent space approach for mixed data modelling and applications", "abstract": "the analysis of mixed data has been raising challenges in statistics and machine learning . one of two most prominent challenges is to develop new statistical techniques and methodologies to effectively handle mixed data by making the data less heterogeneous with minimum loss of information . the other challenge is that such methods must be able to apply in large-scale tasks when dealing with huge amount of mixed data . to tackle these challenges , we introduce parameter sharing and balancing extensions to our recent model , the mixed-variate restricted boltzmann machine ( mv.rbm ) which can transform heterogeneous data into homogeneous representation . we also integrate structured sparsity and distance metric learning into rbm-based models . our proposed methods are applied in various applications including latent patient profile modelling in medical data analysis and representation learning for image retrieval . the experimental results demonstrate the models perform better than baseline methods in medical data and outperform state-of-the-art rivals in image dataset .", "topics": ["baseline ( configuration management )", "feature learning"]}
{"title": "learning structural changes of gaussian graphical models in controlled experiments", "abstract": "graphical models are widely used in scienti fic and engineering research to represent conditional independence structures between random variables . in many controlled experiments , environmental changes or external stimuli can often alter the conditional dependence between the random variables , and potentially produce significant structural changes in the corresponding graphical models . therefore , it is of great importance to be able to detect such structural changes from data , so as to gain novel insights into where and how the structural changes take place and help the system adapt to the new environment . here we report an effective learning strategy to extract structural changes in gaussian graphical model using l1-regularization based convex optimization . we discuss the properties of the problem formulation and introduce an efficient implementation by the block coordinate descent algorithm . we demonstrate the principle of the approach on a numerical simulation experiment , and we then apply the algorithm to the modeling of gene regulatory networks under different conditions and obtain promising yet biologically plausible results .", "topics": ["graphical model", "numerical analysis"]}
{"title": "energy-based generative adversarial network", "abstract": "we introduce the `` energy-based generative adversarial network '' model ( ebgan ) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions . similar to the probabilistic gans , a generator is seen as being trained to produce contrastive samples with minimal energies , while the discriminator is trained to assign high energies to these generated samples . viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output . among them , we show one instantiation of ebgan framework as using an auto-encoder architecture , with the energy being the reconstruction error , in place of the discriminator . we show that this form of ebgan exhibits more stable behavior than regular gans during training . we also show that a single-scale architecture can be trained to generate high-resolution images .", "topics": ["mathematical optimization", "encoder"]}
{"title": "parallelization of the lbg vector quantization algorithm for shared memory systems", "abstract": "this paper proposes a parallel approach for the vector quantization ( vq ) problem in image processing . vq deals with codebook generation from the input training data set and replacement of any arbitrary data with the nearest codevector . most of the efforts in vq have been directed towards designing parallel search algorithms for the codebook , and little has hitherto been done in evolving a parallelized procedure to obtain an optimum codebook . this parallel algorithm addresses the problem of designing an optimum codebook using the traditional lbg type of vector quantization algorithm for shared memory systems and for the efficient usage of parallel processors . using the codebook formed from a training set , any arbitrary input data is replaced with the nearest codevector from the codebook . the effectiveness of the proposed algorithm is indicated .", "topics": ["test set", "image processing"]}
{"title": "neural variational inference and learning in belief networks", "abstract": "highly expressive directed latent variable models , such as sigmoid belief networks , are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well . we propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior . the model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood . although the naive estimator of the inference model gradient is too high-variance to be useful , we make it practical by applying several straightforward model-independent variance reduction techniques . applying our approach to training sigmoid belief networks and deep autoregressive networks , we show that it outperforms the wake-sleep algorithm on mnist and achieves state-of-the-art results on the reuters rcv1 document dataset .", "topics": ["calculus of variations", "bayesian network"]}
{"title": "a recurrent neural network without chaos", "abstract": "we introduce an exceptionally simple gated recurrent neural network ( rnn ) that achieves performance comparable to well-known gated architectures , such as lstms and grus , on the word-level language modeling task . we prove that our model has simple , predicable and non-chaotic dynamics . this stands in stark contrast to more standard gated architectures , whose underlying dynamical systems exhibit chaotic behavior .", "topics": ["recurrent neural network"]}
{"title": "improved strongly adaptive online learning using coin betting", "abstract": "this paper describes a new parameter-free online learning algorithm for changing environments . in comparing against algorithms with the same time complexity as ours , we obtain a strongly adaptive regret bound that is a factor of at least $ \\sqrt { \\log ( t ) } $ better , where $ t $ is the time horizon . empirical results show that our algorithm outperforms state-of-the-art methods in learning with expert advice and metric learning scenarios .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "a cooperative q-learning approach for real-time power allocation in femtocell networks", "abstract": "in this paper , we address the problem of distributed interference management of cognitive femtocells that share the same frequency range with macrocells ( primary user ) using distributed multi-agent q-learning . we formulate and solve three problems representing three different q-learning algorithms : namely , centralized , distributed and partially distributed power control using q-learning ( cpc-q , dpc-q and pdpc-q ) . cpcq , although not of practical interest , characterizes the global optimum . each of dpc-q and pdpc-q works in two different learning paradigms : independent ( il ) and cooperative ( cl ) . the former is considered the simplest form for applying qlearning in multi-agent scenarios , where all the femtocells learn independently . the latter is the proposed scheme in which femtocells share partial information during the learning process in order to strike a balance between practical relevance and performance . in terms of performance , the simulation results showed that the cl paradigm outperforms the il paradigm and achieves an aggregate femtocells capacity that is very close to the optimal one . for the practical relevance issue , we evaluate the robustness and scalability of dpc-q , in real time , by deploying new femtocells in the system during the learning process , where we showed that dpc-q in the cl paradigm is scalable to large number of femtocells and more robust to the network dynamics compared to the il paradigm", "topics": ["mathematical optimization", "simulation"]}
{"title": "torque ripple minimization in a switched reluctance drive by neuro-fuzzy compensation", "abstract": "simple power electronic drive circuit and fault tolerance of converter are specific advantages of srm drives , but excessive torque ripple has limited its use to special applications . it is well known that controlling the current shape adequately can minimize the torque ripple . this paper presents a new method for shaping the motor currents to minimize the torque ripple , using a neuro-fuzzy compensator . in the proposed method , a compensating signal is added to the output of a pi controller , in a current-regulated speed control loop . numerical results are presented in this paper , with an analysis of the effects of changing the form of the membership function of the neuro-fuzzy compensator .", "topics": ["numerical analysis"]}
{"title": "fingerprint verification based on gabor filter enhancement", "abstract": "human fingerprints are reliable characteristics for personnel identification as it is unique and persistence . a fingerprint pattern consists of ridges , valleys and minutiae . in this paper we propose fingerprint verification based on gabor filter enhancement ( fvgfe ) algorithm for minutiae feature extraction and post processing based on 9 pixel neighborhood . a global feature extraction and fingerprints enhancement are based on hong enhancement method which is simultaneously able to extract local ridge orientation and ridge frequency . it is observed that the sensitivity and specificity values are better compared to the existing algorithms .", "topics": ["feature extraction"]}
{"title": "on the expressive efficiency of sum product networks", "abstract": "sum product networks ( spns ) are a recently developed class of deep generative models which compute their associated unnormalized density functions using a special type of arithmetic circuit . when certain sufficient conditions , called the decomposability and completeness conditions ( or `` d & c '' conditions ) , are imposed on the structure of these circuits , marginal densities and other useful quantities , which are typically intractable for other deep generative models , can be computed by what amounts to a single evaluation of the network ( which is a property known as `` validity '' ) . however , the effect that the d & c conditions have on the capabilities of d & c spns is not well understood . in this work we analyze the d & c conditions , expose the various connections that d & c spns have with multilinear arithmetic circuits , and consider the question of how well they can capture various distributions as a function of their size and depth . among our various contributions is a result which establishes the existence of a relatively simple distribution with fully tractable marginal densities which can not be efficiently captured by d & c spns of any depth , but which can be efficiently captured by various other deep generative models . we also show that with each additional layer of depth permitted , the set of distributions which can be efficiently captured by d & c spns grows in size . this kind of `` depth hierarchy '' property has been widely conjectured to hold for various deep models , but has never been proven for any of them . some of our other contributions include a new characterization of the d & c conditions as sufficient and necessary ones for a slightly strengthened notion of validity , and various state-machine characterizations of the types of computations that can be performed efficiently by d & c spns .", "topics": ["generative model", "computation"]}
{"title": "character-level and multi-channel convolutional neural networks for large-scale authorship attribution", "abstract": "convolutional neural networks ( cnns ) have demonstrated superior capability for extracting information from raw signals in computer vision . recently , character-level and multi-channel cnns have exhibited excellent performance for sentence classification tasks . we apply cnns to large-scale authorship attribution , which aims to determine an unknown text 's author among many candidate authors , motivated by their ability to process character-level signals and to differentiate between a large number of classes , while making fast predictions in comparison to state-of-the-art approaches . we extensively evaluate cnn-based approaches that leverage word and character channels and compare them against state-of-the-art methods for a large range of author numbers , shedding new light on traditional approaches . we show that character-level cnns outperform the state-of-the-art on four out of five datasets in different domains . additionally , we present the first application of authorship attribution to reddit .", "topics": ["neural networks", "computer vision"]}
{"title": "robust mission design through evidence theory and multi-agent collaborative search", "abstract": "in this paper , the preliminary design of a space mission is approached introducing uncertainties on the design parameters and formulating the resulting reliable design problem as a multiobjective optimization problem . uncertainties are modelled through evidence theory and the belief , or credibility , in the successful achievement of mission goals is maximised along with the reliability of constraint satisfaction . the multiobjective optimisation problem is solved through a novel algorithm based on the collaboration of a population of agents in search for the set of highly reliable solutions . two typical problems in mission analysis are used to illustrate the proposed methodology .", "topics": ["optimization problem"]}
{"title": "confidence decision trees via online and active learning for streaming ( big ) data", "abstract": "decision tree classifiers are a widely used tool in data stream mining . the use of confidence intervals to estimate the gain associated with each split leads to very effective methods , like the popular hoeffding tree algorithm . from a statistical viewpoint , the analysis of decision tree classifiers in a streaming setting requires knowing when enough new information has been collected to justify splitting a leaf . although some of the issues in the statistical analysis of hoeffding trees have been already clarified , a general and rigorous study of confidence intervals for splitting criteria is missing . we fill this gap by deriving accurate confidence intervals to estimate the splitting gain in decision tree learning with respect to three criteria : entropy , gini index , and a third index proposed by kearns and mansour . our confidence intervals depend in a more detailed way on the tree parameters . we also extend our confidence analysis to a selective sampling setting , in which the decision tree learner adaptively decides which labels to query in the stream . we furnish theoretical guarantee bounding the probability that the classification is non-optimal learning the decision tree via our selective sampling strategy . experiments on real and synthetic data in a streaming setting show that our trees are indeed more accurate than trees with the same number of leaves generated by other techniques and our active learning module permits to save labeling cost . in addition , comparing our labeling strategy with recent methods , we show that our approach is more robust and consistent respect all the other techniques applied to incremental decision trees .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "sentence simplification with deep reinforcement learning", "abstract": "sentence simplification aims to make sentences easier to read and understand . most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences . we address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework . our model , which we call { \\sc dress } ( as shorthand for { \\bf d } eep { \\bf re } inforcement { \\bf s } entence { \\bf s } implification ) , explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple , fluent , and preserve the meaning of the input . experiments on three datasets demonstrate that our model outperforms competitive simplification systems .", "topics": ["machine translation", "reinforcement learning"]}
{"title": "hidden markov random field iterative closest point", "abstract": "when registering point clouds resolved from an underlying 2-d pixel structure , such as those resulting from structured light and flash lidar sensors , or stereo reconstruction , it is expected that some points in one cloud do not have corresponding points in the other cloud , and that these would occur together , such as along an edge of the depth map . in this work , a hidden markov random field model is used to capture this prior within the framework of the iterative closest point algorithm . the em algorithm is used to estimate the distribution parameters and the hidden component memberships . experiments are presented demonstrating that this method outperforms several other outlier rejection methods when the point clouds have low or moderate overlap .", "topics": ["sensor", "pixel"]}
{"title": "smooth neighbors on teacher graphs for semi-supervised learning", "abstract": "the paper proposes an inductive semi-supervised learning method , called smooth neighbors on teacher graphs ( sntg ) . at each iteration during training , a graph is dynamically constructed based on predictions of the teacher model , i.e . , the implicit self-ensemble of models . then the graph serves as a similarity measure with respect to which the representations of `` similar '' neighboring points are learned to be smooth on the low dimensional manifold . we achieve state-of-the-art results on semi-supervised learning benchmarks . the error rates are 9.89 % , 3.99 % for cifar-10 with 4000 labels , svhn with 500 labels , respectively . in particular , the improvements are significant when the labels are scarce . for non-augmented mnist with only 20 labels , the error rate is reduced from previous 4.81 % to 1.36 % . our method is also effective under noisy supervision and shows robustness to incorrect labels .", "topics": ["supervised learning", "mnist database"]}
{"title": "deep networks with large output spaces", "abstract": "deep neural networks have been extremely successful at various image , speech , video recognition tasks because of their ability to model deep structures within the data . however , they are still prohibitively expensive to train and apply for problems containing millions of classes in the output layer . based on the observation that the key computation common to most neural network layers is a vector/matrix product , we propose a fast locality-sensitive hashing technique to approximate the actual dot product enabling us to scale up the training and inference to millions of output classes . we evaluate our technique on three diverse large-scale recognition tasks and show that our approach can train large-scale models at a faster rate ( in terms of steps/total time ) compared to baseline methods .", "topics": ["baseline ( configuration management )", "approximation algorithm"]}
{"title": "bayesian optimization with gradients", "abstract": "bayesian optimization has been successful at global optimization of expensive-to-evaluate multimodal objective functions . however , unlike most optimization methods , bayesian optimization typically does not use derivative information . in this paper we show how bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance . in particular , we develop a novel bayesian optimization algorithm , the derivative-enabled knowledge-gradient ( dkg ) , for which we show one-step bayes-optimality , asymptotic consistency , and greater one-step value of information than is possible in the derivative-free setting . our procedure accommodates noisy and incomplete derivative information , comes in both sequential and batch forms , and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative . we also compute the d-kg acquisition function and its gradient using a novel fast discretization-free technique . we show d-kg provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients , on benchmarks including logistic regression , deep learning , kernel learning , and k-nearest neighbors .", "topics": ["mathematical optimization", "optimization problem"]}
{"title": "fair task allocation in transportation", "abstract": "task allocation problems have traditionally focused on cost optimization . however , more and more attention is being given to cases in which cost should not always be the sole or major consideration . in this paper we study a fair task allocation problem in transportation where an optimal allocation not only has low cost but more importantly , it distributes tasks as even as possible among heterogeneous participants who have different capacities and costs to execute tasks . to tackle this fair minimum cost allocation problem we analyze and solve it in two parts using two novel polynomial-time algorithms . we show that despite the new fairness criterion , the proposed algorithms can solve the fair minimum cost allocation problem optimally in polynomial time . in addition , we conduct an extensive set of experiments to investigate the trade-off between cost minimization and fairness . our experimental results demonstrate the benefit of factoring fairness into task allocation . among the majority of test instances , fairness comes with a very small price in terms of cost .", "topics": ["optimization problem", "time complexity"]}
{"title": "detecting concept-level emotion cause in microblogging", "abstract": "in this paper , we propose a concept-level emotion cause model ( cecm ) , instead of the mere word-level models , to discover causes of microblogging users ' diversified emotions on specific hot event . a modified topic-supervised biterm topic model is utilized in cecm to detect emotion topics ' in event-related tweets , and then context-sensitive topical pagerank is utilized to detect meaningful multiword expressions as emotion causes . experimental results on a dataset from sina weibo , one of the largest microblogging websites in china , show cecm can better detect emotion causes than baseline methods .", "topics": ["baseline ( configuration management )"]}
{"title": "robot vision architecture for autonomous clothes manipulation", "abstract": "this paper presents a novel robot vision architecture for perceiving generic 3d clothes configurations . our architecture is hierarchically structured , starting from low-level curvatures , across mid-level geometric shapes \\ & topology descriptions ; and finally approaching high-level semantic surface structure descriptions . we demonstrate our robot vision architecture in a customised dual-arm industrial robot with our self-designed , off-the-self stereo vision system , carrying out autonomous grasping and dual-arm flattening . it is worth noting that the proposed dual-arm flattening approach is unique among the state-of-the-art robot autonomous system , which is the major contribution of this paper . the experimental results show that the proposed dual-arm flattening using stereo vision system remarkably outperforms the single-arm flattening and widely-cited kinect-based sensing system for dexterous manipulation tasks . in addition , the proposed grasping approach achieves satisfactory performance on grasping various kind of garments , verifying the capability of proposed visual perception architecture to be adapted to more than one clothing manipulation tasks .", "topics": ["high- and low-level", "autonomous car"]}
{"title": "partitioning relational matrices of similarities or dissimilarities using the value of information", "abstract": "in this paper , we provide an approach to clustering relational matrices whose entries correspond to either similarities or dissimilarities between objects . our approach is based on the value of information , a parameterized , information-theoretic criterion that measures the change in costs associated with changes in information . optimizing the value of information yields a deterministic annealing style of clustering with many benefits . for instance , investigators avoid needing to a priori specify the number of clusters , as the partitions naturally undergo phase changes , during the annealing process , whereby the number of clusters changes in a data-driven fashion . the global-best partition can also often be identified .", "topics": ["cluster analysis", "eisenstein 's criterion"]}
{"title": "optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays", "abstract": "we discuss a multiple-play multi-armed bandit ( mab ) problem in which several arms are selected at each round . recently , thompson sampling ( ts ) , a randomized algorithm with a bayesian spirit , has attracted much attention for its empirically excellent performance , and it is revealed to have an optimal regret bound in the standard single-play mab problem . in this paper , we propose the multiple-play thompson sampling ( mp-ts ) algorithm , an extension of ts to the multiple-play mab problem , and discuss its regret analysis . we prove that mp-ts for binary rewards has the optimal regret upper bound that matches the regret lower bound provided by anantharam et al . ( 1987 ) . therefore , mp-ts is the first computationally efficient algorithm with optimal regret . a set of computer simulations was also conducted , which compared mp-ts with state-of-the-art algorithms . we also propose a modification of mp-ts , which is shown to have better empirical performance .", "topics": ["regret ( decision theory )"]}
{"title": "gait pattern recognition using accelerometers", "abstract": "motion ability is one of the most important human properties , including gait as a basis of human transitional movement . gait , as a biometric for recognizing human identities , can be non-intrusively captured signals using wearable or portable smart devices . in this study gait patterns is collected using a wireless platform of two sensors located at chest and right ankle of the subjects . then the raw data has undergone some preprocessing methods and segmented into 5 seconds windows . some time and frequency domain features is extracted and the performance evaluated by 5 different classifiers . decision tree ( with all features ) and k-nearest neighbors ( with 10 selected features ) classifiers reached 99.4 % and 100 % respectively .", "topics": ["sensor"]}
{"title": "darla : improving zero-shot transfer in reinforcement learning", "abstract": "domain adaptation is an important open problem in deep reinforcement learning ( rl ) . in many scenarios of interest data is hard to obtain , so agents may learn a source policy in a setting where data is readily available , with the hope that it generalises well to the target domain . we propose a new multi-stage rl agent , darla ( disentangled representation learning agent ) , which learns to see before learning to act . darla 's vision is based on learning a disentangled representation of the observed environment . once darla can see , it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain . darla significantly outperforms conventional baselines in zero-shot domain adaptation scenarios , an effect that holds across a variety of rl environments ( jaco arm , deepmind lab ) and base rl algorithms ( dqn , a3c and ec ) .", "topics": ["reinforcement learning"]}
{"title": "learning with a wasserstein loss", "abstract": "learning to predict multi-label outputs is challenging , but in many problems there is a natural metric on the outputs that can be used to improve predictions . in this paper we develop a loss function for multi-label learning , based on the wasserstein distance . the wasserstein distance provides a natural notion of dissimilarity for probability measures . although optimizing with respect to the exact wasserstein distance is costly , recent work has described a regularized approximation that is efficiently computed . we describe an efficient learning algorithm based on this regularization , as well as a novel extension of the wasserstein distance from probability measures to unnormalized measures . we also describe a statistical learning bound for the loss . the wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space . we demonstrate this property on a real-data tag prediction problem , using the yahoo flickr creative commons dataset , outperforming a baseline that does n't use the metric .", "topics": ["baseline ( configuration management )", "loss function"]}
{"title": "predicting patient state-of-health using sliding window and recurrent classifiers", "abstract": "bedside monitors in intensive care units ( icus ) frequently sound incorrectly , slowing response times and desensitising nurses to alarms ( chambrin , 2001 ) , causing true alarms to be missed ( hug et al . , 2011 ) . we compare sliding window predictors with recurrent predictors to classify patient state-of-health from icu multivariate time series ; we report slightly improved performance for the rnn for three out of four targets .", "topics": ["time series"]}
{"title": "discrete geodesic calculus in the space of viscous fluidic objects", "abstract": "based on a local approximation of the riemannian distance on a manifold by a computationally cheap dissimilarity measure , a time discrete geodesic calculus is developed , and applications to shape space are explored . the dissimilarity measure is derived from a deformation energy whose hessian reproduces the underlying riemannian metric , and it is used to define length and energy of discrete paths in shape space . the notion of discrete geodesics defined as energy minimizing paths gives rise to a discrete logarithmic map , a variational definition of a discrete exponential map , and a time discrete parallel transport . this new concept is applied to a shape space in which shapes are considered as boundary contours of physical objects consisting of viscous material . the flexibility and computational efficiency of the approach is demonstrated for topology preserving shape morphing , the representation of paths in shape space via local shape variations as path generators , shape extrapolation via discrete geodesic flow , and the transfer of geometric features .", "topics": ["calculus of variations", "time complexity"]}
{"title": "a bayesian approach to learning bayesian networks with local structure", "abstract": "recently several researchers have investigated techniques for using data to learn bayesian networks containing compact representations for the conditional probability distributions ( cpds ) stored at each node . the majority of this work has concentrated on using decision-tree representations for the cpds . in addition , researchers typically apply non-bayesian ( or asymptotically bayesian ) scoring functions such as mdl to evaluate the goodness-of-fit of networks to the data . in this paper we investigate a bayesian approach to learning bayesian networks that contain the more general decision-graph representations of the cpds . first , we describe how to evaluate the posterior probability that is , the bayesian score of such a network , given a database of observed cases . second , we describe various search spaces that can be used , in conjunction with a scoring function and a search procedure , to identify one or more high-scoring networks . finally , we present an experimental evaluation of the search spaces , using a greedy algorithm and a bayesian scoring function .", "topics": ["bayesian network"]}
{"title": "computing multi-relational sufficient statistics for large databases", "abstract": "databases contain information about which relationships do and do not hold among entities . to make this information accessible for statistical analysis requires computing sufficient statistics that combine information from different database tables . such statistics may involve any number of { \\em positive and negative } relationships . with a naive enumeration approach , computing sufficient statistics for negative relationships is feasible only for small databases . we solve this problem with a new dynamic programming algorithm that performs a virtual join , where the requisite counts are computed without materializing join tables . contingency table algebra is a new extension of relational algebra , that facilitates the efficient implementation of this m\\ '' obius virtual join operation . the m\\ '' obius join scales to large datasets ( over 1m tuples ) with complex schemas . empirical evaluation with seven benchmark datasets showed that information about the presence and absence of links can be exploited in feature selection , association rule mining , and bayesian network learning .", "topics": ["entity", "bayesian network"]}
{"title": "polynomial neural networks learnt to classify eeg signals", "abstract": "a neural network based technique is presented , which is able to successfully extract polynomial classification rules from labeled electroencephalogram ( eeg ) signals . to represent the classification rules in an analytical form , we use the polynomial neural networks trained by a modified group method of data handling ( gmdh ) . the classification rules were extracted from clinical eeg data that were recorded from an alzheimer patient and the sudden death risk patients . the third data is eeg recordings that include the normal and artifact segments . these eeg data were visually identified by medical experts . the extracted polynomial rules verified on the testing eeg data allow to correctly classify 72 % of the risk group patients and 96.5 % of the segments . these rules performs slightly better than standard feedforward neural networks .", "topics": ["neural networks", "polynomial"]}
{"title": "ppfnet : global context aware local features for robust 3d point matching", "abstract": "we present ppfnet - point pair feature network for deeply learning a globally informed 3d local feature descriptor to find correspondences in unorganized point clouds . ppfnet learns local descriptors on pure geometry and is highly aware of the global context , an important cue in deep learning . our 3d representation is computed as a collection of point-pair-features combined with the points and normals within a local vicinity . our permutation invariant network design is inspired by pointnet and sets ppfnet to be ordering-free . as opposed to voxelization , our method is able to consume raw point clouds to exploit the full sparsity . ppfnet uses a novel $ \\textit { n-tuple } $ loss and architecture injecting the global information naturally into the local descriptor . it shows that context awareness also boosts the local feature representation . qualitative and quantitative evaluations of our network suggest increased recall , improved robustness and invariance as well as a vital step in the 3d descriptor extraction performance .", "topics": ["sparse matrix"]}
{"title": "deep multi-instance transfer learning", "abstract": "we present a new approach for transferring knowledge from groups to individuals that comprise them . we evaluate our method in text , by inferring the ratings of individual sentences using full-review ratings . this approach , which combines ideas from transfer learning , deep learning and multi-instance learning , reduces the need for laborious human labelling of fine-grained data when abundant labels are available at the group level .", "topics": ["reinforcement learning"]}
{"title": "self-taught support vector machine", "abstract": "in this paper , a new approach for classification of target task using limited labeled target data as well as enormous unlabeled source data is proposed which is called self-taught learning . the target and source data can be drawn from different distributions . in the previous approaches , covariate shift assumption is considered where the marginal distributions p ( x ) change over domains and the conditional distributions p ( y|x ) remain the same . in our approach , we propose a new objective function which simultaneously learns a common space t ( . ) where the conditional distributions over domains p ( t ( x ) |y ) remain the same and learns robust svm classifiers for target task using both source and target data in the new representation . hence , in the proposed objective function , the hidden label of the source data is also incorporated . we applied the proposed approach on caltech-256 , msrc+lmo datasets and compared the performance of our algorithm to the available competing methods . our method has a superior performance to the successful existing algorithms .", "topics": ["optimization problem", "support vector machine"]}
{"title": "learning language from a large ( unannotated ) corpus", "abstract": "a novel approach to the fully automated , unsupervised extraction of dependency grammars and associated syntax-to-semantic-relationship mappings from large text corpora is described . the suggested approach builds on the authors ' prior work with the link grammar , relex and opencog systems , as well as on a number of prior papers and approaches from the statistical language learning literature . if successful , this approach would enable the mining of all the information needed to power a natural language comprehension and generation system , directly from a large , unannotated corpus .", "topics": ["natural language", "text corpus"]}
{"title": "recovery of sparse and low rank components of matrices using iterative method with adaptive thresholding", "abstract": "in this letter , we propose an algorithm for recovery of sparse and low rank components of matrices using an iterative method with adaptive thresholding . in each iteration , the low rank and sparse components are obtained using a thresholding operator . this algorithm is fast and can be implemented easily . we compare it with one of the most common fast methods in which the rank and sparsity are approximated by $ \\ell_1 $ norm . we also apply it to some real applications where the noise is not so sparse . the simulation results show that it has a suitable performance with low run-time .", "topics": ["sparse matrix", "simulation"]}
{"title": "what happened to my dog in that network : unraveling top-down generators in convolutional neural networks", "abstract": "top-down information plays a central role in human perception , but plays relatively little role in many current state-of-the-art deep networks , such as convolutional neural networks ( cnns ) . this work seeks to explore a path by which top-down information can have a direct impact within current deep networks . we explore this path by learning and using `` generators '' corresponding to the network internal effects of three types of transformation ( each a restriction of a general affine transformation ) : rotation , scaling , and translation . we demonstrate how these learned generators can be used to transfer top-down information to novel settings , as mediated by the `` feature flows '' that the transformations ( and the associated generators ) correspond to inside the network . specifically , we explore three aspects : 1 ) using generators as part of a method for synthesizing transformed images -- - given a previously unseen image , produce versions of that image corresponding to one or more specified transformations , 2 ) `` zero-shot learning '' -- - when provided with a feature flow corresponding to the effect of a transformation of unknown amount , leverage learned generators as part of a method by which to perform an accurate categorization of the amount of transformation , even for amounts never observed during training , and 3 ) ( inside-cnn ) `` data augmentation '' -- - improve the classification performance of an existing network by using the learned generators to directly provide additional training `` inside the cnn '' .", "topics": ["neural networks"]}
{"title": "importance weighted autoencoders", "abstract": "the variational autoencoder ( vae ; kingma , welling ( 2014 ) ) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference . it typically makes strong assumptions about posterior inference , for instance that the posterior distribution is approximately factorial , and that its parameters can be approximated with nonlinear regression from the observations . as we show empirically , the vae objective can lead to overly simplified representations which fail to use the network 's entire modeling capacity . we present the importance weighted autoencoder ( iwae ) , a generative model with the same architecture as the vae , but which uses a strictly tighter log-likelihood lower bound derived from importance weighting . in the iwae , the recognition network uses multiple samples to approximate the posterior , giving it increased flexibility to model complex posteriors which do not fit the vae modeling assumptions . we show empirically that iwaes learn richer latent space representations than vaes , leading to improved test log-likelihood on density estimation benchmarks .", "topics": ["generative model", "calculus of variations"]}
{"title": "bayesian sparsification of recurrent neural networks", "abstract": "recurrent neural networks show state-of-the-art results in many text analysis tasks but often require a lot of memory to store their weights . recently proposed sparse variational dropout eliminates the majority of the weights in a feed-forward neural network without significant loss of quality . we apply this technique to sparsify recurrent neural networks . to account for recurrent specifics we also rely on binary variational dropout for rnn . we report 99.5 % sparsity level on sentiment analysis task without a quality drop and up to 87 % sparsity level on language modeling task with slight loss of accuracy .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "progressive neural architecture search", "abstract": "we propose a new method for learning the structure of convolutional neural networks ( cnns ) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms . our approach uses a sequential model-based optimization ( smbo ) strategy , in which we search for structures in order of increasing complexity , while simultaneously learning a surrogate model to guide the search through structure space . direct comparison under the same search space shows that our method is up to 5 times more efficient than the rl method of zoph et al . ( 2018 ) in terms of number of models evaluated , and 8 times faster in terms of total compute . the structures we discover in this way achieve state of the art classification accuracies on cifar-10 and imagenet .", "topics": ["reinforcement learning"]}
{"title": "learning sparse visual representations with leaky capped norm regularizers", "abstract": "sparsity inducing regularization is an important part for learning over-complete visual representations . despite the popularity of $ \\ell_1 $ regularization , in this paper , we investigate the usage of non-convex regularizations in this problem . our contribution consists of three parts . first , we propose the leaky capped norm regularization ( lcnr ) , which allows model weights below a certain threshold to be regularized more strongly as opposed to those above , therefore imposes strong sparsity and only introduces controllable estimation bias . we propose a majorization-minimization algorithm to optimize the joint objective function . second , our study over monocular 3d shape recovery and neural networks with lcnr outperforms $ \\ell_1 $ and other non-convex regularizations , achieving state-of-the-art performance and faster convergence . third , we prove a theoretical global convergence speed on the 3d recovery problem . to the best of our knowledge , this is the first convergence analysis of the 3d recovery problem .", "topics": ["time complexity", "matrix regularization"]}
{"title": "settling the polynomial learnability of mixtures of gaussians", "abstract": "given data drawn from a mixture of multivariate gaussians , a basic problem is to accurately estimate the mixture parameters . we give an algorithm for this problem that has a running time , and data requirement polynomial in the dimension and the inverse of the desired accuracy , with provably minimal assumptions on the gaussians . as simple consequences of our learning algorithm , we can perform near-optimal clustering of the sample points and density estimation for mixtures of k gaussians , efficiently . the building blocks of our algorithm are based on the work kalai et al . [ stoc 2010 ] that gives an efficient algorithm for learning mixtures of two gaussians by considering a series of projections down to one dimension , and applying the method of moments to each univariate projection . a major technical hurdle in kalai et al . is showing that one can efficiently learn univariate mixtures of two gaussians . in contrast , because pathological scenarios can arise when considering univariate projections of mixtures of more than two gaussians , the bulk of the work in this paper concerns how to leverage an algorithm for learning univariate mixtures ( of many gaussians ) to yield an efficient algorithm for learning in high dimensions . our algorithm employs hierarchical clustering and rescaling , together with delicate methods for backtracking and recovering from failures that can occur in our univariate algorithm . finally , while the running time and data requirements of our algorithm depend exponentially on the number of gaussians in the mixture , we prove that such a dependence is necessary .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "scene-centric joint parsing of cross-view videos", "abstract": "cross-view video understanding is an important yet under-explored area in computer vision . in this paper , we introduce a joint parsing framework that integrates view-centric proposals into scene-centric parse graphs that represent a coherent scene-centric understanding of cross-view scenes . our key observations are that overlapping fields of views embed rich appearance and geometry correlations and that knowledge fragments corresponding to individual vision tasks are governed by consistency constraints available in commonsense knowledge . the proposed joint parsing framework represents such correlations and constraints explicitly and generates semantic scene-centric parse graphs . quantitative experiments show that scene-centric predictions in the parse graph outperform view-centric predictions .", "topics": ["computer vision", "parsing"]}
{"title": "data augmentation by pairing samples for images classification", "abstract": "data augmentation is a widely used technique in many machine learning tasks , such as image classification , to virtually enlarge the training dataset size and avoid overfitting . traditional data augmentation techniques for image classification tasks create new samples from the original training data by , for example , flipping , distorting , adding a small amount of noise to , or cropping a patch from an original image . in this paper , we introduce a simple but surprisingly effective data augmentation technique for image classification tasks . with our technique , named samplepairing , we synthesize a new sample from one image by overlaying another image randomly chosen from the training data ( i.e . , taking an average of two images for each pixel ) . by using two images randomly selected from the training set , we can generate $ n^2 $ new samples from $ n $ training samples . this simple data augmentation technique significantly improved classification accuracy for all the tested datasets ; for example , the top-1 error rate was reduced from 33.5 % to 29.0 % for the ilsvrc 2012 dataset with googlenet and from 8.22 % to 6.93 % in the cifar-10 dataset . we also show that our samplepairing technique largely improved accuracy when the number of samples in the training set was very small . therefore , our technique is more valuable for tasks with a limited amount of training data , such as medical imaging tasks .", "topics": ["test set", "computer vision"]}
{"title": "novel views of objects from a single image", "abstract": "taking an image of an object is at its core a lossy process . the rich information about the three-dimensional structure of the world is flattened to an image plane and decisions such as viewpoint and camera parameters are final and not easily revertible . as a consequence , possibilities of changing viewpoint are limited . given a single image depicting an object , novel-view synthesis is the task of generating new images that render the object from a different viewpoint than the one given . the main difficulty is to synthesize the parts that are disoccluded ; disocclusion occurs when parts of an object are hidden by the object itself under a specific viewpoint . in this work , we show how to improve novel-view synthesis by making use of the correlations observed in 3d models and applying them to new image instances . we propose a technique to use the structural information extracted from a 3d model that matches the image object in terms of viewpoint and shape . for the latter part , we propose an efficient 2d-to-3d alignment method that associates precisely the image appearance with the 3d model geometry with minimal user interaction . our technique is able to simulate plausible viewpoint changes for a variety of object classes within seconds . additionally , we show that our synthesized images can be used as additional training data that improves the performance of standard object detectors .", "topics": ["test set"]}
{"title": "privileged multi-label learning", "abstract": "this paper presents privileged multi-label learning ( prml ) to explore and exploit the relationship between labels in multi-label learning problems . we suggest that for each individual label , it can not only be implicitly connected with other labels via the low-rank constraint over label predictors , but also its performance on examples can receive the explicit comments from other labels together acting as an \\emph { oracle teacher } . we generate privileged label feature for each example and its individual label , and then integrate it into the framework of low-rank based multi-label learning . the proposed algorithm can therefore comprehensively explore and exploit label relationships by inheriting all the merits of privileged information and low-rank constraints . we show that prml can be efficiently solved by dual coordinate descent algorithm using iterative optimization strategy with cheap updates . experiments on benchmark datasets show that through privileged label features , the performance can be significantly improved and prml is superior to several competing methods in most cases .", "topics": ["reinforcement learning"]}
{"title": "admissible time series motif discovery with missing data", "abstract": "the discovery of time series motifs has emerged as one of the most useful primitives in time series data mining . researchers have shown its utility for exploratory data mining , summarization , visualization , segmentation , classification , clustering , and rule discovery . although there has been more than a decade of extensive research , there is still no technique to allow the discovery of time series motifs in the presence of missing data , despite the well-documented ubiquity of missing data in scientific , industrial , and medical datasets . in this work , we introduce a technique for motif discovery in the presence of missing data . we formally prove that our method is admissible , producing no false negatives . we also show that our method can piggy-back off the fastest known motif discovery method with a small constant factor time/space overhead . we will demonstrate our approach on diverse datasets with varying amounts of missing data", "topics": ["data mining", "cluster analysis"]}
{"title": "deep reconstruction-classification networks for unsupervised domain adaptation", "abstract": "in this paper , we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition . specifically , we design a new model called deep reconstruction-classification network ( drcn ) , which jointly learns a shared encoding representation for two tasks : i ) supervised classification of labeled source data , and ii ) unsupervised reconstruction of unlabeled target data.in this way , the learnt representation not only preserves discriminability , but also encodes useful information from the target domain . our new drcn model can be optimized by using backpropagation similarly as the standard neural networks . we evaluate the performance of drcn on a series of cross-domain object recognition tasks , where drcn provides a considerable improvement ( up to ~8 % in accuracy ) over the prior state-of-the-art algorithms . interestingly , we also observe that the reconstruction pipeline of drcn transforms images from the source domain into images whose appearance resembles the target dataset . this suggests that drcn 's performance is due to constructing a single composite representation that encodes information about both the structure of target images and the classification of source images . finally , we provide a formal analysis to justify the algorithm 's objective in domain adaptation context .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "many paths to equilibrium : gans do not need to decrease a divergence at every step", "abstract": "generative adversarial networks ( gans ) are a family of generative models that do not minimize a single training criterion . unlike other generative models , the data distribution is learned via a game between a generator ( the generative model ) and a discriminator ( a teacher providing training signal ) that each minimize their own cost . gans are designed to reach a nash equilibrium at which each player can not reduce their cost without changing the other players ' parameters . one useful approach for the theory of gans is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium . several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence . we show that this view is overly restrictive . during gan training , the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful . we provide empirical counterexamples to the view of gan training as divergence minimization . specifically , we demonstrate that gans are able to learn distributions in situations where the divergence minimization point of view predicts they would fail . we also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful . this contributes to a growing body of evidence that gan training may be more usefully viewed as approaching nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step .", "topics": ["generative model", "gradient"]}
{"title": "large scale distributed semi-supervised learning using streaming approximation", "abstract": "traditional graph-based semi-supervised learning ( ssl ) approaches , even though widely applied , are not suited for massive data and large label scenarios since they scale linearly with the number of edges $ |e| $ and distinct labels $ m $ . to deal with the large label size problem , recent works propose sketch-based methods to approximate the distribution on labels per node thereby achieving a space reduction from $ o ( m ) $ to $ o ( \\log m ) $ , under certain conditions . in this paper , we present a novel streaming graph-based ssl approximation that captures the sparsity of the label distribution and ensures the algorithm propagates labels accurately , and further reduces the space complexity per node to $ o ( 1 ) $ . we also provide a distributed version of the algorithm that scales well to large data sizes . experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint . we also study different graph construction mechanisms for natural language applications and propose a robust graph augmentation strategy trained using state-of-the-art unsupervised deep learning architectures that yields further significant quality gains .", "topics": ["approximation algorithm", "supervised learning"]}
{"title": "kronecker determinantal point processes", "abstract": "determinantal point processes ( dpps ) are probabilistic models over all subsets a ground set of $ n $ items . they have recently gained prominence in several applications that rely on `` diverse '' subsets . however , their applicability to large problems is still limited due to the $ \\mathcal o ( n^3 ) $ complexity of core tasks such as sampling and learning . we enable efficient sampling and learning for dpps by introducing krondpp , a dpp model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices . this decomposition immediately enables fast exact sampling . but contrary to what one may expect , leveraging the kronecker product structure for speeding up dpp learning turns out to be more difficult . we overcome this challenge , and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a krondpp .", "topics": ["sampling ( signal processing )"]}
{"title": "auditing black-box models using transparent model distillation with side information", "abstract": "black-box risk scoring models permeate our lives , yet are typically proprietary or opaque . we propose a transparent model distillation approach to audit such models . model distillation was first introduced to transfer knowledge from a large , complex teacher model to a faster , simpler student model without significant loss in prediction accuracy . to this we add a third criterion - transparency . to gain insight into black-box models , we treat them as teachers , training transparent student models to mimic the risk scores assigned by the teacher . moreover , we use side information in the form of the actual outcomes the teacher scoring model was intended to predict in the first place . by training a second transparent model on the outcomes , we can compare the two models to each other . when comparing models trained on risk scores to models trained on outcomes , we show that it is necessary to calibrate the risk-scoring model 's predictions to remove distortion that may have been added to the black-box risk-scoring model during or after its training process . we also show how to compute confidence intervals for the particular class of transparent student models we use - tree-based additive models with pairwise interactions ( ga2ms ) - to support comparison of the two transparent models . we demonstrate the methods on four public datasets : compas , lending club , stop-and-frisk , and chicago police .", "topics": ["interaction", "eisenstein 's criterion"]}
{"title": "polisis : automated analysis and presentation of privacy policies using deep learning", "abstract": "privacy policies are the primary channel through which companies inform users about their data collection and sharing practices . in their current form , policies remain long and difficult to comprehend , thus merely serving the goal of legally protecting the companies . short notices based on information extracted from privacy policies have been shown to be useful and more usable , but face a significant scalability hurdle , given the number of policies and their evolution over time . companies , users , researchers , and regulators still lack usable and scalable tools to cope with the breadth and depth of privacy policies . to address these hurdles , we propose polisis , an automated framework for privacy policies analysis . it enables scalable , dynamic , and multi-dimensional queries on privacy policies . at the core of polisis is a privacy-centric language model , built with 130k privacy policies , and a novel hierarchy of neural network classifiers that caters to the high-level aspects and the fine-grained details of privacy practices . we demonstrate polisis 's modularity and utility with two robust applications that support structured and free-form querying . the structured querying application is the automated assignment of privacy icons from the privacy policies . with polisis , we can achieve an accuracy of 88.4 % on this task , when evaluated against earlier annotations by a group of three legal experts . the second application is pribot , the first free-form question answering about privacy policies . we show that pribot can produce a correct answer among its top-3 results for 82 % of the test questions .", "topics": ["high- and low-level", "scalability"]}
{"title": "3d cell nuclei segmentation with balanced graph partitioning", "abstract": "cell nuclei segmentation is one of the most important tasks in the analysis of biomedical images . with ever-growing sizes and amounts of three-dimensional images to be processed , there is a need for better and faster segmentation methods . graph-based image segmentation has seen a rise in popularity in recent years , but is seen as very costly with regard to computational demand . we propose a new segmentation algorithm which overcomes these limitations . our method uses recursive balanced graph partitioning to segment foreground components of a fast and efficient binarization . we construct a model for the cell nuclei to guide the partitioning process . our algorithm is compared to other state-of-the-art segmentation algorithms in an experimental evaluation on two sets of realistically simulated inputs . our method is faster , has similar or better quality and an acceptable memory overhead .", "topics": ["image segmentation", "simulation"]}
{"title": "active learning with distributional estimates", "abstract": "active learning ( al ) is increasingly important in a broad range of applications . two main al principles to obtain accurate classification with few labeled data are refinement of the current decision boundary and exploration of poorly sampled regions . in this paper we derive a novel al scheme that balances these two principles in a natural way . in contrast to many al strategies , which are based on an estimated class conditional probability ^p ( y|x ) , a key component of our approach is to view this quantity as a random variable , hence explicitly considering the uncertainty in its estimated value . our main contribution is a novel mathematical framework for uncertainty-based al , and a corresponding al scheme , where the uncertainty in ^p ( y|x ) is modeled by a second-order distribution . on the practical side , we show how to approximate such second-order distributions for kernel density classification . finally , we find that over a large number of uci , usps and caltech4 datasets , our al scheme achieves significantly better learning curves than popular al methods such as uncertainty sampling and error reduction sampling , when all use the same kernel density classifier .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "alternating projections for learning with expectation constraints", "abstract": "we present an objective function for learning with unlabeled data that utilizes auxiliary expectation constraints . we optimize this objective function using a procedure that alternates between information and moment projections . our method provides an alternate interpretation of the posterior regularization framework ( graca et al . , 2008 ) , maintains uncertainty during optimization unlike constraint-driven learning ( chang et al . , 2007 ) , and is more efficient than generalized expectation criteria ( mann & mccallum , 2008 ) . applications of this framework include minimally supervised learning , semisupervised learning , and learning with constraints that are more expressive than the underlying model . in experiments , we demonstrate comparable accuracy to generalized expectation criteria for minimally supervised learning , and use expressive structural constraints to guide semi-supervised learning , providing a 3 % -6 % improvement over stateof-the-art constraint-driven learning .", "topics": ["supervised learning", "optimization problem"]}
{"title": "rivalry of two families of algorithms for memory-restricted streaming pca", "abstract": "we study the problem of recovering the subspace spanned by the first $ k $ principal components of $ d $ -dimensional data under the streaming setting , with a memory bound of $ o ( kd ) $ . two families of algorithms are known for this problem . the first family is based on the framework of stochastic gradient descent . nevertheless , the convergence rate of the family can be seriously affected by the learning rate of the descent steps and deserves more serious study . the second family is based on the power method over blocks of data , but setting the block size for its existing algorithms is not an easy task . in this paper , we analyze the convergence rate of a representative algorithm with decayed learning rate ( oja and karhunen , 1985 ) in the first family for the general $ k > 1 $ case . moreover , we propose a novel algorithm for the second family that sets the block sizes automatically and dynamically with faster convergence rate . we then conduct empirical studies that fairly compare the two families on real-world data . the studies reveal the advantages and disadvantages of these two families .", "topics": ["gradient descent", "gradient"]}
{"title": "online keyword spotting with a character-level recurrent neural network", "abstract": "in this paper , we propose a context-aware keyword spotting model employing a character-level recurrent neural network ( rnn ) for spoken term detection in continuous speech . the rnn is end-to-end trained with connectionist temporal classification ( ctc ) to generate the probabilities of character and word-boundary labels . there is no need for the phonetic transcription , senone modeling , or system dictionary in training and testing . also , keywords can easily be added and modified by editing the text based keyword list without retraining the rnn . moreover , the unidirectional rnn processes an infinitely long input audio streams without pre-segmentation and keywords are detected with low-latency before the utterance is finished . experimental results show that the proposed keyword spotter significantly outperforms the deep neural network ( dnn ) and hidden markov model ( hmm ) based keyword-filler model even with less computations .", "topics": ["recurrent neural network", "computation"]}
{"title": "near-optimal reinforcement learning in factored mdps", "abstract": "any reinforcement learning algorithm that applies to all markov decision processes ( mdps ) will suffer $ \\omega ( \\sqrt { sat } ) $ regret on some mdp , where $ t $ is the elapsed time and $ s $ and $ a $ are the cardinalities of the state and action spaces . this implies $ t = \\omega ( sa ) $ time to guarantee a near-optimal policy . in many settings of practical interest , due to the curse of dimensionality , $ s $ and $ a $ can be so enormous that this learning time is unacceptable . we establish that , if the system is known to be a \\emph { factored } mdp , it is possible to achieve regret that scales polynomially in the number of \\emph { parameters } encoding the factored mdp , which may be exponentially smaller than $ s $ or $ a $ . we provide two algorithms that satisfy near-optimal regret bounds in this context : posterior sampling reinforcement learning ( psrl ) and an upper confidence bound algorithm ( ucrl-factored ) .", "topics": ["regret ( decision theory )", "approximation algorithm"]}
{"title": "not-so-clevr : visual relations strain feedforward neural networks", "abstract": "the robust and efficient recognition of visual relations in images is a hallmark of biological vision . here , we argue that , despite recent progress in visual recognition , modern machine vision algorithms are severely limited in their ability to learn visual relations . through controlled experiments , we demonstrate that visual-relation problems strain convolutional neural networks ( cnns ) . the networks eventually break altogether when rote memorization becomes impossible such as when the intra-class variability exceeds their capacity . we further show that another type of feedforward network , called a relational network ( rn ) , which was shown to successfully solve seemingly difficult visual question answering ( vqa ) problems on the clevr datasets , suffers similar limitations . motivated by the comparable success of biological vision , we argue that feedback mechanisms including working memory and attention are the key computational components underlying abstract visual reasoning .", "topics": ["neural networks", "computer vision"]}
{"title": "unbounded human learning : optimal scheduling for spaced repetition", "abstract": "in the study of human learning , there is broad evidence that our ability to retain information improves with repeated exposure and decays with delay since last exposure . this plays a crucial role in the design of educational software , leading to a trade-off between teaching new material and reviewing what has already been taught . a common way to balance this trade-off is spaced repetition , which uses periodic review of content to improve long-term retention . though spaced repetition is widely used in practice , e.g . , in electronic flashcard software , there is little formal understanding of the design of these systems . our paper addresses this gap in three ways . first , we mine log data from spaced repetition software to establish the functional dependence of retention on reinforcement and delay . second , we use this memory model to develop a stochastic model for spaced repetition systems . we propose a queueing network model of the leitner system for reviewing flashcards , along with a heuristic approximation that admits a tractable optimization problem for review scheduling . finally , we empirically evaluate our queueing model through a mechanical turk experiment , verifying a key qualitative prediction of our model : the existence of a sharp phase transition in learning outcomes upon increasing the rate of new item introductions .", "topics": ["optimization problem", "heuristic"]}
{"title": "unsupervised domain adaptation by backpropagation", "abstract": "top-performing deep architectures are trained on massive amounts of labeled data . in the absence of labeled data for a certain task , domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain ( e.g . synthetic images ) are available . here , we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain ( no labeled target-domain data is necessary ) . as the training progresses , the approach promotes the emergence of `` deep '' features that are ( i ) discriminative for the main learning task on the source domain and ( ii ) invariant with respect to the shift between the domains . we show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer . the resulting augmented architecture can be trained using standard backpropagation . overall , the approach can be implemented with little effort using any of the deep-learning packages . the method performs very well in a series of image classification experiments , achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on office datasets .", "topics": ["synthetic data", "gradient"]}
{"title": "from fields to trees", "abstract": "we present new mcmc algorithms for computing the posterior distributions and expectations of the unknown variables in undirected graphical models with regular structure . for demonstration purposes , we focus on markov random fields ( mrfs ) . by partitioning the mrfs into non-overlapping trees , it is possible to compute the posterior distribution of a particular tree exactly by conditioning on the remaining tree . these exact solutions allow us to construct efficient blocked and rao-blackwellised mcmc algorithms . we show empirically that tree sampling is considerably more efficient than other partitioned sampling schemes and the naive gibbs sampler , even in cases where loopy belief propagation fails to converge . we prove that tree sampling exhibits lower variance than the naive gibbs sampler and other naive partitioning schemes using the theoretical measure of maximal correlation . we also construct new information theory tools for comparing different mcmc schemes and show that , under these , tree sampling is more efficient .", "topics": ["sampling ( signal processing )", "graphical model"]}
{"title": "mixing time estimation in reversible markov chains from a single sample path", "abstract": "this article provides the first procedure for computing a fully data-dependent interval that traps the mixing time $ t_ { \\text { mix } } $ of a finite reversible ergodic markov chain at a prescribed confidence level . the interval is computed from a single finite-length sample path from the markov chain , and does not require the knowledge of any parameters of the chain . this stands in contrast to previous approaches , which either only provide point estimates , or require a reset mechanism , or additional prior knowledge . the interval is constructed around the relaxation time $ t_ { \\text { relax } } $ , which is strongly related to the mixing time , and the width of the interval converges to zero roughly at a $ \\sqrt { n } $ rate , where $ n $ is the length of the sample path . upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy . the lower bounds indicate that , unless further restrictions are placed on the chain , no procedure can achieve this accuracy level before seeing each state at least $ \\omega ( t_ { \\text { relax } } ) $ times on the average . finally , future directions of research are identified .", "topics": ["markov chain"]}
{"title": "tensorizing generative adversarial nets", "abstract": "generative adversarial network ( gan ) and its variants demonstrate state-of-the-art performance in the class of generative models . to capture higher dimensional distributions , the common learning procedure requires high computational complexity and large number of parameters . in this paper , we present a new generative adversarial framework by representing each layer as a tensor structure connected by multilinear operations , aiming to reduce the number of model parameters by a large factor while preserving the quality of generalized performance . to learn the model , we develop an efficient algorithm by alternating optimization of the mode connections . experimental results demonstrate that our model can achieve high compression rate for model parameters up to 40 times as compared to the existing gan .", "topics": ["generative model", "computational complexity theory"]}
{"title": "a simple , fast and fully automated approach for midline shift measurement on brain computed tomography", "abstract": "brain ct has become a standard imaging tool for emergent evaluation of brain condition , and measurement of midline shift ( mls ) is one of the most important features to address for brain ct assessment . we present a simple method to estimate mls and propose a new alternative parameter to mls : the ratio of mls over the maximal width of intracranial region ( mls/icwmax ) . three neurosurgeons and our automated system were asked to measure mls and mls/icwmax in the same sets of axial ct images obtained from 41 patients admitted to icu under neurosurgical service . a weighted midline ( wml ) was plotted based on individual pixel intensities , with higher weighted given to the darker portions . the mls could then be measured as the distance between the wml and ideal midline ( iml ) near the foramen of monro . the average processing time to output an automatic mls measurement was around 10 seconds . our automated system achieved an overall accuracy of 90.24 % when the ct images were calibrated automatically , and performed better when the calibrations of head rotation were done manually ( accuracy : 92.68 % ) . mls/icwmax and mls both gave results in same confusion matrices and produced similar roc curve results . we demonstrated a simple , fast and accurate automated system of mls measurement and introduced a new parameter ( mls/icwmax ) as a good alternative to mls in terms of estimating the degree of brain deformation , especially when non-dicom images ( e.g . jpeg ) are more easily accessed .", "topics": ["pixel"]}
{"title": "peaksegjoint : fast supervised peak detection via joint segmentation of multiple count data samples", "abstract": "joint peak detection is a central problem when comparing samples in genomic data analysis , but current algorithms for this task are unsupervised and limited to at most 2 sample types . we propose peaksegjoint , a new constrained maximum likelihood segmentation model for any number of sample types . to select the number of peaks in the segmentation , we propose a supervised penalty learning model . to infer the parameters of these two models , we propose to use a discrete optimization heuristic for the segmentation , and convex optimization for the penalty learning . in comparisons with state-of-the-art peak detection algorithms , peaksegjoint achieves similar accuracy , faster speeds , and a more interpretable model with overlapping peaks that occur in exactly the same positions across all samples .", "topics": ["heuristic"]}
{"title": "learning overcomplete hmms", "abstract": "we study the problem of learning overcomplete hmms -- -those that have many hidden states but a small output alphabet . despite having significant practical importance , such hmms are poorly understood with no known positive or negative results for efficient learning . in this paper , we present several new results -- -both positive and negative -- -which help define the boundaries between the tractable and intractable settings . specifically , we show positive results for a large subclass of hmms whose transition matrices are sparse , well-conditioned , and have small probability mass on short cycles . on the other hand , we show that learning is impossible given only a polynomial number of samples for hmms with a small output alphabet and whose transition matrices are random regular graphs with large degree . we also discuss these results in the context of learning hmms which can capture long-term dependencies .", "topics": ["sparse matrix", "polynomial"]}
{"title": "understanding exhaustive pattern learning", "abstract": "pattern learning in an important problem in natural language processing ( nlp ) . some exhaustive pattern learning ( epl ) methods ( bod , 1992 ) were proved to be flawed ( johnson , 2002 ) , while similar algorithms ( och and ney , 2004 ) showed great advantages on other tasks , such as machine translation . in this article , we first formalize epl , and then show that the probability given by an epl model is constant-factor approximation of the probability given by an ensemble method that integrates exponential number of models obtained with various segmentations of the training data . this work for the first time provides theoretical justification for the widely used epl algorithm in nlp , which was previously viewed as a flawed heuristic method . better understanding of epl may lead to improved pattern learning algorithms in future .", "topics": ["test set", "natural language processing"]}
{"title": "l2 regularization versus batch and weight normalization", "abstract": "batch normalization is a commonly used trick to improve the training of deep neural networks . these neural networks use l2 regularization , also called weight decay , ostensibly to prevent overfitting . however , we show that l2 regularization has no regularizing effect when combined with normalization . instead , regularization has an influence on the scale of weights , and thereby on the effective learning rate . we investigate this dependence , both in theory , and experimentally . we show that popular optimization methods such as adam only partially eliminate the influence of normalization on the learning rate . this leads to a discussion on other ways to mitigate this issue .", "topics": ["matrix regularization"]}
{"title": "adaptive feature abstraction for translating video to text", "abstract": "previous models for video captioning often use the output from a specific layer of a convolutional neural network ( cnn ) as video features . however , the variable context-dependent semantics in the video may make it more appropriate to adaptively select features from the multiple cnn layers . we propose a new approach for generating adaptive spatiotemporal representations of videos for the captioning task . a novel attention mechanism is developed , that adaptively and sequentially focuses on different layers of cnn features ( levels of feature `` abstraction '' ) , as well as local spatiotemporal regions of the feature maps at each layer . the proposed approach is evaluated on three benchmark datasets : youtube2text , m-vad and msr-vtt . along with visualizing the results and how the model works , these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics .", "topics": ["map"]}
{"title": "efficient exact gradient update for training deep networks with very large sparse targets", "abstract": "an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g . neural language models or the learning of word-embeddings , often posed as predicting the probability of next words among a vocabulary of size d ( e.g . 200 000 ) . computing the equally large , but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d ( e.g . 500 ) incurs a prohibitive o ( dd ) computational cost for each example , as does updating the d x d output weight matrix and computing the gradient needed for backpropagation to previous layers . while efficient handling of large sparse network inputs is trivial , the case of large sparse targets is not , and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training . in this work we develop an original algorithmic approach which , for a family of loss functions that includes squared error and spherical softmax , can compute the exact loss , gradient update for the output weights , and gradient for backpropagation , all in o ( d^2 ) per example instead of o ( dd ) , remarkably without ever computing the d-dimensional output . the proposed algorithm yields a speedup of d/4d , i.e . two orders of magnitude for typical sizes , for that critical part of the computations that often dominates the training time in this kind of network architecture .", "topics": ["approximation algorithm", "computational complexity theory"]}
{"title": "spatio-temporal deep de-aliasing for prospective assessment of real-time ventricular volumes", "abstract": "purpose : real-time assessment of ventricular volumes requires high acceleration factors . residual convolutional neural networks ( cnn ) have shown potential for removing artifacts caused by data undersampling . in this study we investigated the effect of different radial sampling patterns on the accuracy of a cnn . we also acquired actual real-time undersampled radial data in patients with congenital heart disease ( chd ) , and compare cnn reconstruction to compressed sensing ( cs ) . methods : a 3d ( 2d plus time ) cnn architecture was developed , and trained using 2276 gold-standard paired 3d data sets , with 14x radial undersampling . four sampling schemes were tested , using 169 previously unseen 3d 'synthetic ' test data sets . actual real-time tiny golden angle ( tga ) radial ssfp data was acquired in 10 new patients ( 122 3d data sets ) , and reconstructed using the 3d cnn as well as a cs algorithm ; grasp . results : sampling pattern was shown to be important for image quality , and accurate visualisation of cardiac structures . for actual real-time data , overall reconstruction time with cnn ( including creation of aliased images ) was shown to be more than 5x faster than grasp . additionally , cnn image quality and accuracy of biventricular volumes was observed to be superior to grasp for the same raw data . conclusion : this paper has demonstrated the potential for the use of a 3d cnn for deep de-aliasing of real-time radial data , within the clinical setting . clinical measures of ventricular volumes using real-time data with cnn reconstruction are not statistically significantly different from the gold-standard , cardiac gated , bh techniques .", "topics": ["sampling ( signal processing )"]}
{"title": "algorithms for approximate minimization of the difference between submodular functions , with applications", "abstract": "we extend the work of narasimhan and bilmes [ 30 ] for minimizing set functions representable as a dierence between submodular functions . similar to [ 30 ] , our new algorithms are guaranteed to monotonically reduce the objective function at every step . we empirically and theoretically show that the per-iteration cost of our algorithms is much less than [ 30 ] , and our algorithms can be used to efficiently minimize a dierence between submodular functions under various combinatorial constraints , a problem not previously addressed . we provide computational bounds and a hardness result on the multiplicative inapproximability of minimizing the dierence between submodular functions . we show , however , that it is possible to give worst-case additive bounds by providing a polynomial time computable lower-bound on the minima . finally we show how a number of machine learning problems can be modeled as minimizing the dierence between submodular functions . we experimentally show the validity of our algorithms by testing them on the problem of feature selection with submodular cost features .", "topics": ["optimization problem", "time complexity"]}
{"title": "improved graph-based sfa : information preservation complements the slowness principle", "abstract": "slow feature analysis ( sfa ) is an unsupervised-learning algorithm that extracts slowly varying features from a multi-dimensional time series . a supervised extension to sfa for classification and regression is graph-based sfa ( gsfa ) . gsfa is based on the preservation of similarities , which are specified by a graph structure derived from the labels . it has been shown that hierarchical gsfa ( hgsfa ) allows learning from images and other high-dimensional data . the feature space spanned by hgsfa is complex due to the composition of the nonlinearities of the nodes in the network . however , we show that the network discards useful information prematurely before it reaches higher nodes , resulting in suboptimal global slowness and an under-exploited feature space . to counteract these problems , we propose an extension called hierarchical information-preserving gsfa ( higsfa ) , where information preservation complements the slowness-maximization goal . we build a 10-layer higsfa network to estimate human age from facial photographs of the morph-ii database , achieving a mean absolute error of 3.50 years , improving the state-of-the-art performance . higsfa and hgsfa support multiple-labels and offer a rich feature space , feed-forward training , and linear complexity in the number of samples and dimensions . furthermore , higsfa outperforms hgsfa in terms of feature slowness , estimation accuracy and input reconstruction , giving rise to a promising hierarchical supervised-learning approach .", "topics": ["feature vector", "supervised learning"]}
{"title": "punny captions : witty wordplay in image descriptions", "abstract": "wit is a quintessential form of rich inter-human interaction , and is often grounded in a specific situation ( e.g . , a comment in response to an event ) . in this work , we attempt to build computational models that can produce witty descriptions for a given image . inspired by a cognitive account of humor appreciation , we employ linguistic wordplay , specifically puns . we compare our approach against meaningful baseline approaches via human studies . in a turing test style evaluation , people find our model 's description for an image to be wittier than a human 's witty description 55 % of the time !", "topics": ["baseline ( configuration management )"]}
{"title": "low rank matrix recovery with simultaneous presence of outliers and sparse corruption", "abstract": "we study a data model in which the data matrix d can be expressed as d = l + s + c , where l is a low rank matrix , s an element-wise sparse matrix and c a matrix whose non-zero columns are outlying data points . to date , robust pca algorithms have solely considered models with either s or c , but not both . as such , existing algorithms can not account for simultaneous element-wise and column-wise corruptions . in this paper , a new robust pca algorithm that is robust to simultaneous types of corruption is proposed . our approach hinges on the sparse approximation of a sparsely corrupted column so that the sparse expansion of a column with respect to the other data points is used to distinguish a sparsely corrupted inlier column from an outlying data point . we also develop a randomized design which provides a scalable implementation of the proposed approach . the core idea of sparse approximation is analyzed analytically where we show that the underlying ell_1-norm minimization can obtain the representation of an inlier in presence of sparse corruptions .", "topics": ["sparse matrix"]}
{"title": "generalised mixability , constant regret , and bayesian updating", "abstract": "mixability of a loss is known to characterise when constant regret bounds are achievable in games of prediction with expert advice through the use of vovk 's aggregating algorithm . we provide a new interpretation of mixability via convex analysis that highlights the role of the kullback-leibler divergence in its definition . this naturally generalises to what we call $ \\phi $ -mixability where the bregman divergence $ d_\\phi $ replaces the kl divergence . we prove that losses that are $ \\phi $ -mixable also enjoy constant regret bounds via a generalised aggregating algorithm that is similar to mirror descent .", "topics": ["regret ( decision theory )", "gradient descent"]}
{"title": "protodash : fast interpretable prototype selection", "abstract": "in this paper we propose an efficient algorithm protodash for selecting prototypical examples from complex datasets . our generalizes the learn to criticize ( l2c ) work by kim et al . ( 2016 ) to not only select prototypes for a given sparsity level $ m $ but also to associate non-negative ( for interpretability ) weights with each of them indicative of the importance of each prototype . this extension provides a single coherent framework under which both prototypes and criticisms can be found . furthermore , our framework works for any symmetric positive definite kernel thus addressing one of the key open questions laid out in kim et al . ( 2016 ) . our additional requirement of learning non-negative weights no longer maintains submodularity of the objective as in the previous work , however , we show that the problem is weakly submodular and derive approximation guarantees for our fast protodash algorithm . we demonstrate the efficacy of our method on diverse domains such as retail , digit recognition ( mnist ) and on publicly available 40 health questionnaires obtained from the center for disease control ( cdc ) website maintained by the us dept . of health . we validate the results quantitatively as well as qualitatively based on expert feedback and recently published scientific studies on public health , thus showcasing the power of our method in providing actionability ( for retail ) , utility ( for mnist ) and insight ( on cdc datasets ) , which presumably are the hallmark of an effective interpretable method .", "topics": ["feature vector", "sparse matrix"]}
{"title": "neural machine translation in linear time", "abstract": "we present a novel neural network for processing sequences . the bytenet is a one-dimensional convolutional neural network that is composed of two parts , one to encode the source sequence and the other to decode the target sequence . the two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences . to address the differing lengths of the source and the target , we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder . the bytenet uses dilation in the convolutional layers to increase its receptive field . the resulting network has two core properties : it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization . the bytenet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks . the bytenet also achieves state-of-the-art performance on character-to-character machine translation on the english-to-german wmt translation task , surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time . we find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens .", "topics": ["recurrent neural network", "time complexity"]}
{"title": "inexact proximal gradient methods for non-convex and non-smooth optimization", "abstract": "non-convex and non-smooth optimization plays an important role in machine learning . proximal gradient method is one of the most important methods for solving the non-convex and non-smooth problems , where a proximal operator need to be solved exactly for each step . however , in a lot of problems the proximal operator does not have an analytic solution , or is expensive to obtain an exact solution . in this paper , we propose inexact proximal gradient methods ( not only a basic inexact proximal gradient method ( ipg ) , but also a nesterov 's accelerated inexact proximal gradient method ( aipg ) ) for non-convex and non-smooth optimization , which tolerate an error in the calculation of the proximal operator . theoretical analysis shows that ipg and aipg have the same convergence rates as in the error-free case , provided that the errors decrease at appropriate rates .", "topics": ["gradient"]}
{"title": "deep image prior", "abstract": "deep convolutional networks have become a popular tool for image generation and restoration . generally , their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images . in this paper , we show that , on the contrary , the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning . in order to do so , we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising , super-resolution , and inpainting . furthermore , the same prior can be used to invert deep neural representations to diagnose them , and to restore images based on flash-no flash input pairs . apart from its diverse applications , our approach highlights the inductive bias captured by standard generator network architectures . it also bridges the gap between two very popular families of image restoration methods : learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity . code and supplementary material are available at https : //dmitryulyanov.github.io/deep_image_prior .", "topics": ["high- and low-level", "noise reduction"]}
{"title": "multi-view constrained clustering with an incomplete mapping between views", "abstract": "multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process . however , many applications provide only a partial mapping between the views , creating a challenge for current methods . to address this problem , we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping . given a set of pairwise constraints in each view , our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views , allowing the propagated constraints to be transferred across views via the partial mapping . it uses co-em to iteratively estimate the propagation within each view based on the current clustering model , transfer the constraints across views , and then update the clustering model . by alternating the learning process between views , this approach produces a unified clustering model that is consistent with all views . we show that this approach significantly improves clustering performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views . our evaluation reveals that the propagated constraints have high precision with respect to the true clusters in the data , explaining their benefit to clustering performance in both single- and multi-view learning scenarios .", "topics": ["cluster analysis"]}
{"title": "latent geometry and memorization in generative models", "abstract": "it can be difficult to tell whether a trained generative model has learned to generate novel examples or has simply memorized a specific set of outputs . in published work , it is common to attempt to address this visually , for example by displaying a generated example and its nearest neighbor ( s ) in the training set ( in , for example , the l2 metric ) . as any generative model induces a probability density on its output domain , we propose studying this density directly . we first study the geometry of the latent representation and generator , relate this to the output density , and then develop techniques to compute and inspect the output density . as an application , we demonstrate that `` memorization '' tends to a density made of delta functions concentrated on the memorized examples . we note that without first understanding the geometry , the measurement would be essentially impossible to make .", "topics": ["generative model"]}
{"title": "the power of depth for feedforward neural networks", "abstract": "we show that there is a simple ( approximately radial ) function on $ \\reals^d $ , expressible by a small 3-layer feedforward neural networks , which can not be approximated by any 2-layer network , to more than a certain constant accuracy , unless its width is exponential in the dimension . the result holds for virtually all known activation functions , including rectified linear units , sigmoids and thresholds , and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks . moreover , compared to related results in the context of boolean functions , our result requires fewer assumptions , and the proof techniques and construction are very different .", "topics": ["time complexity"]}
{"title": "fast optimization of wildfire suppression policies with smac", "abstract": "managers of us national forests must decide what policy to apply for dealing with lightning-caused wildfires . conflicts among stakeholders ( e.g . , timber companies , home owners , and wildlife biologists ) have often led to spirited political debates and even violent eco-terrorism . one way to transform these conflicts into multi-stakeholder negotiations is to provide a high-fidelity simulation environment in which stakeholders can explore the space of alternative policies and understand the tradeoffs therein . such an environment needs to support fast optimization of mdp policies so that users can adjust reward functions and analyze the resulting optimal policies . this paper assesses the suitability of smac -- -a black-box empirical function optimization algorithm -- -for rapid optimization of mdp policies . the paper describes five reward function components and four stakeholder constituencies . it then introduces a parameterized class of policies that can be easily understood by the stakeholders . smac is applied to find the optimal policy in this class for the reward functions of each of the stakeholder constituencies . the results confirm that smac is able to rapidly find good policies that make sense from the domain perspective . because the full-fidelity forest fire simulator is far too expensive to support interactive optimization , smac is applied to a surrogate model constructed from a modest number of runs of the full-fidelity simulator . to check the quality of the smac-optimized policies , the policies are evaluated on the full-fidelity simulator . the results confirm that the surrogate values estimates are valid . this is the first successful optimization of wildfire management policies using a full-fidelity simulation . the same methodology should be applicable to other contentious natural resource management problems where high-fidelity simulation is extremely expensive .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "the structure of optimal parameters for image restoration problems", "abstract": "we study the qualitative properties of optimal regularisation parameters in variational models for image restoration . the parameters are solutions of bilevel optimisation problems with the image restoration problem as constraint . a general type of regulariser is considered , which encompasses total variation ( tv ) , total generalized variation ( tgv ) and infimal-convolution total variation ( ictv ) . we prove that under certain conditions on the given data optimal parameters derived by bilevel optimisation problems exist . a crucial point in the existence proof turns out to be the boundedness of the optimal parameters away from $ 0 $ which we prove in this paper . the analysis is done on the original -- in image restoration typically non-smooth variational problem -- as well as on a smoothed approximation set in hilbert space which is the one considered in numerical computations . for the smoothed bilevel problem we also prove that it $ \\gamma $ converges to the original problem as the smoothing vanishes . all analysis is done in function spaces rather than on the discretised learning problem .", "topics": ["calculus of variations", "mathematical optimization"]}
{"title": "fuzzy least squares twin support vector machines", "abstract": "least squares twin support vector machine ( lstsvm ) is an extremely efficient and fast version of svm algorithm for binary classification . lstsvm combines the idea of least squares svm and twin svm in which two non-parallel hyperplanes are found by solving two systems of linear equations . although the algorithm is very fast and efficient in many classification tasks , it is unable to cope with two features of real-world problems . first , in many real-world classification problems , it is almost impossible to assign data points to a single class . second , data points in real-world problems may have different importance . in this study , we propose a novel version of lstsvm based on fuzzy concepts to deal with these two characteristics of real-world data . the algorithm is called fuzzy lstsvm ( flstsvm ) which provides more flexibility than the binary classification of lstsvm . two models are proposed for the algorithm . in the first model , a fuzzy membership value is assigned to each data point and the hyperplanes are optimized based on these fuzzy samples . in the second model we construct fuzzy hyperplanes to classify data . finally , we apply our proposed flstsvm to an artificial as well as three real-world datasets . results demonstrate that flstsvm obtains better performance than svm and lstsvm .", "topics": ["support vector machine"]}
{"title": "action-depedent control variates for policy optimization via stein 's identity", "abstract": "policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems . however , it still often suffers from the large variance issue on policy gradient estimation , which leads to poor sample efficiency during training . in this work , we propose a control variate method to effectively reduce variance for policy gradient methods . motivated by the stein 's identity , our method extends the previous control variate methods used in reinforce and advantage actor-critic by introducing more general action-dependent baseline functions . empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "maximum a posteriori estimation by search in probabilistic programs", "abstract": "we introduce an approximate search algorithm for fast maximum a posteriori probability estimation in probabilistic programs , which we call bayesian ascent monte carlo ( bamc ) . probabilistic programs represent probabilistic models with varying number of mutually dependent finite , countable , and continuous random variables . bamc is an anytime map search algorithm applicable to any combination of random variables and dependencies . we compare bamc to other map estimation algorithms and show that bamc is faster and more robust on a range of probabilistic models .", "topics": ["sampling ( signal processing )", "graphical model"]}
{"title": "safe exploration of state and action spaces in reinforcement learning", "abstract": "in this paper , we consider the important problem of safe exploration in reinforcement learning . while reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces , an additional challenge is posed by the need for safe and efficient exploration . traditional exploration techniques are not particularly useful for solving dangerous tasks , where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system ( or any other system ) . consequently , when an agent begins an interaction with a dangerous and high-dimensional state-action space , an important question arises ; namely , that of how to avoid ( or at least minimize ) damage caused by the exploration of the state-action space . we introduce the pi-srl algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment . we evaluate the proposed method in four complex tasks : automatic car parking , pole-balancing , helicopter hovering , and business management .", "topics": ["reinforcement learning", "causality"]}
{"title": "informed non-convex robust principal component analysis with features", "abstract": "we revisit the problem of robust principal component analysis with features acting as prior side information . to this aim , a novel , elegant , non-convex optimization approach is proposed to decompose a given observation matrix into a low-rank core and the corresponding sparse residual . rigorous theoretical analysis of the proposed algorithm results in exact recovery guarantees with low computational complexity . aptly designed synthetic experiments demonstrate that our method is the first to wholly harness the power of non-convexity over convexity in terms of both recoverability and speed . that is , the proposed non-convex approach is more accurate and faster compared to the best available algorithms for the problem under study . two real-world applications , namely image classification and face denoising further exemplify the practical superiority of the proposed method .", "topics": ["computational complexity theory", "noise reduction"]}
{"title": "learning to generate samples from noise through infusion training", "abstract": "in this work , we investigate a novel training procedure to learn a generative model as the transition operator of a markov chain , such that , when applied repeatedly on an unstructured random noise sample , it will denoise it into a sample that matches the target distribution from the training set . the novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target . in the training chain we infuse information from the training target example that we would like the chains to reach with a high probability . the thus learned transition operator is able to produce quality and varied samples in a small number of steps . experiments show competitive results compared to the samples generated with a basic generative adversarial net", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "traffic optimization for a mixture of self-interested and compliant agents", "abstract": "this paper focuses on two commonly used path assignment policies for agents traversing a congested network : self-interested routing , and system-optimum routing . in the self-interested routing policy each agent selects a path that optimizes its own utility , while the system-optimum routing agents are assigned paths with the goal of maximizing system performance . this paper considers a scenario where a centralized network manager wishes to optimize utilities over all agents , i.e . , implement a system-optimum routing policy . in many real-life scenarios , however , the system manager is unable to influence the route assignment of all agents due to limited influence on route choice decisions . motivated by such scenarios , a computationally tractable method is presented that computes the minimal amount of agents that the system manager needs to influence ( compliant agents ) in order to achieve system optimal performance . moreover , this methodology can also determine whether a given set of compliant agents is sufficient to achieve system optimum and compute the optimal route assignment for the compliant agents to do so . experimental results are presented showing that in several large-scale , realistic traffic networks optimal flow can be achieved with as low as 13 % of the agent being compliant and up to 54 % .", "topics": ["mathematical optimization"]}
{"title": "ensemble classifier for eye state classification using eeg signals", "abstract": "the growing importance and utilization of measuring brain waves ( e.g . eeg signals of eye state ) in brain-computer interface ( bci ) applications highlighted the need for suitable classification methods . in this paper , a comparison between three of well-known classification methods ( i.e . support vector machine ( svm ) , hidden markov map ( hmm ) , and radial basis function ( rbf ) ) for eeg based eye state classification was achieved . furthermore , a suggested method that is based on ensemble model was tested . the suggested ( ensemble system ) method based on a voting algorithm with two kernels : random forest ( rf ) and kstar classification methods . the performance was tested using three measurement parameters : accuracy , mean absolute error ( mae ) , and confusion matrix . results showed that the proposed method outperforms the other tested methods . for instance , the suggested method 's performance was 97.27 % accuracy and 0.13 mae .", "topics": ["support vector machine"]}
{"title": "boltzmann machines and denoising autoencoders for image denoising", "abstract": "image denoising based on a probabilistic model of local image patches has been employed by various researchers , and recently a deep ( denoising ) autoencoder has been proposed by burger et al . [ 2012 ] and xie et al . [ 2012 ] as a good model for this . in this paper , we propose that another popular family of models in the field of deep learning , called boltzmann machines , can perform image denoising as well as , or in certain cases of high level of noise , better than denoising autoencoders . we empirically evaluate the two models on three different sets of images with different types and levels of noise . throughout the experiments we also examine the effect of the depth of the models . the experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers , especially when the level of noise is high .", "topics": ["noise reduction", "autoencoder"]}
{"title": "a minimalistic approach to sum-product network learning for real applications", "abstract": "sum-product networks ( spns ) are a class of expressive yet tractable hierarchical graphical models . learnspn is a structure learning algorithm for spns that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features . the original learnspn algorithm assumes that all the variables are discrete and there is no missing data . we introduce a practical , simplified version of learnspn , minispn , that runs faster and can handle missing data and heterogeneous features common in real applications . we demonstrate the performance of minispn on standard benchmark datasets and on two datasets from google 's knowledge graph exhibiting high missingness rates and a mix of discrete and continuous features .", "topics": ["graphical model", "cluster analysis"]}
{"title": "improvements to inference compilation for probabilistic programming in large-scale scientific simulators", "abstract": "we consider the problem of bayesian inference in the family of probabilistic models implicitly defined by stochastic generative models of data . in scientific fields ranging from population biology to cosmology , low-level mechanistic components are composed to create complex generative models . these models lead to intractable likelihoods and are typically non-differentiable , which poses challenges for traditional approaches to inference . we extend previous work in `` inference compilation '' , which combines universal probabilistic programming and deep learning methods , to large-scale scientific simulators , and introduce a c++ based probabilistic programming library called cpprob . we successfully use cpprob to interface with sherpa , a large code-base used in particle physics . here we describe the technical innovations realized and planned for this library .", "topics": ["generative model", "high- and low-level"]}
{"title": "pattern analysis with layered self-organizing maps", "abstract": "this paper defines a new learning architecture , layered self-organizing maps ( lsoms ) , that uses the som and supervised-som learning algorithms . the architecture is validated with the mnist database of hand-written digit images . lsoms are similar to convolutional neural nets ( covnets ) in the way they sample data , but different in the way they represent features and learn . lsoms analyze ( or generate ) image patches with maps of exemplars determined by the som learning algorithm rather than feature maps from filter-banks learned via backprop . lsoms provide an alternative to features derived from covnets . multi-layer lsoms are trained bottom-up , without the use of backprop and therefore may be of interest as a model of the visual cortex . the results show organization at multiple levels . the algorithm appears to be resource efficient in learning , classifying and generating images . although lsoms can be used for classification , their validation accuracy for these exploratory runs was well below the state of the art . the goal of this article is to define the architecture and display the structures resulting from its application to the mnist images .", "topics": ["map", "mnist database"]}
{"title": "is a picture worth ten thousand words in a review dataset ?", "abstract": "while textual reviews have become prominent in many recommendation-based systems , automated frameworks to provide relevant visual cues against text reviews where pictures are not available is a new form of task confronted by data mining and machine learning researchers . suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review . we propose a deep learning-based framework to automatically : ( 1 ) tag the images available in a review dataset , ( 2 ) generate a caption for each image that does not have one , and ( 3 ) enhance each review by recommending relevant images that might not be uploaded by the corresponding reviewer . we evaluate the proposed framework using the yelp challenge dataset . while a subset of the images in this particular dataset are correctly captioned , the majority of the pictures do not have any associated text . moreover , there is no mapping between reviews and images . each image has a corresponding business-tag where the picture was taken , though . the overall data setting and unavailability of crucial pieces required for a mapping make the problem of recommending images for reviews a major challenge . qualitative and quantitative evaluations indicate that our proposed framework provides high quality enhancements through automatic captioning , tagging , and recommendation for mapping reviews and images .", "topics": ["data mining", "neural networks"]}
{"title": "proximodistal exploration in motor learning as an emergent property of optimization", "abstract": "to harness the complexity of their high-dimensional bodies during sensorimotor development , infants are guided by patterns of freezing and freeing of degrees of freedom . for instance , when learning to reach , infants free the degrees of freedom in their arm proximodistally , i.e . from joints that are closer to the body to those that are more distant . here , we formulate and study computationally the hypothesis that such patterns can emerge spontaneously as the result of a family of stochastic optimization processes ( evolution strategies with covariance-matrix adaptation ) , without an innate encoding of a maturational schedule . in particular , we present simulated experiments with an arm where a computational learner progressively acquires reaching skills through adaptive exploration , and we show that a proximodistal organization appears spontaneously , which we denote pdff ( proximodistal freezing and freeing of degrees of freedom ) . we also compare this emergent organization between different arm morphologies -- from human-like to quite unnatural ones -- to study the effect of different kinematic structures on the emergence of pdff . keywords : human motor learning ; proximo-distal exploration ; stochastic optimization ; modelling ; evolution strategies ; cross-entropy methods ; policy search ; morphology . }", "topics": ["simulation"]}
{"title": "neural responding machine for short-text conversation", "abstract": "we propose neural responding machine ( nrm ) , a neural network-based response generator for short-text conversation . nrm takes the general encoder-decoder framework : it formalizes the generation of response as a decoding process based on the latent representation of the input text , while both encoding and decoding are realized with recurrent neural networks ( rnn ) . the nrm is trained with a large amount of one-round conversation data collected from a microblogging service . empirical study shows that nrm can generate grammatically correct and content-wise appropriate responses to over 75 % of the input text , outperforming state-of-the-arts in the same setting , including retrieval-based and smt-based models .", "topics": ["recurrent neural network"]}
{"title": "sparse and non-negative bss for noisy data", "abstract": "non-negative blind source separation ( bss ) has raised interest in various fields of research , as testified by the wide literature on the topic of non-negative matrix factorization ( nmf ) . in this context , it is fundamental that the sources to be estimated present some diversity in order to be efficiently retrieved . sparsity is known to enhance such contrast between the sources while producing very robust approaches , especially to noise . in this paper we introduce a new algorithm in order to tackle the blind separation of non-negative sparse sources from noisy measurements . we first show that sparsity and non-negativity constraints have to be carefully applied on the sought-after solution . in fact , improperly constrained solutions are unlikely to be stable and are therefore sub-optimal . the proposed algorithm , named ngmca ( non-negative generalized morphological component analysis ) , makes use of proximal calculus techniques to provide properly constrained solutions . the performance of ngmca compared to other state-of-the-art algorithms is demonstrated by numerical experiments encompassing a wide variety of settings , with negligible parameter tuning . in particular , ngmca is shown to provide robustness to noise and performs well on synthetic mixtures of real nmr spectra .", "topics": ["calculus of variations", "numerical analysis"]}
{"title": "3-phase recognition approach to pseudo 3d building generation from 2d floor plan", "abstract": "nowadays three dimension ( 3d ) architectural visualisation has become a powerful tool in the conceptualisation , design and presentation of architectural products in the construction industry , providing realistic interaction and walkthrough on engineering products . traditional ways of implementing 3d models involves the use of specialised 3d authoring tools along with skilled 3d designers with blueprints of the model and this is a slow and laborious process . the aim of this paper is to automate this process by simply analyzing the blueprint document and generating the 3d scene automatically . for this purpose we have devised a 3-phase recognition approach to pseudo 3d building generation from 2d floor plan and developed a software accordingly . our 3-phased 3d building system has been implemented using c , c++ and opencv library [ 24 ] for the image processing module ; the save module generated an xml file for storing the processed floor plan objects attributes ; while the irrlitch [ 14 ] game engine was used to implement the interactive 3d module . though still at its infancy , our proposed system gave commendable results . we tested our system on 6 floor plans with complexities ranging from low to high and the results seems to be very promising with an average processing time of around 3s and a 3d generation in 4s . in addition the system provides an interactive walk-though and allows users to modify components .", "topics": ["image processing"]}
{"title": "an overview of melanoma detection in dermoscopy images using image processing and machine learning", "abstract": "the incidence of malignant melanoma continues to increase worldwide . this cancer can strike at any age ; it is one of the leading causes of loss of life in young persons . since this cancer is visible on the skin , it is potentially detectable at a very early stage when it is curable . new developments have converged to make fully automatic early melanoma detection a real possibility . first , the advent of dermoscopy has enabled a dramatic boost in clinical diagnostic ability to the point that melanoma can be detected in the clinic at the very earliest stages . the global adoption of this technology has allowed accumulation of large collections of dermoscopy images of melanomas and benign lesions validated by histopathology . the development of advanced technologies in the areas of image processing and machine learning have given us the ability to allow distinction of malignant melanoma from the many benign mimics that require no biopsy . these new technologies should allow not only earlier detection of melanoma , but also reduction of the large number of needless and costly biopsy procedures . although some of the new systems reported for these technologies have shown promise in preliminary trials , widespread implementation must await further technical progress in accuracy and reproducibility . in this paper , we provide an overview of computerized detection of melanoma in dermoscopy images . first , we discuss the various aspects of lesion segmentation . then , we provide a brief overview of clinical feature segmentation . finally , we discuss the classification stage where machine learning algorithms are applied to the attributes generated from the segmented features to predict the existence of melanoma .", "topics": ["image processing", "image segmentation"]}
{"title": "an ontology of preference-based multiobjective metaheuristics", "abstract": "user preference integration is of great importance in multi-objective optimization , in particular in many objective optimization . preferences have long been considered in traditional multicriteria decision making ( mcdm ) which is based on mathematical programming . recently , it is integrated in multi-objective metaheuristics ( momh ) , resulting in focus on preferred parts of the pareto front instead of the whole pareto front . the number of publications on preference-based multi-objective metaheuristics has increased rapidly over the past decades . there already exist various preference handling methods and momh methods , which have been combined in diverse ways . this article proposes to use the web ontology language ( owl ) to model and systematize the results developed in this field . a review of the existing work is provided , based on which an ontology is built and instantiated with state-of-the-art results . the owl ontology is made public and open to future extension . moreover , the usage of the ontology is exemplified for different use-cases , including querying for methods that match an engineering application , bibliometric analysis , checking existence of combinations of preference models and momh techniques , and discovering opportunities for new research and open research questions .", "topics": ["mathematical optimization"]}
{"title": "deep tamer : interactive agent shaping in high-dimensional state spaces", "abstract": "while recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks , existing algorithms generally require a lot of training data . one way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers . although such input can take many forms , real-time , scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations . previous approaches have shown the usefulness of human input provided in this fashion ( e.g . , the tamer framework ) , but they have thus far not considered high-dimensional state spaces or employed the use of deep learning . in this paper , we do both : we propose deep tamer , an extension of the tamer framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer . we demonstrate deep tamer 's success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the atari game of bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods .", "topics": ["test set", "reinforcement learning"]}
{"title": "topical differences between chinese language twitter and sina weibo", "abstract": "sina weibo , china 's most popular microblogging platform , is currently used by over $ 500m $ users and is considered to be a proxy of chinese social life . in this study , we contrast the discussions occurring on sina weibo and on chinese language twitter in order to observe two different strands of chinese culture : people within china who use sina weibo with its government imposed restrictions and those outside that are free to speak completely anonymously . we first propose a simple ad-hoc algorithm to identify topics of tweets and weibo . different from previous works on micro-message topic detection , our algorithm considers topics of the same contents but with different \\ # tags . our algorithm can also detect topics for tweets and weibos without any \\ # tags . using a large corpus of weibo and chinese language tweets , covering the period from january $ 1 $ to december $ 31 $ , $ 2012 $ , we obtain a list of topics using clustered \\ # tags that we can then use to compare the two platforms . surprisingly , we find that there are no common entries among the top $ 100 $ most popular topics . furthermore , only $ 9.2\\ % $ of tweets correspond to the top $ 1000 $ topics on sina weibo platform , and conversely only $ 4.4\\ % $ of weibos were found to discuss the most popular twitter topics . our results reveal significant differences in social attention on the two platforms , with most popular topics on sina weibo relating to entertainment while most tweets corresponded to cultural or political contents that is practically non existent in sina weibo .", "topics": ["text corpus"]}
{"title": "designing fuzzy rule based classifier using self-organizing feature map for analysis of multispectral satellite images", "abstract": "we propose a novel scheme for designing fuzzy rule based classifier . an sofm based method is used for generating a set of prototypes which is used to generate a set of fuzzy rules . each rule represents a region in the feature space that we call the context of the rule . the rules are tuned with respect to their context . we justified that the reasoning scheme may be different in different context leading to context sensitive inferencing . to realize context sensitive inferencing we used a softmin operator with a tunable parameter . the proposed scheme is tested on several multispectral satellite image data sets and the performance is found to be much better than the results reported in the literature .", "topics": ["feature vector"]}
{"title": "pathnet : evolution channels gradient descent in super neural networks", "abstract": "for artificial general intelligence ( agi ) it would be efficient if multiple users trained the same giant neural network , permitting parameter reuse , without catastrophic forgetting . pathnet is a first step in this direction . it is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks . agents are pathways ( views ) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm . during learning , a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation . pathway fitness is the performance of that pathway measured according to a cost function . we demonstrate successful transfer learning ; fixing the parameters along a path learned on task a and re-evolving a new population of paths for task b , allows task b to be learned faster than it could be learned from scratch or after fine-tuning . paths evolved on task b re-use parts of the optimal path evolved on task a . positive transfer was demonstrated for binary mnist , cifar , and svhn supervised learning classification tasks , and a set of atari and labyrinth reinforcement learning tasks , suggesting pathnets have general applicability for neural network training . finally , pathnet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm ( a3c ) .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "a comparative quantitative analysis of contemporary big data clustering algorithms for market segmentation in hospitality industry", "abstract": "the hospitality industry is one of the data-rich industries that receives huge volumes of data streaming at high velocity with considerably variety , veracity , and variability . these properties make the data analysis in the hospitality industry a big data problem . meeting the customers ' expectations is a key factor in the hospitality industry to grasp the customers ' loyalty . to achieve this goal , marketing professionals in this industry actively look for ways to utilize their data in the best possible manner and advance their data analytic solutions , such as identifying a unique market segmentation clustering and developing a recommendation system . in this paper , we present a comprehensive literature review of existing big data clustering algorithms and their advantages and disadvantages for various use cases . we implement the existing big data clustering algorithms and provide a quantitative comparison of the performance of different clustering algorithms for different scenarios . we also present our insights and recommendations regarding the suitability of different big data clustering algorithms for different use cases . these recommendations will be helpful for hoteliers in selecting the appropriate market segmentation clustering algorithm for different clustering datasets to improve the customer experience and maximize the hotel revenue .", "topics": ["cluster analysis"]}
{"title": "`` how was your weekend ? '' a generative model of phatic conversation", "abstract": "unspoken social rules , such as those that govern choosing a proper discussion topic and when to change discussion topics , guide conversational behaviors . we propose a computational model of conversation that can follow or break such rules , with participant agents that respond accordingly . additionally , we demonstrate an application of the model : the experimental social tutor ( est ) , a first step toward a social skills training tool that generates human-readable conversation and a conversational guideline at each point in the dialogue . finally , we discuss the design and results of a pilot study evaluating the est . results show that our model is capable of producing conversations that follow social norms .", "topics": ["generative model"]}
{"title": "darts : deceiving autonomous cars with toxic signs", "abstract": "sign recognition is an integral part of autonomous cars . any misclassification of traffic signs can potentially lead to a multitude of disastrous consequences , ranging from a life-threatening accident to a large-scale interruption of transportation services relying on autonomous cars . in this paper , we propose and examine realistic security attacks against sign recognition systems for deceiving autonomous cars with toxic signs ( we call the proposed attacks darts ) . leveraging the concept of adversarial examples , we modify innocuous signs/advertisements in the environment in such a way that they seem normal to human observers but are interpreted as the adversary 's desired traffic sign by autonomous cars . further , we pursue a fundamentally different perspective to attacking autonomous cars , motivated by the observation that the driver and vehicle-mounted camera see the environment from different angles ( the camera commonly sees the road with a higher angle , e.g . , from top of the car ) . we propose a novel attack against vehicular sign recognition systems : we create signs that change as they are viewed from different angles , and thus , can be interpreted differently by the driver and sign recognition . we extensively evaluate the proposed attacks under various conditions : different distances , lighting conditions , and camera angles . we first examine our attacks virtually , i.e . , we check if the digital images of toxic signs can deceive the sign recognition system . further , we investigate the effectiveness of attacks in real-world settings : we print toxic signs , install them in the environment , capture videos using a vehicle-mounted camera , and process them using our sign recognition pipeline .", "topics": ["computer vision", "autonomous car"]}
{"title": "unsupervised discovery of toxoplasma gondii motility phenotypes", "abstract": "toxoplasma gondii is a parasitic protozoan that causes dis- seminated toxoplasmosis , a disease that afflicts roughly a third of the worlds population . its virulence is predicated on its motility and ability to enter and exit nucleated cells ; therefore , studies elucidating its mechanism of motility and in particular , its motility patterns in the context of its lytic cycle , are critical to the eventual development of therapeutic strate- gies . here , we present an end-to-end computational pipeline for identifying t. gondii motility phenotypes in a completely unsupervised , data-driven way . we track the parasites before and after addition of extracellular ca2+ to study its effects on the parasite motility patterns and use this information to parameterize the motion and group it according to similarity of spatiotemporal dynamics .", "topics": ["end-to-end principle"]}
{"title": "gated graph sequence neural networks", "abstract": "graph-structured data appears frequently in domains including chemistry , natural language semantics , social networks , and knowledge bases . in this work , we study feature learning techniques for graph-structured inputs . our starting point is previous work on graph neural networks ( scarselli et al . , 2009 ) , which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences . the result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models ( e.g . , lstms ) when the problem is graph-structured . we demonstrate the capabilities on some simple ai ( babi ) and graph algorithm learning tasks . we then show it achieves state-of-the-art performance on a problem from program verification , in which subgraphs need to be matched to abstract data structures .", "topics": ["feature learning", "neural networks"]}
{"title": "neural variational inference for text processing", "abstract": "recent advances in neural variational inference have spawned a renaissance in deep latent variable models . in this paper we introduce a generic variational inference framework for generative and conditional models of text . while traditional variational methods derive an analytic approximation for the intractable distributions over latent variables , here we construct an inference network conditioned on the discrete text input to provide the variational distribution . we validate this framework on two very different text modelling applications , generative document modelling and supervised question answering . our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora . the neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair . on two question answering benchmarks this model exceeds all previous published benchmarks .", "topics": ["generative model", "calculus of variations"]}
{"title": "deep learning reconstruction of ultra-short pulses", "abstract": "ultra-short laser pulses with femtosecond to attosecond pulse duration are the shortest systematic events humans can create . characterization ( amplitude and phase ) of these pulses is a key ingredient in ultrafast science , e.g . , exploring chemical reactions and electronic phase transitions . here , we propose and demonstrate , numerically and experimentally , the first deep neural network technique to reconstruct ultra-short optical pulses . we anticipate that this approach will extend the range of ultrashort laser pulses that can be characterized , e.g . , enabling to diagnose very weak attosecond pulses .", "topics": ["numerical analysis"]}
{"title": "biologically inspired radio signal feature extraction with sparse denoising autoencoders", "abstract": "automatic modulation classification ( amc ) is an important task for modern communication systems ; however , it is a challenging problem when signal features and precise models for generating each modulation may be unknown . we present a new biologically-inspired amc method without the need for models or manually specified features -- - thus removing the requirement for expert prior knowledge . we accomplish this task using regularized stacked sparse denoising autoencoders ( ssdas ) . our method selects efficient classification features directly from raw in-phase/quadrature ( i/q ) radio signals in an unsupervised manner . these features are then used to construct higher-complexity abstract features which can be used for automatic modulation classification . we demonstrate this process using a dataset generated with a software defined radio , consisting of random input bits encoded in 100-sample segments of various common digital radio modulations . our results show correct classification rates of > 99 % at 7.5 db signal-to-noise ratio ( snr ) and > 92 % at 0 db snr in a 6-way classification test . our experiments demonstrate a dramatically new and broadly applicable mechanism for performing amc and related tasks without the need for expert-defined or modulation-specific signal information .", "topics": ["feature extraction", "unsupervised learning"]}
{"title": "a way out of the odyssey : analyzing and combining recent insights for lstms", "abstract": "lstms have become a basic building block for many deep nlp models . in recent years , many improvements and variations have been proposed for deep sequence models in general , and lstms in particular . we propose and analyze a series of augmentations and modifications to lstm networks resulting in improved performance for text classification datasets . we observe compounding improvements on traditional lstms using monte carlo test-time model averaging , average pooling , and residual connections , along with four other suggested modifications . our analysis provides a simple , reliable , and high quality baseline model .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "adding gradient noise improves learning for very deep networks", "abstract": "deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications . this success is partially attributed to architectural innovations such as convolutional and long short-term memory networks . the main motivation for these architectural innovations is that they capture better domain knowledge , and importantly are easier to optimize than more basic architectures . recently , more complex architectures such as neural turing machines and memory networks have been proposed for tasks including question answering and general computation , creating a new set of optimization challenges . in this paper , we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures . the technique not only helps to avoid overfitting , but also can result in lower training loss . this method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent , even starting from a poor initialization . we see consistent improvements for many complex models , including a 72 % relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task , and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts . we encourage further application of this technique to additional complex modern architectures .", "topics": ["baseline ( configuration management )", "gradient descent"]}
{"title": "improving sat solvers via blocked clause decomposition", "abstract": "the decision variable selection policy used by the most competitive cdcl ( conflict-driven clause learning ) sat solvers is either vsids ( variable state independent decaying sum ) or its variants such as exponential version evsids . the common characteristic of vsids and its variants is to make use of statistical information in the solving process , but ignore structure information of the problem . for this reason , this paper modifies the decision variable selection policy , and presents a sat solving technique based on bcd ( blocked clause decomposition ) . its basic idea is that a part of decision variables are selected by vsids heuristic , while another part of decision variables are selected by blocked sets that are obtained by bcd . compared with the existing bcd-based technique , our technique is simple , and need not to reencode cnf formulas . sat solvers for certified unsat track can apply also our bcd-based technique . our experiments on application benchmarks demonstrate that the new variables selection policy based on bcd can increase the performance of sat solvers such as abcdsat . the solver with bcd solved an instance from the sat race 2015 that was not solved by any solver so far . this shows that in some cases , the heuristic based on structure information is more efficient than that based on statistical information .", "topics": ["time complexity", "heuristic"]}
{"title": "a short survey on data clustering algorithms", "abstract": "with rapidly increasing data , clustering algorithms are important tools for data analytics in modern research . they have been successfully applied to a wide range of domains ; for instance , bioinformatics , speech recognition , and financial analysis . formally speaking , given a set of data instances , a clustering algorithm is expected to divide the set of data instances into the subsets which maximize the intra-subset similarity and inter-subset dissimilarity , where a similarity measure is defined beforehand . in this work , the state-of-the-arts clustering algorithms are reviewed from design concept to methodology ; different clustering paradigms are discussed . advanced clustering algorithms are also discussed . after that , the existing clustering evaluation metrics are reviewed . a summary with future insights is provided at the end .", "topics": ["cluster analysis", "speech recognition"]}
{"title": "ensemble relational learning based on selective propositionalization", "abstract": "dealing with structured data needs the use of expressive representation formalisms that , however , puts the problem to deal with the computational complexity of the machine learning process . furthermore , real world domains require tools able to manage their typical uncertainty . many statistical relational learning approaches try to deal with these problems by combining the construction of relevant relational features with a probabilistic tool . when the combination is static ( static propositionalization ) , the constructed features are considered as boolean features and used offline as input to a statistical learner ; while , when the combination is dynamic ( dynamic propositionalization ) , the feature construction and probabilistic tool are combined into a single process . in this paper we propose a selective propositionalization method that search the optimal set of relational features to be used by a probabilistic learner in order to minimize a loss function . the new propositionalization approach has been combined with the random subspace ensemble method . experiments on real-world datasets shows the validity of the proposed method .", "topics": ["computational complexity theory", "feature vector"]}
{"title": "decision forests , convolutional networks and the models in-between", "abstract": "this paper investigates the connections between two state of the art classifiers : decision forests ( dfs , including decision jungles ) and convolutional neural networks ( cnns ) . decision forests are computationally efficient thanks to their conditional computation property ( computation is confined to only a small region of the tree , the nodes along a single branch ) . cnns achieve state of the art accuracy , thanks to their representation learning capabilities . we present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency . we call this new family of hybrid models conditional networks . conditional networks can be thought of as : i ) decision trees augmented with data transformation operators , or ii ) cnns , with block-diagonal sparse weight matrices , and explicit data routing functions . experimental validation is performed on the common task of image classification on both the cifar and imagenet datasets . compared to state of the art cnns , our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters .", "topics": ["feature learning", "computational complexity theory"]}
{"title": "pattern recognition and memory mapping using mirroring neural networks", "abstract": "in this paper , we present a new kind of learning implementation to recognize the patterns using the concept of mirroring neural network ( mnn ) which can extract information from distinct sensory input patterns and perform pattern recognition tasks . it is also capable of being used as an advanced associative memory wherein image data is associated with voice inputs in an unsupervised manner . since the architecture is hierarchical and modular it has the potential of being used to devise learning engines of ever increasing complexity .", "topics": ["unsupervised learning"]}
{"title": "on sparsity inducing regularization methods for machine learning", "abstract": "during the past years there has been an explosion of interest in learning methods based on sparsity regularization . in this paper , we discuss a general class of such methods , in which the regularizer can be expressed as the composition of a convex function $ \\omega $ with a linear function . this setting includes several methods such the group lasso , the fused lasso , multi-task learning and many more . we present a general approach for solving regularization problems of this kind , under the assumption that the proximity operator of the function $ \\omega $ is available . furthermore , we comment on the application of this approach to support vector machines , a technique pioneered by the groundbreaking work of vladimir vapnik .", "topics": ["support vector machine", "matrix regularization"]}
{"title": "sequential feature explanations for anomaly detection", "abstract": "in many applications , an anomaly detection system presents the most anomalous data instance to a human analyst , who then must determine whether the instance is truly of interest ( e.g . a threat in a security setting ) . unfortunately , most anomaly detectors provide no explanation about why an instance was considered anomalous , leaving the analyst with no guidance about where to begin the investigation . to address this issue , we study the problems of computing and evaluating sequential feature explanations ( sfes ) for anomaly detectors . an sfe of an anomaly is a sequence of features , which are presented to the analyst one at a time ( in order ) until the information contained in the highlighted features is enough for the analyst to make a confident judgement about the anomaly . since analyst effort is related to the amount of information that they consider in an investigation , an explanation 's quality is related to the number of features that must be revealed to attain confidence . one of our main contributions is to present a novel framework for large scale quantitative evaluations of sfes , where the quality measure is based on analyst effort . to do this we construct anomaly detection benchmarks from real data sets along with artificial experts that can be simulated for evaluation . our second contribution is to evaluate several novel explanation approaches within the framework and on traditional anomaly detection benchmarks , offering several insights into the approaches .", "topics": ["simulation"]}
{"title": "ensemble sampling", "abstract": "thompson sampling has emerged as an effective heuristic for a broad range of online decision problems . in its basic form , the algorithm requires computing and sampling from a posterior distribution over models , which is tractable only for simple special cases . this paper develops ensemble sampling , which aims to approximate thompson sampling while maintaining tractability even in the face of complex models such as neural networks . ensemble sampling dramatically expands on the range of applications for which thompson sampling is viable . we establish a theoretical basis that supports the approach and present computational results that offer further insight .", "topics": ["sampling ( signal processing )", "heuristic"]}
{"title": "learning balanced mixtures of discrete distributions with small sample", "abstract": "we study the problem of partitioning a small sample of $ n $ individuals from a mixture of $ k $ product distributions over a boolean cube $ \\ { 0 , 1\\ } ^k $ according to their distributions . each distribution is described by a vector of allele frequencies in $ \\r^k $ . given two distributions , we use $ \\gamma $ to denote the average $ \\ell_2^2 $ distance in frequencies across $ k $ dimensions , which measures the statistical divergence between them . we study the case assuming that bits are independently distributed across $ k $ dimensions . this work demonstrates that , for a balanced input instance for $ k = 2 $ , a certain graph-based optimization function returns the correct partition with high probability , where a weighted graph $ g $ is formed over $ n $ individuals , whose pairwise hamming distances between their corresponding bit vectors define the edge weights , so long as $ k = \\omega ( \\ln n/\\gamma ) $ and $ kn = \\tilde\\omega ( \\ln n/\\gamma^2 ) $ . the function computes a maximum-weight balanced cut of $ g $ , where the weight of a cut is the sum of the weights across all edges in the cut . this result demonstrates a nice property in the high-dimensional feature space : one can trade off the number of features that are required with the size of the sample to accomplish certain tasks like clustering .", "topics": ["feature vector", "cluster analysis"]}
{"title": "generating text with deep reinforcement learning", "abstract": "we introduce a novel schema for sequence to sequence learning with a deep q-network ( dqn ) , which decodes the output sequence iteratively . the aim here is to enable the decoder to first tackle easier portions of the sequences , and then turn to cope with difficult parts . specifically , in each iteration , an encoder-decoder long short-term memory ( lstm ) network is employed to , from the input sequence , automatically create features to represent the internal states of and formulate a list of potential actions for the dqn . take rephrasing a natural sentence as an example . this list can contain ranked potential words . next , the dqn learns to make decision on which action ( e.g . , word ) will be selected from the list to modify the current decoded sequence . the newly modified output sequence is subsequently used as the input to the dqn for the next decoding iteration . in each iteration , we also bias the reinforcement learning 's attention to explore sequence portions which are previously difficult to be decoded . for evaluation , the proposed strategy was trained to decode ten thousands natural sentences . our experiments indicate that , when compared to a left-to-right greedy beam search lstm decoder , the proposed method performed competitively well when decoding sentences from the training set , but significantly outperformed the baseline when decoding unseen sentences , in terms of bleu score obtained .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "deep neural network approximation using tensor sketching", "abstract": "deep neural networks are powerful learning models that achieve state-of-the-art performance on many computer vision , speech , and language processing tasks . in this paper , we study a fundamental question that arises when designing deep network architectures : given a target network architecture can we design a smaller network architecture that approximates the operation of the target network ? the question is , in part , motivated by the challenge of parameter reduction ( compression ) in modern deep neural networks , as the ever increasing storage and memory requirements of these networks pose a problem in resource constrained environments . in this work , we focus on deep convolutional neural network architectures , and propose a novel randomized tensor sketching technique that we utilize to develop a unified framework for approximating the operation of both the convolutional and fully connected layers . by applying the sketching technique along different tensor dimensions , we design changes to the convolutional and fully connected layers that substantially reduce the number of effective parameters in a network . we show that the resulting smaller network can be trained directly , and has a classification accuracy that is comparable to the original network .", "topics": ["approximation algorithm", "computer vision"]}
{"title": "learning representations of affect from speech", "abstract": "there has been a lot of prior work on representation learning for speech recognition applications , but not much emphasis has been given to an investigation of effective representations of affect from speech , where the paralinguistic elements of speech are separated out from the verbal content . in this paper , we explore denoising autoencoders for learning paralinguistic attributes i.e . categorical and dimensional affective traits from speech . we show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative of activation intensity and at separating out negative valence ( sadness and anger ) from positive valence ( happiness ) . we experiment with different input speech features ( such as fft and log-mel spectrograms with temporal context windows ) , and different autoencoder architectures ( such as stacked and deep autoencoders ) . we also learn utterance specific representations by a combination of denoising autoencoders and blstm based recurrent autoencoders . emotion classification is performed with the learnt temporal/dynamic representations to evaluate the quality of the representations . experiments on a well-established real-life speech dataset ( iemocap ) show that the learnt representations are comparable to state of the art feature extractors ( such as voice quality features and mfccs ) and are competitive with state-of-the-art approaches at emotion and dimensional affect recognition .", "topics": ["feature learning", "noise reduction"]}
{"title": "multivariate regression with grossly corrupted observations : a robust approach and its applications", "abstract": "this paper studies the problem of multivariate linear regression where a portion of the observations is grossly corrupted or is missing , and the magnitudes and locations of such occurrences are unknown in priori . to deal with this problem , we propose a new approach by explicitly consider the error source as well as its sparseness nature . an interesting property of our approach lies in its ability of allowing individual regression output elements or tasks to possess their unique noise levels . moreover , despite working with a non-smooth optimization problem , our approach still guarantees to converge to its optimal solution . experiments on synthetic data demonstrate the competitiveness of our approach compared with existing multivariate regression models . in addition , empirically our approach has been validated with very promising results on two exemplar real-world applications : the first concerns the prediction of \\textit { big-five } personality based on user behaviors at social network sites ( snss ) , while the second is 3d human hand pose estimation from depth images . the implementation of our approach and comparison methods as well as the involved datasets are made publicly available in support of the open-source and reproducible research initiatives .", "topics": ["optimization problem", "synthetic data"]}
{"title": "scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality", "abstract": "we propose a general matrix-valued multiple kernel learning framework for high-dimensional nonlinear multivariate regression problems . this framework allows a broad class of mixed norm regularizers , including those that induce sparsity , to be imposed on a dictionary of vector-valued reproducing kernel hilbert spaces . we develop a highly scalable and eigendecomposition-free algorithm that orchestrates two inexact solvers for simultaneously learning both the input and output components of separable matrix-valued kernels . as a key application enabled by our framework , we show how high-dimensional causal inference tasks can be naturally cast as sparse function estimation problems , leading to novel nonlinear extensions of a class of graphical granger causality techniques . our algorithmic developments and extensive empirical studies are complemented by theoretical analyses in terms of rademacher generalization bounds .", "topics": ["kernel ( operating system )", "nonlinear system"]}
{"title": "an algorithm for pattern discovery in time series", "abstract": "we present a new algorithm for discovering patterns in time series and other sequential data . we exhibit a reliable procedure for building the minimal set of hidden , markovian states that is statistically capable of producing the behavior exhibited in the data -- the underlying process 's causal states . unlike conventional methods for fitting hidden markov models ( hmms ) to data , our algorithm makes no assumptions about the process 's causal architecture ( the number of hidden states and their transition structure ) , but rather infers it from the data . it starts with assumptions of minimal structure and introduces complexity only when the data demand it . moreover , the causal states it infers have important predictive optimality properties that conventional hmm states lack . we introduce the algorithm , review the theory behind it , prove its asymptotic reliability , use large deviation theory to estimate its rate of convergence , and compare it to other algorithms which also construct hmms from data . we also illustrate its behavior on an example process , and report selected numerical results from an implementation .", "topics": ["time series", "numerical analysis"]}
{"title": "the new modality : emoji challenges in prediction , anticipation , and retrieval", "abstract": "over the past decade , emoji have emerged as a new and widespread form of digital communication , spanning diverse social networks and spoken languages . we propose to treat these ideograms as a new modality in their own right , distinct in their semantic structure from both the text in which they are often embedded as well as the images which they resemble . as a new modality , emoji present rich novel possibilities for representation and interaction . in this paper , we explore the challenges that arise naturally from considering the emoji modality through the lens of multimedia research . specifically , the ways in which emoji can be related to other common modalities such as text and images . to do so , we first present a large scale dataset of real-world emoji usage collected from twitter . this dataset contains examples of both text-emoji and image-emoji relationships . we present baseline results on the challenge of predicting emoji from both text and images , using state-of-the-art neural networks . further , we offer a first consideration into the problem of how to account for new , unseen emoji - a relevant issue as the emoji vocabulary continues to expand on a yearly basis . finally , we present results for multimedia retrieval using emoji as queries .", "topics": ["baseline ( configuration management )"]}
{"title": "initial comparison of linguistic networks measures for parallel texts", "abstract": "this paper presents preliminary results of croatian syllable networks analysis . syllable network is a network in which nodes are syllables and links between them are constructed according to their connections within words . in this paper we analyze networks of syllables generated from texts collected from the croatian wikipedia and blogs . as a main tool we use complex network analysis methods which provide mechanisms that can reveal new patterns in a language structure . we aim to show that syllable networks have much higher clustering coefficient in comparison to erd\\ '' os-renyi random networks . the results indicate that croatian syllable networks exhibit certain properties of a small world networks . furthermore , we compared croatian syllable networks with portuguese and chinese syllable networks and we showed that they have similar properties .", "topics": ["text corpus"]}
{"title": "a variational approximation for bayesian networks with discrete and continuous latent variables", "abstract": "we show how to use a variational approximation to the logistic function to perform approximate inference in bayesian networks containing discrete nodes with continuous parents . essentially , we convert the logistic function to a gaussian , which facilitates exact inference , and then iteratively adjust the variational parameters to improve the quality of the approximation . we demonstrate experimentally that this approximation is faster and potentially more accurate than sampling . we also introduce a simple new technique for handling evidence , which allows us to handle arbitrary distributions on observed nodes , as well as achieving a significant speedup in networks with discrete variables of large cardinality .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "semantic relation classification via convolutional neural networks with simple negative sampling", "abstract": "syntactic features play an essential role in identifying relationship in a sentence . previous neural network models often suffer from irrelevant information introduced when subjects and objects are in a long distance . in this paper , we propose to learn more robust relation representations from the shortest dependency path through a convolution neural network . we further propose a straightforward negative sampling strategy to improve the assignment of subjects and objects . experimental results show that our method outperforms the state-of-the-art methods on the semeval-2010 task 8 dataset .", "topics": ["sampling ( signal processing )", "parsing"]}
{"title": "learning deep representation without parameter inference for nonlinear dimensionality reduction", "abstract": "unsupervised deep learning is one of the most powerful representation learning techniques . restricted boltzman machine , sparse coding , regularized auto-encoders , and convolutional neural networks are pioneering building blocks of deep learning . in this paper , we propose a new building block -- distributed random models . the proposed method is a special full implementation of the product of experts : ( i ) each expert owns multiple hidden units and different experts have different numbers of hidden units ; ( ii ) the model of each expert is a k-center clustering , whose k-centers are only uniformly sampled examples , and whose output ( i.e . the hidden units ) is a sparse code that only the similarity values from a few nearest neighbors are reserved . the relationship between the pioneering building blocks , several notable research branches and the proposed method is analyzed . experimental results show that the proposed deep model can learn better representations than deep belief networks and meanwhile can train a much larger network with much less time than deep belief networks .", "topics": ["feature learning", "cluster analysis"]}
{"title": "pac-bayesian majority vote for late classifier fusion", "abstract": "a lot of attention has been devoted to multimedia indexing over the past few years . in the literature , we often consider two kinds of fusion schemes : the early fusion and the late fusion . in this paper we focus on late classifier fusion , where one combines the scores of each modality at the decision level . to tackle this problem , we investigate a recent and elegant well-founded quadratic program named mincq coming from the machine learning pac-bayes theory . mincq looks for the weighted combination , over a set of real-valued functions seen as voters , leading to the lowest misclassification rate , while making use of the voters ' diversity . we provide evidence that this method is naturally adapted to late fusion procedure . we propose an extension of mincq by adding an order- preserving pairwise loss for ranking , helping to improve mean averaged precision measure . we confirm the good behavior of the mincq-based fusion approaches with experiments on a real image benchmark .", "topics": ["bayesian network"]}
{"title": "diving deep into sentiment : understanding fine-tuned cnns for visual sentiment prediction", "abstract": "visual media are powerful means of expressing emotions and sentiments . the constant generation of new content in social networks highlights the need of automated visual sentiment analysis tools . while convolutional neural networks ( cnns ) have established a new state-of-the-art in several vision problems , their application to the task of sentiment analysis is mostly unexplored and there are few studies regarding how to design cnns for this purpose . in this work , we study the suitability of fine-tuning a cnn for visual sentiment prediction as well as explore performance boosting techniques within this deep learning setting . finally , we provide a deep-dive analysis into a benchmark , state-of-the-art network architecture to gain insight about how to design patterns for cnns on the task of visual sentiment prediction .", "topics": ["neural networks"]}
{"title": "query-free clothing retrieval via implicit relevance feedback", "abstract": "image-based clothing retrieval is receiving increasing interest with the growth of online shopping . in practice , users may often have a desired piece of clothing in mind ( e.g . , either having seen it before on the street or requiring certain specific clothing attributes ) but may be unable to supply an image as a query . we model this problem as a new type of image retrieval task in which the target image resides only in the user 's mind ( called `` mental image retrieval '' hereafter ) . because of the absence of an explicit query image , we propose to solve this problem through relevance feedback . specifically , a new bayesian formulation is proposed that simultaneously models the retrieval target and its high-level representation in the mind of the user ( called the `` user metric '' hereafter ) as posterior distributions of pre-fetched shop images and heterogeneous features extracted from multiple clothing attributes , respectively . requiring only clicks as user feedback , the proposed algorithm is able to account for the variability in human decision-making . experiments with real users demonstrate the effectiveness of the proposed algorithm .", "topics": ["high- and low-level", "relevance"]}
{"title": "fashion-mnist : a novel image dataset for benchmarking machine learning algorithms", "abstract": "we present fashion-mnist , a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories , with 7,000 images per category . the training set has 60,000 images and the test set has 10,000 images . fashion-mnist is intended to serve as a direct drop-in replacement for the original mnist dataset for benchmarking machine learning algorithms , as it shares the same image size , data format and the structure of training and testing splits . the dataset is freely available at https : //github.com/zalandoresearch/fashion-mnist", "topics": ["test set", "mnist database"]}
{"title": "discriminative learning via semidefinite probabilistic models", "abstract": "discriminative linear models are a popular tool in machine learning . these can be generally divided into two types : the first is linear classifiers , such as support vector machines , which are well studied and provide state-of-the-art results . one shortcoming of these models is that their output ( known as the 'margin ' ) is not calibrated , and can not be translated naturally into a distribution over the labels . thus , it is difficult to incorporate such models as components of larger systems , unlike probabilistic based approaches . the second type of approach constructs class conditional distributions using a nonlinearity ( e.g . log-linear models ) , but is occasionally worse in terms of classification error . we propose a supervised learning method which combines the best of both approaches . specifically , our method provides a distribution over the labels , which is a linear function of the model parameters . as a consequence , differences between probabilities are linear functions , a property which most probabilistic models ( e.g . log-linear ) do not have . our model assumes that classes correspond to linear subspaces ( rather than to half spaces ) . using a relaxed projection operator , we construct a measure which evaluates the degree to which a given vector 'belongs ' to a subspace , resulting in a distribution over labels . interestingly , this view is closely related to similar concepts in quantum detection theory . the resulting models can be trained either to maximize the margin or to optimize average likelihood measures . the corresponding optimization problems are semidefinite programs which can be solved efficiently . we illustrate the performance of our algorithm on real world datasets , and show that it outperforms 2nd order kernel methods .", "topics": ["support vector machine"]}
{"title": "real-time background subtraction using adaptive sampling and cascade of gaussians", "abstract": "background-foreground classification is a fundamental well-studied problem in computer vision . due to the pixel-wise nature of modeling and processing in the algorithm , it is usually difficult to satisfy real-time constraints . there is a trade-off between the speed ( because of model complexity ) and accuracy . inspired by the rejection cascade of viola-jones classifier , we decompose the gaussian mixture model ( gmm ) into an adaptive cascade of classifiers . this way we achieve a good improvement in speed without compensating for accuracy . in the training phase , we learn multiple kdes for different durations to be used as strong prior distribution and detect probable oscillating pixels which usually results in misclassifications . we propose a confidence measure for the classifier based on temporal consistency and the prior distribution . the confidence measure thus derived is used to adapt the learning rate and the thresholds of the model , to improve accuracy . the confidence measure is also employed to perform temporal and spatial sampling in a principled way . we demonstrate a speed-up factor of 5x to 10x and 17 percent average improvement in accuracy over several standard videos .", "topics": ["sampling ( signal processing )", "computer vision"]}
{"title": "large-scale electron microscopy image segmentation in spark", "abstract": "the emerging field of connectomics aims to unlock the mysteries of the brain by understanding the connectivity between neurons . to map this connectivity , we acquire thousands of electron microscopy ( em ) images with nanometer-scale resolution . after aligning these images , the resulting dataset has the potential to reveal the shapes of neurons and the synaptic connections between them . however , imaging the brain of even a tiny organism like the fruit fly yields terabytes of data . it can take years of manual effort to examine such image volumes and trace their neuronal connections . one solution is to apply image segmentation algorithms to help automate the tracing tasks . in this paper , we propose a novel strategy to apply such segmentation on very large datasets that exceed the capacity of a single machine . our solution is robust to potential segmentation errors which could otherwise severely compromise the quality of the overall segmentation , for example those due to poor classifier generalizability or anomalies in the image dataset . we implement our algorithms in a spark application which minimizes disk i/o , and apply them to a few large em datasets , revealing both their effectiveness and scalability . we hope this work will encourage external contributions to em segmentation by providing 1 ) a flexible plugin architecture that deploys easily on different cluster environments and 2 ) an in-memory representation of segmentation that could be conducive to new advances .", "topics": ["image segmentation", "scalability"]}
{"title": "the neural noisy channel", "abstract": "we formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models . unlike direct models which can suffer from explaining-away effects during training , noisy channel models must produce outputs that explain their inputs , and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution . using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol , we obtain a tractable and effective beam search decoder . experimental results on abstractive sentence summarisation , morphological inflection , and machine translation show that noisy channel models outperform direct models , and that they significantly benefit from increased amounts of unpaired output data that direct models can not easily use .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "denoising adversarial autoencoders", "abstract": "unsupervised learning is of growing interest because it unlocks the potential held in vast amounts of unlabelled data to learn useful representations for inference . autoencoders , a form of generative model , may be trained by learning to reconstruct unlabelled input data from a latent representation space . more robust representations may be produced by an autoencoder if it learns to recover clean input samples from corrupted ones . representations may be further improved by introducing regularisation during training to shape the distribution of the encoded data in latent space . we suggest denoising adversarial autoencoders , which combine denoising and regularisation , shaping the distribution of latent space using adversarial training . we introduce a novel analysis that shows how denoising may be incorporated into the training and sampling of adversarial autoencoders . experiments are performed to assess the contributions that denoising makes to the learning of representations for classification and sample synthesis . our results suggest that autoencoders trained using a denoising criterion achieve higher classification performance , and can synthesise samples that are more consistent with the input data than those trained without a corruption process .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "safer classification by synthesis", "abstract": "the discriminative approach to classification using deep neural networks has become the de-facto standard in various fields . complementing recent reservations about safety against adversarial examples , we show that conventional discriminative methods can easily be fooled to provide incorrect labels with very high confidence to out of distribution examples . we posit that a generative approach is the natural remedy for this problem , and propose a method for classification using generative models . at training time , we learn a generative model for each class , while at test time , given an example to classify , we query each generator for its most similar generation , and select the class corresponding to the most similar one . our approach is general and can be used with expressive models such as gans and vaes . at test time , our method accurately `` knows when it does not know , '' and provides resilience to out of distribution examples while maintaining competitive performance for standard examples .", "topics": ["generative model", "statistical classification"]}
{"title": "appearance invariance in convolutional networks with neighborhood similarity", "abstract": "we present a neighborhood similarity layer ( nsl ) which induces appearance invariance in a network when used in conjunction with convolutional layers . we are motivated by the observation that , even though convolutional networks have low generalization error , their generalization capability does not extend to samples which are not represented by the training data . for instance , while novel appearances of learned concepts pose no problem for the human visual system , feedforward convolutional networks are generally not successful in such situations . motivated by the gestalt principle of grouping with respect to similarity , the proposed nsl transforms its input feature map using the feature vectors at each pixel as a frame of reference , i.e . center of attention , for its surrounding neighborhood . this transformation is spatially varying , hence not a convolution . it is differentiable ; therefore , networks including the proposed layer can be trained in an end-to-end manner . we analyze the invariance of nsl to significant changes in appearance that are not represented in the training data . we also demonstrate its advantages for digit recognition , semantic labeling and cell detection problems .", "topics": ["test set", "convolution"]}
{"title": "embedding projector : interactive visualization and interpretation of embeddings", "abstract": "embeddings are ubiquitous in machine learning , appearing in recommender systems , nlp , and many other applications . researchers and developers often need to explore the properties of a specific embedding , and one way to analyze embeddings is to visualize them . we present the embedding projector , a tool for interactive visualization and interpretation of embeddings .", "topics": ["natural language processing"]}
{"title": "cloud computing framework for computer vision research : an introduction", "abstract": "cloud computing offers the potential to help scientists to process massive number of computing resources often required in machine learning application such as computer vision problems . this proposal would like to show that which benefits can be obtained from cloud in order to help medical image analysis users ( including scientists , clinicians , and research institutes ) . as security and privacy of algorithms are important for most of algorithms inventors , these algorithms can be hidden in a cloud to allow the users to use the algorithms as a package without any access to see/change their inside . in another word , in the user part , users send their images to the cloud and configure the algorithm via an interface . in the cloud part , the algorithms are applied to this image and the results are returned back to the user . my proposal has two parts : ( 1 ) investigate the potential of cloud computing for computer vision problems and ( 2 ) study the components of a proposed cloud-based framework for medical image analysis application and develop them ( depending on the length of the internship ) . the investigation part will involve a study on several aspects of the problem including security , usability ( for medical end users of the service ) , appropriate programming abstractions for vision problems , scalability and resource requirements . in the second part of this proposal i am going to thoroughly study of the proposed framework components and their relations and develop them . the proposed cloud-based framework includes an integrated environment to enable scientists and clinicians to access to the previous and current medical image analysis algorithms using a handful user interface without any access to the algorithm codes and procedures .", "topics": ["computer vision", "scalability"]}
{"title": "stochastic primal-dual coordinate method for regularized empirical risk minimization", "abstract": "we consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors . the problem structure allows us to reformulate it as a convex-concave saddle point problem . we propose a stochastic primal-dual coordinate ( spdc ) method , which alternates between maximizing over a randomly chosen dual variable and minimizing over the primal variable . an extrapolation step on the primal variable is performed to obtain accelerated convergence rate . we also develop a mini-batch version of the spdc method which facilitates parallel computing , and an extension with weighted sampling probabilities on the dual variables , which has a better complexity than uniform sampling on unnormalized data . both theoretically and empirically , we show that the spdc method has comparable or better performance than several state-of-the-art optimization methods .", "topics": ["optimization problem", "loss function"]}
{"title": "idel : in-database entity linking with neural embeddings", "abstract": "we present a novel architecture , in-database entity linking ( idel ) , in which we integrate the analytics-optimized rdbms monetdb with neural text mining abilities . our system design abstracts core tasks of most neural entity linking systems for monetdb . to the best of our knowledge , this is the first defacto implemented system integrating entity-linking in a database . we leverage the ability of monetdb to support in-database-analytics with user defined functions ( udfs ) implemented in python . these functions call machine learning libraries for neural text mining , such as tensorflow . the system achieves zero cost for data shipping and transformation by utilizing monetdb 's ability to embed python processes in the database kernel and exchange data in numpy arrays . idel represents text and relational data in a joint vector space with neural embeddings and can compensate errors with ambiguous entity representations . for detecting matching entities , we propose a novel similarity function based on joint neural embeddings which are learned via minimizing pairwise contrastive ranking loss . this function utilizes a high dimensional index structures for fast retrieval of matching entities . our first implementation and experiments using the webnlg corpus show the effectiveness and the potentials of idel .", "topics": ["database"]}
{"title": "a probabilistic framework for deep learning", "abstract": "we develop a probabilistic framework for deep learning based on the deep rendering mixture model ( drmm ) , a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables . we demonstrate that max-sum inference in the drmm yields an algorithm that exactly reproduces the operations in deep convolutional neural networks ( dcns ) , providing a first principles derivation . our framework provides new insights into the successes and shortcomings of dcns as well as a principled route to their improvement . drmm training via the expectation-maximization ( em ) algorithm is a powerful alternative to dcn back-propagation , and initial training results are promising . classification based on the drmm and other variants outperforms dcns in supervised digit classification , training 2-3x faster while achieving similar accuracy . moreover , the drmm is applicable to semi-supervised and unsupervised learning tasks , achieving results that are state-of-the-art in several categories on the mnist benchmark and comparable to state of the art on the cifar10 benchmark .", "topics": ["unsupervised learning", "mnist database"]}
{"title": "generative adversarial trainer : defense to adversarial perturbations with gan", "abstract": "we propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network . we alternately train both classifier and generator networks . the generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image . simultaneously , the classifier network is trained to classify correctly both original and adversarial images generated by the generator . these procedures help the classifier network to become more robust to adversarial perturbations . furthermore , our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as dropout . we applied our method to supervised learning for cifar datasets , and experimantal results show that our method significantly lowers the generalization error of the network . to the best of our knowledge , this is the first method which uses gan to improve supervised learning .", "topics": ["supervised learning", "matrix regularization"]}
{"title": "modelling collective motion based on the principle of agency", "abstract": "collective motion is an intriguing phenomenon , especially considering that it arises from a set of simple rules governing local interactions between individuals . in theoretical models , these rules are normally \\emph { assumed } to take a particular form , possibly constrained by heuristic arguments . we propose a new class of models , which describe the individuals as \\emph { agents } , capable of deciding for themselves how to act and learning from their experiences . the local interaction rules do not need to be postulated in this model , since they \\emph { emerge } from the learning process . we apply this ansatz to a concrete scenario involving marching locusts , in order to model the phenomenon of density-dependent alignment . we show that our learning agent-based model can account for a fokker-planck equation that describes the collective motion and , most notably , that the agents can learn the appropriate local interactions , requiring no strong previous assumptions on their form . these results suggest that learning agent-based models are a powerful tool for studying a broader class of problems involving collective motion and animal agency in general .", "topics": ["interaction", "heuristic"]}
{"title": "addendum to research mmmcv ; a man/microbio/megabio/computer vision", "abstract": "in october 2007 , a research proposal for the university of sydney , australia , the author suggested that biovie-physical phenomenon as `electrodynamic dependant biological vision ' , is governed by relativistic quantum laws and biovision . the phenomenon on the basis of `biovielectroluminescence ' , satisfies man/microbio/megabio/computer vision ( mmmcv ) , as a robust candidate for physical and visual sciences . the general aim of this addendum is to present a refined text of sections 1-3 of that proposal and highlighting the contents of its appendix in form of a `mechanisms ' section . we then briefly remind in an article aimed for december 2007 , by appending two more equations into section 3 , a theoretical ii-time scenario as a time model well-proposed for the phenomenon . the time model within the core of the proposal , plays a significant role in emphasizing the principle points on objectives no . 1-8 , sub-hypothesis 3.1.2 , mentioned in article [ arxiv:0710.0410 ] . it also expresses the time concept in terms of causing quantized energy f ( |e| ) of time |t| , emit in regard to shortening the probability of particle loci as predictable patterns of particle 's un-occurred motion , a solution to heisenberg 's uncertainty principle ( hup ) into a simplistic manner . we conclude that , practical frames via a time algorithm to this model , fixates such predictable patterns of motion of scenery bodies onto recordable observation points of a mmmcv system . it even suppresses/predicts superposition phenomena coming from a human subject and/or other bio-subjects for any decision making event , e.g . , brainwave quantum patterns based on vision . maintaining the existential probability of riemann surfaces of ii-time scenarios in the context of biovielectroluminescence , makes motion-prediction a possibility .", "topics": ["mathematical optimization", "computer vision"]}
{"title": "multivariate information bottleneck", "abstract": "the information bottleneck method is an unsupervised non-parametric data organization technique . given a joint distribution p ( a , b ) , this method constructs a new variable t that extracts partitions , or clusters , over the values of a that are informative about b . the information bottleneck has already been applied to document classification , gene expression , neural code , and spectral analysis . in this paper , we introduce a general principled framework for multivariate extensions of the information bottleneck method . this allows us to consider multiple systems of data partitions that are inter-related . our approach utilizes bayesian networks for specifying the systems of clusters and what information each captures . we show that this construction provides insight about bottleneck variations and enables us to characterize solutions of these variations . we also present a general framework for iterative algorithms for constructing solutions , and apply it to several examples .", "topics": ["unsupervised learning", "bayesian network"]}
{"title": "parallelpc : an r package for efficient constraint based causal exploration", "abstract": "discovering causal relationships from data is the ultimate goal of many research areas . constraint based causal exploration algorithms , such as pc , fci , rfci , pc-simple , ida and joint-ida have achieved significant progress and have many applications . a common problem with these methods is the high computational complexity , which hinders their applications in real world high dimensional datasets , e.g gene expression datasets . in this paper , we present an r package , parallelpc , that includes the parallelised versions of these causal exploration algorithms . the parallelised algorithms help speed up the procedure of experimenting big datasets and reduce the memory used when running the algorithms . the package is not only suitable for super-computers or clusters , but also convenient for researchers using personal computers with multi core cpus . our experiment results on real world datasets show that using the parallelised algorithms it is now practical to explore causal relationships in high dimensional datasets with thousands of variables in a single multicore computer . parallelpc is available in cran repository at https : //cran.rproject.org/web/packages/parallelpc/index.html .", "topics": ["computational complexity theory", "causality"]}
{"title": "what is ( missing or wrong ) in the scene ? a hybrid deep boltzmann machine for contextualized scene modeling", "abstract": "scene models allow robots to reason about what is in the scene , what else should be in it , and what should not be in it . in this paper , we propose a hybrid boltzmann machine ( bm ) for scene modeling where relations between objects are integrated . to be able to do that , we extend bm to include tri-way edges between visible ( object ) nodes and make the network to share the relations across different objects . we evaluate our method against several baseline models ( deep boltzmann machines , and restricted boltzmann machines ) on a scene classification dataset , and show that it performs better in several scene reasoning tasks .", "topics": ["baseline ( configuration management )", "robot"]}
{"title": "table-to-text generation by structure-aware seq2seq learning", "abstract": "table-to-text generation aims to generate a description for a factual table which can be viewed as a set of field-value records . to encode both the content and the structure of a table , we propose a novel structure-aware seq2seq architecture which consists of field-gating encoder and description generator with dual attention . in the encoding phase , we update the cell memory of the lstm unit by a field gate and its corresponding field value in order to incorporate field information into table representation . in the decoding phase , dual attention mechanism which contains word level attention and field level attention is proposed to model the semantic relevance between the generated description and the table . we conduct experiments on the \\texttt { wikibio } dataset which contains over 700k biographies and corresponding infoboxes from wikipedia . the attention visualizations and case studies show that our model is capable of generating coherent and informative descriptions based on the comprehensive understanding of both the content and the structure of a table . automatic evaluations also show our model outperforms the baselines by a great margin . code for this work is available on https : //github.com/tyliupku/wiki2bio .", "topics": ["relevance", "encoder"]}
{"title": "quality and diversity optimization : a unifying modular framework", "abstract": "the optimization of functions to find the best solution according to one or several objectives has a central role in many engineering and research fields . recently , a new family of optimization algorithms , named quality-diversity optimization , has been introduced , and contrasts with classic algorithms . instead of searching for a single solution , quality-diversity algorithms are searching for a large collection of both diverse and high-performing solutions . the role of this collection is to cover the range of possible solution types as much as possible , and to contain the best solution for each type . the contribution of this paper is threefold . firstly , we present a unifying framework of quality-diversity optimization algorithms that covers the two main algorithms of this family ( multi-dimensional archive of phenotypic elites and the novelty search with local competition ) , and that highlights the large variety of variants that can be investigated within this family . secondly , we propose algorithms with a new selection mechanism for quality-diversity algorithms that outperforms all the algorithms tested in this paper . lastly , we present a new collection management that overcomes the erosion issues observed when using unstructured collections . these three contributions are supported by extensive experimental comparisons of quality-diversity algorithms on three different experimental scenarios .", "topics": ["mathematical optimization"]}
{"title": "visual exploration of simulated and measured blood flow", "abstract": "morphology of cardiovascular tissue is influenced by the unsteady behavior of the blood flow and vice versa . therefore , the pathogenesis of several cardiovascular diseases is directly affected by the blood-flow dynamics . understanding flow behavior is of vital importance to understand the cardiovascular system and potentially harbors a considerable value for both diagnosis and risk assessment . the analysis of hemodynamic characteristics involves qualitative and quantitative inspection of the blood-flow field . visualization plays an important role in the qualitative exploration , as well as the definition of relevant quantitative measures and its validation . there are two main approaches to obtain information about the blood flow : simulation by computational fluid dynamics , and in-vivo measurements . although research on blood flow simulation has been performed for decades , many open problems remain concerning accuracy and patient-specific solutions . possibilities for real measurement of blood flow have recently increased considerably by new developments in magnetic resonance imaging which enable the acquisition of 3d quantitative measurements of blood-flow velocity fields . this chapter presents the visualization challenges for both simulation and real measurements of unsteady blood-flow fields .", "topics": ["simulation"]}
{"title": "efficient reinforcement learning in deterministic systems with value function generalization", "abstract": "we consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation ( ocp ) , an algorithm designed to synthesize efficient exploration and value function generalization . we establish that when the true value function lies within a given hypothesis class , ocp selects optimal actions over all but at most k episodes , where k is the eluder dimension of the given hypothesis class . we establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis class , for the special case where the hypothesis class is the span of pre-specified indicator functions over disjoint sets . we also discuss the computational complexity of ocp and present computational results involving two illustrative examples .", "topics": ["reinforcement learning"]}
{"title": "fairness beyond disparate treatment & disparate impact : learning classification without disparate mistreatment", "abstract": "automated data-driven decision making systems are increasingly being used to assist , or even replace humans in many settings . these systems function by learning from historical decisions , often taken by humans . in order to maximize the utility of these systems ( or , classifiers ) , their training involves minimizing the errors ( or , misclassifications ) over the given historical data . however , it is quite possible that the optimally trained classifier makes decisions for people belonging to different social groups with different misclassification rates ( e.g . , misclassification rates for females are higher than for males ) , thereby placing these groups at an unfair disadvantage . to account for and avoid such unfairness , in this paper , we introduce a new notion of unfairness , disparate mistreatment , which is defined in terms of misclassification rates . we then propose intuitive measures of disparate mistreatment for decision boundary-based classifiers , which can be easily incorporated into their formulation as convex-concave constraints . experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment , often at a small cost in terms of accuracy .", "topics": ["mathematical optimization", "synthetic data"]}
{"title": "analyzing features learned for offline signature verification using deep cnns", "abstract": "research on offline handwritten signature verification explored a large variety of handcrafted feature extractors , ranging from graphology , texture descriptors to interest points . in spite of advancements in the last decades , performance of such systems is still far from optimal when we test the systems against skilled forgeries - signature forgeries that target a particular individual . in previous research , we proposed a formulation of the problem to learn features from data ( signature images ) in a writer-independent format , using deep convolutional neural networks ( cnns ) , seeking to improve performance on the task . in this research , we push further the performance of such method , exploring a range of architectures , and obtaining a large improvement in state-of-the-art performance on the gpds dataset , the largest publicly available dataset on the task . in the gpds-160 dataset , we obtained an equal error rate of 2.74 % , compared to 6.97 % in the best result published in literature ( that used a combination of multiple classifiers ) . we also present a visual analysis of the feature space learned by the model , and an analysis of the errors made by the classifier . our analysis shows that the model is very effective in separating signatures that have a different global appearance , while being particularly vulnerable to forgeries that very closely resemble genuine signatures , even if their line quality is bad , which is the case of slowly-traced forgeries .", "topics": ["feature vector", "neural networks"]}
{"title": "unsupervised feature learning by deep sparse coding", "abstract": "in this paper , we propose a new unsupervised feature learning framework , namely deep sparse coding ( deepsc ) , that extends sparse coding to a multi-layer architecture for visual object recognition tasks . the main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module . the sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process , which takes advantage of the spatial smoothness information in the image . as a result , the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches . combining the feature representations from multiple layers , deepsc achieves the state-of-the-art performance on multiple object recognition tasks .", "topics": ["feature learning", "sparse matrix"]}
{"title": "hybrid code networks : practical and efficient end-to-end dialog control with supervised and reinforcement learning", "abstract": "end-to-end learning of recurrent neural networks ( rnns ) is an attractive solution for dialog systems ; however , current techniques are data-intensive and require thousands of dialogs to learn simple behaviors . we introduce hybrid code networks ( hcns ) , which combine an rnn with domain-specific knowledge encoded as software and system action templates . compared to existing end-to-end approaches , hcns considerably reduce the amount of training data required , while retaining the key benefit of inferring a latent representation of dialog state . in addition , hcns can be optimized with supervised learning , reinforcement learning , or a mixture of both . hcns attain state-of-the-art performance on the babi dialog dataset , and outperform two commercially deployed customer-facing dialog systems .", "topics": ["supervised learning", "test set"]}
{"title": "generating natural language descriptions from owl ontologies : the naturalowl system", "abstract": "we present naturalowl , a natural language generation system that produces texts describing individuals or classes of owl ontologies . unlike simpler owl verbalizers , which typically express a single axiom at a time in controlled , often not entirely fluent natural language primarily for the benefit of domain experts , we aim to generate fluent and coherent multi-sentence texts for end-users . with a system like naturalowl , one can publish information in owl on the web , along with automatically produced corresponding texts in multiple languages , making the information accessible not only to computer programs and domain experts , but also end-users . we discuss the processing stages of naturalowl , the optional domain-dependent linguistic resources that the system can use at each stage , and why they are useful . we also present trials showing that when the domain-dependent llinguistic resources are available , naturalowl produces significantly better texts compared to a simpler verbalizer , and that the resources can be created with relatively light effort .", "topics": ["natural language"]}
{"title": "a winnow-based approach to context-sensitive spelling correction", "abstract": "a large class of machine-learning problems in natural language require the characterization of linguistic context . two characteristic properties of such problems are that their feature space is of very high dimensionality , and their target concepts refer to only a small subset of the features in the space . under such conditions , multiplicative weight-update algorithms such as winnow have been shown to have exceptionally good theoretical properties . we present an algorithm combining variants of winnow and weighted-majority voting , and apply it to a problem in the aforementioned class : context-sensitive spelling correction . this is the task of fixing spelling errors that happen to result in valid words , such as substituting `` to '' for `` too '' , `` casual '' for `` causal '' , etc . we evaluate our algorithm , winspell , by comparing it against bayspell , a statistics-based method representing the state of the art for this task . we find : ( 1 ) when run with a full ( unpruned ) set of features , winspell achieves accuracies significantly higher than bayspell was able to achieve in either the pruned or unpruned condition ; ( 2 ) when compared with other systems in the literature , winspell exhibits the highest performance ; ( 3 ) the primary reason that winspell outperforms bayspell is that winspell learns a better linear separator ; ( 4 ) when run on a test set drawn from a different corpus than the training set was drawn from , winspell is better able than bayspell to adapt , using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the ( noisy ) test set .", "topics": ["test set", "supervised learning"]}
{"title": "the statistical recurrent unit", "abstract": "sophisticated gated recurrent neural network architectures like lstms and grus have been shown to be highly effective in a myriad of applications . we develop an un-gated unit , the statistical recurrent unit ( sru ) , that is able to learn long term dependencies in data by only keeping moving averages of statistics . the sru 's architecture is simple , un-gated , and contains a comparable number of parameters to lstms ; yet , srus perform favorably to more sophisticated lstm and gru alternatives , often outperforming one or both in various tasks . we show the efficacy of srus as compared to lstms and grus in an unbiased manner by optimizing respective architectures ' hyperparameters in a bayesian optimization scheme for both synthetic and real-world tasks .", "topics": ["recurrent neural network", "synthetic data"]}
{"title": "area protection in adversarial path-finding scenarios with multiple mobile agents on graphs : a theoretical and experimental study of target-allocation strategies for defense coordination", "abstract": "we address a problem of area protection in graph-based scenarios with multiple agents . the problem consists of two adversarial teams of agents that move in an undirected graph shared by both teams . agents are placed in vertices of the graph ; at most one agent can occupy a vertex ; and they can move into adjacent vertices in a conflict free way . teams have asymmetric goals : the aim of one team - attackers - is to invade into given area while the aim of the opponent team - defenders - is to protect the area from being entered by attackers by occupying selected vertices . we study strategies for allocating vertices to be occupied by the team of defenders to block attacking agents . we show that the decision version of the problem of area protection is pspace-hard under the assumption that agents can allocate their target vertices multiple times . further we develop various on-line vertex-allocation strategies for the defender team in a simplified variant of the problem with single stage vertex allocation and evaluated their performance in multiple benchmarks . the success of a strategy is heavily dependent on the type of the instance , and so one of the contributions of this work is that we identify suitable vertex-allocation strategies for diverse instance types . in particular , we introduce a simulation-based method that identifies and tries to capture bottlenecks in the graph , that are frequently used by the attackers . our experimental evaluation suggests that this method often allows a successful defense even in instances where the attackers significantly outnumber the defenders .", "topics": ["simulation"]}
{"title": "hierarchical actor-critic", "abstract": "the ability to learn at different resolutions in time may help overcome one of the main challenges in deep reinforcement learning -- sample efficiency . hierarchical agents that operate at different levels of temporal abstraction can learn tasks more quickly because they can divide the work of learning behaviors among multiple policies and can also explore the environment at a higher level . in this paper , we present a novel approach to hierarchical reinforcement learning called hierarchical actor-critic ( hac ) that enables agents to learn to break down problems involving continuous action spaces into simpler subproblems belonging to different time scales . hac has two key advantages over most existing hierarchical learning methods : ( i ) the potential for faster learning as agents learn short policies at each level of the hierarchy and ( ii ) an end-to-end approach . we demonstrate that hac significantly accelerates learning in a series of tasks that require behavior over a relatively long time horizon and involve sparse rewards .", "topics": ["reinforcement learning", "sparse matrix"]}
{"title": "a latent-variable lattice model", "abstract": "markov random field ( mrf ) learning is intractable , and its approximation algorithms are computationally expensive . we target a small subset of mrf that is used frequently in computer vision . we characterize this subset with three concepts : lattice , homogeneity , and inertia ; and design a non-markov model as an alternative . our goal is robust learning from small datasets . our learning algorithm uses vector quantization and , at time complexity o ( u log u ) for a dataset of u pixels , is much faster than that of general-purpose mrf .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "interpretable machine learning models for the digital clock drawing test", "abstract": "the clock drawing test ( cdt ) is a rapid , inexpensive , and popular neuropsychological screening tool for cognitive conditions . the digital clock drawing test ( dcdt ) uses novel software to analyze data from a digitizing ballpoint pen that reports its position with considerable spatial and temporal precision , making possible the analysis of both the drawing process and final product . we developed methodology to analyze pen stroke data from these drawings , and computed a large collection of features which were then analyzed with a variety of machine learning techniques . the resulting scoring systems were designed to be more accurate than the systems currently used by clinicians , but just as interpretable and easy to use . the systems also allow us to quantify the tradeoff between accuracy and interpretability . we created automated versions of the cdt scoring systems currently used by clinicians , allowing us to benchmark our models , which indicated that our machine learning models substantially outperformed the existing scoring systems .", "topics": ["value ( ethics )", "computational complexity theory"]}
{"title": "mob-esp and other improvements in probability estimation", "abstract": "a key prerequisite to optimal reasoning under uncertainty in intelligent systems is to start with good class probability estimates . this paper improves on the current best probability estimation trees ( bagged-pets ) and also presents a new ensemble-based algorithm ( mob-esp ) . comparisons are made using several benchmark datasets and multiple metrics . these experiments show that mob-esp outputs significantly more accurate class probabilities than either the baseline bpets algorithm or the enhanced version presented here ( eb-pets ) . these results are based on metrics closely associated with the average accuracy of the predictions . mob-esp also provides much better probability rankings than b-pets . the paper further suggests how these estimation techniques can be applied in concert with a broader category of classifiers .", "topics": ["baseline ( configuration management )"]}
{"title": "the case for temporal transparency : detecting policy change events in black-box decision making systems", "abstract": "bringing transparency to black-box decision making systems ( dms ) has been a topic of increasing research interest in recent years . traditional active and passive approaches to make these systems transparent are often limited by scalability and/or feasibility issues . in this paper , we propose a new notion of black-box dms transparency , named , temporal transparency , whose goal is to detect if/when the dms policy changes over time , and is mostly invariant to the drawbacks of traditional approaches . we map our notion of temporal transparency to time series changepoint detection methods , and develop a framework to detect policy changes in real-world dms 's . experiments on new york stop-question-and-frisk dataset reveal a number of publicly announced and unannounced policy changes , highlighting the utility of our framework .", "topics": ["time series", "scalability"]}
{"title": "scalable recommendation with poisson factorization", "abstract": "we develop a bayesian poisson matrix factorization model for forming recommendations from sparse user behavior data . these data are large user/item matrices where each user has provided feedback on only a small subset of items , either explicitly ( e.g . , through star ratings ) or implicitly ( e.g . , through views or purchases ) . in contrast to traditional matrix factorization approaches , poisson factorization implicitly models each user 's limited attention to consume items . moreover , because of the mathematical form of the poisson likelihood , the model needs only to explicitly consider the observed entries in the matrix , leading to both scalable computation and good predictive performance . we develop a variational inference algorithm for approximate posterior inference that scales up to massive data sets . this is an efficient algorithm that iterates over the observed entries and adjusts an approximate posterior over the user/item representations . we apply our method to large real-world user data containing users rating movies , users listening to songs , and users reading scientific papers . in all these settings , bayesian poisson factorization outperforms state-of-the-art matrix factorization methods .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "adaptive cost function for pointcloud registration", "abstract": "in this paper we introduce an adaptive cost function for pointcloud registration . the algorithm automatically estimates the sensor noise , which is important for generalization across different sensors and environments . through experiments on real and synthetic data , we show significant improvements in accuracy and robustness over state-of-the-art solutions .", "topics": ["synthetic data", "loss function"]}
{"title": "a compositional object-based approach to learning physical dynamics", "abstract": "we present the neural physics engine ( npe ) , a framework for learning simulators of intuitive physics that naturally generalize across variable object count and different scene configurations . we propose a factorization of a physical scene into composable object-based representations and a neural network architecture whose compositional structure factorizes object dynamics into pairwise interactions . like a symbolic physics engine , the npe is endowed with generic notions of objects and their interactions ; realized as a neural network , it can be trained via stochastic gradient descent to adapt to specific object properties and dynamics of different worlds . we evaluate the efficacy of our approach on simple rigid body dynamics in two-dimensional worlds . by comparing to less structured architectures , we show that the npe 's compositional representation of the structure in physical interactions improves its ability to predict movement , generalize across variable object count and different scene configurations , and infer latent properties of objects such as mass .", "topics": ["interaction", "simulation"]}
{"title": "learning temporal dependence from time-series data with latent variables", "abstract": "we consider the setting where a collection of time series , modeled as random processes , evolve in a causal manner , and one is interested in learning the graph governing the relationships of these processes . a special case of wide interest and applicability is the setting where the noise is gaussian and relationships are markov and linear . we study this setting with two additional features : firstly , each random process has a hidden ( latent ) state , which we use to model the internal memory possessed by the variables ( similar to hidden markov models ) . secondly , each variable can depend on its latent memory state through a random lag ( rather than a fixed lag ) , thus modeling memory recall with differing lags at distinct times . under this setting , we develop an estimator and prove that under a genericity assumption , the parameters of the model can be learned consistently . we also propose a practical adaption of this estimator , which demonstrates significant performance gains in both synthetic and real-world datasets .", "topics": ["time series", "synthetic data"]}
{"title": "information pursuit : a bayesian framework for sequential scene parsing", "abstract": "despite enormous progress in object detection and classification , the problem of incorporating expected contextual relationships among object instances into modern recognition systems remains a key challenge . in this work we propose information pursuit , a bayesian framework for scene parsing that combines prior models for the geometry of the scene and the spatial arrangement of objects instances with a data model for the output of high-level image classifiers trained to answer specific questions about the scene . in the proposed framework , the scene interpretation is progressively refined as evidence accumulates from the answers to a sequence of questions . at each step , we choose the question to maximize the mutual information between the new answer and the full interpretation given the current evidence obtained from previous inquiries . we also propose a method for learning the parameters of the model from synthesized , annotated scenes obtained by top-down sampling from an easy-to-learn generative scene model . finally , we introduce a database of annotated indoor scenes of dining room tables , which we use to evaluate the proposed approach .", "topics": ["object detection", "high- and low-level"]}
{"title": "on preemption and overdetermination in formal theories of causality", "abstract": "one of the key challenges when looking for the causes of a complex event is to determine the causal status of factors that are neither individually necessary nor individually sufficient to produce that event . in order to reason about how such factors should be taken into account , we need a vocabulary to distinguish different cases . in philosophy , the concept of overdetermination and the concept of preemption serve an important purpose in this regard , although their exact meaning tends to remain elusive . in this paper , i provide theory-neutral definitions of these concepts using structural equations in the halpern-pearl tradition . while my definitions do not presuppose any particular causal theory , they take such a theory as a variable parameter . this enables us to specify formal constraints on theories of causality , in terms of a pre-theoretic understanding of what preemption and overdetermination actually mean . i demonstrate the usefulness of this by presenting and arguing for what i call the principle of presumption . roughly speaking , this principle states that a possible cause can only be regarded as having been preempted if there is independent evidence to support such an inference . i conclude by showing that the principle of presumption is violated by the two main theories of causality formulated in the halpern-pearl tradition . the paper concludes by defining the class of empirical causal theories , characterised in terms of a fixed-point of counterfactual reasoning about difference-making . it is argued that theories of actual causality ought to be empirical .", "topics": ["causality"]}
{"title": "using kl-divergence to focus deep visual explanation", "abstract": "we present a method for explaining the image classification predictions of deep convolution neural networks , by highlighting the pixels in the image which influence the final class prediction . our method requires the identification of a heuristic method to select parameters hypothesized to be most relevant in this prediction , and here we use kullback-leibler divergence to provide this focus . overall , our approach helps in understanding and interpreting deep network predictions and we hope contributes to a foundation for such understanding of deep learning networks . in this brief paper , our experiments evaluate the performance of two popular networks in this context of interpretability .", "topics": ["computer vision", "convolution"]}
{"title": "sample-efficient actor-critic reinforcement learning with supervised data for dialogue management", "abstract": "deep reinforcement learning ( rl ) methods have significant potential for dialogue policy optimisation . however , they suffer from a poor performance in the early stages of learning . this is especially problematic for on-line learning with real users . two approaches are introduced to tackle this problem . firstly , to speed up the learning process , two sample-efficient neural networks algorithms : trust region actor-critic with experience replay ( tracer ) and episodic natural actor-critic with experience replay ( enacer ) are presented . for tracer , the trust region helps to control the learning step size and avoid catastrophic model changes . for enacer , the natural gradient identifies the steepest ascent direction in policy space to speed up the convergence . both models employ off-policy learning with experience replay to improve sample-efficiency . secondly , to mitigate the cold start issue , a corpus of demonstration data is utilised to pre-train the models prior to on-line reinforcement learning . combining these two approaches , we demonstrate a practical approach to learn deep rl-based dialogue policies and demonstrate their effectiveness in a task-oriented information seeking domain .", "topics": ["reinforcement learning", "mathematical optimization"]}
{"title": "guaranteed clustering and biclustering via semidefinite programming", "abstract": "identifying clusters of similar objects in data plays a significant role in a wide range of applications . as a model problem for clustering , we consider the densest k-disjoint-clique problem , whose goal is to identify the collection of k disjoint cliques of a given weighted complete graph maximizing the sum of the densities of the complete subgraphs induced by these cliques . in this paper , we establish conditions ensuring exact recovery of the densest k cliques of a given graph from the optimal solution of a particular semidefinite program . in particular , the semidefinite relaxation is exact for input graphs corresponding to data consisting of k large , distinct clusters and a smaller number of outliers . this approach also yields a semidefinite relaxation for the biclustering problem with similar recovery guarantees . given a set of objects and a set of features exhibited by these objects , biclustering seeks to simultaneously group the objects and features according to their expression levels . this problem may be posed as partitioning the nodes of a weighted bipartite complete graph such that the sum of the densities of the resulting bipartite complete subgraphs is maximized . as in our analysis of the densest k-disjoint-clique problem , we show that the correct partition of the objects and features can be recovered from the optimal solution of a semidefinite program in the case that the given data consists of several disjoint sets of objects exhibiting similar features . empirical evidence from numerical experiments supporting these theoretical guarantees is also provided .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "maximum entropy based non-negative optoacoustic tomographic image reconstruction", "abstract": "optoacoustic ( photoacoustic ) tomography reconstructs maps of the initial pressure rise induced by the absorption of light pulses in tissue . in practice , due to inaccurate assumptions in the forward model employed , noise and other experimental factors , the images often contain errors , occasionally manifested as negative values . we present optoacoustic tomography based on an entropy maximization algorithm that uses logarithmic regularization as a potent method for imparting non-negative image reconstruction . we experimentally investigate the performance achieved by the entropy maximization scheme on phantoms and in vivo samples . the findings demonstrate that the proposed scheme reconstructs physically relevant image values devoid of unwanted negative contrast , thus improving quantitative imaging performance .", "topics": ["matrix regularization", "map"]}
{"title": "adversarial vulnerability of neural networks increases with input dimension", "abstract": "over the past four years , neural networks have proven vulnerable to adversarial images : targeted but imperceptible image perturbations lead to drastically different predictions . we show that adversarial vulnerability increases with the gradients of the training objective when seen as a function of the inputs . for most current network architectures , we prove that the $ \\ell_1 $ -norm of these gradients grows as the square root of the input-size . these nets therefore become increasingly vulnerable with growing image size . over the course of our analysis we rediscover and generalize double-backpropagation , a technique that penalizes large gradients in the loss surface to reduce adversarial vulnerability and increase generalization performance . we show that this regularization-scheme is equivalent at first order to training with adversarial noise . finally , we demonstrate that replacing strided by average-pooling layers decreases adversarial vulnerability . our proofs rely on the network 's weight-distribution at initialization , but extensive experiments confirm their conclusions after training .", "topics": ["neural networks", "matrix regularization"]}
{"title": "random subspace with trees for feature selection under memory constraints", "abstract": "dealing with datasets of very high dimension is a major challenge in machine learning . in this paper , we consider the problem of feature selection in applications where the memory is not large enough to contain all features . in this setting , we propose a novel tree-based feature selection approach that builds a sequence of randomized trees on small subsamples of variables mixing both variables already identified as relevant by previous models and variables randomly selected among the other variables . as our main contribution , we provide an in-depth theoretical analysis of this method in infinite sample setting . in particular , we study its soundness with respect to common definitions of feature relevance and its convergence speed under various variable dependance scenarios . we also provide some preliminary empirical results highlighting the potential of the approach .", "topics": ["relevance"]}
{"title": "semi-supervised kernel metric learning using relative comparisons", "abstract": "we consider the problem of metric learning subject to a set of constraints on relative-distance comparisons between the data items . such constraints are meant to reflect side-information that is not expressed directly in the feature vectors of the data items . the relative-distance constraints used in this work are particularly effective in expressing structures at finer level of detail than must-link ( ml ) and can not -link ( cl ) constraints , which are most commonly used for semi-supervised clustering . relative-distance constraints are thus useful in settings where providing an ml or a cl constraint is difficult because the granularity of the true clustering is unknown . our main contribution is an efficient algorithm for learning a kernel matrix using the log determinant divergence -- - a variant of the bregman divergence -- - subject to a set of relative-distance constraints . the learned kernel matrix can then be employed by many different kernel methods in a wide range of applications . in our experimental evaluations , we consider a semi-supervised clustering setting and show empirically that kernels found by our algorithm yield clusterings of higher quality than existing approaches that either use ml/cl constraints or a different means to implement the supervision using relative comparisons .", "topics": ["kernel ( operating system )", "cluster analysis"]}
{"title": "energy saving additive neural network", "abstract": "in recent years , machine learning techniques based on neural networks for mobile computing become increasingly popular . classical multi-layer neural networks require matrix multiplications at each stage . multiplication operation is not an energy efficient operation and consequently it drains the battery of the mobile device . in this paper , we propose a new energy efficient neural network with the universal approximation property over space of lebesgue integrable functions . this network , called , additive neural network , is very suitable for mobile computing . the neural structure is based on a novel vector product definition , called ef-operator , that permits a multiplier-free implementation . in ef-operation , the `` product '' of two real numbers is defined as the sum of their absolute values , with the sign determined by the sign of the product of the numbers . this `` product '' is used to construct a vector product in $ r^n $ . the vector product induces the $ l_1 $ norm . the proposed additive neural network successfully solves the xor problem . the experiments on mnist dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi-layer perceptron and convolutional neural networks ( lenet ) .", "topics": ["mnist database"]}
{"title": "gazedpm : early integration of gaze information in deformable part models", "abstract": "an increasing number of works explore collaborative human-computer systems in which human gaze is used to enhance computer vision systems . for object detection these efforts were so far restricted to late integration approaches that have inherent limitations , such as increased precision without increase in recall . we propose an early integration approach in a deformable part model , which constitutes a joint formulation over gaze and visual data . we show that our gazedpm method improves over the state-of-the-art dpm baseline by 4 % and a recent method for gaze-supported object detection by 3 % on the public poet dataset . our approach additionally provides introspection of the learnt models , can reveal salient image structures , and allows us to investigate the interplay between gaze attracting and repelling areas , the importance of view-specific models , as well as viewers ' personal biases in gaze patterns . we finally study important practical aspects of our approach , such as the impact of using saliency maps instead of real fixations , the impact of the number of fixations , as well as robustness to gaze estimation error .", "topics": ["baseline ( configuration management )", "object detection"]}
{"title": "video question answering via attribute-augmented attention network learning", "abstract": "video question answering is a challenging problem in visual information retrieval , which provides the answer to the referenced video content according to the question . however , the existing visual question answering approaches mainly tackle the problem of static image question , which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents . in this paper , we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism . we propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering . we then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance . we construct a large-scale video question answering dataset . we conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method .", "topics": ["feature learning"]}
{"title": "end-to-end convolutional network for saliency prediction", "abstract": "the prediction of saliency areas in images has been traditionally addressed with hand crafted features based on neuroscience principles . this paper however addresses the problem with a completely data-driven approach by training a convolutional network . the learning process is formulated as a minimization of a loss function that measures the euclidean distance of the predicted saliency map with the provided ground truth . the recent publication of large datasets of saliency prediction has provided enough data to train a not very deep architecture which is both fast and accurate . the convolutional network in this paper , named juntingnet , won the lsun 2015 challenge on saliency prediction with a superior performance in all considered metrics .", "topics": ["loss function", "ground truth"]}
{"title": "learning to search for dependencies", "abstract": "we demonstrate that a dependency parser can be built using a credit assignment compiler which removes the burden of worrying about low-level machine learning details from the parser implementation . the result is a simple parser which robustly applies to many languages that provides similar statistical and computational performance with best-to-date transition-based parsing approaches , while avoiding various downsides including randomization , extra feature requirements , and custom learning algorithms .", "topics": ["parsing"]}
{"title": "clustering by deep nearest neighbor descent ( d-nnd ) : a density-based parameter-insensitive clustering method", "abstract": "most density-based clustering methods largely rely on how well the underlying density is estimated . however , density estimation itself is also a challenging problem , especially the determination of the kernel bandwidth . a large bandwidth could lead to the over-smoothed density estimation in which the number of density peaks could be less than the true clusters , while a small bandwidth could lead to the under-smoothed density estimation in which spurious density peaks , or called the `` ripple noise '' , would be generated in the estimated density . in this paper , we propose a density-based hierarchical clustering method , called the deep nearest neighbor descent ( d-nnd ) , which could learn the underlying density structure layer by layer and capture the cluster structure at the same time . the over-smoothed density estimation could be largely avoided and the negative effect of the under-estimated cases could be also largely reduced . overall , d-nnd presents not only the strong capability of discovering the underlying cluster structure but also the remarkable reliability due to its insensitivity to parameters .", "topics": ["cluster analysis"]}
{"title": "methods of measurement the three-dimensional wind waves spectra , based on the processing of video images of the sea surface", "abstract": "optical instruments for measuring surface-wave characteristics provide a better spatial and temporal resolution than other methods , but they face difficulties while converting the results of indirect measurements into absolute levels of the waves . we have solved this problem to some extent . in this paper , we propose an optical method for measuring the 3d power spectral density of the surface waves and spatio-temporal samples of the wave profiles . the method involves , first , synchronous recording of the brightness field over a patch of a rough surface and measurement of surface oscillations at one or more points and , second , filtering of the spatial image spectrum . filter parameters are chosen to maximize the correlation of the surface oscillations recovered and measured at one or two points . in addition to the measurement procedure , the paper provides experimental results of measuring multidimensional spectra of roughness , which generally agree with theoretical expectations and the results of other authors .", "topics": ["image processing"]}
{"title": "measurement-based adaptation protocol with quantum reinforcement learning", "abstract": "machine learning employs dynamical algorithms that mimic the human capacity to learn , where the reinforcement learning ones are among the most similar to humans in this respect . on the other hand , adaptability is an essential aspect to perform any task efficiently in a changing environment , and it is fundamental for many purposes , such as natural selection . here , we propose an algorithm based on successive measurements to adapt one quantum state to a reference unknown state , in the sense of achieving maximum overlap . the protocol naturally provides many identical copies of the reference state , such that in each measurement iteration more information about it is obtained . in our protocol , we consider a system composed of three parts , the `` environment '' system , which provides the reference state copies ; the register , which is an auxiliary subsystem that interacts with the environment to acquire information from it ; and the agent , which corresponds to the quantum state that is adapted by digital feedback with input corresponding to the outcome of the measurements on the register . with this proposal we can achieve an average fidelity between the environment and the agent of more than $ 90\\ % $ with less than $ 30 $ iterations of the protocol . in addition , we extend the formalism to $ d $ -dimensional states , reaching an average fidelity of around $ 80\\ % $ in less than $ 400 $ iterations for $ d= $ 11 , for a variety of genuinely quantum as well as semiclassical states . this work paves the way for the development of quantum reinforcement learning protocols using quantum data , and the future deployment of semi-autonomous quantum systems .", "topics": ["reinforcement learning"]}
{"title": "neural network regularization via robust weight factorization", "abstract": "regularization is essential when training large neural networks . as deep neural networks can be mathematically interpreted as universal function approximators , they are effective at memorizing sampling noise in the training data . this results in poor generalization to unseen data . therefore , it is no surprise that a new regularization technique , dropout , was partially responsible for the now-ubiquitous winning entry to imagenet 2012 by the university of toronto . currently , dropout ( and related methods such as dropconnect ) are the most effective means of regularizing large neural networks . these amount to efficiently visiting a large number of related models at training time , while aggregating them to a single predictor at test time . the proposed fame model aims to apply a similar strategy , yet learns a factorization of each weight matrix such that the factors are robust to noise .", "topics": ["sampling ( signal processing )", "test set"]}
{"title": "interpretable two-level boolean rule learning for classification", "abstract": "this paper proposes algorithms for learning two-level boolean rules in conjunctive normal form ( cnf , i.e . and-of-ors ) or disjunctive normal form ( dnf , i.e . or-of-ands ) as a type of human-interpretable classification model , aiming for a favorable trade-off between the classification accuracy and the simplicity of the rule . two formulations are proposed . the first is an integer program whose objective function is a combination of the total number of errors and the total number of features used in the rule . we generalize a previously proposed linear programming ( lp ) relaxation from one-level to two-level rules . the second formulation replaces the 0-1 classification error with the hamming distance from the current two-level rule to the closest rule that correctly classifies a sample . based on this second formulation , block coordinate descent and alternating minimization algorithms are developed . experiments show that the two-level rules can yield noticeably better performance than one-level rules due to their dramatically larger modeling capacity , and the two algorithms based on the hamming distance formulation are generally superior to the other two-level rule learning methods in our comparison . a proposed approach to binarize any fractional values in the optimal solutions of lp relaxations is also shown to be effective .", "topics": ["optimization problem", "loss function"]}
{"title": "move evaluation in go using deep convolutional neural networks", "abstract": "the game of go is more challenging than other board games , due to the difficulty of constructing a position or move evaluation function . in this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge . we train a large 12-layer convolutional neural network by supervised learning from a database of human professional games . the network correctly predicts the expert move in 55 % of positions , equalling the accuracy of a 6 dan human player . when the trained convolutional network was used directly to play games of go , without any search , it beat the traditional search program gnugo in 97 % of games , and matched the performance of a state-of-the-art monte-carlo tree search that simulates a million positions per move .", "topics": ["supervised learning"]}
{"title": "towards practical bayesian parameter and state estimation", "abstract": "joint state and parameter estimation is a core problem for dynamic bayesian networks . although modern probabilistic inference toolkits make it relatively easy to specify large and practically relevant probabilistic models , the silver bullet -- -an efficient and general online inference algorithm for such problems -- -remains elusive , forcing users to write special-purpose code for each application . we propose a novel blackbox algorithm -- a hybrid of particle filtering for state variables and assumed density filtering for parameter variables . it has following advantages : ( a ) it is efficient due to its online nature , and ( b ) it is applicable to both discrete and continuous parameter spaces . on a variety of toy and real models , our system is able to generate more accurate results within a fixed computation budget . this preliminary evidence indicates that the proposed approach is likely to be of practical use .", "topics": ["bayesian network", "computation"]}
{"title": "a microwave imaging and enhancement technique from noisy synthetic data", "abstract": "an inverse iterative algorithm for microwave imaging based on moment method solution is presented here . the iterative scheme has been developed on constrained optimization technique and is certain to converge . different mesh size for the model has been used here to overcome the inverse crime . the synthetic data at the receivers is contaminated with different percentage of noise . the ill-posedness of the problem is solved by levenberg-marquardt method . the algorithm is applied to synthetic data and the reconstructed image is then further enhanced through the image enhancement technique", "topics": ["synthetic data", "iteration"]}
{"title": "learning to play with intrinsically-motivated self-aware agents", "abstract": "infants are experts at playing , with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals . we seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation . using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees , we propose a `` world-model '' network that learns to predict the dynamic consequences of the agent 's actions . simultaneously , we train a separate explicit `` self-model '' that allows the agent to track the error map of its own world-model , and then uses the self-model to adversarially challenge the developing world-model . we demonstrate that this policy causes the agent to explore novel and informative interactions with its environment , leading to the generation of a spectrum of complex behaviors , including ego-motion prediction , object attention , and object gathering . moreover , the world-model that the agent learns supports improved performance on object dynamics prediction , detection , localization and recognition tasks . taken together , our results are initial steps toward creating flexible autonomous agents that self-supervise in complex novel physical environments .", "topics": ["interaction", "simulation"]}
{"title": "forecasting of events by tweet data mining", "abstract": "this paper describes the analysis of quantitative characteristics of frequent sets and association rules in the posts of twitter microblogs related to different event discussions . for the analysis , we used a theory of frequent sets , association rules and a theory of formal concept analysis . we revealed the frequent sets and association rules which characterize the semantic relations between the concepts of analyzed subjects . the support of some frequent sets reaches its global maximum before the expected event but with some time delay . such frequent sets may be considered as predictive markers that characterize the significance of expected events for blogosphere users . we showed that the time dynamics of confidence in some revealed association rules can also have predictive characteristics . exceeding a certain threshold may be a signal for corresponding reaction in the society within the time interval between the maximum and the probable coming of an event . in this paper , we considered two types of events : the olympic tennis tournament final in london , 2012 and the prediction of eurovision 2013 winner .", "topics": ["data mining"]}
{"title": "dcfnet : deep neural network with decomposed convolutional filters", "abstract": "filters in a convolutional neural network ( cnn ) contain model parameters learned from enormous amounts of data . in this paper , we suggest to decompose convolutional filters in cnn as a truncated expansion with pre-fixed bases , namely the decomposed convolutional filters network ( dcfnet ) , where the expansion coefficients remain learned from data . such a structure not only reduces the number of trainable parameters and computation , but also imposes filter regularity by bases truncation . through extensive experiments , we consistently observe that dcfnet maintains accuracy for image classification tasks with a significant reduction of model parameters , particularly with fourier-bessel ( fb ) bases , and even with random bases . theoretically , we analyze the representation stability of dcfnet with respect to input variations , and prove representation stability under generic assumptions on the expansion coefficients . the analysis is consistent with the empirical observations .", "topics": ["computer vision", "computation"]}
{"title": "learning sparse representations in reinforcement learning with sparse coding", "abstract": "a variety of representation learning approaches have been investigated for reinforcement learning ; much less attention , however , has been given to investigating the utility of sparse coding . outside of reinforcement learning , sparse coding representations have been widely used , with non-convex objectives that result in discriminative representations . in this work , we develop a supervised sparse coding objective for policy evaluation . despite the non-convexity of this objective , we prove that all local minima are global minima , making the approach amenable to simple optimization strategies . we empirically show that it is key to use a supervised objective , rather than the more straightforward unsupervised sparse coding approach . we compare the learned representations to a canonical fixed sparse representation , called tile-coding , demonstrating that the sparse coding representation outperforms a wide variety of tilecoding representations .", "topics": ["feature learning", "supervised learning"]}
{"title": "alternative markov and causal properties for acyclic directed mixed graphs", "abstract": "we extend andersson-madigan-perlman chain graphs by ( i ) relaxing the semidirected acyclity constraint so that only directed cycles are forbidden , and ( ii ) allowing up to two edges between any pair of nodes . we introduce global , and ordered local and pairwise markov properties for the new models . we show the equivalence of these properties for strictly positive probability distributions . we also show that when the random variables are continuous , the new models can be interpreted as systems of structural equations with correlated errors . this enables us to adapt pearl 's do-calculus to them . finally , we describe an exact algorithm for learning the new models from observational and interventional data via answer set programming .", "topics": ["markov chain", "causality"]}
{"title": "scalable sparse subspace clustering by orthogonal matching pursuit", "abstract": "subspace clustering methods based on $ \\ell_1 $ , $ \\ell_2 $ or nuclear norm regularization have become very popular due to their simplicity , theoretical guarantees and empirical success . however , the choice of the regularizer can greatly impact both theory and practice . for instance , $ \\ell_1 $ regularization is guaranteed to give a subspace-preserving affinity ( i.e . , there are no connections between points from different subspaces ) under broad conditions ( e.g . , arbitrary subspaces and corrupted data ) . however , it requires solving a large scale convex optimization problem . on the other hand , $ \\ell_2 $ and nuclear norm regularization provide efficient closed form solutions , but require very strong assumptions to guarantee a subspace-preserving affinity , e.g . , independent subspaces and uncorrupted data . in this paper we study a subspace clustering method based on orthogonal matching pursuit . we show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions . experiments on synthetic data verify our theoretical analysis , and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "detecting events and patterns in large-scale user generated textual streams with statistical learning methods", "abstract": "a vast amount of textual web streams is influenced by events or phenomena emerging in the real world . the social web forms an excellent modern paradigm , where unstructured user generated content is published on a regular basis and in most occasions is freely distributed . the present ph.d. thesis deals with the problem of inferring information - or patterns in general - about events emerging in real life based on the contents of this textual stream . we show that it is possible to extract valuable information about social phenomena , such as an epidemic or even rainfall rates , by automatic analysis of the content published in social media , and in particular twitter , using statistical machine learning methods . an important intermediate task regards the formation and identification of features which characterise a target event ; we select and use those textual features in several linear , non-linear and hybrid inference approaches achieving a significantly good performance in terms of the applied loss function . by examining further this rich data set , we also propose methods for extracting various types of mood signals revealing how affective norms - at least within the social web 's population - evolve during the day and how significant events emerging in the real world are influencing them . lastly , we present some preliminary findings showing several spatiotemporal characteristics of this textual information as well as the potential of using it to tackle tasks such as the prediction of voting intentions .", "topics": ["nonlinear system", "loss function"]}
{"title": "theoretical analysis of domain adaptation with optimal transport", "abstract": "domain adaptation ( da ) is an important and emerging field of machine learning that tackles the problem occurring when the distributions of training ( source domain ) and test ( target domain ) data are similar but different . current theoretical results show that the efficiency of da algorithms depends on their capacity of minimizing the divergence between source and target probability distributions . in this paper , we provide a theoretical study on the advantages that concepts borrowed from optimal transportation theory can bring to da . in particular , we show that the wasserstein metric can be used as a divergence measure between distributions to obtain generalization guarantees for three different learning settings : ( i ) classic da with unsupervised target data ( ii ) da combining source and target labeled data , ( iii ) multiple source da . based on the obtained results , we provide some insights showing when this analysis can be tighter than other existing frameworks .", "topics": ["loss function"]}
{"title": "beneath the valley of the noncommutative arithmetic-geometric mean inequality : conjectures , case-studies , and consequences", "abstract": "randomized algorithms that base iteration-level decisions on samples from some pool are ubiquitous in machine learning and optimization . examples include stochastic gradient descent and randomized coordinate descent . this paper makes progress at theoretically evaluating the difference in performance between sampling with- and without-replacement in such algorithms . focusing on least means squares optimization , we formulate a noncommutative arithmetic-geometric mean inequality that would prove that the expected convergence rate of without-replacement sampling is faster than that of with-replacement sampling . we demonstrate that this inequality holds for many classes of random matrices and for some pathological examples as well . we provide a deterministic worst-case bound on the gap between the discrepancy between the two sampling models , and explore some of the impediments to proving this inequality in full generality . we detail the consequences of this inequality for stochastic gradient descent and the randomized kaczmarz algorithm for solving linear systems .", "topics": ["sampling ( signal processing )", "gradient descent"]}
{"title": "clusters of driving behavior from observational smartphone data", "abstract": "understanding driving behaviors is essential for improving safety and mobility of our transportation systems . data is usually collected via simulator-based studies or naturalistic driving studies . those techniques allow for understanding relations between demographics , road conditions and safety . on the other hand , they are very costly and time consuming . thanks to the ubiquity of smartphones , we have an opportunity to substantially complement more traditional data collection techniques with data extracted from phone sensors , such as gps , accelerometer gyroscope and camera . we developed statistical models that provided insight into driver behavior in the san francisco metro area based on tens of thousands of driver logs . we used novel data sources to support our work . we used cell phone sensor data drawn from five hundred drivers in san francisco to understand the speed of traffic across the city as well as the maneuvers of drivers in different areas . specifically , we clustered drivers based on their driving behavior . we looked at driver norms by street and flagged driving behaviors that deviated from the norm .", "topics": ["simulation", "sensor"]}
{"title": "exploiting points and lines in regression forests for rgb-d camera relocalization", "abstract": "camera relocalization plays a vital role in many robotics and computer vision tasks , such as global localization , recovery from tracking failure and loop closure detection . recent random forests based methods exploit randomly sampled pixel comparison features to predict 3d world locations for 2d image locations to guide the camera pose optimization . however , these image features are only sampled randomly in the images , without considering the spatial structures or geometric information , leading to large errors or failure cases with the existence of poorly textured areas or in motion blur . line segment features are more robust in these environments . in this work , we propose to jointly exploit points and lines within the framework of uncertainty driven regression forests . the proposed approach is thoroughly evaluated on three publicly available datasets against several strong state-of-the-art baselines in terms of several different error metrics . experimental results prove the efficacy of our method , showing superior or on-par state-of-the-art performance .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "machine comprehension using match-lstm and answer pointer", "abstract": "machine comprehension of text is an important problem in natural language processing . a recently released dataset , the stanford question answering dataset ( squad ) , offers a large number of real questions and their answers created by humans through crowdsourcing . squad provides a challenging testbed for evaluating machine comprehension algorithms , partly because compared with previous datasets , in squad the answers do not come from a small set of candidate answers and they have variable lengths . we propose an end-to-end neural architecture for the task . the architecture is based on match-lstm , a model we proposed previously for textual entailment , and pointer net , a sequence-to-sequence model proposed by vinyals et al . ( 2015 ) to constrain the output tokens to be from the input sequences . we propose two ways of using pointer net for our task . our experiments show that both of our two models substantially outperform the best results obtained by rajpurkar et al . ( 2016 ) using logistic regression and manually crafted features .", "topics": ["natural language processing", "end-to-end principle"]}
{"title": "disentangling aspect and opinion words in target-based sentiment analysis using lifelong learning", "abstract": "given a target name , which can be a product aspect or entity , identifying its aspect words and opinion words in a given corpus is a fine-grained task in target-based sentiment analysis ( tsa ) . this task is challenging , especially when we have no labeled data and we want to perform it for any given domain . to address it , we propose a general two-stage approach . stage one extracts/groups the target-related words ( call t-words ) for a given target . this is relatively easy as we can apply an existing semantics-based learning technique . stage two separates the aspect and opinion words from the grouped t-words , which is challenging because we often do not have enough word-level aspect and opinion labels . in this work , we formulate this problem in a pu learning setting and incorporate the idea of lifelong learning to solve it . experimental results show the effectiveness of our approach .", "topics": ["reinforcement learning"]}
{"title": "what does a textcnn learn ?", "abstract": "textcnn , the convolutional neural network for text , is a useful deep learning algorithm for sentence classification tasks such as sentiment analysis and question classification . however , neural networks have long been known as black boxes because interpreting them is a challenging task . researchers have developed several tools to understand a cnn for image classification by deep visualization , but research about deep textcnns is still insufficient . in this paper , we are trying to understand what a textcnn learns on two classical nlp datasets . our work focuses on functions of different convolutional kernels and correlations between convolutional kernels .", "topics": ["kernel ( operating system )", "natural language processing"]}
{"title": "distinguishing fact from fiction : pattern recognition in texts using complex networks", "abstract": "we establish concrete mathematical criteria to distinguish between different kinds of written storytelling , fictional and non-fictional . specifically , we constructed a semantic network from both novels and news stories , with $ n $ independent words as vertices or nodes , and edges or links allotted to words occurring within $ m $ places of a given vertex ; we call $ m $ the word distance . we then used measures from complex network theory to distinguish between news and fiction , studying the minimal text length needed as well as the optimized word distance $ m $ . the literature samples were found to be most effectively represented by their corresponding power laws over degree distribution $ p ( k ) $ and clustering coefficient $ c ( k ) $ ; we also studied the mean geodesic distance , and found all our texts were small-world networks . we observed a natural break-point at $ k=\\sqrt { n } $ where the power law in the degree distribution changed , leading to separate power law fit for the bulk and the tail of $ p ( k ) $ . our linear discriminant analysis yielded a $ 73.8 \\pm 5.15 % $ accuracy for the correct classification of novels and $ 69.1 \\pm 1.22 % $ for news stories . we found an optimal word distance of $ m=4 $ and a minimum text length of 100 to 200 words $ n $ .", "topics": ["coefficient"]}
{"title": "parameter estimation in computational biology by approximate bayesian computation coupled with sensitivity analysis", "abstract": "we address the problem of parameter estimation in models of systems biology from noisy observations . the models we consider are characterized by simultaneous deterministic nonlinear differential equations whose parameters are either taken from in vitro experiments , or are hand-tuned during the model development process to reproduces observations from the system . we consider the family of algorithms coming under the bayesian formulation of approximate bayesian computation ( abc ) , and show that sensitivity analysis could be deployed to quantify the relative roles of different parameters in the system . parameters to which a system is relatively less sensitive ( known as sloppy parameters ) need not be estimated to high precision , while the values of parameters that are more critical ( stiff parameters ) need to be determined with care . a tradeoff between computational complexity and the accuracy with which the posterior distribution may be probed is an important characteristic of this class of algorithms .", "topics": ["computational complexity theory", "nonlinear system"]}
{"title": "gradient descent for spiking neural networks", "abstract": "much of studies on neural computation are based on network models of static neurons that produce analog output , despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes . research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks . here , we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation . for demonstration , we trained recurrent spiking networks on two dynamic tasks : one that requires optimizing fast ( ~millisecond ) spike-based interactions for efficient encoding of information , and a delayed memory xor task over extended duration ( ~second ) . the results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as behavioral time scales . in conclusion , our result offers a general purpose supervised learning algorithm for spiking neural networks , thus advancing further investigations on spike-based computation .", "topics": ["supervised learning", "neural networks"]}
{"title": "a synthetic approach for recommendation : combining ratings , social relations , and reviews", "abstract": "recommender systems ( rss ) provide an effective way of alleviating the information overload problem by selecting personalized choices . online social networks and user-generated content provide diverse sources for recommendation beyond ratings , which present opportunities as well as challenges for traditional rss . although social matrix factorization ( social mf ) can integrate ratings with social relations and topic matrix factorization can integrate ratings with item reviews , both of them ignore some useful information . in this paper , we investigate the effective data fusion by combining the two approaches , in two steps . first , we extend social mf to exploit the graph structure of neighbors . second , we propose a novel framework mr3 to jointly model these three types of information effectively for rating prediction by aligning latent factors and hidden topics . we achieve more accurate rating prediction on two real-life datasets . furthermore , we measure the contribution of each data source to the proposed framework .", "topics": ["synthetic data"]}
{"title": "a note on active learning for smooth problems", "abstract": "we show that the disagreement coefficient of certain smooth hypothesis classes is $ o ( m ) $ , where $ m $ is the dimension of the hypothesis space , thereby answering a question posed in \\cite { friedman09 } .", "topics": ["coefficient"]}
{"title": "out-of-focus blur : image de-blurring", "abstract": "image de-blurring is important in many cases of imaging a real scene or object by a camera . this project focuses on de-blurring an image distorted by an out-of-focus blur through a simulation study . a pseudo-inverse filter is first explored but it fails because of severe noise amplification . then tikhonov regularization methods are employed , which produce greatly improved results compared to the pseudo-inverse filter . in tikhonov regularization , the choice of the regularization parameter plays a critical rule in obtaining a high-quality image , and the regularized solutions possess a semi-convergence property . the best result , with the relative restoration error of 8.49 % , is achieved when the prescribed discrepancy principle is used to decide an optimal value . furthermore , an iterative method , conjugated gradient , is employed for image de-blurring , which is fast in computation and leads to an even better result with the relative restoration error of 8.22 % . the number of iteration in cg acts as a regularization parameter , and the iterates have a semi-convergence property as well .", "topics": ["matrix regularization", "simulation"]}
{"title": "clad : a complex and long activities dataset with rich crowdsourced annotations", "abstract": "this paper introduces a novel activity dataset which exhibits real-life and diverse scenarios of complex , temporally-extended human activities and actions . the dataset presents a set of videos of actors performing everyday activities in a natural and unscripted manner . the dataset was recorded using a static kinect 2 sensor which is commonly used on many robotic platforms . the dataset comprises of rgb-d images , point cloud data , automatically generated skeleton tracks in addition to crowdsourced annotations . furthermore , we also describe the methodology used to acquire annotations through crowdsourcing . finally some activity recognition benchmarks are presented using current state-of-the-art techniques . we believe that this dataset is particularly suitable as a testbed for activity recognition research but it can also be applicable for other common tasks in robotics/computer vision research such as object detection and human skeleton tracking .", "topics": ["object detection", "computer vision"]}
{"title": "stochastic gradient descent algorithms for strongly convex functions at o ( 1/t ) convergence rates", "abstract": "with a weighting scheme proportional to t , a traditional stochastic gradient descent ( sgd ) algorithm achieves a high probability convergence rate of o ( { \\kappa } /t ) for strongly convex functions , instead of o ( { \\kappa } ln ( t ) /t ) . we also prove that an accelerated sgd algorithm also achieves a rate of o ( { \\kappa } /t ) .", "topics": ["gradient descent", "gradient"]}
{"title": "efficient modeling of latent information in supervised learning using gaussian processes", "abstract": "often in machine learning , data are collected as a combination of multiple conditions , e.g . , the voice recordings of multiple persons , each labeled with an id . how could we build a model that captures the latent information related to these conditions and generalize to a new one with few data ? we present a new model called latent variable multiple output gaussian processes ( lvmogp ) and that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time . lvmogp infers the posteriors of gaussian processes together with a latent space representing the information about different conditions . we derive an efficient variational inference method for lvmogp , of which the computational complexity is as low as sparse gaussian processes . we show that lvmogp significantly outperforms related gaussian process methods on various tasks with both synthetic and real data .", "topics": ["calculus of variations", "supervised learning"]}
{"title": "network flow algorithms for structured sparsity", "abstract": "we consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of $ \\ell_\\infty $ -norms over groups of variables . whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure , we address here the case of general overlapping groups . to this end , we show that the corresponding optimization problem is related to network flow optimization . more precisely , the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem . we propose an efficient procedure which computes its solution exactly in polynomial time . our algorithm scales up to millions of variables , and opens up a whole new range of applications for structured sparse models . we present several experiments on image and video data , demonstrating the applicability and scalability of our approach for various problems .", "topics": ["optimization problem", "computational complexity theory"]}
{"title": "proceedings of the workshop on data mining for oil and gas", "abstract": "the process of exploring and exploiting oil and gas ( o & g ) generates a lot of data that can bring more efficiency to the industry . the opportunities for using data mining techniques in the `` digital oil-field '' remain largely unexplored or uncharted . with the high rate of data expansion , companies are scrambling to develop ways to develop near-real-time predictive analytics , data mining and machine learning capabilities , and are expanding their data storage infrastructure and resources . with these new goals , come the challenges of managing data growth , integrating intelligence tools , and analyzing the data to glean useful insights . oil and gas companies need data solutions to economically extract value from very large volumes of a wide variety of data generated from exploration , well drilling and production devices and sensors . data mining for oil and gas industry throughout the lifecycle of the reservoir includes the following roles : locating hydrocarbons , managing geological data , drilling and formation evaluation , well construction , well completion , and optimizing production through the life of the oil field . for each of these phases during the lifecycle of oil field , data mining play a significant role . based on which phase were talking about , knowledge creation through scientific models , data analytics and machine learning , a effective , productive , and on demand data insight is critical for decision making within the organization . the significant challenges posed by this complex and economically vital field justify a meeting of data scientists that are willing to share their experience and knowledge . thus , the worskhop on data mining for oil and gas ( dm4og ) aims to provide a quality forum for researchers that work on the significant challenges arising from the synergy between data science , machine learning , and the modeling and optimization problems in the o & g industry .", "topics": ["data mining", "sensor"]}
{"title": "sequential document representations and simplicial curves", "abstract": "the popular bag of words assumption represents a document as a histogram of word occurrences . while computationally efficient , such a representation is unable to maintain any sequential information . we present a continuous and differentiable sequential document representation that goes beyond the bag of words assumption , and yet is efficient and effective . this representation employs smooth curves in the multinomial simplex to account for sequential information . we discuss the representation and its geometric properties and demonstrate its applicability for the task of text classification .", "topics": ["computational complexity theory"]}
{"title": "towards learning object affordance priors from technical texts", "abstract": "everyday activities performed by artificial assistants can potentially be executed naively and dangerously given their lack of common sense knowledge . this paper presents conceptual work towards obtaining prior knowledge on the usual modality ( passive or active ) of any given entity , and their affordance estimates , by extracting high-confidence ability modality semantic relations ( x can y relationship ) from non-figurative texts , by analyzing co-occurrence of grammatical instances of subjects and verbs , and verbs and objects . the discussion includes an outline of the concept , potential and limitations , and possible feature and learning framework adoption .", "topics": ["artificial intelligence"]}
{"title": "probable convexity and its application to correlated topic models", "abstract": "non-convex optimization problems often arise from probabilistic modeling , such as estimation of posterior distributions . non-convexity makes the problems intractable , and poses various obstacles for us to design efficient algorithms . in this work , we attack non-convexity by first introducing the concept of \\emph { probable convexity } for analyzing convexity of real functions in practice . we then use the new concept to analyze an inference problem in the \\emph { correlated topic model } ( ctm ) and related nonconjugate models . contrary to the existing belief of intractability , we show that this inference problem is concave under certain conditions . one consequence of our analyses is a novel algorithm for learning ctm which is significantly more scalable and qualitative than existing methods . finally , we highlight that stochastic gradient algorithms might be a practical choice to resolve efficiently non-convex problems . this finding might find beneficial in many contexts which are beyond probabilistic modeling .", "topics": ["scalability", "gradient"]}
{"title": "semi-supervised qa with generative domain-adaptive nets", "abstract": "we study the problem of semi-supervised question answering -- -- utilizing unlabeled text to boost the performance of question answering models . we propose a novel training framework , the generative domain-adaptive nets . in this framework , we train a generative model to generate questions based on the unlabeled text , and combine model-generated questions with human-generated questions for training question answering models . we develop novel domain adaptation algorithms , based on reinforcement learning , to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution . experiments show that our proposed framework obtains substantial improvement from unlabeled text .", "topics": ["generative model", "reinforcement learning"]}
{"title": "a simple non-parametric topic mixture for authors and documents", "abstract": "this article reviews the author-topic model and presents a new non-parametric extension based on the hierarchical dirichlet process . the extension is especially suitable when no prior information about the number of components necessary is available . a blocked gibbs sampler is described and focus put on staying as close as possible to the original model with only the minimum of theoretical and implementation overhead necessary .", "topics": ["sampling ( signal processing )"]}
{"title": "compression , restoration , re-sampling , compressive sensing : fast transforms in digital imaging", "abstract": "transform image processing methods are methods that work in domains of image transforms , such as discrete fourier , discrete cosine , wavelet and alike . they are the basic tool in image compression , in image restoration , in image re-sampling and geometrical transformations and can be traced back to early 1970-ths . the paper presents a review of these methods with emphasis on their comparison and relationships , from the very first steps of transform image compression methods to adaptive and local adaptive transform domain filters for image restoration , to methods of precise image re-sampling and image reconstruction from sparse samples and up to `` compressive sensing '' approach that has gained popularity in last few years . the review has a tutorial character and purpose .", "topics": ["image processing", "sparse matrix"]}
{"title": "multiscale sequence modeling with a learned dictionary", "abstract": "we propose a generalization of neural network sequence models . instead of predicting one symbol at a time , our multi-scale model makes predictions over multiple , potentially overlapping multi-symbol tokens . a variation of the byte-pair encoding ( bpe ) compression algorithm is used to learn the dictionary of tokens that the model is trained with . when applied to language modelling , our model has the flexibility of character-level models while maintaining many of the performance benefits of word-level models . our experiments show that this model performs better than a regular lstm on language modeling tasks , especially for smaller models .", "topics": ["dictionary"]}
{"title": "top-down tree long short-term memory networks", "abstract": "long short-term memory ( lstm ) networks , a type of recurrent neural network with a more complex computational unit , have been successfully applied to a variety of sequence modeling tasks . in this paper we develop tree long short-term memory ( treelstm ) , a neural network model based on lstm , which is designed to predict a tree rather than a linear sequence . treelstm defines the probability of a sentence by estimating the generation probability of its dependency tree . at each time step , a node is generated based on the representation of the generated sub-tree . we further enhance the modeling power of treelstm by explicitly representing the correlations between left and right dependents . application of our model to the msr sentence completion challenge achieves results beyond the current state of the art . we also report results on dependency parsing reranking achieving competitive performance .", "topics": ["recurrent neural network"]}
{"title": "safe feature elimination in sparse supervised learning", "abstract": "we investigate fast methods that allow to quickly eliminate variables ( features ) in supervised learning problems involving a convex loss function and a $ l_1 $ -norm penalty , leading to a potentially substantial reduction in the number of variables prior to running the supervised learning algorithm . the methods are not heuristic : they only eliminate features that are { \\em guaranteed } to be absent after solving the learning problem . our framework applies to a large class of problems , including support vector machine classification , logistic regression and least-squares . the complexity of the feature elimination step is negligible compared to the typical computational effort involved in the sparse supervised learning problem : it grows linearly with the number of features times the number of examples , with much better count if data is sparse . we apply our method to data sets arising in text classification and observe a dramatic reduction of the dimensionality , hence in computational effort required to solve the learning problem , especially when very sparse classifiers are sought . our method allows to immediately extend the scope of existing algorithms , allowing us to run them on data sets of sizes that were out of their reach before .", "topics": ["supervised learning", "statistical classification"]}
{"title": "generalizing k-means for an arbitrary distance matrix", "abstract": "the original k-means clustering method works only if the exact vectors representing the data points are known . therefore calculating the distances from the centroids needs vector operations , since the average of abstract data points is undefined . existing algorithms can be extended for those cases when the sole input is the distance matrix , and the exact representing vectors are unknown . this extension may be named relational k-means after a notation for a similar algorithm invented for fuzzy clustering . a method is then proposed for generalizing k-means for scenarios when the data points have absolutely no connection with a euclidean space .", "topics": ["cluster analysis"]}
{"title": "formal ontology learning on factual is-a corpus in english using description logics", "abstract": "ontology learning ( ol ) is the computational task of generating a knowledge base in the form of an ontology given an unstructured corpus whose content is in natural language ( nl ) . several works can be found in this area most of which are limited to statistical and lexico-syntactic pattern matching based techniques light-weight ol . these techniques do not lead to very accurate learning mostly because of several linguistic nuances in nl . formal ol is an alternative ( less explored ) methodology were deep linguistics analysis is made using theory and tools found in computational linguistics to generate formal axioms and definitions instead simply inducing a taxonomy . in this paper we propose `` description logic ( dl ) '' based formal ol framework for learning factual is-a type sentences in english . we claim that semantic construction of is-a sentences is non trivial . hence , we also claim that such sentences requires special studies in the context of ol before any truly formal ol can be proposed . we introduce a learner tool , called dlol_is-a , that generated such ontologies in the owl format . we have adopted `` gold standard '' based ol evaluation on is-a rich wcl v.1.1 dataset and our own community representative is-a dataset . we observed significant improvement of dlol_is-a when compared to the light-weight ol tool text2onto and formal ol tool fred .", "topics": ["natural language"]}
{"title": "uncertainty estimates for efficient neural network-based dialogue policy optimisation", "abstract": "in statistical dialogue management , the dialogue manager learns a policy that maps a belief state to an action for the system to perform . efficient exploration is key to successful policy optimisation . current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration , thus subjecting the user to a random choice of action during learning . alternative approaches such as gaussian process sarsa ( gpsarsa ) estimate uncertainties and are sample efficient , leading to better user experience , but on the expense of a greater computational complexity . this paper examines approaches to extract uncertainty estimates from deep q-networks ( dqn ) in the context of dialogue management . we perform an extensive benchmark of deep bayesian methods to extract uncertainty estimates , namely bayes-by-backprop , dropout , its concrete variation , bootstrapped ensemble and alpha-divergences , combining it with dqn algorithm .", "topics": ["mathematical optimization", "computational complexity theory"]}
{"title": "coherence pursuit : fast , simple , and robust principal component analysis", "abstract": "this paper presents a remarkably simple , yet powerful , algorithm termed coherence pursuit ( cop ) to robust principal component analysis ( pca ) . as inliers lie in a low dimensional subspace and are mostly correlated , an inlier is likely to have strong mutual coherence with a large number of data points . by contrast , outliers either do not admit low dimensional structures or form small clusters . in either case , an outlier is unlikely to bear strong resemblance to a large number of data points . given that , cop sets an outlier apart from an inlier by comparing their coherence with the rest of the data points . the mutual coherences are computed by forming the gram matrix of the normalized data points . subsequently , the sought subspace is recovered from the span of the subset of the data points that exhibit strong coherence with the rest of the data . as cop only involves one simple matrix multiplication , it is significantly faster than the state-of-the-art robust pca algorithms . we derive analytical performance guarantees for cop under different models for the distributions of inliers and outliers in both noise-free and noisy settings . cop is the first robust pca algorithm that is simultaneously non-iterative , provably robust to both unstructured and structured outliers , and can tolerate a large number of unstructured outliers .", "topics": ["cluster analysis"]}
{"title": "object classification by means of multi-feature concept learning in a multi expert-agent system", "abstract": "classification of some objects in classes of concepts is an essential and even breathtaking task in many applications . a solution is discussed here based on multi-agent systems . a kernel of some expert agents in several classes is to consult a central agent decide among the classification problem of a certain object . this kernel is moderated with the center agent , trying to manage the querying agents for any decision problem by means of a data-header like feature set . agents have cooperation among concepts related to the classes of this classification decision-making ; and may affect on each others ' results on a certain query object in a multi-agent learning approach . this leads to an online feature learning via the consulting trend . the performance is discussed to be much better in comparison to some other prior trends while system 's message passing overload is decreased to less agents and the expertism helps the performance and operability of system win the comparison .", "topics": ["kernel ( operating system )", "feature learning"]}
{"title": "the computational power of dynamic bayesian networks", "abstract": "this paper considers the computational power of constant size , dynamic bayesian networks . although discrete dynamic bayesian networks are no more powerful than hidden markov models , dynamic bayesian networks with continuous random variables and discrete children of continuous parents are capable of performing turing-complete computation . with modified versions of existing algorithms for belief propagation , such a simulation can be carried out in real time . this result suggests that dynamic bayesian networks may be more powerful than previously considered . relationships to causal models and recurrent neural networks are also discussed .", "topics": ["recurrent neural network", "bayesian network"]}
{"title": "on the high-dimensional power of linear-time kernel two-sample testing under mean-difference alternatives", "abstract": "nonparametric two sample testing deals with the question of consistently deciding if two distributions are different , given samples from both , without making any parametric assumptions about the form of the distributions . the current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ ( \\textit { general } alternatives ) , and those which are designed to specifically test easier alternatives , like a difference in means ( \\textit { mean-shift } alternatives ) . the main contribution of this paper is to explicitly characterize the power of a popular nonparametric two sample test , designed for general alternatives , under a mean-shift alternative in the high-dimensional setting . specifically , we explicitly derive the power of the linear-time maximum mean discrepancy statistic using the gaussian kernel , where the dimension and sample size can both tend to infinity at any rate , and the two distributions differ in their means . as a corollary , we find that if the signal-to-noise ratio is held constant , then the test 's power goes to one if the number of samples increases faster than the dimension increases . this is the first explicit power derivation for a general nonparametric test in the high-dimensional setting , and also the first analysis of how tests designed for general alternatives perform when faced with easier ones .", "topics": ["kernel ( operating system )", "time complexity"]}
{"title": "realizing an optimization approach inspired from piagets theory on cognitive development", "abstract": "the objective of this paper is to introduce an artificial intelligence based optimization approach , which is inspired from piagets theory on cognitive development . the approach has been designed according to essential processes that an individual may experience while learning something new or improving his / her knowledge . these processes are associated with the piagets ideas on an individuals cognitive development . the approach expressed in this paper is a simple algorithm employing swarm intelligence oriented tasks in order to overcome single-objective optimization problems . for evaluating effectiveness of this early version of the algorithm , test operations have been done via some benchmark functions . the obtained results show that the approach / algorithm can be an alternative to the literature in terms of single-objective optimization . the authors have suggested the name : cognitive development optimization algorithm ( codoa ) for the related intelligent optimization approach .", "topics": ["mathematical optimization", "artificial intelligence"]}
{"title": "big data regression using tree based segmentation", "abstract": "scaling regression to large datasets is a common problem in many application areas . we propose a two step approach to scaling regression to large datasets . using a regression tree ( cart ) to segment the large dataset constitutes the first step of this approach . the second step of this approach is to develop a suitable regression model for each segment . since segment sizes are not very large , we have the ability to apply sophisticated regression techniques if required . a nice feature of this two step approach is that it can yield models that have good explanatory power as well as good predictive performance . ensemble methods like gradient boosted trees can offer excellent predictive performance but may not provide interpretable models . in the experiments reported in this study , we found that the predictive performance of the proposed approach matched the predictive performance of gradient boosted trees .", "topics": ["gradient"]}
{"title": "computing p-values of lingam outputs via multiscale bootstrap", "abstract": "structural equation models and bayesian networks have been widely used to study causal relationships between continuous variables . recently , a non-gaussian method called lingam was proposed to discover such causal models and has been extended in various directions . an important problem with lingam is that the results are affected by the random sampling of the data as with any statistical method . thus , some analysis of the statistical reliability or confidence level should be conducted . a common method to evaluate a confidence level is a bootstrap method . however , a confidence level computed by ordinary bootstrap method is known to be biased as a probability-value ( $ p $ -value ) of hypothesis testing . in this paper , we propose a new procedure to apply an advanced bootstrap method called multiscale bootstrap to compute confidence levels , i.e . , p-values , of lingam outputs . the multiscale bootstrap method gives unbiased $ p $ -values with asymptotic much higher accuracy . experiments on artificial data demonstrate the utility of our approach .", "topics": ["sampling ( signal processing )", "bayesian network"]}
{"title": "optimal number of choices in rating contexts", "abstract": "in many settings people must give numerical scores to entities from a small discrete set . for instance , rating physical attractiveness from 1 -- 5 on dating sites , or papers from 1 -- 10 for conference reviewing . we study the problem of understanding when using a different number of options is optimal . for concreteness we assume the true underlying scores are integers from 1 -- 100 . we consider the case when scores are uniform random and gaussian . we study when using 2 , 3 , 4 , 5 , and 10 options is optimal in these models . one may expect that using more options would always improve performance in this model , but we show that this is not necessarily the case , and that using fewer choices -- -even just two -- -can surprisingly be optimal in certain situations . while in theory for this setting it would be optimal to use all 100 options , in practice this is prohibitive , and it is preferable to utilize a smaller number of options due to humans ' limited computational resources . our results suggest that using a smaller number of options than is typical could be optimal in certain situations . this would have many potential applications , as settings requiring entities to be ranked by humans are ubiquitous .", "topics": ["numerical analysis", "entity"]}
{"title": "toward interpretable topic discovery via anchored correlation explanation", "abstract": "many predictive tasks , such as diagnosing a patient based on their medical chart , are ultimately defined by the decisions of human experts . unfortunately , encoding experts ' knowledge is often time consuming and expensive . we propose a simple way to use fuzzy and informal knowledge from experts to guide discovery of interpretable latent topics in text . the underlying intuition of our approach is that latent factors should be informative about both correlations in the data and a set of relevance variables specified by an expert . mathematically , this approach is a combination of the information bottleneck and total correlation explanation ( corex ) . we give a preliminary evaluation of anchored corex , showing that it produces more coherent and interpretable topics on two distinct corpora .", "topics": ["relevance", "text corpus"]}
{"title": "learning topology and dynamics of large recurrent neural networks", "abstract": "large-scale recurrent networks have drawn increasing attention recently because of their capabilities in modeling a large variety of real-world phenomena and physical mechanisms . this paper studies how to identify all authentic connections and estimate system parameters of a recurrent network , given a sequence of node observations . this task becomes extremely challenging in modern network applications , because the available observations are usually very noisy and limited , and the associated dynamical system is strongly nonlinear . by formulating the problem as multivariate sparse sigmoidal regression , we develop simple-to-implement network learning algorithms , with rigorous convergence guarantee in theory , for a variety of sparsity-promoting penalty forms . a quantile variant of progressive recurrent network screening is proposed for efficient computation and allows for direct cardinality control of network topology in estimation . moreover , we investigate recurrent network stability conditions in lyapunov 's sense , and integrate such stability constraints into sparse network learning . experiments show excellent performance of the proposed algorithms in network topology identification and forecasting .", "topics": ["recurrent neural network", "nonlinear system"]}
{"title": "a modified physarum-inspired model for the user equilibrium traffic assignment problem", "abstract": "the user equilibrium traffic assignment principle is very important in the traffic assignment problem . mathematical programming models are designed to solve the user equilibrium problem in traditional algorithms . recently , the physarum shows the ability to address the user equilibrium and system optimization traffic assignment problems . however , the physarum model are not efficient in real traffic networks with two-way traffic characteristics and multiple origin-destination pairs . in this article , a modified physarum-inspired model for the user equilibrium problem is proposed . by decomposing traffic flux based on origin nodes , the traffic flux from different origin-destination pairs can be distinguished in the proposed model . the physarum can obtain the equilibrium traffic flux when no shorter path can be discovered between each origin-destination pair . finally , numerical examples use the sioux falls network to demonstrate the rationality and convergence properties of the proposed model .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "evolution and the structure of learning agents", "abstract": "this paper presents the thesis that all learning agents of finite information size are limited by their informational structure in what goals they can efficiently learn to achieve in a complex environment . evolutionary change is critical for creating the required structure for all learning agents in any complex environment . the thesis implies that there is no efficient universal learning algorithm . an agent can go past the learning limits imposed by its structure only by slow evolutionary change or blind search which in a very complex environment can only give an agent an inefficient universal learning capability that can work only in evolutionary timescales or improbable luck .", "topics": ["reinforcement learning", "computation"]}
{"title": "scalable meta-learning for bayesian optimization", "abstract": "bayesian optimization has become a standard technique for hyperparameter optimization , including data-intensive models such as deep neural networks that may take days or weeks to train . we consider the setting where previous optimization runs are available , and we wish to use their results to warm-start a new optimization run . we develop an ensemble model that can incorporate the results of past optimization runs , while avoiding the poor scaling that comes with putting all results into a single gaussian process model . the ensemble combines models from past runs according to estimates of their generalization performance on the current optimization . results from a large collection of hyperparameter optimization benchmark problems and from optimization of a production computer vision platform at facebook show that the ensemble can substantially reduce the time it takes to obtain near-optimal configurations , and is useful for warm-starting expensive searches or running quick re-optimizations .", "topics": ["mathematical optimization", "computer vision"]}
{"title": "a model of sensory neural responses in the presence of unknown modulatory inputs", "abstract": "neural responses are highly variable , and some portion of this variability arises from fluctuations in modulatory factors that alter their gain , such as adaptation , attention , arousal , expected or actual reward , emotion , and local metabolic resource availability . regardless of their origin , fluctuations in these signals can confound or bias the inferences that one derives from spiking responses . recent work demonstrates that for sensory neurons , these effects can be captured by a modulated poisson model , whose rate is the product of a stimulus-driven response function and an unknown modulatory signal . here , we extend this model , by incorporating explicit modulatory elements that are known ( specifically , spike-history dependence , as in previous models ) , and by constraining the remaining latent modulatory signals to be smooth in time . we develop inference procedures for fitting the entire model , including hyperparameters , via evidence optimization , and apply these to simulated data , and to responses of ferret auditory midbrain and cortical neurons to complex sounds . we show that integrating out the latent modulators yields better ( or more readily-interpretable ) receptive field estimates than a standard poisson model . conversely , integrating out the stimulus dependence yields estimates of the slowly-varying latent modulators .", "topics": ["simulation"]}
{"title": "seven : deep semi-supervised verification networks", "abstract": "verification determines whether two samples belong to the same class or not , and has important applications such as face and fingerprint verification , where thousands or millions of categories are present but each category has scarce labeled examples , presenting two major challenges for existing deep learning models . we propose a deep semi-supervised model named semi-supervised verification network ( seven ) to address these challenges . the model consists of two complementary components . the generative component addresses the lack of supervision within each category by learning general salient structures from a large amount of data across categories . the discriminative component exploits the learned general features to mitigate the lack of supervision within categories , and also directs the generative component to find more informative structures of the whole data manifold . the two components are tied together in seven to allow an end-to-end training of the two components . extensive experiments on four verification tasks demonstrate that seven significantly outperforms other state-of-the-art deep semi-supervised techniques when labeled data are in short supply . furthermore , seven is competitive with fully supervised baselines trained with a larger amount of labeled data . it indicates the importance of the generative component in seven .", "topics": ["baseline ( configuration management )", "end-to-end principle"]}
{"title": "on extending neural networks with loss ensembles for text classification", "abstract": "ensemble techniques are powerful approaches that combine several weak learners to build a stronger one . as a meta learning framework , ensemble techniques can easily be applied to many machine learning techniques . in this paper we propose a neural network extended with an ensemble loss function for text classification . the weight of each weak loss function is tuned within the training phase through the gradient propagation optimization method of the neural network . the approach is evaluated on several text classification datasets . we also evaluate its performance in various environments with several degrees of label noise . experimental results indicate an improvement of the results and strong resilience against label noise in comparison with other methods .", "topics": ["neural networks", "loss function"]}
{"title": "group-sparse signal denoising : non-convex regularization , convex optimization", "abstract": "convex optimization with sparsity-promoting convex regularization is a standard approach for estimating sparse signals in noise . in order to promote sparsity more strongly than convex regularization , it is also standard practice to employ non-convex optimization . in this paper , we take a third approach . we utilize a non-convex regularization term chosen such that the total cost function ( consisting of data consistency and regularization terms ) is convex . therefore , sparsity is more strongly promoted than in the standard convex formulation , but without sacrificing the attractive aspects of convex optimization ( unique minimum , robust algorithms , etc . ) . we use this idea to improve the recently developed 'overlapping group shrinkage ' ( ogs ) algorithm for the denoising of group-sparse signals . the algorithm is applied to the problem of speech enhancement with favorable results in terms of both snr and perceptual quality .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "provable self-representation based outlier detection in a union of subspaces", "abstract": "many computer vision tasks involve processing large amounts of data contaminated by outliers , which need to be detected and rejected . while outlier detection methods based on robust statistics have existed for decades , only recently have methods based on sparse and low-rank representation been developed along with guarantees of correct outlier detection when the inliers lie in one or more low-dimensional subspaces . this paper proposes a new outlier detection method that combines tools from sparse representation with random walks on a graph . by exploiting the property that data points can be expressed as sparse linear combinations of each other , we obtain an asymmetric affinity matrix among data points , which we use to construct a weighted directed graph . by defining a suitable markov chain from this graph , we establish a connection between inliers/outliers and essential/inessential states of the markov chain , which allows us to detect outliers by using random walks . we provide a theoretical analysis that justifies the correctness of our method under geometric and connectivity assumptions . experimental results on image databases demonstrate its superiority with respect to state-of-the-art sparse and low-rank outlier detection methods .", "topics": ["computer vision", "sparse matrix"]}
{"title": "how do neurons operate on sparse distributed representations ? a mathematical theory of sparsity , neurons and active dendrites", "abstract": "we propose a formal mathematical model for sparse representations and active dendrites in neocortex . our model is inspired by recent experimental findings on active dendritic processing and nmda spikes in pyramidal neurons . these experimental and modeling studies suggest that the basic unit of pattern memory in the neocortex is instantiated by small clusters of synapses operated on by localized non-linear dendritic processes . we derive a number of scaling laws that characterize the accuracy of such dendrites in detecting activation patterns in a neuronal population under adverse conditions . we introduce the union property which shows that synapses for multiple patterns can be randomly mixed together within a segment and still lead to highly accurate recognition . we describe simulation results that provide further insight into sparse representations as well as two primary results . first we show that pattern recognition by a neuron with active dendrites can be extremely accurate and robust with high dimensional sparse inputs even when using a tiny number of synapses to recognize large patterns . second , equations representing recognition accuracy of a dendrite predict optimal nmda spiking thresholds under a generous set of assumptions . the prediction tightly matches nmda spiking thresholds measured in the literature . our model matches many of the known properties of pyramidal neurons . as such the theory provides a mathematical framework for understanding the benefits and limits of sparse representations in cortical networks .", "topics": ["nonlinear system", "simulation"]}
{"title": "investigating gated recurrent neural networks for speech synthesis", "abstract": "recently , recurrent neural networks ( rnns ) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis ( spss ) . the long short-term memory ( lstm ) architecture is particularly attractive because it addresses the vanishing gradient problem in standard rnns , making them easier to train . although recent studies have demonstrated that lstms can achieve significantly better performance on spss than deep feed-forward neural networks , little is known about why . here we attempt to answer two questions : a ) why do lstms work well as a sequence model for spss ; b ) which component ( e.g . , input gate , output gate , forget gate ) is most important . we present a visual analysis alongside a series of experiments , resulting in a proposal for a simplified architecture . the simplified architecture has significantly fewer parameters than an lstm , thus reducing generation complexity considerably without degrading quality .", "topics": ["recurrent neural network", "gradient"]}
{"title": "discovering structure in high-dimensional data through correlation explanation", "abstract": "we introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective . intuitively , the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information . the method is unsupervised , requires no model assumptions , and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems . we demonstrate that correlation explanation ( corex ) automatically discovers meaningful structure for data from diverse sources including personality tests , dna , and human language .", "topics": ["unsupervised learning"]}
{"title": "applying deep belief networks to word sense disambiguation", "abstract": "in this paper , we applied a novel learning algorithm , namely , deep belief networks ( dbn ) to word sense disambiguation ( wsd ) . dbn is a probabilistic generative model composed of multiple layers of hidden units . dbn uses restricted boltzmann machine ( rbm ) to greedily train layer by layer as a pretraining . then , a separate fine tuning step is employed to improve the discriminative power . we compared dbn with various state-of-the-art supervised learning algorithms in wsd such as support vector machine ( svm ) , maximum entropy model ( maxent ) , naive bayes classifier ( nb ) and kernel principal component analysis ( kpca ) . we used all words in the given paragraph , surrounding context words and part-of-speech of surrounding words as our knowledge sources . we conducted our experiment on the senseval-2 data set . we observed that dbn outperformed all other learning algorithms .", "topics": ["generative model", "supervised learning"]}
{"title": "evolving classifiers : methods for incremental learning", "abstract": "the ability of a classifier to take on new information and classes by evolving the classifier without it having to be fully retrained is known as incremental learning . incremental learning has been successfully applied to many classification problems , where the data is changing and is not all available at once . in this paper there is a comparison between learn++ , which is one of the most recent incremental learning algorithms , and the new proposed method of incremental learning using genetic algorithm ( iluga ) . learn++ has shown good incremental learning capabilities on benchmark datasets on which the new iluga method has been tested . iluga has also shown good incremental learning ability using only a few classifiers and does not suffer from catastrophic forgetting . the results obtained for iluga on the optical character recognition ( ocr ) and wine datasets are good , with an overall accuracy of 93 % and 94 % respectively showing a 4 % improvement over learn++.mt for the difficult multi-class ocr dataset .", "topics": ["reinforcement learning"]}
{"title": "spoken english intelligibility remediation with pocketsphinx alignment and feature extraction improves substantially over the state of the art", "abstract": "we use automatic speech recognition to assess spoken english learner pronunciation based on the authentic intelligibility of the learners ' spoken responses determined from support vector machine ( svm ) classifier or deep learning neural network model predictions of transcription correctness . using numeric features produced by pocketsphinx alignment mode and many recognition passes searching for the substitution and deletion of each expected phoneme and insertion of unexpected phonemes in sequence , the svm models achieve 82 percent agreement with the accuracy of amazon mechanical turk crowdworker transcriptions , up from 75 percent reported by multiple independent researchers . using such features with svm classifier probability prediction models can help computer-aided pronunciation teaching ( capt ) systems provide intelligibility remediation .", "topics": ["feature extraction", "speech recognition"]}
{"title": "adaptation and learning over networks for nonlinear system modeling", "abstract": "in this chapter , we analyze nonlinear filtering problems in distributed environments , e.g . , sensor networks or peer-to-peer protocols . in these scenarios , the agents in the environment receive measurements in a streaming fashion , and they are required to estimate a common ( nonlinear ) model by alternating local computations and communications with their neighbors . we focus on the important distinction between single-task problems , where the underlying model is common to all agents , and multitask problems , where each agent might converge to a different model due to , e.g . , spatial dependencies or other factors . currently , most of the literature on distributed learning in the nonlinear case has focused on the single-task case , which may be a strong limitation in real-world scenarios . after introducing the problem and reviewing the existing approaches , we describe a simple kernel-based algorithm tailored for the multitask case . we evaluate the proposal on a simulated benchmark task , and we conclude by detailing currently open problems and lines of research .", "topics": ["nonlinear system", "reinforcement learning"]}
{"title": "decoupling learning rules from representations", "abstract": "in the artificial intelligence field , learning often corresponds to changing the parameters of a parameterized function . a learning rule is an algorithm or mathematical expression that specifies precisely how the parameters should be changed . when creating an artificial intelligence system , we must make two decisions : what representation should be used ( i.e . , what parameterized function should be used ) and what learning rule should be used to search through the resulting set of representable functions . using most learning rules , these two decisions are coupled in a subtle ( and often unintentional ) way . that is , using the same learning rule with two different representations that can represent the same sets of functions can result in two different outcomes . after arguing that this coupling is undesirable , particularly when using artificial neural networks , we present a method for partially decoupling these two decisions for a broad class of learning rules that span unsupervised learning , reinforcement learning , and supervised learning .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "hajj and umrah event recognition datasets", "abstract": "in this note , new hajj and umrah event recognition datasets ( huer ) are presented . the demonstrated datasets are based on videos and images taken during 2011-2012 hajj and umrah seasons . huer is the first collection of datasets covering the six types of hajj and umrah ritual events ( rotating in tawaf around kabaa , performing sa'y between safa and marwa , standing on the mount of arafat , staying overnight in muzdalifah , staying two or three days in mina , and throwing jamarat ) . the huer datasets also contain video and image databases for nine types of human actions during hajj and umrah ( walking , drinking from zamzam water , sleeping , smiling , eating , praying , sitting , shaving hairs and ablutions , reading the holy quran and making duaa ) . the spatial resolutions are 1280 x 720 pixels for images and 640 x 480 pixels for videos and have lengths of 20 seconds in average with 30 frame per second rates .", "topics": ["database", "pixel"]}
{"title": "identifying 3 moss species by deep learning , using the `` chopped picture '' method", "abstract": "in general , object identification tends not to work well on ambiguous , amorphous objects such as vegetation . in this study , we developed a simple but effective approach to identify ambiguous objects and applied the method to several moss species . as a result , the model correctly classified test images with accuracy more than 90 % . using this approach will help progress in computer vision studies .", "topics": ["computer vision"]}
{"title": "neural language correction with character-based attention", "abstract": "natural language correction has the potential to help language learners improve their writing skills . while approaches with separate classifiers for different error types have high precision , they do not flexibly handle errors such as redundancy or non-idiomatic phrasing . on the other hand , word and phrase-based machine translation methods are not designed to cope with orthographic errors , and have recently been outpaced by neural models . motivated by these issues , we present a neural network-based approach to language correction . the core component of our method is an encoder-decoder recurrent neural network with an attention mechanism . by operating at the character level , the network avoids the problem of out-of-vocabulary words . we illustrate the flexibility of our approach on dataset of noisy , user-generated text collected from an english learner forum . when combined with a language model , our method achieves a state-of-the-art $ f_ { 0.5 } $ -score on the conll 2014 shared task . we further demonstrate that training the network on additional data with synthesized errors can improve performance .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "marmara turkish coreference corpus and coreference resolution baseline", "abstract": "we describe the marmara turkish coreference corpus , which is an annotation of the whole metu-sabanci turkish treebank with mentions and coreference chains . collecting nine or more independent annotations for each document allowed for fully automatic adjudication . we provide a baseline system for turkish mention detection and coreference resolution and evaluate it on the corpus .", "topics": ["baseline ( configuration management )"]}
{"title": "global optimization for low-dimensional switching linear regression and bounded-error estimation", "abstract": "the paper provides global optimization algorithms for two particularly difficult nonconvex problems raised by hybrid system identification : switching linear regression and bounded-error estimation . while most works focus on local optimization heuristics without global optimality guarantees or with guarantees valid only under restrictive conditions , the proposed approach always yields a solution with a certificate of global optimality . this approach relies on a branch-and-bound strategy for which we devise lower bounds that can be efficiently computed . in order to obtain scalable algorithms with respect to the number of data , we directly optimize the model parameters in a continuous optimization setting without involving integer variables . numerical experiments show that the proposed algorithms offer a higher accuracy than convex relaxations with a reasonable computational burden for hybrid system identification . in addition , we discuss how bounded-error estimation is related to robust estimation in the presence of outliers and exact recovery under sparse noise , for which we also obtain promising numerical results .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "deep transfer in reinforcement learning by language grounding", "abstract": "in this paper , we explore the utilization of natural language to drive transfer for reinforcement learning ( rl ) . despite the wide-spread application of deep rl techniques , learning generalized policy representations that work across domains remains a challenging problem . we demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer . we employ a model-based rl approach consisting of a differentiable planning module , a model-free component and a factorized representation to effectively utilize entity descriptions . our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments .", "topics": ["reinforcement learning", "natural language"]}
{"title": "learning loss for knowledge distillation with conditional adversarial networks", "abstract": "there is an increasing interest on accelerating neural networks for real-time applications . we study the student-teacher strategy , in which a small and fast student network is trained with the auxiliary information provided by a large and accurate teacher network . we use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student . the proposed method is particularly effective for relatively small student networks . moreover , experimental results show the effect of network size when the modern networks are used as student . we empirically study trade-off between inference time and classification accuracy , and provide suggestions on choosing a proper student .", "topics": ["loss function"]}
{"title": "parallel and distributed block-coordinate frank-wolfe algorithms", "abstract": "we develop parallel and distributed frank-wolfe algorithms ; the former on shared memory machines with mini-batching , and the latter in a delayed update framework . whenever possible , we perform computations asynchronously , which helps attain speedups on multicore machines as well as in distributed environments . moreover , instead of worst-case bounded delays , our methods only depend ( mildly ) on \\emph { expected } delays , allowing them to be robust to stragglers and faulty worker threads . our algorithms assume block-separable constraints , and subsume the recent block-coordinate frank-wolfe ( bcfw ) method~\\citep { lacoste2013block } . our analysis reveals problem-dependent quantities that govern the speedups of our methods over bcfw . we present experiments on structural svm and group fused lasso , obtaining significant speedups over competing state-of-the-art ( and synchronous ) methods .", "topics": ["approximation algorithm"]}
{"title": "first-order modeling and stability analysis of illusory contours", "abstract": "in visual cognition , illusions help elucidate certain intriguing latent perceptual functions of the human vision system , and their proper mathematical modeling and computational simulation are therefore deeply beneficial to both biological and computer vision . inspired by existent prior works , the current paper proposes a first-order energy-based model for analyzing and simulating illusory contours . the lower complexity of the proposed model facilitates rigorous mathematical analysis on the detailed geometric structures of illusory contours . after being asymptotically approximated by classical active contours , the proposed model is then robustly computed using the celebrated level-set method of osher and sethian ( j. comput . phys . , 79:12-49 , 1988 ) with a natural supervising scheme . potential cognitive implications of the mathematical results are addressed , and generic computational examples are demonstrated and discussed .", "topics": ["computer vision", "simulation"]}
{"title": "community detection and classification in hierarchical stochastic blockmodels", "abstract": "we propose a robust , scalable , integrated methodology for community detection and community comparison in graphs . in our procedure , we first embed a graph into an appropriate euclidean space to obtain a low-dimensional representation , and then cluster the vertices into communities . we next employ nonparametric graph inference techniques to identify structural similarity among these communities . these two steps are then applied recursively on the communities , allowing us to detect more fine-grained structure . we describe a hierarchical stochastic blockmodel -- -namely , a stochastic blockmodel with a natural hierarchical structure -- -and establish conditions under which our algorithm yields consistent estimates of model parameters and motifs , which we define to be stochastically similar groups of subgraphs . finally , we demonstrate the effectiveness of our algorithm in both simulated and real data . specifically , we address the problem of locating similar subcommunities in a partially reconstructed drosophila connectome and in the social network friendster .", "topics": ["simulation", "scalability"]}
{"title": "a unified approach for modeling and recognition of individual actions and group activities", "abstract": "recognizing group activities is challenging due to the difficulties in isolating individual entities , finding the respective roles played by the individuals and representing the complex interactions among the participants . individual actions and group activities in videos can be represented in a common framework as they share the following common feature : both are composed of a set of low-level features describing motions , e.g . , optical flow for each pixel or a trajectory for each feature point , according to a set of composition constraints in both temporal and spatial dimensions . in this paper , we present a unified model to assess the similarity between two given individual or group activities . our approach avoids explicit extraction of individual actors , identifying and representing the inter-person interactions . with the proposed approach , retrieval from a video database can be performed through query-by-example ; and activities can be recognized by querying videos containing known activities . the suggested video matching process can be performed in an unsupervised manner . we demonstrate the performance of our approach by recognizing a set of human actions and football plays .", "topics": ["high- and low-level", "unsupervised learning"]}
{"title": "d-mfvi : distributed mean field variational inference using bregman admm", "abstract": "bayesian models provide a framework for probabilistic modelling of complex datasets . however , many of such models are computationally demanding especially in the presence of large datasets . on the other hand , in sensor network applications , statistical ( bayesian ) parameter estimation usually needs distributed algorithms , in which both data and computation are distributed across the nodes of the network . in this paper we propose a general framework for distributed bayesian learning using bregman alternating direction method of multipliers ( b-admm ) . we demonstrate the utility of our framework , with mean field variational bayes ( mfvb ) as the primitive for distributed matrix factorization ( mf ) and distributed affine structure from motion ( sfm ) .", "topics": ["computation"]}
{"title": "a neurodynamical system for finding a minimal vc dimension classifier", "abstract": "the recently proposed minimal complexity machine ( mcm ) finds a hyperplane classifier by minimizing an exact bound on the vapnik-chervonenkis ( vc ) dimension . the vc dimension measures the capacity of a learning machine , and a smaller vc dimension leads to improved generalization . on many benchmark datasets , the mcm generalizes better than svms and uses far fewer support vectors than the number used by svms . in this paper , we describe a neural network based on a linear dynamical system , that converges to the mcm solution . the proposed mcm dynamical system is conducive to an analogue circuit implementation on a chip or simulation using ordinary differential equation ( ode ) solvers . numerical experiments on benchmark datasets from the uci repository show that the proposed approach is scalable and accurate , as we obtain improved accuracies and fewer number of support vectors ( upto 74.3 % reduction ) with the mcm dynamical system .", "topics": ["simulation", "scalability"]}
{"title": "near-optimal joint object matching via convex relaxation", "abstract": "joint matching over a collection of objects aims at aggregating information from a large collection of similar instances ( e.g . images , graphs , shapes ) to improve maps between pairs of them . given multiple matches computed between a few object pairs in isolation , the goal is to recover an entire collection of maps that are ( 1 ) globally consistent , and ( 2 ) close to the provided maps -- - and under certain conditions provably the ground-truth maps . despite recent advances on this problem , the best-known recovery guarantees are limited to a small constant barrier -- - none of the existing methods find theoretical support when more than $ 50\\ % $ of input correspondences are corrupted . moreover , prior approaches focus mostly on fully similar objects , while it is practically more demanding to match instances that are only partially similar to each other . in this paper , we develop an algorithm to jointly match multiple objects that exhibit only partial similarities , given a few pairwise matches that are densely corrupted . specifically , we propose to recover the ground-truth maps via a parameter-free convex program called matchlift , following a spectral method that pre-estimates the total number of distinct elements to be matched . encouragingly , matchlift exhibits near-optimal error-correction ability , i.e . in the asymptotic regime it is guaranteed to work even when a dominant fraction $ 1-\\theta\\left ( \\frac { \\log^ { 2 } n } { \\sqrt { n } } \\right ) $ of the input maps behave like random outliers . furthermore , matchlift succeeds with minimal input complexity , namely , perfect matching can be achieved as soon as the provided maps form a connected map graph . we evaluate the proposed algorithm on various benchmark data sets including synthetic examples and real-world examples , all of which confirm the practical applicability of matchlift .", "topics": ["synthetic data", "numerical analysis"]}
{"title": "consensus attention-based neural networks for chinese reading comprehension", "abstract": "reading comprehension has embraced a booming in recent nlp research . several institutes have released the cloze-style reading comprehension data , and these have greatly accelerated the research of machine comprehension . in this work , we firstly present chinese reading comprehension datasets , which consist of people daily news dataset and children 's fairy tale ( cft ) dataset . also , we propose a consensus attention-based neural network architecture to tackle the cloze-style reading comprehension problem , which aims to induce a consensus attention over every words in the query . experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets . furthermore , we setup a baseline for chinese reading comprehension task , and hopefully this would speed up the process for future research .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "a dynamic programming solution to bounded dejittering problems", "abstract": "we propose a dynamic programming solution to image dejittering problems with bounded displacements and obtain efficient algorithms for the removal of line jitter , line pixel jitter , and pixel jitter .", "topics": ["pixel"]}
{"title": "probabilistic neural programs", "abstract": "we present probabilistic neural programs , a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks . probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice . thus , a program describes both a collection of decisions as well as the neural network architecture used to make each one . we evaluate our approach on a challenging diagram question answering task where probabilistic neural programs correctly execute nearly twice as many programs as a baseline model .", "topics": ["baseline ( configuration management )", "computation"]}
{"title": "deep learning the indus script", "abstract": "standardized corpora of undeciphered scripts , a necessary starting point for computational epigraphy , requires laborious human effort for their preparation from raw archaeological records . automating this process through machine learning algorithms can be of significant aid to epigraphical research . here , we take the first steps in this direction and present a deep learning pipeline that takes as input images of the undeciphered indus script , as found in archaeological artifacts , and returns as output a string of graphemes , suitable for inclusion in a standard corpus . the image is first decomposed into regions using selective search and these regions are classified as containing textual and/or graphical information using a convolutional neural network . regions classified as potentially containing text are hierarchically merged and trimmed to remove non-textual information . the remaining textual part of the image is segmented using standard image processing techniques to isolate individual graphemes . this set is finally passed to a second convolutional neural network to classify the graphemes , based on a standard corpus . the classifier can identify the presence or absence of the most frequent indus grapheme , the `` jar '' sign , with an accuracy of 92 % . our results demonstrate the great potential of deep learning approaches in computational epigraphy and , more generally , in the digital humanities .", "topics": ["image processing", "text corpus"]}
{"title": "challenges in bayesian adaptive data analysis", "abstract": "traditional statistical analysis requires that the analysis process and data are independent . by contrast , the new field of adaptive data analysis hopes to understand and provide algorithms and accuracy guarantees for research as it is commonly performed in practice , as an iterative process of interacting repeatedly with the same data set , such as repeated tests against a holdout set . previous work has defined a model with a rather strong lower bound on sample complexity in terms of the number of queries , $ n\\sim\\sqrt q $ , arguing that adaptive data analysis is much harder than static data analysis , where $ n\\sim\\log q $ is possible . instead , we argue that those strong lower bounds point to a limitation of the previous model in that it must consider wildly asymmetric scenarios which do not hold in typical applications . to better understand other difficulties of adaptivity , we propose a new bayesian version of the problem that mandates symmetry . since the other lower bound techniques are ruled out , we can more effectively see difficulties that might otherwise be overshadowed . as a first contribution to this model , we produce a new problem using error-correcting codes on which a large family of methods , including all previously proposed algorithms , require roughly $ n\\sim\\sqrt [ 4 ] q $ . these early results illustrate new difficulties in adaptive data analysis regarding slightly correlated queries on problems with concentrated uncertainty .", "topics": ["iteration"]}
{"title": "the possibilities and limitations of private prediction markets", "abstract": "we consider the design of private prediction markets , financial markets designed to elicit predictions about uncertain events without revealing too much information about market participants ' actions or beliefs . our goal is to design market mechanisms in which participants ' trades or wagers influence the market 's behavior in a way that leads to accurate predictions , yet no single participant has too much influence over what others are able to observe . we study the possibilities and limitations of such mechanisms using tools from differential privacy . we begin by designing a private one-shot wagering mechanism in which bettors specify a belief about the likelihood of a future event and a corresponding monetary wager . wagers are redistributed among bettors in a way that more highly rewards those with accurate predictions . we provide a class of wagering mechanisms that are guaranteed to satisfy truthfulness , budget balance in expectation , and other desirable properties while additionally guaranteeing epsilon-joint differential privacy in the bettors ' reported beliefs , and analyze the trade-off between the achievable level of privacy and the sensitivity of a bettor 's payment to her own report . we then ask whether it is possible to obtain privacy in dynamic prediction markets , focusing our attention on the popular cost-function framework in which securities with payments linked to future events are bought and sold by an automated market maker . we show that under general conditions , it is impossible for such a market maker to simultaneously achieve bounded worst-case loss and epsilon-differential privacy without allowing the privacy guarantee to degrade extremely quickly as the number of trades grows , making such markets impractical in settings in which privacy is valued . we conclude by suggesting several avenues for potentially circumventing this lower bound .", "topics": ["loss function"]}
{"title": "boolean network robotics : a proof of concept", "abstract": "dynamical systems theory and complexity science provide powerful tools for analysing artificial agents and robots . furthermore , they have been recently proposed also as a source of design principles and guidelines . boolean networks are a prominent example of complex dynamical systems and they have been shown to effectively capture important phenomena in gene regulation . from an engineering perspective , these models are very compelling , because they can exhibit rich and complex behaviours , in spite of the compactness of their description . in this paper , we propose the use of boolean networks for controlling robots ' behaviour . the network is designed by means of an automatic procedure based on stochastic local search techniques . we show that this approach makes it possible to design a network which enables the robot to accomplish a task that requires the capability of navigating the space using a light stimulus , as well as the formation and use of an internal memory .", "topics": ["robot"]}
{"title": "deep learning for distant speech recognition", "abstract": "deep learning is an emerging technology that is considered one of the most promising directions for reaching higher levels of artificial intelligence . among the other achievements , building computers that understand speech represents a crucial leap towards intelligent machines . despite the great efforts of the past decades , however , a natural and robust human-machine speech interaction still appears to be out of reach , especially when users interact with a distant microphone in noisy and reverberant environments . the latter disturbances severely hamper the intelligibility of a speech signal , making distant speech recognition ( dsr ) one of the major open challenges in the field . this thesis addresses the latter scenario and proposes some novel techniques , architectures , and algorithms to improve the robustness of distant-talking acoustic models . we first elaborate on methodologies for realistic data contamination , with a particular emphasis on dnn training with simulated data . we then investigate on approaches for better exploiting speech contexts , proposing some original methodologies for both feed-forward and recurrent neural networks . lastly , inspired by the idea that cooperation across different dnns could be the key for counteracting the harmful effects of noise and reverberation , we propose a novel deep learning paradigm called network of deep neural networks . the analysis of the original concepts were based on extensive experimental validations conducted on both real and simulated data , considering different corpora , microphone configurations , environments , noisy conditions , and asr tasks .", "topics": ["recurrent neural network", "simulation"]}
{"title": "learning mixtures of ising models using pseudolikelihood", "abstract": "maximum pseudolikelihood method has been among the most important methods for learning parameters of statistical physics models , such as ising models . in this paper , we study how pseudolikelihood can be derived for learning parameters of a mixture of ising models . the performance of the proposed approach is demonstrated for ising and potts models on both synthetic and real data .", "topics": ["synthetic data"]}
