{"title": "on sequential elimination algorithms for best-arm identification in multi-armed bandits", "abstract": "we consider the best-arm identification problem in multi-armed bandits , which focuses purely on exploration . a player is given a fixed budget to explore a finite set of arms , and the rewards of each arm are drawn independently from a fixed , unknown distribution . the player aims to identify the arm with the largest expected reward . we propose a general framework to unify sequential elimination algorithms , where the arms are dismissed iteratively until a unique arm is left . our analysis reveals a novel performance measure expressed in terms of the sampling mechanism and number of eliminated arms at each round . based on this result , we develop an algorithm that divides the budget according to a nonlinear function of remaining arms at each round . we provide theoretical guarantees for the algorithm , characterizing the suitable nonlinearity for different problem environments described by the number of competitive arms . matching the theoretical results , our experiments show that the nonlinear algorithm outperforms the state-of-the-art . we finally study the side-observation model , where pulling an arm reveals the rewards of its related arms , and we establish improved theoretical guarantees in the pure-exploration setting .", "topics": ["sampling ( signal processing )", "nonlinear system"]}
{"title": "quantum annealing for variational bayes inference", "abstract": "this paper presents studies on a deterministic annealing algorithm based on quantum annealing for variational bayes ( qavb ) inference , which can be seen as an extension of the simulated annealing for variational bayes ( savb ) inference . qavb is as easy as savb to implement . experiments revealed qavb finds a better local optimum than savb in terms of the variational free energy in latent dirichlet allocation ( lda ) .", "topics": ["calculus of variations", "simulation"]}
{"title": "the ideal of the trifocal variety", "abstract": "techniques from representation theory , symbolic computational algebra , and numerical algebraic geometry are used to find the minimal generators of the ideal of the trifocal variety . an effective test for determining whether a given tensor is a trifocal tensor is also given .", "topics": ["numerical analysis"]}
{"title": "unsupervised and semi-supervised learning with categorical generative adversarial networks", "abstract": "in this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data . our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution , against robustness of the classifier to an adversarial generative model . the resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks ( gan ) framework or as an extension of the regularized information maximization ( rim ) framework to robust classification against an optimal adversary . we empirically evaluate our method - which we dub categorical generative adversarial networks ( or catgan ) - on synthetic data as well as on challenging image classification tasks , demonstrating the robustness of the learned classifiers . we further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier , and identify links between the catgan objective and discriminative clustering algorithms ( such as rim ) .", "topics": ["generative model", "statistical classification"]}
{"title": "the earth ai n't flat : monocular reconstruction of vehicles on steep and graded roads from a moving camera", "abstract": "accurate localization of other traffic participants is a vital task in autonomous driving systems . state-of-the-art systems employ a combination of sensing modalities such as rgb cameras and lidars for localizing traffic participants , but most such demonstrations have been confined to plain roads . we demonstrate , to the best of our knowledge , the first results for monocular object localization and shape estimation on surfaces that do not share the same plane with the moving monocular camera . we approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the pose and shape of the vehicles , and the orientation of the local ground plane on which the vehicle stands as well . we evaluate the proposed approach on the kitti and synthia-sf benchmarks , for a variety of road plane configurations . the proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads .", "topics": ["autonomous car"]}
{"title": "accelerometer based activity classification with variational inference on sticky hdp-slds", "abstract": "as part of daily monitoring of human activities , wearable sensors and devices are becoming increasingly popular sources of data . with the advent of smartphones equipped with acceloremeter , gyroscope and camera ; it is now possible to develop activity classification platforms everyone can use conveniently . in this paper , we propose a fast inference method for an unsupervised non-parametric time series model namely variational inference for sticky hdp-slds ( hierarchical dirichlet process switching linear dynamical system ) . we show that the proposed algorithm can differentiate various indoor activities such as sitting , walking , turning , going up/down the stairs and taking the elevator using only the acceloremeter of an android smartphone samsung galaxy s4 . we used the front camera of the smartphone to annotate activity types precisely . we compared the proposed method with hidden markov models with gaussian emission probabilities on a dataset of 10 subjects . we showed that the efficacy of the stickiness property . we further compared the variational inference to the gibbs sampler on the same model and show that variational inference is faster in one order of magnitude .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "structural attention neural networks for improved sentiment analysis", "abstract": "we introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification . our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation . also , the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree . to our knowledge , the proposed models achieve state of the art performance on the stanford sentiment treebank dataset .", "topics": ["neural networks"]}
{"title": "exploiting convolutional neural network for risk prediction with medical feature embedding", "abstract": "the widespread availability of electronic health records ( ehrs ) promises to usher in the era of personalized medicine . however , the problem of extracting useful clinical representations from longitudinal ehr data remains challenging . in this paper , we explore deep neural network models with learned medical feature embedding to deal with the problems of high dimensionality and temporality . specifically , we use a multi-layer convolutional neural network ( cnn ) to parameterize the model and is thus able to capture complex non-linear longitudinal evolution of ehrs . our model can effectively capture local/short temporal dependency in ehrs , which is beneficial for risk prediction . to account for high dimensionality , we use the embedding medical features in the cnn model which hold the natural medical concepts . our initial experiments produce promising results and demonstrate the effectiveness of both the medical feature embedding and the proposed convolutional neural network in risk prediction on cohorts of congestive heart failure and diabetes patients compared with several strong baselines .", "topics": ["recurrent neural network", "nonlinear system"]}
{"title": "ocrapose ii : an ocr-based indoor positioning system using mobile phone images", "abstract": "in this paper , we propose an ocr ( optical character recognition ) -based localization system called ocrapose ii , which is applicable in a number of indoor scenarios including office buildings , parkings , airports , grocery stores , etc . in these scenarios , characters ( i.e . texts or numbers ) can be used as suitable distinctive landmarks for localization . the proposed system takes advantage of ocr to read these characters in the query still images and provides a rough location estimate using a floor plan . then , it finds depth and angle-of-view of the query using the information provided by the ocr engine in order to refine the location estimate . we derive novel formulas for the query angle-of-view and depth estimation using image line segments and the ocr box information . we demonstrate the applicability and effectiveness of the proposed system through experiments in indoor scenarios . it is shown that our system demonstrates better performance compared to the state-of-the-art benchmarks in terms of location recognition rate and average localization error specially under sparse database condition .", "topics": ["sparse matrix"]}
{"title": "refinement of operator-valued reproducing kernels", "abstract": "this paper studies the construction of a refinement kernel for a given operator-valued reproducing kernel such that the vector-valued reproducing kernel hilbert space of the refinement kernel contains that of the given one as a subspace . the study is motivated from the need of updating the current operator-valued reproducing kernel in multi-task learning when underfitting or overfitting occurs . numerical simulations confirm that the established refinement kernel method is able to meet this need . various characterizations are provided based on feature maps and vector-valued integral representations of operator-valued reproducing kernels . concrete examples of refining translation invariant and finite hilbert-schmidt operator-valued reproducing kernels are provided . other examples include refinement of hessian of scalar-valued translation-invariant kernels and transformation kernels . existence and properties of operator-valued reproducing kernels preserved during the refinement process are also investigated .", "topics": ["kernel ( operating system )", "numerical analysis"]}
{"title": "the use of classifiers in sequential inference", "abstract": "we study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints . in particular , we develop two general approaches for an important subproblem-identifying phrase structure . the first is a markovian approach that extends standard hmms to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies . the second is an extension of constraint satisfaction formalisms . we develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing .", "topics": ["parsing"]}
{"title": "annotation artifacts in natural language inference data", "abstract": "large-scale datasets for natural language inference are created by presenting crowd workers with a sentence ( premise ) , and asking them to generate three new sentences ( hypotheses ) that it entails , contradicts , or is logically neutral with respect to . we show that , in a significant portion of such data , this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis , without observing the premise . specifically , we show that a simple text categorization model can correctly classify the hypothesis alone in about 67 % of snli ( bowman et . al , 2015 ) and 53 % of multinli ( williams et . al , 2017 ) . our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes . our findings suggest that the success of natural language inference models to date has been overestimated , and that the task remains a hard open problem .", "topics": ["natural language"]}
{"title": "minimum message length clustering using gibbs sampling", "abstract": "the k-mean and em algorithms are popular in clustering and mixture modeling , due to their simplicity and ease of implementation . however , they have several significant limitations . both coverage to a local optimum of their respective objective functions ( ignoring the uncertainty in the model space ) , require the apriori specification of the number of classes/clsuters , and are inconsistent . in this work we overcome these limitations by using the minimum message length ( mml ) principle and a variation to the k-means/em observation assignment and parameter calculation scheme . we maintain the simplicity of these approaches while constructing a bayesian mixture modeling tool that samples/searches the model space using a markov chain monte carlo ( mcmc ) sampler known as a gibbs sampler . gibbs sampling allows us to visit each model according to its posterior probability . therefore , if the model space is multi-modal we will visit all models and not get stuck in local optima . we call our approach multiple chains at equilibrium ( mce ) mml sampling .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "natural language understanding with distributed representation", "abstract": "this is a lecture note for the course ds-ga 3001 < natural language understanding with distributed representation > at the center for data science , new york university in fall , 2015 . as the name of the course suggests , this lecture note introduces readers to a neural network based approach to natural language understanding/processing . in order to make it as self-contained as possible , i spend much time on describing basics of machine learning and neural networks , only after which how they are used for natural languages is introduced . on the language front , i almost solely focus on language modelling and machine translation , two of which i personally find most fascinating and most fundamental to natural language understanding .", "topics": ["natural language processing", "machine translation"]}
{"title": "incorporating external knowledge to answer open-domain visual questions with dynamic memory networks", "abstract": "visual question answering ( vqa ) has attracted much attention since it offers insight into the relationships between the multi-modal analysis of images and natural language . most of the current algorithms are incapable of answering open-domain questions that require to perform reasoning beyond the image contents . to address this issue , we propose a novel framework which endows the model capabilities in answering more complex questions by leveraging massive external knowledge with dynamic memory networks . specifically , the questions along with the corresponding images trigger a process to retrieve the relevant information in external knowledge bases , which are embedded into a continuous vector space by preserving the entity-relation structures . afterwards , we employ dynamic memory networks to attend to the large body of facts in the knowledge graph and images , and then perform reasoning over these facts to generate corresponding answers . extensive experiments demonstrate that our model not only achieves the state-of-the-art performance in the visual question answering task , but can also answer open-domain questions effectively by leveraging the external knowledge .", "topics": ["natural language"]}
{"title": "mirror descent meets fixed share ( and feels no regret )", "abstract": "mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension . this is done using either a carefully designed projection or by a weight sharing technique . via a novel unified analysis , we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting , adaptive , discounted , and other related regrets . our analysis also captures and extends the generalized weight sharing technique of bousquet and warmuth , and can be refined in several ways , including improvements for small losses and adaptive tuning of parameters .", "topics": ["regret ( decision theory )"]}
{"title": "further results on dissimilarity spaces for hyperspectral images rf-cbir", "abstract": "content-based image retrieval ( cbir ) systems are powerful search tools in image databases that have been little applied to hyperspectral images . relevance feedback ( rf ) is an iterative process that uses machine learning techniques and user 's feedback to improve the cbir systems performance . we pursued to expand previous research in hyperspectral cbir systems built on dissimilarity functions defined either on spectral and spatial features extracted by spectral unmixing techniques , or on dictionaries extracted by dictionary-based compressors . these dissimilarity functions were not suitable for direct application in common machine learning techniques . we propose to use a rf general approach based on dissimilarity spaces which is more appropriate for the application of machine learning algorithms to the hyperspectral rf-cbir . we validate the proposed rf method for hyperspectral cbir systems over a real hyperspectral dataset .", "topics": ["relevance", "database"]}
{"title": "efficient auc optimization for information ranking applications", "abstract": "adequate evaluation of an information retrieval system to estimate future performance is a crucial task . area under the roc curve ( auc ) is widely used to evaluate the generalization of a retrieval system . however , the objective function optimized in many retrieval systems is the error rate and not the auc value . this paper provides an efficient and effective non-linear approach to optimize auc using additive regression trees , with a special emphasis on the use of multi-class auc ( mauc ) because multiple relevance levels are widely used in many ranking applications . compared to a conventional linear approach , the performance of the non-linear approach is comparable on binary-relevance benchmark datasets and is better on multi-relevance benchmark datasets .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "rethinking generalization requires revisiting old ideas : statistical mechanics approaches and complex learning behavior", "abstract": "we describe an approach to understand the peculiar and counterintuitive generalization properties of deep neural networks . the approach involves going beyond worst-case theoretical capacity control frameworks that have been popular in machine learning in recent years to revisit old ideas in the statistical mechanics of neural networks . within this approach , we present a prototypical very simple deep learning ( vsdl ) model , whose behavior is controlled by two control parameters , one describing an effective amount of data , or load , on the network ( that decreases when noise is added to the input ) , and one with an effective temperature interpretation ( that increases when algorithms are early stopped ) . using this model , we describe how a very simple application of ideas from the statistical mechanics theory of generalization provides a strong qualitative description of recently-observed empirical results regarding the inability of deep neural networks not to overfit training data , discontinuous learning and sharp transitions in the generalization properties of learning algorithms , etc .", "topics": ["test set"]}
{"title": "chemgan challenge for drug discovery : can ai reproduce natural chemical diversity ?", "abstract": "generating molecules with desired chemical properties is important for drug discovery . the use of generative neural networks is promising for this task . however , from visual inspection , it often appears that generated samples lack diversity . in this paper , we quantify this internal chemical diversity , and we raise the following challenge : can a nontrivial ai model reproduce natural chemical diversity for desired molecules ? to illustrate this question , we consider two generative models : a reinforcement learning model and the recently introduced organ . both fail at this challenge . we hope this challenge will stimulate research in this direction .", "topics": ["generative model", "reinforcement learning"]}
{"title": "generalized beta mixtures of gaussians", "abstract": "in recent years , a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems . in general , these new priors can be expressed as scale mixtures of normals , but have more complex forms and better properties than traditional cauchy and double exponential priors . we first propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases . this encompassing framework should prove useful in comparing competing priors , considering properties and revealing close connections . we then develop a class of variational bayes approximations through the new hierarchy presented that will scale more efficiently to the types of truly massive data sets that are now encountered routinely .", "topics": ["calculus of variations", "time complexity"]}
{"title": "compact neural networks based on the multiscale entanglement renormalization ansatz", "abstract": "the goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states , the multi-scale entanglement renormalization ansatz ( mera ) . we employ mera as a replacement for linear layers in a neural network and test this implementation on the cifar-10 dataset . the proposed method outperforms factorization using tensor trains , providing greater compression for the same level of accuracy and greater accuracy for the same level of compression . we demonstrate mera-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1 % compared to the equivalent fully connected layers .", "topics": ["approximation algorithm"]}
{"title": "interpretation and generalization of score matching", "abstract": "score matching is a recently developed parameter learning method that is particularly effective to complicated high dimensional density models with intractable partition functions . in this paper , we study two issues that have not been completely resolved for score matching . first , we provide a formal link between maximum likelihood and score matching . our analysis shows that score matching finds model parameters that are more robust with noisy training data . second , we develop a generalization of score matching . based on this generalization , we further demonstrate an extension of score matching to models of discrete data .", "topics": ["test set"]}
{"title": "optimizing non-decomposable performance measures : a tale of two classes", "abstract": "modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics , and require optimizing performance measures that are non-decomposable over the dataset , such as f-measure . such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent . in this paper we reveal that for two large families of performance measures that can be expressed as functions of true positive/negative rates , it is indeed possible to implement point stochastic updates . the families we consider are concave and pseudo-linear functions of tpr , tnr which cover several popularly used performance measures such as f-measure , g-mean and h-mean . our core contribution is an adaptive linearization scheme for these families , using which we develop optimization techniques that enable truly point-based stochastic updates . for concave performance measures we propose spade , a stochastic primal dual solver ; for pseudo-linear measures we propose stamp , a stochastic alternate maximization procedure . both methods have crisp convergence guarantees , demonstrate significant speedups over existing methods - often by an order of magnitude or more , and give similar or more accurate predictions on test data .", "topics": ["gradient descent", "gradient"]}
{"title": "the dutch 's real world financial institute : introducing quantum-like bayesian networks as an alternative model to deal with uncertainty", "abstract": "in this work , we analyse and model a real life financial loan application belonging to a sample bank in the netherlands . the log is robust in terms of data , containing a total of 262 200 event logs , belonging to 13 087 different credit applications . the dataset is heterogeneous and consists of a mixture of computer generated automatic processes and manual human tasks . the goal is to work out a decision model , which represents the underlying tasks that make up the loan application service , and to assess potential areas of improvement of the institution 's internal processes . to this end we study the impact of incomplete event logs for the extraction and analysis of business processes . it is quite common that event logs are incomplete with several amounts of missing information ( for instance , workers forget to register their tasks ) . absence of data is translated into a drastic decrease of precision and compromises the decision models , leading to biased and unrepresentative results . we investigate how classical probabilistic models are affected by incomplete event logs and we explore quantum-like probabilistic inferences as an alternative mathematical model to classical probability . this work represents a first step towards systematic investigation of the impact of quantum interference in a real life large scale decision scenario . the results obtained in this study indicate that , under high levels of uncertainty , the quantum-like models generate quantum interference terms , which allow an additional non-linear parameterisation of the data . experimental results attest the efficiency of the quantum-like bayesian networks , since the application of interference terms is able to reduce the error percentage of inferences performed over quantum-like models when compared to inferences produced by classical models .", "topics": ["nonlinear system", "bayesian network"]}
{"title": "a safe screening rule for sparse logistic regression", "abstract": "the l1-regularized logistic regression ( or sparse logistic regression ) is a widely used method for simultaneous classification and feature selection . although many recent efforts have been devoted to its efficient implementation , its application to high dimensional data still poses significant challenges . in this paper , we present a fast and effective sparse logistic regression screening rule ( slores ) to identify the 0 components in the solution vector , which may lead to a substantial reduction in the number of features to be entered to the optimization . an appealing feature of slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem . moreover , slores is independent of solvers for sparse logistic regression , thus slores can be integrated with any existing solver to improve the efficiency . we have evaluated slores using high-dimensional data sets from different applications . extensive experimental results demonstrate that slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression is improved by one magnitude in general .", "topics": ["sparse matrix"]}
{"title": "evolution in groups : a deeper look at synaptic cluster driven evolution of deep neural networks", "abstract": "a promising paradigm for achieving highly efficient deep neural networks is the idea of evolutionary deep intelligence , which mimics biological evolution processes to progressively synthesize more efficient networks . a crucial design factor in evolutionary deep intelligence is the genetic encoding scheme used to simulate heredity and determine the architectures of offspring networks . in this study , we take a deeper look at the notion of synaptic cluster-driven evolution of deep neural networks which guides the evolution process towards the formation of a highly sparse set of synaptic clusters in offspring networks . utilizing a synaptic cluster-driven genetic encoding , the probabilistic encoding of synaptic traits considers not only individual synaptic properties but also inter-synaptic relationships within a deep neural network . this process results in highly sparse offspring networks which are particularly tailored for parallel computational devices such as gpus and deep neural network accelerator chips . comprehensive experimental results using four well-known deep neural network architectures ( lenet-5 , alexnet , resnet-56 , and detectnet ) on two different tasks ( object categorization and object detection ) demonstrate the efficiency of the proposed method . cluster-driven genetic encoding scheme synthesizes networks that can achieve state-of-the-art performance with significantly smaller number of synapses than that of the original ancestor network . ( $ \\sim $ 125-fold decrease in synapses for mnist ) . furthermore , the improved cluster efficiency in the generated offspring networks ( $ \\sim $ 9.71-fold decrease in clusters for mnist and a $ \\sim $ 8.16-fold decrease in clusters for kitti ) is particularly useful for accelerated performance on parallel computing hardware architectures such as those in gpus and deep neural network accelerator chips .", "topics": ["object detection", "sparse matrix"]}
{"title": "semi-supervised learning with gans : manifold invariance with improved inference", "abstract": "semi-supervised learning methods using generative adversarial networks ( gans ) have shown promising empirical success recently . most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label . motivated by the ability of the gans generator to capture the data manifold well , we propose to estimate the tangent space to the data manifold using gans and employ it to inject invariances into the classifier . in the process , we propose enhancements over existing methods for learning the inverse mapping ( i.e . , the encoder ) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample . we observe considerable empirical gains in semi-supervised learning over baselines , particularly in the cases when the number of labeled examples is low . we also provide insights into how fake examples influence the semi-supervised learning procedure .", "topics": ["supervised learning", "statistical classification"]}
{"title": "stereotyping and bias in the flickr30k dataset", "abstract": "an untested assumption behind the crowdsourced descriptions of the images in the flickr30k dataset ( young et al . , 2014 ) is that they `` focus only on the information that can be obtained from the image alone '' ( hodosh et al . , 2013 , p . 859 ) . this paper presents some evidence against this assumption , and provides a list of biases and unwarranted inferences that can be found in the flickr30k dataset . finally , it considers methods to find examples of these , and discusses how we should deal with stereotype-driven descriptions in future applications .", "topics": ["computer vision", "text corpus"]}
{"title": "recognition of visually perceived compositional human actions by multiple spatio-temporal scales recurrent neural networks", "abstract": "the current paper proposes a novel neural network model for recognizing visually perceived human actions . the proposed multiple spatio-temporal scales recurrent neural network ( mstrnn ) model is derived by introducing multiple timescale recurrent dynamics to the conventional convolutional neural network model . one of the essential characteristics of the mstrnn is that its architecture imposes both spatial and temporal constraints simultaneously on the neural activity which vary in multiple scales among different layers . as suggested by the principle of the upward and downward causation , it is assumed that the network can develop meaningful structures such as functional hierarchy by taking advantage of such constraints during the course of learning . to evaluate the characteristics of the model , the current study uses three types of human action video dataset consisting of different types of primitive actions and different levels of compositionality on them . the performance of the mstrnn in testing with these dataset is compared with the ones by other representative deep learning models used in the field . the analysis of the internal representation obtained through the learning with the dataset clarifies what sorts of functional hierarchy can be developed by extracting the essential compositionality underlying the dataset .", "topics": ["recurrent neural network", "time series"]}
{"title": "adversarial ranking for language generation", "abstract": "generative adversarial networks ( gans ) have great successes on synthesizing data . however , the existing gans restrict the discriminator to be a binary classifier , and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions . in this paper , we propose a novel generative adversarial network , rankgan , for generating high-quality language descriptions . rather than training the discriminator to learn and assign absolute binary predicate for individual data sample , the proposed rankgan is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group . by viewing a set of data samples collectively and evaluating their quality through relative ranking scores , the discriminator is able to make better assessment which in turn helps to learn a better generator . the proposed rankgan is optimized through the policy gradient technique . experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach .", "topics": ["natural language"]}
{"title": "a deep hierarchical approach to lifelong learning in minecraft", "abstract": "we propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base . knowledge is transferred by learning reusable skills to solve tasks in minecraft , a popular video game which is an unsolved and high-dimensional lifelong learning problem . these reusable skills , which we refer to as deep skill networks , are then incorporated into our novel hierarchical deep reinforcement learning network ( h-drln ) architecture using two techniques : ( 1 ) a deep skill array and ( 2 ) skill distillation , our novel variation of policy distillation ( rusu et . al . 2015 ) for learning skills . skill distillation enables the hdrln to efficiently retain knowledge and therefore scale in lifelong learning , by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network . the h-drln exhibits superior performance and lower learning sample complexity compared to the regular deep q network ( mnih et . al . 2015 ) in sub-domains of minecraft .", "topics": ["reinforcement learning", "artificial intelligence"]}
{"title": "figuring out actors in text streams : using collocations to establish incremental mind-maps", "abstract": "the recognition , involvement , and description of main actors influences the story line of the whole text . this is of higher importance as the text per se represents a flow of words and expressions that once it is read it is lost . in this respect , the understanding of a text and moreover on how the actor exactly behaves is not only a major concern : as human beings try to store a given input on short-term memory while associating diverse aspects and actors with incidents , the following approach represents a virtual architecture , where collocations are concerned and taken as the associative completion of the actors ' acting . once that collocations are discovered , they become managed in separated memory blocks broken down by the actors . as for human beings , the memory blocks refer to associative mind-maps . we then present several priority functions to represent the actual temporal situation inside a mind-map to enable the user to reconstruct the recent events from the discovered temporal results .", "topics": ["map"]}
{"title": "memorize or generalize ? searching for a compositional rnn in a haystack", "abstract": "neural networks are very powerful learning systems , but they do not readily generalize from one task to the other . this is partly due to the fact that they do not learn in a compositional way , that is , by discovering skills that are shared by different tasks , and recombining them to solve new problems . in this paper , we explore the compositional generalization capabilities of recurrent neural networks ( rnns ) . we first propose the lookup table composition domain as a simple setup to test compositional behaviour and show that it is theoretically possible for a standard rnn to learn to behave compositionally in this domain when trained with standard gradient descent and provided with additional supervision . we then remove this additional supervision and perform a search over a large number of model initializations to investigate the proportion of rnns that can still converge to a compositional solution . we discover that a small but non-negligible proportion of rnns do reach partial compositional solutions even without special architectural constraints . this suggests that a combination of gradient descent and evolutionary strategies directly favouring the minority models that developed more compositional approaches might suffice to lead standard rnns towards compositional solutions .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "relevant knowledge first - reinforcement learning and forgetting in knowledge based configuration", "abstract": "in order to solve complex configuration tasks in technical domains , various knowledge based methods have been developed . however their applicability is often unsuccessful due to their low efficiency . one of the reasons for this is that ( parts of the ) problems have to be solved again and again , instead of being `` learnt '' from preceding processes . however , learning processes bring with them the problem of conservatism , for in technical domains innovation is a deciding factor in competition . on the other hand a certain amount of conservatism is often desired since uncontrolled innovation as a rule is also detrimental . this paper proposes the heuristic rkf ( relevant knowledge first ) for making decisions in configuration processes based on the so-called relevance of objects in a knowledge base . the underlying relevance-function has two components , one based on reinforcement learning and the other based on forgetting ( fading ) . relevance of an object increases with its successful use and decreases with age when it is not used . rkf has been developed to speed up the configuration process and to improve the quality of the solutions relative to the reward value that is given by users .", "topics": ["reinforcement learning"]}
{"title": "unsupervised document embedding with cnns", "abstract": "we propose a new model for unsupervised document embedding . leading existing approaches either require complex inference or use recurrent neural networks ( rnn ) that are difficult to parallelize . we take a different route and develop a convolutional neural network ( cnn ) embedding model . our cnn architecture is fully parallelizable resulting in over 10x speedup in inference time over rnn models . parallelizable architecture enables to train deeper models where each successive layer has increasingly larger receptive field and models longer range semantic structure within the document . we additionally propose a fully unsupervised learning algorithm to train this model based on stochastic forward prediction . empirical results on two public benchmarks show that our approach produces comparable to state-of-the-art accuracy at a fraction of computational cost .", "topics": ["recurrent neural network", "unsupervised learning"]}
{"title": "statistical pattern recognition for driving styles based on bayesian probability and kernel density estimation", "abstract": "driving styles have a great influence on vehicle fuel economy , active safety , and drivability . to recognize driving styles of path-tracking behaviors for different divers , a statistical pattern-recognition method is developed to deal with the uncertainty of driving styles or characteristics based on probability density estimation . first , to describe driver path-tracking styles , vehicle speed and throttle opening are selected as the discriminative parameters , and a conditional kernel density function of vehicle speed and throttle opening is built , respectively , to describe the uncertainty and probability of two representative driving styles , e.g . , aggressive and normal . meanwhile , a posterior probability of each element in feature vector is obtained using full bayesian theory . second , a euclidean distance method is involved to decide to which class the driver should be subject instead of calculating the complex covariance between every two elements of feature vectors . by comparing the euclidean distance between every elements in feature vector , driving styles are classified into seven levels ranging from low normal to high aggressive . subsequently , to show benefits of the proposed pattern-recognition method , a cross-validated method is used , compared with a fuzzy logic-based pattern-recognition method . the experiment results show that the proposed statistical pattern-recognition method for driving styles based on kernel density estimation is more efficient and stable than the fuzzy logic-based method .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "rasa : open source language understanding and dialogue management", "abstract": "we introduce a pair of tools , rasa nlu and rasa core , which are open source python libraries for building conversational software . their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers . in terms of design philosophy , we aim for ease of use , and bootstrapping from minimal ( or no ) initial training data . both packages are extensively documented and ship with a comprehensive suite of tests . the code is available at https : //github.com/rasahq/", "topics": ["test set"]}
{"title": "unveiling the relationship between complex networks metrics and word senses", "abstract": "the automatic disambiguation of word senses ( i.e . , the identification of which of the meanings is used in a given context for a word that has multiple meanings ) is essential for such applications as machine translation and information retrieval , and represents a key step for developing the so-called semantic web . humans disambiguate words in a straightforward fashion , but this does not apply to computers . in this paper we address the problem of word sense disambiguation ( wsd ) by treating texts as complex networks , and show that word senses can be distinguished upon characterizing the local structure around ambiguous words . our goal was not to obtain the best possible disambiguation system , but we nevertheless found that in half of the cases our approach outperforms traditional shallow methods . we show that the hierarchical connectivity and clustering of words are usually the most relevant features for wsd . the results reported here shine light on the relationship between semantic and structural parameters of complex networks . they also indicate that when combined with traditional techniques the complex network approach may be useful to enhance the discrimination of senses in large texts", "topics": ["cluster analysis", "natural language processing"]}
{"title": "a d.c . programming approach to the sparse generalized eigenvalue problem", "abstract": "in this paper , we consider the sparse eigenvalue problem wherein the goal is to obtain a sparse solution to the generalized eigenvalue problem . we achieve this by constraining the cardinality of the solution to the generalized eigenvalue problem and obtain sparse principal component analysis ( pca ) , sparse canonical correlation analysis ( cca ) and sparse fisher discriminant analysis ( fda ) as special cases . unlike the $ \\ell_1 $ -norm approximation to the cardinality constraint , which previous methods have used in the context of sparse pca , we propose a tighter approximation that is related to the negative log-likelihood of a student 's t-distribution . the problem is then framed as a d.c. ( difference of convex functions ) program and is solved as a sequence of convex programs by invoking the majorization-minimization method . the resulting algorithm is proved to exhibit \\emph { global convergence } behavior , i.e . , for any random initialization , the sequence ( subsequence ) of iterates generated by the algorithm converges to a stationary point of the d.c. program . the performance of the algorithm is empirically demonstrated on both sparse pca ( finding few relevant genes that explain as much variance as possible in a high-dimensional gene dataset ) and sparse cca ( cross-language document retrieval and vocabulary selection for music retrieval ) applications .", "topics": ["sparse matrix"]}
{"title": "sufficient conditions for coarse-graining evolutionary dynamics", "abstract": "it is commonly assumed that the ability to track the frequencies of a set of schemata in the evolving population of an infinite population genetic algorithm ( ipga ) under different fitness functions will advance efforts to obtain a theory of adaptation for the simple ga . unfortunately , for ipgas with long genomes and non-trivial fitness functions there do not currently exist theoretical results that allow such a study . we develop a simple framework for analyzing the dynamics of an infinite population evolutionary algorithm ( ipea ) . this framework derives its simplicity from its abstract nature . in particular we make no commitment to the data-structure of the genomes , the kind of variation performed , or the number of parents involved in a variation operation . we use this framework to derive abstract conditions under which the dynamics of an ipea can be coarse-grained . we then use this result to derive concrete conditions under which it becomes computationally feasible to closely approximate the frequencies of a family of schemata of relatively low order over multiple generations , even when the bitstsrings in the evolving population of the ipga are long .", "topics": ["approximation algorithm"]}
{"title": "learning and transfer of modulated locomotor controllers", "abstract": "we study a novel architecture and training procedure for locomotion tasks . a high-frequency , low-level `` spinal '' network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks . this pre-trained module is fixed and connected to a low-frequency , high-level `` cortical '' network , with access to all sensors , which drives behavior by modulating the inputs to the spinal network . where a monolithic end-to-end architecture fails completely , learning with a pre-trained spinal module succeeds at multiple high-level tasks , and enables the effective exploration required to learn from sparse rewards . we test our proposed architecture on three simulated bodies : a 16-dimensional swimming snake , a 20-dimensional quadruped , and a 54-dimensional humanoid . our results are illustrated in the accompanying video at https : //youtu.be/sbopyvhpraq", "topics": ["high- and low-level", "simulation"]}
{"title": "direct acoustics-to-word models for english conversational speech recognition", "abstract": "recent work on end-to-end automatic speech recognition ( asr ) has shown that the connectionist temporal classification ( ctc ) loss can be used to convert acoustics to phone or character sequences . such systems are used with a dictionary and separately-trained language model ( lm ) to produce word sequences . however , they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation . in this paper , we present the first results employing direct acoustics-to-word ctc models on two well-known public benchmark tasks : switchboard and callhome . these models do not require an lm or even a decoder at run-time and hence recognize speech with minimal complexity . however , due to the large number of word output units , ctc word models require orders of magnitude more data to train reliably compared to traditional systems . we present some techniques to mitigate this issue . our ctc word model achieves a word error rate of 13.0 % /18.8 % on the hub5-2000 switchboard/callhome test sets without any lm or decoder compared with 9.6 % /16.0 % for phone-based ctc with a 4-gram lm . we also present rescoring results on ctc word model lattices to quantify the performance benefits of a lm , and contrast the performance of word and phone ctc models .", "topics": ["speech recognition", "end-to-end principle"]}
{"title": "a deep learning model for structured outputs with high-order interaction", "abstract": "many real-world applications are associated with structured data , where not only input but also output has interplay . however , typical classification and regression models often lack the ability of simultaneously exploring high-order interaction within input and that within output . in this paper , we present a deep learning model aiming to generate a powerful nonlinear functional mapping from structured input to structured output . more specifically , we propose to integrate high-order hidden units , guided discriminative pretraining , and high-order auto-encoders for this purpose . we evaluate the model with three datasets , and obtain state-of-the-art performances among competitive methods . our current work focuses on structured output regression , which is a less explored area , although the model can be extended to handle structured label classification .", "topics": ["nonlinear system", "autoencoder"]}
{"title": "precise but natural specification for robot tasks", "abstract": "we present flipper , a natural language interface for describing high level task specifications for robots that are compiled into robot actions . flipper starts with a formal core language for task planning that allows expressing rich temporal specifications and uses a semantic parser to provide a natural language interface . flipper provides immediate visual feedback by executing an automatically constructed plan of the task in a graphical user interface . this allows the user to resolve potentially ambiguous interpretations . flipper extends itself via naturalization : users of flipper can define new commands , which are generalized and added as new rules to the core language , gradually growing a more and more natural task specification language . unlike other task-specification systems , flipper enables natural language interactions while maintaining the expressive power and formal precision of a programming language . we show through an initial user study that natural language interactions and generalization can considerably ease the description of tasks . moreover , over time , users employ more and more concepts outside of the initial core language . such extensions are available to the flipper community , and users can use concepts that others have defined .", "topics": ["natural language", "interaction"]}
{"title": "sentiment analysis in scholarly book reviews", "abstract": "so far different studies have tackled the sentiment analysis in several domains such as restaurant and movie reviews . but , this problem has not been studied in scholarly book reviews which is different in terms of review style and size . in this paper , we propose to combine different features in order to be presented to a supervised classifiers which extract the opinion target expressions and detect their polarities in scholarly book reviews . we construct a labeled corpus for training and evaluating our methods in french book reviews . we also evaluate them on english restaurant reviews in order to measure their robustness across the domains and languages . the evaluation shows that our methods are enough robust for english restaurant reviews and french book reviews .", "topics": ["supervised learning", "text corpus"]}
{"title": "hybrid srl with optimization modulo theories", "abstract": "generally speaking , the goal of constructive learning could be seen as , given an example set of structured objects , to generate novel objects with similar properties . from a statistical-relational learning ( srl ) viewpoint , the task can be interpreted as a constraint satisfaction problem , i.e . the generated objects must obey a set of soft constraints , whose weights are estimated from the data . traditional srl approaches rely on ( finite ) first-order logic ( fol ) as a description language , and on max-sat solvers to perform inference . alas , fol is unsuited for con- structive problems where the objects contain a mixture of boolean and numerical variables . it is in fact difficult to implement , e.g . linear arithmetic constraints within the language of fol . in this paper we propose a novel class of hybrid srl methods that rely on satisfiability modulo theories , an alternative class of for- mal languages that allow to describe , and reason over , mixed boolean-numerical objects and constraints . the resulting methods , which we call learning mod- ulo theories , are formulated within the structured output svm framework , and employ a weighted smt solver as an optimization oracle to perform efficient in- ference and discriminative max margin weight learning . we also present a few examples of constructive learning applications enabled by our method .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "inference in kingman 's coalescent with particle markov chain monte carlo method", "abstract": "we propose a new algorithm to do posterior sampling of kingman 's coalescent , based upon the particle markov chain monte carlo methodology . specifically , the algorithm is an instantiation of the particle gibbs sampling method , which alternately samples coalescent times conditioned on coalescent tree structures , and tree structures conditioned on coalescent times via the conditional sequential monte carlo procedure . we implement our algorithm as a c++ package , and demonstrate its utility via a parameter estimation task in population genetics on both single- and multiple-locus data . the experiment results show that the proposed algorithm performs comparable to or better than several well-developed methods .", "topics": ["markov chain"]}
{"title": "a provable svd-based algorithm for learning topics in dominant admixture corpus", "abstract": "topic models , such as latent dirichlet allocation ( lda ) , posit that documents are drawn from admixtures of distributions over words , known as topics . the inference problem of recovering topics from admixtures , is np-hard . assuming separability , a strong assumption , [ 4 ] gave the first provable algorithm for inference . for lda model , [ 6 ] gave a provable algorithm using tensor-methods . but [ 4,6 ] do not learn topic vectors with bounded $ l_1 $ error ( a natural measure for probability vectors ) . our aim is to develop a model which makes intuitive and empirically supported assumptions and to design an algorithm with natural , simple components such as svd , which provably solves the inference problem for the model with bounded $ l_1 $ error . a topic in lda and other models is essentially characterized by a group of co-occurring words . motivated by this , we introduce topic specific catchwords , group of words which occur with strictly greater frequency in a topic than any other topic individually and are required to have high frequency together rather than individually . a major contribution of the paper is to show that under this more realistic assumption , which is empirically verified on real corpora , a singular value decomposition ( svd ) based algorithm with a crucial pre-processing step of thresholding , can provably recover the topics from a collection of documents drawn from dominant admixtures . dominant admixtures are convex combination of distributions in which one distribution has a significantly higher contribution than others . apart from the simplicity of the algorithm , the sample complexity has near optimal dependence on $ w_0 $ , the lowest probability that a topic is dominant , and is better than [ 4 ] . empirical evidence shows that on several real world corpora , both catchwords and dominant admixture assumptions hold and the proposed algorithm substantially outperforms the state of the art [ 5 ] .", "topics": ["text corpus"]}
{"title": "a rigorously bayesian beam model and an adaptive full scan model for range finders in dynamic environments", "abstract": "this paper proposes and experimentally validates a bayesian network model of a range finder adapted to dynamic environments . all modeling assumptions are rigorously explained , and all model parameters have a physical interpretation . this approach results in a transparent and intuitive model . with respect to the state of the art beam model this paper : ( i ) proposes a different functional form for the probability of range measurements caused by unmodeled objects , ( ii ) intuitively explains the discontinuity encountered in te state of the art beam model , and ( iii ) reduces the number of model parameters , while maintaining the same representational power for experimental data . the proposed beam model is called rbbm , short for rigorously bayesian beam model . a maximum likelihood and a variational bayesian estimator ( both based on expectation-maximization ) are proposed to learn the model parameters . furthermore , the rbbm is extended to a full scan model in two steps : first , to a full scan model for static environments and next , to a full scan model for general , dynamic environments . the full scan model accounts for the dependency between beams and adapts to the local sample density when using a particle filter . in contrast to gaussian-based state of the art models , the proposed full scan model uses a sample-based approximation . this sample-based approximation enables handling dynamic environments and capturing multi-modality , which occurs even in simple static environments .", "topics": ["calculus of variations", "approximation"]}
{"title": "prosocial learning agents solve generalized stag hunts better than selfish ones", "abstract": "deep reinforcement learning has become an important paradigm for constructing agents that can enter complex multi-agent situations and improve their policies through experience . one commonly used technique is reactive training - applying standard rl methods while treating other agents as a part of the learner 's environment . it is known that in general-sum games reactive training can lead groups of agents to converge to inefficient outcomes . we focus on one such class of environments : stag hunt games . here agents either choose a risky cooperative policy ( which leads to high payoffs if both choose it but low payoffs to an agent who attempts it alone ) or a safe one ( which leads to a safe payoff no matter what ) . we ask how we can change the learning rule of a single agent to improve its outcomes in stag hunts that include other reactive learners . we extend existing work on reward-shaping in multi-agent reinforcement learning and show that that making a single agent prosocial , that is , making them care about the rewards of their partners can increase the probability that groups converge to good outcomes . thus , even if we control a single agent in a group making that agent prosocial can increase our agent 's long-run payoff . we show experimentally that this result carries over to a variety of more complex environments with stag hunt-like dynamics including ones where agents must learn from raw input pixels .", "topics": ["reinforcement learning", "pixel"]}
{"title": "multi-task domain adaptation for sequence tagging", "abstract": "many domain adaptation approaches rely on learning cross domain shared representations to transfer the knowledge learned in one domain to other domains . traditional domain adaptation only considers adapting for one task . in this paper , we explore multi-task representation learning under the domain adaptation scenario . we propose a neural network framework that supports domain adaptation for multiple tasks simultaneously , and learns shared representations that better generalize for domain adaptation . we apply the proposed framework to domain adaptation for sequence tagging problems considering two tasks : chinese word segmentation and named entity recognition . experiments show that multi-task domain adaptation works better than disjoint domain adaptation for each task , and achieves the state-of-the-art results for both tasks in the social media domain .", "topics": ["feature learning"]}
{"title": "deep ctr prediction in display advertising", "abstract": "click through rate ( ctr ) prediction of image ads is the core task of online display advertising systems , and logistic regression ( lr ) has been frequently applied as the prediction model . however , lr model lacks the ability of extracting complex and intrinsic nonlinear features from handcrafted high-dimensional image features , which limits its effectiveness . to solve this issue , in this paper , we introduce a novel deep neural network ( dnn ) based model that directly predicts the ctr of an image ad based on raw image pixels and other basic features in one step . the dnn model employs convolution layers to automatically extract representative visual features from images , and nonlinear ctr features are then learned from visual features and other contextual features by using fully-connected layers . empirical evaluations on a real world dataset with over 50 million records demonstrate the effectiveness and efficiency of this method .", "topics": ["nonlinear system", "convolution"]}
{"title": "one-shot adaptation of supervised deep convolutional models", "abstract": "dataset bias remains a significant barrier towards solving real world computer vision tasks . though deep convolutional networks have proven to be a competitive approach for image classification , a question remains : have these models have solved the dataset bias problem ? in general , training or fine-tuning a state-of-the-art deep model on a new domain requires a significant amount of data , which for many applications is simply not available . transfer of models directly to new domains without adaptation has historically led to poor recognition performance . in this paper , we pose the following question : is a single image dataset , much larger than previously explored for adaptation , comprehensive enough to learn general deep models that may be effectively applied to new image domains ? in other words , are deep cnns trained on large amounts of labeled data as susceptible to dataset bias as previous methods have been shown to be ? we show that a generic supervised deep cnn model trained on a large dataset reduces , but does not remove , dataset bias . furthermore , we propose several methods for adaptation with deep models that are able to operate with little ( one example per category ) or no labeled domain specific data . our experiments show that adaptation of deep models on benchmark visual domain adaptation datasets can provide a significant performance boost .", "topics": ["computer vision"]}
{"title": "classifying unordered feature sets with convolutional deep averaging networks", "abstract": "unordered feature sets are a nonstandard data structure that traditional neural networks are incapable of addressing in a principled manner . providing a concatenation of features in an arbitrary order may lead to the learning of spurious patterns or biases that do not actually exist . another complication is introduced if the number of features varies between each set . we propose convolutional deep averaging networks ( cdans ) for classifying and learning representations of datasets whose instances comprise variable-size , unordered feature sets . cdans are efficient , permutation-invariant , and capable of accepting sets of arbitrary size . we emphasize the importance of nonlinear feature embeddings for obtaining effective cdan classifiers and illustrate their advantages in experiments versus linear embeddings and alternative permutation-invariant and -equivariant architectures .", "topics": ["nonlinear system"]}
{"title": "logdet rank minimization with application to subspace clustering", "abstract": "low-rank matrix is desired in many machine learning and computer vision problems . most of the recent studies use the nuclear norm as a convex surrogate of the rank operator . however , all singular values are simply added together by the nuclear norm , and thus the rank may not be well approximated in practical problems . in this paper , we propose to use a log-determinant ( logdet ) function as a smooth and closer , though non-convex , approximation to rank for obtaining a low-rank representation in subspace clustering . augmented lagrange multipliers strategy is applied to iteratively optimize the logdet-based non-convex objective function on potentially large-scale data . by making use of the angular information of principal directions of the resultant low-rank representation , an affinity graph matrix is constructed for spectral clustering . experimental results on motion segmentation and face clustering data demonstrate that the proposed method often outperforms state-of-the-art subspace clustering algorithms .", "topics": ["cluster analysis", "loss function"]}
{"title": "localization under topological uncertainty for lane identification of autonomous vehicles", "abstract": "autonomous vehicles ( avs ) require accurate metric and topological location estimates for safe , effective navigation and decision-making . although many high-definition ( hd ) roadmaps exist , they are not always accurate since public roads are dynamic , shaped unpredictably by both human activity and nature . thus , avs must be able to handle situations in which the topology specified by the map does not agree with reality . we present the variable structure multiple hidden markov model ( vsm-hmm ) as a framework for localizing in the presence of topological uncertainty , and demonstrate its effectiveness on an av where lane membership is modeled as a topological localization process . vsm-hmms use a dynamic set of hmms to simultaneously reason about location within a set of most likely current topologies and therefore may also be applied to topological structure estimation as well as av lane estimation . in addition , we present an extension to the earth mover 's distance which allows uncertainty to be taken into account when computing the distance between belief distributions on simplices of arbitrary relative sizes .", "topics": ["autonomous car"]}
{"title": "a review on deep learning techniques applied to semantic segmentation", "abstract": "image semantic segmentation is more and more being of interest for computer vision and machine learning researchers . many applications on the rise need accurate and efficient segmentation mechanisms : autonomous driving , indoor navigation , and even virtual or augmented reality systems to name a few . this demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision , including semantic segmentation or scene understanding . this paper provides a review on deep learning methods for semantic segmentation applied to various application areas . firstly , we describe the terminology of this field as well as mandatory background concepts . next , the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets . then , existing methods are reviewed , highlighting their contributions and their significance in the field . finally , quantitative results are given for the described methods and the datasets in which they were evaluated , following up with a discussion of the results . at last , we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques .", "topics": ["computer vision", "autonomous car"]}
{"title": "statistical multiresolution estimation for variational imaging : with an application in poisson-biophotonics", "abstract": "in this paper we present a spatially-adaptive method for image reconstruction that is based on the concept of statistical multiresolution estimation as introduced in [ frick k , marnitz p , and munk a . `` statistical multiresolution dantzig estimation in imaging : fundamental concepts and algorithmic framework '' . electron . j . stat . , 6:231-268 , 2012 ] . it constitutes a variational regularization technique that uses an supremum-type distance measure as data-fidelity combined with a convex cost functional . the resulting convex optimization problem is approached by a combination of an inexact alternating direction method of multipliers and dykstra 's projection algorithm . we describe a novel method for balancing data-fit and regularity that is fully automatic and allows for a sound statistical interpretation . the performance of our estimation approach is studied for various problems in imaging . among others , this includes deconvolution problems that arise in poisson nanoscale fluorescence microscopy .", "topics": ["calculus of variations", "mathematical optimization"]}
{"title": "what do we need to build explainable ai systems for the medical domain ?", "abstract": "artificial intelligence ( ai ) generally and machine learning ( ml ) specifically demonstrate impressive practical success in many different application domains , e.g . in autonomous driving , speech recognition , or recommender systems . deep learning approaches , trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks , particularly on playing games such as atari , or mastering the game of go . even in the medical domain there are remarkable results . the central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles , they lack an explicit declarative knowledge representation , hence have difficulty in generating the underlying explanatory structures . this calls for systems enabling to make decisions transparent , understandable and explainable . a huge motivation for our approach are rising legal and privacy aspects . the new european general data protection regulation entering into force on may 25th 2018 , will make black-box approaches difficult to use in business . this does not imply a ban on automatic learning approaches or an obligation to explain everything all the time , however , there must be a possibility to make the results re-traceable on demand . in this paper we outline some of our research topics in the context of the relatively new area of explainable-ai with a focus on the application in medicine , which is a very special domain . this is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data . in this paper we concentrate on three sources : images , *omics data and text . we argue that research in explainable-ai would generally help to facilitate the implementation of ai/ml in the medical domain , and specifically help to facilitate transparency and trust .", "topics": ["reinforcement learning", "speech recognition"]}
{"title": "convolutional neural network-based place recognition", "abstract": "recently convolutional neural networks ( cnns ) have been shown to achieve state-of-the-art performance on various classification tasks . in this paper , we present for the first time a place recognition technique based on cnn models , by combining the powerful features learnt by cnns with a spatial and sequential filter . applying the system to a 70 km benchmark place recognition dataset we achieve a 75 % increase in recall at 100 % precision , significantly outperforming all previous state of the art techniques . we also conduct a comprehensive performance comparison of the utility of features from all 21 layers for place recognition , both for the benchmark dataset and for a second dataset with more significant viewpoint changes .", "topics": ["neural networks"]}
{"title": "generative neuroevolution for deep learning", "abstract": "an important goal for the machine learning ( ml ) community is to create approaches that can learn solutions with human-level capability . one domain where humans have held a significant advantage is visual processing . a significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems , such as artificial neural networks ( anns ) , evolutionary computation ( ec ) , and generative and developmental systems ( gds ) . research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks ; however , these systems have been primarily trained through supervised and unsupervised learning algorithms . alternatively , research is showing that evolution may have a significant role in the development of visual systems . thus this paper investigates the role neuro-evolution ( ne ) can take in deep learning . in particular , the hypercube-based neuroevolution of augmenting topologies is a ne approach that can effectively learn large neural structures by training an indirect encoding that compresses the ann weight pattern as a function of geometry . the results show that hyperneat struggles with performing image classification by itself , but can be effective in training a feature extractor that other ml approaches can learn from . thus neuroevolution combined with other ml methods provides an intriguing area of research that can replicate the processes in nature .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "improving recurrent neural networks for sequence labelling", "abstract": "in this paper we study different types of recurrent neural networks ( rnn ) for sequence labeling tasks . we propose two new variants of rnns integrating improvements for sequence labeling , and we compare them to the more traditional elman and jordan rnns . we compare all models , either traditional or new , on four distinct tasks of sequence labeling : two on spoken language understanding ( atis and media ) ; and two of pos tagging for the french treebank ( ftb ) and the penn treebank ( ptb ) corpora . the results show that our new variants of rnns are always more effective than the others .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "spectral analysis of symmetric and anti-symmetric pairwise kernels", "abstract": "we consider the problem of learning regression functions from pairwise data when there exists prior knowledge that the relation to be learned is symmetric or anti-symmetric . such prior knowledge is commonly enforced by symmetrizing or anti-symmetrizing pairwise kernel functions . through spectral analysis , we show that these transformations reduce the kernel 's effective dimension . further , we provide an analysis of the approximation properties of the resulting kernels , and bound the regularization bias of the kernels in terms of the corresponding bias of the original kernel .", "topics": ["matrix regularization"]}
{"title": "variational continual learning", "abstract": "this paper develops variational continual learning ( vcl ) , a simple but general framework for continual learning that fuses online variational inference ( vi ) and recent advances in monte carlo vi for neural networks . the framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge . experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks , avoiding catastrophic forgetting in a fully automatic way .", "topics": ["calculus of variations"]}
{"title": "constraint reduction using marginal polytope diagrams for map lp relaxations", "abstract": "lp relaxation-based message passing algorithms provide an effective tool for map inference over probabilistic graphical models . however , different lp relaxations often have different objective functions and variables of differing dimensions , which presents a barrier to effective comparison and analysis . in addition , the computational complexity of lp relaxation-based methods grows quickly with the number of constraints . reducing the number of constraints without sacrificing the quality of the solutions is thus desirable . we propose a unified formulation under which existing map lp relaxations may be compared and analysed . furthermore , we propose a new tool called marginal polytope diagrams . some properties of marginal polytope diagrams are exploited such as node redundancy and edge equivalence . we show that using marginal polytope diagrams allows the number of constraints to be reduced without loosening the lp relaxations . then , using marginal polytope diagrams and constraint reduction , we develop three novel message passing algorithms , and demonstrate that two of these show a significant improvement in speed over state-of-art algorithms while delivering a competitive , and sometimes higher , quality of solution .", "topics": ["graphical model", "computational complexity theory"]}
{"title": "piecewise training for undirected models", "abstract": "for many large undirected models that arise in real-world applications , exact maximumlikelihood training is intractable , because it requires computing marginal distributions of the model . conditional training is even more difficult , because the partition function depends not only on the parameters , but also on the observed input , requiring repeated inference over each training example . an appealing idea for such models is to independently train a local undirected classifier over each clique , afterwards combining the learned weights into a single global model . in this paper , we show that this piecewise method can be justified as minimizing a new family of upper bounds on the log partition function . on three natural-language data sets , piecewise training is more accurate than pseudolikelihood , and often performs comparably to global training using belief propagation .", "topics": ["natural language"]}
{"title": "real-time interactive sequence generation and control with recurrent neural network ensembles", "abstract": "recurrent neural networks ( rnn ) , particularly long short term memory ( lstm ) rnns , are a popular and very successful method for learning and generating sequences . however , current generative rnn techniques do not allow real-time interactive control of the sequence generation process , thus are n't well suited for live creative expression . we propose a method of real-time continuous control and 'steering ' of sequence generation using an ensemble of rnns and dynamically altering the mixture weights of the models . we demonstrate the method using character based lstm networks and a gestural interface allowing users to 'conduct ' the generation of text .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "local minima in training of neural networks", "abstract": "there has been a lot of recent interest in trying to characterize the error surface of deep models . this stems from a long standing question . given that deep networks are highly nonlinear systems optimized by local gradient methods , why do they not seem to be affected by bad local minima ? it is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima , or if they exist they need to be close in value to the global minimum . it is known that such results hold under very strong assumptions which are not satisfied by real models . in this paper we present examples showing that for such theorem to be true additional assumptions on the data , initialization schemes and/or the model classes have to be made . we look at the particular case of finite size datasets . we demonstrate that in this scenario one can construct counter-examples ( datasets or initialization schemes ) when the network does become susceptible to bad local minima over the weight space .", "topics": ["nonlinear system", "gradient"]}
{"title": "an empirical comparison of svm and some supervised learning algorithms for vowel recognition", "abstract": "in this article , we conduct a study on the performance of some supervised learning algorithms for vowel recognition . this study aims to compare the accuracy of each algorithm . thus , we present an empirical comparison between five supervised learning classifiers and two combined classifiers : svm , knn , naive bayes , quadratic bayes normal ( qdc ) and nearst mean . those algorithms were tested for vowel recognition using timit corpus and mel-frequency cepstral coefficients ( mfccs ) .", "topics": ["supervised learning", "support vector machine"]}
{"title": "learning latent representations for speech generation and transformation", "abstract": "an ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data . recently , deep probabilistic generative models such as variational autoencoders ( vaes ) have achieved tremendous success in modeling natural images . in this paper , we apply a convolutional vae to model the generative process of natural speech . we derive latent space arithmetic operations to disentangle learned latent representations . we demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations , without the need for parallel supervisory data .", "topics": ["unsupervised learning", "natural language"]}
{"title": "targeting bayes factors with direct-path non-equilibrium thermodynamic integration", "abstract": "thermodynamic integration ( ti ) for computing marginal likelihoods is based on an inverse annealing path from the prior to the posterior distribution . in many cases , the resulting estimator suffers from high variability , which particularly stems from the prior regime . when comparing complex models with differences in a comparatively small number of parameters , intrinsic errors from sampling fluctuations may outweigh the differences in the log marginal likelihood estimates . in the present article , we propose a thermodynamic integration scheme that directly targets the log bayes factor . the method is based on a modified annealing path between the posterior distributions of the two models compared , which systematically avoids the high variance prior regime . we combine this scheme with the concept of non-equilibrium ti to minimise discretisation errors from numerical integration . results obtained on bayesian regression models applied to standard benchmark data , and a complex hierarchical model applied to biopathway inference , demonstrate a significant reduction in estimator variance over state-of-the-art ti methods .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "compositional distributional semantics with long short term memory", "abstract": "we are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture . the extension allows information low in parse trees to be stored in a memory register ( the `memory cell ' ) and used much later higher up in the parse tree . this provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies . experimental results show that our composition outperformed the traditional neural-network composition on the stanford sentiment treebank .", "topics": ["parsing", "gradient"]}
{"title": "nonparametric bayesian topic modelling with the hierarchical pitman-yor processes", "abstract": "the dirichlet process and its extension , the pitman-yor process , are stochastic processes that take probability distributions as a parameter . these processes can be stacked up to form a hierarchical nonparametric bayesian model . in this article , we present efficient methods for the use of these processes in this hierarchical context , and apply them to latent variable models for text analytics . in particular , we propose a general framework for designing these bayesian models , which are called topic models in the computer science community . we then propose a specific nonparametric bayesian topic model for modelling text from social media . we focus on tweets ( posts on twitter ) in this article due to their ease of access . we find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "variational inference with normalizing flows", "abstract": "the choice of approximate posterior distribution is one of the core problems in variational inference . most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference , focusing on mean-field or other simple structured approximations . this restriction has a significant impact on the quality of inferences made using variational methods . we introduce a new approach for specifying flexible , arbitrarily complex and scalable approximate posterior distributions . our approximations are distributions constructed through a normalizing flow , whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained . we use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations . we demonstrate that the theoretical advantages of having posteriors that better match the true posterior , combined with the scalability of amortized variational approaches , provides a clear improvement in performance and applicability of variational inference .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "analyzing language learned by an active question answering agent", "abstract": "we analyze the language learned by an agent trained with reinforcement learning as a component of the activeqa system [ buck et al . , 2017 ] . in activeqa , question answering is framed as a reinforcement learning task in which an agent sits between the user and a black box question-answering system . the agent learns to reformulate the user 's questions to elicit the optimal answers . it probes the system with many versions of a question that are generated via a sequence-to-sequence question reformulation model , then aggregates the returned evidence to find the best answer . this process is an instance of \\emph { machine-machine } communication . the question reformulation model must adapt its language to increase the quality of the answers returned , matching the language of the question answering system . we find that the agent does not learn transformations that align with semantic intuitions but discovers through learning classical information retrieval techniques such as tf-idf re-weighting and stemming .", "topics": ["reinforcement learning"]}
{"title": "separating a real-life nonlinear image mixture", "abstract": "when acquiring an image of a paper document , the image printed on the back page sometimes shows through . the mixture of the front- and back-page images thus obtained is markedly nonlinear , and thus constitutes a good real-life test case for nonlinear blind source separation . this paper addresses a difficult version of this problem , corresponding to the use of `` onion skin '' paper , which results in a relatively strong nonlinearity of the mixture , which becomes close to singular in the lighter regions of the images . the separation is achieved through the misep technique , which is an extension of the well known infomax method . the separation results are assessed with objective quality measures . they show an improvement over the results obtained with linear separation , but have room for further improvement .", "topics": ["nonlinear system"]}
{"title": "cross-domain transfer in reinforcement learning using target apprentice", "abstract": "in this paper , we present a new approach to transfer learning ( tl ) in reinforcement learning ( rl ) for cross-domain tasks . many of the available techniques approach the transfer architecture as a method of speeding up the target task learning . we propose to adapt and reuse the mapped source task optimal-policy directly in related domains . we show the optimal policy from a related source task can be near optimal in target domain provided an adaptive policy accounts for the model error between target and source . the main benefit of this policy augmentation is generalizing policies across multiple related domains without having to re-learn the new tasks . our results show that this architecture leads to better sample efficiency in the transfer , reducing sample complexity of target task learning to target apprentice learning .", "topics": ["reinforcement learning"]}
{"title": "a multivariate discretization method for learning bayesian networks from mixed data", "abstract": "in this paper we address the problem of discretization in the context of learning bayesian networks ( bns ) from data containing both continuous and discrete variables . we describe a new technique for < em > multivariate < /em > discretization , whereby each continuous variable is discretized while taking into account its interaction with the other variables . the technique is based on the use of a bayesian scoring metric that scores the discretization policy for a continuous variable given a bn structure and the observed data . since the metric is relative to the bn structure currently being evaluated , the discretization of a variable needs to be dynamically adjusted as the bn structure changes .", "topics": ["bayesian network"]}
{"title": "deep complex networks", "abstract": "at present , the vast majority of building blocks , techniques , and architectures for deep learning are based on real-valued operations and representations . however , recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms . despite their attractive properties and potential for opening up entirely new neural architectures , complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models . in this work , we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks and convolutional lstms . more precisely , we rely on complex convolutions and present algorithms for complex batch-normalization , complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes . we demonstrate that such complex-valued models are competitive with their real-valued counterparts . we test deep complex models on several computer vision tasks , on music transcription using the musicnet dataset and on speech spectrum prediction using the timit dataset . we achieve state-of-the-art performance on these audio-related tasks .", "topics": ["recurrent neural network", "computer vision"]}
{"title": "deep adaptive network : an efficient deep neural network with sparse binary connections", "abstract": "deep neural networks are state-of-the-art models for understanding the content of images , video and raw input data . however , implementing a deep neural network in embedded systems is a challenging task , because a typical deep neural network , such as a deep belief network using 128x128 images as input , could exhaust giga bytes of memory and result in bandwidth and computing bottleneck . to address this challenge , this paper presents a hardware-oriented deep learning algorithm , named as the deep adaptive network , which attempts to exploit the sparsity in the neural connections . the proposed method adaptively reduces the weights associated with negligible features to zero , leading to sparse feedforward network architecture . furthermore , since the small proportion of important weights are significantly larger than zero , they can be robustly thresholded and represented using single-bit integers ( -1 and +1 ) , leading to implementations of deep neural networks with sparse and binary connections . our experiments showed that , for the application of recognizing mnist handwritten digits , the features extracted by a two-layer deep adaptive network with about 25 % reserved important connections achieved 97.2 % classification accuracy , which was almost the same with the standard deep belief network ( 97.3 % ) . furthermore , for efficient hardware implementations , the sparse-and-binary-weighted deep neural network could save about 99.3 % memory and 99.9 % computation units without significant loss of classification accuracy for pattern recognition applications .", "topics": ["sparse matrix", "computation"]}
{"title": "learning embedding representations for knowledge inference on imperfect and incomplete repositories", "abstract": "this paper considers the problem of knowledge inference on large-scale imperfect repositories with incomplete coverage by means of embedding entities and relations at the first attempt . we propose iike ( imperfect and incomplete knowledge embedding ) , a probabilistic model which measures the probability of each belief , i.e . $ \\langle h , r , t\\rangle $ , in large-scale knowledge bases such as nell and freebase , and our objective is to learn a better low-dimensional vector representation for each entity ( $ h $ and $ t $ ) and relation ( $ r $ ) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning ( nell ) or crowdsouring ( freebase ) , so that we can use $ || { \\bf h } + { \\bf r } - { \\bf t } || $ to assess the plausibility of a belief when conducting inference . we use subsets of those inexact knowledge bases to train our model and test the performances of link prediction and triplet classification on ground truth beliefs , respectively . the results of extensive experiments show that iike achieves significant improvement compared with the baseline and state-of-the-art approaches .", "topics": ["baseline ( configuration management )", "entity"]}
{"title": "bayesian optimization with exponential convergence", "abstract": "this paper presents a bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling . most bayesian optimization methods require auxiliary optimization : an additional non-convex global optimization problem , which can be time-consuming and hard to implement in practice . also , the existing bayesian optimization method with exponential convergence requires access to the delta-cover sampling , which was considered to be impractical . our approach eliminates both requirements and achieves an exponential convergence rate .", "topics": ["sampling ( signal processing )", "optimization problem"]}
{"title": "diffusion methods for classification with pairwise relationships", "abstract": "we define two algorithms for propagating information in classification problems with pairwise relationships . the algorithms are based on contraction maps and are related to non-linear diffusion and random walks on graphs . the approach is also related to message passing algorithms , including belief propagation and mean field methods . the algorithms we describe are guaranteed to converge on graphs with arbitrary topology . moreover they always converge to a unique fixed point , independent of initialization . we prove that the fixed points of the algorithms under consideration define lower-bounds on the energy function and the max-marginals of a markov random field . the theoretical results also illustrate a relationship between message passing algorithms and value iteration for an infinite horizon markov decision process . we illustrate the practical application of the algorithms under study with numerical experiments in image restoration , stereo depth estimation and binary classification on a grid .", "topics": ["mathematical optimization", "nonlinear system"]}
{"title": "improving malware detection accuracy by extracting icon information", "abstract": "detecting pe malware files is now commonly approached using statistical and machine learning models . while these models commonly use features extracted from the structure of pe files , we propose that icons from these files can also help better predict malware . we propose an innovative machine learning approach to extract information from icons . our proposed approach consists of two steps : 1 ) extracting icon features using summary statics , histogram of gradients ( hog ) , and a convolutional autoencoder , 2 ) clustering icons based on the extracted icon features . using publicly available data and by using machine learning experiments , we show our proposed icon clusters significantly boost the efficacy of malware prediction models . in particular , our experiments show an average accuracy increase of 10 % when icon clusters are used in the prediction model .", "topics": ["cluster analysis", "autoencoder"]}
{"title": "multidimensional counting grids : inferring word order from disordered bags of words", "abstract": "models of bags of words typically assume topic mixing so that the words in a single bag come from a limited number of topics . we show here that many sets of bag of words exhibit a very different pattern of variation than the patterns that are efficiently captured by topic mixing . in many cases , from one bag of words to the next , the words disappear and new ones appear as if the theme slowly and smoothly shifted across documents ( providing that the documents are somehow ordered ) . examples of latent structure that describe such ordering are easily imagined . for example , the advancement of the date of the news stories is reflected in a smooth change over the theme of the day as certain evolving news stories fall out of favor and new events create new stories . overlaps among the stories of consecutive days can be modeled by using windows over linearly arranged tight distributions over words . we show here that such strategy can be extended to multiple dimensions and cases where the ordering of data is not readily obvious . we demonstrate that this way of modeling covariation in word occurrences outperforms standard topic models in classification and prediction tasks in applications in biology , text modeling and computer vision .", "topics": ["computer vision"]}
{"title": "not just a black box : learning important features through propagating activation differences", "abstract": "note : this paper describes an older version of deeplift . see https : //arxiv.org/abs/1704.02685 for the newer version . original abstract follows : the purported `` black box '' nature of neural networks is a barrier to adoption in applications where interpretability is essential . here we present deeplift ( learning important features ) , an efficient and effective method for computing importance scores in a neural network . deeplift compares the activation of each neuron to its 'reference activation ' and assigns contribution scores according to the difference . we apply deeplift to models trained on natural images and genomic data , and show significant advantages over gradient-based methods .", "topics": ["gradient"]}
{"title": "a statistical framework for fair predictive algorithms", "abstract": "predictive modeling is increasingly being employed to assist human decision-makers . one purported advantage of replacing human judgment with computer models in high stakes settings -- such as sentencing , hiring , policing , college admissions , and parole decisions -- is the perceived `` neutrality '' of computers . it is argued that because computer models do not hold personal prejudice , the predictions they produce will be equally free from prejudice . there is growing recognition that employing algorithms does not remove the potential for bias , and can even amplify it , since training data were inevitably generated by a process that is itself biased . in this paper , we provide a probabilistic definition of algorithmic bias . we propose a method to remove bias from predictive models by removing all information regarding protected variables from the permitted training data . unlike previous work in this area , our framework is general enough to accommodate arbitrary data types , e.g . binary , continuous , etc . motivated by models currently in use in the criminal justice system that inform decisions on pre-trial release and paroling , we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce `` race-neutral '' predictions of re-arrest . in the process , we demonstrate that the most common approach to creating `` race-neutral '' models -- omitting race as a covariate -- still results in racially disparate predictions . we then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy .", "topics": ["test set"]}
{"title": "theoretical and practical advances on smoothing for extensive-form games", "abstract": "sparse iterative methods , in particular first-order methods , are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games . the convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on . we investigate the acceleration of first-order methods for solving extensive-form games through better design of the dilated entropy function -- -a class of distance-generating functions related to the domains associated with the extensive-form games . by introducing a new weighting scheme for the dilated entropy function , we develop the first distance-generating function for the strategy spaces of sequential games that has no dependence on the branching factor of the player . this result improves the convergence rate of several first-order methods by a factor of $ \\omega ( b^dd ) $ , where $ b $ is the branching factor of the player , and $ d $ is the depth of the game tree . thus far , counterfactual regret minimization methods have been faster in practice , and more popular , than first-order methods despite their theoretically inferior convergence rates . using our new weighting scheme and practical tuning we show that , for the first time , the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm , cfr+ , in practice .", "topics": ["regret ( decision theory )", "sparse matrix"]}
{"title": "automatic renal segmentation in dce-mri using convolutional neural networks", "abstract": "kidney function evaluation using dynamic contrast-enhanced mri ( dce-mri ) images could help in diagnosis and treatment of kidney diseases of children . automatic segmentation of renal parenchyma is an important step in this process . in this paper , we propose a time and memory efficient fully automated segmentation method which achieves high segmentation accuracy with running time in the order of seconds in both normal kidneys and kidneys with hydronephrosis . the proposed method is based on a cascaded application of two 3d convolutional neural networks that employs spatial and temporal information at the same time in order to learn the tasks of localization and segmentation of kidneys , respectively . segmentation performance is evaluated on both normal and abnormal kidneys with varying levels of hydronephrosis . we achieved a mean dice coefficient of 91.4 and 83.6 for normal and abnormal kidneys of pediatric patients , respectively .", "topics": ["time complexity", "coefficient"]}
{"title": "artificial intelligence framework for simulating clinical decision-making : a markov decision process approach", "abstract": "in the modern healthcare system , rapidly expanding costs/complexity , the growing myriad of treatment options , and exploding information streams that often do not effectively reach the front lines hinder the ability to choose optimal treatment decisions over time . the goal in this paper is to develop a general purpose ( non-disease-specific ) computational/artificial intelligence ( ai ) framework to address these challenges . this serves two potential functions : 1 ) a simulation environment for exploring various healthcare policies , payment methodologies , etc . , and 2 ) the basis for clinical artificial intelligence - an ai that can think like a doctor . this approach combines markov decision processes and dynamic decision networks to learn from clinical data and develop complex plans via simulation of alternative sequential decision paths while capturing the sometimes conflicting , sometimes synergistic interactions of various components in the healthcare system . it can operate in partially observable environments ( in the case of missing observations or data ) by maintaining belief states about patient health status and functions as an online agent that plans and re-plans . this framework was evaluated using real patient data from an electronic health record . such an ai framework easily outperforms the current treatment-as-usual ( tau ) case-rate/fee-for-service models of healthcare ( cost per unit change : $ 189 vs. $ 497 ) while obtaining a 30-35 % increase in patient outcomes . tweaking certain model parameters further enhances this advantage , obtaining roughly 50 % more improvement for roughly half the costs . given careful design and problem formulation , an ai simulation framework can approximate optimal decisions even in complex and uncertain environments . future work is described that outlines potential lines of research and integration of machine learning algorithms for personalized medicine .", "topics": ["approximation algorithm", "simulation"]}
{"title": "generalization in machine learning via analytical learning theory", "abstract": "this paper introduces a novel measure-theoretic learning theory to analyze generalization behaviors of practical interest . the proposed learning theory has the following abilities : 1 ) to utilize the qualities of each learned representation on the path from raw inputs to outputs in representation learning , 2 ) to guarantee good generalization errors possibly with arbitrarily rich hypothesis spaces ( e.g . , arbitrarily large capacity and rademacher complexity ) and non-stable/non-robust learning algorithms , and 3 ) to clearly distinguish each individual problem instance from each other . our generalization bounds are relative to a representation of the data , and hold true even if the representation is learned . we discuss several consequences of our results on deep learning , one-shot learning and curriculum learning . unlike statistical learning theory , the proposed learning theory analyzes each problem instance individually via measure theory , rather than a set of problem instances via statistics . because of the differences in the assumptions and the objectives , the proposed learning theory is meant to be complementary to previous learning theory and is not designed to compete with it .", "topics": ["feature learning"]}
{"title": "fast dempster-shafer clustering using a neural network structure", "abstract": "in this paper we study a problem within dempster-shafer theory where 2**n - 1 pieces of evidence are clustered by a neural structure into n clusters . the clustering is done by minimizing a metaconflict function . previously we developed a method based on iterative optimization . however , for large scale problems we need a method with lower computational complexity . the neural structure was found to be effective and much faster than iterative optimization for larger problems . while the growth in metaconflict was faster for the neural structure compared with iterative optimization in medium sized problems , the metaconflict per cluster and evidence was moderate . the neural structure was able to find a global minimum over ten runs for problem sizes up to six clusters .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "pruning nearest neighbor cluster trees", "abstract": "nearest neighbor ( k-nn ) graphs are widely used in machine learning and data mining applications , and our aim is to better understand what they reveal about the cluster structure of the unknown underlying distribution of points . moreover , is it possible to identify spurious structures that might arise due to sampling variability ? our first contribution is a statistical analysis that reveals how certain subgraphs of a k-nn graph form a consistent estimator of the cluster tree of the underlying distribution of points . our second and perhaps most important contribution is the following finite sample guarantee . we carefully work out the tradeoff between aggressive and conservative pruning and are able to guarantee the removal of all spurious cluster structures at all levels of the tree while at the same time guaranteeing the recovery of salient clusters . this is the first such finite sample result in the context of clustering .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "a spectral approach for the design of experiments : design , analysis and algorithms", "abstract": "this paper proposes a new approach to construct high quality space-filling sample designs . first , we propose a novel technique to quantify the space-filling property and optimally trade-off uniformity and randomness in sample designs in arbitrary dimensions . second , we connect the proposed metric ( defined in the spatial domain ) to the objective measure of the design performance ( defined in the spectral domain ) . this connection serves as an analytic framework for evaluating the qualitative properties of space-filling designs in general . using the theoretical insights provided by this spatial-spectral analysis , we derive the notion of optimal space-filling designs , which we refer to as space-filling spectral designs . third , we propose an efficient estimator to evaluate the space-filling properties of sample designs in arbitrary dimensions and use it to develop an optimization framework to generate high quality space-filling designs . finally , we carry out a detailed performance comparison on two different applications in 2 to 6 dimensions : a ) image reconstruction and b ) surrogate modeling on several benchmark optimization functions and an inertial confinement fusion ( icf ) simulation code . we demonstrate that the propose spectral designs significantly outperform existing approaches especially in high dimensions .", "topics": ["numerical analysis", "simulation"]}
{"title": "one-shot and few-shot learning of word embeddings", "abstract": "standard deep learning systems require thousands or millions of examples to learn a concept , and can not integrate new concepts easily . by contrast , humans have an incredible ability to do one-shot or few-shot learning . for instance , from just hearing a word used in a sentence , humans can infer a great deal about it , by leveraging what the syntax and semantics of the surrounding words tells us . here , we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data . this could make natural language processing systems much more flexible , by allowing them to learn continually from the new words they encounter .", "topics": ["natural language processing"]}
{"title": "robust , deep and inductive anomaly detection", "abstract": "pca is a classical statistical technique whose simplicity and maturity has seen it find widespread use as an anomaly detection technique . however , it is limited in this regard by being sensitive to gross perturbations of the input , and by seeking a linear subspace that captures normal behaviour . the first issue has been dealt with by robust pca , a variant of pca that explicitly allows for some data points to be arbitrarily corrupted , however , this does not resolve the second issue , and indeed introduces the new issue that one can no longer inductively find anomalies on a test set . this paper addresses both issues in a single model , the robust autoencoder . this method learns a nonlinear subspace that captures the majority of data points , while allowing for some data to have arbitrary corruption . the model is simple to train and leverages recent advances in the optimisation of deep neural networks . experiments on a range of real-world datasets highlight the model 's effectiveness .", "topics": ["test set", "mathematical optimization"]}
{"title": "bayesian masking : sparse bayesian estimation with weaker shrinkage bias", "abstract": "a common strategy for sparse linear regression is to introduce regularization , which eliminates irrelevant features by letting the corresponding weights be zeros . however , regularization often shrinks the estimator for relevant features , which leads to incorrect feature selection . motivated by the above-mentioned issue , we propose bayesian masking ( bm ) , a sparse estimation method which imposes no regularization on the weights . the key concept of bm is to introduce binary latent variables that randomly mask features . estimating the masking rates determines the relevance of the features automatically . we derive a variational bayesian inference algorithm that maximizes the lower bound of the factorized information criterion ( fic ) , which is a recently developed asymptotic criterion for evaluating the marginal log-likelihood . in addition , we propose reparametrization to accelerate the convergence of the derived algorithm . finally , we show that bm outperforms lasso and automatic relevance determination ( ard ) in terms of the sparsity-shrinkage trade-off .", "topics": ["calculus of variations", "matrix regularization"]}
{"title": "online learning with multiple operator-valued kernels", "abstract": "we consider the problem of learning a vector-valued function f in an online learning setting . the function f is assumed to lie in a reproducing hilbert space of operator-valued kernels . we describe two online algorithms for learning f while taking into account the output structure . a first contribution is an algorithm , onorma , that extends the standard kernel-based online learning algorithm norma from scalar-valued to operator-valued setting . we report a cumulative error bound that holds both for classification and regression . we then define a second algorithm , monorma , which addresses the limitation of pre-defining the output structure in onorma by learning sequentially a linear combination of operator-valued kernels . our experiments show that the proposed algorithms achieve good performance results with low computational cost .", "topics": ["statistical classification"]}
{"title": "a new kernel-based approach to system identification with quantized output data", "abstract": "in this paper we introduce a novel method for linear system identification with quantized output data . we model the impulse response as a zero-mean gaussian process whose covariance ( kernel ) is given by the recently proposed stable spline kernel , which encodes information on regularity and exponential stability . this serves as a starting point to cast our system identification problem into a bayesian framework . we employ markov chain monte carlo methods to provide an estimate of the system . in particular , we design two methods based on the so-called gibbs sampler that allow also to estimate the kernel hyperparameters by marginal likelihood maximization via the expectation-maximization method . numerical simulations show the effectiveness of the proposed scheme , as compared to the state-of-the-art kernel-based methods when these are employed in system identification with quantized data .", "topics": ["kernel ( operating system )", "sampling ( signal processing )"]}
{"title": "unsupervised ranking of multi-attribute objects based on principal curves", "abstract": "unsupervised ranking faces one critical challenge in evaluation applications , that is , no ground truth is available . when pagerank and its variants show a good solution in related subjects , they are applicable only for ranking from link-structure data . in this work , we focus on unsupervised ranking from multi-attribute data which is also common in evaluation tasks . to overcome the challenge , we propose five essential meta-rules for the design and assessment of unsupervised ranking approaches : scale and translation invariance , strict monotonicity , linear/nonlinear capacities , smoothness , and explicitness of parameter size . these meta-rules are regarded as high level knowledge for unsupervised ranking tasks . inspired by the works in [ 8 ] and [ 14 ] , we propose a ranking principal curve ( rpc ) model , which learns a one-dimensional manifold function to perform unsupervised ranking tasks on multi-attribute observations . furthermore , the rpc is modeled to be a cubic b\\'ezier curve with control points restricted in the interior of a hypercube , thereby complying with all the five meta-rules to infer a reasonable ranking list . with control points as the model parameters , one is able to understand the learned manifold and to interpret the ranking list semantically . numerical experiments of the presented rpc model are conducted on two open datasets of different ranking applications . in comparison with the state-of-the-art approaches , the new model is able to show more reasonable ranking lists .", "topics": ["unsupervised learning", "nonlinear system"]}
{"title": "adaptive simulation-based training of ai decision-makers using bayesian optimization", "abstract": "this work studies how an ai-controlled dog-fighting agent with tunable decision-making parameters can learn to optimize performance against an intelligent adversary , as measured by a stochastic objective function evaluated on simulated combat engagements . gaussian process bayesian optimization ( gpbo ) techniques are developed to automatically learn global gaussian process ( gp ) surrogate models , which provide statistical performance predictions in both explored and unexplored areas of the parameter space . this allows a learning engine to sample full-combat simulations at parameter values that are most likely to optimize performance and also provide highly informative data points for improving future predictions . however , standard gpbo methods do not provide a reliable surrogate model for the highly volatile objective functions found in aerial combat , and thus do not reliably identify global maxima . these issues are addressed by novel repeat sampling ( rs ) and hybrid repeat/multi-point sampling ( hrms ) techniques . simulation studies show that hrms improves the accuracy of gp surrogate models , allowing ai decision-makers to more accurately predict performance and efficiently tune parameters .", "topics": ["optimization problem", "loss function"]}
{"title": "a sparse and adaptive prior for time-dependent model parameters", "abstract": "we consider the scenario where the parameters of a probabilistic model are expected to vary over time . we construct a novel prior distribution that promotes sparsity and adapts the strength of correlation between parameters at successive timesteps , based on the data . we derive approximate variational inference procedures for learning and prediction with this prior . we test the approach on two tasks : forecasting financial quantities from relevant text , and modeling language contingent on time-varying financial measurements .", "topics": ["calculus of variations", "approximation algorithm"]}
{"title": "content based video retrieval", "abstract": "content based video retrieval is an approach for facilitating the searching and browsing of large image collections over world wide web . in this approach , video analysis is conducted on low level visual properties extracted from video frame . we believed that in order to create an effective video retrieval system , visual perception must be taken into account . we conjectured that a technique which employs multiple features for indexing and retrieval would be more effective in the discrimination and search tasks of videos . in order to validate this claim , content based indexing and retrieval systems were implemented using color histogram , various texture features and other approaches . videos were stored in oracle 9i database and a user study measured correctness of response .", "topics": ["high- and low-level"]}
{"title": "on the consistency of the bootstrap approach for support vector machines and related kernel based methods", "abstract": "it is shown that bootstrap approximations of support vector machines ( svms ) based on a general convex and smooth loss function and on a general kernel are consistent . this result is useful to approximate the unknown finite sample distribution of svms by the bootstrap approach .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "compatible value gradients for reinforcement learning of continuous deep policies", "abstract": "this paper proposes gprop , a deep reinforcement learning algorithm for continuous policies with compatible function approximation . the algorithm is based on two innovations . firstly , we present a temporal-difference based method for learning the gradient of the value-function . secondly , we present the deviator-actor-critic ( dac ) model , which comprises three neural networks that estimate the value function , its gradient , and determine the actor 's policy respectively . we evaluate gprop on two challenging tasks : a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients ; and the octopus arm , a challenging reinforcement learning benchmark . gprop is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "hessian-based analysis of large batch training and robustness to adversaries", "abstract": "large batch size training of neural networks has been shown to incur accuracy loss when trained with the current methods . the precise underlying reasons for this are still not completely understood . here , we study large batch size training through the lens of the hessian operator and robust optimization . in particular , we perform a hessian based study to analyze how the landscape of the loss functional is different for large batch size training . we compute the true hessian spectrum , without approximation , by back-propagating the second derivative . our results on multiple networks show that , when training at large batch sizes , one tends to stop at points in the parameter space with noticeably higher/larger hessian spectrum , i.e . , where the eigenvalues of the hessian are much larger . we then study how batch size affects robustness of the model in the face of adversarial attacks . all the results show that models trained with large batches are more susceptible to adversarial attacks , as compared to models trained with small batch sizes . furthermore , we prove a theoretical result which shows that the problem of finding an adversarial perturbation is a saddle-free optimization problem . finally , we show empirical results that demonstrate that adversarial training leads to areas with smaller hessian spectrum . we present detailed experiments with five different network architectures tested on mnist , cifar-10 , and cifar-100 datasets .", "topics": ["optimization problem", "mnist database"]}
{"title": "deep networks with internal selective attention through feedback connections", "abstract": "traditional convolutional neural networks ( cnn ) are stationary and feedforward . they neither change their parameters during evaluation nor use feedback from higher to lower layers . real brains , however , do . so does our deep attention selective network ( dasnet ) architecture . dasnets feedback structure can dynamically alter its convolutional filter sensitivities during classification . it harnesses the power of sequential processing to improve classification performance , by allowing the network to iteratively focus its internal attention on some of its convolutional filters . feedback is trained through direct policy search in a huge million-dimensional parameter space , through scalable natural evolution strategies ( snes ) . on the cifar-10 and cifar-100 datasets , dasnet outperforms the previous state-of-the-art model .", "topics": ["scalability"]}
{"title": "a diffusion process on riemannian manifold for visual tracking", "abstract": "robust visual tracking for long video sequences is a research area that has many important applications . the main challenges include how the target image can be modeled and how this model can be updated . in this paper , we model the target using a covariance descriptor , as this descriptor is robust to problems such as pixel-pixel misalignment , pose and illumination changes , that commonly occur in visual tracking . we model the changes in the template using a generative process . we introduce a new dynamical model for the template update using a random walk on the riemannian manifold where the covariance descriptors lie in . this is done using log-transformed space of the manifold to free the constraints imposed inherently by positive semidefinite matrices . modeling template variations and poses kinetics together in the state space enables us to jointly quantify the uncertainties relating to the kinematic states and the template in a principled way . finally , the sequential inference of the posterior distribution of the kinematic states and the template is done using a particle filter . our results shows that this principled approach can be robust to changes in illumination , poses and spatial affine transformation . in the experiments , our method outperformed the current state-of-the-art algorithm - the incremental principal component analysis method , particularly when a target underwent fast poses changes and also maintained a comparable performance in stable target tracking cases .", "topics": ["calculus of variations", "pixel"]}
{"title": "convolutional spike timing dependent plasticity based feature learning in spiking neural networks", "abstract": "brain-inspired learning models attempt to mimic the cortical architecture and computations performed in the neurons and synapses constituting the human brain to achieve its efficiency in cognitive tasks . in this work , we present convolutional spike timing dependent plasticity based feature learning with biologically plausible leaky-integrate-and-fire neurons in spiking neural networks ( snns ) . we use shared weight kernels that are trained to encode representative features underlying the input patterns thereby improving the sparsity as well as the robustness of the learning model . we demonstrate that the proposed unsupervised learning methodology learns several visual categories for object recognition with fewer number of examples and outperforms traditional fully-connected snn architectures while yielding competitive accuracy . additionally , we observe that the learning model performs out-of-set generalization further making the proposed biologically plausible framework a viable and efficient architecture for future neuromorphic applications .", "topics": ["feature learning", "unsupervised learning"]}
{"title": "illuminating search spaces by mapping elites", "abstract": "many fields use search algorithms , which automatically explore a search space to find high-performing solutions : chemists search through the space of molecules to discover new drugs ; engineers search for stronger , cheaper , safer designs , scientists search for models that best explain data , etc . the goal of search algorithms has traditionally been to return the single highest-performing solution in a search space . here we describe a new , fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space . it creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose . this multi-dimensional archive of phenotypic elites ( map-elites ) algorithm illuminates search spaces , allowing researchers to understand how interesting attributes of solutions combine to affect performance , either positively or , equally of interest , negatively . for example , a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary . map-elites produces a large diversity of high-performing , yet qualitatively different solutions , which can be more helpful than a single , high-performing solution . interestingly , because map-elites explores more of the search space , it also tends to find a better overall solution than state-of-the-art search algorithms . we demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots . because map- elites ( 1 ) illuminates the relationship between performance and dimensions of interest in solutions , ( 2 ) returns a set of high-performing , yet diverse solutions , and ( 3 ) improves finding a single , best solution , it will advance science and engineering .", "topics": ["simulation", "robot"]}
{"title": "optimising spatial and tonal data for pde-based inpainting", "abstract": "some recent methods for lossy signal and image compression store only a few selected pixels and fill in the missing structures by inpainting with a partial differential equation ( pde ) . suitable operators include the laplacian , the biharmonic operator , and edge-enhancing anisotropic diffusion ( eed ) . the quality of such approaches depends substantially on the selection of the data that is kept . optimising this data in the domain and codomain gives rise to challenging mathematical problems that shall be addressed in our work . in the 1d case , we prove results that provide insights into the difficulty of this problem , and we give evidence that a splitting into spatial and tonal ( i.e . function value ) optimisation does hardly deteriorate the results . in the 2d setting , we present generic algorithms that achieve a high reconstruction quality even if the specified data is very sparse . to optimise the spatial data , we use a probabilistic sparsification , followed by a nonlocal pixel exchange that avoids getting trapped in bad local optima . after this spatial optimisation we perform a tonal optimisation that modifies the function values in order to reduce the global reconstruction error . for homogeneous diffusion inpainting , this comes down to a least squares problem for which we prove that it has a unique solution . we demonstrate that it can be found efficiently with a gradient descent approach that is accelerated with fast explicit diffusion ( fed ) cycles . our framework allows to specify the desired density of the inpainting mask a priori . moreover , is more generic than other data optimisation approaches for the sparse inpainting problem , since it can also be extended to nonlinear inpainting operators such as eed . this is exploited to achieve reconstructions with state-of-the-art quality . we also give an extensive literature survey on pde-based image compression methods .", "topics": ["mathematical optimization", "gradient descent"]}
{"title": "a probabilistic framework for nonlinearities in stochastic neural networks", "abstract": "we present a probabilistic framework for nonlinearities , based on doubly truncated gaussian distributions . by setting the truncation points appropriately , we are able to generate various types of nonlinearities within a unified framework , including sigmoid , tanh and relu , the most commonly used nonlinearities in neural networks . the framework readily integrates into existing stochastic neural networks ( with hidden units characterized as random variables ) , allowing one for the first time to learn the nonlinearities alongside model weights in these networks . extensive experiments demonstrate the performance improvements brought about by the proposed framework when integrated with the restricted boltzmann machine ( rbm ) , temporal rbm and the truncated gaussian graphical model ( tggm ) .", "topics": ["graphical model", "neural networks"]}
{"title": "brain inspired cognitive model with attention for self-driving cars", "abstract": "perception-driven approach and end-to-end system are two major vision-based frameworks for self-driving cars . however , it is difficult to introduce attention and historical information of autonomous driving process , which are the essential factors for achieving human-like driving into these two methods . in this paper , we propose a novel model for self-driving cars named brain-inspired cognitive model with attention ( cma ) . this model consists of three parts : a convolutional neural network for simulating human visual cortex , a cognitive map built to describe relationships between objects in complex traffic scene and a recurrent neural network that combines with the real-time updated cognitive map to implement attention mechanism and long-short term memory . the benefit of our model is that can accurately solve three tasks simultaneously:1 ) detection of the free space and boundaries of the current and adjacent lanes . 2 ) estimation of obstacle distance and vehicle attitude , and 3 ) learning of driving behavior and decision making from human driver . more significantly , the proposed model could accept external navigating instructions during an end-to-end driving process . for evaluation , we build a large-scale road-vehicle dataset which contains more than forty thousand labeled road images captured by three cameras on our self-driving car . moreover , human driving activities and vehicle states are recorded in the meanwhile .", "topics": ["recurrent neural network", "end-to-end principle"]}
{"title": "manifold relevance determination", "abstract": "in this paper we present a fully bayesian latent variable model which exploits conditional nonlinear ( in ) -dependence structures to learn an efficient latent representation . the latent space is factorized to represent shared and private information from multiple views of the data . in contrast to previous approaches , we introduce a relaxation to the discrete segmentation and allow for a `` softly '' shared latent space . further , bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces . the model is capable of capturing structure underlying extremely high dimensional spaces . this is illustrated by modelling unprocessed images with tenths of thousands of pixels . this also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces . we also demonstrate the model by prediction of human pose in an ambiguous setting . our bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data .", "topics": ["sampling ( signal processing )", "nonlinear system"]}
{"title": "hashing as tie-aware learning to rank", "abstract": "hashing , or learning binary embeddings of data , is frequently used in nearest neighbor retrieval . in this paper , we develop learning to rank formulations for hashing , aimed at directly optimizing ranking-based evaluation metrics such as average precision ( ap ) and normalized discounted cumulative gain ( ndcg ) . we first observe that the integer-valued hamming distance often leads to tied rankings , and propose to use tie-aware versions of ap and ndcg to evaluate hashing for retrieval . then , to optimize tie-aware ranking metrics , we derive their continuous relaxations , and perform gradient-based optimization with deep neural networks . our results establish the new state-of-the-art for image retrieval by hamming ranking in common benchmarks .", "topics": ["gradient descent"]}
{"title": "anusaaraka : machine translation in stages", "abstract": "fully-automatic general-purpose high-quality machine translation systems ( fgh-mt ) are extremely difficult to build . in fact , there is no system in the world for any pair of languages which qualifies to be called fgh-mt . the reasons are not far to seek . translation is a creative process which involves interpretation of the given text by the translator . translation would also vary depending on the audience and the purpose for which it is meant . this would explain the difficulty of building a machine translation system . since , the machine is not capable of interpreting a general text with sufficient accuracy automatically at present - let alone re-expressing it for a given audience , it fails to perform as fgh-mt . footnote { the major difficulty that the machine faces in interpreting a given text is the lack of general world knowledge or common sense knowledge . }", "topics": ["machine translation"]}
{"title": "variational inference via transformations on distributions", "abstract": "variational inference methods often focus on the problem of efficient model optimization , with little emphasis on the choice of the approximating posterior . in this paper , we review and implement the various methods that enable us to develop a rich family of approximating posteriors . we show that one particular method employing transformations on distributions results in developing very rich and complex posterior approximation . we analyze its performance on the mnist dataset by implementing with a variational autoencoder and demonstrate its effectiveness in learning better posterior distributions .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "learning causal bayes networks using interventional path queries in polynomial time and sample complexity", "abstract": "causal discovery from empirical data is a fundamental problem in many scientific domains . observational data allows for identifiability only up to markov equivalence class . in this paper we first propose a polynomial time algorithm for learning the exact correctly-oriented structure of the transitive reduction of any causal bayesian networks with high probability , by using interventional path queries . each path query takes as input an origin node and a target node , and answers whether there is a directed path from the origin to the target . this is done by intervening the origin node and observing samples from the target node . we theoretically show the logarithmic sample complexity for the size of interventional data per path query , for continuous and discrete networks . we further extend our work to learn the transitive edges using logarithmic sample complexity ( albeit in time exponential in the maximum number of parents for discrete networks ) . this allows us to learn the full network . we also provide an analysis of imperfect interventions .", "topics": ["time complexity", "synthetic data"]}
{"title": "sparse nested markov models with log-linear parameters", "abstract": "hidden variables are ubiquitous in practical data analysis , and therefore modeling marginal densities and doing inference with the resulting models is an important problem in statistics , machine learning , and causal inference . recently , a new type of graphical model , called the nested markov model , was developed which captures equality constraints found in marginals of directed acyclic graph ( dag ) models . some of these constraints , such as the so called `verma constraint ' , strictly generalize conditional independence . to make modeling and inference with nested markov models practical , it is necessary to limit the number of parameters in the model , while still correctly capturing the constraints in the marginal of a dag model . placing such limits is similar in spirit to sparsity methods for undirected graphical models , and regression models . in this paper , we give a log-linear parameterization which allows sparse modeling with nested markov models . we illustrate the advantages of this parameterization with a simulation study .", "topics": ["graphical model", "simulation"]}
{"title": "elastic solver : balancing solution time and energy consumption", "abstract": "combinatorial decision problems arise in many different domains such as scheduling , routing , packing , bioinformatics , and many more . despite recent advances in developing scalable solvers , there are still many problems which are often very hard to solve . typically the most advanced solvers include elements which are stochastic in nature . if a same instance is solved many times using different seeds then depending on the inherent characteristics of a problem instance and the solver , one can observe a highly-variant distribution of times spanning multiple orders of magnitude . therefore , to solve a problem instance efficiently it is often useful to solve the same instance in parallel with different seeds . with the proliferation of cloud computing , it is natural to think about an elastic solver which can scale up by launching searches in parallel on thousands of machines ( or cores ) . however , this could result in consuming a lot of energy . moreover , not every instance would require thousands of machines . the challenge is to resolve the tradeoff between solution time and energy consumption optimally for a given problem instance . we analyse the impact of the number of machines ( or cores ) on not only solution time but also on energy consumption . we highlight that although solution time always drops as the number of machines increases , the relation between the number of machines and energy consumption is more complicated . in many cases , the optimal energy consumption may be achieved by a middle ground , we analyse this relationship in detail . the tradeoff between solution time and energy consumption is studied further , showing that the energy consumption of a solver can be reduced drastically if we increase the solution time marginally . we also develop a prediction model , demonstrating that such insights can be exploited to achieve faster solutions times in a more energy efficient manor .", "topics": ["mathematical optimization", "scalability"]}
{"title": "a gradient descent algorithm on the grassman manifold for matrix completion", "abstract": "we consider the problem of reconstructing a low-rank matrix from a small subset of its entries . in this paper , we describe the implementation of an efficient algorithm called optspace , based on singular value decomposition followed by local manifold optimization , for solving the low-rank matrix completion problem . it has been shown that if the number of revealed entries is large enough , the output of singular value decomposition gives a good estimate for the original matrix , so that local optimization reconstructs the correct matrix with high probability . we present numerical results which show that this algorithm can reconstruct the low rank matrix exactly from a very small subset of its entries . we further study the robustness of the algorithm with respect to noise , and its performance on actual collaborative filtering datasets .", "topics": ["numerical analysis", "gradient descent"]}
{"title": "bayesian recurrent neural networks", "abstract": "in this work we explore a straightforward variational bayes scheme for recurrent neural networks . firstly , we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training , also reducing the amount of parameters by 80\\ % . secondly , we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of bayesian rnns . we incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics . we show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train bayesian neural networks . we also empirically demonstrate how bayesian rnns are superior to traditional rnns on a language modelling benchmark and an image captioning task , as well as showing how each of these methods improve our model over a variety of other schemes for training them . we also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared .", "topics": ["calculus of variations", "recurrent neural network"]}
{"title": "unsupervised learning of word-sequence representations from scratch via convolutional tensor decomposition", "abstract": "unsupervised text embeddings extraction is crucial for text understanding in machine learning . word2vec and its variants have received substantial success in mapping words with similar syntactic or semantic meaning to vectors close to each other . however , extracting context-aware word-sequence embedding remains a challenging task . training over large corpus is difficult as labels are difficult to get . more importantly , it is challenging for pre-trained models to obtain word-sequence embeddings that are universally good for all downstream tasks or for any new datasets . we propose a two-phased convdic+deconvdec framework to solve the problem by combining a word-sequence dictionary learning model with a word-sequence embedding decode model . we propose a convolutional tensor decomposition mechanism to learn good word-sequence phrase dictionary in the learning phase . it is proved to be more accurate and much more efficient than the popular alternating minimization method . in the decode phase , we introduce a deconvolution framework that is immune to the problem of varying sentence lengths . the word-sequence embeddings we extracted using convdic+deconvdec are universally good for a few downstream tasks we test on . the framework requires neither pre-training nor prior/outside information .", "topics": ["unsupervised learning", "text corpus"]}
{"title": "incorporating copying mechanism in sequence-to-sequence learning", "abstract": "we address an important problem in sequence-to-sequence ( seq2seq ) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence . a similar phenomenon is observable in human language communication . for example , humans tend to repeat entity names or even long phrases in conversation . the challenge with regard to copying in seq2seq is that new machinery is needed to decide when to perform the operation . in this paper , we incorporate copying into neural network-based seq2seq learning and propose a new model called copynet with encoder-decoder structure . copynet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence . our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of copynet . for example , copynet can outperform regular rnn-based model with remarkable margins on text summarization tasks .", "topics": ["synthetic data"]}
{"title": "unsupervised dependency parsing : let 's use supervised parsers", "abstract": "we present a self-training approach to unsupervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms . our approach , called `iterated reranking ' ( ir ) , starts with dependency trees generated by an unsupervised parser , and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees . our system achieves 1.8 % accuracy higher than the state-of-the-part parser of spitkovsky et al . ( 2013 ) on the wsj corpus .", "topics": ["unsupervised learning", "parsing"]}
{"title": "deep learning models of the retinal response to natural scenes", "abstract": "a central challenge in neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant , natural stimuli . in multilayered neural circuits , nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli . here we demonstrate that deep convolutional neural networks ( cnns ) capture retinal responses to natural scenes nearly to within the variability of a cell 's response , and are markedly more accurate than linear-nonlinear ( ln ) models and generalized linear models ( glms ) . moreover , we find two additional surprising properties of cnns : they are less susceptible to overfitting than their ln counterparts when trained on small amounts of data , and generalize better when tested on stimuli drawn from a different distribution ( e.g . between natural scenes and white noise ) . examination of trained cnns reveals several properties . first , a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise . second , temporally precise responses to slowly varying inputs originate from feedforward inhibition , similar to known retinal mechanisms . third , the injection of latent noise sources in intermediate layers enables our model to capture the sub-poisson spiking variability observed in retinal ganglion cells . fourth , augmenting our cnns with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes . these methods can be readily generalized to other sensory modalities and stimulus ensembles . overall , this work demonstrates that cnns not only accurately capture sensory circuit responses to natural scenes , but also yield information about the circuit 's internal structure and function .", "topics": ["nonlinear system", "map"]}
{"title": "model guided sampling optimization for low-dimensional problems", "abstract": "optimization of very expensive black-box functions requires utilization of maximum information gathered by the process of optimization . model guided sampling optimization ( mgso ) forms a more robust alternative to jones ' gaussian-process-based ego algorithm . instead of ego 's maximizing expected improvement , the mgso uses sampling the probability of improvement which is shown to be helpful against trapping in local minima . further , the mgso can reach close-to-optimum solutions faster than standard optimization algorithms on low dimensional or smooth problems .", "topics": ["sampling ( signal processing )", "mathematical optimization"]}
{"title": "learning to navigate by growing deep networks", "abstract": "adaptability is central to autonomy . intuitively , for high-dimensional learning problems such as navigating based on vision , internal models with higher complexity allow to accurately encode the information available . however , most learning methods rely on models with a fixed structure and complexity . in this paper , we present a self-supervised framework for robots to learn to navigate , without any prior knowledge of the environment , by incrementally building the structure of a deep network as new data becomes available . our framework captures images from a monocular camera and self labels the images to continuously train and predict actions from a computationally efficient adaptive deep architecture based on autoencoders ( ae ) , in a self-supervised fashion . the deep architecture , named reinforced adaptive denoising autoencoders ( ra-dae ) , uses reinforcement learning to dynamically change the network structure by adding or removing neurons . experiments were conducted in simulation and real-world indoor and outdoor environments to assess the potential of self-supervised navigation . ra-dae demonstrates better performance than equivalent non-adaptive deep learning alternatives and can continue to expand its knowledge , trading-off past and present information .", "topics": ["computational complexity theory", "reinforcement learning"]}
{"title": "gaussian process pseudo-likelihood models for sequence labeling", "abstract": "several machine learning problems arising in natural language processing can be modeled as a sequence labeling problem . we provide gaussian process models based on pseudo-likelihood approximation to perform sequence labeling . gaussian processes ( gps ) provide a bayesian approach to learning in a kernel based framework . the pseudo-likelihood model enables one to capture long range dependencies among the output components of the sequence without becoming computationally intractable . we use an efficient variational gaussian approximation method to perform inference in the proposed model . we also provide an iterative algorithm which can effectively make use of the information from the neighboring labels to perform prediction . the ability to capture long range dependencies makes the proposed approach useful for a wide range of sequence labeling problems . numerical experiments on some sequence labeling data sets demonstrate the usefulness of the proposed approach .", "topics": ["calculus of variations", "natural language processing"]}
{"title": "delving deeper into mooc student dropout prediction", "abstract": "in order to obtain reliable accuracy estimates for automatic mooc dropout predictors , it is important to train and test them in a manner consistent with how they will be used in practice . yet most prior research on mooc dropout prediction has measured test accuracy on the same course used for training the classifier , which can lead to overly optimistic accuracy estimates . in order to understand better how accuracy is affected by the training+testing regime , we compared the accuracy of a standard dropout prediction architecture ( clickstream features + logistic regression ) across 4 different training paradigms . results suggest that ( 1 ) training and testing on the same course ( `` post-hoc '' ) can overestimate accuracy by several percentage points ; ( 2 ) dropout classifiers trained on proxy labels based on students ' persistence are surprisingly competitive with post-hoc training ( 87.33 % versus 90.20 % auc averaged over 8 weeks of 40 harvardx moocs ) ; and ( 3 ) classifier performance does not vary significantly with the academic discipline . finally , we also research new dropout prediction architectures based on deep , fully-connected , feed-forward neural networks and find that ( 4 ) networks with as many as 5 hidden layers can statistically significantly increase test accuracy over that of logistic regression .", "topics": ["test set", "statistical classification"]}
{"title": "learning to avoid errors in gans by manipulating input spaces", "abstract": "despite recent advances , large scale visual artifacts are still a common occurrence in images generated by gans . previous work has focused on improving the generator 's capability to accurately imitate the data distribution $ p_ { data } $ . in this paper , we instead explore methods that enable gans to actively avoid errors by manipulating the input space . the core idea is to apply small changes to each noise vector in order to shift them away from areas in the input space that tend to result in errors . we derive three different architectures from that idea . the main one of these consists of a simple residual module that leads to significantly less visual artifacts , while only slightly decreasing diversity . the module is trivial to add to existing gans and costs almost zero computation and memory .", "topics": ["computation"]}
{"title": "automatic detection and categorization of election-related tweets", "abstract": "with the rise in popularity of public social media and micro-blogging services , most notably twitter , the people have found a venue to hear and be heard by their peers without an intermediary . as a consequence , and aided by the public nature of twitter , political scientists now potentially have the means to analyse and understand the narratives that organically form , spread and decline among the public in a political campaign . however , the volume and diversity of the conversation on twitter , combined with its noisy and idiosyncratic nature , make this a hard task . thus , advanced data mining and language processing techniques are required to process and analyse the data . in this paper , we present and evaluate a technical framework , based on recent advances in deep neural networks , for identifying and analysing election-related conversation on twitter on a continuous , longitudinal basis . our models can detect election-related tweets with an f-score of 0.92 and can categorize these tweets into 22 topics with an f-score of 0.90 .", "topics": ["data mining"]}
{"title": "exploring outliers in crowdsourced ranking for qoe", "abstract": "outlier detection is a crucial part of robust evaluation for crowdsourceable assessment of quality of experience ( qoe ) and has attracted much attention in recent years . in this paper , we propose some simple and fast algorithms for outlier detection and robust qoe evaluation based on the nonconvex optimization principle . several iterative procedures are designed with or without knowing the number of outliers in samples . theoretical analysis is given to show that such procedures can reach statistically good estimates under mild conditions . finally , experimental results with simulated and real-world crowdsourcing datasets show that the proposed algorithms could produce similar performance to huber-lasso approach in robust ranking , yet with nearly 8 or 90 times speed-up , without or with a prior knowledge on the sparsity size of outliers , respectively . therefore the proposed methodology provides us a set of helpful tools for robust qoe evaluation with crowdsourcing data .", "topics": ["simulation", "iteration"]}
{"title": "learning gaussian graphical models with fractional marginal pseudo-likelihood", "abstract": "we propose a bayesian approximate inference method for learning the dependence structure of a gaussian graphical model . using pseudo-likelihood , we derive an analytical expression to approximate the marginal likelihood for an arbitrary graph structure without invoking any assumptions about decomposability . the majority of the existing methods for learning gaussian graphical models are either restricted to decomposable graphs or require specification of a tuning parameter that may have a substantial impact on learned structures . by combining a simple sparsity inducing prior for the graph structures with a default reference prior for the model parameters , we obtain a fast and easily applicable scoring function that works well for even high-dimensional data . we demonstrate the favourable performance of our approach by large-scale comparisons against the leading methods for learning non-decomposable gaussian graphical models . a theoretical justification for our method is provided by showing that it yields a consistent estimator of the graph structure .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "learning time series detection models from temporally imprecise labels", "abstract": "in this paper , we consider a new low-quality label learning problem : learning time series detection models from temporally imprecise labels . in this problem , the data consist of a set of input time series , and supervision is provided by a sequence of noisy time stamps corresponding to the occurrence of positive class events . such temporally imprecise labels commonly occur in areas like mobile health research where human annotators are tasked with labeling the occurrence of very short duration events . we propose a general learning framework for this problem that can accommodate different base classifiers and noise models . we present results on real mobile health data showing that the proposed framework significantly outperforms a number of alternatives including assuming that the label time stamps are noise-free , transforming the problem into the multiple instance learning framework , and learning on labels that were manually re-aligned .", "topics": ["time series", "reinforcement learning"]}
{"title": "nearest prime simplicial complex for object recognition", "abstract": "the structure representation of data distribution plays an important role in understanding the underlying mechanism of generating data . in this paper , we propose nearest prime simplicial complex approaches ( nsc ) by utilizing persistent homology to capture such structures . assuming that each class is represented with a prime simplicial complex , we classify unlabeled samples based on the nearest projection distances from the samples to the simplicial complexes . we also extend the extrapolation ability of these complexes with a projection constraint term . experiments in simulated and practical datasets indicate that compared with several published algorithms , the proposed nsc approaches achieve promising performance without losing the structure representation .", "topics": ["simulation"]}
{"title": "skip rnn : learning to skip state updates in recurrent neural networks", "abstract": "recurrent neural networks ( rnns ) continue to show outstanding performance in sequence modeling tasks . however , training rnns on long sequences often face challenges like slow inference , vanishing gradients and difficulty in capturing long term dependencies . in backpropagation through time settings , these issues are tightly coupled with the large , sequential computational graph resulting from unfolding the rnn in time . we introduce the skip rnn model which extends existing rnn models by learning to skip state updates and shortens the effective size of the computational graph . this model can also be encouraged to perform fewer state updates through a budget constraint . we evaluate the proposed model on various tasks and show how it can reduce the number of required rnn updates while preserving , and sometimes even improving , the performance of the baseline rnn models . source code is publicly available at https : //imatge-upc.github.io/skiprnn-2017-telecombcn/ .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "on central tendency and dispersion measures for intervals and hypercubes", "abstract": "the uncertainty or the variability of the data may be treated by considering , rather than a single value for each data , the interval of values in which it may fall . this paper studies the derivation of basic description statistics for interval-valued datasets . we propose a geometrical approach in the determination of summary statistics ( central tendency and dispersion measures ) for interval-valued variables .", "topics": ["value ( ethics )"]}
{"title": "prioritized sweeping neural dynaq with multiple predecessors , and hippocampal replays", "abstract": "during sleep and awake rest , the hippocampus replays sequences of place cells that have been activated during prior experiences . these have been interpreted as a memory consolidation process , but recent results suggest a possible interpretation in terms of reinforcement learning . the dyna reinforcement learning algorithms use off-line replays to improve learning . under limited replay budget , a prioritized sweeping approach , which requires a model of the transitions to the predecessors , can be used to improve performance . we investigate whether such algorithms can explain the experimentally observed replays . we propose a neural network version of prioritized sweeping q-learning , for which we developed a growing multiple expert algorithm , able to cope with multiple predecessors . the resulting architecture is able to improve the learning of simulated agents confronted to a navigation task . we predict that , in animals , learning the world model should occur during rest periods , and that the corresponding replays should be shuffled .", "topics": ["reinforcement learning", "simulation"]}
{"title": "generating retinal flow maps from structural optical coherence tomography with artificial intelligence", "abstract": "despite significant advances in artificial intelligence ( ai ) for computer vision , its application in medical imaging has been limited by the burden and limits of expert-generated labels . we used images from optical coherence tomography angiography ( octa ) , a relatively new imaging modality that measures perfusion of the retinal vasculature , to train an ai algorithm to generate vasculature maps from standard structural optical coherence tomography ( oct ) images of the same retinae , both exceeding the ability and bypassing the need for expert labeling . deep learning was able to infer perfusion of microvasculature from structural oct images with similar fidelity to octa and significantly better than expert clinicians ( p < 0.00001 ) . octa suffers from need of specialized hardware , laborious acquisition protocols , and motion artifacts ; whereas our model works directly from standard oct which are ubiquitous and quick to obtain , and allows unlocking of large volumes of previously collected standard oct data both in existing clinical trials and clinical practice . this finding demonstrates a novel application of ai to medical imaging , whereby subtle regularities between different modalities are used to image the same body part and ai is used to generate detailed and accurate inferences of tissue function from structure imaging .", "topics": ["recurrent neural network", "map"]}
{"title": "end-to-end memory networks", "abstract": "we introduce a neural network with a recurrent attention model over a possibly large external memory . the architecture is a form of memory network ( weston et al . , 2015 ) but unlike the model in that work , it is trained end-to-end , and hence requires significantly less supervision during training , making it more generally applicable in realistic settings . it can also be seen as an extension of rnnsearch to the case where multiple computational steps ( hops ) are performed per output symbol . the flexibility of the model allows us to apply it to tasks as diverse as ( synthetic ) question answering and to language modeling . for the former our approach is competitive with memory networks , but with less supervision . for the latter , on the penn treebank and text8 datasets our approach demonstrates comparable performance to rnns and lstms . in both cases we show that the key concept of multiple computational hops yields improved results .", "topics": ["synthetic data", "end-to-end principle"]}
{"title": "hierarchical question-image co-attention for visual question answering", "abstract": "a number of recent works have proposed attention models for visual question answering ( vqa ) that generate spatial maps highlighting image regions relevant to answering the question . in this paper , we argue that in addition to modeling `` where to look '' or visual attention , it is equally important to model `` what words to listen to '' or question attention . we present a novel co-attention model for vqa that jointly reasons about image and question attention . in addition , our model reasons about the question ( and consequently the image via the co-attention mechanism ) in a hierarchical fashion via a novel 1-dimensional convolution neural networks ( cnn ) . our model improves the state-of-the-art on the vqa dataset from 60.3 % to 60.5 % , and from 61.6 % to 63.3 % on the coco-qa dataset . by using resnet , the performance is further improved to 62.1 % for vqa and 65.4 % for coco-qa .", "topics": ["map", "convolution"]}
{"title": "recurrent neural networks with external memory for language understanding", "abstract": "recurrent neural networks ( rnns ) have become increasingly popular for the task of language understanding . in this task , a semantic tagger is deployed to associate a semantic label to each word in an input sequence . the success of rnn may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away . however , the memory capacity of simple rnns is limited because of the gradient vanishing and exploding problem . we propose to use an external memory to improve memorization capability of rnns . we conducted experiments on the atis dataset , and observed that the proposed model was able to achieve the state-of-the-art results . we compare our proposed model with alternative models and report analysis results that may provide insights for future research .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "on the quality of the initial basin in overspecified neural networks", "abstract": "deep learning , in the form of artificial neural networks , has achieved remarkable practical success in recent years , for a variety of difficult machine learning applications . however , a theoretical explanation for this remains a major open problem , since training neural networks involves optimizing a highly non-convex objective function , and is known to be computationally hard in the worst case . in this work , we study the \\emph { geometric } structure of the associated non-convex objective function , in the context of relu networks and starting from a random initialization of the network parameters . we identify some conditions under which it becomes more favorable to optimization , in the sense of ( i ) high probability of initializing at a point from which there is a monotonically decreasing path to a global minimum ; and ( ii ) high probability of initializing at a basin ( suitably defined ) with a small minimal objective value . a common theme in our results is that such properties are more likely to hold for larger ( `` overspecified '' ) networks , which accords with some recent empirical and theoretical observations .", "topics": ["optimization problem", "neural networks"]}
{"title": "fast graph-based object segmentation for rgb-d images", "abstract": "object segmentation is an important capability for robotic systems , in particular for grasping . we present a graph- based approach for the segmentation of simple objects from rgb-d images . we are interested in segmenting objects with large variety in appearance , from lack of texture to strong textures , for the task of robotic grasping . the algorithm does not rely on image features or machine learning . we propose a modified canny edge detector for extracting robust edges by using depth information and two simple cost functions for combining color and depth cues . the cost functions are used to build an undirected graph , which is partitioned using the concept of internal and external differences between graph regions . the partitioning is fast with o ( nlogn ) complexity . we also discuss ways to deal with missing depth information . we test the approach on different publicly available rgb-d object datasets , such as the rutgers apc rgb-d dataset and the rgb-d object dataset , and compare the results with other existing methods .", "topics": ["robot"]}
{"title": "probabilistic rule realization and selection", "abstract": "abstraction and realization are bilateral processes that are key in deriving intelligence and creativity . in many domains , the two processes are approached through rules : high-level principles that reveal invariances within similar yet diverse examples . under a probabilistic setting for discrete input spaces , we focus on the rule realization problem which generates input sample distributions that follow the given rules . more ambitiously , we go beyond a mechanical realization that takes whatever is given , but instead ask for proactively selecting reasonable rules to realize . this goal is demanding in practice , since the initial rule set may not always be consistent and thus intelligent compromises are needed . we formulate both rule realization and selection as two strongly connected components within a single and symmetric bi-convex problem , and derive an efficient algorithm that works at large scale . taking music compositional rules as the main example throughout the paper , we demonstrate our model 's efficiency in not only music realization ( composition ) but also music interpretation and understanding ( analysis ) .", "topics": ["high- and low-level"]}
{"title": "efficient learning of mixed membership models", "abstract": "we present an efficient algorithm for learning mixed membership models when the number of variables $ p $ is much larger than the number of hidden components $ k $ . this algorithm reduces the computational complexity of state-of-the-art tensor methods , which require decomposing an $ o\\left ( p^3\\right ) $ tensor , to factorizing $ o\\left ( p/k\\right ) $ sub-tensors each of size $ o\\left ( k^3\\right ) $ . in addition , we address the issue of negative entries in the empirical method of moments based estimators . we provide sufficient conditions under which our approach has provable guarantees . our approach obtains competitive empirical results on both simulated and real data .", "topics": ["computational complexity theory", "simulation"]}
{"title": "benchmarking decoupled neural interfaces with synthetic gradients", "abstract": "artifical neural networks are a particular class of learning systems modeled after biological neural functions with an interesting penchant for hebbian learning , that is `` neurons that wire together , fire together '' . however , unlike their natural counterparts , artificial neural networks have a close and stringent coupling between the modules of neurons in the network . this coupling or locking imposes upon the network a strict and inflexible structure that prevent layers in the network from updating their weights until a full feed-forward and backward pass has occurred . such a constraint though may have sufficed for a while , is now no longer feasible in the era of very-large-scale machine learning , coupled with the increased desire for parallelization of the learning process across multiple computing infrastructures . to solve this problem , synthetic gradients ( sg ) with decoupled neural interfaces ( dni ) are introduced as a viable alternative to the backpropagation algorithm . this paper performs a speed benchmark to compare the speed and accuracy capabilities of sg-dni as opposed to a standard neural interface using multilayer perceptron mlp . sg-dni shows good promise , in that it not only captures the learning problem , it is also over 3-fold faster due to it asynchronous learning capabilities .", "topics": ["synthetic data"]}
{"title": "on the identification and mitigation of weaknesses in the knowledge gradient policy for multi-armed bandits", "abstract": "the knowledge gradient ( kg ) policy was originally proposed for online ranking and selection problems but has recently been adapted for use in online decision making in general and multi-armed bandit problems ( mabs ) in particular . we study its use in a class of exponential family mabs and identify weaknesses , including a propensity to take actions which are dominated with respect to both exploitation and exploration . we propose variants of kg which avoid such errors . these new policies include an index heuristic which deploys a kg approach to develop an approximation to the gittins index . a numerical study shows this policy to perform well over a range of mabs including those for which index policies are not optimal . while kg does not make dominated actions when bandits are gaussian , it fails to be index consistent and appears not to enjoy a performance advantage over competitor policies when arms are correlated to compensate for its greater computational demands .", "topics": ["numerical analysis", "gradient"]}
{"title": "dynamic template tracking and recognition", "abstract": "in this paper we address the problem of tracking non-rigid objects whose local appearance and motion changes as a function of time . this class of objects includes dynamic textures such as steam , fire , smoke , water , etc . , as well as articulated objects such as humans performing various actions . we model the temporal evolution of the object 's appearance/motion using a linear dynamical system ( lds ) . we learn such models from sample videos and use them as dynamic templates for tracking objects in novel videos . we pose the problem of tracking a dynamic non-rigid object in the current frame as a maximum a-posteriori estimate of the location of the object and the latent state of the dynamical system , given the current image features and the best estimate of the state in the previous frame . the advantage of our approach is that we can specify a-priori the type of texture to be tracked in the scene by using previously trained models for the dynamics of these textures . our framework naturally generalizes common tracking methods such as ssd and kernel-based tracking from static templates to dynamic templates . we test our algorithm on synthetic as well as real examples of dynamic textures and show that our simple dynamics-based trackers perform at par if not better than the state-of-the-art . since our approach is general and applicable to any image feature , we also apply it to the problem of human action tracking and build action-specific optical flow trackers that perform better than the state-of-the-art when tracking a human performing a particular action . finally , since our approach is generative , we can use a-priori trained trackers for different texture or action classes to simultaneously track and recognize the texture or action in the video .", "topics": ["synthetic data"]}
{"title": "n-body networks : a covariant hierarchical neural network architecture for learning atomic potentials", "abstract": "we describe n-body networks , a neural network architecture for learning the behavior and properties of complex many body physical systems . our specific application is to learn atomic potential energy surfaces for use in molecular dynamics simulations . our architecture is novel in that ( a ) it is based on a hierarchical decomposition of the many body system into subsytems , ( b ) the activations of the network correspond to the internal state of each subsystem , ( c ) the `` neurons '' in the network are constructed explicitly so as to guarantee that each of the activations is covariant to rotations , ( d ) the neurons operate entirely in fourier space , and the nonlinearities are realized by tensor products followed by clebsch-gordan decompositions . as part of the description of our network , we give a characterization of what way the weights of the network may interact with the activations so as to ensure that the covariance property is maintained .", "topics": ["simulation"]}
{"title": "support vector machine active learning algorithms with query-by-committee versus closest-to-hyperplane selection", "abstract": "this paper investigates and evaluates support vector machine active learning algorithms for use with imbalanced datasets , which commonly arise in many applications such as information extraction applications . algorithms based on closest-to-hyperplane selection and query-by-committee selection are combined with methods for addressing imbalance such as positive amplification based on prevalence statistics from initial random samples . three algorithms ( closestpa , qbagpa , and qboostpa ) are presented and carefully evaluated on datasets for text classification and relation extraction . the closestpa algorithm is shown to consistently outperform the other two in a variety of ways and insights are provided as to why this is the case .", "topics": ["support vector machine"]}
{"title": "generative choreography using deep learning", "abstract": "recent advances in deep learning have enabled the extraction of high-level features from raw sensor data which has opened up new possibilities in many different fields , including computer generated choreography . in this paper we present a system chor-rnn for generating novel choreographic material in the nuanced choreographic language and style of an individual choreographer . it also shows promising results in producing a higher level compositional cohesion , rather than just generating sequences of movement . at the core of chor-rnn is a deep recurrent neural network trained on raw motion capture data and that can generate new dance sequences for a solo dancer . chor-rnn can be used for collaborative human-machine choreography or as a creative catalyst , serving as inspiration for a choreographer .", "topics": ["high- and low-level", "recurrent neural network"]}
{"title": "two gaussian approaches to black-box optomization", "abstract": "outline of several strategies for using gaussian processes as surrogate models for the covariance matrix adaptation evolution strategy ( cma-es ) .", "topics": ["sampling ( signal processing )"]}
{"title": "dependency parsing with lstms : an empirical evaluation", "abstract": "we propose a transition-based dependency parser using recurrent neural networks with long short-term memory ( lstm ) units . this extends the feedforward neural network parser of chen and manning ( 2014 ) and enables modelling of entire sequences of shift/reduce transition decisions . on the google web treebank , our lstm parser is competitive with the best feedforward parser on overall accuracy and notably achieves more than 3 % improvement for long-range dependencies , which has proved difficult for previous transition-based parsers due to error propagation and limited context information . our findings additionally suggest that dropout regularisation on the embedding layer is crucial to improve the lstm 's generalisation .", "topics": ["recurrent neural network", "parsing"]}
{"title": "quantized minimum error entropy criterion", "abstract": "comparing with traditional learning criteria , such as mean square error ( mse ) , the minimum error entropy ( mee ) criterion is superior in nonlinear and non-gaussian signal processing and machine learning . the argument of the logarithm in renyis entropy estimator , called information potential ( ip ) , is a popular mee cost in information theoretic learning ( itl ) . the computational complexity of ip is however quadratic in terms of sample number due to double summation . this creates computational bottlenecks especially for large-scale datasets . to address this problem , in this work we propose an efficient quantization approach to reduce the computational burden of ip , which decreases the complexity from o ( n*n ) to o ( mn ) with m < < n. the new learning criterion is called the quantized mee ( qmee ) . some basic properties of qmee are presented . illustrative examples are provided to verify the excellent performance of qmee .", "topics": ["computational complexity theory", "nonlinear system"]}
{"title": "feature learning in deep neural networks - studies on speech recognition tasks", "abstract": "recent studies have shown that deep neural networks ( dnns ) perform significantly better than shallow networks and gaussian mixture models ( gmms ) on large vocabulary speech recognition tasks . in this paper , we argue that the improved accuracy achieved by the dnns is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals . we show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth , which leads to better speech recognition performance with deeper networks . we also show that dnns can not extrapolate to test samples that are substantially different from the training examples . if the training data are sufficiently representative , however , internal features learned by the dnn are relatively stable with respect to speaker differences , bandwidth differences , and environment distortion . this enables dnn-based recognizers to perform as well or better than state-of-the-art systems based on gmms or shallow networks without the need for explicit model adaptation or feature normalization .", "topics": ["test set", "feature learning"]}
{"title": "maximum entropy vector kernels for mimo system identification", "abstract": "recent contributions have framed linear system identification as a nonparametric regularized inverse problem . relying on $ \\ell_2 $ -type regularization which accounts for the stability and smoothness of the impulse response to be estimated , these approaches have been shown to be competitive w.r.t classical parametric methods . in this paper , adopting maximum entropy arguments , we derive a new $ \\ell_2 $ penalty deriving from a vector-valued kernel ; to do so we exploit the structure of the hankel matrix , thus controlling at the same time complexity , measured by the mcmillan degree , stability and smoothness of the identified models . as a special case we recover the nuclear norm penalty on the squared block hankel matrix . in contrast with previous literature on reweighted nuclear norm penalties , our kernel is described by a small number of hyper-parameters , which are iteratively updated through marginal likelihood maximization ; constraining the structure of the kernel acts as a ( hyper ) regularizer which helps controlling the effective degrees of freedom of our estimator . to optimize the marginal likelihood we adapt a scaled gradient projection ( sgp ) algorithm which is proved to be significantly computationally cheaper than other first and second order off-the-shelf optimization methods . the paper also contains an extensive comparison with many state-of-the-art methods on several monte-carlo studies , which confirms the effectiveness of our procedure .", "topics": ["kernel ( operating system )", "time complexity"]}
{"title": "scatter component analysis : a unified framework for domain adaptation and domain generalization", "abstract": "this paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from ( but related to ) the target . two closely related frameworks , domain adaptation and domain generalization , are concerned with such tasks , where the only difference between those frameworks is the availability of the unlabeled target data : domain adaptation can leverage unlabeled target information , while domain generalization can not . we propose scatter component analyis ( sca ) , a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization . sca is based on a simple geometrical measure , i.e . , scatter , which operates on reproducing kernel hilbert space . sca finds a representation that trades between maximizing the separability of classes , minimizing the mismatch between domains , and maximizing the separability of data ; each of which is quantified through scatter . the optimization problem of sca can be reduced to a generalized eigenvalue problem , which results in a fast and exact solution . comprehensive experiments on benchmark cross-domain object recognition datasets verify that sca performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization . we also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation .", "topics": ["test set", "feature learning"]}
{"title": "improved frame level features and svm supervectors approach for the recogniton of emotional states from speech : application to categorical and dimensional states", "abstract": "the purpose of speech emotion recognition system is to classify speakers utterances into different emotional states such as disgust , boredom , sadness , neutral and happiness . speech features that are commonly used in speech emotion recognition rely on global utterance level prosodic features . in our work , we evaluate the impact of frame level feature extraction . the speech samples are from berlin emotional database and the features extracted from these utterances are energy , different variant of mel frequency cepstrum coefficients , velocity and acceleration features .", "topics": ["feature extraction", "speech recognition"]}
{"title": "ecg feature extraction techniques - a survey approach", "abstract": "ecg feature extraction plays a significant role in diagnosing most of the cardiac diseases . one cardiac cycle in an ecg signal consists of the p-qrs-t waves . this feature extraction scheme determines the amplitudes and intervals in the ecg signal for subsequent analysis . the amplitudes and intervals value of p-qrs-t segment determines the functioning of heart of every human . recently , numerous research and techniques have been developed for analyzing the ecg signal . the proposed schemes were mostly based on fuzzy logic methods , artificial neural networks ( ann ) , genetic algorithm ( ga ) , support vector machines ( svm ) , and other signal analysis techniques . all these techniques and algorithms have their advantages and limitations . this proposed paper discusses various techniques and transformations proposed earlier in literature for extracting feature from an ecg signal . in addition this paper also provides a comparative study of various methods proposed by researchers in extracting the feature from ecg signal .", "topics": ["feature extraction"]}
{"title": "context-dependent translation selection using convolutional neural network", "abstract": "we propose a novel method for translation selection in statistical machine translation , in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages . the specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair , but also the context containing the phrase in the source language . therefore , our approach is able to capture context-dependent semantic similarities of translation pairs . we adopt a curriculum learning strategy to train the model : we classify the training examples into easy , medium , and difficult categories , and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult . experimental results show that our approach significantly outperforms the baseline system by up to 1.4 bleu points .", "topics": ["baseline ( configuration management )", "machine translation"]}
{"title": "human trajectory prediction using spatially aware deep attention models", "abstract": "trajectory prediction of dynamic objects is a widely studied topic in the field of artificial intelligence . thanks to a large number of applications like predicting abnormal events , navigation system for the blind , etc . there have been many approaches to attempt learning patterns of motion directly from data using a wide variety of techniques ranging from hand-crafted features to sophisticated deep learning models for unsupervised feature learning . all these approaches have been limited by problems like inefficient features in the case of hand crafted features , large error propagation across the predicted trajectory and no information of static artefacts around the dynamic moving objects . we propose an end to end deep learning model to learn the motion patterns of humans using different navigational modes directly from data using the much popular sequence to sequence model coupled with a soft attention mechanism . we also propose a novel approach to model the static artefacts in a scene and using these to predict the dynamic trajectories . the proposed method , tested on trajectories of pedestrians , consistently outperforms previously proposed state of the art approaches on a variety of large scale data sets . we also show how our architecture can be naturally extended to handle multiple modes of movement ( say pedestrians , skaters , bikers and buses ) simultaneously .", "topics": ["feature learning", "artificial intelligence"]}
{"title": "incorporating literals into knowledge graph embeddings", "abstract": "knowledge graphs , on top of entities and their relationships , contain another important element : literals . literals encode interesting properties ( e.g . the height ) of entities that are not captured by links between entities alone . most of the existing work on embedding ( or latent feature ) based knowledge graph modeling focuses mainly on the relations between entities . in this work , we study the effect of incorporating literal information into existing knowledge graph models . our approach , which we name literale , is an extension that can be plugged into existing latent feature methods . literale merges entity embeddings with their literal information using a learnable , parametrized function , such as a simple linear or nonlinear transformation , or a multilayer neural network . we extend several popular embedding models using literale and evaluate the performance on the task of link prediction . despite its simplicity , literale proves to be an effective way to incorporate literal information into existing embedding based models , improving their performance on different standard datasets , which we augmented with their literals and provide as testbed for further research .", "topics": ["nonlinear system", "entity"]}
{"title": "fuzzy logic based direct torque control of induction motor with space vector modulation", "abstract": "the induction motors have wide range of applications for due to its well-known advantages like brushless structures , low costs and robust performances . over the past years , many kind of control methods are proposed for the induction motors and direct torque control has gained huge importance inside of them due to fast dynamic torque responses and simple control structures . however , the direct torque control method has still some handicaps against the other control methods and most of the important of these handicaps is high torque ripple . this paper suggests a new approach , fuzzy logic based space vector modulation , on the direct torque controlled induction motors and aim of the approach is to overcome high torque ripple disadvantages of conventional direct torque control . in order to test and compare the proposed direct torque control method with conventional direct torque control method simulations , in matlab/simulink , have been carried out in different working conditions . the simulation results showed that a significant improvement in the dynamic torque and speed responses when compared to the conventional direct torque control method .", "topics": ["simulation"]}
{"title": "a family of computationally efficient and simple estimators for unnormalized statistical models", "abstract": "we introduce a new family of estimators for unnormalized statistical models . our family of estimators is parameterized by two nonlinear functions and uses a single sample from an auxiliary distribution , generalizing maximum likelihood monte carlo estimation of geyer and thompson ( 1992 ) . the family is such that we can estimate the partition function like any other parameter in the model . the estimation is done by optimizing an algebraically simple , well defined objective function , which allows for the use of dedicated optimization methods . we establish consistency of the estimator family and give an expression for the asymptotic covariance matrix , which enables us to further analyze the influence of the nonlinearities and the auxiliary density on estimation performance . some estimators in our family are particularly stable for a wide range of auxiliary densities . interestingly , a specific choice of the nonlinearity establishes a connection between density estimation and classification by nonlinear logistic regression . finally , the optimal amount of auxiliary samples relative to the given amount of the data is considered from the perspective of computational efficiency .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "convex relaxations of bregman divergence clustering", "abstract": "although many convex relaxations of clustering have been proposed in the past decade , current formulations remain restricted to spherical gaussian or discriminative models and are susceptible to imbalanced clusters . to address these shortcomings , we propose a new class of convex relaxations that can be flexibly applied to more general forms of bregman divergence clustering . by basing these new formulations on normalized equivalence relations we retain additional control on relaxation quality , which allows improvement in clustering quality . we furthermore develop optimization methods that improve scalability by exploiting recent implicit matrix norm methods . in practice , we find that the new formulations are able to efficiently produce tighter clusterings that improve the accuracy of state of the art methods .", "topics": ["cluster analysis", "scalability"]}
{"title": "iedc : an integrated approach for overlapping and non-overlapping community detection", "abstract": "community detection is a task of fundamental importance in social network analysis that can be used in a variety of knowledge-based domains . while there exist many works on community detection based on connectivity structures , they suffer from either considering the overlapping or non-overlapping communities . in this work , we propose a novel approach for general community detection through an integrated framework to extract the overlapping and non-overlapping community structures without assuming prior structural connectivity on networks . our general framework is based on a primary node based criterion which consists of the internal association degree along with the external association degree . the evaluation of the proposed method is investigated through the extensive simulation experiments and several benchmark real network datasets . the experimental results show that the proposed method outperforms the earlier state-of-the-art algorithms based on the well-known evaluation criteria .", "topics": ["interaction", "simulation"]}
{"title": "sparse dueling bandits", "abstract": "the dueling bandit problem is a variation of the classical multi-armed bandit in which the allowable actions are noisy comparisons between pairs of arms . this paper focuses on a new approach for finding the `` best '' arm according to the borda criterion using noisy comparisons . we prove that in the absence of structural assumptions , the sample complexity of this problem is proportional to the sum of the inverse squared gaps between the borda scores of each suboptimal arm and the best arm . we explore this dependence further and consider structural constraints on the pairwise comparison matrix ( a particular form of sparsity natural to this problem ) that can significantly reduce the sample complexity . this motivates a new algorithm called successive elimination with comparison sparsity ( secs ) that exploits sparsity to find the borda winner using fewer samples than standard algorithms . we also evaluate the new algorithm experimentally with synthetic and real data . the results show that the sparsity model and the new algorithm can provide significant improvements over standard approaches .", "topics": ["synthetic data", "sparse matrix"]}
{"title": "online structure learning for sum-product networks with gaussian leaves", "abstract": "sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable . those properties follow from some conditions ( i.e . , completeness and decomposability ) that must be respected by the structure of the network . as a result , it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice . this paper describes the first online structure learning technique for continuous spns with gaussian leaves . we also introduce an accompanying new parameter learning technique .", "topics": ["graphical model"]}
{"title": "balancing interpretability and predictive accuracy for unsupervised tensor mining", "abstract": "the parafac tensor decomposition has enjoyed an increasing success in exploratory multi-aspect data mining scenarios . a major challenge remains the estimation of the number of latent factors ( i.e . , the rank ) of the decomposition , which yields high-quality , interpretable results . previously , we have proposed an automated tensor mining method which leverages a well-known quality heuristic from the field of chemometrics , the core consistency diagnostic ( corcondia ) , in order to automatically determine the rank for the parafac decomposition . in this work we set out to explore the trade-off between 1 ) the interpretability/quality of the results ( as expressed by corcondia ) , and 2 ) the predictive accuracy of the results , in order to further improve the rank estimation quality . our preliminary results indicate that striking a good balance in that trade-off benefits rank estimation .", "topics": ["data mining", "heuristic"]}
{"title": "context and interference effects in the combinations of natural concepts", "abstract": "the mathematical formalism of quantum theory exhibits significant effectiveness when applied to cognitive phenomena that have resisted traditional ( set theoretical ) modeling . relying on a decade of research on the operational foundations of micro-physical and conceptual entities , we present a theoretical framework for the representation of concepts and their conjunctions and disjunctions that uses the quantum formalism . this framework provides a unified solution to the 'conceptual combinations problem ' of cognitive psychology , explaining the observed deviations from classical ( boolean , fuzzy set and kolmogorovian ) structures in terms of genuine quantum effects . in particular , natural concepts 'interfere ' when they combine to form more complex conceptual entities , and they also exhibit a 'quantum-type context-dependence ' , which are responsible of the 'over- and under-extension ' that are systematically observed in experiments on membership judgments .", "topics": ["entity"]}
{"title": "slice sampling particle belief propagation", "abstract": "inference in continuous label markov random fields is a challenging task . we use particle belief propagation ( pbp ) for solving the inference problem in continuous label space . sampling particles from the belief distribution is typically done by using metropolis-hastings markov chain monte carlo methods which involves sampling from a proposal distribution . this proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence . we propose to avoid dependence on a proposal distribution by introducing a slice sampling based pbp algorithm . the proposed approach shows superior convergence performance on an image denoising toy example . our findings are validated on a challenging relational 2d feature tracking application .", "topics": ["sampling ( signal processing )", "noise reduction"]}
{"title": "feature extraction and automated classification of heartbeats by machine learning", "abstract": "we present algorithms for the detection of a class of heart arrhythmias with the goal of eventual adoption by practicing cardiologists . in clinical practice , detection is based on a small number of meaningful features extracted from the heartbeat cycle . however , techniques proposed in the literature use high dimensional vectors consisting of morphological , and time based features for detection . using electrocardiogram ( ecg ) signals , we found smaller subsets of features sufficient to detect arrhythmias with high accuracy . the features were found by an iterative step-wise feature selection method . we depart from common literature in the following aspects : 1 . as opposed to a high dimensional feature vectors , we use a small set of features with meaningful clinical interpretation , 2. we eliminate the necessity of short-duration patient-specific ecg data to append to the global training data for classification 3 . we apply semi-parametric classification procedures ( in an ensemble framework ) for arrhythmia detection , and 4. our approach is based on a reduced sampling rate of ~ 115 hz as opposed to 360 hz in standard literature .", "topics": ["sampling ( signal processing )", "test set"]}
{"title": "practical riemannian neural networks", "abstract": "we provide the first experimental results on non-synthetic datasets for the quasi-diagonal riemannian gradient descents for neural networks introduced in [ ollivier , 2015 ] . these include the mnist , svhn , and face datasets as well as a previously unpublished electroencephalogram dataset . the quasi-diagonal riemannian algorithms consistently beat simple stochastic gradient gradient descents by a varying margin . the computational overhead with respect to simple backpropagation is around a factor $ 2 $ . perhaps more interestingly , these methods also reach their final performance quickly , thus requiring fewer training epochs and a smaller total computation time . we also present an implementation guide to these riemannian gradient descents for neural networks , showing how the quasi-diagonal versions can be implemented with minimal effort on top of existing routines which compute gradients .", "topics": ["time complexity", "value ( ethics )"]}
{"title": "improving 6d pose estimation of objects in clutter via physics-aware monte carlo tree search", "abstract": "this work proposes a process for efficiently searching over combinations of individual object 6d pose hypotheses in cluttered scenes , especially in cases involving occlusions and objects resting on each other . the initial set of candidate object poses is generated from state-of-the-art object detection and global point cloud registration techniques . the best-scored pose per object by using these techniques may not be accurate due to overlaps and occlusions . nevertheless , experimental indications provided in this work show that object poses with lower ranks may be closer to the real poses than ones with high ranks according to registration techniques . this motivates a global optimization process for improving these poses by taking into account scene-level physical interactions between objects . it also implies that the cartesian product of candidate poses for interacting objects must be searched so as to identify the best scene-level hypothesis . to perform the search efficiently , the candidate poses for each object are clustered so as to reduce their number but still keep a sufficient diversity . then , searching over the combinations of candidate object poses is performed through a monte carlo tree search ( mcts ) process that uses the similarity between the observed depth image of the scene and a rendering of the scene given the hypothesized pose as a score that guides the search procedure . mcts handles in a principled way the tradeoff between fine-tuning the most promising poses and exploring new ones , by using the upper confidence bound ( ucb ) technique . experimental results indicate that this process is able to quickly identify in cluttered scenes physically-consistent object poses that are significantly closer to ground truth compared to poses found by point cloud registration methods .", "topics": ["object detection", "interaction"]}
{"title": "experiments with random projection", "abstract": "recent theoretical work has identified random projection as a promising dimensionality reduction technique for learning mixtures of gausians . here we summarize these results and illustrate them by a wide variety of experiments on synthetic and real data .", "topics": ["synthetic data"]}
{"title": "empirical evaluation of gated recurrent neural networks on sequence modeling", "abstract": "in this paper we compare different types of recurrent units in recurrent neural networks ( rnns ) . especially , we focus on more sophisticated units that implement a gating mechanism , such as a long short-term memory ( lstm ) unit and a recently proposed gated recurrent unit ( gru ) . we evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling . our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units . also , we found gru to be comparable to lstm .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "learning optimal wavelet bases using a neural network approach", "abstract": "a novel method for learning optimal , orthonormal wavelet bases for representing 1- and 2d signals , based on parallels between the wavelet transform and fully connected artificial neural networks , is described . the structural similarities between these two concepts are reviewed and combined to a `` wavenet '' , allowing for the direct learning of optimal wavelet filter coefficient through stochastic gradient descent with back-propagation over ensembles of training inputs , where conditions on the filter coefficients for constituting orthonormal wavelet bases are cast as quadratic regularisations terms . we describe the practical implementation of this method , and study its performance for high-energy physics collision events for qcd $ 2 \\to 2 $ processes . it is shown that an optimal solution is found , even in a high-dimensional search space , and the implications of the result are discussed .", "topics": ["optimization problem", "gradient descent"]}
{"title": "learning using 1-local membership queries", "abstract": "classic machine learning algorithms learn from labelled examples . for example , to design a machine translation system , a typical training set will consist of english sentences and their translation . there is a stronger model , in which the algorithm can also query for labels of new examples it creates . e.g , in the translation task , the algorithm can create a new english sentence , and request its translation from the user during training . this combination of examples and queries has been widely studied . yet , despite many theoretical results , query algorithms are almost never used . one of the main causes for this is a report ( baum and lang , 1992 ) on very disappointing empirical performance of a query algorithm . these poor results were mainly attributed to the fact that the algorithm queried for labels of examples that are artificial , and impossible to interpret by humans . in this work we study a new model of local membership queries ( awasthi et al . , 2012 ) , which tries to resolve the problem of artificial queries . in this model , the algorithm is only allowed to query the labels of examples which are close to examples from the training set . e.g . , in translation , the algorithm can change individual words in a sentence it has already seen , and then ask for the translation . in this model , the examples queried by the algorithm will be close to natural examples and hence , hopefully , will not appear as artificial or random . we focus on 1-local queries ( i.e . , queries of distance 1 from an example in the training sample ) . we show that 1-local membership queries are already stronger than the standard learning model . we also present an experiment on a well known nlp task of sentiment analysis . in this experiment , the users were asked to provide more information than merely indicating the label . we present results that illustrate that this extra information is beneficial in practice .", "topics": ["test set", "natural language processing"]}
{"title": "correlated-pca : principal components ' analysis when data and noise are correlated", "abstract": "given a matrix of observed data , principal components analysis ( pca ) computes a small number of orthogonal directions that contain most of its variability . provably accurate solutions for pca have been in use for a long time . however , to the best of our knowledge , all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent , or at least uncorrelated . this is valid in practice often , but not always . in this paper , we study the pca problem in the setting where the data and noise can be correlated . such noise is often also referred to as `` data-dependent noise '' . we obtain a correctness result for the standard eigenvalue decomposition ( evd ) based solution to pca under simple assumptions on the data-noise correlation . we also develop and analyze a generalization of evd , cluster-evd , that improves upon evd in certain regimes .", "topics": ["cluster analysis"]}
{"title": "repeated inverse reinforcement learning", "abstract": "we introduce a novel repeated inverse reinforcement learning problem : the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted . each time the human is surprised , the agent is provided a demonstration of the desired behavior by the human . we formalize this problem , including how the sequence of tasks is chosen , in a few different ways and provide some foundational results .", "topics": ["reinforcement learning"]}
{"title": "efficient delivery policy to minimize user traffic consumption in guaranteed advertising", "abstract": "in this work , we study the guaranteed delivery model which is widely used in online display advertising . in the guaranteed delivery scenario , ad exposures ( which are also called impressions in some works ) to users are guaranteed by contracts signed in advance between advertisers and publishers . a crucial problem for the advertising platform is how to fully utilize the valuable user traffic to generate as much as possible revenue . different from previous works which usually minimize the penalty of unsatisfied contracts and some other cost ( e.g . representativeness ) , we propose the novel consumption minimization model , in which the primary objective is to minimize the user traffic consumed to satisfy all contracts . under this model , we develop a near optimal method to deliver ads for users . the main advantage of our method lies in that it consumes nearly as least as possible user traffic to satisfy all contracts , therefore more contracts can be accepted to produce more revenue . it also enables the publishers to estimate how much user traffic is redundant or short so that they can sell or buy this part of traffic in bulk in the exchange market . furthermore , it is robust with regard to priori knowledge of user type distribution . finally , the simulation shows that our method outperforms the traditional state-of-the-art methods .", "topics": ["simulation"]}
{"title": "video ladder networks", "abstract": "we present the video ladder network ( vln ) for efficiently generating future video frames . vln is a neural encoder-decoder model augmented at all layers by both recurrent and feedforward lateral connections . at each layer , these connections form a lateral recurrent residual block , where the feedforward connection represents a skip connection and the recurrent connection represents the residual . thanks to the recurrent connections , the decoder can exploit temporal summaries generated from all layers of the encoder . this way , the top layer is relieved from the pressure of modeling lower-level spatial and temporal details . furthermore , we extend the basic version of vln to incorporate resnet-style residual blocks in the encoder and decoder , which help improving the prediction results . vln is trained in self-supervised regime on the moving mnist dataset , achieving competitive results while having very simple structure and providing fast inference .", "topics": ["encoder", "mnist database"]}
{"title": "a comparative study of gaussian mixture model and radial basis function for voice recognition", "abstract": "a comparative study of the application of gaussian mixture model ( gmm ) and radial basis function ( rbf ) in biometric recognition of voice has been carried out and presented . the application of machine learning techniques to biometric authentication and recognition problems has gained a widespread acceptance . in this research , a gmm model was trained , using expectation maximization ( em ) algorithm , on a dataset containing 10 classes of vowels and the model was used to predict the appropriate classes using a validation dataset . for experimental validity , the model was compared to the performance of two different versions of rbf model using the same learning and validation datasets . the results showed very close recognition accuracy between the gmm and the standard rbf model , but with gmm performing better than the standard rbf by less than 1 % and the two models outperformed similar models reported in literature . the dtreg version of rbf outperformed the other two models by producing 94.8 % recognition accuracy . in terms of recognition time , the standard rbf was found to be the fastest among the three models .", "topics": ["artificial intelligence"]}
{"title": "solving a path planning problem in a partially known environment using a swarm algorithm", "abstract": "this paper proposes a path planning strategy for an autonomous ground vehicle ( agv ) navigating in a partially known environment . global path planning is performed by first using a spatial database of the region to be traversed containing selected attributes such as height data and soil information from a suitable spatial database . the database is processed using a biomimetic swarm algorithm that is inspired by the nest building strategies followed by termites . local path planning is performed online utilizing information regarding contingencies that affect the safe navigation of the agv from various sensors . the simulation discussed has been implemented on the open source player-stage-gazebo platform .", "topics": ["simulation", "sensor"]}
{"title": "reliable force aggregation using a refined evidence specification from dempster-shafer clustering", "abstract": "in this paper we develop methods for selection of templates and use these templates to recluster an already performed dempster-shafer clustering taking into account intelligence to template fit during the reclustering phase . by this process the risk of erroneous force aggregation based on some misplace pieces of evidence from the first clustering process is greatly reduced . finally , a more reliable force aggregation is performed using the result of the second clustering . these steps are taken in order to maintain most of the excellent computational performance of dempster-shafer clustering , while at the same time improve on the clustering result by including some higher relations among intelligence reports described by the templates . the new improved algorithm has a computational complexity of o ( n**3 log**2 n ) compared to o ( n**2 log**2 n ) of standard dempster-shafer clustering using potts spin mean field theory .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "constructing datasets for multi-hop reading comprehension across documents", "abstract": "most reading comprehension methods limit themselves to queries which can be answered using a single sentence , paragraph , or document . enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods , but currently there exist no resources to train and test this capability . we propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods . in our task , a model learns to seek and combine evidence - effectively performing multi-hop ( alias multi-step ) inference . we devise a methodology to produce datasets for this task , given a collection of query-answer pairs and thematically linked documents . two datasets from different domains are induced , and we identify potential pitfalls and devise circumvention strategies . we evaluate two previously proposed competitive models and find that one can integrate information across documents . however , both models struggle to select relevant information , as providing documents guaranteed to be relevant greatly improves their performance . while the models outperform several strong baselines , their best accuracy reaches 42.9 % compared to human performance at 74.0 % - leaving ample room for improvement .", "topics": ["baseline ( configuration management )"]}
{"title": "modular proximal optimization for multidimensional total-variation regularization", "abstract": "we study \\emph { tv regularization } , a widely used technique for eliciting structured sparsity . in particular , we propose efficient algorithms for computing prox-operators for $ \\ell_p $ -norm tv . the most important among these is $ \\ell_1 $ -norm tv , for whose prox-operator we present a new geometric analysis which unveils a hitherto unknown connection to taut-string methods . this connection turns out to be remarkably useful as it shows how our geometry guided implementation results in efficient weighted and unweighted 1d-tv solvers , surpassing state-of-the-art methods . our 1d-tv solvers provide the backbone for building more complex ( two or higher-dimensional ) tv solvers within a modular proximal optimization approach . we review the literature for an array of methods exploiting this strategy , and illustrate the benefits of our modular design through extensive suite of experiments on ( i ) image denoising , ( ii ) image deconvolution , ( iii ) four variants of fused-lasso , and ( iv ) video denoising . to underscore our claims and permit easy reproducibility , we provide all the reviewed and our new tv solvers in an easy to use multi-threaded c++ , matlab and python library .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "audio visual emotion recognition with temporal alignment and perception attention", "abstract": "this paper focuses on two key problems for audio-visual emotion recognition in the video . one is the audio and visual streams temporal alignment for feature level fusion . the other one is locating and re-weighting the perception attentions in the whole audio-visual stream for better recognition . the long short term memory recurrent neural network ( lstm-rnn ) is employed as the main classification architecture . firstly , soft attention mechanism aligns the audio and visual streams . secondly , seven emotion embedding vectors , which are corresponding to each classification emotion type , are added to locate the perception attentions . the locating and re-weighting process is also based on the soft attention mechanism . the experiment results on emotiw2015 dataset and the qualitative analysis show the efficiency of the proposed two techniques .", "topics": ["recurrent neural network"]}
{"title": "q-learning with basic emotions", "abstract": "q-learning is a simple and powerful tool in solving dynamic problems where environments are unknown . it uses a balance of exploration and exploitation to find an optimal solution to the problem . in this paper , we propose using four basic emotions : joy , sadness , fear , and anger to influence a qlearning agent . simulations show that the proposed affective agent requires lesser number of steps to find the optimal path . we found when affective agent finds the optimal path , the ratio between exploration to exploitation gradually decreases , indicating lower total step count in the long run", "topics": ["optimization problem", "simulation"]}
{"title": "enhancenet : single image super-resolution through automated texture synthesis", "abstract": "single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input . traditionally , the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio ( psnr ) which have been shown to correlate poorly with the human perception of image quality . as a result , algorithms minimizing these metrics tend to produce over-smoothed images that lack high-frequency textures and do not look natural despite yielding high psnr values . we propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixel-accurate reproduction of ground truth images during training . by using feed-forward fully convolutional neural networks in an adversarial training setting , we achieve a significant boost in image quality at high magnification ratios . extensive experiments on a number of datasets show the effectiveness of our approach , yielding state-of-the-art results in both quantitative and qualitative benchmarks .", "topics": ["ground truth", "pixel"]}
{"title": "expected policy gradients for reinforcement learning", "abstract": "we propose expected policy gradients ( epg ) , which unify stochastic policy gradients ( spg ) and deterministic policy gradients ( dpg ) for reinforcement learning . inspired by expected sarsa , epg integrates ( or sums ) across actions when estimating the gradient , instead of relying only on the action in the sampled trajectory . for continuous action spaces , we first derive a practical result for gaussian policies and quadric critics and then extend it to an analytical method for the universal case , covering a broad class of actors and critics , including gaussian , exponential families , and reparameterised policies with bounded support . for gaussian policies , we show that it is optimal to explore using covariance proportional to the matrix exponential of the scaled hessian of the critic with respect to the actions . epg also provides a general framework for reasoning about policy gradient methods , which we use to establish a new general policy gradient theorem , of which the stochastic and deterministic policy gradient theorems are special cases . furthermore , we prove that epg reduces the variance of the gradient estimates without requiring deterministic policies and with little computational overhead . finally , we show that epg outperforms existing approaches on six challenging domains involving the simulated control of physical systems .", "topics": ["reinforcement learning", "simulation"]}
{"title": "adaptive learning in cartesian product of reproducing kernel hilbert spaces", "abstract": "we propose a novel adaptive learning algorithm based on iterative orthogonal projections in the cartesian product of multiple reproducing kernel hilbert spaces ( rkhss ) . the task is estimating/tracking nonlinear functions which are supposed to contain multiple components such as ( i ) linear and nonlinear components , ( ii ) high- and low- frequency components etc . in this case , the use of multiple rkhss permits a compact representation of multicomponent functions . the proposed algorithm is where two different methods of the author meet : multikernel adaptive filtering and the algorithm of hyperplane projection along affine subspace ( hypass ) . in a certain particular case , the sum space of the rkhss is isomorphic to the product space and hence the proposed algorithm can also be regarded as an iterative projection method in the sum space . the efficacy of the proposed algorithm is shown by numerical examples .", "topics": ["nonlinear system", "numerical analysis"]}
{"title": "towards learning to perceive and reason about liquids", "abstract": "recent advances in ai and robotics have claimed many incredible results with deep learning , yet no work to date has applied deep learning to the problem of liquid perception and reasoning . in this paper , we apply fully-convolutional deep neural networks to the tasks of detecting and tracking liquids . we evaluate three models : a single-frame network , multi-frame network , and a lstm recurrent network . our results show that the best liquid detection results are achieved when aggregating data over multiple frames and that the lstm network outperforms the other two in both tasks . this suggests that lstm-based neural networks have the potential to be a key component for enabling robots to handle liquids using robust , closed-loop controllers .", "topics": ["recurrent neural network", "robot"]}
{"title": "automatic sleep stage scoring with single-channel eeg using convolutional neural networks", "abstract": "we used convolutional neural networks ( cnns ) for automatic sleep stage scoring based on single-channel electroencephalography ( eeg ) to learn task-specific filters for classification without using prior domain knowledge . we used an openly available dataset from 20 healthy young adults for evaluation and applied 20-fold cross-validation . we used class-balanced random sampling within the stochastic gradient descent ( sgd ) optimization of the cnn to avoid skewed performance in favor of the most represented sleep stages . we achieved high mean f1-score ( 81 % , range 79-83 % ) , mean accuracy across individual sleep stages ( 82 % , range 80-84 % ) and overall accuracy ( 74 % , range 71-76 % ) over all subjects . by analyzing and visualizing the filters that our cnn learns , we found that rules learned by the filters correspond to sleep scoring criteria in the american academy of sleep medicine ( aasm ) manual that human experts follow . our method 's performance is balanced across classes and our results are comparable to state-of-the-art methods with hand-engineered features . we show that , without using prior domain knowledge , a cnn can automatically learn to distinguish among different normal sleep stages .", "topics": ["neural networks", "gradient descent"]}
{"title": "zhusuan : a library for bayesian deep learning", "abstract": "in this paper we introduce zhusuan , a python probabilistic programming library for bayesian deep learning , which conjoins the complimentary advantages of bayesian methods and deep learning . zhusuan is built upon tensorflow . unlike existing deep learning libraries , which are mainly designed for deterministic neural networks and supervised tasks , zhusuan is featured for its deep root into bayesian inference , thus supporting various kinds of probabilistic models , including both the traditional hierarchical bayesian models and recent deep generative models . we use running examples to illustrate the probabilistic programming on zhusuan , including bayesian logistic regression , variational auto-encoders , deep sigmoid belief networks and bayesian recurrent neural networks .", "topics": ["recurrent neural network", "calculus of variations"]}
{"title": "persian wordnet construction using supervised learning", "abstract": "this paper presents an automated supervised method for persian wordnet construction . using a persian corpus and a bi-lingual dictionary , the initial links between persian words and princeton wordnet synsets have been generated . these links will be discriminated later as correct or incorrect by employing seven features in a trained classification system . the whole method is just a classification system , which has been trained on a train set containing farsnet as a set of correct instances . state of the art results on the automatically derived persian wordnet is achieved . the resulted wordnet with a precision of 91.18 % includes more than 16,000 words and 22,000 synsets .", "topics": ["supervised learning", "text corpus"]}
{"title": "a comparison of linear and non-linear calibrations for speaker recognition", "abstract": "in recent work on both generative and discriminative score to log-likelihood-ratio calibration , it was shown that linear transforms give good accuracy only for a limited range of operating points . moreover , these methods required tailoring of the calibration training objective functions in order to target the desired region of best accuracy . here , we generalize the linear recipes to non-linear ones . we experiment with a non-linear , non-parametric , discriminative pav solution , as well as parametric , generative , maximum-likelihood solutions that use gaussian , student 's t and normal-inverse-gaussian score distributions . experiments on nist sre'12 scores suggest that the non-linear methods provide wider ranges of optimal accuracy and can be trained without having to resort to objective function tailoring .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "normalized direction-preserving adam", "abstract": "optimization algorithms for training deep models not only affects the convergence rate and stability of the training process , but are also highly related to the generalization performance of the models . while adaptive algorithms , such as adam and rmsprop , have shown better optimization performance than stochastic gradient descent ( sgd ) in many scenarios , they often lead to worse generalization performance than sgd , when used for training deep neural networks ( dnns ) . in this work , we identify two problems of adam that may degrade the generalization performance . as a solution , we propose the normalized direction-preserving adam ( nd-adam ) algorithm , which combines the best of both worlds , i.e . , the good optimization performance of adam , and the good generalization performance of sgd . in addition , we further improve the generalization performance in classification tasks , by using batch-normalized softmax . this study suggests the need for more precise control over the training process of dnns .", "topics": ["gradient descent", "gradient"]}
{"title": "approximate kalman filter q-learning for continuous state-space mdps", "abstract": "we seek to learn an effective policy for a markov decision process ( mdp ) with continuous states via q-learning . given a set of basis functions over state action pairs we search for a corresponding set of linear weights that minimizes the mean bellman residual . our algorithm uses a kalman filter model to estimate those weights and we have developed a simpler approximate kalman filter model that outperforms the current state of the art projected td-learning methods on several standard benchmark problems .", "topics": ["approximation algorithm", "markov chain"]}
{"title": "lose the views : limited angle ct reconstruction via implicit sinogram completion", "abstract": "computed tomography ( ct ) reconstruction is a fundamental component to a wide variety of applications ranging from security , to healthcare . the classical techniques require measuring projections , called sinograms , from a full 180 $ ^\\circ $ view of the object . this is impractical in a limited angle scenario , when the viewing angle is less than 180 $ ^\\circ $ , which can occur due to different factors including restrictions on scanning time , limited flexibility of scanner rotation , etc . the sinograms obtained as a result , cause existing techniques to produce highly artifact-laden reconstructions . in this paper , we propose to address this problem through implicit sinogram completion , on a challenging real world dataset containing scans of common checked-in luggage . we propose a system , consisting of 1d and 2d convolutional neural networks , that operates on a limited angle sinogram to directly produce the best estimate of a reconstruction . next , we use the x-ray transform on this reconstruction to obtain a `` completed '' sinogram , as if it came from a full 180 $ ^\\circ $ measurement . we feed this to standard analytical and iterative reconstruction techniques to obtain the final reconstruction . we show with extensive experimentation that this combined strategy outperforms many competitive baselines . we also propose a measure of confidence for the reconstruction that enables a practitioner to gauge the reliability of a prediction made by our network . we show that this measure is a strong indicator of quality as measured by the psnr , while not requiring ground truth at test time . finally , using a segmentation experiment , we show that our reconstruction preserves the 3d structure of objects effectively .", "topics": ["baseline ( configuration management )", "ground truth"]}
{"title": "word representations , tree models and syntactic functions", "abstract": "word representations induced from models with discrete latent variables ( e.g.\\ hmms ) have been shown to be beneficial in many nlp applications . in this work , we exploit labeled syntactic dependency trees and formalize the induction problem as unsupervised learning of tree-structured hidden markov models . syntactic functions are used as additional observed variables in the model , influencing both transition and emission components . such syntactic information can potentially lead to capturing more fine-grain and functional distinctions between words , which , in turn , may be desirable in many nlp applications . we evaluate the word representations on two tasks -- named entity recognition and semantic frame identification . we observe improvements from exploiting syntactic function information in both cases , and the results rivaling those of state-of-the-art representation learning methods . additionally , we revisit the relationship between sequential and unlabeled-tree models and find that the advantage of the latter is not self-evident .", "topics": ["feature learning", "natural language processing"]}
{"title": "feature control as intrinsic motivation for hierarchical reinforcement learning", "abstract": "the problem of sparse rewards is one of the hardest challenges in contemporary reinforcement learning . hierarchical reinforcement learning ( hrl ) tackles this problem by using a set of temporally-extended actions , or options , each of which has its own subgoal . these subgoals are normally handcrafted for specific tasks . here , though , we introduce a generic class of subgoals with broad applicability in the visual domain . underlying our approach ( in common with work using `` auxiliary tasks '' ) is the hypothesis that the ability to control aspects of the environment is an inherently useful skill to have . we incorporate such subgoals in an end-to-end hierarchical reinforcement learning system and test two variants of our algorithm on a number of games from the atari suite . we highlight the advantage of our approach in one of the hardest games -- montezuma 's revenge -- for which the ability to handle sparse rewards is key . our agent learns several times faster than the current state-of-the-art hrl agent in this game , reaching a similar level of performance . update 22/11/17 : we found that a standard a3c agent with a simple shaped reward , i.e . extrinsic reward + feature control intrinsic reward , has comparable performance to our agent in montezuma revenge . in light of the new experiments performed , the advantage of our hrl approach can be attributed more to its ability to learn useful features from intrinsic rewards rather than its ability to explore and reuse abstracted skills with hierarchical components . this has led us to a new conclusion about the result .", "topics": ["feature learning", "reinforcement learning"]}
{"title": "the projected power method : an efficient algorithm for joint alignment from pairwise differences", "abstract": "various applications involve assigning discrete label values to a collection of objects based on some pairwise noisy data . due to the discrete -- -and hence nonconvex -- -structure of the problem , computing the optimal assignment ( e.g.~maximum likelihood assignment ) becomes intractable at first sight . this paper makes progress towards efficient computation by focusing on a concrete joint alignment problem -- -that is , the problem of recovering $ n $ discrete variables $ x_i \\in \\ { 1 , \\cdots , m\\ } $ , $ 1\\leq i\\leq n $ given noisy observations of their modulo differences $ \\ { x_i - x_j~\\mathsf { mod } ~m\\ } $ . we propose a low-complexity and model-free procedure , which operates in a lifted space by representing distinct label values in orthogonal directions , and which attempts to optimize quadratic functions over hypercubes . starting with a first guess computed via a spectral method , the algorithm successively refines the iterates via projected power iterations . we prove that for a broad class of statistical models , the proposed projected power method makes no error -- -and hence converges to the maximum likelihood estimate -- -in a suitable regime . numerical experiments have been carried out on both synthetic and real data to demonstrate the practicality of our algorithm . we expect this algorithmic framework to be effective for a broad range of discrete assignment problems .", "topics": ["value ( ethics )", "numerical analysis"]}
{"title": "emergence-focused design in complex system simulation", "abstract": "emergence is a phenomenon taken for granted in science but also still not well understood . we have developed a model of artificial genetic evolution intended to allow for emergence on genetic , population and social levels . we present the details of the current state of our environment , agent , and reproductive models . in developing our models we have relied on a principle of using non-linear systems to model as many systems as possible including mutation and recombination , gene-environment interaction , agent metabolism , agent survival , resource gathering and sexual reproduction . in this paper we review the genetic dynamics that have emerged in our system including genotype-phenotype divergence , genetic drift , pseudogenes , and gene duplication . we conclude that emergence-focused design in complex system simulation is necessary to reproduce the multilevel emergence seen in the natural world .", "topics": ["nonlinear system", "simulation"]}
{"title": "weakly supervised plda training", "abstract": "plda is a popular normalization approach for the i-vector model , and it has delivered state-of-the-art performance in speaker verification . however , plda training requires a large amount of labelled development data , which is highly expensive in most cases . we present a cheap plda training approach , which assumes that speakers in the same session can be easily separated , and speakers in different sessions are simply different . this results in `weak labels ' which are not fully accurate but cheap , leading to a weak plda training . our experimental results on real-life large-scale telephony customer service achieves demonstrated that the weak training can offer good performance when human-labelled data are limited . more interestingly , the weak training can be employed as a discriminative adaptation approach , which is more efficient than the prevailing unsupervised method when human-labelled data are insufficient .", "topics": ["unsupervised learning"]}
{"title": "a lower bound for the optimization of finite sums", "abstract": "this paper presents a lower bound for optimizing a finite sum of $ n $ functions , where each function is $ l $ -smooth and the sum is $ \\mu $ -strongly convex . we show that no algorithm can reach an error $ \\epsilon $ in minimizing all functions from this class in fewer than $ \\omega ( n + \\sqrt { n ( \\kappa-1 ) } \\log ( 1/\\epsilon ) ) $ iterations , where $ \\kappa=l/\\mu $ is a surrogate condition number . we then compare this lower bound to upper bounds for recently developed methods specializing to this setting . when the functions involved in this sum are not arbitrary , but based on i.i.d . random data , then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum . the conclusion we draw is that a lot of caution is necessary for an accurate comparison , and identify machine learning scenarios where the new methods help computationally .", "topics": ["iteration"]}
{"title": "wavelet methods for shape perception in electro-sensing", "abstract": "this paper aims at presenting a new approach to the electro-sensing problem using wavelets . it provides an efficient algorithm for recognizing the shape of a target from micro-electrical impedance measurements . stability and resolution capabilities of the proposed algorithm are quantified in numerical simulations .", "topics": ["numerical analysis", "simulation"]}
{"title": "minimizing the misclassification error rate using a surrogate convex loss", "abstract": "we carefully study how well minimizing convex surrogate loss functions , corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors . in particular , we show that amongst all convex surrogate losses , the hinge loss gives essentially the best possible bound , of all convex loss functions , for the misclassification error rate of the resulting linear predictor in terms of the best possible margin error rate . we also provide lower bounds for specific convex surrogates that show how different commonly used losses qualitatively differ from each other .", "topics": ["loss function"]}
{"title": "differentiable pooling for unsupervised acoustic model adaptation", "abstract": "we present a deep neural network ( dnn ) acoustic model that includes parametrised and differentiable pooling operators . unsupervised acoustic model adaptation is cast as the problem of updating the decision boundaries implemented by each pooling operator . in particular , we experiment with two types of pooling parametrisations : learned $ l_p $ -norm pooling and weighted gaussian pooling , in which the weights of both operators are treated as speaker-dependent . we perform investigations using three different large vocabulary speech recognition corpora : ami meetings , ted talks and switchboard conversational telephone speech . we demonstrate that differentiable pooling operators provide a robust and relatively low-dimensional way to adapt acoustic models , with relative word error rates reductions ranging from 5 -- 20 % with respect to unadapted systems , which themselves are better than the baseline fully-connected dnn-based acoustic models . we also investigate how the proposed techniques work under various adaptation conditions including the quality of adaptation data and complementarity to other feature- and model-space adaptation methods , as well as providing an analysis of the characteristics of each of the proposed approaches .", "topics": ["baseline ( configuration management )", "unsupervised learning"]}
{"title": "spatio-temporal modeling of users ' check-ins in location-based social networks", "abstract": "social networks are getting closer to our real physical world . people share the exact location and time of their check-ins and are influenced by their friends . modeling the spatio-temporal behavior of users in social networks is of great importance for predicting the future behavior of users , controlling the users ' movements , and finding the latent influence network . it is observed that users have periodic patterns in their movements . also , they are influenced by the locations that their close friends recently visited . leveraging these two observations , we propose a probabilistic model based on a doubly stochastic point process with a periodic decaying kernel for the time of check-ins and a time-varying multinomial distribution for the location of check-ins of users in the location-based social networks . we learn the model parameters using an efficient em algorithm , which distributes over the users . experiments on synthetic and real data gathered from foursquare show that the proposed inference algorithm learns the parameters efficiently and our model outperforms the other alternatives in the prediction of time and location of check-ins .", "topics": ["synthetic data"]}
{"title": "a feasible roadmap for developing volumetric probability atlas of localized prostate cancer", "abstract": "a statistical volumetric model , showing the probability map of localized prostate cancer within the host anatomical structure , has been developed from 90 optically-imaged surgical specimens . this master model permits an accurate characterization of prostate cancer distribution patterns and an atlas-informed biopsy sampling strategy . the model is constructed by mapping individual prostate models onto a site model , together with localized tumors . an accurate multi-object non-rigid warping scheme is developed based on a mixture of principal-axis registrations . we report our evaluation and pilot studies on the effectiveness of the method and its application to optimizing needle biopsy strategies .", "topics": ["image processing", "simulation"]}
{"title": "deconvolutional latent-variable model for text sequence matching", "abstract": "a latent-variable model is introduced for text matching , inferring sentence representations by jointly optimizing generative and discriminative objectives . to alleviate typical optimization challenges in latent-variable models for text , we employ deconvolutional networks as the sequence decoder ( generator ) , providing learned latent codes with more semantic information and better generalization . our model , trained in an unsupervised manner , yields stronger empirical predictive performance than a decoder based on long short-term memory ( lstm ) , with less parameters and considerably faster training . further , we apply it to text sequence-matching problems . the proposed model significantly outperforms several strong sentence-encoding baselines , especially in the semi-supervised setting .", "topics": ["unsupervised learning"]}
{"title": "markov logic networks for natural language question answering", "abstract": "our goal is to answer elementary-level science questions using knowledge extracted automatically from science textbooks , expressed in a subset of first-order logic . given the incomplete and noisy nature of these automatically extracted rules , markov logic networks ( mlns ) seem a natural model to use , but the exact way of leveraging mlns is by no means obvious . we investigate three ways of applying mlns to our task . in the first , we simply use the extracted science rules directly as mln clauses . unlike typical mln applications , our domain has long and complex rules , leading to an unmanageable number of groundings . we exploit the structure present in hard constraints to improve tractability , but the formulation remains ineffective . in the second approach , we instead interpret science rules as describing prototypical entities , thus mapping rules directly to grounded mln assertions , whose constants are then clustered using existing entity resolution methods . this drastically simplifies the network , but still suffers from brittleness . finally , our third approach , called praline , uses mlns to align the lexical elements as well as define and control how inference should be performed in this task . our experiments , demonstrating a 15\\ % accuracy boost and a 10x reduction in runtime , suggest that the flexibility and different inference semantics of praline are a better fit for the natural language question answering task .", "topics": ["natural language", "entity"]}
{"title": "woah : preliminaries to zero-shot ontology learning for conversational agents", "abstract": "the present paper presents the weighted ontology approximation heuristic ( woah ) , a novel zero-shot approach to ontology estimation for conversational agents development environments . this methodology extracts verbs and nouns separately from data by distilling the dependencies obtained and applying similarity and sparsity metrics to generate an ontology estimation configurable in terms of the level of generalization .", "topics": ["approximation", "sparse matrix"]}
{"title": "hierarchical attentive recurrent tracking", "abstract": "class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models can not be learned a priori . inspired by how the human visual cortex employs spatial attention and separate `` where '' and `` what '' processing pathways to actively suppress irrelevant visual features , this work develops a hierarchical attentive recurrent model for single object tracking in videos . the first layer of attention discards the majority of background by selecting a region containing the object of interest , while the subsequent layers tune in on visual features particular to the tracked object . this framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods . to improve training convergence , we augment the loss function with terms for a number of auxiliary tasks relevant for tracking . evaluation of the proposed model is performed on two datasets : pedestrian tracking on the kth activity recognition dataset and the more difficult kitti object tracking dataset .", "topics": ["recurrent neural network", "loss function"]}
{"title": "a novel model-based heuristic for energy optimal motion planning for automated driving", "abstract": "predictive motion planning is the key to achieve energy-efficient driving , which is one of the main benefits of automated driving . researchers have been studying the planning of velocity trajectories , a simpler form of motion planning , for over a decade now and many different methods are available . dynamic programming has shown to be the most common choice due to its numerical background and ability to include nonlinear constraints and models . although planning of an optimal trajectory is done in a systematic way , dynamic programming does not use any knowledge about the considered problem to guide the exploration and therefore explores all possible trajectories . a* is a search algorithm which enables using knowledge about the problem to guide the exploration to the most promising solutions first . knowledge has to be represented in a form of a heuristic function , which gives an optimistic estimate of cost for transitioning to the final state , which is not a straightforward task . this paper presents a novel heuristics incorporating air drag and auxiliary power as well as operational costs of the vehicle , besides kinetic and potential energy and rolling resistance known in the literature . furthermore , optimal cruising velocity , which depends on vehicle aerodynamic properties and auxiliary power , is derived . results are compared for different variants of heuristic functions and dynamic programming as well .", "topics": ["nonlinear system", "numerical analysis"]}
{"title": "fooling ocr systems with adversarial text images", "abstract": "we demonstrate that state-of-the-art optical character recognition ( ocr ) based on deep learning is vulnerable to adversarial images . minor modifications to images of printed text , which do not change the meaning of the text to a human reader , cause the ocr system to `` recognize '' a different text where certain words chosen by the adversary are replaced by their semantic opposites . this completely changes the meaning of the output produced by the ocr system and by the nlp applications that use ocr for preprocessing their inputs .", "topics": ["natural language processing"]}
{"title": "flying triangulation - towards the 3d movie camera", "abstract": "flying triangulation sensors enable a free-hand and motion-robust 3d data acquisition of complex shaped objects . the measurement principle is based on a multi-line light-sectioning approach and uses sophisticated algorithms for real-time registration ( s. ettl et al . , appl . opt . 51 ( 2012 ) 281-289 ) . as `` single-shot principle '' , light sectioning enables the option to get surface data from one single camera exposure . but there is a drawback : a pixel-dense measurement is not possible because of fundamental information-theoretical reasons . by `` pixel-dense '' we understand that each pixel displays individually measured distance information , neither interpolated from its neighbour pixels nor using lateral context information . hence , for monomodal single-shot principles , the 3d data generated from one 2d raw image display a significantly lower space-bandwidth than the camera permits . this is the price one must pay for motion robustness . currently , our sensors project about 10 lines ( each with 1000 pixels ) , reaching an considerable lower data efficiency than theoretically possible for a single-shot sensor . our aim is to push flying triangulation to its information-theoretical limits . therefore , the line density as well as the measurement depth needs to be significantly increased . this causes serious indexing ambiguities . on the road to a single-shot 3d movie camera , we are working on solutions to overcome the problem of false line indexing by utilizing yet unexploited information . we will present several approaches and will discuss profound information-theoretical questions about the information efficiency of 3d sensors .", "topics": ["sensor", "pixel"]}
{"title": "empirical evaluation of rectified activations in convolutional network", "abstract": "in this paper we investigate the performance of different types of rectified activation functions in convolutional neural network : standard rectified linear unit ( relu ) , leaky rectified linear unit ( leaky relu ) , parametric rectified linear unit ( prelu ) and a new randomized leaky rectified linear units ( rrelu ) . we evaluate these activation function on standard image classification task . our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results . thus our findings are negative on the common belief that sparsity is the key of good performance in relu . moreover , on small scale dataset , using deterministic negative slope or learning it are both prone to overfitting . they are not as effective as using their randomized counterpart . by using rrelu , we achieved 75.68\\ % accuracy on cifar-100 test set without multiple test or ensemble .", "topics": ["computer vision", "sparse matrix"]}
{"title": "computational ghost imaging using deep learning", "abstract": "computational ghost imaging ( cgi ) is a single-pixel imaging technique that exploits the correlation between known random patterns and the measured intensity of light transmitted ( or reflected ) by an object . although cgi can obtain two- or three- dimensional images with a single or a few bucket detectors , the quality of the reconstructed images is reduced by noise due to the reconstruction of images from random patterns . in this study , we improve the quality of cgi images using deep learning . a deep neural network is used to automatically learn the features of noise-contaminated cgi images . after training , the network is able to predict low-noise images from new noise-contaminated cgi images .", "topics": ["pixel"]}
{"title": "find my mug : efficient object search with a mobile robot using semantic segmentation", "abstract": "in this paper , we propose an efficient semantic segmentation framework for indoor scenes , tailored to the application on a mobile robot . semantic segmentation can help robots to gain a reasonable understanding of their environment , but to reach this goal , the algorithms not only need to be accurate , but also fast and robust . therefore , we developed an optimized 3d point cloud processing framework based on a randomized decision forest , achieving competitive results at sufficiently high frame rates . we evaluate the capabilities of our method on the popular nyu depth dataset and our own data and demonstrate its feasibility by deploying it on a mobile service robot , for which we could optimize an object search procedure using our results .", "topics": ["robot"]}
{"title": "unsupervised learning with truncated gaussian graphical models", "abstract": "gaussian graphical models ( ggms ) are widely used for statistical modeling , because of ease of inference and the ubiquitous use of the normal distribution in practical approximations . however , they are also known for their limited modeling abilities , due to the gaussian assumption . in this paper , we introduce a novel variant of ggms , which relaxes the gaussian restriction and yet admits efficient inference . specifically , we impose a bipartite structure on the ggm and govern the hidden variables by truncated normal distributions . the nonlinearity of the model is revealed by its connection to rectified linear unit ( relu ) neural networks . meanwhile , thanks to the bipartite structure and appealing properties of truncated normals , we are able to train the models efficiently using contrastive divergence . we consider three output constructs , accounting for real-valued , binary and count data . we further extend the model to deep constructions and show that deep models can be used for unsupervised pre-training of rectifier neural networks . extensive experimental results are provided to validate the proposed models and demonstrate their superiority over competing models .", "topics": ["graphical model", "unsupervised learning"]}
{"title": "unit commitment using nearest neighbor as a short-term proxy", "abstract": "we devise the unit commitment nearest neighbor ( ucnn ) algorithm to be used as a proxy for quickly approximating outcomes of short-term decisions , to make tractable hierarchical long-term assessment and planning for large power systems . experimental results on updated versions of ieee-rts79 and ieee-rts96 show high accuracy measured on operational cost , achieved in runtimes that are lower in several orders of magnitude than the traditional approach .", "topics": ["approximation algorithm"]}
{"title": "a critical investigation of deep reinforcement learning for navigation", "abstract": "the navigation problem is classically approached in two steps : an exploration step , where map-information about the environment is gathered ; and an exploitation step , where this information is used to navigate efficiently . deep reinforcement learning ( drl ) algorithms , alternatively , approach the problem of navigation in an end-to-end fashion . inspired by the classical approach , we ask whether drl algorithms are able to inherently explore , gather and exploit map-information over the course of navigation . we build upon mirowski et al . [ 2017 ] work and introduce a systematic suite of experiments that vary three parameters : the agent 's starting location , the agent 's target location , and the maze structure . we choose evaluation metrics that explicitly measure the algorithm 's ability to gather and exploit map-information . our experiments show that when trained and tested on the same maps , the algorithm successfully gathers and exploits map-information . however , when trained and tested on different sets of maps , the algorithm fails to transfer the ability to gather and exploit map-information to unseen maps . furthermore , we find that when the goal location is randomized and the map is kept static , the algorithm is able to gather and exploit map-information but the exploitation is far from optimal . we open-source our experimental suite in the hopes that it serves as a framework for the comparison of future algorithms and leads to the discovery of robust alternatives to classical navigation methods .", "topics": ["reinforcement learning", "map"]}
{"title": "proceedings 6th international workshop on theorem proving components for educational software", "abstract": "the 6th international workshop on theorem proving components for educational software ( thedu'17 ) was held in gothenburg , sweden , on 6 aug 2017 . it was associated to the conference cade26 . topics of interest include : methods of automated deduction applied to checking students ' input ; methods of automated deduction applied to prove post-conditions for particular problem solutions ; combinations of deduction and computation enabling systems to propose next steps ; automated provers specific for dynamic geometry systems ; proof and proving in mathematics education . thedu'17 was a vibrant workshop , with one invited talk and eight contributions . it triggered the post-proceedings at hand .", "topics": ["computation"]}
{"title": "diving into the shallows : a computational perspective on large-scale shallow learning", "abstract": "in this paper we first identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels . an analysis based on the spectral properties of the kernel demonstrates that only a vanishingly small portion of the function space is reachable after a polynomial number of gradient descent iterations . this lack of approximating power drastically limits gradient descent for a fixed computational budget leading to serious over-regularization/underfitting . the issue is purely algorithmic , persisting even in the limit of infinite data . to address this shortcoming in practice , we introduce eigenpro iteration , based on a preconditioning scheme using a small number of approximately computed eigenvectors . it can also be viewed as learning a new kernel optimized for gradient descent . it turns out that injecting this small ( computationally inexpensive and sgd-compatible ) amount of approximate second-order information leads to major improvements in convergence . for large data , this translates into significant performance boost over the standard kernel methods . in particular , we are able to consistently match or improve the state-of-the-art results recently reported in the literature with a small fraction of their computational budget . finally , we feel that these results show a need for a broader computational perspective on modern large-scale learning to complement more traditional statistical and convergence analyses . in particular , many phenomena of large-scale high-dimensional inference are best understood in terms of optimization on infinite dimensional hilbert spaces , where standard algorithms can sometimes have properties at odds with finite-dimensional intuition . a systematic analysis concentrating on the approximation power of such algorithms within a budget of computation may lead to progress both in theory and practice .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "wavelet scattering on the pitch spiral", "abstract": "we present a new representation of harmonic sounds that linearizes the dynamics of pitch and spectral envelope , while remaining stable to deformations in the time-frequency plane . it is an instance of the scattering transform , a generic operator which cascades wavelet convolutions and modulus nonlinearities . it is derived from the pitch spiral , in that convolutions are successively performed in time , log-frequency , and octave index . we give a closed-form approximation of spiral scattering coefficients for a nonstationary generalization of the harmonic source-filter model .", "topics": ["convolution", "coefficient"]}
{"title": "glasgow 's stereo image database of garments", "abstract": "to provide insight into cloth perception and manipulation with an active binocular robotic vision system , we compiled a database of 80 stereo-pair colour images with corresponding horizontal and vertical disparity maps and mask annotations , for 3d garment point cloud rendering has been created and released . the stereo-image garment database is part of research conducted under the eu-fp7 clothes perception and manipulation ( clopema ) project and belongs to a wider database collection released through clopema ( www.clopema.eu ) . this database is based on 16 different off-the-shelve garments . each garment has been imaged in five different pose configurations on the project 's binocular robot head . a full copy of the database is made available for scientific research only at https : //sites.google.com/site/ugstereodatabase/ .", "topics": ["map", "robot"]}
{"title": "solutions of quadratic first-order odes applied to computer vision problems", "abstract": "the article proves the existence of a maximum of two possible solutions to the initial value problem composed by the planar-perspective equation and an initial condition . this initial value problem has a geometric interpretation . solutions are curves than pass trough the initial condition which is a point of the plane .", "topics": ["nonlinear system", "loss function"]}
{"title": "characterizing driving styles with deep learning", "abstract": "characterizing driving styles of human drivers using vehicle sensor data , e.g . , gps , is an interesting research problem and an important real-world requirement from automotive industries . a good representation of driving features can be highly valuable for autonomous driving , auto insurance , and many other application scenarios . however , traditional methods mainly rely on handcrafted features , which limit machine learning algorithms to achieve a better performance . in this paper , we propose a novel deep learning solution to this problem , which could be the first attempt of extending deep learning to driving behavior analysis based on gps data . the proposed approach can effectively extract high level and interpretable features describing complex driving patterns . it also requires significantly less human experience and work . the power of the learned driving style representations are validated through the driver identification problem using a large real dataset .", "topics": ["autonomous car"]}
{"title": "distributed flexible nonlinear tensor factorization", "abstract": "tensor factorization is a powerful tool to analyse multi-way data . compared with traditional multi-linear methods , nonlinear tensor factorization models are capable of capturing more complex relationships in the data . however , they are computationally expensive and may suffer severe learning bias in case of extreme data sparsity . to overcome these limitations , in this paper we propose a distributed , flexible nonlinear tensor factorization model . our model can effectively avoid the expensive computations and structural restrictions of the kronecker-product in existing tgp formulations , allowing an arbitrary subset of tensorial entries to be selected to contribute to the training . at the same time , we derive a tractable and tight variational evidence lower bound ( elbo ) that enables highly decoupled , parallel computations and high-quality inference . based on the new bound , we develop a distributed inference algorithm in the mapreduce framework , which is key-value-free and can fully exploit the memory cache mechanism in fast mapreduce systems such as spark . experimental results fully demonstrate the advantages of our method over several state-of-the-art approaches , in terms of both predictive performance and computational efficiency . moreover , our approach shows a promising potential in the application of click-through-rate ( ctr ) prediction for online advertising .", "topics": ["calculus of variations", "nonlinear system"]}
{"title": "automatic selection of t-sne perplexity", "abstract": "t-distributed stochastic neighbor embedding ( t-sne ) is one of the most widely used dimensionality reduction methods for data visualization , but it has a perplexity hyperparameter that requires manual selection . in practice , proper tuning of t-sne perplexity requires users to understand the inner working of the method as well as to have hands-on experience . we propose a model selection objective for t-sne perplexity that requires negligible extra computation beyond that of the t-sne itself . we empirically validate that the perplexity settings found by our approach are consistent with preferences elicited from human experts across a number of datasets . the similarities of our approach to bayesian information criteria ( bic ) and minimum description length ( mdl ) are also analyzed .", "topics": ["computation"]}
{"title": "photosensor oculography : survey and parametric analysis of designs using model-based simulation", "abstract": "this paper presents a renewed overview of photosensor oculography ( psog ) , an eye-tracking technique based on the principle of using simple photosensors to measure the amount of reflected ( usually infrared ) light when the eye rotates . photosensor oculography can provide measurements with high precision , low latency and reduced power consumption , and thus it appears as an attractive option for performing eye-tracking in the emerging head-mounted interaction devices , e.g . augmented and virtual reality ( ar/vr ) headsets . in our current work we employ an adjustable simulation framework as a common basis for performing an exploratory study of the eye-tracking behavior of different photosensor oculography designs . with the performed experiments we explore the effects from the variation of some basic parameters of the designs on the resulting accuracy and cross-talk , which are crucial characteristics for the seamless operation of human-computer interaction applications based on eye-tracking . our experimental results reveal the design trade-offs that need to be adopted to tackle the competing conditions that lead to optimum performance of different eye-tracking characteristics . we also present the transformations that arise in the eye-tracking output when sensor shifts occur , and assess the resulting degradation in accuracy for different combinations of eye movements and sensor shifts .", "topics": ["simulation"]}
{"title": "attention-based graph neural network for semi-supervised learning", "abstract": "recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning , improving significantly over existing approaches . these architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer . perhaps surprisingly , we show that a linear model , that removes all the intermediate fully-connected layers , is still able to achieve a performance comparable to the state-of-the-art models . this significantly reduces the number of parameters , which is critical for semi-supervised learning where number of labeled examples are small . this in turn allows a room for designing more innovative propagation layers . based on this insight , we propose a novel graph neural network that removes all the intermediate fully-connected layers , and replaces the propagation layers with attention mechanisms that respect the structure of the graph . the attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions . in a number of experiments on benchmark citation networks datasets , we demonstrate that our approach outperforms competing methods . by examining the attention weights among neighbors , we show that our model provides some interesting insights on how neighbors influence each other .", "topics": ["supervised learning"]}
{"title": "bayesian compressed regression", "abstract": "as an alternative to variable selection or shrinkage in high dimensional regression , we propose to randomly compress the predictors prior to analysis . this dramatically reduces storage and computational bottlenecks , performing well when the predictors can be projected to a low dimensional linear subspace with minimal loss of information about the response . as opposed to existing bayesian dimensionality reduction approaches , the exact posterior distribution conditional on the compressed data is available analytically , speeding up computation by many orders of magnitude while also bypassing robustness issues due to convergence and mixing problems with mcmc . model averaging is used to reduce sensitivity to the random projection matrix , while accommodating uncertainty in the subspace dimension . strong theoretical support is provided for the approach by showing near parametric convergence rates for the predictive density in the large p small n asymptotic paradigm . practical performance relative to competitors is illustrated in simulations and real data applications .", "topics": ["simulation", "computation"]}
{"title": "invariants of objects and their images under surjective maps", "abstract": "we examine the relationships between the differential invariants of objects and of their images under a surjective map . we analyze both the case when the underlying transformation group is projectable and hence induces an action on the image , and the case when only a proper subgroup of the entire group acts projectably . in the former case , we establish a constructible isomorphism between the algebra of differential invariants of the images and the algebra of fiber-wise constant ( gauge ) differential invariants of the objects . in the latter case , we describe residual effects of the full transformation group on the image invariants . our motivation comes from the problem of reconstruction of an object from multiple-view images , with central and parallel projections of curves from three-dimensional space to the two-dimensional plane serving as our main examples .", "topics": ["map"]}
{"title": "approximate inference with amortised mcmc", "abstract": "we propose a novel approximate inference algorithm that approximates a target distribution by amortising the dynamics of a user-selected mcmc sampler . the idea is to initialise mcmc using samples from an approximation network , apply the mcmc operator to improve these samples , and finally use the samples to update the approximation network thereby improving its quality . this provides a new generic framework for approximate inference , allowing us to deploy highly complex , or implicitly defined approximation families with intractable densities , including approximations produced by warping a source of randomness through a deep neural network . experiments consider image modelling with deep generative models as a challenging test for the method . deep models trained using amortised mcmc are shown to generate realistic looking samples as well as producing diverse imputations for images with regions of missing pixels .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "tuning the scheduling of distributed stochastic gradient descent with bayesian optimization", "abstract": "we present an optimizer which uses bayesian optimization to tune the system parameters of distributed stochastic gradient descent ( sgd ) . given a specific context , our goal is to quickly find efficient configurations which appropriately balance the load between the available machines to minimize the average sgd iteration time . our experiments consider setups with over thirty parameters . traditional bayesian optimization , which uses a gaussian process as its model , is not well suited to such high dimensional domains . to reduce convergence time , we exploit the available structure . we design a probabilistic model which simulates the behavior of distributed sgd and use it within bayesian optimization . our model can exploit many runtime measurements for inference per evaluation of the objective function . our experiments show that our resulting optimizer converges to efficient configurations within ten iterations , the optimized configurations outperform those found by generic optimizer in thirty iterations by up to 2x .", "topics": ["mathematical optimization", "loss function"]}
{"title": "information-theoretical label embeddings for large-scale image classification", "abstract": "we present a method for training multi-label , massively multi-class image classification models , that is faster and more accurate than supervision via a sigmoid cross-entropy loss ( logistic regression ) . our method consists in embedding high-dimensional sparse labels onto a lower-dimensional dense sphere of unit-normed vectors , and treating the classification problem as a cosine proximity regression problem on this sphere . we test our method on a dataset of 300 million high-resolution images with 17,000 labels , where it yields considerably faster convergence , as well as a 7 % higher mean average precision compared to logistic regression .", "topics": ["sparse matrix", "computer vision"]}
{"title": "a new learning paradigm for random vector functional-link network : rvfl+", "abstract": "in school , a teacher plays an important role in various classroom teaching patterns . likewise to this human learning activity , the learning using privileged information ( lupi ) paradigm provides additional information generated by the teacher to 'teach ' learning algorithms during the training stage . therefore , this novel learning paradigm is a typical teacher-student interaction mechanism . this paper is the first to present a random vector functional link network based on the lupi paradigm , called rvfl+ . rather than simply combining two existing approaches , the newly-derived rvfl+ fills the gap between neural networks and the lupi paradigm , which offers an alternative way to train rvfl networks . moreover , the proposed rvfl+ can perform in conjunction with the kernel trick for highly complicated nonlinear feature learning , which is termed krvfl+ . furthermore , the statistical property of the proposed rvfl+ is investigated , and we derive a sharp and high-quality generalization error bound based on the rademacher complexity . competitive experimental results on 14 real-world datasets illustrate the great effectiveness and efficiency of the novel rvfl+ and krvfl+ , which can achieve better generalization performance than state-of-the-art algorithms .", "topics": ["feature learning", "nonlinear system"]}
{"title": "reducing the model order of deep neural networks using information theory", "abstract": "deep neural networks are typically represented by a much larger number of parameters than shallow models , making them prohibitive for small footprint devices . recent research shows that there is considerable redundancy in the parameter space of deep neural networks . in this paper , we propose a method to compress deep neural networks by using the fisher information metric , which we estimate through a stochastic optimization method that keeps track of second-order information in the network . we first remove unimportant parameters and then use non-uniform fixed point quantization to assign more bits to parameters with higher fisher information estimates . we evaluate our method on a classification task with a convolutional neural network trained on the mnist data set . experimental results show that our method outperforms existing methods for both network pruning and quantization .", "topics": ["neural networks", "mnist database"]}
{"title": "from photo streams to evolving situations", "abstract": "photos are becoming spontaneous , objective , and universal sources of information . this paper develops evolving situation recognition using photo streams coming from disparate sources combined with the advances of deep learning . using visual concepts in photos together with space and time information , we formulate the situation detection into a semi-supervised learning framework and propose new graph-based models to solve the problem . to extend the method for unknown situations , we introduce a soft label method which enables the traditional semi-supervised learning framework to accurately predict predefined labels as well as effectively form new clusters . to overcome the noisy data which degrades graph quality , leading to poor recognition results , we take advantage of two kinds of noise-robust norms which can eliminate the adverse effects of outliers in visual concepts and improve the accuracy of situation recognition . finally , we demonstrate the idea and the effectiveness of the proposed model on yahoo flickr creative commons 100 million .", "topics": ["supervised learning"]}
{"title": "on considering uncertainty and alternatives in low-level vision", "abstract": "in this paper we address the uncertainty issues involved in the low-level vision task of image segmentation . researchers in computer vision have worked extensively on this problem , in which the goal is to partition ( or segment ) an image into regions that are homogeneous or uniform in some sense . this segmentation is often utilized by some higher level process , such as an object recognition system . we show that by considering uncertainty in a bayesian formalism , we can use statistical image models to build an approximate representation of a probability distribution over a space of alternative segmentations . we give detailed descriptions of the various levels of uncertainty associated with this problem , discuss the interaction of prior and posterior distributions , and provide the operations for constructing this representation .", "topics": ["image segmentation", "high- and low-level"]}
{"title": "scheduled denoising autoencoders", "abstract": "we present a representation learning method that learns features at multiple different levels of scale . working within the unsupervised framework of denoising autoencoders , we observe that when the input is heavily corrupted during training , the network tends to learn coarse-grained features , whereas when the input is only slightly corrupted , the network tends to learn fine-grained features . this motivates the scheduled denoising autoencoder , which starts with a high level of noise that lowers as training progresses . we find that the resulting representation yields a significant boost on a later supervised task compared to the original input , or to a standard denoising autoencoder trained at a single noise level . after supervised fine-tuning our best model achieves the lowest ever reported error on the cifar-10 data set among permutation-invariant methods .", "topics": ["feature learning", "noise reduction"]}
{"title": "context encoders : feature learning by inpainting", "abstract": "we present an unsupervised visual feature learning algorithm driven by context-based pixel prediction . by analogy with auto-encoders , we propose context encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings . in order to succeed at this task , context encoders need to both understand the content of the entire image , as well as produce a plausible hypothesis for the missing part ( s ) . when training context encoders , we have experimented with both a standard pixel-wise reconstruction loss , as well as a reconstruction plus an adversarial loss . the latter produces much sharper results because it can better handle multiple modes in the output . we found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures . we quantitatively demonstrate the effectiveness of our learned features for cnn pre-training on classification , detection , and segmentation tasks . furthermore , context encoders can be used for semantic inpainting tasks , either stand-alone or as initialization for non-parametric methods .", "topics": ["feature learning", "image segmentation"]}
{"title": "a unifying framework for gaussian process pseudo-point approximations using power expectation propagation", "abstract": "gaussian processes ( gps ) are flexible distributions over functions that enable high-level assumptions about unknown functions to be encoded in a parsimonious , flexible and general way . although elegant , the application of gps is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-gaussian models . consequently , a wealth of gp approximation schemes have been developed over the last 15 years to address these key limitations . many of these schemes employ a small set of pseudo data points to summarise the actual data . in this paper , we develop a new pseudo-point approximation framework using power expectation propagation ( power ep ) that unifies a large number of these pseudo-point approximations . unlike much of the previous venerable work in this area , the new framework is built on standard methods for approximate inference ( variational free-energy , ep and power ep methods ) rather than employing approximations to the probabilistic generative model itself . in this way , all of approximation is performed at `inference time ' rather than at `modelling time ' resolving awkward philosophical and empirical questions that trouble previous approaches . crucially , we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks .", "topics": ["generative model", "approximation algorithm"]}
{"title": "on parameters transformations for emulating sparse priors using variational-laplace inference", "abstract": "so-called sparse estimators arise in the context of model fitting , when one a priori assumes that only a few ( unknown ) model parameters deviate from zero . sparsity constraints can be useful when the estimation problem is under-determined , i.e . when number of model parameters is much higher than the number of data points . typically , such constraints are enforced by minimizing the l1 norm , which yields the so-called lasso estimator . in this work , we propose a simple parameter transform that emulates sparse priors without sacrificing the simplicity and robustness of l2-norm regularization schemes . we show how l1 regularization can be obtained with a `` sparsify '' remapping of parameters under normal bayesian priors , and we demonstrate the ensuing variational laplace approach using monte-carlo simulations .", "topics": ["calculus of variations", "matrix regularization"]}
{"title": "increasing the action gap : new operators for reinforcement learning", "abstract": "this paper introduces new optimality-preserving operators on q-functions . we first describe an operator for tabular representations , the consistent bellman operator , which incorporates a notion of local policy consistency . we show that this local consistency leads to an increase in the action gap at each state ; increasing this gap , we argue , mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies . this operator can also be applied to discretized continuous space and time problems , and we provide empirical results evidencing superior performance in this context . extending the idea of a locally consistent operator , we then derive sufficient conditions for an operator to preserve optimality , leading to a family of operators which includes our consistent bellman operator . as corollaries we provide a proof of optimality for baird 's advantage learning algorithm and derive other gap-increasing operators with interesting properties . we conclude with an empirical study on 60 atari 2600 games illustrating the strong potential of these new operators .", "topics": ["reinforcement learning", "approximation"]}
{"title": "human skin detection using rgb , hsv and ycbcr color models", "abstract": "human skin detection deals with the recognition of skin-colored pixels and regions in a given image . skin color is often used in human skin detection because it is invariant to orientation and size and is fast to process . a new human skin detection algorithm is proposed in this paper . the three main parameters for recognizing a skin pixel are rgb ( red , green , blue ) , hsv ( hue , saturation , value ) and ycbcr ( luminance , chrominance ) color models . the objective of proposed algorithm is to improve the recognition of skin pixels in given images . the algorithm not only considers individual ranges of the three color parameters but also takes into ac- count combinational ranges which provide greater accuracy in recognizing the skin area in a given image .", "topics": ["pixel"]}
{"title": "structured-based curriculum learning for end-to-end english-japanese speech translation", "abstract": "sequence-to-sequence attentional-based neural network architectures have been shown to provide a powerful model for machine translation and speech recognition . recently , several works have attempted to extend the models for end-to-end speech translation task . however , the usefulness of these models were only investigated on language pairs with similar syntax and word order ( e.g . , english-french or english-spanish ) . in this work , we focus on end-to-end speech translation tasks on syntactically distant language pairs ( e.g . , english-japanese ) that require distant word reordering . to guide the encoder-decoder attentional model to learn this difficult problem , we propose a structured-based curriculum learning strategy . unlike conventional curriculum learning that gradually emphasizes difficult data examples , we formalize learning strategies from easier network structures to more difficult network structures . here , we start the training with end-to-end encoder-decoder for speech recognition or text-based machine translation task then gradually move to end-to-end speech translation task . the experiment results show that the proposed approach could provide significant improvements in comparison with the one without curriculum learning .", "topics": ["machine translation", "speech recognition"]}
{"title": "sample complexity of episodic fixed-horizon reinforcement learning", "abstract": "recently , there has been significant progress in understanding reinforcement learning in discounted infinite-horizon markov decision processes ( mdps ) by deriving tight sample complexity bounds . however , in many real-world applications , an interactive learning agent operates for a fixed or bounded period of time , for example tutoring students for exams or handling customer service requests . such scenarios can often be better treated as episodic fixed-horizon mdps , for which only looser bounds on the sample complexity exist . a natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability ( pac guarantee ) . in this paper , we derive an upper pac bound $ \\tilde o ( \\frac { |\\mathcal s|^2 |\\mathcal a| h^2 } { \\epsilon^2 } \\ln\\frac 1 \\delta ) $ and a lower pac bound $ \\tilde \\omega ( \\frac { |\\mathcal s| |\\mathcal a| h^2 } { \\epsilon^2 } \\ln \\frac 1 { \\delta + c } ) $ that match up to log-terms and an additional linear dependency on the number of states $ |\\mathcal s| $ . the lower bound is the first of its kind for this setting . our upper bound leverages bernstein 's inequality to improve on previous bounds for episodic finite-horizon mdps which have a time-horizon dependency of at least $ h^3 $ .", "topics": ["reinforcement learning", "markov chain"]}
{"title": "automorphism groups of graphical models and lifted variational inference", "abstract": "using the theory of group action , we first introduce the concept of the automorphism group of an exponential family or a graphical model , thus formalizing the general notion of symmetry of a probabilistic model . this automorphism group provides a precise mathematical framework for lifted inference in the general exponential family . its group action partitions the set of random variables and feature functions into equivalent classes ( called orbits ) having identical marginals and expectations . then the inference problem is effectively reduced to that of computing marginals or expectations for each class , thus avoiding the need to deal with each individual variable or feature . we demonstrate the usefulness of this general framework in lifting two classes of variational approximation for map inference : local lp relaxation and local lp relaxation with cycle constraints ; the latter yields the first lifted inference that operate on a bound tighter than local constraints . initial experimental results demonstrate that lifted map inference with cycle constraints achieved the state of the art performance , obtaining much better objective function values than local approximation while remaining relatively efficient .", "topics": ["graphical model", "calculus of variations"]}
{"title": "on column selection in approximate kernel canonical correlation analysis", "abstract": "we study the problem of column selection in large-scale kernel canonical correlation analysis ( kcca ) using the nystr\\ '' om approximation , where one approximates two positive semi-definite kernel matrices using `` landmark '' points from the training set . when building low-rank kernel approximations in kcca , previous work mostly samples the landmarks uniformly at random from the training set . we propose novel strategies for sampling the landmarks non-uniformly based on a version of statistical leverage scores recently developed for kernel ridge regression . we study the approximation accuracy of the proposed non-uniform sampling strategy , develop an incremental algorithm that explores the path of approximation ranks and facilitates efficient model selection , and derive the kernel stability of out-of-sample mapping for our method . experimental results on both synthetic and real-world datasets demonstrate the promise of our method .", "topics": ["sampling ( signal processing )", "kernel ( operating system )"]}
{"title": "correlated random features for fast semi-supervised learning", "abstract": "this paper presents correlated nystrom views ( xnv ) , a fast semi-supervised algorithm for regression and classification . the algorithm draws on two main ideas . first , it generates two views consisting of computationally inexpensive random features . second , xnv applies multiview regression using canonical correlation analysis ( cca ) on unlabeled data to bias the regression towards useful features . it has been shown that , if the views contains accurate estimators , cca regression can substantially reduce variance with a minimal increase in bias . random views are justified by recent theoretical and empirical work showing that regression with random features closely approximates kernel regression , implying that random views can be expected to contain accurate estimators . we show that xnv consistently outperforms a state-of-the-art algorithm for semi-supervised learning : substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets , whilst also reducing runtime by orders of magnitude .", "topics": ["supervised learning"]}
{"title": "reduced-set kernel principal components analysis for improving the training and execution speed of kernel machines", "abstract": "this paper presents a practical , and theoretically well-founded , approach to improve the speed of kernel manifold learning algorithms relying on spectral decomposition . utilizing recent insights in kernel smoothing and learning with integral operators , we propose reduced set kpca ( rskpca ) , which also suggests an easy-to-implement method to remove or replace samples with minimal effect on the empirical operator . a simple data point selection procedure is given to generate a substitute density for the data , with accuracy that is governed by a user-tunable parameter . the effect of the approximation on the quality of the kpca solution , in terms of spectral and operator errors , can be shown directly in terms of the density estimate error and as a function of the parameter . we show in experiments that rskpca can improve both training and evaluation time of kpca by up to an order of magnitude , and compares favorably to the widely-used nystrom and density-weighted nystrom methods .", "topics": ["kernel ( operating system )"]}
{"title": "a computational model for situated task learning with interactive instruction", "abstract": "learning novel tasks is a complex cognitive activity requiring the learner to acquire diverse declarative and procedural knowledge . prior act-r models of acquiring task knowledge from instruction focused on learning procedural knowledge from declarative instructions encoded in semantic memory . in this paper , we identify the requirements for designing compu- tational models that learn task knowledge from situated task- oriented interactions with an expert and then describe and evaluate a model of learning from situated interactive instruc- tion that is implemented in the soar cognitive architecture .", "topics": ["interaction"]}
{"title": "learning from noisy label distributions", "abstract": "in this paper , we consider a novel machine learning problem , that is , learning a classifier from noisy label distributions . in this problem , each instance with a feature vector belongs to at least one group . then , instead of the true label of each instance , we observe the label distribution of the instances associated with a group , where the label distribution is distorted by an unknown noise . our goals are to ( 1 ) estimate the true label of each instance , and ( 2 ) learn a classifier that predicts the true label of a new instance . we propose a probabilistic model that considers true label distributions of groups and parameters that represent the noise as hidden variables . the model can be learned based on a variational bayesian method . in numerical experiments , we show that the proposed model outperforms existing methods in terms of the estimation of the true labels of instances .", "topics": ["calculus of variations", "feature vector"]}
{"title": "learning polytrees", "abstract": "we consider the task of learning the maximum-likelihood polytree from data . our first result is a performance guarantee establishing that the optimal branching ( or chow-liu tree ) , which can be computed very easily , constitutes a good approximation to the best polytree . we then show that it is not possible to do very much better , since the learning problem is np-hard even to approximately solve within some constant factor .", "topics": ["approximation"]}
{"title": "a new approach in persian handwritten letters recognition using error correcting output coding", "abstract": "classification ensemble , which uses the weighed polling of outputs , is the art of combining a set of basic classifiers for generating high-performance , robust and more stable results . this study aims to improve the results of identifying the persian handwritten letters using error correcting output coding ( ecoc ) ensemble method . furthermore , the feature selection is used to reduce the costs of errors in our proposed method . ecoc is a method for decomposing a multi-way classification problem into many binary classification tasks ; and then combining the results of the subtasks into a hypothesized solution to the original problem . firstly , the image features are extracted by principal components analysis ( pca ) . after that , ecoc is used for identification the persian handwritten letters which it uses support vector machine ( svm ) as the base classifier . the empirical results of applying this ensemble method using 10 real-world data sets of persian handwritten letters indicate that this method has better results in identifying the persian handwritten letters than other ensemble methods and also single classifications . moreover , by testing a number of different features , this paper found that we can reduce the additional cost in feature selection stage by using this method .", "topics": ["support vector machine"]}
{"title": "unsupervised learning of depth and motion", "abstract": "we present a model for the joint estimation of disparity and motion . the model is based on learning about the interrelations between images from multiple cameras , multiple frames in a video , or the combination of both . we show that learning depth and motion cues , as well as their combinations , from data is possible within a single type of architecture and a single type of learning algorithm , by using biologically inspired `` complex cell '' like units , which encode correlations between the pixels across image pairs . our experimental results show that the learning of depth and motion makes it possible to achieve state-of-the-art performance in 3-d activity analysis , and to outperform existing hand-engineered 3-d motion features by a very large margin .", "topics": ["unsupervised learning", "pixel"]}
{"title": "an end to end deep neural network for iris segmentation in unconstraint scenarios", "abstract": "with the increasing imaging and processing capabilities of today 's mobile devices , user authentication using iris biometrics has become feasible . however , as the acquisition conditions become more unconstrained and as image quality is typically lower than dedicated iris acquisition systems , the accurate segmentation of iris regions is crucial for these devices . in this work , an end to end fully convolutional deep neural network ( fcdnn ) design is proposed to perform the iris segmentation task for lower-quality iris images . the network design process is explained in detail , and the resulting network is trained and tuned using several large public iris datasets . a set of methods to generate and augment suitable lower quality iris images from the high-quality public databases are provided . the network is trained on near infrared ( nir ) images initially and later tuned on additional datasets derived from visible images . comprehensive inter-database comparisons are provided together with results from a selection of experiments detailing the effects of different tunings of the network . finally , the proposed model is compared with segnet-basic , and a near-optimal tuning of the network is compared to a selection of other state-of-art iris segmentation algorithms . the results show very promising performance from the optimized deep neural networks design when compared with state-of-art techniques applied to the same lower quality datasets .", "topics": ["neural networks", "database"]}
{"title": "using svdd in simplemkl for 3d-shapes filtering", "abstract": "this paper proposes the adaptation of support vector data description ( svdd ) to the multiple kernel case ( mk-svdd ) , based on simplemkl . it also introduces a variant called slim-mk-svdd that is able to produce a tighter frontier around the data . for the sake of comparison , the equivalent methods are also developed for one-class svm , known to be very similar to svdd for certain shapes of kernels . those algorithms are illustrated in the context of 3d-shapes filtering and outliers detection . for the 3d-shapes problem , the objective is to be able to select a sub-category of 3d-shapes , each sub-category being learned with our algorithm in order to create a filter . for outliers detection , we apply the proposed algorithms for unsupervised outliers detection as well as for the supervised case .", "topics": ["support vector machine", "unsupervised learning"]}
{"title": "efficient non-parametric estimation of multiple embeddings per word in vector space", "abstract": "there is rising interest in vector-space word embeddings and their use in nlp , especially given recent methods for their fast estimation at very large scale . nearly all this work , however , assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks . we present an extension to the skip-gram model that efficiently learns multiple embeddings per word type . it differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability . we present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .", "topics": ["natural language processing", "text corpus"]}
{"title": "surpassing human-level face verification performance on lfw with gaussianface", "abstract": "face verification remains a challenging problem in very complex conditions with large variations such as pose , illumination , expression , and occlusions . this problem is exacerbated when we rely unrealistically on a single training data source , which is often insufficient to cover the intrinsically complex face variations . this paper proposes a principled multi-task learning approach based on discriminative gaussian process latent variable model , named gaussianface , to enrich the diversity of training data . in comparison to existing methods , our model exploits additional data from multiple source-domains to improve the generalization performance of face verification in an unknown target-domain . importantly , our model can adapt automatically to complex data distributions , and therefore can well capture complex face variations inherent in multiple sources . extensive experiments demonstrate the effectiveness of the proposed model in learning from diverse data sources and generalize to unseen domain . specifically , the accuracy of our algorithm achieves an impressive accuracy rate of 98.52 % on the well-known and challenging labeled faces in the wild ( lfw ) benchmark . for the first time , the human-level performance in face verification ( 97.53 % ) on lfw is surpassed .", "topics": ["test set"]}
{"title": "zero-shot learning for semantic utterance classification", "abstract": "we propose a novel zero-shot learning method for semantic utterance classification ( suc ) . it learns a classifier $ f : x \\to y $ for problems where none of the semantic categories $ y $ are present in the training set . the framework uncovers the link between categories and utterances using a semantic space . we show that this semantic space can be learned by deep neural networks trained on large amounts of search engine query log data . more precisely , we propose a novel method that can learn discriminative semantic features without supervision . it uses the zero-shot learning framework to guide the learning of the semantic features . we demonstrate the effectiveness of the zero-shot semantic learning algorithm on the suc dataset collected by ( tur , 2012 ) . furthermore , we achieve state-of-the-art results by combining the semantic features with a supervised method .", "topics": ["statistical classification"]}
{"title": "rare speed-up in automatic theorem proving reveals tradeoff between computational time and information value", "abstract": "we show that strategies implemented in automatic theorem proving involve an interesting tradeoff between execution speed , proving speedup/computational time and usefulness of information . we advance formal definitions for these concepts by way of a notion of normality related to an expected ( optimal ) theoretical speedup when adding useful information ( other theorems as axioms ) , as compared with actual strategies that can be effectively and efficiently implemented . we propose the existence of an ineluctable tradeoff between this normality and computational time complexity . the argument quantifies the usefulness of information in terms of ( positive ) speed-up . the results disclose a kind of no-free-lunch scenario and a tradeoff of a fundamental nature . the main theorem in this paper together with the numerical experiment -- -undertaken using two different automatic theorem provers apros and prover9 on random theorems of propositional logic -- -provide strong theoretical and empirical arguments for the fact that finding new useful information for solving a specific problem ( theorem ) is , in general , as hard as the problem ( theorem ) itself .", "topics": ["time complexity", "numerical analysis"]}
{"title": "surprising properties of dropout in deep networks", "abstract": "we analyze dropout in deep networks with rectified linear units and the quadratic loss . our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay . for example , on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs . this provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights . we also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear , and that dropout is insensitive to various re-scalings of the input features , outputs , and network weights . this last insensitivity implies that there are no isolated local minima of the dropout training criterion . our work uncovers new properties of dropout , extends our understanding of why dropout succeeds , and lays the foundation for further progress .", "topics": ["eisenstein 's criterion"]}
{"title": "words or characters ? fine-grained gating for reading comprehension", "abstract": "previous work combines word-level and character-level representations using concatenation or scalar weighting , which is suboptimal for high-level tasks like reading comprehension . we present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words . we also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension . experiments show that our approach can improve the performance on reading comprehension tasks , achieving new state-of-the-art results on the children 's book test dataset . to demonstrate the generality of our gating mechanism , we also show improved results on a social media tag prediction task .", "topics": ["high- and low-level"]}
{"title": "joint optimization and variable selection of high-dimensional gaussian processes", "abstract": "maximizing high-dimensional , non-convex functions through noisy observations is a notoriously hard problem , but one that arises in many applications . in this paper , we tackle this challenge by modeling the unknown function as a sample from a high-dimensional gaussian process ( gp ) distribution . assuming that the unknown function only depends on few relevant variables , we show that it is possible to perform joint variable selection and gp optimization . we provide strong performance guarantees for our algorithm , bounding the sample complexity of variable selection , and as well as providing cumulative regret bounds . we further provide empirical evidence on the effectiveness of our algorithm on several benchmark optimization problems .", "topics": ["mathematical optimization"]}
{"title": "predicting parameters in deep learning", "abstract": "we demonstrate that there is significant redundancy in the parameterization of several deep learning models . given only a few weight values for each feature it is possible to accurately predict the remaining values . moreover , we show that not only can the parameter values be predicted , but many of them need not be learned at all . we train several different architectures by learning only a small number of weights and predicting the rest . in the best case we are able to predict more than 95 % of the weights of a network without any drop in accuracy .", "topics": ["value ( ethics )"]}
{"title": "large-sample learning of bayesian networks is np-hard", "abstract": "in this paper , we provide new complexity results for algorithms that learn discrete-variable bayesian networks from data . our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest model able to represent the generative distribution exactly . our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset . we show that identifying high-scoring structures is hard , even when we are given an independence oracle , an inference oracle , and/or an information oracle . our negative results also apply to the learning of discrete-variable bayesian networks in which each node has at most k parents , for all k > 3 .", "topics": ["bayesian network", "eisenstein 's criterion"]}
{"title": "metric learning with adaptive density discrimination", "abstract": "distance metric learning ( dml ) approaches learn a transformation to a representation space where distance is in correspondence with a predefined notion of similarity . while such models offer a number of compelling benefits , it has been difficult for these to compete with modern classification algorithms in performance and even in feature extraction . in this work , we propose a novel approach explicitly designed to address a number of subtle yet important issues which have stymied earlier dml algorithms . it maintains an explicit model of the distributions of the different classes in representation space . it then employs this knowledge to adaptively assess similarity , and achieve local discrimination by penalizing class distribution overlap . we demonstrate the effectiveness of this idea on several tasks . our approach achieves state-of-the-art classification results on a number of fine-grained visual recognition datasets , surpassing the standard softmax classifier and outperforming triplet loss by a relative margin of 30-40 % . in terms of computational performance , it alleviates training inefficiencies in the traditional triplet loss , reaching the same error in 5-30 times fewer iterations . beyond classification , we further validate the saliency of the learnt representations via their attribute concentration and hierarchy recovery properties , achieving 10-25 % relative gains on the softmax classifier and 25-50 % on triplet loss in these tasks .", "topics": ["feature extraction", "computer vision"]}
{"title": "generalized inverse classification", "abstract": "inverse classification is the process of perturbing an instance in a meaningful way such that it is more likely to conform to a specific class . historical methods that address such a problem are often framed to leverage only a single classifier , or specific set of classifiers . these works are often accompanied by naive assumptions . in this work we propose generalized inverse classification ( gic ) , which avoids restricting the classification model that can be used . we incorporate this formulation into a refined framework in which gic takes place . under this framework , gic operates on features that are immediately actionable . each change incurs an individual cost , either linear or non-linear . such changes are subjected to occur within a specified level of cumulative change ( budget ) . furthermore , our framework incorporates the estimation of features that change as a consequence of direct actions taken ( indirectly changeable features ) . to solve such a problem , we propose three real-valued heuristic-based methods and two sensitivity analysis-based comparison methods , each of which is evaluated on two freely available real-world datasets . our results demonstrate the validity and benefits of our formulation , framework , and methods .", "topics": ["nonlinear system", "heuristic"]}
{"title": "kernel interpolation for scalable structured gaussian processes ( kiss-gp )", "abstract": "we introduce a new structured kernel interpolation ( ski ) framework , which generalises and unifies inducing point methods for scalable gaussian processes ( gps ) . ski methods produce kernel approximations for fast computations through kernel interpolation . the ski framework clarifies how the quality of an inducing point approach depends on the number of inducing ( aka interpolation ) points , interpolation strategy , and gp covariance kernel . ski also provides a mechanism to create new scalable kernel methods , through choosing different kernel interpolation strategies . using ski , with local cubic kernel interpolation , we introduce kiss-gp , which is 1 ) more scalable than inducing point alternatives , 2 ) naturally enables kronecker and toeplitz algebra for substantial additional gains in scalability , without requiring any grid data , and 3 ) can be used for fast and expressive kernel learning . kiss-gp costs o ( n ) time and storage for gp inference . we evaluate kiss-gp for kernel matrix approximation , kernel learning , and natural sound modelling .", "topics": ["kernel ( operating system )", "approximation"]}
{"title": "privileged information for data clustering", "abstract": "many machine learning algorithms assume that all input samples are independently and identically distributed from some common distribution on either the input space x , in the case of unsupervised learning , or the input and output space x x y in the case of supervised and semi-supervised learning . in the last number of years the relaxation of this assumption has been explored and the importance of incorporation of additional information within machine learning algorithms became more apparent . traditionally such fusion of information was the domain of semi-supervised learning . more recently the inclusion of knowledge from separate hypothetical spaces has been proposed by vapnik as part of the supervised setting . in this work we are interested in exploring vapnik 's idea of master-class learning and the associated learning using privileged information , however within the unsupervised setting . adoption of the advanced supervised learning paradigm for the unsupervised setting instigates investigation into the difference between privileged and technical data . by means of our proposed ari-max method stability of the kmeans algorithm is improved and identification of the best clustering solution is achieved on an artificial dataset . subsequently an information theoretic dot product based algorithm called p-dot is proposed . this method has the ability to utilize a wide variety of clustering techniques , individually or in combination , while fusing privileged and technical data for improved clustering . application of the p-dot method to the task of digit recognition confirms our findings in a real-world scenario .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "shape-based plagiarism detection for flowchart figures in texts", "abstract": "plagiarism detection is well known phenomenon in the academic arena . copying other people is considered as serious offence that needs to be checked . there are many plagiarism detection systems such as turn-it-in that has been developed to provide this checks . most , if not all , discard the figures and charts before checking for plagiarism . discarding the figures and charts results in look holes that people can take advantage . that means people can plagiarized figures and charts easily without the current plagiarism systems detecting it . there are very few papers which talks about flowcharts plagiarism detection . therefore , there is a need to develop a system that will detect plagiarism in figures and charts . this paper presents a method for detecting flow chart figure plagiarism based on shape-based image processing and multimedia retrieval . the method managed to retrieve flowcharts with ranked similarity according to different matching sets .", "topics": ["image processing"]}
{"title": "gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks", "abstract": "we analyze algorithms for approximating a function $ f ( x ) = \\phi x $ mapping $ \\re^d $ to $ \\re^d $ using deep linear neural networks , i.e . that learn a function $ h $ parameterized by matrices $ \\theta_1 , ... , \\theta_l $ and defined by $ h ( x ) = \\theta_l \\theta_ { l-1 } ... \\theta_1 x $ . we focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic . we provide polynomial bounds on the number of iterations for gradient descent to approximate the optimum , in the case where the initial hypothesis $ \\theta_1 = ... = \\theta_l = i $ has loss bounded by a small enough constant . on the other hand , we show that gradient descent fails to converge for $ \\phi $ whose distance from the identity is a larger constant , and we show that some forms of regularization toward the identity in each layer do not help . if $ \\phi $ is symmetric positive definite , we show that an algorithm that initializes $ \\theta_i = i $ learns an $ \\epsilon $ -approximation of $ f $ using a number of updates polynomial in $ l $ , the condition number of $ \\phi $ , and $ \\log ( d/\\epsilon ) $ . in contrast , we show that if the target $ \\phi $ is symmetric and has a negative eigenvalue , then all members of a class of algorithms that perform gradient descent with identity initialization , and optionally regularize toward the identity in each layer , fail to converge . we analyze an algorithm for the case that $ \\phi $ satisfies $ u^ { \\top } \\phi u > 0 $ for all $ u $ , but may not be symmetric . this algorithm uses two regularizers : one that maintains the invariant $ u^ { \\top } \\theta_l \\theta_ { l-1 } ... \\theta_1 u > 0 $ for all $ u $ , and another that `` balances '' $ \\theta_1 ... \\theta_l $ so that they have the same singular values .", "topics": ["approximation algorithm", "matrix regularization"]}
{"title": "experimenting with innate immunity", "abstract": "in a previous paper the authors argued the case for incorporating ideas from innate immunity into artificial immune systems ( aiss ) and presented an outline for a conceptual framework for such systems . a number of key general properties observed in the biological innate and adaptive immune systems were highlighted , and how such properties might be instantiated in artificial systems was discussed in detail . the next logical step is to take these ideas and build a software system with which aiss with these properties can be implemented and experimentally evaluated . this paper reports on the results of that step - the libtissue system .", "topics": ["sensor"]}
{"title": "bridging belief function theory to modern machine learning", "abstract": "machine learning is a quickly evolving field which now looks really different from what it was 15 years ago , when classification and clustering were major issues . this document proposes several trends to explore the new questions of modern machine learning , with the strong afterthought that the belief function framework has a major role to play .", "topics": ["cluster analysis"]}
{"title": "temporal attention-gated model for robust sequence classification", "abstract": "typical techniques for sequence classification are designed for well-segmented sequences which have been edited to remove noisy or irrelevant parts . therefore , such methods can not be easily applied on noisy sequences expected in real-world applications . in this paper , we present the temporal attention-gated model ( tagm ) which integrates ideas from attention models and gated recurrent networks to better deal with noisy or unsegmented sequences . specifically , we extend the concept of attention model to measure the relevance of each observation ( time step ) of a sequence . we then use a novel gated recurrent network to learn the hidden representation for the final prediction . an important advantage of our approach is interpretability since the temporal attention weights provide a meaningful value for the salience of each time step in the sequence . we demonstrate the merits of our tagm approach , both for prediction accuracy and interpretability , on three different tasks : spoken digit recognition , text-based sentiment analysis and visual event recognition .", "topics": ["recurrent neural network", "relevance"]}
{"title": "regularization and nonlinearities for neural language models : when are they needed ?", "abstract": "neural language models ( lms ) based on recurrent neural networks ( rnn ) are some of the most successful word and character-level lms . why do they work so well , in particular better than linear neural lms ? possible explanations are that rnns have an implicitly better regularization or that rnns have a higher capacity for storing patterns due to their nonlinearities or both . here we argue for the first explanation in the limit of little training data and the second explanation for large amounts of text data . we show state-of-the-art performance on the popular and small penn dataset when rnn lms are regularized with random dropout . nonetheless , we show even better performance from a simplified , much less expressive linear rnn model without off-diagonal entries in the recurrent matrix . we call this model an impulse-response lm ( irlm ) . using random dropout , column normalization and annealed learning rates , irlms develop neurons that keep a memory of up to 50 words in the past and achieve a perplexity of 102.5 on the penn dataset . on two large datasets however , the same regularization methods are unsuccessful for both models and the rnn 's expressivity allows it to overtake the irlm by 10 and 20 percent perplexity , respectively . despite the perplexity gap , irlms still outperform rnns on the microsoft research sentence completion ( mrsc ) task . we develop a slightly modified irlm that separates long-context units ( lcus ) from short-context units and show that the lcus alone achieve a state-of-the-art performance on the mrsc task of 60.8 % . our analysis indicates that a fruitful direction of research for neural lms lies in developing more accessible internal representations , and suggests an optimization regime of very high momentum terms for effectively training such models .", "topics": ["test set", "recurrent neural network"]}
{"title": "consistent on-line off-policy evaluation", "abstract": "the problem of on-line off-policy evaluation ( ope ) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme . however , most temporal difference ( td ) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied . in this paper we propose the consistent off-policy temporal difference ( cop-td ( $ \\lambda $ , $ \\beta $ ) ) algorithm that addresses this issue and reduces this bias at some computational expense . we show that cop-td ( $ \\lambda $ , $ \\beta $ ) can be designed to converge to the same value that would have been obtained by using on-policy td ( $ \\lambda $ ) with the target policy . subsequently , the proposed scheme leads to a related and promising heuristic we call log-cop-td ( $ \\lambda $ , $ \\beta $ ) . both algorithms have favorable empirical results to the current state of the art on-line ope algorithms . finally , our formulation sheds some new light on the recently proposed emphatic td learning .", "topics": ["heuristic"]}
{"title": "approximate stochastic subgradient estimation training for support vector machines", "abstract": "subgradient algorithms for training support vector machines have been quite successful for solving large-scale and online learning problems . however , they have been restricted to linear kernels and strongly convex formulations . this paper describes efficient subgradient approaches without such limitations . our approaches make use of randomized low-dimensional approximations to nonlinear kernels , and minimization of a reduced primal formulation using an algorithm based on robust stochastic approximation , which do not require strong convexity . experiments illustrate that our approaches produce solutions of comparable prediction accuracy with the solutions acquired from existing svm solvers , but often in much shorter time . we also suggest efficient prediction schemes that depend only on the dimension of kernel approximation , not on the number of support vectors .", "topics": ["support vector machine", "nonlinear system"]}
{"title": "conditional random field autoencoders for unsupervised structured prediction", "abstract": "we introduce a framework for unsupervised learning of structured predictors with overlapping , global features . each input 's latent representation is predicted conditional on the observable data using a feature-rich conditional random field . then a reconstruction of the input is ( re ) generated , conditional on the latent structure , using models for which maximum likelihood estimation has a closed-form . our autoencoder formulation enables efficient learning without making unrealistic independence assumptions or restricting the kinds of features that can be used . we illustrate insightful connections to traditional autoencoders , posterior regularization and multi-view learning . we show competitive results with instantiations of the model for two canonical nlp tasks : part-of-speech induction and bitext word alignment , and show that training our model can be substantially more efficient than comparable feature-rich baselines .", "topics": ["baseline ( configuration management )", "generative model"]}
{"title": "learning to learn without gradient descent by gradient descent", "abstract": "we learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent . we show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions , including gaussian process bandits , simple control objectives , global optimization benchmarks and hyper-parameter tuning tasks . up to the training horizon , the learned optimizers learn to trade-off exploration and exploitation , and compare favourably with heavily engineered bayesian optimization packages for hyper-parameter tuning .", "topics": ["recurrent neural network", "synthetic data"]}
{"title": "the euler-poincare theory of metamorphosis", "abstract": "in the pattern matching approach to imaging science , the process of `` metamorphosis '' is template matching with dynamical templates . here , we recast the metamorphosis equations of into the euler-poincare variational framework of and show that the metamorphosis equations contain the equations for a perfect complex fluid \\cite { ho2002 } . this result connects the ideas underlying the process of metamorphosis in image matching to the physical concept of order parameter in the theory of complex fluids . after developing the general theory , we reinterpret various examples , including point set , image and density metamorphosis . we finally discuss the issue of matching measures with metamorphosis , for which we provide existence theorems for the initial and boundary value problems .", "topics": ["calculus of variations"]}
{"title": "zero-shot recognition via semantic embeddings and knowledge graphs", "abstract": "we consider the problem of zero-shot recognition : learning a visual classifier for a category with zero training examples , just using the word embedding of the category and its relationship to other categories , which visual data are provided . the key to dealing with the unfamiliar or novel category is to transfer knowledge obtained from familiar classes to describe the unfamiliar class . in this paper , we build upon the recently introduced graph convolutional network ( gcn ) and propose an approach that uses both semantic embeddings and the categorical relationships to predict the classifiers . given a learned knowledge graph ( kg ) , our approach takes as input semantic embeddings for each node ( representing visual category ) . after a series of graph convolutions , we predict the visual classifier for each category . during training , the visual classifiers for a few categories are given to learn the gcn parameters . at test time , these filters are used to predict the visual classifiers of unseen categories . we show that our approach is robust to noise in the kg . more importantly , our approach provides significant improvement in performance compared to the current state-of-the-art results ( from 2 ~ 3 % on some metrics to whopping 20 % on a few ) .", "topics": ["convolution"]}
{"title": "optimizing the latent space of generative networks", "abstract": "generative adversarial networks ( gans ) have been shown to be able to sample impressively realistic images . gan training consists of a saddle point optimization problem that can be thought of as an adversarial game between a generator which produces the images , and a discriminator , which judges if the images are real . both the generator and the discriminator are commonly parametrized as deep convolutional neural networks . the goal of this paper is to disentangle the contribution of the optimization procedure and the network parametrization to the success of gans . to this end we introduce and study generative latent optimization ( glo ) , a framework to train a generator without the need to learn a discriminator , thus avoiding challenging adversarial optimization problems . we show experimentally that glo enjoys many of the desirable properties of gans : learning from large data , synthesizing visually-appealing samples , interpolating meaningfully between samples , and performing linear arithmetic with noise vectors .", "topics": ["optimization problem"]}
{"title": "metrics for multivariate dictionaries", "abstract": "overcomplete representations and dictionary learning algorithms kept attracting a growing interest in the machine learning community . this paper addresses the emerging problem of comparing multivariate overcomplete representations . despite a recurrent need to rely on a distance for learning or assessing multivariate overcomplete representations , no metrics in their underlying spaces have yet been proposed . henceforth we propose to study overcomplete representations from the perspective of frame theory and matrix manifolds . we consider distances between multivariate dictionaries as distances between their spans which reveal to be elements of a grassmannian manifold . we introduce wasserstein-like set-metrics defined on grassmannian spaces and study their properties both theoretically and numerically . indeed a deep experimental study based on tailored synthetic datasetsand real eeg signals for brain-computer interfaces ( bci ) have been conducted . in particular , the introduced metrics have been embedded in clustering algorithm and applied to bci competition iv-2a for dataset quality assessment . besides , a principled connection is made between three close but still disjoint research fields , namely , grassmannian packing , dictionary learning and compressed sensing .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "efficient web-based facial recognition system employing 2dhog", "abstract": "in this paper , a system for facial recognition to identify missing and found people in hajj and umrah is described as a web portal . explicitly , we present a novel algorithm for recognition and classifications of facial images based on applying 2dpca to a 2d representation of the histogram of oriented gradients ( 2d-hog ) which maintains the spatial relation between pixels of the input images . this algorithm allows a compact representation of the images which reduces the computational complexity and the storage requirments , while maintaining the highest reported recognition accuracy . this promotes this method for usage with very large datasets . large dataset was collected for people in hajj . experimental results employing orl , umist , jaffe , and hajj datasets confirm these excellent properties .", "topics": ["computational complexity theory", "pixel"]}
{"title": "bridging cognitive programs and machine learning", "abstract": "while great advances are made in pattern recognition and machine learning , the successes of such fields remain restricted to narrow applications and seem to break down when training data is scarce , a shift in domain occurs , or when intelligent reasoning is required for rapid adaptation to new environments . in this work , we list several of the shortcomings of modern machine-learning solutions , specifically in the contexts of computer vision and in reinforcement learning and suggest directions to explore in order to try to ameliorate these weaknesses .", "topics": ["test set", "reinforcement learning"]}
{"title": "dynamic amelioration of resolution mismatches for local feature based identity inference", "abstract": "while existing face recognition systems based on local features are robust to issues such as misalignment , they can exhibit accuracy degradation when comparing images of differing resolutions . this is common in surveillance environments where a gallery of high resolution mugshots is compared to low resolution cctv probe images , or where the size of a given image is not a reliable indicator of the underlying resolution ( eg . poor optics ) . to alleviate this degradation , we propose a compensation framework which dynamically chooses the most appropriate face recognition system for a given pair of image resolutions . this framework applies a novel resolution detection method which does not rely on the size of the input images , but instead exploits the sensitivity of local features to resolution using a probabilistic multi-region histogram approach . experiments on a resolution-modified version of the `` labeled faces in the wild '' dataset show that the proposed resolution detector frontend obtains a 99 % average accuracy in selecting the most appropriate face recognition system , resulting in higher overall face discrimination accuracy ( across several resolutions ) compared to the individual baseline face recognition systems .", "topics": ["baseline ( configuration management )"]}
{"title": "recurrent highway networks", "abstract": "many sequential processing tasks require complex nonlinear transition functions from one step to the next . however , recurrent neural networks with 'deep ' transition functions remain difficult to train , even when using long short-term memory ( lstm ) networks . we introduce a novel theoretical analysis of recurrent networks based on gersgorin 's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the lstm cell . based on this analysis we propose recurrent highway networks , which extend the lstm architecture to allow step-to-step transition depths larger than one . several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models . on the penn treebank corpus , solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters . on the larger wikipedia datasets for character prediction ( text8 and enwik8 ) , rhns outperform all previous results and achieve an entropy of 1.27 bits per character .", "topics": ["test set", "recurrent neural network"]}
{"title": "structured attention networks", "abstract": "attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network . however , for many tasks we may want to model richer structural dependencies without abandoning end-to-end training . in this work , we experiment with incorporating richer structural distributions , encoded using graphical models , within deep networks . we show that these structured attention networks are simple extensions of the basic attention procedure , and that they allow for extending attention beyond the standard soft-selection approach , such as attending to partial segmentations or to subtrees . we experiment with two different classes of structured attention networks : a linear-chain conditional random field and a graph-based parsing model , and describe how these models can be practically implemented as neural network layers . experiments show that this approach is effective for incorporating structural biases , and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks : tree transduction , neural machine translation , question answering , and natural language inference . we further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention .", "topics": ["baseline ( configuration management )", "graphical model"]}
{"title": "using social dynamics to make individual predictions : variational inference with a stochastic kinetic model", "abstract": "social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors , modeling the temporal evolution of social systems via the interactions of individuals within these systems . in particular , the availability of large-scale data from social networks and sensor networks offers an unprecedented opportunity to predict state-changing events at the individual level . examples of such events include disease transmission , opinion transition in elections , and rumor propagation . unlike previous research focusing on the collective effects of social systems , this study makes efficient inferences at the individual level . in order to cope with dynamic interactions among a large number of individuals , we introduce the stochastic kinetic model to capture adaptive transition probabilities and propose an efficient variational inference algorithm the complexity of which grows linearly -- - rather than exponentially -- - with the number of individuals . to validate this method , we have performed epidemic-dynamics experiments on wireless sensor network data collected from more than ten thousand people over three years . the proposed algorithm was used to track disease transmission and predict the probability of infection for each individual . our results demonstrate that this method is more efficient than sampling while nonetheless achieving high accuracy .", "topics": ["sampling ( signal processing )", "calculus of variations"]}
{"title": "a deep convolutional auto-encoder with pooling - unpooling layers in caffe", "abstract": "this paper presents the development of several models of a deep convolutional auto-encoder in the caffe deep learning framework and their experimental evaluation on the example of mnist dataset . we have created five models of a convolutional auto-encoder which differ architecturally by the presence or absence of pooling and unpooling layers in the auto-encoder 's encoder and decoder parts . our results show that the developed models provide very good results in dimensionality reduction and unsupervised clustering tasks , and small classification errors when we used the learned internal code as an input of a supervised linear classifier and multi-layer perceptron . the best results were provided by a model where the encoder part contains convolutional and pooling layers , followed by an analogous decoder part with deconvolution and unpooling layers without the use of switch variables in the decoder part . the paper also discusses practical details of the creation of a deep convolutional auto-encoder in the very popular caffe deep learning framework . we believe that our approach and results presented in this paper could help other researchers to build efficient deep neural network architectures in the future .", "topics": ["cluster analysis", "encoder"]}
{"title": "deep rewiring : training very sparse deep networks", "abstract": "neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them . but also generic hardware and software implementations of deep learning run more efficiently for sparse networks . several methods exist for pruning connections of a neural network after it was trained without connectivity constraints . we present an algorithm , deep r , that enables us to train directly a sparsely connected neural network . deep r automatically rewires the network during supervised training so that connections are there where they are most needed for the task , while its total number is all the time strictly bounded . we demonstrate that deep r can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance . deep r is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior .", "topics": ["sampling ( signal processing )", "recurrent neural network"]}
{"title": "sparse-posterior gaussian processes for general likelihoods", "abstract": "gaussian processes ( gps ) provide a probabilistic nonparametric representation of functions in regression , classification , and other problems . unfortunately , exact learning with gps is intractable for large datasets . a variety of approximate gp methods have been proposed that essentially map the large dataset into a small set of basis points . among them , two state-of-the-art methods are sparse pseudo-input gaussian process ( spgp ) ( snelson and ghahramani , 2006 ) and variablesigma gp ( vsgp ) walder et al . ( 2008 ) , which generalizes spgp and allows each basis point to have its own length scale . however , vsgp was only derived for regression . in this paper , we propose a new sparse gp framework that uses expectation propagation to directly approximate general gp likelihoods using a sparse and smooth basis . it includes both spgp and vsgp for regression as special cases . plus as an ep algorithm , it inherits the ability to process data online . as a particular choice of approximating family , we blur each basis point with a gaussian distribution that has a full covariance matrix representing the data distribution around that basis point ; as a result , we can summarize local data manifold information with a small set of basis points . our experiments demonstrate that this framework outperforms previous gp classification methods on benchmark datasets in terms of minimizing divergence to the non-sparse gp solution as well as lower misclassification rate .", "topics": ["approximation algorithm", "statistical classification"]}
{"title": "deep learning with elastic averaging sgd", "abstract": "we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints . a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes ( local workers ) , is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server ( master ) . the algorithm enables the local workers to perform more exploration , i.e . the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master . we empirically demonstrate that in the deep learning setting , due to the existence of many local optima , allowing more exploration can lead to the improved performance . we propose synchronous and asynchronous variants of the new algorithm . we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm . we show that the stability of easgd is guaranteed when a simple stability condition is satisfied , which is not the case for admm . we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings . asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets . experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "learning activation functions from data using cubic spline interpolation", "abstract": "neural networks require a careful design in order to perform properly on a given task . in particular , selecting a good activation function ( possibly in a data-dependent fashion ) is a crucial step , which remains an open problem in the research community . despite a large amount of investigations , most current implementations simply select one fixed function from a small set of candidates , which is not adapted during training , and is shared among all neurons throughout the different layers . however , neither two of these assumptions can be supposed optimal in practice . in this paper , we present a principled way to have data-dependent adaptation of the activation functions , which is performed independently for each neuron . this is achieved by leveraging over past and present advances on cubic spline interpolation , allowing for local adaptation of the functions around their regions of use . the resulting algorithm is relatively cheap to implement , and overfitting is counterbalanced by the inclusion of a novel damping criterion , which penalizes unwanted oscillations from a predefined shape . experimental results validate the proposal over two well-known benchmarks .", "topics": ["eisenstein 's criterion"]}
{"title": "efficient mixture model for clustering of sparse high dimensional binary data", "abstract": "in this paper we propose a mixture model , sparsemix , for clustering of sparse high dimensional binary data , which connects model-based with centroid-based clustering . every group is described by a representative and a probability distribution modeling dispersion from this representative . in contrast to classical mixture models based on em algorithm , sparsemix : -is especially designed for the processing of sparse data , -can be efficiently realized by an on-line hartigan optimization algorithm , -is able to automatically reduce unnecessary clusters . we perform extensive experimental studies on various types of data , which confirm that sparsemix builds partitions with higher compatibility with reference grouping than related methods . moreover , constructed representatives often better reveal the internal structure of data .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "online optimization of smoothed piecewise constant functions", "abstract": "we study online optimization of smoothed piecewise constant functions over the domain [ 0 , 1 ) . this is motivated by the problem of adaptively picking parameters of learning algorithms as in the recently introduced framework by gupta and roughgarden ( 2016 ) . majority of the machine learning literature has focused on lipschitz-continuous functions or functions with bounded gradients . 1 this is with good reason -- -any learning algorithm suffers linear regret even against piecewise constant functions that are chosen adversarially , arguably the simplest of non-lipschitz continuous functions . the smoothed setting we consider is inspired by the seminal work of spielman and teng ( 2004 ) and the recent work of gupta and roughgarden -- -in this setting , the sequence of functions may be chosen by an adversary , however , with some uncertainty in the location of discontinuities . we give algorithms that achieve sublinear regret in the full information and bandit settings .", "topics": ["regret ( decision theory )", "mathematical optimization"]}
{"title": "reinforcement learning of speech recognition system based on policy gradient and hypothesis selection", "abstract": "speech recognition systems have achieved high recognition performance for several tasks . however , the performance of such systems is dependent on the tremendously costly development work of preparing vast amounts of task-matched transcribed speech data for supervised training . the key problem here is the cost of transcribing speech data . the cost is repeatedly required to support new languages and new tasks . assuming broad network services for transcribing speech data for many users , a system would become more self-sufficient and more useful if it possessed the ability to learn from very light feedback from the users without annoying them . in this paper , we propose a general reinforcement learning framework for speech recognition systems based on the policy gradient method . as a particular instance of the framework , we also propose a hypothesis selection-based reinforcement learning method . the proposed framework provides a new view for several existing training and adaptation methods . the experimental results show that the proposed method improves the recognition performance compared to unsupervised adaptation .", "topics": ["unsupervised learning", "reinforcement learning"]}
{"title": "streaming label learning for modeling labels on the fly", "abstract": "it is challenging to handle a large volume of labels in multi-label learning . however , existing approaches explicitly or implicitly assume that all the labels in the learning process are given , which could be easily violated in changing environments . in this paper , we define and study streaming label learning ( sll ) , i.e . , labels are arrived on the fly , to model newly arrived labels with the help of the knowledge learned from past labels . the core of sll is to explore and exploit the relationships between new labels and past labels and then inherit the relationship into hypotheses of labels to boost the performance of new classifiers . in specific , we use the label self-representation to model the label relationship , and sll will be divided into two steps : a regression problem and a empirical risk minimization ( erm ) problem . both problems are simple and can be efficiently solved . we further show that sll can generate a tighter generalization error bound for new labels than the general erm framework with trace norm or frobenius norm regularization . finally , we implement extensive experiments on various benchmark datasets to validate the new setting . and results show that sll can effectively handle the constantly emerging new labels and provides excellent classification performance .", "topics": ["matrix regularization"]}
{"title": "crowdsourcing in computer vision", "abstract": "computer vision systems require large amounts of manually annotated data to properly learn challenging visual concepts . crowdsourcing platforms offer an inexpensive method to capture human knowledge and understanding , for a vast number of visual perception tasks . in this survey , we describe the types of annotations computer vision researchers have collected using crowdsourcing , and how they have ensured that this data is of high quality while annotation effort is minimized . we begin by discussing data collection on both classic ( e.g . , object recognition ) and recent ( e.g . , visual story-telling ) vision tasks . we then summarize key design decisions for creating effective data collection interfaces and workflows , and present strategies for intelligently selecting the most important data instances to annotate . finally , we conclude with some thoughts on the future of crowdsourcing in computer vision .", "topics": ["computer vision"]}
{"title": "a confident information first principle for parametric reduction and model selection of boltzmann machines", "abstract": "typical dimensionality reduction ( dr ) methods are often data-oriented , focusing on directly reducing the number of random variables ( features ) while retaining the maximal variations in the high-dimensional data . in unsupervised situations , one of the main limitations of these methods lies in their dependency on the scale of data features . this paper aims to address the problem from a new perspective and considers model-oriented dimensionality reduction in parameter spaces of binary multivariate distributions . specifically , we propose a general parameter reduction criterion , called confident-information-first ( cif ) principle , to maximally preserve confident parameters and rule out less confident parameters . formally , the confidence of each parameter can be assessed by its contribution to the expected fisher information distance within the geometric manifold over the neighbourhood of the underlying real distribution . we then revisit boltzmann machines ( bm ) from a model selection perspective and theoretically show that both the fully visible bm ( vbm ) and the bm with hidden units can be derived from the general binary multivariate distribution using the cif principle . this can help us uncover and formalize the essential parts of the target density that bm aims to capture and the non-essential parts that bm should discard . guided by the theoretical analysis , we develop a sample-specific cif for model selection of bm that is adaptive to the observed samples . the method is studied in a series of density estimation experiments and has been shown effective in terms of the estimate accuracy .", "topics": ["unsupervised learning", "heuristic"]}
{"title": "a quasi-newton approach to nonsmooth convex optimization problems in machine learning", "abstract": "we extend the well-known bfgs quasi-newton method and its memory-limited variant lbfgs to the optimization of nonsmooth convex objectives . this is done in a rigorous fashion by generalizing three components of bfgs to subdifferentials : the local quadratic model , the identification of a descent direction , and the wolfe line search conditions . we prove that under some technical conditions , the resulting subbfgs algorithm is globally convergent in objective function value . we apply its memory-limited variant ( sublbfgs ) to l_2-regularized risk minimization with the binary hinge loss . to extend our algorithm to the multiclass and multilabel settings , we develop a new , efficient , exact line search algorithm . we prove its worst-case time complexity bounds , and show that our line search can also be used to extend a recently developed bundle method to the multiclass and multilabel settings . we also apply the direction-finding component of our algorithm to l_1-regularized risk minimization with logistic loss . in all these contexts our methods perform comparable to or better than specialized state-of-the-art solvers on a number of publicly available datasets . an open source implementation of our algorithms is freely available .", "topics": ["optimization problem", "time complexity"]}
{"title": "comparison of the language networks from literature and blogs", "abstract": "in this paper we present the comparison of the linguistic networks from literature and blog texts . the linguistic networks are constructed from texts as directed and weighted co-occurrence networks of words . words are nodes and links are established between two nodes if they are directly co-occurring within the sentence . the comparison of the networks structure is performed at global level ( network ) in terms of : average node degree , average shortest path length , diameter , clustering coefficient , density and number of components . furthermore , we perform analysis on the local level ( node ) by comparing the rank plots of in and out degree , strength and selectivity . the selectivity-based results point out that there are differences between the structure of the networks constructed from literature and blogs .", "topics": ["coefficient"]}
{"title": "incremental spectral sparsification for large-scale graph-based semi-supervised learning", "abstract": "while the harmonic function solution performs well in many semi-supervised learning ( ssl ) tasks , it is known to scale poorly with the number of samples . recent successful and scalable methods , such as the eigenfunction method focus on efficiently approximating the whole spectrum of the graph laplacian constructed from the data . this is in contrast to various subsampling and quantization methods proposed in the past , which may fail in preserving the graph spectra . however , the impact of the approximation of the spectrum on the final generalization error is either unknown , or requires strong assumptions on the data . in this paper , we introduce sparse-hfs , an efficient edge-sparsification algorithm for ssl . by constructing an edge-sparse and spectrally similar graph , we are able to leverage the approximation guarantees of spectral sparsification methods to bound the generalization error of sparse-hfs . as a result , we obtain a theoretically-grounded approximation scheme for graph-based ssl that also empirically matches the performance of known large-scale methods .", "topics": ["approximation algorithm", "supervised learning"]}
{"title": "crdt : correlation ratio based decision tree model for healthcare data mining", "abstract": "the phenomenal growth in the healthcare data has inspired us in investigating robust and scalable models for data mining . for classification problems information gain ( ig ) based decision tree is one of the popular choices . however , depending upon the nature of the dataset , ig based decision tree may not always perform well as it prefers the attribute with more number of distinct values as the splitting attribute . healthcare datasets generally have many attributes and each attribute generally has many distinct values . in this paper , we have tried to focus on this characteristics of the datasets while analysing the performance of our proposed approach which is a variant of decision tree model and uses the concept of correlation ratio ( cr ) . unlike ig based approach , this cr based approach has no biasness towards the attribute with more number of distinct values . we have applied our model on some benchmark healthcare datasets to show the effectiveness of the proposed technique .", "topics": ["data mining", "value ( ethics )"]}
{"title": "on the application of hierarchical coevolutionary genetic algorithms : recombination and evaluation partners", "abstract": "this paper examines the use of a hierarchical coevolutionary genetic algorithm under different partnering strategies . cascading clusters of sub-populations are built from the bottom up , with higher-level sub-populations optimising larger parts of the problem . hence higher-level sub-populations potentially search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution . the effects of different partner selection schemes amongst the sub-populations on solution quality are examined for two constrained optimisation problems . we examine a number of recombination partnering strategies in the construction of higher-level individuals and a number of related schemes for evaluating sub-solutions . it is shown that partnering strategies that exploit problem-specific knowledge are superior and can counter inappropriate ( sub ) fitness measurements .", "topics": ["mathematical optimization"]}
{"title": "automating rule generation for grammar checkers", "abstract": "in this paper , i describe several approaches to automatic or semi-automatic development of symbolic rules for grammar checkers from the information contained in corpora . the rules obtained this way are an important addition to manually-created rules that seem to dominate in rule-based checkers . however , the manual process of creation of rules is costly , time-consuming and error-prone . it seems therefore advisable to use machine-learning algorithms to create the rules automatically or semi-automatically . the results obtained seem to corroborate my initial hypothesis that symbolic machine learning algorithms can be useful for acquiring new rules for grammar checking . it turns out , however , that for practical uses , error corpora can not be the sole source of information used in grammar checking . i suggest therefore that only by using different approaches , grammar-checkers , or more generally , computer-aided proofreading tools , will be able to cover most frequent and severe mistakes and avoid false alarms that seem to distract users .", "topics": ["text corpus"]}
{"title": "beyond rgb : very high resolution urban remote sensing with multimodal deep networks", "abstract": "in this work , we investigate various methods to deal with semantic labeling of very high resolution multi-modal remote sensing data . especially , we study how deep fully convolutional networks can be adapted to deal with multi-modal and multi-scale remote sensing data for semantic labeling . our contributions are threefold : a ) we present an efficient multi-scale approach to leverage both a large spatial context and the high resolution data , b ) we investigate early and late fusion of lidar and multispectral data , c ) we validate our methods on two public datasets with state-of-the-art results . our results indicate that late fusion make it possible to recover errors steaming from ambiguous data , while early fusion allows for better joint-feature learning but at the cost of higher sensitivity to missing data .", "topics": ["feature learning"]}
{"title": "how many dissimilarity/kernel self organizing map variants do we need ?", "abstract": "in numerous applicative contexts , data are too rich and too complex to be represented by numerical vectors . a general approach to extend machine learning and data mining techniques to such data is to really on a dissimilarity or on a kernel that measures how different or similar two objects are . this approach has been used to define several variants of the self organizing map ( som ) . this paper reviews those variants in using a common set of notations in order to outline differences and similarities between them . it discusses the advantages and drawbacks of the variants , as well as the actual relevance of the dissimilarity/kernel som for practical applications .", "topics": ["kernel ( operating system )", "data mining"]}
{"title": "variational gaussian process dynamical systems", "abstract": "high dimensional time series are endemic in applications of machine learning such as robotics ( sensor data ) , computational biology ( gene expression data ) , vision ( video sequences ) and graphics ( motion capture data ) . practical nonlinear probabilistic approaches to this data are required . in this paper we introduce the variational gaussian process dynamical system . our work builds on recent variational approximations for gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space . the approach also allows for the appropriate dimensionality of the latent space to be automatically determined . we demonstrate the model on a human motion capture data set and a series of high resolution video sequences .", "topics": ["calculus of variations", "time series"]}
{"title": "an out-of-the-box full-network embedding for convolutional neural networks", "abstract": "transfer learning for feature extraction can be used to exploit deep representations in contexts where there is very few training data , where there are limited computational resources , or when tuning the hyper-parameters needed for training is not an option . while previous contributions to feature extraction propose embeddings based on a single layer of the network , in this paper we propose a full-network embedding which successfully integrates convolutional and fully connected features , coming from all layers of a deep convolutional neural network . to do so , the embedding normalizes features in the context of the problem , and discretizes their values to reduce noise and regularize the embedding space . significantly , this also reduces the computational cost of processing the resultant representations . the proposed method is shown to outperform single layer embeddings on several image classification tasks , while also being more robust to the choice of the pre-trained model used for obtaining the initial features . the performance gap in classification accuracy between thoroughly tuned solutions and the full-network embedding is also reduced , which makes of the proposed approach a competitive solution for a large set of applications .", "topics": ["test set", "feature extraction"]}
{"title": "( more ) efficient reinforcement learning via posterior sampling", "abstract": "most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration . we study an alternative approach for efficient exploration , posterior sampling for reinforcement learning ( psrl ) . this algorithm proceeds in repeated episodes of known duration . at the start of each episode , psrl updates a prior distribution over markov decision processes and takes one sample from this posterior . psrl then follows the policy that is optimal for this sample during the episode . the algorithm is conceptually simple , computationally efficient and allows an agent to encode prior knowledge in a natural way . we establish an $ \\tilde { o } ( \\tau s \\sqrt { at } ) $ bound on the expected regret , where $ t $ is time , $ \\tau $ is the episode length and $ s $ and $ a $ are the cardinalities of the state and action spaces . this bound is one of the first for an algorithm not based on optimism , and close to the state of the art for any reinforcement learning algorithm . we show through simulation that psrl significantly outperforms existing algorithms with similar regret bounds .", "topics": ["regret ( decision theory )", "computational complexity theory"]}
{"title": "a web-based tool for identifying strategic intervention points in complex systems", "abstract": "steering a complex system towards a desired outcome is a challenging task . the lack of clarity on the system 's exact architecture and the often scarce scientific data upon which to base the operationalisation of the dynamic rules that underpin the interactions between participant entities are two contributing factors . we describe an analytical approach that builds on fuzzy cognitive mapping ( fcm ) to address the latter and represent the system as a complex network . we apply results from network controllability to address the former and determine minimal control configurations - subsets of factors , or system levers , which comprise points for strategic intervention in steering the system . we have implemented the combination of these techniques in an analytical tool that runs in the browser , and generates all minimal control configurations of a complex network . we demonstrate our approach by reporting on our experience of working alongside industrial , local-government , and ngo stakeholders in the humber region , uk . our results are applied to the decision-making process involved in the transition of the region to a bio-based economy .", "topics": ["interaction", "entity"]}
{"title": "large-margin classification with multiple decision rules", "abstract": "binary classification is a common statistical learning problem in which a model is estimated on a set of covariates for some outcome indicating the membership of one of two classes . in the literature , there exists a distinction between hard and soft classification . in soft classification , the conditional class probability is modeled as a function of the covariates . in contrast , hard classification methods only target the optimal prediction boundary . while hard and soft classification methods have been studied extensively , not much work has been done to compare the actual tasks of hard and soft classification . in this paper we propose a spectrum of statistical learning problems which span the hard and soft classification tasks based on fitting multiple decision rules to the data . by doing so , we reveal a novel collection of learning tasks of increasing complexity . we study the problems using the framework of large-margin classifiers and a class of piecewise linear convex surrogates , for which we derive statistical properties and a corresponding sub-gradient descent algorithm . we conclude by applying our approach to simulation settings and a magnetic resonance imaging ( mri ) dataset from the alzheimer 's disease neuroimaging initiative ( adni ) study .", "topics": ["statistical classification", "simulation"]}
{"title": "evolutionary optimization in an algorithmic setting", "abstract": "evolutionary processes proved very useful for solving optimization problems . in this work , we build a formalization of the notion of cooperation and competition of multiple systems working toward a common optimization goal of the population using evolutionary computation techniques . it is justified that evolutionary algorithms are more expressive than conventional recursive algorithms . three subclasses of evolutionary algorithms are proposed here : bounded finite , unbounded finite and infinite types . some results on completeness , optimality and search decidability for the above classes are presented . a natural extension of evolutionary turing machine model developed in this paper allows one to mathematically represent and study properties of cooperation and competition in a population of optimized species .", "topics": ["mathematical optimization", "computation"]}
{"title": "an empirical analysis of dropout in piecewise linear networks", "abstract": "the recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer , as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters . in this work we empirically investigate several questions related to the efficacy of dropout , specifically as it concerns networks employing the popular rectified linear activation function . we investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models , as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques . we explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights . finally , we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient .", "topics": ["gradient", "eisenstein 's criterion"]}
{"title": "the continuous hint factory - providing hints in vast and sparsely populated edit distance spaces", "abstract": "intelligent tutoring systems can support students in solving multi-step tasks by providing a hint regarding what to do next . however , engineering such next-step hints manually or using an expert model becomes infeasible if the space of possible states is too large . therefore , several approaches have emerged to infer next-step hints automatically , relying on past student 's data . such hints typically have the form of an edit which could have been performed by capable students in the given situation , based on what past capable students have done . in this contribution we provide a mathematical framework to analyze edit-based hint policies and , based on this theory , propose a novel hint policy to provide edit hints for learning tasks with a vast state space and sparse student data . we call this technique the continuous hint factory because it embeds student data in a continuous space , in which the most likely edit can be inferred in a probabilistic sense , similar to the hint factory . in our experimental evaluation we demonstrate that the continuous hint factory can predict what capable students would do in solving a multi-step programming task and that hints provided by the continuous hint factory match to some extent the edit hints that human tutors would have given in the same situation .", "topics": ["sparse matrix"]}
{"title": "matrix-normal models for fmri analysis", "abstract": "multivariate analysis of fmri data has benefited substantially from advances in machine learning . most recently , a range of probabilistic latent variable models applied to fmri data have been successful in a variety of tasks , including identifying similarity patterns in neural data ( representational similarity analysis and its empirical bayes variant , rsa and brsa ; intersubject functional connectivity , isfc ) , combining multi-subject datasets ( shared response mapping ; srm ) , and mapping between brain and behavior ( joint modeling ) . although these methods share some underpinnings , they have been developed as distinct methods , with distinct algorithms and software tools . we show how the matrix-variate normal ( mn ) formalism can unify some of these methods into a single framework . in doing so , we gain the ability to reuse noise modeling assumptions , algorithms , and code across models . our primary theoretical contribution shows how some of these methods can be written as instantiations of the same model , allowing us to generalize them to flexibly modeling structured noise covariances . our formalism permits novel model variants and improved estimation strategies : in contrast to srm , the number of parameters for mn-srm does not scale with the number of voxels or subjects ; in contrast to brsa , the number of parameters for mn-rsa scales additively rather than multiplicatively in the number of voxels . we empirically demonstrate advantages of two new methods derived in the formalism : for mn-rsa , we show up to 10x improvement in runtime , up to 6x improvement in rmse , and more conservative behavior under the null . for mn-srm , our method grants a modest improvement to out-of-sample reconstruction while relaxing an orthonormality constraint of srm . we also provide a software prototyping tool for mn models that can flexibly reuse noise covariance assumptions and algorithms across models .", "topics": ["scalability"]}
{"title": "learning from distributions via support measure machines", "abstract": "this paper presents a kernel-based discriminative learning framework on probability measures . rather than relying on large collections of vectorial training examples , our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data . by representing these probability distributions as mean embeddings in the reproducing kernel hilbert space ( rkhs ) , we are able to apply many standard kernel-based learning techniques in straightforward fashion . to accomplish this , we construct a generalization of the support vector machine ( svm ) called a support measure machine ( smm ) . our analyses of smms provides several insights into their relationship to traditional svms . based on such insights , we propose a flexible svm ( flex-svm ) that places different kernel functions on each training example . experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework .", "topics": ["test set", "support vector machine"]}
{"title": "radnet : radiologist level accuracy using deep learning for hemorrhage detection in ct scans", "abstract": "we describe a deep learning approach for automated brain hemorrhage detection from computed tomography ( ct ) scans . our model emulates the procedure followed by radiologists to analyse a 3d ct scan in real-world . similar to radiologists , the model sifts through 2d cross-sectional slices while paying close attention to potential hemorrhagic regions . further , the model utilizes 3d context from neighboring slices to improve predictions at each slice and subsequently , aggregates the slice-level predictions to provide diagnosis at ct level . we refer to our proposed approach as recurrent attention densenet ( radnet ) as it employs original densenet architecture along with adding the components of attention for slice level predictions and recurrent neural network layer for incorporating 3d context . the real-world performance of radnet has been benchmarked against independent analysis performed by three senior radiologists for 77 brain cts . radnet demonstrates 81.82 % hemorrhage prediction accuracy at ct level that is comparable to radiologists . further , radnet achieves higher recall than two of the three radiologists , which is remarkable .", "topics": ["recurrent neural network"]}
{"title": "deepsetnet : predicting sets with deep neural networks", "abstract": "this paper addresses the task of set prediction using deep learning . this is important because the output of many computer vision tasks , including image tagging and object detection , are naturally expressed as sets of entities rather than vectors . as opposed to a vector , the size of a set is not fixed in advance , and it is invariant to the ordering of entities within it . we define a likelihood for a set distribution and learn its parameters using a deep neural network . we also derive a loss for predicting a discrete distribution corresponding to set cardinality . set prediction is demonstrated on the problem of multi-class image classification . moreover , we show that the proposed cardinality loss can also trivially be applied to the tasks of object counting and pedestrian detection . our approach outperforms existing methods in all three cases on standard datasets .", "topics": ["object detection", "neural networks"]}
{"title": "dynamic network surgery for efficient dnns", "abstract": "deep learning has become a ubiquitous technology to improve machine intelligence . however , most of the existing deep models are structurally very complex , making them difficult to be deployed on the mobile platforms with limited computational power . in this paper , we propose a novel network compression method called dynamic network surgery , which can remarkably reduce the network complexity by making on-the-fly connection pruning . unlike the previous methods which accomplish this task in a greedy way , we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance . the effectiveness of our method is proved with experiments . without any accuracy loss , our method can efficiently compress the number of parameters in lenet-5 and alexnet by a factor of $ \\bm { 108 } \\times $ and $ \\bm { 17.7 } \\times $ respectively , proving that it outperforms the recent pruning method by considerable margins . code and some models are available at https : //github.com/yiwenguo/dynamic-network-surgery .", "topics": ["artificial intelligence"]}
{"title": "indefinite kernel logistic regression", "abstract": "traditionally , kernel learning methods requires positive definitiveness on the kernel , which is too strict and excludes many sophisticated similarities , that are indefinite , in multimedia area . to utilize those indefinite kernels , indefinite learning methods are of great interests . this paper aims at the extension of the logistic regression from positive semi-definite kernels to indefinite kernels . the model , called indefinite kernel logistic regression ( iklr ) , keeps consistency to the regular klr in formulation but it essentially becomes non-convex . thanks to the positive decomposition of an indefinite matrix , iklr can be transformed into a difference of two convex models , which follows the use of concave-convex procedure . moreover , we employ an inexact solving scheme to speed up the sub-problem and develop a concave-inexact-convex procedure ( ccicp ) algorithm with theoretical convergence analysis . systematical experiments on multi-modal datasets demonstrate the superiority of the proposed iklr method over kernel logistic regression with positive definite kernels and other state-of-the-art indefinite learning based algorithms .", "topics": ["kernel ( operating system )"]}
{"title": "imposing higher-level structure in polyphonic music generation using convolutional restricted boltzmann machines and constraints", "abstract": "we introduce a method for imposing higher-level structure on generated , polyphonic music . a convolutional restricted boltzmann machine ( c-rbm ) as a generative model is combined with gradient descent constraint optimization to provide further control over the generation process . among other things , this allows for the use of a `` template '' piece , from which some structural properties can be extracted , and transferred as constraints to newly generated material . the sampling process is guided with simulated annealing in order to avoid local optima , and find solutions that both satisfy the constraints , and are relatively stable with respect to the c-rbm . results show that with this approach it is possible to control the higher level self-similarity structure , the meter , as well as tonal properties of the resulting musical piece while preserving its local musical coherence .", "topics": ["sampling ( signal processing )", "generative model"]}
{"title": "extracting biomolecular interactions using semantic parsing of biomedical text", "abstract": "we advance the state of the art in biomolecular interaction extraction with three contributions : ( i ) we show that deep , abstract meaning representations ( amr ) significantly improve the accuracy of a biomolecular interaction extraction system when compared to a baseline that relies solely on surface- and syntax-based features ; ( ii ) in contrast with previous approaches that infer relations on a sentence-by-sentence basis , we expand our framework to enable consistent predictions over sets of sentences ( documents ) ; ( iii ) we further modify and expand a graph kernel learning framework to enable concurrent exploitation of automatically induced amr ( semantic ) and dependency structure ( syntactic ) representations . our experiments show that our approach yields interaction extraction systems that are more robust in environments where there is a significant mismatch between training and test conditions .", "topics": ["baseline ( configuration management )", "parsing"]}
{"title": "an easy to use repository for comparing and improving machine learning algorithm usage", "abstract": "the results from most machine learning experiments are used for a specific purpose and then discarded . this results in a significant loss of information and requires rerunning experiments to compare learning algorithms . this also requires implementation of another algorithm for comparison , that may not always be correctly implemented . by storing the results from previous experiments , machine learning algorithms can be compared easily and the knowledge gained from them can be used to improve their performance . the purpose of this work is to provide easy access to previous experimental results for learning and comparison . these stored results are comprehensive -- storing the prediction for each test instance as well as the learning algorithm , hyperparameters , and training set that were used . previous results are particularly important for meta-learning , which , in a broad sense , is the process of learning from previous machine learning results such that the learning process is improved . while other experiment databases do exist , one of our focuses is on easy access to the data . we provide meta-learning data sets that are ready to be downloaded for meta-learning experiments . in addition , queries to the underlying database can be made if specific information is desired . we also differ from previous experiment databases in that our databases is designed at the instance level , where an instance is an example in a data set . we store the predictions of a learning algorithm trained on a specific training set for each instance in the test set . data set level information can then be obtained by aggregating the results from the instances . the instance level information can be used for many tasks such as determining the diversity of a classifier or algorithmically determining the optimal subset of training instances for a learning algorithm .", "topics": ["test set", "database"]}
{"title": "arguing machines : perception-control system redundancy and edge case discovery in real-world autonomous driving", "abstract": "safe autonomous driving may be one of the most difficult engineering challenges that any artificial intelligence system has been asked to do since the birth of ai over sixty years ago . the difficulty is not within the task itself , but rather in the extremely small margin of allowable error given the human life at stake and the extremely large number of edge cases that have to be accounted for . in other words , we task these systems to expect the unexpected with near 100 % accuracy , which is a technical challenge for machine learning methods that to date have generally been better at memorizing the expected than predicting the unexpected . in fact , the process of efficiently and automatically discovering the edge cases of driving may be the key to solving this engineering challenge . in this work , we propose and evaluate a method for discovering edge cases by monitoring the disagreement between two monocular-vision-based automated steering systems . the first is a proprietary tesla autopilot system equipped in the first generation of autopilot-capable vehicles . the second is a end-to-end neural network trained on a large-scale naturalistic dataset of 420 hours or 45 million frames of autonomous driving in tesla vehicles .", "topics": ["end-to-end principle", "artificial intelligence"]}
{"title": "a new data representation based on training data characteristics to extract drug named-entity in medical text", "abstract": "one essential task in information extraction from the medical corpus is drug name recognition . compared with text sources come from other domains , the medical text is special and has unique characteristics . in addition , the medical text mining poses more challenges , e.g . , more unstructured text , the fast growing of new terms addition , a wide range of name variation for the same drug . the mining is even more challenging due to the lack of labeled dataset sources and external knowledge , as well as multiple token representations for a single drug name that is more common in the real application setting . although many approaches have been proposed to overwhelm the task , some problems remained with poor f-score performance ( less than 0.75 ) . this paper presents a new treatment in data representation techniques to overcome some of those challenges . we propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training . the first technique is evaluated with the standard nn model , i.e . , mlp ( multi-layer perceptrons ) . the second technique involves two deep network classifiers , i.e . , dbn ( deep belief networks ) , and sae ( stacked denoising encoders ) . the third technique represents the sentence as a sequence that is evaluated with a recurrent nn model , i.e . , lstm ( long short term memory ) . in extracting the drug name entities , the third technique gives the best f-score performance compared to the state of the art , with its average f-score being 0.8645 .", "topics": ["noise reduction", "bayesian network"]}
{"title": "variational relevance vector machines", "abstract": "the support vector machine ( svm ) of vapnik ( 1998 ) has become widely established as one of the leading approaches to pattern recognition and machine learning . it expresses predictions in terms of a linear combination of kernel functions centred on a subset of the training data , known as support vectors . despite its widespread success , the svm suffers from some important limitations , one of the most significant being that it makes point predictions rather than generating predictive distributions . recently tipping ( 1999 ) has formulated the relevance vector machine ( rvm ) , a probabilistic model whose functional form is equivalent to the svm . it achieves comparable recognition accuracy to the svm , yet provides a full predictive distribution , and also requires substantially fewer kernel functions . the original treatment of the rvm relied on the use of type ii maximum likelihood ( the `evidence framework ' ) to provide point estimates of the hyperparameters which govern model sparsity . in this paper we show how the rvm can be formulated and solved within a completely bayesian paradigm through the use of variational inference , thereby giving a posterior distribution over both parameters and hyperparameters . we demonstrate the practicality and performance of the variational rvm using both synthetic and real world examples .", "topics": ["test set", "calculus of variations"]}
{"title": "efficient benchmarking of algorithm configuration procedures via model-based surrogates", "abstract": "the optimization of algorithm ( hyper- ) parameters is crucial for achieving peak performance across a wide range of domains , ranging from deep neural networks to solvers for hard combinatorial problems . the resulting algorithm configuration ( ac ) problem has attracted much attention from the machine learning community . however , the proper evaluation of new ac procedures is hindered by two key hurdles . first , ac benchmarks are hard to set up . second and even more significantly , they are computationally expensive : a single run of an ac procedure involves many costly runs of the target algorithm whose performance is to be optimized in a given ac benchmark scenario . one common workaround is to optimize cheap-to-evaluate artificial benchmark functions ( e.g . , branin ) instead of actual algorithms ; however , these have different properties than realistic ac problems . here , we propose an alternative benchmarking approach that is similarly cheap to evaluate but much closer to the original ac problem : replacing expensive benchmarks by surrogate benchmarks constructed from ac benchmarks . these surrogate benchmarks approximate the response surface corresponding to true target algorithm performance using a regression model , and the original and surrogate benchmark share the same ( hyper- ) parameter space . in our experiments , we construct and evaluate surrogate benchmarks for hyperparameter optimization as well as for ac problems that involve performance optimization of solvers for hard combinatorial problems , drawing training data from the runs of existing ac procedures . we show that our surrogate benchmarks capture overall important characteristics of the ac scenarios , such as high- and low-performing regions , from which they were derived , while being much easier to use and orders of magnitude cheaper to evaluate .", "topics": ["test set", "approximation algorithm"]}
{"title": "chinese poetry generation with planning based neural network", "abstract": "chinese poetry generation is a very challenging task in natural language processing . in this paper , we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user 's writing intent , and then generates each line of the poem sequentially , using a modified recurrent neural network encoder-decoder framework . the proposed planning-based method can ensure that the generated poem is coherent and semantically consistent with the user 's intent . a comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets .", "topics": ["natural language processing", "recurrent neural network"]}
{"title": "clustering is easy when ... .what ?", "abstract": "it is well known that most of the common clustering objectives are np-hard to optimize . in practice , however , clustering is being routinely carried out . one approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets . the hope is that there will be clustering algorithms that are provably efficient on such `` clusterable '' instances . this paper addresses the thesis that the computational hardness of clustering tasks goes away for inputs that one really cares about . in other words , that `` clustering is difficult only when it does not matter '' ( the \\emph { cdnm thesis } for short ) . i wish to present a a critical bird 's eye overview of the results published on this issue so far and to call attention to the gap between available and desirable results on this issue . a longer , more detailed version of this note is available as arxiv:1507.05307 . i discuss which requirements should be met in order to provide formal support to the the cdnm thesis and then examine existing results in view of these requirements and list some significant unsolved research challenges in that direction .", "topics": ["cluster analysis"]}
{"title": "a shallow high-order parametric approach to data visualization and compression", "abstract": "explicit high-order feature interactions efficiently capture essential structural knowledge about the data of interest and have been used for constructing generative models . we present a supervised discriminative high-order parametric embedding ( hope ) approach to data visualization and compression . compared to deep embedding models with complicated deep architectures , hope generates more effective high-order feature mapping through an embarrassingly simple shallow model . furthermore , two approaches to generating a small number of exemplars conveying high-order interactions to represent large-scale data sets are proposed . these exemplars in combination with the feature mapping learned by hope effectively capture essential data variations . moreover , through hope , these exemplars are employed to increase the computational efficiency of knn classification for fast information retrieval by thousands of times . for classification in two-dimensional embedding space on mnist and usps datasets , our shallow method hope with simple sigmoid transformations significantly outperforms state-of-the-art supervised deep embedding models based on deep neural networks , and even achieved historically low test error rate of 0.65 % in two-dimensional space on mnist , which demonstrates the representational efficiency and power of supervised shallow models with high-order feature interactions .", "topics": ["generative model", "supervised learning"]}
{"title": "efficient preconditioning for noisy separable nmfs by successive projection based low-rank approximations", "abstract": "the successive projection algorithm ( spa ) can quickly solve a nonnegative matrix factorization problem under a separability assumption . even if noise is added to the problem , spa is robust as long as the perturbations caused by the noise are small . in particular , robustness against noise should be high when handling the problems arising from real applications . the preconditioner proposed by gillis and vavasis ( 2015 ) makes it possible to enhance the noise robustness of spa . meanwhile , an additional computational cost is required . the construction of the preconditioner contains a step to compute the top- $ k $ truncated singular value decomposition of an input matrix . it is known that the decomposition provides the best rank- $ k $ approximation to the input matrix ; in other words , a matrix with the smallest approximation error among all matrices of rank less than $ k $ . this step is an obstacle to an efficient implementation of the preconditioned spa . to address the cost issue , we propose a modification of the algorithm for constructing the preconditioner . although the original algorithm uses the best rank- $ k $ approximation , instead of it , our modification uses an alternative . ideally , this alternative should have high approximation accuracy and low computational cost . to ensure this , our modification employs a rank- $ k $ approximation produced by an spa based algorithm . we analyze the accuracy of the approximation and evaluate the computational cost of the algorithm . we then present an empirical study revealing the actual performance of the spa based rank- $ k $ approximation algorithm and the modified preconditioned spa .", "topics": ["approximation algorithm", "approximation"]}
{"title": "stable distribution alignment using the dual of the adversarial distance", "abstract": "methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results . however , these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization . we investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to maximum mean discrepancy . our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max gan-like objective and an mmd objective under the same restrictions . we test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits . in both cases , the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time .", "topics": ["optimization problem", "synthetic data"]}
{"title": "all-but-the-top : simple and effective postprocessing for word representations", "abstract": "real-valued word representations have transformed nlp applications ; popular examples are word2vec and glove , recognized for their ability to capture linguistic regularities . in this paper , we demonstrate a { \\em very simple } , and yet counter-intuitive , postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations { \\em even stronger } . the postprocessing is empirically validated on a variety of lexical-level intrinsic tasks ( word similarity , concept categorization , word analogy ) and sentence-level tasks ( semantic textural similarity and { text classification } ) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages ; in each case , the processed representations are consistently better than the original ones .", "topics": ["natural language processing"]}
{"title": "smoothing multivariate performance measures", "abstract": "a support vector method for multivariate performance measures was recently introduced by joachims ( 2005 ) . the underlying optimization problem is currently solved using cutting plane methods such as svm-perf and bmrm . one can show that these algorithms converge to an eta accurate solution in o ( 1/lambda*e ) iterations , where lambda is the trade-off parameter between the regularizer and the loss function . we present a smoothing strategy for multivariate performance scores , in particular precision/recall break-even point and rocarea . when combined with nesterov 's accelerated gradient algorithm our smoothing strategy yields an optimization algorithm which converges to an eta accurate solution in o ( min { 1/e,1/sqrt ( lambda*e ) } ) iterations . furthermore , the cost per iteration of our scheme is the same as that of svm-perf and bmrm . empirical evaluation on a number of publicly available datasets shows that our method converges significantly faster than cutting plane methods without sacrificing generalization ability .", "topics": ["optimization problem", "loss function"]}
{"title": "modelling radiological language with bidirectional long short-term memory networks", "abstract": "motivated by the need to automate medical information extraction from free-text radiological reports , we present a bi-directional long short-term memory ( bilstm ) neural network architecture for modelling radiological language . the model has been used to address two nlp tasks : medical named-entity recognition ( ner ) and negation detection . we investigate whether learning several types of word embeddings improves bilstm 's performance on those tasks . using a large dataset of chest x-ray reports , we compare the proposed model to a baseline dictionary-based ner system and a negation detection system that leverages the hand-crafted rules of the negex algorithm and the grammatical relations obtained from the stanford dependency parser . compared to these more traditional rule-based systems , we argue that bilstm offers a strong alternative for both our tasks .", "topics": ["baseline ( configuration management )", "natural language processing"]}
{"title": "a fixed-size encoding method for variable-length sequences with its application to neural network language models", "abstract": "in this paper , we propose the new fixed-size ordinally-forgetting encoding ( fofe ) method , which can almost uniquely encode any variable-length sequence of words into a fixed-size representation . fofe can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words . in this work , we have applied fofe to feedforward neural network language models ( fnn-lms ) . experimental results have shown that without using any recurrent feedbacks , fofe based fnn-lms can significantly outperform not only the standard fixed-input fnn-lms but also the popular rnn-lms .", "topics": ["recurrent neural network"]}
{"title": "bayesian neural word embedding", "abstract": "recently , several works in the domain of natural language processing presented successful methods for word embedding . among them , the skip-gram with negative sampling , known also as word2vec , advanced the state-of-the-art of various linguistics tasks . in this paper , we propose a scalable bayesian neural word embedding algorithm . the algorithm relies on a variational bayes solution for the skip-gram objective and a detailed step by step description is provided . we present experimental results that demonstrate the performance of the proposed algorithm for word analogy and similarity tasks on six different datasets and show it is competitive with the original skip-gram method .", "topics": ["natural language processing", "scalability"]}
{"title": "pseudo-recursal : solving the catastrophic forgetting problem in deep neural networks", "abstract": "in general , neural networks are not currently capable of learning tasks in a sequential fashion . when a novel , unrelated task is learnt by a neural network , it substantially forgets how to solve previously learnt tasks . one of the original solutions to this problem is pseudo-rehearsal , which involves learning the new task while rehearsing generated items representative of the previous task/s . this is very effective for simple tasks . however , pseudo-rehearsal has not yet been successfully applied to very complex tasks because in these tasks it is difficult to generate representative items . we accomplish pseudo-rehearsal by using a generative adversarial network to generate items so that our deep network can learn to sequentially classify the cifar-10 , svhn and mnist datasets . after training on all tasks , our network loses only 1.67 % absolute accuracy on cifar-10 and gains 0.24 % absolute accuracy on svhn . our model 's performance is a substantial improvement compared to the current state of the art solution .", "topics": ["neural networks", "mnist database"]}
{"title": "phase transitions and a model order selection criterion for spectral graph clustering", "abstract": "one of the longstanding open problems in spectral graph clustering ( sgc ) is the so-called model order selection problem : automated selection of the correct number of clusters . this is equivalent to the problem of finding the number of connected components or communities in an undirected graph . we propose automated model order selection ( amos ) , a solution to the sgc model selection problem under a random interconnection model ( rim ) using a novel selection criterion that is based on an asymptotic phase transition analysis . amos can more generally be applied to discovering hidden block diagonal structure in symmetric non-negative matrices . numerical experiments on simulated graphs validate the phase transition analysis , and real-world network data is used to validate the performance of the proposed model selection procedure .", "topics": ["cluster analysis", "numerical analysis"]}
{"title": "very fast kernel svm under budget constraints", "abstract": "in this paper we propose a fast online kernel svm algorithm under tight budget constraints . we propose to split the input space using lvq and train a kernel svm in each cluster . to allow for online training , we propose to limit the size of the support vector set of each cluster using different strategies . we show in the experiment that our algorithm is able to achieve high accuracy while having a very high number of samples processed per second both in training and in the evaluation .", "topics": ["kernel ( operating system )", "support vector machine"]}
{"title": "accelerating mcmc via parallel predictive prefetching", "abstract": "we present a general framework for accelerating a large class of widely used markov chain monte carlo ( mcmc ) algorithms . our approach exploits fast , iterative approximations to the target density to speculatively evaluate many potential future steps of the chain in parallel . the approach can accelerate computation of the target distribution of a bayesian inference problem , without compromising exactness , by exploiting subsets of data . it takes advantage of whatever parallel resources are available , but produces results exactly equivalent to standard serial execution . in the initial burn-in phase of chain evaluation , it achieves speedup over serial evaluation that is close to linear in the number of available cores .", "topics": ["sampling ( signal processing )", "approximation"]}
{"title": "cognitive-mapping and contextual pyramid based digital elevation model registration and its effective storage using fractal based compression", "abstract": "digital elevation models ( dem ) are images having terrain information embedded into them . using cognitive mapping concepts for dem registration , has evolved from this basic idea of using the mapping between the space to objects and defining their relationships to form the basic landmarks that need to be marked , stored and manipulated in and about the environment or other candidate environments , namely , in our case , the dems . the progressive two-level encapsulation of methods of geo-spatial cognition includes landmark knowledge and layout knowledge and can be useful for dem registration . space-based approach , that emphasizes on explicit extent of the environment under consideration , and object-based approach , that emphasizes on the relationships between objects in the local environment being the two paradigms of cognitive mapping can be methodically integrated in this three-architecture for dem registration . initially , p-model based segmentation is performed followed by landmark formation for contextual mapping that uses contextual pyramid formation . apart from landmarks being used for registration key-point finding , euclidean distance based deformation calculation has been used for transformation and change detection . landmarks have been categorized to belong to either being flat-plain areas without much variation in the land heights ; peaks that can be found when there is gradual increase in height as compared to the flat areas ; valleys , marked with gradual decrease in the height seen in dem ; and finally , ripple areas with very shallow crests and nadirs . fractal based compression was used for storage of co-registered dems . this method may further be extended for dem-topographic map and dem-to-remote sensed image registration . experimental results further cement the fact that dem registration may be effectively done using the proposed method .", "topics": ["computation"]}
{"title": "possibilistic networks : parameters learning from imprecise data and evaluation strategy", "abstract": "there has been an ever-increasing interest in multidisciplinary research on representing and reasoning with imperfect data . possibilistic networks present one of the powerful frameworks of interest for representing uncertain and imprecise information . this paper covers the problem of their parameters learning from imprecise datasets , i.e . , containing multi-valued data . we propose in the rst part of this paper a possibilistic networks sampling process . in the second part , we propose a likelihood function which explores the link between random sets theory and possibility theory . this function is then deployed to parametrize possibilistic networks .", "topics": ["sampling ( signal processing )"]}
{"title": "binary excess risk for smooth convex surrogates", "abstract": "in statistical learning theory , convex surrogates of the 0-1 loss are highly preferred because of the computational and theoretical virtues that convexity brings in . this is of more importance if we consider smooth surrogates as witnessed by the fact that the smoothness is further beneficial both computationally- by attaining an { \\it optimal } convergence rate for optimization , and in a statistical sense- by providing an improved { \\it optimistic } rate for generalization bound . in this paper we investigate the smoothness property from the viewpoint of statistical consistency and show how it affects the binary excess risk . we show that in contrast to optimization and generalization errors that favor the choice of smooth surrogate loss , the smoothness of loss function may degrade the binary excess risk . motivated by this negative result , we provide a unified analysis that integrates optimization error , generalization bound , and the error in translating convex excess risk into a binary excess risk when examining the impact of smoothness on the binary excess risk . we show that under favorable conditions appropriate choice of smooth convex loss will result in a binary excess risk that is better than $ o ( 1/\\sqrt { n } ) $ .", "topics": ["loss function"]}
{"title": "computing presuppositions by contextual reasoning", "abstract": "this paper describes how automated deduction methods for natural language processing can be applied more efficiently by encoding context in a more elaborate way . our work is based on formal approaches to context , and we provide a tableau calculus for contextual reasoning . this is explained by considering an example from the problem area of presupposition projection .", "topics": ["natural language processing"]}
{"title": "visual storytelling", "abstract": "we introduce the first dataset for sequential vision-to-language , and explore how this data may be used for the task of visual storytelling . the first release of this dataset , sind v.1 , includes 81,743 unique photos in 20,211 sequences , aligned to both descriptive ( caption ) and story language . we establish several strong baselines for the storytelling task , and motivate an automatic metric to benchmark progress . modelling concrete description as well as figurative and social language , as provided in this dataset and the storytelling task , has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression .", "topics": ["baseline ( configuration management )", "artificial intelligence"]}
{"title": "pjait systems for the iwslt 2015 evaluation campaign enhanced by comparable corpora", "abstract": "in this paper , we attempt to improve statistical machine translation ( smt ) systems on a very diverse set of language pairs ( in both directions ) : czech - english , vietnamese - english , french - english and german - english . to accomplish this , we performed translation model training , created adaptations of training settings for each language pair , and obtained comparable corpora for our smt systems . innovative tools and data adaptation techniques were employed . the ted parallel text corpora for the iwslt 2015 evaluation campaign were used to train language models , and to develop , tune , and test the system . in addition , we prepared wikipedia-based comparable corpora for use with our smt system . this data was specified as permissible for the iwslt 2015 evaluation . we explored the use of domain adaptation techniques , symmetrized word alignment models , the unsupervised transliteration models and the kenlm language modeling tool . to evaluate the effects of different preparations on translation results , we conducted experiments and used the bleu , nist and ter metrics . our results indicate that our approach produced a positive impact on smt quality .", "topics": ["machine translation", "text corpus"]}
{"title": "taming the curse of dimensionality : discrete integration by hashing and optimization", "abstract": "integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows . we propose a randomized algorithm that , with high probability , gives a constant-factor approximation of a general discrete integral defined over an exponentially large set . this algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function . as an application , we demonstrate that with a small number of map queries we can efficiently approximate the partition function of discrete graphical models , which can in turn be used , for instance , for marginal computation or model selection .", "topics": ["approximation algorithm", "graphical model"]}
{"title": "image processing in floriculture using a robotic mobile platform", "abstract": "colombia has a privileged geographical location which makes it a cornerstone and equidistant point to all regional markets . the country has a great ecological diversity and it is one of the largest suppliers of flowers for us . colombian flower companies have made innovations in the marketing process , using methods to reach all conditions for final consumers . this article develops a monitoring system for floriculture industries . the system was implemented in a robotic platform . this device has the ability to be programmed in different programming languages . the robot takes the necessary environment information from its camera . the algorithm of the monitoring system was developed with the image processing toolbox on matlab . the implemented algorithm acquires images through its camera , it performs a preprocessing of the image , noise filter , enhancing of the color and adjusting the dimension in order to increase processing speed . then , the image is segmented by color and with the binarized version of the image using morphological operations ( erosion and dilation ) , extract relevant features such as centroid , perimeter and area . the data obtained from the image processing helps the robot with the automatic identification of objectives , orientation and move towards them . also , the results generate a diagnostic quality of each object scanned .", "topics": ["image processing", "noise reduction"]}
{"title": "recurrent attentional networks for saliency detection", "abstract": "convolutional-deconvolution networks can be adopted to perform end-to-end saliency detection . but , they do not work well with objects of multiple scales . to overcome such a limitation , in this work , we propose a recurrent attentional convolutional-deconvolution network ( racdnn ) . using spatial transformer and recurrent network units , racdnn is able to iteratively attend to selected image sub-regions to perform saliency refinement progressively . besides tackling the scale problem , racdnn can also learn context-aware features from past iterations to enhance saliency refinement in future iterations . experiments on several challenging saliency detection datasets validate the effectiveness of racdnn , and show that racdnn outperforms state-of-the-art saliency detection methods .", "topics": ["recurrent neural network", "iteration"]}
{"title": "feature uncertainty bounding schemes for large robust nonlinear svm classifiers", "abstract": "we consider the binary classification problem when data are large and subject to unknown but bounded uncertainties . we address the problem by formulating the nonlinear support vector machine training problem with robust optimization . to do so , we analyze and propose two bounding schemes for uncertainties associated to random approximate features in low dimensional spaces . the proposed techniques are based on random fourier features and the nystr\\ '' om methods . the resulting formulations can be solved with efficient stochastic approximation techniques such as stochastic ( sub ) -gradient , stochastic proximal gradient techniques or their variants .", "topics": ["approximation algorithm", "support vector machine"]}
{"title": "model-powered conditional independence test", "abstract": "we consider the problem of non-parametric conditional independence testing ( ci testing ) for continuous random variables . given i.i.d samples from the joint distribution $ f ( x , y , z ) $ of continuous random vectors $ x , y $ and $ z , $ we determine whether $ x \\perp y | z $ . we approach this by converting the conditional independence test into a classification problem . this allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks . these models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art , for high-dimensional ci testing . the main technical challenge in the classification problem is the need for samples from the conditional product distribution $ f^ { ci } ( x , y , z ) = f ( x|z ) f ( y|z ) f ( z ) $ -- the joint distribution if and only if $ x \\perp y | z . $ -- when given access only to i.i.d . samples from the true joint distribution $ f ( x , y , z ) $ . to tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to $ f^ { ci } $ in terms of total variational distance . we then develop theoretical results regarding the generalization bounds for classification for our problem , which translate into error bounds for ci testing . we provide a novel analysis of rademacher type classification bounds in the presence of non-i.i.d near-independent samples . we empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods .", "topics": ["calculus of variations", "simulation"]}
{"title": "ordering-based search : a simple and effective algorithm for learning bayesian networks", "abstract": "one of the basic tasks for bayesian networks ( bns ) is that of learning a network structure from data . the bn-learning problem is np-hard , so the standard solution is heuristic search . many approaches have been proposed for this task , but only a very small number outperform the baseline of greedy hill-climbing with tabu lists ; moreover , many of the proposed algorithms are quite complex and hard to implement . in this paper , we propose a very simple and easy-to-implement method for addressing this task . our approach is based on the well-known fact that the best network ( of bounded in-degree ) consistent with a given node ordering can be found very efficiently . we therefore propose a search not over the space of structures , but over the space of orderings , selecting for each ordering the best network consistent with it . this search space is much smaller , makes more global search steps , has a lower branching factor , and avoids costly acyclicity checks . we present results for this algorithm on both synthetic and real data sets , evaluating both the score of the network found and in the running time . we show that ordering-based search outperforms the standard baseline , and is competitive with recent algorithms that are much harder to implement .", "topics": ["baseline ( configuration management )", "time complexity"]}
{"title": "distributed robust learning", "abstract": "we propose a framework for distributed robust statistical learning on { \\em big contaminated data } . the distributed robust learning ( drl ) framework can reduce the computational time of traditional robust learning methods by several orders of magnitude . we analyze the robustness property of drl , showing that drl not only preserves the robustness of the base robust learning method , but also tolerates contaminations on a constant fraction of results from computing nodes ( node failures ) . more precisely , even in presence of the most adversarial outlier distribution over computing nodes , drl still achieves a breakdown point of at least $ \\lambda^*/2 $ , where $ \\lambda^* $ is the break down point of corresponding centralized algorithm . this is in stark contrast with naive division-and-averaging implementation , which may reduce the breakdown point by a factor of $ k $ when $ k $ computing nodes are used . we then specialize the drl framework for two concrete cases : distributed robust principal component analysis and distributed robust regression . we demonstrate the efficiency and the robustness advantages of drl through comprehensive simulations and predicting image tags on a large-scale image set .", "topics": ["simulation"]}
{"title": "asymptotic efficiency of deterministic estimators for discrete energy-based models : ratio matching and pseudolikelihood", "abstract": "standard maximum likelihood estimation can not be applied to discrete energy-based models in the general case because the computation of exact model probabilities is intractable . recent research has seen the proposal of several new estimators designed specifically to overcome this intractability , but virtually nothing is known about their theoretical properties . in this paper , we present a generalized estimator that unifies many of the classical and recently proposed estimators . we use results from the standard asymptotic theory for m-estimators to derive a generic expression for the asymptotic covariance matrix of our generalized estimator . we apply these results to study the relative statistical efficiency of classical pseudolikelihood and the recently-proposed ratio matching estimator .", "topics": ["computation"]}
{"title": "large gaps imputation in remote sensed imagery of the environment", "abstract": "imputation of missing data in large regions of satellite imagery is necessary when the acquired image has been damaged by shadows due to clouds , or information gaps produced by sensor failure . the general approach for imputation of missing data , that could not be considered missed at random , suggests the use of other available data . previous work , like local linear histogram matching , take advantage of a co-registered older image obtained by the same sensor , yielding good results in filling homogeneous regions , but poor results if the scenes being combined have radical differences in target radiance due , for example , to the presence of sun glint or snow . this study proposes three different alternatives for filling the data gaps . the first two involves merging radiometric information from a lower resolution image acquired at the same time , in the fourier domain ( method a ) , and using linear regression ( method b ) . the third method consider segmentation as the main target of processing , and propose a method to fill the gaps in the map of classes , avoiding direct imputation ( method c ) . all the methods were compared by means of a large simulation study , evaluating performance with a multivariate response vector with four measures : q , rmse , kappa and overall accuracy coefficients . difference in performance were tested with a manova mixed model design with two main effects , imputation method and type of lower resolution extra data , and a blocking third factor with a nested sub-factor , introduced by the real landsat image and the sub-images that were used . method b proved to be the best for all criteria .", "topics": ["simulation", "coefficient"]}
{"title": "an evolving neuro-fuzzy system with online learning/self-learning", "abstract": "an architecture of a new neuro-fuzzy system is proposed . the basic idea of this approach is to tune both synaptic weights and membership functions with the help of the supervised learning and self-learning paradigms . the approach to solving the problem has to do with evolving online neuro-fuzzy systems that can process data under uncertainty conditions . the results prove the effectiveness of the developed architecture and the learning procedure .", "topics": ["supervised learning"]}
{"title": "on multilingual training of neural dependency parsers", "abstract": "we show that a recently proposed neural dependency parser can be improved by joint training on multiple languages from the same family . the parser is implemented as a deep neural network whose only input is orthographic representations of words . in order to successfully parse , the network has to discover how linguistically relevant concepts can be inferred from word spellings . we analyze the representations of characters and words that are learned by the network to establish which properties of languages were accounted for . in particular we show that the parser has approximately learned to associate latin characters with their cyrillic counterparts and that it can group polish and russian words that have a similar grammatical function . finally , we evaluate the parser on selected languages from the universal dependencies dataset and show that it is competitive with other recently proposed state-of-the art methods , while having a simple structure .", "topics": ["parsing"]}
{"title": "thompson sampling for online learning with linear experts", "abstract": "in this note , we present a version of the thompson sampling algorithm for the problem of online linear generalization with full information ( i.e . , the experts setting ) , studied by kalai and vempala , 2005 . the algorithm uses a gaussian prior and time-varying gaussian likelihoods , and we show that it essentially reduces to kalai and vempala 's follow-the-perturbed-leader strategy , with exponentially distributed noise replaced by gaussian noise . this implies sqrt ( t ) regret bounds for thompson sampling ( with time-varying likelihood ) for online learning with full information .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "toward efficient task assignment and motion planning for large scale underwater mission", "abstract": "an autonomous underwater vehicle ( auv ) needs to acquire a certain degree of autonomy for any particular underwater mission to fulfill the mission objectives successfully and ensure its safety in all stages of the mission in a large scale operating filed . in this paper , a novel combinatorial conflict-free-task assignment strategy consisting an interactive engagement of a local path planner and an adaptive global route planner , is introduced . the method is established upon the heuristic search potency of the particle swarm optimisation ( pso ) algorithm to address the discrete nature of routing-task assignment approach and the complexity of np-hard path planning problem . the proposed hybrid method is highly efficient for having a reactive guidance framework that guarantees successful completion of missions specifically in cluttered environments . to examine the performance of the method in a context of mission productivity , mission time management and vehicle safety , a series of simulation studies are undertaken . the results of simulations declare that the proposed method is reliable and robust , particularly in dealing with uncertainties , and it can significantly enhance the level of vehicle 's autonomy by relying on its reactive nature and capability of providing fast feasible solutions .", "topics": ["simulation", "heuristic"]}
{"title": "neural aggregation network for video face recognition", "abstract": "this paper presents a neural aggregation network ( nan ) for video face recognition . the network takes a face video or face image set of a person with a variable number of face images as its input , and produces a compact , fixed-dimension feature representation for recognition . the whole network is composed of two modules . the feature embedding module is a deep convolutional neural network ( cnn ) which maps each face image to a feature vector . the aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them . due to the attention mechanism , the aggregation is invariant to the image order . our nan is trained with a standard classification or verification loss without any extra supervision signal , and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred , occluded and improperly exposed faces . the experiments on ijb-a , youtube face , celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy .", "topics": ["feature vector"]}
{"title": "brain abnormality detection by deep convolutional neural network", "abstract": "in this paper , we describe our method for classification of brain magnetic resonance ( mr ) images into different abnormalities and healthy classes based on the deep neural network . we propose our method to detect high and low-grade glioma , multiple sclerosis , and alzheimer diseases as well as healthy cases . our network architecture has ten learning layers that include seven convolutional layers and three fully connected layers . we have achieved a promising result in five categories of brain images ( classification task ) with 95.7 % accuracy .", "topics": ["statistical classification"]}
{"title": "stochastic training of neural networks via successive convex approximations", "abstract": "this paper proposes a new family of algorithms for training neural networks ( nns ) . these are based on recent developments in the field of non-convex optimization , going under the general name of successive convex approximation ( sca ) techniques . the basic idea is to iteratively replace the original ( non-convex , highly dimensional ) learning problem with a sequence of ( strongly convex ) approximations , which are both accurate and simple to optimize . differently from similar ideas ( e.g . , quasi-newton algorithms ) , the approximations can be constructed using only first-order information of the neural network function , in a stochastic fashion , while exploiting the overall structure of the learning problem for a faster convergence . we discuss several use cases , based on different choices for the loss function ( e.g . , squared loss and cross-entropy loss ) , and for the regularization of the nn 's weights . we experiment on several medium-sized benchmark problems , and on a large-scale dataset involving simulated physical data . the results show how the algorithm outperforms state-of-the-art techniques , providing faster convergence to a better minimum . additionally , we show how the algorithm can be easily parallelized over multiple computational units without hindering its performance . in particular , each computational unit can optimize a tailored surrogate function defined on a randomly assigned subset of the input variables , whose dimension can be selected depending entirely on the available computational power .", "topics": ["loss function", "neural networks"]}
{"title": "mastering chess and shogi by self-play with a general reinforcement learning algorithm", "abstract": "the game of chess is the most widely-studied domain in the history of artificial intelligence . the strongest programs are based on a combination of sophisticated search techniques , domain-specific adaptations , and handcrafted evaluation functions that have been refined by human experts over several decades . in contrast , the alphago zero program recently achieved superhuman performance in the game of go , by tabula rasa reinforcement learning from games of self-play . in this paper , we generalise this approach into a single alphazero algorithm that can achieve , tabula rasa , superhuman performance in many challenging domains . starting from random play , and given no domain knowledge except the game rules , alphazero achieved within 24 hours a superhuman level of play in the games of chess and shogi ( japanese chess ) as well as go , and convincingly defeated a world-champion program in each case .", "topics": ["computational complexity theory", "optimization problem"]}
{"title": "deepjdot : deep joint distribution optimal transport for unsupervised domain adaptation", "abstract": "in computer vision , one is often confronted with problems of domain shifts , which occur when one applies a classifier trained on a source dataset to target data sharing similar characteristics ( e.g . same classes ) , but also different latent data structures ( e.g . different acquisition conditions ) . in such a situation , the model will perform poorly on the new data , since the classifier is specialized to recognize visual cues specific to the source domain . in this work we explore a solution , named deepjdot , to tackle this problem : through a measure of discrepancy on joint deep representations/labels based on optimal transport , we not only learn new data representations aligned between the source and target domain , but also simultaneously preserve the discriminative information used by the classifier . we applied deepjdot to a series of visual recognition tasks , where it compares favorably against state-of-the-art deep domain adaptation methods .", "topics": ["computer vision"]}
{"title": "named entity sequence classification", "abstract": "named entity recognition ( ner ) aims at locating and classifying named entities in text . in some use cases of ner , including cases where detected named entities are used in creating content recommendations , it is crucial to have a reliable confidence level for the detected named entities . in this work we study the problem of finding confidence levels for detected named entities . we refer to this problem as named entity sequence classification ( nesc ) . we frame nesc as a binary classification problem and we use ner as well as recurrent neural networks to find the probability of candidate named entity is a real named entity . we apply this approach to tweet texts and we show how we could find named entities with high confidence levels from tweets .", "topics": ["entity", "recurrent neural network"]}
{"title": "mixed graphical models for causal analysis of multi-modal variables", "abstract": "graphical causal models are an important tool for knowledge discovery because they can represent both the causal relations between variables and the multivariate probability distributions over the data . once learned , causal graphs can be used for classification , feature selection and hypothesis generation , while revealing the underlying causal network structure and thus allowing for arbitrary likelihood queries over the data . however , current algorithms for learning sparse directed graphs are generally designed to handle only one type of data ( continuous-only or discrete-only ) , which limits their applicability to a large class of multi-modal biological datasets that include mixed type variables . to address this issue , we developed new methods that modify and combine existing methods for finding undirected graphs with methods for finding directed graphs . these hybrid methods are not only faster , but also perform better than the directed graph estimation methods alone for a variety of parameter settings and data set sizes . here , we describe a new conditional independence test for learning directed graphs over mixed data types and we compare performances of different graph learning strategies on synthetic data .", "topics": ["graphical model", "synthetic data"]}
{"title": "multi-agent actor-critic for mixed cooperative-competitive environments", "abstract": "we explore deep reinforcement learning methods for multi-agent domains . we begin by analyzing the difficulty of traditional algorithms in the multi-agent case : q-learning is challenged by an inherent non-stationarity of the environment , while policy gradient suffers from a variance that increases as the number of agents grows . we then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination . additionally , we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies . we show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios , where agent populations are able to discover various physical and informational coordination strategies .", "topics": ["reinforcement learning", "gradient"]}
{"title": "diagnostic classification of lung nodules using 3d neural networks", "abstract": "lung cancer is the leading cause of cancer-related death worldwide . early diagnosis of pulmonary nodules in computed tomography ( ct ) chest scans provides an opportunity for designing effective treatment and making financial and care plans . in this paper , we consider the problem of diagnostic classification between benign and malignant lung nodules in ct images , which aims to learn a direct mapping from 3d images to class labels . to achieve this goal , four two-pathway convolutional neural networks ( cnn ) are proposed , including a basic 3d cnn , a novel multi-output network , a 3d densenet , and an augmented 3d densenet with multi-outputs . these four networks are evaluated on the public lidc-idri dataset and outperform most existing methods . in particular , the 3d multi-output densenet ( modensenet ) achieves the state-of-the-art classification accuracy on the task of end-to-end lung nodule diagnosis . in addition , the networks pretrained on the lidc-idri dataset can be further extended to handle smaller datasets using transfer learning . this is demonstrated on our dataset with encouraging prediction accuracy in lung nodule classification .", "topics": ["neural networks", "end-to-end principle"]}
{"title": "identifying mirror symmetry density with delay in spiking neural networks", "abstract": "the ability to rapidly identify symmetry and anti-symmetry is an essential attribute of intelligence . symmetry perception is a central process in human vision and may be key to human 3d visualization . while previous work in understanding neuron symmetry perception has concentrated on the neuron as an integrator , here we show how the coincidence detecting property of the spiking neuron can be used to reveal symmetry density in spatial data . we develop a method for synchronizing symmetry-identifying spiking artificial neural networks to enable layering and feedback in the network . we show a method for building a network capable of identifying symmetry density between sets of data and present a digital logic implementation demonstrating an 8x8 leaky-integrate-and-fire symmetry detector in a field programmable gate array . our results show that the efficiencies of spiking neural networks can be harnessed to rapidly identify symmetry in spatial data with applications in image processing , 3d computer vision , and robotics .", "topics": ["image processing", "neural networks"]}
{"title": "myths and legends of the baldwin effect", "abstract": "this position paper argues that the baldwin effect is widely misunderstood by the evolutionary computation community . the misunderstandings appear to fall into two general categories . firstly , it is commonly believed that the baldwin effect is concerned with the synergy that results when there is an evolving population of learning individuals . this is only half of the story . the full story is more complicated and more interesting . the baldwin effect is concerned with the costs and benefits of lifetime learning by individuals in an evolving population . several researchers have focussed exclusively on the benefits , but there is much to be gained from attention to the costs . this paper explains the two sides of the story and enumerates ten of the costs and benefits of lifetime learning by individuals in an evolving population . secondly , there is a cluster of misunderstandings about the relationship between the baldwin effect and lamarckian inheritance of acquired characteristics . the baldwin effect is not lamarckian . a lamarckian algorithm is not better for most evolutionary computing problems than a baldwinian algorithm . finally , lamarckian inheritance is not a better model of memetic ( cultural ) evolution than the baldwin effect .", "topics": ["computation"]}
{"title": "men also like shopping : reducing gender bias amplification using corpus-level constraints", "abstract": "language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web . structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora . in this work , we study data and models associated with multilabel object classification and visual semantic role labeling . we find that ( a ) datasets for these tasks contain significant gender bias and ( b ) models trained on these datasets further amplify existing bias . for example , the activity cooking is over 33 % more likely to involve females than males in a training set , and a trained model further amplifies the disparity to 68 % at test time . we propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on lagrangian relaxation for collective inference . our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5 % and 40.5 % for multilabel classification and visual semantic role labeling , respectively .", "topics": ["text corpus"]}
{"title": "multi-task learning of pairwise sequence classification tasks over disparate label spaces", "abstract": "we combine multi-task learning and semi-supervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label embeddings , enabling us to jointly leverage unlabelled data and auxiliary , annotated datasets . we evaluate our approach on a variety of sequence classification tasks with disparate label spaces . we outperform strong single and multi-task baselines and achieve a new state-of-the-art for aspect- and topic-based sentiment analysis .", "topics": ["baseline ( configuration management )"]}
{"title": "classification with the pot-pot plot", "abstract": "we propose a procedure for supervised classification that is based on potential functions . the potential of a class is defined as a kernel density estimate multiplied by the class 's prior probability . the method transforms the data to a potential-potential ( pot-pot ) plot , where each data point is mapped to a vector of potentials . separation of the classes , as well as classification of new data points , is performed on this plot . for this , either the $ \\alpha $ -procedure ( $ \\alpha $ -p ) or $ k $ -nearest neighbors ( $ k $ -nn ) are employed . for data that are generated from continuous distributions , these classifiers prove to be strongly bayes-consistent . the potentials depend on the kernel and its bandwidth used in the density estimate . we investigate several variants of bandwidth selection , including joint and separate pre-scaling and a bandwidth regression approach . the new method is applied to benchmark data from the literature , including simulated data sets as well as 50 sets of real data . it compares favorably to known classification methods such as lda , qda , max kernel density estimates , $ k $ -nn , and $ dd $ -plot classification using depth functions .", "topics": ["supervised learning", "simulation"]}
{"title": "fast parallel same gibbs sampling on general discrete bayesian networks", "abstract": "a fundamental task in machine learning and related fields is to perform inference on bayesian networks . since exact inference takes exponential time in general , a variety of approximate methods are used . gibbs sampling is one of the most accurate approaches and provides unbiased samples from the posterior but it has historically been too expensive for large models . in this paper , we present an optimized , parallel gibbs sampler augmented with state replication ( same or state augmented marginal estimation ) to decrease convergence time . we find that same can improve the quality of parameter estimates while accelerating convergence . experiments on both synthetic and real data show that our gibbs sampler is substantially faster than the state of the art sampler , jags , without sacrificing accuracy . our ultimate objective is to introduce the gibbs sampler to researchers in many fields to expand their range of feasible inference problems .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "letter-based speech recognition with gated convnets", "abstract": "in this paper we introduce a new speech recognition system , leveraging a simple letter-based convnet acoustic model . the acoustic model requires -- only audio transcription for training -- no alignment annotations , nor any forced alignment step is needed . at inference , our decoder takes only a word list and a language model , and is fed with letter scores from the -- acoustic model -- no phonetic word lexicon is needed . key ingredients for the acoustic model are gated linear units and high dropout . we show near state-of-the-art results in word error rate on the librispeech corpus using log-mel filterbanks , both on the `` clean '' and `` other '' configurations .", "topics": ["speech recognition"]}
{"title": "large-scale low-rank matrix learning with nonconvex regularizers", "abstract": "low-rank modeling has many important applications in computer vision and machine learning . while the matrix rank is often approximated by the convex nuclear norm , the use of nonconvex low-rank regularizers has demonstrated better empirical performance . however , the resulting optimization problem is much more challenging . recent state-of-the-art requires an expensive full svd in each iteration . in this paper , we show that for many commonly-used nonconvex low-rank regularizers , a cutoff can be derived to automatically threshold the singular values obtained from the proximal operator . this allows such operator being efficiently approximated by power method . based on it , we develop a proximal gradient algorithm ( and its accelerated variant ) with inexact proximal splitting and prove that a convergence rate of o ( 1/t ) where t is the number of iterations is guaranteed . furthermore , we show the proposed algorithm can be well parallelized , which achieves nearly linear speedup w.r.t the number of threads . extensive experiments are performed on matrix completion and robust principal component analysis , which shows a significant speedup over the state-of-the-art . moreover , the matrix solution obtained is more accurate and has a lower rank than that of the nuclear norm regularizer .", "topics": ["optimization problem", "computer vision"]}
{"title": "cultural diffusion and trends in facebook photographs", "abstract": "online social media is a social vehicle in which people share various moments of their lives with their friends , such as playing sports , cooking dinner or just taking a selfie for fun , via visual means , that is , photographs . our study takes a closer look at the popular visual concepts illustrating various cultural lifestyles from aggregated , de-identified photographs . we perform analysis both at macroscopic and microscopic levels , to gain novel insights about global and local visual trends as well as the dynamics of interpersonal cultural exchange and diffusion among facebook friends . we processed images by automatically classifying the visual content by a convolutional neural network ( cnn ) . through various statistical tests , we find that socially tied individuals more likely post images showing similar cultural lifestyles . to further identify the main cause of the observed social correlation , we use the shuffle test and the preference-based matched estimation ( pme ) test to distinguish the effects of influence and homophily . the results indicate that the visual content of each user 's photographs are temporally , although not necessarily causally , correlated with the photographs of their friends , which may suggest the effect of influence . our paper demonstrates that facebook photographs exhibit diverse cultural lifestyles and preferences and that the social interaction mediated through the visual channel in social media can be an effective mechanism for cultural diffusion .", "topics": ["data mining", "value ( ethics )"]}
{"title": "a deep reinforcement learning chatbot", "abstract": "we present milabot : a deep reinforcement learning chatbot developed by the montreal institute for learning algorithms ( mila ) for the amazon alexa prize competition . milabot is capable of conversing with humans on popular small talk topics through both speech and text . the system consists of an ensemble of natural language generation and retrieval models , including template-based models , bag-of-words models , sequence-to-sequence neural network and latent variable neural network models . by applying reinforcement learning to crowdsourced data and real-world user interactions , the system has been trained to select an appropriate response from the models in its ensemble . the system has been evaluated through a/b testing with real-world users , where it performed significantly better than many competing systems . due to its machine learning architecture , the system is likely to improve with additional data .", "topics": ["reinforcement learning", "natural language"]}
{"title": "$ l_ { 2 , p } $ matrix norm and its application in feature selection", "abstract": "recently , $ l_ { 2,1 } $ matrix norm has been widely applied to many areas such as computer vision , pattern recognition , biological study and etc . as an extension of $ l_1 $ vector norm , the mixed $ l_ { 2,1 } $ matrix norm is often used to find jointly sparse solutions . moreover , an efficient iterative algorithm has been designed to solve $ l_ { 2,1 } $ -norm involved minimizations . actually , computational studies have showed that $ l_p $ -regularization ( $ 0 < p < 1 $ ) is sparser than $ l_1 $ -regularization , but the extension to matrix norm has been seldom considered . this paper presents a definition of mixed $ l_ { 2 , p } $ $ ( p\\in ( 0 , 1 ] ) $ matrix pseudo norm which is thought as both generalizations of $ l_p $ vector norm to matrix and $ l_ { 2,1 } $ -norm to nonconvex cases $ ( 0 < p < 1 ) $ . fortunately , an efficient unified algorithm is proposed to solve the induced $ l_ { 2 , p } $ -norm $ ( p\\in ( 0 , 1 ] ) $ optimization problems . the convergence can also be uniformly demonstrated for all $ p\\in ( 0 , 1 ] $ . typical $ p\\in ( 0,1 ] $ are applied to select features in computational biology and the experimental results show that some choices of $ 0 < p < 1 $ do improve the sparse pattern of using $ p=1 $ .", "topics": ["optimization problem", "matrix regularization"]}
{"title": "modeling compositionality with multiplicative recurrent neural networks", "abstract": "we present the multiplicative recurrent neural network as a general model for compositional meaning in language , and evaluate it on the task of fine-grained sentiment analysis . we establish a connection to the previously investigated matrix-space models for compositionality , and show they are special cases of the multiplicative recurrent net . our experiments show that these models perform comparably or better than elman-type additive recurrent neural networks and outperform matrix-space models on a standard fine-grained sentiment analysis corpus . furthermore , they yield comparable results to structural deep models on the recently published stanford sentiment treebank without the need for generating parse trees .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "compressing word embeddings", "abstract": "recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic . however , these vector space representations ( created through large-scale text analysis ) are typically stored verbatim , since their internal structure is opaque . using word-analogy tests to monitor the level of detail stored in compressed re-representations of the same vector space , the trade-offs between the reduction in memory usage and expressiveness are investigated . a simple scheme is outlined that can reduce the memory footprint of a state-of-the-art embedding by a factor of 10 , with only minimal impact on performance . then , using the same `bit budget ' , a binary ( approximate ) factorisation of the same space is also explored , with the aim of creating an equivalent representation with better interpretability .", "topics": ["sparse matrix"]}
{"title": "issues , challenges and tools of clustering algorithms", "abstract": "clustering is an unsupervised technique of data mining . it means grouping similar objects together and separating the dissimilar ones . each object in the data set is assigned a class label in the clustering process using a distance measure . this paper has captured the problems that are faced in real when clustering algorithms are implemented .it also considers the most extensively used tools which are readily available and support functions which ease the programming . once algorithms have been implemented , they also need to be tested for its validity . there exist several validation indexes for testing the performance and accuracy which have also been discussed here .", "topics": ["data mining", "cluster analysis"]}
{"title": "image processing operations identification via convolutional neural network", "abstract": "in recent years , image forensics has attracted more and more attention , and many forensic methods have been proposed for identifying image processing operations . up to now , most existing methods are based on hand crafted features , and just one specific operation is considered in their methods . in many forensic scenarios , however , multiple classification for various image processing operations is more practical . besides , it is difficult to obtain effective features by hand for some image processing operations . in this paper , therefore , we propose a new convolutional neural network ( cnn ) based method to adaptively learn discriminative features for identifying typical image processing operations . we carefully design the high pass filter bank to get the image residuals of the input image , the channel expansion layer to mix up the resulting residuals , the pooling layers , and the activation functions employed in our method . the extensive results show that the proposed method can outperform the currently best method based on hand crafted features and three related methods based on cnn for image steganalysis and/or forensics , achieving the state-of-the-art results . furthermore , we provide more supplementary results to show the rationality and robustness of the proposed model .", "topics": ["image processing"]}
{"title": "long-distance loop closure using general object landmarks", "abstract": "visual localization under large changes in scale is an important capability in many robotic mapping applications , such as localizing at low altitudes in maps built at high altitudes , or performing loop closure over long distances . existing approaches , however , are robust only up to a 3x difference in scale between map and query images . we propose a novel combination of deep-learning-based object features and hand-engineered point-features that yields improved robustness to scale change , perspective change , and image noise . we conduct experiments in simulation and in real-world outdoor scenes exhibiting up to a 7x change in scale , and compare our approach against localization using state-of-the-art sift features . this technique is training-free and class-agnostic , and in principle can be deployed in any environment out-of-the-box .", "topics": ["simulation", "map"]}
{"title": "mapping out narrative structures and dynamics using networks and textual information", "abstract": "human communication is often executed in the form of a narrative , an account of connected events composed of characters , actions , and settings . a coherent narrative structure is therefore a requisite for a well-formulated narrative -- be it fictional or nonfictional -- for informative and effective communication , opening up the possibility of a deeper understanding of a narrative by studying its structural properties . in this paper we present a network-based framework for modeling and analyzing the structure of a narrative , which is further expanded by incorporating methods from computational linguistics to utilize the narrative text . modeling a narrative as a dynamically unfolding system , we characterize its progression via the growth patterns of the character network , and use sentiment analysis and topic modeling to represent the actual content of the narrative in the form of interaction maps between characters with associated sentiment values and keywords . this is a network framework advanced beyond the simple occurrence-based one most often used until now , allowing one to utilize the unique characteristics of a given narrative to a high degree . given the ubiquity and importance of narratives , such advanced network-based representation and analysis framework may lead to a more systematic modeling and understanding of narratives for social interactions , expression of human sentiments , and communication .", "topics": ["interaction", "map"]}
{"title": "region segmentation for sparse decompositions : better brain parcellations from rest fmri", "abstract": "functional magnetic resonance images acquired during resting-state provide information about the functional organization of the brain through measuring correlations between brain areas . independent components analysis is the reference approach to estimate spatial components from weakly structured data such as brain signal time courses ; each of these components may be referred to as a brain network and the whole set of components can be conceptualized as a brain functional atlas . recently , new methods using a sparsity prior have emerged to deal with low signal-to-noise ratio data . however , even when using sophisticated priors , the results may not be very sparse and most often do not separate the spatial components into brain regions . this work presents post-processing techniques that automatically sparsify brain maps and separate regions properly using geometric operations , and compares these techniques according to faithfulness to data and stability metrics . in particular , among threshold-based approaches , hysteresis thresholding and random walker segmentation , the latter improves significantly the stability of both dense and sparse models .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "densely connected pyramid dehazing network", "abstract": "we propose a new end-to-end single image dehazing method , called densely connected pyramid dehazing network ( dcpdn ) , which can jointly learn the transmission map , atmospheric light and dehazing all together . the end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network , thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing . inspired by the dense network that can maximize the information flow along features from different levels , we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map . this network is optimized using a newly introduced edge-preserving loss function . to further incorporate the mutual structural information between the estimated transmission map and the dehazed result , we propose a joint-discriminator based on generative adversarial network framework to decide whether the corresponding dehazed image and the estimated transmission map are real or fake . an ablation study is conducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result . extensive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods . code will be made available at : https : //github.com/hezhangsprinter", "topics": ["loss function", "end-to-end principle"]}
{"title": "continuous control with deep reinforcement learning", "abstract": "we adapt the ideas underlying the success of deep q-learning to the continuous action domain . we present an actor-critic , model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces . using the same learning algorithm , network architecture and hyper-parameters , our algorithm robustly solves more than 20 simulated physics tasks , including classic problems such as cartpole swing-up , dexterous manipulation , legged locomotion and car driving . our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives . we further demonstrate that for many of the tasks the algorithm can learn policies end-to-end : directly from raw pixel inputs .", "topics": ["reinforcement learning", "simulation"]}
{"title": "generalization tower network : a novel deep neural network architecture for multi-task learning", "abstract": "deep learning ( dl ) advances state-of-the-art reinforcement learning ( rl ) , by incorporating deep neural networks in learning representations from the input to rl . however , the conventional deep neural network architecture is limited in learning representations for multi-task rl ( mt-rl ) , as multiple tasks can refer to different kinds of representations . in this paper , we thus propose a novel deep neural network architecture , namely generalization tower network ( gtn ) , which can achieve mt-rl within a single learned model . specifically , the architecture of gtn is composed of both horizontal and vertical streams . in our gtn architecture , horizontal streams are used to learn representation shared in similar tasks . in contrast , the vertical streams are introduced to be more suitable for handling diverse tasks , which encodes hierarchical shared knowledge of these tasks . the effectiveness of the introduced vertical stream is validated by experimental results . experimental results further verify that our gtn architecture is able to advance the state-of-the-art mt-rl , via being tested on 51 atari games .", "topics": ["reinforcement learning"]}
{"title": "learning shared representations in multi-task reinforcement learning", "abstract": "we investigate a paradigm in multi-task reinforcement learning ( mt-rl ) in which an agent is placed in an environment and needs to learn to perform a series of tasks , within this space . since the environment does not change , there is potentially a lot of common ground amongst tasks and learning to solve them individually seems extremely wasteful . in this paper , we explicitly model and learn this shared structure as it arises in the state-action value space . we will show how one can jointly learn optimal value-functions by modifying the popular value-iteration and policy-iteration procedures to accommodate this shared representation assumption and leverage the power of multi-task supervised learning . finally , we demonstrate that the proposed model and training procedures , are able to infer good value functions , even under low samples regimes . in addition to data efficiency , we will show in our analysis , that learning abstractions of the state space jointly across tasks leads to more robust , transferable representations with the potential for better generalization . this shared representation assumption and leverage the power of multi-task supervised learning . finally , we demonstrate that the proposed model and training procedures , are able to infer good value functions , even under low samples regimes . in addition to data efficiency , we will show in our analysis , that learning abstractions of the state space jointly across tasks leads to more robust , transferable representations with the potential for better generalization .", "topics": ["reinforcement learning"]}
{"title": "recurrent inference machines for solving inverse problems", "abstract": "much of the recent research on solving iterative inference problems focuses on moving away from hand-chosen inference algorithms and towards learned inference . in the latter , the inference process is unrolled in time and interpreted as a recurrent neural network ( rnn ) which allows for joint learning of model and inference parameters with back-propagation through time . in this framework , the rnn architecture is directly derived from a hand-chosen inference algorithm , effectively limiting its capabilities . we propose a learning framework , called recurrent inference machines ( rim ) , in which we turn algorithm construction the other way round : given data and a task , train an rnn to learn an inference algorithm . because rnns are turing complete [ 1 , 2 ] they are capable to implement any inference algorithm . the framework allows for an abstraction which removes the need for domain knowledge . we demonstrate in several image restoration experiments that this abstraction is effective , allowing us to achieve state-of-the-art performance on image denoising and super-resolution tasks and superior across-task generalization .", "topics": ["recurrent neural network", "noise reduction"]}
{"title": "abductive matching in question answering", "abstract": "we study question-answering over semi-structured data . we introduce a new way to apply the technique of semantic parsing by applying machine learning only to provide annotations that the system infers to be missing ; all the other parsing logic is in the form of manually authored rules . in effect , the machine learning is used to provide non-syntactic matches , a step that is ill-suited to manual rules . the advantage of this approach is in its debuggability and in its transparency to the end-user . we demonstrate the effectiveness of the approach by achieving state-of-the-art performance of 40.42 % accuracy on a standard benchmark dataset over tables from wikipedia .", "topics": ["parsing"]}
{"title": "joint learning from earth observation and openstreetmap data to get faster better semantic maps", "abstract": "in this work , we investigate the use of openstreetmap data for semantic labeling of earth observation images . deep neural networks have been used in the past for remote sensing data classification from various sensors , including multispectral , hyperspectral , sar and lidar data . while openstreetmap has already been used as ground truth data for training such networks , this abundant data source remains rarely exploited as an input information layer . in this paper , we study different use cases and deep network architectures to leverage openstreetmap data for semantic labeling of aerial and satellite images . especially , we look into fusion based architectures and coarse-to-fine segmentation to include the openstreetmap layer into multispectral-based deep fully convolutional networks . we illustrate how these methods can be successfully used on two public datasets : isprs potsdam and dfc2017 . we show that openstreetmap data can efficiently be integrated into the vision-based deep learning models and that it significantly improves both the accuracy performance and the convergence speed of the networks .", "topics": ["ground truth", "sensor"]}
{"title": "equation embeddings", "abstract": "we present an unsupervised approach for discovering semantic representations of mathematical equations . equations are challenging to analyze because each is unique , or nearly unique . our method , which we call equation embeddings , finds good representations of equations by using the representations of their surrounding words . we used equation embeddings to analyze four collections of scientific articles from the arxiv , covering four computer science domains ( nlp , ir , ai , and ml ) and $ \\sim $ 98.5k equations . quantitatively , we found that equation embeddings provide better models when compared to existing word embedding approaches . qualitatively , we found that equation embeddings provide coherent semantic representations of equations and can capture semantic similarity to other equations and to words .", "topics": ["natural language processing", "unsupervised learning"]}
{"title": "data noising as smoothing in neural network language models", "abstract": "data noising is an effective technique for regularizing neural network models . while noising is widely adopted in application domains such as vision and speech , commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling . in this paper , we derive a connection between input noising in neural network language models and smoothing in $ n $ -gram models . using this connection , we draw upon ideas from smoothing to develop effective noising schemes . we demonstrate performance gains when applying the proposed schemes to language modeling and machine translation . finally , we provide empirical analysis validating the relationship between noising and smoothing .", "topics": ["machine translation"]}
{"title": "supersaliency : predicting smooth pursuit-based attention with slicing cnns improves fixation prediction for naturalistic videos", "abstract": "predicting attention is a popular topic at the intersection of human and computer vision , but video saliency prediction has only recently begun to benefit from deep learning-based approaches . even though most of the available video-based saliency data sets and models claim to target human observers ' fixations , they fail to differentiate them from smooth pursuit ( sp ) , a major eye movement type that is unique to perception of dynamic scenes . in this work , we aim to make this distinction explicit , to which end we ( i ) use both algorithmic and manual annotations of sp traces and other eye movements for two well-established video saliency data sets , ( ii ) train slicing convolutional neural networks ( s-cnn ) for saliency prediction on either fixation- or sp-salient locations , and ( iii ) evaluate ours and over 20 popular published saliency models on the two annotated data sets for predicting both sp and fixations , as well as on another data set of human fixations . our proposed model , trained on an independent set of videos , outperforms the state-of-the-art saliency models in the task of sp prediction on all considered data sets . moreover , this model also demonstrates superior performance in the prediction of `` classical '' fixation-based saliency . our results emphasize the importance of selectively approaching training set construction for attention modelling .", "topics": ["test set", "computer vision"]}
{"title": "learning causal structures using regression invariance", "abstract": "we study causal inference in a multi-environment setting , in which the functional relations for producing the variables from their direct causes remain the same across environments , while the distribution of exogenous noises may vary . we introduce the idea of using the invariance of the functional relations of the variables to their causes across a set of environments . we define a notion of completeness for a causal inference algorithm in this setting and prove the existence of such algorithm by proposing the baseline algorithm . additionally , we present an alternate algorithm that has significantly improved computational and sample complexity compared to the baseline algorithm . the experiment results show that the proposed algorithm outperforms the other existing algorithms .", "topics": ["baseline ( configuration management )", "causality"]}
{"title": "hybrid optimization algorithm for large-scale qos-aware service composition", "abstract": "in this paper we present a hybrid approach for automatic composition of web services that generates semantic input-output based compositions with optimal end-to-end qos , minimizing the number of services of the resulting composition . the proposed approach has four main steps : 1 ) generation of the composition graph for a request ; 2 ) computation of the optimal composition that minimizes a single objective qos function ; 3 ) multi-step optimizations to reduce the search space by identifying equivalent and dominated services ; and 4 ) hybrid local-global search to extract the optimal qos with the minimum number of services . an extensive validation with the datasets of the web service challenge 2009-2010 and randomly generated datasets shows that : 1 ) the combination of local and global optimization is a general and powerful technique to extract optimal compositions in diverse scenarios ; and 2 ) the hybrid strategy performs better than the state-of-the-art , obtaining solutions with less services and optimal qos .", "topics": ["computation", "end-to-end principle"]}
{"title": "a harmonic mean linear discriminant analysis for robust image classification", "abstract": "linear discriminant analysis ( lda ) is a widely-used supervised dimensionality reduction method in computer vision and pattern recognition . in null space based lda ( nlda ) , a well-known lda extension , between-class distance is maximized in the null space of the within-class scatter matrix . however , there are some limitations in nlda . firstly , for many data sets , null space of within-class scatter matrix does not exist , thus nlda is not applicable to those datasets . secondly , nlda uses arithmetic mean of between-class distances and gives equal consideration to all between-class distances , which makes larger between-class distances can dominate the result and thus limits the performance of nlda . in this paper , we propose a harmonic mean based linear discriminant analysis , multi-class discriminant analysis ( mcda ) , for image classification , which minimizes the reciprocal of weighted harmonic mean of pairwise between-class distance . more importantly , mcda gives higher priority to maximize small between-class distances . mcda can be extended to multi-label dimension reduction . results on 7 single-label data sets and 4 multi-label data sets show that mcda has consistently better performance than 10 other single-label approaches and 4 other multi-label approaches in terms of classification accuracy , macro and micro average f1 score .", "topics": ["computer vision"]}
{"title": "ensemble committees for stock return classification and prediction", "abstract": "this paper considers a portfolio trading strategy formulated by algorithms in the field of machine learning . the profitability of the strategy is measured by the algorithm 's capability to consistently and accurately identify stock indices with positive or negative returns , and to generate a preferred portfolio allocation on the basis of a learned model . stocks are characterized by time series data sets consisting of technical variables that reflect market conditions in a previous time interval , which are utilized produce binary classification decisions in subsequent intervals . the learned model is constructed as a committee of random forest classifiers , a non-linear support vector machine classifier , a relevance vector machine classifier , and a constituent ensemble of k-nearest neighbors classifiers . the global industry classification standard ( gics ) is used to explore the ensemble model 's efficacy within the context of various fields of investment including energy , materials , financials , and information technology . data from 2006 to 2012 , inclusive , are considered , which are chosen for providing a range of market circumstances for evaluating the model . the model is observed to achieve an accuracy of approximately 70 % when predicting stock price returns three months in advance .", "topics": ["cluster analysis", "time series"]}
{"title": "detailed derivations of small-variance asymptotics for some hierarchical bayesian nonparametric models", "abstract": "in this note we provide detailed derivations of two versions of small-variance asymptotics for hierarchical dirichlet process ( hdp ) mixture models and the hdp hidden markov model ( hdp-hmm , a.k.a . the infinite hmm ) . we include derivations for the probabilities of certain crp and crf partitions , which are of more general interest .", "topics": ["cluster analysis", "time series"]}
{"title": "training of deep neural networks based on distance measures using rmsprop", "abstract": "the vanishing gradient problem was a major obstacle for the success of deep learning . in recent years it was gradually alleviated through multiple different techniques . however the problem was not really overcome in a fundamental way , since it is inherent to neural networks with activation functions based on dot products . in a series of papers , we are going to analyze alternative neural network structures which are not based on dot products . in this first paper , we revisit neural networks built up of layers based on distance measures and gaussian activation functions . these kinds of networks were only sparsely used in the past since they are hard to train when using plain stochastic gradient descent methods . we show that by using root mean square propagation ( rmsprop ) it is possible to efficiently learn multi-layer neural networks . furthermore we show that when appropriately initialized these kinds of neural networks suffer much less from the vanishing and exploding gradient problem than traditional neural networks even for deep networks .", "topics": ["neural networks", "gradient descent"]}
{"title": "a logic for global and local announcements", "abstract": "in this paper we introduce { \\em global and local announcement logic } ( glal ) , a dynamic epistemic logic with two distinct announcement operators -- $ [ \\phi ] ^+_a $ and $ [ \\phi ] ^-_a $ indexed to a subset $ a $ of the set $ ag $ of all agents -- for global and local announcements respectively . the boundary case $ [ \\phi ] ^+_ { ag } $ corresponds to the public announcement of $ \\phi $ , as known from the literature . unlike standard public announcements , which are { \\em model transformers } , the global and local announcements are { \\em pointed model transformers } . in particular , the update induced by the announcement may be different in different states of the model . therefore , the resulting computations are trees of models , rather than the typical sequences . a consequence of our semantics is that modally bisimilar states may be distinguished in our logic . then , we provide a stronger notion of bisimilarity and we show that it preserves modal equivalence in glal . additionally , we show that glal is strictly more expressive than public announcement logic with common knowledge . we prove a wide range of validities for glal involving the interaction between dynamics and knowledge , and show that the satisfiability problem for glal is decidable . we illustrate the formal machinery by means of detailed epistemic scenarios .", "topics": ["computation"]}
{"title": "new insights and perspectives on the natural gradient method", "abstract": "natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry , and works well for many applications as an alternative to stochastic gradient descent . in this paper we critically analyze this method and its properties , and show how it can be viewed as a type of approximate 2nd-order optimization method , where the fisher information matrix can be viewed as an approximation of the hessian . this perspective turns out to have significant implications for how to design a practical and robust version of the method . additionally , we make the following contributions to the understanding of natural gradient and 2nd-order methods : a thorough analysis of the convergence speed of stochastic natural gradient descent ( and more general stochastic 2nd-order methods ) as applied to convex quadratics , a critical examination of the oft-used `` empirical '' approximation of the fisher matrix , and an analysis of the ( approximate ) parameterization invariance property possessed by natural gradient methods , which we show still holds for certain choices of the curvature matrix other than the fisher , but notably not the hessian .", "topics": ["approximation algorithm", "gradient descent"]}
{"title": "accelerating innovation through analogy mining", "abstract": "the availability of large idea repositories ( e.g . , the u.s. patent database ) could significantly accelerate innovation and discovery by providing people with inspiration from solutions to analogous problems . however , finding useful analogies in these large , messy , real-world repositories remains a persistent challenge for either human or automated methods . previous approaches include costly hand-created databases that have high relational structure ( e.g . , predicate calculus representations ) but are very sparse . simpler machine-learning/information-retrieval similarity metrics can scale to large , natural-language datasets , but struggle to account for structural similarity , which is central to analogy . in this paper we explore the viability and value of learning simpler structural representations , specifically , `` problem schemas '' , which specify the purpose of a product and the mechanisms by which it achieves that purpose . our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions . we demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional information-retrieval methods . in an ideation experiment , analogies retrieved by our models significantly increased people 's likelihood of generating creative ideas compared to analogies retrieved by traditional methods . our results suggest a promising approach to enabling computational analogy at scale is to learn and leverage weaker structural representations .", "topics": ["recurrent neural network", "natural language"]}
{"title": "invariant stochastic encoders", "abstract": "the theory of stochastic vector quantisers ( svq ) has been extended to allow the quantiser to develop invariances , so that only `` large '' degrees of freedom in the input vector are represented in the code . this has been applied to the problem of encoding data vectors which are a superposition of a `` large '' jammer and a `` small '' signal , so that only the jammer is represented in the code . this allows the jammer to be subtracted from the total input vector ( i.e . the jammer is nulled ) , leaving a residual that contains only the underlying signal . the main advantage of this approach to jammer nulling is that little prior knowledge of the jammer is assumed , because these properties are automatically discovered by the svq as it is trained on examples of input vectors .", "topics": ["encoder"]}
{"title": "unsupervised learning for computational phenotyping", "abstract": "with large volumes of health care data comes the research area of computational phenotyping , making use of techniques such as machine learning to describe illnesses and other clinical concepts from the data itself . the `` traditional '' approach of using supervised learning relies on a domain expert , and has two main limitations : requiring skilled humans to supply correct labels limits its scalability and accuracy , and relying on existing clinical descriptions limits the sorts of patterns that can be found . for instance , it may fail to acknowledge that a disease treated as a single condition may really have several subtypes with different phenotypes , as seems to be the case with asthma and heart disease . some recent papers cite successes instead using unsupervised learning . this shows great potential for finding patterns in electronic health records that would otherwise be hidden and that can lead to greater understanding of conditions and treatments . this work implements a method derived strongly from lasko et al . , but implements it in apache spark and python and generalizes it to laboratory time-series data in mimic-iii . it is released as an open-source tool for exploration , analysis , and visualization , available at https : //github.com/hodapp87/mimic3_phenotyping", "topics": ["supervised learning", "time series"]}
{"title": "numerical approaches for linear left-invariant diffusions on se ( 2 ) , their comparison to exact solutions , and their applications in retinal imaging", "abstract": "left-invariant pde-evolutions on the roto-translation group $ se ( 2 ) $ ( and their resolvent equations ) have been widely studied in the fields of cortical modeling and image analysis . they include hypo-elliptic diffusion ( for contour enhancement ) proposed by citti & sarti , and petitot , and they include the direction process ( for contour completion ) proposed by mumford . this paper presents a thorough study and comparison of the many numerical approaches , which , remarkably , is missing in the literature . existing numerical approaches can be classified into 3 categories : finite difference methods , fourier based methods ( equivalent to $ se ( 2 ) $ -fourier methods ) , and stochastic methods ( monte carlo simulations ) . there are also 3 types of exact solutions to the pde-evolutions that were derived explicitly ( in the spatial fourier domain ) in previous works by duits and van almsick in 2005 . here we provide an overview of these 3 types of exact solutions and explain how they relate to each of the 3 numerical approaches . we compute relative errors of all numerical approaches to the exact solutions , and the fourier based methods show us the best performance with smallest relative errors . we also provide an improvement of mathematica algorithms for evaluating mathieu-functions , crucial in implementations of the exact solutions . furthermore , we include an asymptotical analysis of the singularities within the kernels and we propose a probabilistic extension of underlying stochastic processes that overcomes the singular behavior in the origin of time-integrated kernels . finally , we show retinal imaging applications of combining left-invariant pde-evolutions with invertible orientation scores .", "topics": ["numerical analysis", "simulation"]}
{"title": "meshed up : learnt error correction in 3d reconstructions", "abstract": "dense reconstructions often contain errors that prior work has so far minimised using high quality sensors and regularising the output . nevertheless , errors still persist . this paper proposes a machine learning technique to identify errors in three dimensional ( 3d ) meshes . beyond simply identifying errors , our method quantifies both the magnitude and the direction of depth estimate errors when viewing the scene . this enables us to improve the reconstruction accuracy . we train a suitably deep network architecture with two 3d meshes : a high-quality laser reconstruction , and a lower quality stereo image reconstruction . the network predicts the amount of error in the lower quality reconstruction with respect to the high-quality one , having only view the former through its input . we evaluate our approach by correcting two-dimensional ( 2d ) inverse-depth images extracted from the 3d model , and show that our method improves the quality of these depth reconstructions by up to a relative 10 % rmse .", "topics": ["sensor"]}
{"title": "beyond the zipf-mandelbrot law in quantitative linguistics", "abstract": "in this paper the zipf-mandelbrot law is revisited in the context of linguistics . despite its widespread popularity the zipf -- mandelbrot law can only describe the statistical behaviour of a rather restricted fraction of the total number of words contained in some given corpus . in particular , we focus our attention on the important deviations that become statistically relevant as larger corpora are considered and that ultimately could be understood as salient features of the underlying complex process of language generation . finally , it is shown that all the different observed regimes can be accurately encompassed within a single mathematical framework recently introduced by c . tsallis .", "topics": ["text corpus"]}
{"title": "power-law graph cuts", "abstract": "algorithms based on spectral graph cut objectives such as normalized cuts , ratio cuts and ratio association have become popular in recent years because they are widely applicable and simple to implement via standard eigenvector computations . despite strong performance for a number of clustering tasks , spectral graph cut algorithms still suffer from several limitations : first , they require the number of clusters to be known in advance , but this information is often unknown a priori ; second , they tend to produce clusters with uniform sizes . in some cases , the true clusters exhibit a known size distribution ; in image segmentation , for instance , human-segmented images tend to yield segment sizes that follow a power-law distribution . in this paper , we propose a general framework of power-law graph cut algorithms that produce clusters whose sizes are power-law distributed , and also does not fix the number of clusters upfront . to achieve our goals , we treat the pitman-yor exchangeable partition probability function ( eppf ) as a regularizer to graph cut objectives . because the resulting objectives can not be solved by relaxing via eigenvectors , we derive a simple iterative algorithm to locally optimize the objectives . moreover , we show that our proposed algorithm can be viewed as performing map inference on a particular pitman-yor mixture model . our experiments on various data sets show the effectiveness of our algorithms .", "topics": ["cluster analysis", "image segmentation"]}
{"title": "time series segmentation through automatic feature learning", "abstract": "internet of things ( iot ) applications have become increasingly popular in recent years , with applications ranging from building energy monitoring to personal health tracking and activity recognition . in order to leverage these data , automatic knowledge extraction - whereby we map from observations to interpretable states and transitions - must be done at scale . as such , we have seen many recent iot data sets include annotations with a human expert specifying states , recorded as a set of boundaries and associated labels in a data sequence . these data can be used to build automatic labeling algorithms that produce labels as an expert would . here , we refer to human-specified boundaries as breakpoints . traditional changepoint detection methods only look for statistically-detectable boundaries that are defined as abrupt variations in the generative parameters of a data sequence . however , we observe that breakpoints occur on more subtle boundaries that are non-trivial to detect with these statistical methods . in this work , we propose a new unsupervised approach , based on deep learning , that outperforms existing techniques and learns the more subtle , breakpoint boundaries with a high accuracy . through extensive experiments on various real-world data sets - including human-activity sensing data , speech signals , and electroencephalogram ( eeg ) activity traces - we demonstrate the effectiveness of our algorithm for practical applications . furthermore , we show that our approach achieves significantly better performance than previous methods .", "topics": ["feature learning", "time series"]}
{"title": "asymptotic learnability of reinforcement problems with arbitrary dependence", "abstract": "we address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions . the task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments . we find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class . we analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields , such as markov decision processes and mixing conditions .", "topics": ["reinforcement learning"]}
{"title": "exploring approximations for floating-point arithmetic using uppsat", "abstract": "we consider the problem of solving floating-point constraints obtained from software verification . we present uppsat -- - a new implementation of a systematic approximation refinement framework [ zwr17 ] as an abstract smt solver . provided with an approximation and a decision procedure ( implemented in an off-the-shelf smt solver ) , uppsat yields an approximating smt solver . additionally , uppsat includes a library of predefined approximation components which can be combined and extended to define new encodings , orderings and solving strategies . we propose that uppsat can be used as a sandbox for easy and flexible exploration of new approximations . to substantiate this , we explore several approximations of floating-point arithmetic . approximations can be viewed as a composition of an encoding into a target theory , a precision ordering , and a number of strategies for model reconstruction and precision ( or approximation ) refinement . we present encodings of floating-point arithmetic into reduced precision floating-point arithmetic , real-arithmetic , and fixed-point arithmetic ( encoded in the theory of bit-vectors ) . in an experimental evaluation , we compare the advantages and disadvantages of approximating solvers obtained by combining various encodings and decision procedures ( based on existing state-of-the-art smt solvers for floating-point , real , and bit-vector arithmetic ) .", "topics": ["approximation algorithm", "approximation"]}
{"title": "reservoir computing for spatiotemporal signal classification without trained output weights", "abstract": "reservoir computing is a recently introduced machine learning paradigm that has been shown to be well-suited for the processing of spatiotemporal data . rather than training the network node connections and weights via backpropagation in traditional recurrent neural networks , reservoirs instead have fixed connections and weights among the `hidden layer ' nodes , and traditionally only the weights to the output layer of neurons are trained using linear regression . we claim that for signal classification tasks one may forgo the weight training step entirely and instead use a simple supervised clustering method based upon principal components of norms of reservoir states . the proposed method is mathematically analyzed and explored through numerical experiments on real-world data . the examples demonstrate that the proposed may outperform the traditional trained output weight approach in terms of classification accuracy and sensitivity to reservoir parameters .", "topics": ["recurrent neural network", "cluster analysis"]}
{"title": "a comparative study of meta-heuristic algorithms for solving quadratic assignment problem", "abstract": "quadratic assignment problem ( qap ) is an np-hard combinatorial optimization problem , therefore , solving the qap requires applying one or more of the meta-heuristic algorithms . this paper presents a comparative study between meta-heuristic algorithms : genetic algorithm , tabu search , and simulated annealing for solving a real-life ( qap ) and analyze their performance in terms of both runtime efficiency and solution quality . the results show that genetic algorithm has a better solution quality while tabu search has a faster execution time in comparison with other meta-heuristic algorithms for solving qap .", "topics": ["optimization problem", "heuristic"]}
{"title": "algorithms for approximate minimization of the difference between submodular functions , with applications", "abstract": "we extend the work of narasimhan and bilmes [ 30 ] for minimizing set functions representable as a difference between submodular functions . similar to [ 30 ] , our new algorithms are guaranteed to monotonically reduce the objective function at every step . we empirically and theoretically show that the per-iteration cost of our algorithms is much less than [ 30 ] , and our algorithms can be used to efficiently minimize a difference between submodular functions under various combinatorial constraints , a problem not previously addressed . we provide computational bounds and a hardness result on the mul- tiplicative inapproximability of minimizing the difference between submodular functions . we show , however , that it is possible to give worst-case additive bounds by providing a polynomial time computable lower-bound on the minima . finally we show how a number of machine learning problems can be modeled as minimizing the difference between submodular functions . we experimentally show the validity of our algorithms by testing them on the problem of feature selection with submodular cost features .", "topics": ["optimization problem", "time complexity"]}
{"title": "learning k-way d-dimensional discrete code for compact embedding representations", "abstract": "embedding methods such as word embedding have become pillars for many applications containing discrete structures . conventional embedding methods directly associate each symbol with a continuous embedding vector , which is equivalent to applying linear transformation based on `` one-hot '' encoding of the discrete symbols . despite its simplicity , such approach yields number of parameters that grows linearly with the vocabulary size and can lead to overfitting . in this work we propose a much more compact k-way d-dimensional discrete encoding scheme to replace the `` one-hot '' encoding . in `` kd encoding '' , each symbol is represented by a $ d $ -dimensional code , and each of its dimension has a cardinality of $ k $ . the final symbol embedding vector can be generated by composing the code embedding vectors . to learn the semantically meaningful code , we derive a relaxed discrete optimization technique based on stochastic gradient descent . by adopting the new coding system , the efficiency of parameterization can be significantly improved ( from linear to logarithmic ) , and this can also mitigate the over-fitting problem . in our experiments with language modeling , the number of embedding parameters can be reduced by 97\\ % while achieving similar or better performance .", "topics": ["gradient descent", "gradient"]}
{"title": "higher-order factorization machines", "abstract": "factorization machines ( fms ) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional . unfortunately , despite increasing interest in fms , there exists to date no efficient training algorithm for higher-order fms ( hofms ) . in this paper , we present the first generic yet efficient algorithms for training arbitrary-order hofms . we also present new variants of hofms with shared parameters , which greatly reduce model size and prediction times while maintaining similar accuracy . we demonstrate the proposed approaches on four different link prediction tasks .", "topics": ["supervised learning"]}
{"title": "variable computation in recurrent neural networks", "abstract": "recurrent neural networks ( rnns ) have been used extensively and with increasing success to model various types of sequential data . much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data , such as long range dependency or localized attention phenomena . however , while many sequential data ( such as video , speech or language ) can have highly variable information flow , most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step , which can be detrimental to both speed and model capacity . in this paper , we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step , without prior knowledge of the sequence 's time structure . we show experimentally that not only do our models require fewer operations , they also lead to better performance overall on evaluation tasks .", "topics": ["recurrent neural network", "computational complexity theory"]}
{"title": "egotransfer : transferring motion across egocentric and exocentric domains using deep neural networks", "abstract": "mirror neurons have been observed in the primary motor cortex of primate species , in particular in humans and monkeys . a mirror neuron fires when a person performs a certain action , and also when he observes the same action being performed by another person . a crucial step towards building fully autonomous intelligent systems with human-like learning abilities is the capability in modeling the mirror neuron . on one hand , the abundance of egocentric cameras in the past few years has offered the opportunity to study a lot of vision problems from the first-person perspective . a great deal of interesting research has been done during the past few years , trying to explore various computer vision tasks from the perspective of the self . on the other hand , videos recorded by traditional static cameras , capture humans performing different actions from an exocentric third-person perspective . in this work , we take the first step towards relating motion information across these two perspectives . we train models that predict motion in an egocentric view , by observing it from an exocentric view , and vice versa . this allows models to predict how an egocentric motion would look like from outside . to do so , we train linear and nonlinear models and evaluate their performance in terms of retrieving the egocentric ( exocentric ) motion features , while having access to an exocentric ( egocentric ) motion feature . our experimental results demonstrate that motion information can be successfully transferred across the two views .", "topics": ["nonlinear system", "computer vision"]}
{"title": "per-instance differential privacy and the adaptivity of posterior sampling in linear and ridge regression", "abstract": "differential privacy ( dp ) , ever since its advent , has been a controversial object . on the one hand , it provides strong provable protection of individuals in a data set , on the other hand , it has been heavily criticized for being not practical , partially due to its complete independence to the actual data set it tries to protect . in this paper , we address this issue by a new and more fine-grained notion of differential privacy -- - per instance differential privacy ( pdp ) , which captures the privacy of a specific individual with respect to a fixed data set . we show that this is a strict generalization of the standard dp and inherits all its desirable properties , e.g . , composition , invariance to side information and closedness to postprocessing , except that they all hold for every instance separately . when the data is drawn from a distribution , we show that per-instance dp implies generalization . moreover , we provide explicit calculations of the per-instance dp for the output perturbation on a class of smooth learning problems . the result reveals an interesting and intuitive fact that an individual has stronger privacy if he/she has small `` leverage score '' with respect to the data set and if he/she can be predicted more accurately using the leave-one-out data set . using the developed techniques , we provide a novel analysis of the one-posterior-sample ( ops ) estimator and show that when the data set is well-conditioned it provides $ ( \\epsilon , \\delta ) $ -pdp for any target individuals and matches the exact lower bound up to a $ 1+\\tilde { o } ( n^ { -1 } \\epsilon^ { -2 } ) $ multiplicative factor . we also propose adaops which uses adaptive regularization to achieve the same results with $ ( \\epsilon , \\delta ) $ -dp . simulation shows several orders-of-magnitude more favorable privacy and utility trade-off when we consider the privacy of only the users in the data set .", "topics": ["simulation", "coefficient"]}
{"title": "the mrmr variable selection method : a comparative study for functional data", "abstract": "the use of variable selection methods is particularly appealing in statistical problems with functional data . the obvious general criterion for variable selection is to choose the `most representative ' or `most relevant ' variables . however , it is also clear that a purely relevance-oriented criterion could lead to select many redundant variables . the mrmr ( minimum redundance maximum relevance ) procedure , proposed by ding and peng ( 2005 ) and peng et al . ( 2005 ) is an algorithm to systematically perform variable selection , achieving a reasonable trade-off between relevance and redundancy . in its original form , this procedure is based on the use of the so-called mutual information criterion to assess relevance and redundancy . keeping the focus on functional data problems , we propose here a modified version of the mrmr method , obtained by replacing the mutual information by the new association measure ( called distance correlation ) suggested by sz\\'ekely et al . ( 2007 ) . we have also performed an extensive simulation study , including 1600 functional experiments ( 100 functional models $ \\times $ 4 sample sizes $ \\times $ 4 classifiers ) and three real-data examples aimed at comparing the different versions of the mrmr methodology . the results are quite conclusive in favor of the new proposed alternative .", "topics": ["simulation", "relevance"]}
{"title": "convolutional neural fabrics", "abstract": "despite the success of cnns , selecting the optimal architecture for a given task remains an open problem . instead of aiming to select a single optimal architecture , we propose a `` fabric '' that embeds an exponentially large number of architectures . the fabric consists of a 3d trellis that connects response maps at different layers , scales , and channels with a sparse homogeneous local connectivity pattern . the only hyper-parameters of a fabric are the number of channels and layers . while individual architectures can be recovered as paths , the fabric can in addition ensemble all embedded architectures together , sharing their weights where their paths overlap . parameters can be learned using standard methods based on back-propagation , at a cost that scales linearly in the fabric size . we present benchmark results competitive with the state of the art for image classification on mnist and cifar10 , and for semantic segmentation on the part labels dataset .", "topics": ["computer vision", "map"]}
{"title": "distortion varieties", "abstract": "the distortion varieties of a given projective variety are parametrized by duplicating coordinates and multiplying them with monomials . we study their degrees and defining equations . exact formulas are obtained for the case of one-parameter distortions . these are based on chow polytopes and gr\\ '' obner bases . multi-parameter distortions are studied using tropical geometry . the motivation for distortion varieties comes from multi-view geometry in computer vision . our theory furnishes a new framework for formulating and solving minimal problems for camera models with image distortion .", "topics": ["computer vision"]}
{"title": "learning unmanned aerial vehicle control for autonomous target following", "abstract": "while deep reinforcement learning ( rl ) methods have achieved unprecedented successes in a range of challenging problems , their applicability has been mainly limited to simulation or game domains due to the high sample complexity of the trial-and-error learning process . however , real-world robotic applications often need a data-efficient learning process with safety-critical constraints . in this paper , we consider the challenging problem of learning unmanned aerial vehicle ( uav ) control for tracking a moving target . to acquire a strategy that combines perception and control , we represent the policy by a convolutional neural network . we develop a hierarchical approach that combines a model-free policy gradient method with a conventional feedback proportional-integral-derivative ( pid ) controller to enable stable learning without catastrophic failure . the neural network is trained by a combination of supervised learning from raw images and reinforcement learning from games of self-play . we show that the proposed approach can learn a target following policy in a simulator efficiently and the learned behavior can be successfully transferred to the dji quadrotor platform for real-world uav control .", "topics": ["supervised learning", "reinforcement learning"]}
{"title": "learning what to read : focused machine reading", "abstract": "recent efforts in bioinformatics have achieved tremendous progress in the machine reading of biomedical literature , and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways . however , batch machine reading of literature at today 's scale ( pubmed alone indexes over 1 million papers per year ) is unfeasible due to both cost and processing overhead . in this work , we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible . we introduce a family of algorithms for focused reading , including an intuitive , strong baseline , and a second approach which uses a reinforcement learning ( rl ) framework that learns when to explore ( widen the search ) or exploit ( narrow it ) . we demonstrate that the rl approach is capable of answering more queries than the baseline , while being more efficient , i.e . , reading fewer documents .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "generalized convolutional neural networks for point cloud data", "abstract": "the introduction of cheap rgb-d cameras , stereo cameras , and lidar devices has given the computer vision community 3d information that conventional rgb cameras can not provide . this data is often stored as a point cloud . in this paper , we present a novel method to apply the concept of convolutional neural networks to this type of data . by creating a mapping of nearest neighbors in a dataset , and individually applying weights to spatial relationships between points , we achieve an architecture that works directly with point clouds , but closely resembles a convolutional neural net in both design and behavior . such a method bypasses the need for extensive feature engineering , while proving to be computationally efficient and requiring few parameters .", "topics": ["computational complexity theory", "computer vision"]}
{"title": "hierarchical partitioning of the output space in multi-label data", "abstract": "hierarchy of multi-label classifiers ( homer ) is a multi-label learning algorithm that breaks the initial learning task to several , easier sub-tasks by first constructing a hierarchy of labels from a given label set and secondly employing a given base multi-label classifier ( mlc ) to the resulting sub-problems . the primary goal is to effectively address class imbalance and scalability issues that often arise in real-world multi-label classification problems . in this work , we present the general setup for a homer model and a simple extension of the algorithm that is suited for mlcs that output rankings . furthermore , we provide a detailed analysis of the properties of the algorithm , both from an aspect of effectiveness and computational complexity . a secondary contribution involves the presentation of a balanced variant of the k means algorithm , which serves in the first step of the label hierarchy construction . we conduct extensive experiments on six real-world datasets , studying empirically homer 's parameters and providing examples of instantiations of the algorithm with different clustering approaches and mlcs , the empirical results demonstrate a significant improvement over the given base mlc .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "inference of fine-grained event causality from blogs and films", "abstract": "human understanding of narrative is mainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding . computational work in this area has approached this via two different routes : by focusing on acquiring a knowledge base of common causal relations between events , or by attempting to understand a particular story or macro-event , along with its storyline . in this position paper , we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning fine-grained causal relations between everyday events . we describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions . we show that our method learns fine-grained causal relations , judged by humans as likely to be causal over 80 % of the time . we also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire .", "topics": ["unsupervised learning", "causality"]}
{"title": "fundamental limits of online and distributed algorithms for statistical learning and estimation", "abstract": "many machine learning approaches are characterized by information constraints on how they interact with the training data . these include memory and sequential access constraints ( e.g . fast first-order methods to solve stochastic optimization problems ) ; communication constraints ( e.g . distributed learning ) ; partial access to the underlying data ( e.g . missing features and multi-armed bandits ) and more . however , currently we have little understanding how such information constraints fundamentally affect our performance , independent of the learning problem semantics . for example , are there learning problems where any algorithm which has small memory footprint ( or can use any bounded number of bits from each example , or has certain communication constraints ) will perform worse than what is possible without such constraints ? in this paper , we describe how a single set of results implies positive answers to the above , for several different settings .", "topics": ["test set"]}
{"title": "cma-es for hyperparameter optimization of deep neural networks", "abstract": "hyperparameters of deep neural networks are often optimized by grid search , random search or bayesian optimization . as an alternative , we propose to use the covariance matrix adaptation evolution strategy ( cma-es ) , which is known for its state-of-the-art performance in derivative-free optimization . cma-es has some useful invariance properties and is friendly to parallel evaluations of solutions . we provide a toy example comparing cma-es and state-of-the-art bayesian optimization algorithms for tuning the hyperparameters of a convolutional neural network for the mnist dataset on 30 gpus in parallel .", "topics": ["sampling ( signal processing )", "neural networks"]}
{"title": "asynchronous stochastic gradient mcmc with elastic coupling", "abstract": "we consider parallel asynchronous markov chain monte carlo ( mcmc ) sampling for problems where we can leverage ( stochastic ) gradients to define continuous dynamics which explore the target distribution . we outline a solution strategy for this setting based on stochastic gradient hamiltonian monte carlo sampling ( sghmc ) which we alter to include an elastic coupling term that ties together multiple mcmc instances . the proposed strategy turns inherently sequential hmc algorithms into asynchronous parallel versions . first experiments empirically show that the resulting parallel sampler significantly speeds up exploration of the target distribution , when compared to standard sghmc , and is less prone to the harmful effects of stale gradients than a naive parallelization approach .", "topics": ["sampling ( signal processing )", "gradient"]}
{"title": "a generalized least squares matrix decomposition", "abstract": "variables in many massive high-dimensional data sets are structured , arising for example from measurements on a regular grid as in imaging and time series or from spatial-temporal measurements as in climate studies . classical multivariate techniques ignore these structural relationships often resulting in poor performance . we propose a generalization of the singular value decomposition ( svd ) and principal components analysis ( pca ) that is appropriate for massive data sets with structured variables or known two-way dependencies . by finding the best low rank approximation of the data with respect to a transposable quadratic norm , our decomposition , entitled the generalized least squares matrix decomposition ( gmd ) , directly accounts for structural relationships . as many variables in high-dimensional settings are often irrelevant or noisy , we also regularize our matrix decomposition by adding two-way penalties to encourage sparsity or smoothness . we develop fast computational algorithms using our methods to perform generalized pca ( gpca ) , sparse gpca , and functional gpca on massive data sets . through simulations and a whole brain functional mri example we demonstrate the utility of our methodology for dimension reduction , signal recovery , and feature selection with high-dimensional structured data .", "topics": ["time series", "sparse matrix"]}
{"title": "a comparative study of collaborative filtering algorithms", "abstract": "collaborative filtering is a rapidly advancing research area . every year several new techniques are proposed and yet it is not clear which of the techniques work best and under what conditions . in this paper we conduct a study comparing several collaborative filtering techniques -- both classic and recent state-of-the-art -- in a variety of experimental contexts . specifically , we report conclusions controlling for number of items , number of users , sparsity level , performance criteria , and computational complexity . our conclusions identify what algorithms work well and in what conditions , and contribute to both industrial deployment collaborative filtering algorithms and to the research community .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "smoothing proximal gradient method for general structured sparse learning", "abstract": "we study the problem of learning high dimensional regression models regularized by a structured-sparsity-inducing penalty that encodes prior structural information on either input or output sides . we consider two widely adopted types of such penalties as our motivating examples : 1 ) overlapping group lasso penalty , based on the l1/l2 mixed-norm penalty , and 2 ) graph-guided fusion penalty . for both types of penalties , due to their non-separability , developing an efficient optimization method has remained a challenging problem . in this paper , we propose a general optimization approach , called smoothing proximal gradient method , which can solve the structured sparse regression problems with a smooth convex loss and a wide spectrum of structured-sparsity-inducing penalties . our approach is based on a general smoothing technique of nesterov . it achieves a convergence rate faster than the standard first-order method , subgradient method , and is much more scalable than the most widely used interior-point method . numerical results are reported to demonstrate the efficiency and scalability of the proposed method .", "topics": ["optimization problem", "sparse matrix"]}
{"title": "learning neural trans-dimensional random field language models with noise-contrastive estimation", "abstract": "trans-dimensional random field language models ( trf lms ) where sentences are modeled as a collection of random fields , have shown close performance with lstm lms in speech recognition and are computationally more efficient in inference . however , the training efficiency of neural trf lms is not satisfactory , which limits the scalability of trf lms on large training corpus . in this paper , several techniques on both model formulation and parameter estimation are proposed to improve the training efficiency and the performance of neural trf lms . first , trfs are reformulated in the form of exponential tilting of a reference distribution . second , noise-contrastive estimation ( nce ) is introduced to jointly estimate the model parameters and normalization constants . third , we extend the neural trf lms by marrying the deep convolutional neural network ( cnn ) and the bidirectional lstm into the potential function to extract the deep hierarchical features and bidirectionally sequential features . utilizing all the above techniques enables the successful and efficient training of neural trf lms on a 40x larger training set with only 1/3 training time and further reduces the wer with relative reduction of 4.7 % on top of a strong lstm lm baseline .", "topics": ["baseline ( configuration management )", "speech recognition"]}
{"title": "approximation algorithms for bregman co-clustering and tensor clustering", "abstract": "in the past few years powerful generalizations to the euclidean k-means problem have been made , such as bregman clustering [ 7 ] , co-clustering ( i.e . , simultaneous clustering of rows and columns of an input matrix ) [ 9,18 ] , and tensor clustering [ 8,34 ] . like k-means , these more general problems also suffer from the np-hardness of the associated optimization . researchers have developed approximation algorithms of varying degrees of sophistication for k-means , k-medians , and more recently also for bregman clustering [ 2 ] . however , there seem to be no approximation algorithms for bregman co- and tensor clustering . in this paper we derive the first ( to our knowledge ) guaranteed methods for these increasingly important clustering settings . going beyond bregman divergences , we also prove an approximation factor for tensor clustering with arbitrary separable metrics . through extensive experiments we evaluate the characteristics of our method , and show that it also has practical impact .", "topics": ["cluster analysis", "approximation algorithm"]}
{"title": "dppred : an effective prediction framework with concise discriminative patterns", "abstract": "in the literature , two series of models have been proposed to address prediction problems including classification and regression . simple models , such as generalized linear models , have ordinary performance but strong interpretability on a set of simple features . the other series , including tree-based models , organize numerical , categorical and high dimensional features into a comprehensive structure with rich interpretable information in the data . in this paper , we propose a novel discriminative pattern-based prediction framework ( dppred ) to accomplish the prediction tasks by taking their advantages of both effectiveness and interpretability . specifically , dppred adopts the concise discriminative patterns that are on the prefix paths from the root to leaf nodes in the tree-based models . dppred selects a limited number of the useful discriminative patterns by searching for the most effective pattern combination to fit generalized linear models . extensive experiments show that in many scenarios , dppred provides competitive accuracy with the state-of-the-art as well as the valuable interpretability for developers and experts . in particular , taking a clinical application dataset as a case study , our dppred outperforms the baselines by using only 40 concise discriminative patterns out of a potentially exponentially large set of patterns .", "topics": ["statistical classification", "numerical analysis"]}
{"title": "fast and strong convergence of online learning algorithms", "abstract": "in this paper , we study the online learning algorithm without explicit regularization terms . this algorithm is essentially a stochastic gradient descent scheme in a reproducing kernel hilbert space ( rkhs ) . the polynomially decaying step size in each iteration can play a role of regularization to ensure the generalization ability of online learning algorithm . we develop a novel capacity dependent analysis on the performance of the last iterate of online learning algorithm . the contribution of this paper is two-fold . first , our nice analysis can lead to the convergence rate in the standard mean square distance which is the best so far . second , we establish , for the first time , the strong convergence of the last iterate with polynomially decaying step sizes in the rkhs norm . we demonstrate that the theoretical analysis established in this paper fully exploits the fine structure of the underlying rkhs , and thus can lead to sharp error estimates of online learning algorithm .", "topics": ["matrix regularization", "gradient descent"]}
{"title": "structured sparsity and generalization", "abstract": "we present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints . the bound can be applied to standard squared-norm regularization , the lasso , the group lasso , some versions of the group lasso with overlapping groups , multiple kernel learning and other regularization schemes . in all these cases competitive results are obtained . a novel feature of our bound is that it can be applied in an infinite dimensional setting such as the lasso in a separable hilbert space or multiple kernel learning with a countable number of kernels .", "topics": ["kernel ( operating system )", "matrix regularization"]}
{"title": "on the complexity of learning with kernels", "abstract": "a well-recognized limitation of kernel learning is the requirement to handle a kernel matrix , whose size is quadratic in the number of training examples . many methods have been proposed to reduce this computational cost , mostly by using a subset of the kernel matrix entries , or some form of low-rank matrix approximation , or a random projection method . in this paper , we study lower bounds on the error attainable by such methods as a function of the number of entries observed in the kernel matrix or the rank of an approximate kernel matrix . we show that there are kernel learning problems where no such method will lead to non-trivial computational savings . our results also quantify how the problem difficulty depends on parameters such as the nature of the loss function , the regularization parameter , the norm of the desired predictor , and the kernel matrix rank . our results also suggest cases where more efficient kernel learning might be possible .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "an architecture combining convolutional neural network ( cnn ) and support vector machine ( svm ) for image classification", "abstract": "convolutional neural networks ( cnns ) are similar to `` ordinary '' neural networks in the sense that they are made up of hidden layers consisting of neurons with `` learnable '' parameters . these neurons receive inputs , performs a dot product , and then follows it with a non-linearity . the whole network expresses the mapping between raw image pixels and their class scores . conventionally , the softmax function is the classifier used at the last layer of this network . however , there have been studies ( alalshekmubarak and smith , 2013 ; agarap , 2017 ; tang , 2013 ) conducted to challenge this norm . the cited studies introduce the usage of linear support vector machine ( svm ) in an artificial neural network architecture . this project is yet another take on the subject , and is inspired by ( tang , 2013 ) . empirical data has shown that the cnn-svm model was able to achieve a test accuracy of ~99.04 % using the mnist dataset ( lecun , cortes , and burges , 2010 ) . on the other hand , the cnn-softmax was able to achieve a test accuracy of ~99.23 % using the same dataset . both models were also tested on the recently-published fashion-mnist dataset ( xiao , rasul , and vollgraf , 2017 ) , which is suppose to be a more difficult image classification dataset than mnist ( zalandoresearch , 2017 ) . this proved to be the case as cnn-svm reached a test accuracy of ~90.72 % , while the cnn-softmax reached a test accuracy of ~91.86 % . the said results may be improved if data preprocessing techniques were employed on the datasets , and if the base cnn model was a relatively more sophisticated than the one used in this study .", "topics": ["support vector machine", "nonlinear system"]}
{"title": "blaming humans in autonomous vehicle accidents : shared responsibility across levels of automation", "abstract": "when a semi-autonomous car crashes and harms someone , how are blame and causal responsibility distributed across the human and machine drivers ? in this article , we consider cases in which a pedestrian was hit and killed by a car being operated under shared control of a primary and a secondary driver . we find that when only one driver makes an error , that driver receives the blame and is considered causally responsible for the harm , regardless of whether that driver is a machine or a human . however , when both drivers make errors in cases of shared control between a human and a machine , the blame and responsibility attributed to the machine is reduced . this finding portends a public under-reaction to the malfunctioning ai components of semi-autonomous cars and therefore has a direct policy implication : a bottom-up regulatory scheme ( which operates through tort law that is adjudicated through the jury system ) could fail to properly regulate the safety of shared-control vehicles ; instead , a top-down scheme ( enacted through federal laws ) may be called for .", "topics": ["autonomous car"]}
{"title": "high-level numerical simulations of noise in ccd and cmos photosensors : review and tutorial", "abstract": "in many applications , such as development and testing of image processing algorithms , it is often necessary to simulate images containing realistic noise from solid-state photosensors . a high-level model of ccd and cmos photosensors based on a literature review is formulated in this paper . the model includes photo-response non-uniformity , photon shot noise , dark current fixed pattern noise , dark current shot noise , offset fixed pattern noise , source follower noise , sense node reset noise , and quantisation noise . the model also includes voltage-to-voltage , voltage-to-electrons , and analogue-to-digital converter non-linearities . the formulated model can be used to create synthetic images for testing and validation of image processing algorithms in the presence of realistic images noise . an example of the simulated cmos photosensor and a comparison with a custom-made cmos hardware sensor is presented . procedures for characterisation from both light and dark noises are described . experimental results that confirm the validity of the numerical model are provided . the paper addresses the issue of the lack of comprehensive high-level photosensor models that enable engineers to simulate realistic effects of noise on the images obtained from solid-state photosensors .", "topics": ["image processing", "high- and low-level"]}
{"title": "nonparametric bayesian mixed-effect model : a sparse gaussian process approach", "abstract": "multi-task learning models using gaussian processes ( gp ) have been developed and successfully applied in various applications . the main difficulty with this approach is the computational cost of inference using the union of examples from all tasks . therefore sparse solutions , that avoid using the entire data directly and instead use a set of informative `` representatives '' are desirable . the paper investigates this problem for the grouped mixed-effect gp model where each individual response is given by a fixed-effect , taken from one of a set of unknown groups , plus a random individual effect function that captures variations among individuals . such models have been widely used in previous work but no sparse solutions have been developed . the paper presents the first sparse solution for such problems , showing how the sparse approximation can be obtained by maximizing a variational lower bound on the marginal likelihood , generalizing ideas from single-task gaussian processes to handle the mixed-effect model as well as grouping . experiments using artificial and real data validate the approach showing that it can recover the performance of inference with the full sample , that it outperforms baseline methods , and that it outperforms state of the art sparse solutions for other multi-task gp formulations .", "topics": ["sparse matrix"]}
{"title": "universal deep neural network compression", "abstract": "compression of deep neural networks ( dnns ) for memory- and computation-efficient compact feature representations becomes a critical problem particularly for deployment of dnns on resource-limited platforms . in this paper , we investigate lossy compression of dnns by weight quantization and lossless source coding for memory-efficient inference . whereas the previous work addressed non-universal scalar quantization and entropy coding of dnn weights , we for the first time introduce universal dnn compression by universal vector quantization and universal source coding . in particular , we examine universal randomized lattice quantization of dnns , which randomizes dnn weights by uniform random dithering before lattice quantization and can perform near-optimally on any source without relying on knowledge of its probability distribution . entropy coding schemes such as huffman codes require prior calculation of source statistics , which is computationally consuming . instead , we propose universal lossless source coding schemes such as variants of lempel-ziv-welch or the burrows-wheeler transform . finally , we present the methods of fine-tuning vector quantized dnns to recover the performance loss after quantization . our experimental results show that the proposed universal dnn compression scheme achieves compression ratios of 124.80 , 47.10 and 42.46 for lenet5 , 32-layer resnet and alexnet , respectively .", "topics": ["computation"]}
{"title": "a new vision of collaborative active learning", "abstract": "active learning ( al ) is a learning paradigm where an active learner has to train a model ( e.g . , a classifier ) which is in principal trained in a supervised way , but in al it has to be done by means of a data set with initially unlabeled samples . to get labels for these samples , the active learner has to ask an oracle ( e.g . , a human expert ) for labels . the goal is to maximize the performance of the model and to minimize the number of queries at the same time . in this article , we first briefly discuss the state of the art and own , preliminary work in the field of al . then , we propose the concept of collaborative active learning ( cal ) . with cal , we will overcome some of the harsh limitations of current al . in particular , we envision scenarios where an expert may be wrong for various reasons , there might be several or even many experts with different expertise , the experts may label not only samples but also knowledge at a higher level such as rules , and we consider that the labeling costs depend on many conditions . moreover , in a cal process human experts will profit by improving their own knowledge , too .", "topics": ["supervised learning", "statistical classification"]}
{"title": "effective bayesian modeling of groups of related count time series", "abstract": "time series of counts arise in a variety of forecasting applications , for which traditional models are generally inappropriate . this paper introduces a hierarchical bayesian formulation applicable to count time series that can easily account for explanatory variables and share statistical strength across groups of related time series . we derive an efficient approximate inference technique , and illustrate its performance on a number of datasets from supply chain planning .", "topics": ["time series", "approximation algorithm"]}
{"title": "an effective and efficient approach for clusterability evaluation", "abstract": "clustering is an essential data mining tool that aims to discover inherent cluster structure in data . as such , the study of clusterability , which evaluates whether data possesses such structure , is an integral part of cluster analysis . yet , despite their central role in the theory and application of clustering , current notions of clusterability fall short in two crucial aspects that render them impractical ; most are computationally infeasible and others fail to classify the structure of real datasets . in this paper , we propose a novel approach to clusterability evaluation that is both computationally efficient and successfully captures the structure of real data . our method applies multimodality tests to the ( one-dimensional ) set of pairwise distances based on the original , potentially high-dimensional data . we present extensive analyses of our approach for both the dip and silverman multimodality tests on real data as well as 17,000 simulations , demonstrating the success of our approach as the first practical notion of clusterability .", "topics": ["cluster analysis", "data mining"]}
{"title": "vector symbolic architectures answer jackendoff 's challenges for cognitive neuroscience", "abstract": "jackendoff ( 2002 ) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function . the essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing . he contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling . this paper claims that a little-known family of connectionist models ( vector symbolic architectures ) are able to meet jackendoff 's challenges .", "topics": ["support vector machine"]}
{"title": "single camera pose estimation using bayesian filtering and kinect motion priors", "abstract": "traditional approaches to upper body pose estimation using monocular vision rely on complex body models and a large variety of geometric constraints . we argue that this is not ideal and somewhat inelegant as it results in large processing burdens , and instead attempt to incorporate these constraints through priors obtained directly from training data . a prior distribution covering the probability of a human pose occurring is used to incorporate likely human poses . this distribution is obtained offline , by fitting a gaussian mixture model to a large dataset of recorded human body poses , tracked using a kinect sensor . we combine this prior information with a random walk transition model to obtain an upper body model , suitable for use within a recursive bayesian filtering framework . our model can be viewed as a mixture of discrete ornstein-uhlenbeck processes , in that states behave as random walks , but drift towards a set of typically observed poses . this model is combined with measurements of the human head and hand positions , using recursive bayesian estimation to incorporate temporal information . measurements are obtained using face detection and a simple skin colour hand detector , trained using the detected face . the suggested model is designed with analytical tractability in mind and we show that the pose tracking can be rao-blackwellised using the mixture kalman filter , allowing for computational efficiency while still incorporating bio-mechanical properties of the upper body . in addition , the use of the proposed upper body model allows reliable three-dimensional pose estimates to be obtained indirectly for a number of joints that are often difficult to detect using traditional object recognition strategies . comparisons with kinect sensor results and the state of the art in 2d pose estimation highlight the efficacy of the proposed approach .", "topics": ["test set", "computer vision"]}
{"title": "simple regret optimization in online planning for markov decision processes", "abstract": "we consider online planning in markov decision processes ( mdps ) . in online planning , the agent focuses on its current state only , deliberates about the set of possible policies from that state onwards and , when interrupted , uses the outcome of that exploratory deliberation to choose what action to perform next . the performance of algorithms for online planning is assessed in terms of simple regret , which is the agent 's expected performance loss when the chosen action , rather than an optimal one , is followed . to date , state-of-the-art algorithms for online planning in general mdps are either best effort , or guarantee only polynomial-rate reduction of simple regret over time . here we introduce a new monte-carlo tree search algorithm , brue , that guarantees exponential-rate reduction of simple regret and error probability . this algorithm is based on a simple yet non-standard state-space sampling scheme , mcts2e , in which different parts of each sample are dedicated to different exploratory objectives . our empirical evaluation shows that brue not only provides superior performance guarantees , but is also very effective in practice and favorably compares to state-of-the-art . we then extend brue with a variant of `` learning by forgetting . '' the resulting set of algorithms , brue ( alpha ) , generalizes brue , improves the exponential factor in the upper bound on its reduction rate , and exhibits even more attractive empirical performance .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "optimally pruning decision tree ensembles with feature cost", "abstract": "we consider the problem of learning decision rules for prediction with feature budget constraint . in particular , we are interested in pruning an ensemble of decision trees to reduce expected feature cost while maintaining high prediction accuracy for any test example . we propose a novel 0-1 integer program formulation for ensemble pruning . our pruning formulation is general - it takes any ensemble of decision trees as input . by explicitly accounting for feature-sharing across trees together with accuracy/cost trade-off , our method is able to significantly reduce feature cost by pruning subtrees that introduce more loss in terms of feature cost than benefit in terms of prediction accuracy gain . theoretically , we prove that a linear programming relaxation produces the exact solution of the original integer program . this allows us to use efficient convex optimization tools to obtain an optimally pruned ensemble for any given budget . empirically , we see that our pruning algorithm significantly improves the performance of the state of the art ensemble method budgetrf .", "topics": ["mathematical optimization"]}
{"title": "cause identification from aviation safety incident reports via weakly supervised semantic lexicon construction", "abstract": "the aviation safety reporting system collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents . to effectively reduce these incidents , it is vital to accurately identify why these incidents occurred . more precisely , given a set of possible causes , or shaping factors , this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report . we investigate two approaches to cause identification . both approaches exploit information provided by a semantic lexicon , which is automatically constructed via thelen and riloffs basilisk framework augmented with our linguistic and algorithmic modifications . the first approach labels a report using a simple heuristic , which looks for the words and phrases acquired during the semantic lexicon learning process in the report . the second approach recasts cause identification as a text classification problem , employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports . our experiments show that both the heuristic-based approach and the learning-based approach ( when given sufficient training data ) outperform the baseline system significantly .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "mindx : denoising mixed impulse poisson-gaussian noise using proximal algorithms", "abstract": "we present a novel algorithm for blind denoising of images corrupted by mixed impulse , poisson , and gaussian noises . the algorithm starts by applying the anscombe variance-stabilizing transformation to convert the poisson into white gaussian noise . then it applies a combinatorial optimization technique to denoise the mixed impulse gaussian noise using proximal algorithms . the result is then processed by the inverse anscombe transform . we compare our algorithm to state of the art methods on standard images , and show its superior performance in various noise conditions .", "topics": ["noise reduction"]}
{"title": "rgtsvm : support vector machines on a gpu in r", "abstract": "rgtsvm provides a fast and flexible support vector machine ( svm ) implementation for the r language . the distinguishing feature of rgtsvm is that support vector classification and support vector regression tasks are implemented on a graphical processing unit ( gpu ) , allowing the libraries to scale to millions of examples with > 100-fold improvement in performance over existing implementations . nevertheless , rgtsvm retains feature parity and has an interface that is compatible with the popular e1071 svm package in r . altogether , rgtsvm enables large svm models to be created by both experienced and novice practitioners .", "topics": ["support vector machine"]}
{"title": "emergence of foveal image sampling from learning to attend in visual scenes", "abstract": "we describe a neural attention model with a learnable retinal sampling lattice . the model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations . we explore the tiling properties that emerge in the model 's retinal sampling lattice after training . specifically , we show that this lattice resembles the eccentricity dependent sampling lattice of the primate retina , with a high resolution region in the fovea surrounded by a low resolution periphery . furthermore , we find conditions where these emergent properties are amplified or eliminated providing clues to their function .", "topics": ["sampling ( signal processing )"]}
{"title": "sparse online learning via truncated gradient", "abstract": "we propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions . this method has several essential properties : the degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification . the approach is theoretically motivated , and an instance of it can be regarded as an online counterpart of the popular $ l_1 $ -regularization method in the batch setting . we prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees . the approach works well empirically . we apply the approach to several datasets and find that for datasets with large numbers of features , substantial sparsity is discoverable .", "topics": ["regret ( decision theory )", "sparse matrix"]}
{"title": "towards monocular digital elevation model ( dem ) estimation by convolutional neural networks - application on synthetic aperture radar images", "abstract": "synthetic aperture radar ( sar ) interferometry ( insar ) is performed using repeat-pass geometry . insar technique is used to estimate the topographic reconstruction of the earth surface . the main problem of the range-doppler focusing technique is the nature of the two-dimensional sar result , affected by the layover indetermination . in order to resolve this problem , a minimum of two sensor acquisitions , separated by a baseline and extended in the cross-slant-range , are needed . however , given its multi-temporal nature , these techniques are vulnerable to atmosphere and earth environment parameters variation in addition to physical platform instabilities . furthermore , either two radars are needed or an interferometric cycle is required ( that spans from days to weeks ) , which makes real time dem estimation impossible . in this work , the authors propose a novel experimental alternative to the insar method that uses single-pass acquisitions , using a data driven approach implemented by deep neural networks . we propose a fully convolutional neural network ( cnn ) encoder-decoder architecture , training it on radar images in order to estimate dems from single pass image acquisitions . our results on a set of sentinel images show that this method is able to learn to some extent the statistical properties of the dem . the results of this exploratory analysis are encouraging and open the way to the solution of single-pass dem estimation problem with data driven approaches .", "topics": ["baseline ( configuration management )", "synthetic data"]}
{"title": "sever : a robust meta-algorithm for stochastic optimization", "abstract": "in high dimensions , most machine learning methods are brittle to even a small fraction of structured outliers . to address this , we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent , and harden the learner to be resistant to outliers . our method , sever , possesses strong theoretical guarantees yet is also highly scalable -- beyond running the base learner itself , it only requires computing the top singular vector of a certain $ n \\times d $ matrix . we apply sever on a drug design dataset and a spam classification dataset , and find that in both cases it has substantially greater robustness than several baselines . on the spam dataset , with $ 1\\ % $ corruptions , we achieved $ 7.4\\ % $ test error , compared to $ 13.4\\ % -20.5\\ % $ for the baselines , and $ 3\\ % $ error on the uncorrupted dataset . similarly , on the drug design dataset , with $ 10\\ % $ corruptions , we achieved $ 1.42 $ mean-squared error test error , compared to $ 1.51 $ - $ 2.33 $ for the baselines , and $ 1.23 $ error on the uncorrupted dataset .", "topics": ["gradient descent", "scalability"]}
{"title": "topic discovery through data dependent and random projections", "abstract": "we present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns . this perspective gains significance under the so called separability condition . this is a condition on existence of novel-words that are unique to each topic . we present a suite of highly efficient algorithms based on data-dependent and random projections of word-frequency patterns to identify novel words and associated topics . we will also discuss the statistical guarantees of the data-dependent projections method based on two mild assumptions on the prior density of topic document matrix . our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words . while our sample complexity bounds for topic recovery are similar to the state-of-art , the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document . we present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme .", "topics": ["computational complexity theory", "synthetic data"]}
{"title": "deepisp : learning end-to-end image processing pipeline", "abstract": "we present deepisp , a full end-to-end deep neural model of the camera image signal processing ( isp ) pipeline . our model learns a mapping from the raw low-light mosaiced image to the final visually compelling image and encompasses low-level tasks such as demosaicing and denoising as well as higher-level tasks such as color correction and image adjustment . the training and evaluation of the pipeline was performed on a dedicated dataset containing pairs of low-light and well-lit images captured by a samsung s7 smartphone camera in both raw and processed jpeg formats . the proposed solution achieves state-of-the-art performance in objective evaluation of psnr on the subtask of joint denoising and demosaicing . for the full end-to-end pipeline , it achieves better visual quality compared to the manufacturer isp , in both a subjective human assessment and when rated by a deep model trained for assessing image quality .", "topics": ["image processing", "high- and low-level"]}
{"title": "adversarial examples for semantic image segmentation", "abstract": "machine learning methods in general and deep neural networks in particular have shown to be vulnerable to adversarial perturbations . so far this phenomenon has mainly been studied in the context of whole-image classification . in this contribution , we analyse how adversarial perturbations can affect the task of semantic segmentation . we show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations that lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class .", "topics": ["neural networks", "image segmentation"]}
{"title": "gated xnor networks : deep neural networks with ternary weights and activations under a unified discretization framework", "abstract": "there is a pressing need to build an architecture that could subsume these networks undera unified framework that achieves both higher performance and less overhead . to this end , two fundamental issues are yet to be addressed . the first one is how to implement the back propagation when neuronal activations are discrete . the second one is how to remove the full-precision hidden weights in the training phase to break the bottlenecks of memory/computation consumption . to address the first issue , we present a multistep neuronal activation discretization method and a derivative approximation technique that enable the implementing the back propagation algorithm on discrete dnns . while for the second issue , we propose a discrete state transition ( dst ) methodology to constrain the weights in a discrete space without saving the hidden weights . in this way , we build a unified framework that subsumes the binary or ternary networks as its special cases.more particularly , we find that when both the weights and activations become ternary values , the dnns can be reduced to gated xnor networks ( or sparse binary networks ) since only the event of non-zero weight and non-zero activation enables the control gate to start the xnor logic operations in the original binary networks . this promises the event-driven hardware design for efficient mobile intelligence . we achieve advanced performance compared with state-of-the-art algorithms . furthermore , the computational sparsity and the number of states in the discrete space can be flexibly modified to make it suitable for various hardware platforms .", "topics": ["sparse matrix", "computation"]}
{"title": "bayesian approach for near-duplicate image detection", "abstract": "in this paper we propose a bayesian approach for near-duplicate image detection , and investigate how different probabilistic models affect the performance obtained . the task of identifying an image whose metadata are missing is often demanded for a myriad of applications : metadata retrieval in cultural institutions , detection of copyright violations , investigation of latent cross-links in archives and libraries , duplicate elimination in storage management , etc . the majority of current solutions are based either on voting algorithms , which are very precise , but expensive ; either on the use of visual dictionaries , which are efficient , but less precise . our approach , uses local descriptors in a novel way , which by a careful application of decision theory , allows a very fine control of the compromise between precision and efficiency . in addition , the method attains a great compromise between those two axes , with more than 99 % accuracy with less than 10 database operations .", "topics": ["feature vector", "dictionary"]}
{"title": "polynomial learning of distribution families", "abstract": "the question of polynomial learnability of probability distributions , particularly gaussian mixture distributions , has recently received significant attention in theoretical computer science and machine learning . however , despite major progress , the general question of polynomial learnability of gaussian mixture distributions still remained open . the current work resolves the question of polynomial learnability for gaussian mixtures in high dimension with an arbitrary fixed number of components . the result on learning gaussian mixtures relies on an analysis of distributions belonging to what we call `` polynomial families '' in low dimension . these families are characterized by their moments being polynomial in parameters and include almost all common probability distributions as well as their mixtures and products . using tools from real algebraic geometry , we show that parameters of any distribution belonging to such a family can be learned in polynomial time and using a polynomial number of sample points . the result on learning polynomial families is quite general and is of independent interest . to estimate parameters of a gaussian mixture distribution in high dimensions , we provide a deterministic algorithm for dimensionality reduction . this allows us to reduce learning a high-dimensional mixture to a polynomial number of parameter estimations in low dimension . combining this reduction with the results on polynomial families yields our result on learning arbitrary gaussian mixtures in high dimensions .", "topics": ["time complexity", "polynomial"]}
{"title": "continuous semantic topic embedding model using variational autoencoder", "abstract": "this paper proposes the continuous semantic topic embedding model ( cstem ) which finds latent topic variables in documents using continuous semantic distance function between the topics and the words by means of the variational autoencoder ( vae ) . the semantic distance could be represented by any symmetric bell-shaped geometric distance function on the euclidean space , for which the mahalanobis distance is used in this paper . in order for the semantic distance to perform more properly , we newly introduce an additional model parameter for each word to take out the global factor from this distance indicating how likely it occurs regardless of its topic . it certainly improves the problem that the gaussian distribution which is used in previous topic model with continuous word embedding could not explain the semantic relation correctly and helps to obtain the higher topic coherence . through the experiments with the dataset of 20 newsgroup , nips papers and cnn/dailymail corpus , the performance of the recent state-of-the-art models is accomplished by our model as well as generating topic embedding vectors which makes possible to observe where the topic vectors are embedded with the word vectors in the real euclidean space and how the topics are related each other semantically .", "topics": ["calculus of variations", "autoencoder"]}
{"title": "when neurons fail", "abstract": "we view a neural network as a distributed system of which neurons can fail independently , and we evaluate its robustness in the absence of any ( recovery ) learning phase . we give tight bounds on the number of neurons that can fail without harming the result of a computation . to determine our bounds , we leverage the fact that neural activation functions are lipschitz-continuous . our bound is on a quantity , we call the \\textit { forward error propagation } , capturing how much error is propagated by a neural network when a given number of components is failing , computing this quantity only requires looking at the topology of the network , while experimentally assessing the robustness of a network requires the costly experiment of looking at all the possible inputs and testing all the possible configurations of the network corresponding to different failure situations , facing a discouraging combinatorial explosion . we distinguish the case of neurons that can fail and stop their activity ( crashed neurons ) from the case of neurons that can fail by transmitting arbitrary values ( byzantine neurons ) . interestingly , as we show in the paper , our bound can easily be extended to the case where synapses can fail . we show how our bound can be leveraged to quantify the effect of memory cost reduction on the accuracy of a neural network , to estimate the amount of information any neuron needs from its preceding layer , enabling thereby a boosting scheme that prevents neurons from waiting for unnecessary signals . we finally discuss the trade-off between neural networks robustness and learning cost .", "topics": ["neural networks", "computation"]}
{"title": "survival of the flexible : explaining the recent dominance of nature-inspired optimization within a rapidly evolving world", "abstract": "although researchers often comment on the rising popularity of nature-inspired meta-heuristics ( nim ) , there has been a paucity of data to directly support the claim that nim are growing in prominence compared to other optimization techniques . this study presents evidence that the use of nim is not only growing , but indeed appears to have surpassed mathematical optimization techniques ( mot ) in several important metrics related to academic research activity ( publication frequency ) and commercial activity ( patenting frequency ) . motivated by these findings , this article discusses some of the possible origins of this growing popularity . i review different explanations for nim popularity and discuss why some of these arguments remain unsatisfying . i argue that a compelling and comprehensive explanation should directly account for the manner in which most nim success has actually been achieved , e.g . through hybridization and customization to different problem environments . by taking a problem lifecycle perspective , this paper offers a fresh look at the hypothesis that nature-inspired meta-heuristics derive much of their utility from being flexible . i discuss global trends within the business environments where optimization algorithms are applied and i speculate that highly flexible algorithm frameworks could become increasingly popular within our diverse and rapidly changing world .", "topics": ["mathematical optimization", "heuristic"]}
{"title": "efficiently sampling multiplicative attribute graphs using a ball-dropping process", "abstract": "we introduce a novel and efficient sampling algorithm for the multiplicative attribute graph model ( magm - kim and leskovec ( 2010 ) } ) . our algorithm is \\emph { strictly } more efficient than the algorithm proposed by yun and vishwanathan ( 2012 ) , in the sense that our method extends the \\emph { best } time complexity guarantee of their algorithm to a larger fraction of parameter space . both in theory and in empirical evaluation on sparse graphs , our new algorithm outperforms the previous one . to design our algorithm , we first define a stochastic \\emph { ball-dropping process } ( bdp ) . although a special case of this process was introduced as an efficient approximate sampling algorithm for the kronecker product graph model ( kpgm - leskovec et al . ( 2010 ) } ) , neither \\emph { why } such an approximation works nor \\emph { what } is the actual distribution this process is sampling from has been addressed so far to the best of our knowledge . our rigorous treatment of the bdp enables us to clarify the rational behind a bdp approximation of kpgm , and design an efficient sampling algorithm for the magm .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "a relativistic extension of hopfield neural networks via the mechanical analogy", "abstract": "we propose a modification of the cost function of the hopfield model whose salient features shine in its taylor expansion and result in more than pairwise interactions with alternate signs , suggesting a unified framework for handling both with deep learning and network pruning . in our analysis , we heavily rely on the hamilton-jacobi correspondence relating the statistical model with a mechanical system . in this picture , our model is nothing but the relativistic extension of the original hopfield model ( whose cost function is a quadratic form in the mattis magnetization which mimics the non-relativistic hamiltonian for a free particle ) . we focus on the low-storage regime and solve the model analytically by taking advantage of the mechanical analogy , thus obtaining a complete characterization of the free energy and the associated self-consistency equations in the thermodynamic limit . on the numerical side , we test the performances of our proposal with mc simulations , showing that the stability of spurious states ( limiting the capabilities of the standard hebbian construction ) is sensibly reduced due to presence of unlearning contributions in this extended framework .", "topics": ["numerical analysis", "simulation"]}
{"title": "visual place recognition with probabilistic vertex voting", "abstract": "we propose a novel scoring concept for visual place recognition based on nearest neighbor descriptor voting and demonstrate how the algorithm naturally emerges from the problem formulation . based on the observation that the number of votes for matching places can be evaluated using a binomial distribution model , loop closures can be detected with high precision . by casting the problem into a probabilistic framework , we not only remove the need for commonly employed heuristic parameters but also provide a powerful score to classify matching and non-matching places . we present methods for both a 2d-2d pose-graph vertex matching and a 2d-3d landmark matching based on the above scoring . the approach maintains accuracy while being efficient enough for online application through the use of compact ( low dimensional ) descriptors and fast nearest neighbor retrieval techniques . the proposed methods are evaluated on several challenging datasets in varied environments , showing state-of-the-art results with high precision and high recall .", "topics": ["heuristic"]}
{"title": "nurse rostering with genetic algorithms", "abstract": "in recent years genetic algorithms have emerged as a useful tool for the heuristic solution of complex discrete optimisation problems . in particular there has been considerable interest in their use in tackling problems arising in the areas of scheduling and timetabling . however , the classical genetic algorithm paradigm is not well equipped to handle constraints and successful implementations usually require some sort of modification to enable the search to exploit problem specific knowledge in order to overcome this shortcoming . this paper is concerned with the development of a family of genetic algorithms for the solution of a nurse rostering problem at a major uk hospital . the hospital is made up of wards of up to 30 nurses . each ward has its own group of nurses whose shifts have to be scheduled on a weekly basis . in addition to fulfilling the minimum demand for staff over three daily shifts , nurses ' wishes and qualifications have to be taken into account . the schedules must also be seen to be fair , in that unpopular shifts have to be spread evenly amongst all nurses , and other restrictions , such as team nursing and special conditions for senior staff , have to be satisfied . the basis of the family of genetic algorithms is a classical genetic algorithm consisting of n-point crossover , single-bit mutation and a rank-based selection . the solution space consists of all schedules in which each nurse works the required number of shifts , but the remaining constraints , both hard and soft , are relaxed and penalised in the fitness function . the talk will start with a detailed description of the problem and the initial implementation and will go on to highlight the shortcomings of such an approach , in terms of the key element of balancing feasibility , i.e . covering the demand and work regulations , and quality , as measured by the nurses ' preferences . a series of experiments involving parameter adaptation , niching , intelligent weights , delta coding , local hill climbing , migration and special selection rules will then be outlined and it will be shown how a series of these enhancements were able to eradicate these difficulties . results based on several months ' real data will be used to measure the impact of each modification , and to show that the final algorithm is able to compete with a tabu search approach currently employed at the hospital . the talk will conclude with some observations as to the overall quality of this approach to this and similar problems .", "topics": ["mathematical optimization", "heuristic"]}
{"title": "high performance latent variable models", "abstract": "latent variable models have accumulated a considerable amount of interest from the industry and academia for their versatility in a wide range of applications . a large amount of effort has been made to develop systems that is able to extend the systems to a large scale , in the hope to make use of them on industry scale data . in this paper , we describe a system that operates at a scale orders of magnitude higher than previous works , and an order of magnitude faster than state-of-the-art system at the same scale , at the same time showing more robustness and more accurate results . our system uses a number of advances in distributed inference : high performance in synchronization of sufficient statistics with relaxed consistency model ; fast sampling , using the metropolis-hastings-walker method to overcome dense generative models ; statistical modeling , moving beyond latent dirichlet allocation ( lda ) to pitman-yor distributions ( pdp ) and hierarchical dirichlet process ( hdp ) models ; sophisticated parameter projection schemes , to resolve the conflicts within the constraint between parameters arising from the relaxed consistency model . this work significantly extends the domain of applicability of what is commonly known as the parameter server . we obtain results with up to hundreds billion oftokens , thousands of topics , and a vocabulary of a few million token-types , using up to 60,000 processor cores operating on a production cluster of a large internet company . this demonstrates the feasibility to scale to problems orders of magnitude larger than any previously published work .", "topics": ["sampling ( signal processing )"]}
{"title": "the neural representation benchmark and its evaluation on brain and machine", "abstract": "a key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective . in natural sensory domains , the community has viewed the brain as a source of inspiration and as an implicit benchmark for success . however , it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems . here , we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque ( utilizing data from [ majaj et al . , 2012 ] ) , and on which any computer vision algorithm that produces a feature space can be tested . the benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [ montavon et al . , 2011 ] . in our analysis we find that the neural representation in visual area it is superior to visual area v4 . in our analysis of representational learning algorithms , we find that three-layer models approach the representational performance of v4 and the algorithm in [ le et al . , 2012 ] surpasses the performance of v4 . impressively , we find that a recent supervised algorithm [ krizhevsky et al . , 2012 ] achieves performance comparable to that of it for an intermediate level of image variation difficulty , and surpasses it at a higher difficulty level . we believe this result represents a major milestone : it is the first learning algorithm we have found that exceeds our current estimate of it representation performance . we hope that this benchmark will assist the community in matching the representational performance of visual cortex and will serve as an initial rallying point for further correspondence between representations derived in brains and machines .", "topics": ["feature vector", "computer vision"]}
{"title": "deep reinforcement learning for event-driven multi-agent decision processes", "abstract": "the incorporation of macro-actions ( temporally extended actions ) into multi-agent decision problems has the potential to address the curse of dimensionality associated with such decision problems . since macro-actions last for stochastic durations , multiple agents executing decentralized policies in cooperative environments must act asynchronously . we present an algorithm that modifies generalized advantage estimation for temporally extended actions , allowing a state-of-the-art policy optimization algorithm to optimize policies in dec-pomdps in which agents act asynchronously . we show that our algorithm is capable of learning optimal policies in two cooperative domains , one involving real-time bus holding control and one involving wildfire fighting with unmanned aircraft . our algorithm works by framing problems as `` event-driven decision processes , '' which are scenarios where the sequence and timing of actions and events are random and governed by an underlying stochastic process . in addition to optimizing policies with continuous state and action spaces , our algorithm also facilitates the use of event-driven simulators , which do not require time to be discretized into time-steps . we demonstrate the benefit of using event-driven simulation in the context of multiple agents taking asynchronous actions . we show that fixed time-step simulation risks obfuscating the sequence in which closely-separated events occur , adversely affecting the policies learned . additionally , we show that arbitrarily shrinking the time-step scales poorly with the number of agents .", "topics": ["reinforcement learning", "simulation"]}
{"title": "posenet : a convolutional network for real-time 6-dof camera relocalization", "abstract": "we present a robust and real-time monocular six degree of freedom relocalization system . our system trains a convolutional neural network to regress the 6-dof camera pose from a single rgb image in an end-to-end manner with no need of additional engineering or graph optimisation . the algorithm can operate indoors and outdoors in real time , taking 5ms per frame to compute . it obtains approximately 2m and 6 degree accuracy for large scale outdoor scenes and 0.5m and 10 degree accuracy indoors . this is achieved using an efficient 23 layer deep convnet , demonstrating that convnets can be used to solve complicated out of image plane regression problems . this was made possible by leveraging transfer learning from large scale classification data . we show the convnet localizes from high level features and is robust to difficult lighting , motion blur and different camera intrinsics where point based sift registration fails . furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples . posenet code , dataset and an online demonstration is available on our project webpage , at http : //mi.eng.cam.ac.uk/projects/relocalisation/", "topics": ["mathematical optimization", "end-to-end principle"]}
{"title": "binarized neural networks", "abstract": "we introduce a method to train binarized neural networks ( bnns ) - neural networks with binary weights and activations at run-time and when computing the parameters ' gradient at train-time . we conduct two sets of experiments , each based on a different framework , namely torch7 and theano , where we train bnns on mnist , cifar-10 and svhn , and achieve nearly state-of-the-art results . during the forward pass , bnns drastically reduce memory size and accesses , and replace most arithmetic operations with bit-wise operations , which might lead to a great increase in power-efficiency . last but not least , we wrote a binary matrix multiplication gpu kernel with which it is possible to run our mnist bnn 7 times faster than with an unoptimized gpu kernel , without suffering any loss in classification accuracy . the code for training and running our bnns is available .", "topics": ["neural networks", "convolution"]}
{"title": "throwing fuel on the embers : probability or dichotomy , cognitive or linguistic ?", "abstract": "prof. robert berwick 's abstract for his forthcoming invited talk at the acl2016 workshop on cognitive aspects of computational language learning revives an ancient debate . entitled `` why take a chance ? `` , berwick seems to refer implicitly to chomsky 's critique of the statistical approach of harris as well as the currently dominant paradigms in conll . berwick avoids chomsky 's use of `` innate '' but states that `` the debate over the existence of sophisticated mental grammars was settled with chomsky 's logical structure of linguistic theory ( 1957/1975 ) '' , acknowledging that `` this debate has often been revived '' . this paper agrees with the view that this debate has long since been settled , but with the opposite outcome ! given the embers have not yet died away , and the questions remain fundamental , perhaps it is appropriate to refuel the debate , so i would like to join bob in throwing fuel on this fire by reviewing the evidence against the chomskian position !", "topics": ["approximation"]}
{"title": "adversarial delays in online strongly-convex optimization", "abstract": "we consider the problem of strongly-convex online optimization in presence of adversarial delays ; in a t-iteration online game , the feedback of the player 's query at time t is arbitrarily delayed by an adversary for d_t rounds and delivered before the game ends , at iteration t+d_t-1 . specifically for \\algo { online-gradient-descent } algorithm we show it has a simple regret bound of \\oh { \\sum_ { t=1 } ^t \\log ( 1+ \\frac { d_t } { t } ) } . this gives a clear and simple bound without resorting any distributional and limiting assumptions on the delays . we further show how this result encompasses and generalizes several of the existing known results in the literature . specifically it matches the celebrated logarithmic regret \\oh { \\log t } when there are no delays ( i.e . d_t = 1 ) and regret bound of \\oh { \\tau \\log t } for constant delays d_t = \\tau .", "topics": ["regret ( decision theory )", "iteration"]}
{"title": "a convolutional neural network neutrino event classifier", "abstract": "convolutional neural networks ( cnns ) have been widely applied in the computer vision community to solve complex problems in image recognition and analysis . we describe an application of the cnn technology to the problem of identifying particle interactions in sampling calorimeters used commonly in high energy physics and high energy neutrino physics in particular . following a discussion of the core concepts of cnns and recent innovations in cnn architectures related to the field of deep learning , we outline a specific application to the nova neutrino detector . this algorithm , cvn ( convolutional visual network ) identifies neutrino interactions based on their topology without the need for detailed reconstruction and outperforms algorithms currently in use by the nova collaboration .", "topics": ["sampling ( signal processing )", "computer vision"]}
{"title": "a tutorial on deep learning for music information retrieval", "abstract": "following their success in computer vision and other areas , deep learning techniques have recently become widely adopted in music information retrieval ( mir ) research . however , the majority of works aim to adopt and assess methods that have been shown to be effective in other domains , while there is still a great need for more original research focusing on music primarily and utilising musical knowledge and insight . the goal of this paper is to boost the interest of beginners by providing a comprehensive tutorial and reducing the barriers to entry into deep learning for mir . we lay out the basic principles and review prominent works in this hard to navigate field . we then outline the network structures that have been successful in mir problems and facilitate the selection of building blocks for the problems at hand . finally , guidelines for new tasks and some advanced topics in deep learning are discussed to stimulate new research in this fascinating field .", "topics": ["computer vision"]}
{"title": "the optimal reward baseline for gradient-based reinforcement learning", "abstract": "there exist a number of reinforcement learning algorithms which learnby climbing the gradient of expected reward . their long-runconvergence has been proved , even in partially observableenvironments with non-deterministic actions , and without the need fora system model . however , the variance of the gradient estimator hasbeen found to be a significant practical problem . recent approacheshave discounted future rewards , introducing a bias-variance trade-offinto the gradient estimate . we incorporate a reward baseline into thelearning system , and show that it affects variance without introducingfurther bias . in particular , as we approach the zero-bias , high-variance parameterization , the optimal ( or variance minimizing ) constant reward baseline is equal to the long-term average expectedreward . modified policy-gradient algorithms are presented , and anumber of experiments demonstrate their improvement over previous work .", "topics": ["baseline ( configuration management )", "reinforcement learning"]}
{"title": "smart application for ams using face recognition", "abstract": "attendance management system ( ams ) can be made into smarter way by using face recognition technique , where we use a cctv camera to be fixed at the entry point of a classroom , which automatically captures the image of the person and checks the observed image with the face database using android enhanced smart phone . it is typically used for two purposes . firstly , marking attendance for student by comparing the face images produced recently and secondly , recognition of human who are strange to the environment i.e . an unauthorized person for verification of image , a newly emerging trend 3d face recognition is used which claims to provide more accuracy in matching the image databases and has an ability to recognize a subject at different view angles .", "topics": ["database"]}
{"title": "optimal binary classifier aggregation for general losses", "abstract": "we address the problem of aggregating an ensemble of predictors with known loss bounds in a semi-supervised binary classification setting , to minimize prediction loss incurred on the unlabeled data . we find the minimax optimal predictions for a very general class of loss functions including all convex and many non-convex losses , extending a recent analysis of the problem for misclassification error . the result is a family of semi-supervised ensemble aggregation algorithms which are as efficient as linear learning by convex optimization , but are minimax optimal without any relaxations . their decision rules take a form familiar in decision theory -- applying sigmoid functions to a notion of ensemble margin -- without the assumptions typically made in margin-based learning .", "topics": ["loss function"]}
{"title": "machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations", "abstract": "high-dimensional partial differential equations ( pde ) appear in a number of models from the financial industry , such as in derivative pricing models , credit valuation adjustment ( cva ) models , or portfolio optimization models . the pdes in such applications are high-dimensional as the dimension corresponds to the number of financial assets in a portfolio . moreover , such pdes are often fully nonlinear due to the need to incorporate certain nonlinear phenomena in the model such as default risks , transaction costs , volatility uncertainty ( knightian uncertainty ) , or trading constraints in the model . such high-dimensional fully nonlinear pdes are exceedingly difficult to solve as the computational effort for standard approximation methods grows exponentially with the dimension . in this work we propose a new method for solving high-dimensional fully nonlinear second-order pdes . our method can in particular be used to sample from high-dimensional nonlinear expectations . the method is based on ( i ) a connection between fully nonlinear second-order pdes and second-order backward stochastic differential equations ( 2bsdes ) , ( ii ) a merged formulation of the pde and the 2bsde problem , ( iii ) a temporal forward discretization of the 2bsde and a spatial approximation via deep neural nets , and ( iv ) a stochastic gradient descent-type optimization procedure . numerical results obtained using $ { \\rm t { \\small ensor } f { \\small low } } $ in $ { \\rm p { \\small ython } } $ illustrate the efficiency and the accuracy of the method in the cases of a $ 100 $ -dimensional black-scholes-barenblatt equation , a $ 100 $ -dimensional hamilton-jacobi-bellman equation , and a nonlinear expectation of a $ 100 $ -dimensional $ g $ -brownian motion .", "topics": ["value ( ethics )", "approximation algorithm"]}
{"title": "shallow updates for deep reinforcement learning", "abstract": "deep reinforcement learning ( drl ) methods such as the deep q-network ( dqn ) have achieved state-of-the-art results in a variety of challenging , high-dimensional domains . this success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy . batch reinforcement learning methods with linear representations , on the other hand , are more stable and require less hyper parameter tuning . yet , substantial feature engineering is necessary to achieve good results . in this work we propose a hybrid approach -- the least squares deep q-network ( ls-dqn ) , which combines rich feature representations learned by a drl algorithm with the stability of a linear least squares method . we do this by periodically re-training the last hidden layer of a drl network with a batch least squares update . key to our approach is a bayesian regularization term for the least squares update , which prevents over-fitting to the more recent data . we tested ls-dqn on five atari games and demonstrate significant improvement over vanilla dqn and double-dqn . we also investigated the reasons for the superior performance of our method . interestingly , we found that the performance improvement can be attributed to the large batch size used by the ls method when optimizing the last layer .", "topics": ["approximation algorithm", "reinforcement learning"]}
{"title": "uniform deviation bounds for unbounded loss functions like k-means", "abstract": "uniform deviation bounds limit the difference between a model 's expected loss and its loss on an empirical sample uniformly for all models in a learning problem . as such , they are a critical component to empirical risk minimization . in this paper , we provide a novel framework to obtain uniform deviation bounds for loss functions which are *unbounded* . in our main application , this allows us to obtain bounds for $ k $ -means clustering under weak assumptions on the underlying distribution . if the fourth moment is bounded , we prove a rate of $ \\mathcal { o } \\left ( m^ { -\\frac12 } \\right ) $ compared to the previously known $ \\mathcal { o } \\left ( m^ { -\\frac14 } \\right ) $ rate . furthermore , we show that the rate also depends on the kurtosis - the normalized fourth moment which measures the `` tailedness '' of a distribution . we further provide improved rates under progressively stronger assumptions , namely , bounded higher moments , subgaussianity and bounded support .", "topics": ["cluster analysis", "loss function"]}
{"title": "queuing theory guided intelligent traffic scheduling through video analysis using dirichlet process mixture model", "abstract": "accurate prediction of traffic signal duration for roadway junction is a challenging problem due to the dynamic nature of traffic flows . though supervised learning can be used , parameters may vary across roadway junctions . in this paper , we present a computer vision guided expert system that can learn the departure rate of a given traffic junction modeled using traditional queuing theory . first , we temporally group the optical flow of the moving vehicles using dirichlet process mixture model ( dpmm ) . these groups are referred to as tracklets or temporal clusters . tracklet features are then used to learn the dynamic behavior of a traffic junction , especially during on/off cycles of a signal . the proposed queuing theory based approach can predict the signal open duration for the next cycle with higher accuracy when compared with other popular features used for tracking . the hypothesis has been verified on two publicly available video datasets . the results reveal that the dpmm based features are better than existing tracking frameworks to estimate $ \\mu $ . thus , signal duration prediction is more accurate when tested on these datasets.the method can be used for designing intelligent operator-independent traffic control systems for roadway junctions at cities and highways .", "topics": ["supervised learning", "computer vision"]}
{"title": "classification with costly features using deep reinforcement learning", "abstract": "we study a classification problem where each feature can be acquired for a cost and the goal is to optimize the trade-off between classification precision and the total feature cost . we frame the problem as a sequential decision-making problem , where we classify one sample in each episode . at each step , an agent can use values of acquired features to decide whether to purchase another one or whether to classify the sample . we use vanilla double deep q-learning , a standard reinforcement learning technique , to find a classification policy . we show that this generic approach outperforms adapt-gbrt , currently the best-performing algorithm developed specifically for classification with costly features .", "topics": ["value ( ethics )", "reinforcement learning"]}
{"title": "it-dendrogram : a new member of the in-tree ( it ) clustering family", "abstract": "previously , we proposed a physically-inspired method to construct data points into an effective in-tree ( it ) structure , in which the underlying cluster structure in the dataset is well revealed . although there are some edges in the it structure requiring to be removed , such undesired edges are generally distinguishable from other edges and thus are easy to be determined . for instance , when the it structures for the 2-dimensional ( 2d ) datasets are graphically presented , those undesired edges can be easily spotted and interactively determined . however , in practice , there are many datasets that do not lie in the 2d euclidean space , thus their it structures can not be graphically presented . but if we can effectively map those it structures into a visualized space in which the salient features of those undesired edges are preserved , then the undesired edges in the it structures can still be visually determined in a visualization environment . previously , this purpose was reached by our method called it-map . the outstanding advantage of it-map is that clusters can still be found even with the so-called crowding problem in the embedding . in this paper , we propose another method , called it-dendrogram , to achieve the same goal through an effective combination of the it structure and the single link hierarchical clustering ( slhc ) method . like it-map , it-dendrogram can also effectively represent the it structures in a visualization environment , whereas using another form , called the dendrogram . it-dendrogram can serve as another visualization method to determine the undesired edges in the it structures and thus benefit the it-based clustering analysis . this was demonstrated on several datasets with different shapes , dimensions , and attributes . unlike it-map , it-dendrogram can always avoid the crowding problem , which could help users make more reliable cluster analysis in certain problems .", "topics": ["cluster analysis"]}
{"title": "compressive sensing via convolutional factor analysis", "abstract": "we solve the compressive sensing problem via convolutional factor analysis , where the convolutional dictionaries are learned { \\em in situ } from the compressed measurements . an alternating direction method of multipliers ( admm ) paradigm for compressive sensing inversion based on convolutional factor analysis is developed . the proposed algorithm provides reconstructed images as well as features , which can be directly used for recognition ( $ e.g . $ , classification ) tasks . when a deep ( multilayer ) model is constructed , a stochastic unpooling process is employed to build a generative model . during reconstruction and testing , we project the upper layer dictionary to the data level and only a single layer deconvolution is required . we demonstrate that using $ \\sim30\\ % $ ( relative to pixel numbers ) compressed measurements , the proposed model achieves the classification accuracy comparable to the original data on mnist . we also observe that when the compressed measurements are very limited ( $ e.g . $ , $ < 10\\ % $ ) , the upper layer dictionary can provide better reconstruction results than the bottom layer .", "topics": ["generative model", "statistical classification"]}
{"title": "efficient tracking of a growing number of experts", "abstract": "we consider a variation on the problem of prediction with expert advice , where new forecasters that were unknown until then may appear at each round . as often in prediction with expert advice , designing an algorithm that achieves near-optimal regret guarantees is straightforward , using aggregation of experts . however , when the comparison class is sufficiently rich , for instance when the best expert and the set of experts itself changes over time , such strategies naively require to maintain a prohibitive number of weights ( typically exponential with the time horizon ) . by contrast , designing strategies that both achieve a near-optimal regret and maintain a reasonable number of weights is highly non-trivial . we consider three increasingly challenging objectives ( simple regret , shifting regret and sparse shifting regret ) that extend existing notions defined for a fixed expert ensemble ; in each case , we design strategies that achieve tight regret bounds , adaptive to the parameters of the comparison class , while being computationally inexpensive . moreover , our algorithms are anytime , agnostic to the number of incoming experts and completely parameter-free . such remarkable results are made possible thanks to two simple but highly effective recipes : first the `` abstention trick '' that comes from the specialist framework and enables to handle the least challenging notions of regret , but is limited when addressing more sophisticated objectives . second , the `` muting trick '' that we introduce to give more flexibility . we show how to combine these two tricks in order to handle the most challenging class of comparison strategies .", "topics": ["regret ( decision theory )", "time complexity"]}
{"title": "evaluation of image segmentation and filtering with ann in the papaya leaf", "abstract": "precision agriculture is area with lack of cheap technology . the refinement of the production system brings large advantages to the producer and the use of images makes the monitoring a more cheap methodology . macronutrients monitoring can to determine the health and vulnerability of the plant in specific stages . in this paper is analyzed the method based on computational intelligence to work with image segmentation in the identification of symptoms of plant nutrient deficiency . artificial neural networks are evaluated for image segmentation and filtering , several variations of parameters and insertion impulsive noise were evaluated too . satisfactory results are achieved with artificial neural for segmentation same with high noise levels .", "topics": ["image segmentation"]}
{"title": "learning to represent mechanics via long-term extrapolation and interpolation", "abstract": "while the basic laws of newtonian mechanics are well understood , explaining a physical scenario still requires manually modeling the problem with suitable equations and associated parameters . in order to adopt such models for artificial intelligence , researchers have handcrafted the relevant states , and then used neural networks to learn the state transitions using simulation runs as training data . unfortunately , such approaches can be unsuitable for modeling complex real-world scenarios , where manually authoring relevant state spaces tend to be challenging . in this work , we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data , and thus enable long-term physical extrapolation . we develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates . we evaluate our setup to extrapolate motion of a rolling ball on bowl of varying shape and orientation using only images as input , and report competitive results with approaches that assume access to internal physics models and parameters .", "topics": ["test set", "recurrent neural network"]}
{"title": "learning deep architectures for interaction prediction in structure-based virtual screening", "abstract": "we introduce a deep learning architecture for structure-based virtual screening that generates fixed-sized fingerprints of proteins and small molecules by applying learnable atom convolution and softmax operations to each compound separately . these fingerprints are further transformed non-linearly , their inner-product is calculated and used to predict the binding potential . moreover , we show that widely used benchmark datasets may be insufficient for testing structure-based virtual screening methods that utilize machine learning . therefore , we introduce a new benchmark dataset , which we constructed based on dud-e and pdbbind databases .", "topics": ["convolution", "database"]}
{"title": "dialog state tracking , a machine reading approach using memory network", "abstract": "in an end-to-end dialog system , the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules . this paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an end-to-end memory network , memn2n , a memory-enhanced neural network architecture . we evaluate the proposed approach on the second dialog state tracking challenge ( dstc-2 ) dataset . the corpus has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog . we show that the proposed tracker gives encouraging results . then , we propose to extend the dstc-2 dataset with specific reasoning capabilities requirement like counting , list maintenance , yes-no question answering and indefinite knowledge management . finally , we present encouraging results using our proposed memn2n based tracking model .", "topics": ["reinforcement learning", "natural language"]}
{"title": "sideeye : a generative neural network based simulator of human peripheral vision", "abstract": "foveal vision makes up less than 1 % of the visual field . the other 99 % is peripheral vision . precisely what human beings see in the periphery is both obvious and mysterious in that we see it with our own eyes but ca n't visualize what we see , except in controlled lab experiments . degradation of information in the periphery is far more complex than what might be mimicked with a radial blur . rather , behaviorally-validated models hypothesize that peripheral vision measures a large number of local texture statistics in pooling regions that overlap and grow with eccentricity . in this work , we develop a new method for peripheral vision simulation by training a generative neural network on a behaviorally-validated full-field synthesis model . by achieving a 21,000 fold reduction in running time , our approach is the first to combine realism and speed of peripheral vision simulation to a degree that provides a whole new way to approach visual design : through peripheral visualization .", "topics": ["time complexity", "simulation"]}
{"title": "natural language generation enhances human decision-making with uncertain information", "abstract": "decision-making is often dependent on uncertain data , e.g . data associated with confidence scores or probabilities . we present a comparison of different information presentations for uncertain data and , for the first time , measure their effects on human decision-making . we show that the use of natural language generation ( nlg ) improves decision-making under uncertainty , compared to state-of-the-art graphical-based representation methods . in a task-based study with 442 adults , we found that presentations using nlg lead to 24 % better decision-making on average than the graphical presentations , and to 44 % better decision-making when nlg is combined with graphics . we also show that women achieve significantly better results when presented with nlg output ( an 87 % increase on average compared to graphical presentations ) .", "topics": ["natural language"]}
{"title": "cooperative multi-agent planning : a survey", "abstract": "cooperative multi-agent planning ( map ) is a relatively recent research field that combines technologies , algorithms and techniques developed by the artificial intelligence planning and multi-agent systems communities . while planning has been generally treated as a single-agent task , map generalizes this concept by considering multiple intelligent agents that work cooperatively to develop a course of action that satisfies the goals of the group . this paper reviews the most relevant approaches to map , putting the focus on the solvers that took part in the 2015 competition of distributed and multi-agent planning , and classifies them according to their key features and relative performance .", "topics": ["artificial intelligence"]}
{"title": "pitfalls and best practices in algorithm configuration", "abstract": "good parameter settings are crucial to achieve high performance in many areas of artificial intelligence ( ai ) , such as satisfiability solving , ai planning , scheduling , and machine learning ( in particular deep learning ) . automated algorithm configuration methods have recently received much attention in the ai community since they replace tedious , irreproducible and error-prone manual parameter tuning and can lead to new state-of-the-art performance . however , practical applications of algorithm configuration are prone to several ( often subtle ) pitfalls in the experimental design that can render the procedure ineffective . we identify several common issues and propose best practices for avoiding them , including a tool called genericwrapper4ac for preventing the many possible problems in measuring the performance of the algorithm being optimized by executing it in a standardized , controlled manner .", "topics": ["artificial intelligence"]}
{"title": "exact and inexact subsampled newton methods for optimization", "abstract": "the paper studies the solution of stochastic optimization problems in which approximations to the gradient and hessian are obtained through subsampling . we first consider newton-like methods that employ these approximations and discuss how to coordinate the accuracy in the gradient and hessian to yield a superlinear rate of convergence in expectation . the second part of the paper analyzes an inexact newton method that solves linear systems approximately using the conjugate gradient ( cg ) method , and that samples the hessian and not the gradient ( the gradient is assumed to be exact ) . we provide a complexity analysis for this method based on the properties of the cg iteration and the quality of the hessian approximation , and compare it with a method that employs a stochastic gradient iteration instead of the cg method . we report preliminary numerical results that illustrate the performance of inexact subsampled newton methods on machine learning applications based on logistic regression .", "topics": ["numerical analysis", "iteration"]}
{"title": "predicting movie genres based on plot summaries", "abstract": "this project explores several machine learning methods to predict movie genres based on plot summaries . naive bayes , word2vec+xgboost and recurrent neural networks are used for text classification , while k-binary transformation , rank method and probabilistic classification with learned probability threshold are employed for the multi-label problem involved in the genre tagging task.experiments with more than 250,000 movies show that employing the gated recurrent units ( gru ) neural networks for the probabilistic classification with learned probability threshold approach achieves the best result on the test set . the model attains a jaccard index of 50.0 % , a f-score of 0.56 , and a hit rate of 80.5 % .", "topics": ["test set", "recurrent neural network"]}
{"title": "an influence-based fast preceding questionnaire model for elderly assessments", "abstract": "to improve the efficiency of elderly assessments , an influence-based fast preceding questionnaire model ( fpqm ) is proposed . compared with traditional assessments , the fpqm optimizes questionnaires by reordering their attributes . the values of low-ranking attributes can be predicted by the values of the high-ranking attributes . therefore , the number of attributes can be reduced without redesigning the questionnaires . a new function for calculating the influence of the attributes is proposed based on probability theory . reordering and reducing algorithms are given based on the attributes ' influences . the model is verified through a practical application . the practice in an elderly-care company shows that the fpqm can reduce the number of attributes by 90.56 % with a prediction accuracy of 98.39 % . compared with other methods , such as the expert knowledge , rough set and c4.5 methods , the fpqm achieves the best performance . in addition , the fpqm can also be applied to other questionnaires .", "topics": ["value ( ethics )"]}
{"title": "language modeling with power low rank ensembles", "abstract": "we present power low rank ensembles ( plre ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context . our method can be understood as a generalization of n-gram modeling to non-integer n , and includes standard techniques such as absolute discounting and kneser-ney smoothing as special cases . plre training is efficient and our approach outperforms state-of-the-art modified kneser ney baselines in terms of perplexity on large corpora as well as on bleu score in a downstream machine translation task .", "topics": ["machine translation", "text corpus"]}
{"title": "eliciting disease data from wikipedia articles", "abstract": "traditional disease surveillance systems suffer from several disadvantages , including reporting lags and antiquated technology , that have caused a movement towards internet-based disease surveillance systems . internet systems are particularly attractive for disease outbreaks because they can provide data in near real-time and can be verified by individuals around the globe . however , most existing systems have focused on disease monitoring and do not provide a data repository for policy makers or researchers . in order to fill this gap , we analyzed wikipedia article content . we demonstrate how a named-entity recognizer can be trained to tag case counts , death counts , and hospitalization counts in the article narrative that achieves an f1 score of 0.753 . we also show , using the 2014 west african ebola virus disease epidemic article as a case study , that there are detailed time series data that are consistently updated that closely align with ground truth data . we argue that wikipedia can be used to create the first community-driven open-source emerging disease detection , monitoring , and repository system .", "topics": ["time series", "ground truth"]}
{"title": "natasha 2 : faster non-convex optimization than sgd", "abstract": "we design a stochastic algorithm to train any smooth neural network to $ \\varepsilon $ -approximate local minima , using $ o ( \\varepsilon^ { -3.25 } ) $ backpropagations . the best result was essentially $ o ( \\varepsilon^ { -4 } ) $ by sgd . more broadly , it finds $ \\varepsilon $ -approximate local minima of any smooth nonconvex function in rate $ o ( \\varepsilon^ { -3.25 } ) $ , with only oracle access to stochastic gradients .", "topics": ["matrix regularization", "gradient"]}
{"title": "a rigorous definition of axial lines : ridges on isovist fields", "abstract": "we suggest that 'axial lines ' defined by ( hillier and hanson , 1984 ) as lines of uninterrupted movement within urban streetscapes or buildings , appear as ridges in isovist fields ( benedikt , 1979 ) . these are formed from the maximum diametric lengths of the individual isovists , sometimes called viewsheds , that make up these fields ( batty and rana , 2004 ) . we present an image processing technique for the identification of lines from ridges , discuss current strengths and weaknesses of the method , and show how it can be implemented easily and effectively .", "topics": ["image processing", "computer vision"]}
{"title": "massively deep artificial neural networks for handwritten digit recognition", "abstract": "greedy restrictive boltzmann machines yield an fairly low 0.72 % error rate on the famous mnist database of handwritten digits . all that was required to achieve this result was a high number of hidden layers consisting of many neurons , and a graphics card to greatly speed up the rate of learning .", "topics": ["mnist database"]}
{"title": "sk_p : a neural program corrector for moocs", "abstract": "we present a novel technique for automatic program correction in moocs , capable of fixing both syntactic and semantic errors without manual , problem specific correction strategies . given an incorrect student program , it generates candidate programs from a distribution of likely corrections , and checks each candidate for correctness against a test suite . the key observation is that in moocs many programs share similar code fragments , and the seq2seq neural network model , used in the natural-language processing task of machine translation , can be modified and trained to recover these fragments . experiment shows our scheme can correct 29 % of all incorrect submissions and out-performs state of the art approach which requires manual , problem specific correction strategies .", "topics": ["machine translation"]}
{"title": "fast generation of best interval patterns for nonmonotonic constraints", "abstract": "in pattern mining , the main challenge is the exponential explosion of the set of patterns . typically , to solve this problem , a constraint for pattern selection is introduced . one of the first constraints proposed in pattern mining is support ( frequency ) of a pattern in a dataset . frequency is an anti-monotonic function , i.e . , given an infrequent pattern , all its superpatterns are not frequent . however , many other constraints for pattern selection are not ( anti- ) monotonic , which makes it difficult to generate patterns satisfying these constraints . in this paper we introduce the notion of projection-antimonotonicity and $ \\theta $ - $ \\sigma\\o\\phi\\iota\\alpha $ algorithm that allows efficient generation of the best patterns for some nonmonotonic constraints . in this paper we consider stability and $ \\delta $ -measure , which are nonmonotonic constraints , and apply them to interval tuple datasets . in the experiments , we compute best interval tuple patterns w.r.t . these measures and show the advantage of our approach over postfiltering approaches . keywords : pattern mining , nonmonotonic constraints , interval tuple data", "topics": ["data mining", "time complexity"]}
{"title": "tikhonov regularization for long short-term memory networks", "abstract": "it is a well-known fact that adding noise to the input data often improves network performance . while the dropout technique may be a cause of memory loss , when it is applied to recurrent connections , tikhonov regularization , which can be regarded as the training with additive noise , avoids this issue naturally , though it implies regularizer derivation for different architectures . in case of feedforward neural networks this is straightforward , while for networks with recurrent connections and complicated layers it leads to some difficulties . in this paper , a tikhonov regularizer is derived for long-short term memory ( lstm ) networks . although it is independent of time for simplicity , it considers interaction between weights of the lstm unit , which in theory makes it possible to regularize the unit with complicated dependences by using only one parameter that measures the input data perturbation . the regularizer that is proposed in this paper has three parameters : one to control the regularization process , and other two to maintain computation stability while the network is being trained . the theory developed in this paper can be applied to get such regularizers for different recurrent neural networks with hadamard products and lipschitz continuous functions .", "topics": ["recurrent neural network", "matrix regularization"]}
{"title": "patchwork kriging for large-scale gaussian process regression", "abstract": "this paper presents a new approach for gaussian process ( gp ) regression for large datasets . the approach involves partitioning the regression input domain into multiple local regions with a different local gp model fitted in each region . unlike existing local partitioned gp approaches , we introduce a technique for patching together the local gp models nearly seamlessly to ensure that the local gp models for two neighboring regions produce nearly the same response prediction and prediction error variance on the boundary between the two regions . this effectively solves the well-known discontinuity problem that degrades the boundary accuracy of existing local partitioned gp methods . our main innovation is to represent the continuity conditions as additional pseudo-observations that the differences between neighboring gp responses are identically zero at an appropriately chosen set of boundary input locations . to predict the response at any input location , we simply augment the actual response observations with the pseudo-observations and apply standard gp prediction methods to the augmented data . in contrast to heuristic continuity adjustments , this has an advantage of working within a formal gp framework , so that the gp-based predictive uncertainty quantification remains valid . our approach also inherits a sparse block-like structure for the sample covariance matrix , which results in computationally efficient closed-form expressions for the predictive mean and variance . in addition , we provide a new spatial partitioning scheme based on a recursive space partitioning along local principal component directions , which makes the proposed approach applicable for regression domains having more than two dimensions . using three spatial datasets and three higher dimensional datasets , we investigate the numerical performance of the approach and compare it to several state-of-the-art approaches .", "topics": ["computational complexity theory", "numerical analysis"]}
{"title": "expectation propagation for approximate bayesian inference", "abstract": "this paper presents a new deterministic approximation technique in bayesian networks . this method , `` expectation propagation '' , unifies two previous techniques : assumed-density filtering , an extension of the kalman filter , and loopy belief propagation , an extension of belief propagation in bayesian networks . all three algorithms try to recover an approximate distribution which is close in kl divergence to the true distribution . loopy belief propagation , because it propagates exact belief states , is useful for a limited class of belief networks , such as those which are purely discrete . expectation propagation approximates the belief states by only retaining certain expectations , such as mean and variance , and iterates until these expectations are consistent throughout the network . this makes it applicable to hybrid networks with discrete and continuous nodes . expectation propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes . experiments with gaussian mixture models show expectation propagation to be convincingly better than methods with similar computational cost : laplace 's method , variational bayes , and monte carlo . expectation propagation also provides an efficient algorithm for training bayes point machine classifiers .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "fast spectral clustering using autoencoders and landmarks", "abstract": "in this paper , we introduce an algorithm for performing spectral clustering efficiently . spectral clustering is a powerful clustering algorithm that suffers from high computational complexity , due to eigen decomposition . in this work , we first build the adjacency matrix of the corresponding graph of the dataset . to build this matrix , we only consider a limited number of points , called landmarks , and compute the similarity of all data points with the landmarks . then , we present a definition of the laplacian matrix of the graph that enable us to perform eigen decomposition efficiently , using a deep autoencoder . the overall complexity of the algorithm for eigen decomposition is $ o ( np ) $ , where $ n $ is the number of data points and $ p $ is the number of landmarks . at last , we evaluate the performance of the algorithm in different experiments .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "multi-task video captioning with video and entailment generation", "abstract": "video captioning , the task of describing the content of a video , has seen some promising improvements in recent years with sequence-to-sequence models , but accurately learning the temporal and logical dynamics involved in the task still remains a challenge , especially given the lack of sufficient annotated data . we improve video captioning by sharing knowledge with two related directed-generation tasks : a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations , and a logically-directed language entailment generation task to learn better video-entailed caption decoder representations . for this , we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks . we achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations . we also show mutual multi-task improvements on the entailment generation task .", "topics": ["encoder"]}
{"title": "em algorithms for weighted-data clustering with application to audio-visual scene analysis", "abstract": "data clustering has received a lot of attention and numerous methods , algorithms and software packages are available . among these techniques , parametric finite-mixture models play a central role due to their interesting mathematical properties and to the existence of maximum-likelihood estimators based on expectation-maximization ( em ) . in this paper we propose a new mixture model that associates a weight with each observed point . we introduce the weighted-data gaussian mixture and we derive two em algorithms . the first one considers a fixed weight for each observation . the second one treats each weight as a random variable following a gamma distribution . we propose a model selection method based on a minimum message length criterion , provide a weight initialization strategy , and validate the proposed algorithms by comparing them with several state of the art parametric and non-parametric clustering techniques . we also demonstrate the effectiveness and robustness of the proposed clustering technique in the presence of heterogeneous data , namely audio-visual scene analysis .", "topics": ["cluster analysis", "eisenstein 's criterion"]}
{"title": "computational geometry column 38", "abstract": "recent results on curve reconstruction are described .", "topics": ["computer vision", "heuristic"]}
{"title": "learning gradient descent : better generalization and longer horizons", "abstract": "training deep neural networks is a highly nontrivial task , involving carefully selecting appropriate training algorithms , scheduling step sizes and tuning other hyperparameters . trying different combinations can be quite labor-intensive and time consuming . recently , researchers have tried to use deep learning algorithms to exploit the landscape of the loss function of the training problem of interest , and learn how to optimize over it in an automatic way . in this paper , we propose a new learning-to-learn model and some useful and practical tricks . our optimizer outperforms generic , hand-crafted optimization algorithms and state-of-the-art learning-to-learn optimizers by deepmind in many tasks . we demonstrate the effectiveness of our algorithms on a number of tasks , including deep mlps , cnns , and simple lstms .", "topics": ["mathematical optimization", "loss function"]}
{"title": "constrained variable clustering and the best basis problem in functional data analysis", "abstract": "functional data analysis involves data described by regular functions rather than by a finite number of real valued variables . while some robust data analysis methods can be applied directly to the very high dimensional vectors obtained from a fine grid sampling of functional data , all methods benefit from a prior simplification of the functions that reduces the redundancy induced by the regularity . in this paper we propose to use a clustering approach that targets variables rather than individual to design a piecewise constant representation of a set of functions . the contiguity constraint induced by the functional nature of the variables allows a polynomial complexity algorithm to give the optimal solution .", "topics": ["sampling ( signal processing )", "cluster analysis"]}
{"title": "explorations on high dimensional landscapes", "abstract": "finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science . we provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points . this is in contrast with the low dimensional picture in which this band is wide . our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity . furthermore our experiments on teacher-student networks with the mnist dataset establish a similar phenomenon in deep networks . we finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps .", "topics": ["simulation", "gradient descent"]}
{"title": "selection in scale-free small world", "abstract": "in this paper we compare the performance characteristics of our selection based learning algorithm for web crawlers with the characteristics of the reinforcement learning algorithm . the task of the crawlers is to find new information on the web . the selection algorithm , called weblog update , modifies the starting url lists of our crawlers based on the found urls containing new information . the reinforcement learning algorithm modifies the url orderings of the crawlers based on the received reinforcements for submitted documents . we performed simulations based on data collected from the web . the collected portion of the web is typical and exhibits scale-free small world ( sfsw ) structure . we have found that on this sfsw , the weblog update algorithm performs better than the reinforcement learning algorithm . it finds the new information faster than the reinforcement learning algorithm and has better new information/all submitted documents ratio . we believe that the advantages of the selection algorithm over reinforcement learning algorithm is due to the small world property of the web .", "topics": ["reinforcement learning", "simulation"]}
{"title": "an explicit sampling dependent spectral error bound for column subset selection", "abstract": "in this paper , we consider the problem of column subset selection . we present a novel analysis of the spectral norm reconstruction for a simple randomized algorithm and establish a new bound that depends explicitly on the sampling probabilities . the sampling dependent error bound ( i ) allows us to better understand the tradeoff in the reconstruction error due to sampling probabilities , ( ii ) exhibits more insights than existing error bounds that exploit specific probability distributions , and ( iii ) implies better sampling distributions . in particular , we show that a sampling distribution with probabilities proportional to the square root of the statistical leverage scores is always better than uniform sampling and is better than leverage-based sampling when the statistical leverage scores are very nonuniform . and by solving a constrained optimization problem related to the error bound with an efficient bisection search we are able to achieve better performance than using either the leverage-based distribution or that proportional to the square root of the statistical leverage scores . numerical simulations demonstrate the benefits of the new sampling distributions for low-rank matrix approximation and least square approximation compared to state-of-the art algorithms .", "topics": ["sampling ( signal processing )", "optimization problem"]}
{"title": "the role of principal angles in subspace classification", "abstract": "subspace models play an important role in a wide range of signal processing tasks , and this paper explores how the pairwise geometry of subspaces influences the probability of misclassification . when the mismatch between the signal and the model is vanishingly small , the probability of misclassification is determined by the product of the sines of the principal angles between subspaces . when the mismatch is more significant , the probability of misclassification is determined by the sum of the squares of the sines of the principal angles . reliability of classification is derived in terms of the distribution of signal energy across principal vectors . larger principal angles lead to smaller classification error , motivating a linear transform that optimizes principal angles . the transform presented here ( trait ) preserves some specific characteristic of each individual class , and this approach is shown to be complementary to a previously developed transform ( lrt ) that enlarges inter-class distance while suppressing intra-class dispersion . theoretical results are supported by demonstration of superior classification accuracy on synthetic and measured data even in the presence of significant model mismatch .", "topics": ["synthetic data"]}
{"title": "exploiting the structure : stochastic gradient methods using raw clusters", "abstract": "the amount of data available in the world is growing faster than our ability to deal with it . however , if we take advantage of the internal \\emph { structure } , data may become much smaller for machine learning purposes . in this paper we focus on one of the fundamental machine learning tasks , empirical risk minimization ( erm ) , and provide faster algorithms with the help from the clustering structure of the data . we introduce a simple notion of raw clustering that can be efficiently computed from the data , and propose two algorithms based on clustering information . our accelerated algorithm clusteracdm is built on a novel haar transformation applied to the dual space of the erm problem , and our variance-reduction based algorithm clustersvrg introduces a new gradient estimator using clustering . our algorithms outperform their classical counterparts acdm and svrg respectively .", "topics": ["cluster analysis", "gradient"]}
{"title": "parametric return density estimation for reinforcement learning", "abstract": "most conventional reinforcement learning ( rl ) algorithms aim to optimize decision-making rules in terms of the expected returns . however , especially for risk management purposes , other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications . here , we describe a parametric method for estimating density of the returns , which allows us to handle various criteria in a unified manner . we first extend the bellman equation for the conditional expected return to cover a conditional probability density of the returns . then we derive an extension of the td-learning algorithm for estimating the return densities in an unknown environment . as test instances , several parametric density estimation algorithms are presented for the gaussian , laplace , and skewed laplace distributions . we show that these algorithms lead to risk-sensitive as well as robust rl paradigms through numerical experiments .", "topics": ["numerical analysis", "reinforcement learning"]}
{"title": "multi-view learning in the presence of view disagreement", "abstract": "traditional multi-view learning approaches suffer in the presence of view disagreement , i.e . , when samples in each view do not belong to the same class due to view corruption , occlusion or other noise processes . in this paper we present a multi-view learning approach that uses a conditional entropy criterion to detect view disagreement . once detected , samples with view disagreement are filtered and standard multi-view learning methods can be successfully applied to the remaining samples . experimental evaluation on synthetic and audio-visual databases demonstrates that the detection and filtering of view disagreement considerably increases the performance of traditional multi-view learning approaches .", "topics": ["synthetic data", "database"]}
{"title": "deep semi-random features for nonlinear function approximation", "abstract": "we propose semi-random features for nonlinear function approximation . the flexibility of semi-random feature lies between the fully adjustable units in deep learning and the random features used in kernel methods . for one hidden layer models with semi-random features , we prove with no unrealistic assumptions that the model classes contain an arbitrarily good function as the width increases ( universality ) , and despite non-convexity , we can find such a good function ( optimization theory ) that generalizes to unseen new data ( generalization bound ) . for deep models , with no unrealistic assumptions , we prove universal approximation ability , a lower bound on approximation error , a partial optimization guarantee , and a generalization bound . depending on the problems , the generalization bound of deep semi-random features can be exponentially better than the known bounds of deep relu nets ; our generalization error bound can be independent of the depth , the number of trainable weights as well as the input dimensionality . in experiments , we show that semi-random features can match the performance of neural networks by using slightly more units , and it outperforms random features by using significantly fewer units . moreover , we introduce a new implicit ensemble method by using semi-random features .", "topics": ["mathematical optimization", "nonlinear system"]}
{"title": "sublinear partition estimation", "abstract": "the output scores of a neural network classifier are converted to probabilities via normalizing over the scores of all competing categories . computing this partition function , $ z $ , is then linear in the number of categories , which is problematic as real-world problem sets continue to grow in categorical types , such as in visual object recognition or discriminative language modeling . we propose three approaches for sublinear estimation of the partition function , based on approximate nearest neighbor search and kernel feature maps and compare the performance of the proposed approaches empirically .", "topics": ["approximation algorithm", "map"]}
{"title": "efficiently summarising event sequences with rich interleaving patterns", "abstract": "discovering the key structure of a database is one of the main goals of data mining . in pattern set mining we do so by discovering a small set of patterns that together describe the data well . the richer the class of patterns we consider , and the more powerful our description language , the better we will be able to summarise the data . in this paper we propose \\ourmethod , a novel greedy mdl-based method for summarising sequential data using rich patterns that are allowed to interleave . experiments show \\ourmethod is orders of magnitude faster than the state of the art , results in better models , as well as discovers meaningful semantics in the form patterns that identify multiple choices of values .", "topics": ["data mining"]}
{"title": "a semi-supervised fuzzy growcut algorithm to segment and classify regions of interest of mammographic images", "abstract": "according to the world health organization , breast cancer is the most common form of cancer in women . it is the second leading cause of death among women round the world , becoming the most fatal form of cancer . mammographic image segmentation is a fundamental task to support image analysis and diagnosis , taking into account shape analysis of mammary lesions and their borders . however , mammogram segmentation is a very hard process , once it is highly dependent on the types of mammary tissues . in this work we present a new semi-supervised segmentation algorithm based on the modification of the growcut algorithm to perform automatic mammographic image segmentation once a region of interest is selected by a specialist . in our proposal , we used fuzzy gaussian membership functions to modify the evolution rule of the original growcut algorithm , in order to estimate the uncertainty of a pixel being object or background . the main impact of the proposed method is the significant reduction of expert effort in the initialization of seed points of growcut to perform accurate segmentation , once it removes the need of selection of background seeds . we also constructed an automatic point selection process based on the simulated annealing optimization method , avoiding the need of human intervention . the proposed approach was qualitatively compared with other state-of-the-art segmentation techniques , considering the shape of segmented regions . in order to validate our proposal , we built an image classifier using a classical multilayer perceptron . we used zernike moments to extract segmented image features . this analysis employed 685 mammograms from irma breast cancer database , using fat and fibroid tissues . results show that the proposed technique could achieve a classification rate of 91.28\\ % for fat tissues , evidencing the feasibility of our approach .", "topics": ["image segmentation", "simulation"]}
{"title": "spectral feature mapping with mimic loss for robust speech recognition", "abstract": "for the task of speech enhancement , local learning objectives are agnostic to phonetic structures helpful for speech recognition . we propose to add a global criterion to ensure de-noised speech is useful for downstream tasks like asr . we first train a spectral classifier on clean speech to predict senone labels . then , the spectral classifier is joined with our speech enhancer as a noisy speech recognizer . this model is taught to imitate the output of the spectral classifier alone on clean speech . this \\textit { mimic loss } is combined with the traditional local criterion to train the speech enhancer to produce de-noised speech . feeding the de-noised speech to an off-the-shelf kaldi training recipe for the chime-2 corpus shows significant improvements in wer .", "topics": ["speech recognition", "eisenstein 's criterion"]}
{"title": "an associative memory for the on-line recognition and prediction of temporal sequences", "abstract": "this paper presents the design of an associative memory with feedback that is capable of on-line temporal sequence learning . a framework for on-line sequence learning has been proposed , and different sequence learning models have been analysed according to this framework . the network model is an associative memory with a separate store for the sequence context of a symbol . a sparse distributed memory is used to gain scalability . the context store combines the functionality of a neural layer with a shift register . the sensitivity of the machine to the sequence context is controllable , resulting in different characteristic behaviours . the model can store and predict on-line sequences of various types and length . numerical simulations on the model have been carried out to determine its properties .", "topics": ["numerical analysis", "simulation"]}
{"title": "generative adversarial networks using adaptive convolution", "abstract": "most existing gans architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator . we argue that this kind of fixed operation is problematic for gans to model objects that have very different visual appearances . we propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem . we modify a baseline gans architecture by replacing normal convolutions with adaptive convolutions in the generator . experiments on cifar-10 dataset show that our modified models improve the baseline model by a large margin . furthermore , our models achieve state-of-the-art performance on cifar-10 and stl-10 datasets in the unsupervised setting .", "topics": ["baseline ( configuration management )", "map"]}
{"title": "salientdso : bringing attention to direct sparse odometry", "abstract": "although cluttered indoor scenes have a lot of useful high-level semantic information which can be used for mapping and localization , most visual odometry ( vo ) algorithms rely on the usage of geometric features such as points , lines and planes . lately , driven by this idea , the joint optimization of semantic labels and obtaining odometry has gained popularity in the robotics community . the joint optimization is good for accurate results but is generally very slow . at the same time , in the vision community , direct and sparse approaches for vo have stricken the right balance between speed and accuracy . we merge the successes of these two communities and present a way to incorporate semantic information in the form of visual saliency to direct sparse odometry - a highly successful direct sparse vo algorithm . we also present a framework to filter the visual saliency based on scene parsing . our framework , salientdso , relies on the widely successful deep learning based approaches for visual saliency and scene parsing which drives the feature selection for obtaining highly-accurate and robust vo even in the presence of as few as 40 point features per frame . we provide extensive quantitative evaluation of salientdso on the icl-nuim and tum monovo datasets and show that we outperform dso and orb-slam - two very popular state-of-the-art approaches in the literature . we also collect and publicly release a cvl-umd dataset which contains two indoor cluttered sequences on which we show qualitative evaluations . to our knowledge this is the first paper to use visual saliency and scene parsing to drive the feature selection in direct vo .", "topics": ["high- and low-level", "parsing"]}
{"title": "the intriguing properties of model explanations", "abstract": "linear approximations to the decision boundary of a complex model have become one of the most popular tools for interpreting predictions . in this paper , we study such linear explanations produced either post-hoc by a few recent methods or generated along with predictions with contextual explanation networks ( cens ) . we focus on two questions : ( i ) whether linear explanations are always consistent or can be misleading , and ( ii ) when integrated into the prediction process , whether and how explanations affect the performance of the model . our analysis sheds more light on certain properties of explanations produced by different methods and suggests that learning models that explain and predict jointly is often advantageous .", "topics": ["approximation"]}
{"title": "flexibly mining better subgroups", "abstract": "in subgroup discovery , also known as supervised pattern mining , discovering high quality one-dimensional subgroups and refinements of these is a crucial task . for nominal attributes , this is relatively straightforward , as we can consider individual attribute values as binary features . for numerical attributes , the task is more challenging as individual numeric values are not reliable statistics . instead , we can consider combinations of adjacent values , i.e . bins . existing binning strategies , however , are not tailored for subgroup discovery . that is , they do not directly optimize for the quality of subgroups , therewith potentially degrading the mining result . to address this issue , we propose flexi . in short , with flexi we propose to use optimal binning to find high quality binary features for both numeric and ordinal attributes . we instantiate flexi with various quality measures and show how to achieve efficiency accordingly . experiments on both synthetic and real-world data sets show that flexi outperforms state of the art with up to 25 times improvement in subgroup quality .", "topics": ["data mining", "value ( ethics )"]}
{"title": "inferring logical forms from denotations", "abstract": "a core problem in learning semantic parsers from denotations is picking out consistent logical forms -- those that yield the correct denotation -- from a combinatorially large space . to control the search space , previous work relied on restricted set of rules , which limits expressivity . in this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms . expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance . to address this , we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms . on the wikitablequestions dataset , we increase the coverage of answerable questions from 53.5 % to 76 % , and the additional crowdsourced supervision lets us rule out 92.1 % of spurious logical forms .", "topics": ["parsing"]}
{"title": "understanding symmetries in deep networks", "abstract": "recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the euclidean gradient based stochastic gradient descent optimization . in this work , we show that a commonly used deep network , which uses convolution , batch normalization , relu , max-pooling , and sub-sampling pipeline , possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights . we propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold . consequently , training the network boils down to using stochastic gradient descent updates on the unit-norm manifold . our empirical evidence based on the mnist dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates .", "topics": ["sampling ( signal processing )", "gradient descent"]}
{"title": "the marginal value of adaptive gradient methods in machine learning", "abstract": "adaptive optimization methods , which perform local optimization with a metric constructed from the history of iterates , are becoming increasingly popular for training deep neural networks . examples include adagrad , rmsprop , and adam . we show that for simple overparameterized problems , adaptive methods often find drastically different solutions than gradient descent ( gd ) or stochastic gradient descent ( sgd ) . we construct an illustrative binary classification problem where the data is linearly separable , gd and sgd achieve zero test error , and adagrad , adam , and rmsprop attain test errors arbitrarily close to half . we additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models . we observe that the solutions found by adaptive methods generalize worse ( often significantly worse ) than sgd , even when these solutions have better training performance . these results suggest that practitioners should reconsider the use of adaptive methods to train neural networks .", "topics": ["gradient descent", "gradient"]}
{"title": "kernel robust bias-aware prediction under covariate shift", "abstract": "under covariate shift , training ( source ) data and testing ( target ) data differ in input space distribution , but share the same conditional label distribution . this poses a challenging machine learning task . robust bias-aware ( rba ) prediction provides the conditional label distribution that is robust to the worstcase logarithmic loss for the target distribution while matching feature expectation constraints from the source distribution . however , employing rba with insufficient feature constraints may result in high certainty predictions for much of the source data , while leaving too much uncertainty for target data predictions . to overcome this issue , we extend the representer theorem to the rba setting , enabling minimization of regularized expected target risk by a reweighted kernel expectation under the source distribution . by applying kernel methods , we establish consistency guarantees and demonstrate better performance of the rba classifier than competing methods on synthetically biased uci datasets as well as datasets that have natural covariate shift .", "topics": ["kernel ( operating system )"]}
{"title": "lower bounds over boolean inputs for deep neural networks with relu gates", "abstract": "motivated by the resurgence of neural networks in being able to solve complex learning tasks we undertake a study of high depth networks using relu gates which implement the function $ x \\mapsto \\max\\ { 0 , x\\ } $ . we try to understand the role of depth in such neural networks by showing size lowerbounds against such network architectures in parameter regimes hitherto unexplored . in particular we show the following two main results about neural nets computing boolean functions of input dimension $ n $ , 1 . we use the method of random restrictions to show almost linear , $ \\omega ( \\epsilon^ { 2 ( 1-\\delta ) } n^ { 1-\\delta } ) $ , lower bound for completely weight unrestricted ltf-of-relu circuits to match the andreev function on at least $ \\frac { 1 } { 2 } +\\epsilon $ fraction of the inputs for $ \\epsilon > \\sqrt { 2\\frac { \\log^ { \\frac { 2 } { 2-\\delta } } ( n ) } { n } } $ for any $ \\delta \\in ( 0 , \\frac 1 2 ) $ 2 . we use the method of sign-rank to show exponential in dimension lower bounds for relu circuits ending in a ltf gate and of depths upto $ o ( n^ { \\xi } ) $ with $ \\xi < \\frac { 1 } { 8 } $ with some restrictions on the weights in the bottom most layer . all other weights in these circuits are kept unrestricted . this in turns also implies the same lowerbounds for ltf circuits with the same architecture and the same weight restrictions on their bottom most layer . along the way we also show that there exists a $ \\mathbb { r } ^ n\\rightarrow \\mathbb { r } $ sum-of-relu-of-relu function which sum-of-relu neural nets can never represent no matter how large they are allowed to be .", "topics": ["time complexity"]}
{"title": "a probabilistic disease progression model for predicting future clinical outcome", "abstract": "in this work , we consider the problem of predicting the course of a progressive disease , such as cancer or alzheimer 's . progressive diseases often start with mild symptoms that might precede a diagnosis , and each patient follows their own trajectory . patient trajectories exhibit wild variability , which can be associated with many factors such as genotype , age , or sex . an additional layer of complexity is that , in real life , the amount and type of data available for each patient can differ significantly . for example , for one patient we might have no prior history , whereas for another patient we might have detailed clinical assessments obtained at multiple prior time-points . this paper presents a probabilistic model that can handle multiple modalities ( including images and clinical assessments ) and variable patient histories with irregular timings and missing entries , to predict clinical scores at future time-points . we use a sigmoidal function to model latent disease progression , which gives rise to clinical observations in our generative model . we implemented an approximate bayesian inference strategy on the proposed model to estimate the parameters on data from a large population of subjects . furthermore , the bayesian framework enables the model to automatically fine-tune its predictions based on historical observations that might be available on the test subject . we applied our method to a longitudinal alzheimer 's disease dataset with more than 3000 subjects [ 23 ] and present a detailed empirical analysis of prediction performance under different scenarios , with comparisons against several benchmarks . we also demonstrate how the proposed model can be interrogated to glean insights about temporal dynamics in alzheimer 's disease .", "topics": ["generative model", "approximation algorithm"]}
{"title": "towards a formal distributional semantics : simulating logical calculi with tensors", "abstract": "the development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature . this paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models , such as tensors and matrices , can be used to simulate different aspects of predicate logic . this paper discusses how the canonical isomorphism between tensors and multilinear maps can be exploited to simulate a full-blown quantifier-free predicate calculus using tensors . it provides tensor interpretations of the set of logical connectives required to model propositional calculi . it suggests a variant of these tensor calculi capable of modelling quantifiers , using few non-linear operations . it finally discusses the relation between these variants , and how this relation should constitute the subject of future work .", "topics": ["nonlinear system", "map"]}
{"title": "the kit motion-language dataset", "abstract": "linking human motion and natural language is of great interest for the generation of semantic representations of human activities as well as for the generation of robot activities based on natural language input . however , while there have been years of research in this area , no standardized and openly available dataset exists to support the development and evaluation of such systems . we therefore propose the kit motion-language dataset , which is large , open , and extensible . we aggregate data from multiple motion capture databases and include them in our dataset using a unified representation that is independent of the capture system or marker set , making it easy to work with the data regardless of its origin . to obtain motion annotations in natural language , we apply a crowd-sourcing approach and a web-based tool that was specifically build for this purpose , the motion annotation tool . we thoroughly document the annotation process itself and discuss gamification methods that we used to keep annotators motivated . we further propose a novel method , perplexity-based selection , which systematically selects motions for further annotation that are either under-represented in our dataset or that have erroneous annotations . we show that our method mitigates the two aforementioned problems and ensures a systematic annotation process . we provide an in-depth analysis of the structure and contents of our resulting dataset , which , as of june 14 , 2016 , contains 3917 motions with a total duration of 11.26 hours and 5486 annotations in natural language that contain 45779 words . we believe that this makes our dataset an excellent choice that enables more transparent and comparable research in this important area .", "topics": ["natural language", "database"]}
{"title": "orthogonal parallel mcmc methods for sampling and optimization", "abstract": "monte carlo ( mc ) methods are widely used for bayesian inference and optimization in statistics , signal processing and machine learning . a well-known class of mc methods are markov chain monte carlo ( mcmc ) algorithms . in order to foster better exploration of the state space , specially in high-dimensional applications , several schemes employing multiple parallel mcmc chains have been recently introduced . in this work , we describe a novel parallel interacting mcmc scheme , called { \\it orthogonal mcmc } ( o-mcmc ) , where a set of `` vertical '' parallel mcmc chains share information using some `` horizontal '' mcmc techniques working on the entire population of current states . more specifically , the vertical chains are led by random-walk proposals , whereas the horizontal mcmc techniques employ independent proposals , thus allowing an efficient combination of global exploration and local approximation . the interaction is contained in these horizontal iterations . within the analysis of different implementations of o-mcmc , novel schemes in order to reduce the overall computational cost of parallel multiple try metropolis ( mtm ) chains are also presented . furthermore , a modified version of o-mcmc for optimization is provided by considering parallel simulated annealing ( sa ) algorithms . numerical results show the advantages of the proposed sampling scheme in terms of efficiency in the estimation , as well as robustness in terms of independence with respect to initial values and the choice of the parameters .", "topics": ["sampling ( signal processing )", "simulation"]}
{"title": "deep learning reconstruction for 9-view dual energy ct baggage scanner", "abstract": "for homeland and transportation security applications , 2d x-ray explosive detection system ( eds ) have been widely used , but they have limitations in recognizing 3d shape of the hidden objects . among various types of 3d computed tomography ( ct ) systems to address this issue , this paper is interested in a stationary ct using fixed x-ray sources and detectors . however , due to the limited number of projection views , analytic reconstruction algorithms produce severe streaking artifacts . inspired by recent success of deep learning approach for sparse view ct reconstruction , here we propose a novel image and sinogram domain deep learning architecture for 3d reconstruction from very sparse view measurement . the algorithm has been tested with the real data from a prototype 9-view dual energy stationary ct eds carry-on baggage scanner developed by gemss medical systems , korea , which confirms the superior reconstruction performance over the existing approaches .", "topics": ["sparse matrix"]}
{"title": "learning activation functions to improve deep neural networks", "abstract": "artificial neural networks typically have a fixed , non-linear activation function at each neuron . we have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent . with this adaptive activation function , we are able to improve upon deep neural network architectures composed of static rectified linear units , achieving state-of-the-art performance on cifar-10 ( 7.51 % ) , cifar-100 ( 30.83 % ) , and a benchmark from high-energy physics involving higgs boson decay modes .", "topics": ["nonlinear system", "neural networks"]}
{"title": "diverse embedding neural network language models", "abstract": "we propose diverse embedding neural network ( denn ) , a novel architecture for language models ( lms ) . a dennlm projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network lms . we encourage these sub-spaces to be diverse during network training through an augmented loss function . our language modeling experiments on the penn treebank data set show the performance benefit of using a dennlm .", "topics": ["loss function"]}
{"title": "leveraging distributional semantics for multi-label learning", "abstract": "we present a novel and scalable label embedding framework for large-scale multi-label learning a.k.a exmlds ( extreme multi-label learning using distributional semantics ) . our approach draws inspiration from ideas rooted in distributional semantics , specifically the skip gram negative sampling ( sgns ) approach , widely used to learn word embeddings for natural language processing tasks . learning such embeddings can be reduced to a certain matrix factorization . our approach is novel in that it highlights interesting connections between label embedding methods used for multi-label learning and paragraph/document embedding methods commonly used for learning representations of text data . the framework can also be easily extended to incorporate auxiliary information such as label-label correlations ; this is crucial especially when there are a lot of missing labels in the training data . we demonstrate the effectiveness of our approach through an extensive set of experiments on a variety of benchmark datasets , and show that the proposed learning methods perform favorably compared to several baselines and state-of-the-art methods for large-scale multi-label learning . to facilitate end-to-end learning , we develop a joint learning algorithm that can learn the embeddings as well as a regression model that predicts these embeddings given input features , via efficient gradient-based methods .", "topics": ["test set", "natural language processing"]}
{"title": "robust subspace clustering via tighter rank approximation", "abstract": "matrix rank minimization problem is in general np-hard . the nuclear norm is used to substitute the rank function in many recent studies . nevertheless , the nuclear norm approximation adds all singular values together and the approximation error may depend heavily on the magnitudes of singular values . this might restrict its capability in dealing with many practical problems . in this paper , an arctangent function is used as a tighter approximation to the rank function . we use it on the challenging subspace clustering problem . for this nonconvex minimization problem , we develop an effective optimization procedure based on a type of augmented lagrange multipliers ( alm ) method . extensive experiments on face clustering and motion segmentation show that the proposed method is effective for rank approximation .", "topics": ["cluster analysis", "value ( ethics )"]}
{"title": "knowledge distillation using unlabeled mismatched images", "abstract": "current approaches for knowledge distillation ( kd ) either directly use training data or sample from the training data distribution . in this paper , we demonstrate effectiveness of 'mismatched ' unlabeled stimulus to perform kd for image classification networks . for illustration , we consider scenarios where this is a complete absence of training data , or mismatched stimulus has to be used for augmenting a small amount of training data . we demonstrate that stimulus complexity is a key factor for distillation 's good performance . our examples include use of various datasets for stimulating mnist and cifar teachers .", "topics": ["test set", "computer vision"]}
{"title": "causal inference on discrete data via estimating distance correlations", "abstract": "in this paper , we deal with the problem of inferring causal directions when the data is on discrete domain . by considering the distribution of the cause $ p ( x ) $ and the conditional distribution mapping cause to effect $ p ( y|x ) $ as independent random variables , we propose to infer the causal direction via comparing the distance correlation between $ p ( x ) $ and $ p ( y|x ) $ with the distance correlation between $ p ( y ) $ and $ p ( x|y ) $ . we infer `` $ x $ causes $ y $ '' if the dependence coefficient between $ p ( x ) $ and $ p ( y|x ) $ is smaller . experiments are performed to show the performance of the proposed method .", "topics": ["causality", "coefficient"]}
{"title": "deep captioning with multimodal recurrent neural networks ( m-rnn )", "abstract": "in this paper , we present a multimodal recurrent neural network ( m-rnn ) model for generating novel image captions . it directly models the probability distribution of generating a word given previous words and an image . image captions are generated by sampling from this distribution . the model consists of two sub-networks : a deep recurrent neural network for sentences and a deep convolutional network for images . these two sub-networks interact with each other in a multimodal layer to form the whole m-rnn model . the effectiveness of our model is validated on four benchmark datasets ( iapr tc-12 , flickr 8k , flickr 30k and ms coco ) . our model outperforms the state-of-the-art methods . in addition , we apply the m-rnn model to retrieval tasks for retrieving images or sentences , and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval . the project page of this work is : www.stat.ucla.edu/~junhua.mao/m-rnn.html .", "topics": ["recurrent neural network", "optimization problem"]}
{"title": "neural programmer : inducing latent programs with gradient descent", "abstract": "deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition , speech recognition , and sequence to sequence learning . however , this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning . a major limitation of these models is in their inability to learn even simple arithmetic and logic operations . for example , it has been shown that neural networks fail to learn to add two binary numbers reliably . in this work , we propose neural programmer , an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations . neural programmer can call these augmented operations over several steps , thereby inducing compositional programs that are more complex than the built-in operations . the model learns from a weak supervision signal which is the result of execution of the correct program , hence it does not require expensive annotation of the correct program itself . the decisions of what operations to call , and what data segments to apply to are inferred by neural programmer . such decisions , during training , are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent . we find that training the model is difficult , but it can be greatly improved by adding random noise to the gradient . on a fairly complex synthetic table-comprehension dataset , traditional recurrent networks and attentional models perform poorly while neural programmer typically obtains nearly perfect accuracy .", "topics": ["supervised learning", "synthetic data"]}
{"title": "learning inductive biases with simple neural networks", "abstract": "people use rich prior knowledge about the world in order to efficiently learn new concepts . these priors - also known as `` inductive biases '' - pertain to the space of internal models considered by a learner , and they help the learner make inferences that go beyond the observed data . a recent study found that deep neural networks optimized for object recognition develop the shape bias ( ritter et al . , 2017 ) , an inductive bias possessed by children that plays an important role in early word learning . however , these networks use unrealistically large quantities of training data , and the conditions required for these biases to develop are not well understood . moreover , it is unclear how the learning dynamics of these networks relate to developmental processes in childhood . we investigate the development and influence of the shape bias in neural networks using controlled datasets of abstract patterns and synthetic images , allowing us to systematically vary the quantity and form of the experience provided to the learning algorithms . we find that simple neural networks develop a shape bias after seeing as few as 3 examples of 4 object categories . the development of these biases predicts the onset of vocabulary acceleration in our networks , consistent with the developmental process in children .", "topics": ["test set", "neural networks"]}
{"title": "resolving language and vision ambiguities together : joint segmentation & prepositional attachment resolution in captioned scenes", "abstract": "we present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images . some ambiguities in language can not be resolved without simultaneously reasoning about an associated image . if we consider the sentence `` i shot an elephant in my pajamas '' , looking at language alone ( and not using common sense ) , it is unclear if it is the person or the elephant wearing the pajamas or both . our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair . we show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths , and that joint reasoning produces more accurate results than any module operating in isolation . multiple hypotheses are also shown to be crucial to improved multiple-module reasoning . our vision and language approach significantly outperforms the stanford parser ( de marneffe et al . , 2006 ) by 17.91 % ( 28.69 % relative ) and 12.83 % ( 25.28 % relative ) in two different experiments . we also make small improvements over deeplab-crf ( chen et al . , 2015 ) .", "topics": ["natural language processing"]}
{"title": "online and batch supervised background estimation via l1 regression", "abstract": "we propose a surprisingly simple model for supervised video background estimation . our model is based on $ \\ell_1 $ regression . as existing methods for $ \\ell_1 $ regression do not scale to high-resolution videos , we propose several simple and scalable methods for solving the problem , including iteratively reweighted least squares , a homotopy method , and stochastic gradient descent . we show through extensive experiments that our model and methods match or outperform the state-of-the-art online and batch methods in virtually all quantitative and qualitative measures .", "topics": ["gradient descent", "gradient"]}
{"title": "score function features for discriminative learning", "abstract": "feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech , computer vision and natural language processing . in this paper , we consider a novel class of matrix and tensor-valued features , which can be pre-trained using unlabeled samples . we present efficient algorithms for extracting discriminative information , given these pre-trained features and labeled samples for any related task . our class of features are based on higher-order score functions , which capture local variations in the probability density function of the input . we establish a theoretical framework to characterize the nature of discriminative information that can be extracted from score-function features , when used in conjunction with labeled samples . we employ efficient spectral decomposition algorithms ( on matrices and tensors ) for extracting discriminative components . the advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of an overcomplete representations . thus , we present a novel framework for employing generative models of the input for discriminative learning .", "topics": ["feature learning", "generative model"]}
{"title": "semi-supervised classification for natural language processing", "abstract": "semi-supervised classification is an interesting idea where classification models are learned from both labeled and unlabeled data . it has several advantages over supervised classification in natural language processing domain . for instance , supervised classification exploits only labeled data that are expensive , often difficult to get , inadequate in quantity , and require human experts for annotation . on the other hand , unlabeled data are inexpensive and abundant . despite the fact that many factors limit the wide-spread use of semi-supervised classification , it has become popular since its level of performance is empirically as good as supervised classification . this study explores the possibilities and achievements as well as complexity and limitations of semi-supervised classification for several natural langue processing tasks like parsing , biomedical information processing , text classification , and summarization .", "topics": ["supervised learning", "natural language processing"]}
{"title": "credal networks under epistemic irrelevance", "abstract": "a credal network under epistemic irrelevance is a generalised type of bayesian network that relaxes its two main building blocks . on the one hand , the local probabilities are allowed to be partially specified . on the other hand , the assessments of independence do not have to hold exactly . conceptually , these two features turn credal networks under epistemic irrelevance into a powerful alternative to bayesian networks , offering a more flexible approach to graph-based multivariate uncertainty modelling . however , in practice , they have long been perceived as very hard to work with , both theoretically and computationally . the aim of this paper is to demonstrate that this perception is no longer justified . we provide a general introduction to credal networks under epistemic irrelevance , give an overview of the state of the art , and present several new theoretical results . most importantly , we explain how these results can be combined to allow for the design of recursive inference methods . we provide numerous concrete examples of how this can be achieved , and use these to demonstrate that computing with credal networks under epistemic irrelevance is most definitely feasible , and in some cases even highly efficient . we also discuss several philosophical aspects , including the lack of symmetry , how to deal with probability zero , the interpretation of lower expectations , the axiomatic status of graphoid properties , and the difference between updating and conditioning .", "topics": ["bayesian network", "relevance"]}
{"title": "low latency anomaly detection and bayesian network prediction of anomaly likelihood", "abstract": "we develop a supervised machine learning model that detects anomalies in systems in real time . our model processes unbounded streams of data into time series which then form the basis of a low-latency anomaly detection model . moreover , we extend our preliminary goal of just anomaly detection to simultaneous anomaly prediction . we approach this very challenging problem by developing a bayesian network framework that captures the information about the parameters of the lagged regressors calibrated in the first part of our approach and use this structure to learn local conditional probability distributions .", "topics": ["time series", "supervised learning"]}
{"title": "direct feedback alignment provides learning in deep neural networks", "abstract": "artificial neural networks are most commonly trained with the back-propagation algorithm , where the gradient for learning is provided by back-propagating the error , layer by layer , from the output layer to the hidden layers . a recently discovered method called feedback-alignment shows that the weights used for propagating the error backward do n't have to be symmetric with the weights used for propagation the activation forward . in fact , random feedback weights work evenly well , because the network learns how to make the feedback useful . in this work , the feedback alignment principle is used for training hidden layers more independently from the rest of the network , and from a zero initial condition . the error is propagated through fixed random feedback connections directly from the output layer to each hidden layer . this simple method is able to achieve zero training error even in convolutional networks and very deep networks , completely without error back-propagation . the method is a step towards biologically plausible machine learning because the error signal is almost local , and no symmetric or reciprocal weights are required . experiments show that the test performance on mnist and cifar is almost as good as those obtained with back-propagation for fully connected networks . if combined with dropout , the method achieves 1.45 % error on the permutation invariant mnist task .", "topics": ["neural networks", "gradient"]}
{"title": "the potential benefits of filtering versus hyper-parameter optimization", "abstract": "the quality of an induced model by a learning algorithm is dependent on the quality of the training data and the hyper-parameters supplied to the learning algorithm . prior work has shown that improving the quality of the training data ( i.e . , by removing low quality instances ) or tuning the learning algorithm hyper-parameters can significantly improve the quality of an induced model . a comparison of the two methods is lacking though . in this paper , we estimate and compare the potential benefits of filtering and hyper-parameter optimization . estimating the potential benefit gives an overly optimistic estimate but also empirically demonstrates an approximation of the maximum potential benefit of each method . we find that , while both significantly improve the induced model , improving the quality of the training set has a greater potential effect than hyper-parameter optimization .", "topics": ["test set"]}
{"title": "graph-structured representations for visual question answering", "abstract": "this paper proposes to improve visual question answering ( vqa ) with structured representations of both scene contents and questions . a key challenge in vqa is to require joint reasoning over the visual and text domains . the predominant cnn/lstm-based approach to vqa is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question . cnn feature vectors can not effectively capture situations as simple as multiple object instances , and lstms process questions as series of words , which does not reflect the true complexity of language structure . we instead propose to build graphs over the scene objects and over the question words , and we describe a deep neural network that exploits the structure in these representations . this shows significant benefit over the sequential processing of lstms . the overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art , from 71.2 % to 74.4 % in accuracy on the `` abstract scenes '' multiple-choice benchmark , and from 34.7 % to 39.1 % in accuracy over pairs of `` balanced '' scenes , i.e . images with fine-grained differences and opposite yes/no answers to a same question .", "topics": ["feature vector"]}
{"title": "dual density operators and natural language meaning", "abstract": "density operators allow for representing ambiguity about a vector representation , both in quantum theory and in distributional natural language meaning . formally equivalently , they allow for discarding part of the description of a composite system , where we consider the discarded part to be the context . we introduce dual density operators , which allow for two independent notions of context . we demonstrate the use of dual density operators within a grammatical-compositional distributional framework for natural language meaning . we show that dual density operators can be used to simultaneously represent : ( i ) ambiguity about word meanings ( e.g . queen as a person vs. queen as a band ) , and ( ii ) lexical entailment ( e.g . tiger - > mammal ) . we provide a proof-of-concept example .", "topics": ["natural language"]}
{"title": "logic tensor networks : deep learning and logical reasoning from data and knowledge", "abstract": "we propose logic tensor networks : a uniform framework for integrating automatic learning and reasoning . a logic formalism called real logic is defined on a first-order language whereby formulas have truth-value in the interval [ 0,1 ] and semantics defined concretely on the domain of real numbers . logical constants are interpreted as feature vectors of real numbers . real logic promotes a well-founded integration of deductive reasoning on a knowledge-base and efficient data-driven relational machine learning . we show how real logic can be implemented in deep tensor neural networks with the use of google 's tensorflow primitives . the paper concludes with experiments applying logic tensor networks on a simple but representative example of knowledge completion .", "topics": ["feature vector"]}
{"title": "neural tuning size is a key factor underlying holistic face processing", "abstract": "faces are a class of visual stimuli with unique significance , for a variety of reasons . they are ubiquitous throughout the course of a person 's life , and face recognition is crucial for daily social interaction . faces are also unlike any other stimulus class in terms of certain physical stimulus characteristics . furthermore , faces have been empirically found to elicit certain characteristic behavioral phenomena , which are widely held to be evidence of `` holistic '' processing of faces . however , little is known about the neural mechanisms underlying such holistic face processing . in other words , for the processing of faces by the primate visual system , the input and output characteristics are relatively well known , but the internal neural computations are not . the main aim of this work is to further the fundamental understanding of what causes the visual processing of faces to be different from that of objects . in this computational modeling work , we show that a single factor - `` neural tuning size '' - is able to account for three key phenomena that are characteristic of face processing , namely the composite face effect ( cfe ) , face inversion effect ( fie ) and whole-part effect ( wpe ) . our computational proof-of-principle provides specific neural tuning properties that correspond to the poorly-understood notion of holistic face processing , and connects these neural properties to psychophysical behavior . overall , our work provides a unified and parsimonious theoretical account for the disparate empirical data on face-specific processing , deepening the fundamental understanding of face processing .", "topics": ["computation"]}
{"title": "privacy preserving id3 over horizontally , vertically and grid partitioned data", "abstract": "we consider privacy preserving decision tree induction via id3 in the case where the training data is horizontally or vertically distributed . furthermore , we consider the same problem in the case where the data is both horizontally and vertically distributed , a situation we refer to as grid partitioned data . we give an algorithm for privacy preserving id3 over horizontally partitioned data involving more than two parties . for grid partitioned data , we discuss two different evaluation methods for preserving privacy id3 , namely , first merging horizontally and developing vertically or first merging vertically and next developing horizontally . next to introducing privacy preserving data mining over grid-partitioned data , the main contribution of this paper is that we show , by means of a complexity analysis that the former evaluation method is the more efficient .", "topics": ["test set", "data mining"]}
{"title": "professor forcing : a new algorithm for training recurrent networks", "abstract": "the teacher forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network 's own one-step-ahead predictions to do multi-step sampling . we introduce the professor forcing algorithm , which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps . we apply professor forcing to language modeling , vocal synthesis on raw waveforms , handwriting generation , and image generation . empirically we find that professor forcing acts as a regularizer , improving test likelihood on character level penn treebank and sequential mnist . we also find that the model qualitatively improves samples , especially when sampling for a large number of time steps . this is supported by human evaluation of sample quality . trade-offs between professor forcing and scheduled sampling are discussed . we produce t-snes showing that professor forcing successfully makes the dynamics of the network during training and sampling more similar .", "topics": ["sampling ( signal processing )", "recurrent neural network"]}
{"title": "recognizing implicit discourse relations via repeated reading : neural networks with multi-level attention", "abstract": "recognizing implicit discourse relations is a challenging but important task in the field of natural language processing . for such a complex text processing task , different from previous studies , we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations . to mimic the repeated reading strategy , we propose the neural networks with multi-level attention ( nnma ) , combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations . experiments on the pdtb dataset show that our proposed method achieves the state-of-art results . the visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words .", "topics": ["natural language processing", "neural networks"]}
{"title": "the complexity of campaigning", "abstract": "in `` the logic of campaigning '' , dean and parikh consider a candidate making campaign statements to appeal to the voters . they model these statements as boolean formulas over variables that represent stances on the issues , and study optimal candidate strategies under three proposed models of voter preferences based on the assignments that satisfy these formulas . we prove that voter utility evaluation is computationally hard under these preference models ( in one case , # p-hard ) , along with certain problems related to candidate strategic reasoning . our results raise questions about the desirable characteristics of a voter preference model and to what extent a polynomial-time-evaluable function can capture them .", "topics": ["time complexity"]}
{"title": "inference in graphical models via semidefinite programming hierarchies", "abstract": "maximum a posteriori probability ( map ) inference in graphical models amounts to solving a graph-structured combinatorial optimization problem . popular inference algorithms such as belief propagation ( bp ) and generalized belief propagation ( gbp ) are intimately related to linear programming ( lp ) relaxation within the sherali-adams hierarchy . despite the popularity of these algorithms , it is well understood that the sum-of-squares ( sos ) hierarchy based on semidefinite programming ( sdp ) can provide superior guarantees . unfortunately , sos relaxations for a graph with $ n $ vertices require solving an sdp with $ n^ { \\theta ( d ) } $ variables where $ d $ is the degree in the hierarchy . in practice , for $ d\\ge 4 $ , this approach does not scale beyond a few tens of variables . in this paper , we propose binary sdp relaxations for map inference using the sos hierarchy with two innovations focused on computational efficiency . firstly , in analogy to bp and its variants , we only introduce decision variables corresponding to contiguous regions in the graphical model . secondly , we solve the resulting sdp using a non-convex burer-monteiro style method , and develop a sequential rounding procedure . we demonstrate that the resulting algorithm can solve problems with tens of thousands of variables within minutes , and outperforms bp and gbp on practical problems such as image denoising and ising spin glasses . finally , for specific graph types , we establish a sufficient condition for the tightness of the proposed partial sos relaxation .", "topics": ["graphical model", "optimization problem"]}
{"title": "an empirical evaluation of generic convolutional and recurrent networks for sequence modeling", "abstract": "for most deep learning practitioners , sequence modeling is synonymous with recurrent networks . yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation . given a new sequence modeling task or dataset , which architecture should one use ? we conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling . the models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks . our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as lstms across a diverse range of tasks and datasets , while demonstrating longer effective memory . we conclude that the common association between sequence modeling and recurrent networks should be reconsidered , and convolutional networks should be regarded as a natural starting point for sequence modeling tasks .", "topics": ["recurrent neural network", "machine translation"]}
{"title": "word sense disambiguation based on mutual information and syntactic patterns", "abstract": "this paper describes a hybrid system for wsd , presented to the english all-words and lexical-sample tasks , that relies on two different unsupervised approaches . the first one selects the senses according to mutual information proximity between a context word a variant of the sense . the second heuristic analyzes the examples of use in the glosses of the senses so that simple syntactic patterns are inferred . this patterns are matched against the disambiguation contexts . we show that the first heuristic obtains a precision and recall of .58 and .35 respectively in the all words task while the second obtains .80 and .25 . the high precision obtained recommends deeper research of the techniques . results for the lexical sample task are also provided .", "topics": ["heuristic"]}
{"title": "learning to factor policies and action-value functions : factored action space representations for deep reinforcement learning", "abstract": "deep reinforcement learning ( drl ) methods have performed well in an increasing numbering of high-dimensional visual decision making domains . among all such visual decision making problems , those with discrete action spaces often tend to have underlying compositional structure in the said action space . such action spaces often contain actions such as go left , go up as well as go diagonally up and left ( which is a composition of the former two actions ) . the representations of control policies in such domains have traditionally been modeled without exploiting this inherent compositional structure in the action spaces . we propose a new learning paradigm , factored action space representations ( far ) wherein we decompose a control policy learned using a deep reinforcement learning algorithm into independent components , analogous to decomposing a vector in terms of some orthogonal basis vectors . this architectural modification of the control policy representation allows the agent to learn about multiple actions simultaneously , while executing only one of them . we demonstrate that far yields considerable improvements on top of two drl algorithms in atari 2600 : fara3c outperforms a3c ( asynchronous advantage actor critic ) in 9 out of 14 tasks and faraql outperforms aql ( asynchronous n-step q-learning ) in 9 out of 13 tasks .", "topics": ["reinforcement learning"]}
{"title": "acceleration of the shiftable o ( 1 ) algorithm for bilateral filtering and non-local means", "abstract": "a direct implementation of the bilateral filter [ 1 ] requires o ( \\sigma_s^2 ) operations per pixel , where \\sigma_s is the ( effective ) width of the spatial kernel . a fast implementation of the bilateral filter was recently proposed in [ 2 ] that required o ( 1 ) operations per pixel with respect to \\sigma_s . this was done by using trigonometric functions for the range kernel of the bilateral filter , and by exploiting their so-called shiftability property . in particular , a fast implementation of the gaussian bilateral filter was realized by approximating the gaussian range kernel using raised cosines . later , it was demonstrated in [ 3 ] that this idea could be extended to a larger class of filters , including the popular non-local means filter [ 4 ] . as already observed in [ 2 ] , a flip side of this approach was that the run time depended on the width \\sigma_r of the range kernel . for an image with ( local ) intensity variations in the range [ 0 , t ] , the run time scaled as o ( t^2/\\sigma^2_r ) with \\sigma_r . this made it difficult to implement narrow range kernels , particularly for images with large dynamic range . we discuss this problem in this note , and propose some simple steps to accelerate the implementation in general , and for small \\sigma_r in particular . [ 1 ] c. tomasi and r. manduchi , `` bilateral filtering for gray and color images '' , proc . ieee international conference on computer vision , 1998 . [ 2 ] k.n . chaudhury , daniel sage , and m. unser , `` fast o ( 1 ) bilateral filtering using trigonometric range kernels '' , ieee transactions on image processing , 2011 . [ 3 ] k.n . chaudhury , `` constant-time filtering using shiftable kernels '' , ieee signal processing letters , 2011 . [ 4 ] a. buades , b. coll , and j.m . morel , `` a review of image denoising algorithms , with a new one '' , multiscale modeling and simulation , 2005 .", "topics": ["kernel ( operating system )", "approximation algorithm"]}
{"title": "sparse principal component analysis via rotation and truncation", "abstract": "sparse principal component analysis ( sparse pca ) aims at finding a sparse basis to improve the interpretability over the dense basis of pca , meanwhile the sparse basis should cover the data subspace as much as possible . in contrast to most of existing work which deal with the problem by adding some sparsity penalties on various objectives of pca , in this paper , we propose a new method spcart , whose motivation is to find a rotation matrix and a sparse basis such that the sparse basis approximates the basis of pca after the rotation . the algorithm of spcart consists of three alternating steps : rotate pca basis , truncate small entries , and update the rotation matrix . its performance bounds are also given . spcart is efficient , with each iteration scaling linearly with the data dimension . it is easy to choose parameters in spcart , due to its explicit physical explanations . besides , we give a unified view to several existing sparse pca methods and discuss the connection with spcart . some ideas in spcart are extended to gpower , a popular sparse pca algorithm , to overcome its drawback . experimental results demonstrate that spcart achieves the state-of-the-art performance . it also achieves a good tradeoff among various criteria , including sparsity , explained variance , orthogonality , balance of sparsity among loadings , and computational speed .", "topics": ["sparse matrix", "iteration"]}
{"title": "bag-of-words method applied to accelerometer measurements for the purpose of classification and energy estimation", "abstract": "accelerometer measurements are the prime type of sensor information most think of when seeking to measure physical activity . on the market , there are many fitness measuring devices which aim to track calories burned and steps counted through the use of accelerometers . these measurements , though good enough for the average consumer , are noisy and unreliable in terms of the precision of measurement needed in a scientific setting . the contribution of this paper is an innovative and highly accurate regression method which uses an intermediary two-stage classification step to better direct the regression of energy expenditure values from accelerometer counts . we show that through an additional unsupervised layer of intermediate feature construction , we can leverage latent patterns within accelerometer counts to provide better grounds for activity classification than expert-constructed timeseries features . for this , our approach utilizes a mathematical model originating in natural language processing , the bag-of-words model , that has in the past years been appearing in diverse disciplines outside of the natural language processing field such as image processing . further emphasizing the natural language connection to stochastics , we use a gaussian mixture model to learn the dictionary upon which the bag-of-words model is built . moreover , we show that with the addition of these features , we 're able to improve regression root mean-squared error of energy expenditure by approximately 1.4 units over existing state-of-the-art methods .", "topics": ["image processing", "natural language processing"]}
{"title": "rotting bandits", "abstract": "the multi-armed bandits ( mab ) framework highlights the tension between acquiring new knowledge ( exploration ) and leveraging available knowledge ( exploitation ) . in the classical mab problem , a decision maker must choose an arm at each time step , upon which she receives a reward . the decision maker 's objective is to maximize her cumulative expected reward over the time horizon . the mab problem has been studied extensively , specifically under the assumption of the arms ' rewards distributions being stationary , or quasi-stationary , over time . we consider a variant of the mab framework , which we termed rotting bandits , where each arm 's expected reward decays as a function of the number of times it has been pulled . we are motivated by many real-world scenarios such as online advertising , content recommendation , crowdsourcing , and more . we present algorithms , accompanied by simulations , and derive theoretical guarantees .", "topics": ["simulation"]}
{"title": "semi-supervised learning of deep metrics for stereo reconstruction", "abstract": "deep-learning metrics have recently demonstrated extremely good performance to match image patches for stereo reconstruction . however , training such metrics requires large amount of labeled stereo images , which can be difficult or costly to collect for certain applications . the main contribution of our work is a new semi-supervised method for learning deep metrics from unlabeled stereo images , given coarse information about the scenes and the optical system . our method alternatively optimizes the metric with a standard stochastic gradient descent , and applies stereo constraints to regularize its prediction . experiments on reference data-sets show that , for a given network architecture , training with this new method without ground-truth produces a metric with performance as good as state-of-the-art baselines trained with the said ground-truth . this work has three practical implications . firstly , it helps to overcome limitations of training sets , in particular noisy ground truth . secondly it allows to use much more training data during learning . thirdly , it allows to tune deep metric for a particular stereo system , even if ground truth is not available .", "topics": ["baseline ( configuration management )", "test set"]}
{"title": "verbalizing ontologies in controlled baltic languages", "abstract": "controlled natural languages ( mostly english-based ) recently have emerged as seemingly informal supplementary means for owl ontology authoring , if compared to the formal notations that are used by professional knowledge engineers . in this paper we present by examples controlled latvian language that has been designed to be compliant with the state of the art attempto controlled english . we also discuss relation with controlled lithuanian language that is being designed in parallel .", "topics": ["natural language"]}
{"title": "balotage in argentina 2015 , a sentiment analysis of tweets", "abstract": "twitter social network contains a large amount of information generated by its users . that information is composed of opinions and comments that may reflect trends in social behavior . there is talk of trend when it is possible to identify opinions and comments geared towards the same shared by a lot of people direction . to determine if two or more written opinions share the same address , techniques natural language processing ( nlp ) are used . this paper proposes a methodology for predicting reflected in twitter from the use of sentiment analysis functions nlp based on social behaviors . the case study was selected the 2015 presidential in argentina , and a software architecture big data composed vertica data base with the component called pulse was used . through the analysis it was possible to detect trends in voting intentions with regard to the presidential candidates , achieving greater accuracy in predicting that achieved with traditional systems surveys .", "topics": ["natural language processing", "natural language"]}
{"title": "deep structured energy based models for anomaly detection", "abstract": "in this paper , we attack the anomaly detection problem by directly modeling the data distribution with deep architectures . we propose deep structured energy based models ( dsebms ) , where the energy function is the output of a deterministic deep neural network with structure . we develop novel model architectures to integrate ebms with different types of data such as static data , sequential data , and spatial data , and apply appropriate model architectures to adapt to the data structure . our training algorithm is built upon the recent development of score matching \\cite { sm } , which connects an ebm with a regularized autoencoder , eliminating the need for complicated sampling method . statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution . we investigate two decision criteria for performing anomaly detection : the energy score and the reconstruction error . extensive empirical studies on benchmark tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods .", "topics": ["sampling ( signal processing )", "mathematical optimization"]}
{"title": "3d seismic data denoising using two-dimensional sparse coding scheme", "abstract": "seismic data denoising is vital to geophysical applications and the transform-based function method is one of the most widely used techniques . however , it is challenging to design a suit- able sparse representation to express a transform-based func- tion group due to the complexity of seismic data . in this paper , we apply a seismic data denoising method based on learning- type overcomplete dictionaries which uses two-dimensional sparse coding ( 2dsc ) . first , we model the input seismic data and dictionaries as third-order tensors and introduce tensor- linear combinations for data approximation . second , we ap- ply learning-type overcomplete dictionary , i.e . , optimal sparse data representation is achieved through learning and training . third , we exploit the alternating minimization algorithm to solve the optimization problem of seismic denoising . finally we evaluate its denoising performance on synthetic seismic data and land data survey . experiment results show that the two-dimensional sparse coding scheme reduces computational costs and enhances the signal-to-noise ratio .", "topics": ["noise reduction", "sparse matrix"]}
{"title": "compression of neural machine translation models via pruning", "abstract": "neural machine translation ( nmt ) , like many other deep learning domains , typically suffers from over-parameterization , resulting in large storage sizes . this paper examines three simple magnitude-based pruning schemes to compress nmt models , namely class-blind , class-uniform , and class-distribution , which differ in terms of how pruning thresholds are computed for the different classes of weights in the nmt architecture . we demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art nmt system . we show that an nmt model with over 200 million parameters can be pruned by 40 % with very little performance loss as measured on the wmt'14 english-german translation task . this sheds light on the distribution of redundancy in the nmt architecture . our main result is that with retraining , we can recover and even surpass the original performance with an 80 % -pruned model .", "topics": ["machine translation"]}
{"title": "deepest neural networks", "abstract": "this paper shows that a long chain of perceptrons ( that is , a multilayer perceptron , or mlp , with many hidden layers of width one ) can be a universal classifier . the classification procedure is not necessarily computationally efficient , but the technique throws some light on the kind of computations possible with narrow and deep mlps .", "topics": ["computational complexity theory", "neural networks"]}
{"title": "behavior and path planning for the coalition of cognitive robots in smart relocation tasks", "abstract": "in this paper we outline the approach of solving special type of navigation tasks for robotic systems , when a coalition of robots ( agents ) acts in the 2d environment , which can be modified by the actions , and share the same goal location . the latter is originally unreachable for some members of the coalition , but the common task still can be accomplished as the agents can assist each other ( e.g . by modifying the environment ) . we call such tasks smart relocation tasks ( as the can not be solved by pure path planning methods ) and study spatial and behavior interaction of robots while solving them . we use cognitive approach and introduce semiotic knowledge representation - sign world model which underlines behavioral planning methodology . planning is viewed as a recursive search process in the hierarchical state-space induced by sings with path planning signs reside on the lowest level . reaching this level triggers path planning which is accomplished by state of the art grid-based planners focused on producing smooth paths ( e.g . lian ) and thus indirectly guarantying feasibility of that paths against agent 's dynamic constraints .", "topics": ["robot"]}
{"title": "multidefender security games", "abstract": "stackelberg security game models and associated computational tools have seen deployment in a number of high-consequence security settings , such as lax canine patrols and federal air marshal service . these models focus on isolated systems with only one defender , despite being part of a more complex system with multiple players . furthermore , many real systems such as transportation networks and the power grid exhibit interdependencies between targets and , consequently , between decision makers jointly charged with protecting them . to understand such multidefender strategic interactions present in security , we investigate game theoretic models of security games with multiple defenders . unlike most prior analysis , we focus on the situations in which each defender must protect multiple targets , so that even a single defender 's best response decision is , in general , highly non-trivial . we start with an analytical investigation of multidefender security games with independent targets , offering an equilibrium and price-of-anarchy analysis of three models with increasing generality . in all models , we find that defenders have the incentive to over-protect targets , at times significantly . additionally , in the simpler models , we find that the price of anarchy is unbounded , linearly increasing both in the number of defenders and the number of targets per defender . considering interdependencies among targets , we develop a novel mixed-integer linear programming formulation to compute a defender 's best response , and make use of this formulation in approximating nash equilibria of the game . we apply this approach towards computational strategic analysis of several models of networks representing interdependencies , including real-world power networks . our analysis shows how network structure and the probability of failure spread determine the propensity of defenders to over- or under-invest in security .", "topics": ["interaction"]}
{"title": "a generative word embedding model and its low rank positive semidefinite solution", "abstract": "most existing word embedding methods can be categorized into neural embedding models and matrix factorization ( mf ) -based methods . however some models are opaque to probabilistic interpretation , and mf-based methods , typically solved using singular value decomposition ( svd ) , may incur loss of corpus information . in addition , it is desirable to incorporate global latent factors , such as topics , sentiments or writing styles , into the word embedding model . since generative models provide a principled way to incorporate latent factors , we propose a generative word embedding model , which is easy to interpret , and can serve as a basis of more sophisticated latent factor models . the model inference reduces to a low rank weighted positive semidefinite approximation problem . its optimization is approached by eigendecomposition on a submatrix , followed by online blockwise regression , which is scalable and avoids the information loss in svd . in experiments on 7 common benchmark datasets , our vectors are competitive to word2vec , and better than other mf-based methods .", "topics": ["generative model", "scalability"]}
{"title": "ladar-based mover detection from moving vehicles", "abstract": "detecting moving vehicles and people is crucial for safe operation of ugvs but is challenging in cluttered , real world environments . we propose a registration technique that enables objects to be robustly matched and tracked , and hence movers to be detected even in high clutter . range data are acquired using a 2d scanning ladar from a moving platform . these are automatically clustered into objects and modeled using a surface density function . a bhattacharya similarity is optimized to register subsequent views of each object enabling good discrimination and tracking , and hence mover detection .", "topics": ["cluster analysis"]}
{"title": "extend natural neighbor : a novel classification method with self-adaptive neighborhood parameters in different stages", "abstract": "various kinds of k-nearest neighbor ( knn ) based classification methods are the bases of many well-established and high-performance pattern-recognition techniques , but both of them are vulnerable to their parameter choice . essentially , the challenge is to detect the neighborhood of various data sets , while utterly ignorant of the data characteristic . this article introduces a new supervised classification method : the extend natural neighbor ( enan ) method , and shows that it provides a better classification result without choosing the neighborhood parameter artificially . unlike the original knn based method which needs a prior k , the enane method predicts different k in different stages . therefore , the enane method is able to learn more from flexible neighbor information both in training stage and testing stage , and provide a better classification result .", "topics": ["supervised learning"]}
{"title": "kronecker pca based spatio-temporal modeling of video for dismount classification", "abstract": "we consider the application of kronpca spatio-temporal modeling techniques [ greenewald et al 2013 , tsiligkaridis et al 2013 ] to the extraction of spatiotemporal features for video dismount classification . kronpca performs a low-rank type of dimensionality reduction that is adapted to spatio-temporal data and is characterized by the t frame multiframe mean and covariance of p spatial features . for further regularization and improved inverse estimation , we also use the diagonally corrected kronpca shrinkage methods we presented in [ greenewald et al 2013 ] . we apply this very general method to the modeling of the multivariate temporal behavior of hog features extracted from pedestrian bounding boxes in video , with gender classification in a challenging dataset chosen as a specific application . the learned covariances for each class are used to extract spatiotemporal features which are then classified , achieving competitive classification performance .", "topics": ["matrix regularization"]}
{"title": "graph-based learning with unbalanced clusters", "abstract": "graph construction is a crucial step in spectral clustering ( sc ) and graph-based semi-supervised learning ( ssl ) . spectral methods applied on standard graphs such as full-rbf , $ \\epsilon $ -graphs and $ k $ -nn graphs can lead to poor performance in the presence of proximal and unbalanced data . this is because spectral methods based on minimizing ratiocut or normalized cut on these graphs tend to put more importance on balancing cluster sizes over reducing cut values . we propose a novel graph construction technique and show that the ratiocut solution on this new graph is able to handle proximal and unbalanced data . our method is based on adaptively modulating the neighborhood degrees in a $ k $ -nn graph , which tends to sparsify neighborhoods in low density regions . our method adapts to data with varying levels of unbalancedness and can be naturally used for small cluster detection . we justify our ideas through limit cut analysis . unsupervised and semi-supervised experiments on synthetic and real data sets demonstrate the superiority of our method .", "topics": ["cluster analysis", "supervised learning"]}
{"title": "a physical metaphor to study semantic drift", "abstract": "in accessibility tests for digital preservation , over time we experience drifts of localized and labelled content in statistical models of evolving semantics represented as a vector field . this articulates the need to detect , measure , interpret and model outcomes of knowledge dynamics . to this end we employ a high-performance machine learning algorithm for the training of extremely large emergent self-organizing maps for exploratory data analysis . the working hypothesis we present here is that the dynamics of semantic drifts can be modeled on a relaxed version of newtonian mechanics called social mechanics . by using term distances as a measure of semantic relatedness vs . their pagerank values indicating social importance and applied as variable `term mass ' , gravitation as a metaphor to express changes in the semantic content of a vector field lends a new perspective for experimentation . from `term gravitation ' over time , one can compute its generating potential whose fluctuations manifest modifications in pairwise term similarity vs. social importance , thereby updating osgood 's semantic differential . the dataset examined is the public catalog metadata of tate galleries , london .", "topics": ["value ( ethics )", "map"]}
{"title": "inferring generative model structure with static analysis", "abstract": "obtaining enough labeled data to robustly train complex discriminative models is a major bottleneck in the machine learning pipeline . a popular solution is combining multiple sources of weak supervision using generative models . the structure of these models affects training label quality , but is difficult to learn without any ground truth labels . we instead rely on these weak supervision sources having some structure by virtue of being encoded programmatically . we present coral , a paradigm that infers generative model structure by statically analyzing the code for these heuristics , thus reducing the data required to learn structure significantly . we prove that coral 's sample complexity scales quasilinearly with the number of heuristics and number of relations found , improving over the standard sample complexity , which is exponential in $ n $ for identifying $ n^ { \\textrm { th } } $ degree relations . experimentally , coral matches or outperforms traditional structure learning approaches by up to 3.81 f1 points . using coral to model dependencies instead of assuming independence results in better performance than a fully supervised model by 3.07 accuracy points when heuristics are used to label radiology data without ground truth labels .", "topics": ["generative model", "time complexity"]}
{"title": "alice : towards understanding adversarial learning for joint distribution matching", "abstract": "we investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching . within a framework of conditional entropy , we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks . we unify a broad family of adversarial models as joint distribution matching problems . our approach stabilizes learning of unsupervised bidirectional adversarial learning methods . further , we introduce an extension for semi-supervised learning tasks . theoretical results are validated in synthetic data and real-world applications .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "sockpuppet detection in wikipedia : a corpus of real-world deceptive writing for linking identities", "abstract": "this paper describes the corpus of sockpuppet cases we gathered from wikipedia . a sockpuppet is an online user account created with a fake identity for the purpose of covering abusive behavior and/or subverting the editing regulation process . we used a semi-automated method for crawling and curating a dataset of real sockpuppet investigation cases . to the best of our knowledge , this is the first corpus available on real-world deceptive writing . we describe the process for crawling the data and some preliminary results that can be used as baseline for benchmarking research . the dataset will be released under a creative commons license from our project website : http : //docsig.cis.uab.edu .", "topics": ["baseline ( configuration management )", "text corpus"]}
{"title": "encoding reality : prediction-assisted cortical learning algorithm in hierarchical temporal memory", "abstract": "in the decade since jeff hawkins proposed hierarchical temporal memory ( htm ) as a model of neocortical computation , the theory and the algorithms have evolved dramatically . this paper presents a detailed description of htm 's cortical learning algorithm ( cla ) , including for the first time a rigorous mathematical formulation of all aspects of the computations . prediction assisted cla ( pacla ) , a refinement of the cla is presented , which is both closer to the neuroscience and adds significantly to the computational power . finally , we summarise the key functions of neocortex which are expressed in pacla implementations .", "topics": ["computation"]}
{"title": "policy optimization by genetic distillation", "abstract": "genetic algorithms have been widely used in many practical optimization problems . inspired by natural selection , operators , including mutation , crossover and selection , provide effective heuristics for search and black-box optimization . however , they have not been shown useful for deep reinforcement learning , possibly due to the catastrophic consequence of parameter crossovers of neural networks . here , we present genetic policy optimization ( gpo ) , a new genetic algorithm for sample-efficient deep policy optimization . gpo uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation . our experiments on mujoco tasks show that gpo as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency .", "topics": ["mathematical optimization", "reinforcement learning"]}
{"title": "a trust-region method for stochastic variational inference with applications to streaming data", "abstract": "stochastic variational inference allows for fast posterior inference in complex bayesian models . however , the algorithm is prone to local optima which can make the quality of the posterior approximation sensitive to the choice of hyperparameters and initialization . we address this problem by replacing the natural gradient step of stochastic varitional inference with a trust-region update . we show that this leads to generally better results and reduced sensitivity to hyperparameters . we also describe a new strategy for variational inference on streaming data and show that here our trust-region method is crucial for getting good performance .", "topics": ["calculus of variations", "gradient"]}
{"title": "adaptive image denoising by targeted databases", "abstract": "we propose a data-dependent denoising procedure to restore noisy images . different from existing denoising algorithms which search for patches from either the noisy image or a generic database , the new algorithm finds patches from a database that contains only relevant patches . we formulate the denoising problem as an optimal filter design problem and make two contributions . first , we determine the basis function of the denoising filter by solving a group sparsity minimization problem . the optimization formulation generalizes existing denoising algorithms and offers systematic analysis of the performance . improvement methods are proposed to enhance the patch search process . second , we determine the spectral coefficients of the denoising filter by considering a localized bayesian prior . the localized prior leverages the similarity of the targeted database , alleviates the intensive bayesian computation , and links the new method to the classical linear minimum mean squared error estimation . we demonstrate applications of the proposed method in a variety of scenarios , including text images , multiview images and face images . experimental results show the superiority of the new algorithm over existing methods .", "topics": ["noise reduction", "sparse matrix"]}
{"title": "a deep recurrent framework for cleaning motion capture data", "abstract": "we present a deep , bidirectional , recurrent framework for cleaning noisy and incomplete motion capture data . it exploits temporal coherence and joint correlations to infer adaptive filters for each joint in each frame . a single model can be trained to denoise a heterogeneous mix of action types , under substantial amounts of noise . a signal that has both noise and gaps is preprocessed with a second bidirectional network that synthesizes missing frames from surrounding context . the approach handles a wide variety of noise types and long gaps , does not rely on knowledge of the noise distribution , and operates in a streaming setting . we validate our approach through extensive evaluations on noise both in joint angles and in joint positions , and show that it improves upon various alternatives .", "topics": ["noise reduction"]}
{"title": "the fast haar wavelet transform for signal & image processing", "abstract": "a method for the design of fast haar wavelet for signal processing and image processing has been proposed . in the proposed work , the analysis bank and synthesis bank of haar wavelet is modified by using polyphase structure . finally , the fast haar wavelet was designed and it satisfies alias free and perfect reconstruction condition . computational time and computational complexity is reduced in fast haar wavelet transform .", "topics": ["image processing", "computational complexity theory"]}
{"title": "perception games , the image understanding and interpretational geometry", "abstract": "the interactive game theoretical approach to the description of perception processes is proposed . the subject is treated formally in terms of a new class of the verbalizable interactive games which are called the perception games . an application of the previously elaborated formalism of dialogues and verbalizable interactive games to the visual perception allows to combine the linguistic ( such as formal grammars ) , psycholinguistic and ( interactive ) game theoretical methods for analysis of the image understanding by a human that may be also useful for the elaboration of computer vision systems . by the way the interactive game theoretical aspects of interpretational geometries are clarified .", "topics": ["computer vision"]}
{"title": "hi-rf : incremental learning random forest for large-scale multi-class data classification", "abstract": "in recent years , dynamically growing data and incrementally growing number of classes pose new challenges to large-scale data classification research . most traditional methods struggle to balance the precision and computational burden when data and its number of classes increased . however , some methods are with weak precision , and the others are time-consuming . in this paper , we propose an incremental learning method , namely , heterogeneous incremental nearest class mean random forest ( hi-rf ) , to handle this issue . it is a heterogeneous method that either replaces trees or updates trees leaves in the random forest adaptively , to reduce the computational time in comparable performance , when data of new classes arrive . specifically , to keep the accuracy , one proportion of trees are replaced by new ncm decision trees ; to reduce the computational load , the rest trees are updated their leaves probabilities only . most of all , out-of-bag estimation and out-of-bag boosting are proposed to balance the accuracy and the computational efficiency . fair experiments were conducted and demonstrated its comparable precision with much less computational time .", "topics": ["time complexity"]}
{"title": "an application of backpropagation artificial neural network method for measuring the severity of osteoarthritis", "abstract": "the examination of osteoarthritis disease through x-ray by rheumatology can be classified into four grade of severity . this paper discusses about the application of artificial neural network backpropagation method for measuring the severity of the disease , where the observed x-ray range from wrist to fingers . the main procedures of system in this paper is divided into three , which are image processing , feature extraction , and artificial neural network process . first , an x-ray image digital ( 200x150 pixels and greyscale ) will be thresholded , then extracted features based on probabilistic values of the color intensity of seven bit quantization result , and statistical textures . that feature values then will be normalizing to interval [ 0.1 , 0.9 ] , and then the result would be processing on backpropagation artificial neural network system as input to determine the severity of disease from an x-ray had input before it . from testing with learning rate 0.3 , momentum 0.4 , hidden units five pieces and about 132 feature vectors , this system had had a level of accuracy of 100 % for learning data , 80 % for learning and non-learning data , and 66.6 % for non-learning data", "topics": ["image processing", "feature vector"]}
{"title": "secure surf with fully homomorphic encryption", "abstract": "cloud computing is an important part of today 's world because offloading computations is a method to reduce costs . in this paper , we investigate computing the speeded up robust features ( surf ) using fully homomorphic encryption ( fhe ) . performing surf in fhe enables a method to offload the computations while maintaining security and privacy of the original data . in support of this research , we developed a framework to compute surf via a rational number based compatible with fhe . although floating point ( r ) to rational numbers ( q ) conversion introduces error , our research provides tight bounds on the magnitude of error in terms of parameters of fhe . we empirically verified the proposed method against a set of images at different sizes and showed that our framework accurately computes most of the surf keypoints in fhe .", "topics": ["computation"]}
{"title": "variational inference of latent state sequences using recurrent networks", "abstract": "recent advances in the estimation of deep directed graphical models and recurrent networks let us contribute to the removal of a blind spot in the area of probabilistc modelling of time series . the proposed methods i ) can infer distributed latent state-space trajectories with nonlinear transitions , ii ) scale to large data sets thanks to the use of a stochastic objective and fast , approximate inference , iii ) enable the design of rich emission models which iv ) will naturally lead to structured outputs . two different paths of introducing latent state sequences are pursued , leading to the variational recurrent auto encoder ( vrae ) and the variational one step predictor ( vosp ) . the use of independent wiener processes as priors on the latent state sequence is a viable compromise between efficient computation of the kullback-leibler divergence from the variational approximation of the posterior and maintaining a reasonable belief in the dynamics . we verify our methods empirically , obtaining results close or superior to the state of the art . we also show qualitative results for denoising and missing value imputation .", "topics": ["graphical model", "calculus of variations"]}
{"title": "spike and slab gaussian process latent variable models", "abstract": "the gaussian process latent variable model ( gp-lvm ) is a popular approach to non-linear probabilistic dimensionality reduction . one design choice for the model is the number of latent variables . we present a spike and slab prior for the gp-lvm and propose an efficient variational inference procedure that gives a lower bound of the log marginal likelihood . the new model provides a more principled approach for selecting latent dimensions than the standard way of thresholding the length-scale parameters . the effectiveness of our approach is demonstrated through experiments on real and simulated data . further , we extend multi-view gaussian processes that rely on sharing latent dimensions ( known as manifold relevance determination ) with spike and slab priors . this allows a more principled approach for selecting a subset of the latent space for each view of data . the extended model outperforms the previous state-of-the-art when applied to a cross-modal multimedia retrieval task .", "topics": ["calculus of variations", "nonlinear system"]}
{"title": "jump-means : small-variance asymptotics for markov jump processes", "abstract": "markov jump processes ( mjps ) are used to model a wide range of phenomena from disease progression to rna path folding . however , maximum likelihood estimation of parametric models leads to degenerate trajectories and inferential performance is poor in nonparametric models . we take a small-variance asymptotics ( sva ) approach to overcome these limitations . we derive the small-variance asymptotics for parametric and nonparametric mjps for both directly observed and hidden state models . in the parametric case we obtain a novel objective function which leads to non-degenerate trajectories . to derive the nonparametric version we introduce the gamma-gamma process , a novel extension to the gamma-exponential process . we propose algorithms for each of these formulations , which we call \\emph { jump-means } . our experiments demonstrate that jump-means is competitive with or outperforms widely used mjp inference approaches in terms of both speed and reconstruction accuracy .", "topics": ["time complexity", "optimization problem"]}
{"title": "algorithms for generalized cluster-wise linear regression", "abstract": "cluster-wise linear regression ( clr ) , a clustering problem intertwined with regression , is to find clusters of entities such that the overall sum of squared errors from regressions performed over these clusters is minimized , where each cluster may have different variances . we generalize the clr problem by allowing each entity to have more than one observation , and refer to it as generalized clr . we propose an exact mathematical programming based approach relying on column generation , a column generation based heuristic algorithm that clusters predefined groups of entities , a metaheuristic genetic algorithm with adapted lloyd 's algorithm for k-means clustering , a two-stage approach , and a modified algorithm of sp { \\ '' a } th \\cite { spath1979 } for solving generalized clr . we examine the performance of our algorithms on a stock keeping unit ( sku ) clustering problem employed in forecasting halo and cannibalization effects in promotions using real-world retail data from a large supermarket chain . in the sku clustering problem , the retailer needs to cluster skus based on their seasonal effects in response to promotions . the seasonal effects are the results of regressions with predictors being promotion mechanisms and seasonal dummies performed over clusters generated . we compare the performance of all proposed algorithms for the sku problem with real-world and synthetic data .", "topics": ["cluster analysis", "mathematical optimization"]}
{"title": "bounded policy synthesis for pomdps with safe-reachability objectives", "abstract": "planning robust executions under uncertainty is a fundamental challenge for building autonomous robots . partially observable markov decision processes ( pomdps ) provide a standard framework for modeling uncertainty in many robot applications . a key algorithmic problem for pomdps is policy synthesis . while this problem has traditionally been posed w.r.t . optimality objectives , many robot applications are better modeled by pomdps where the objective is a boolean requirement . in this paper , we study the latter problem in a setting where the requirement is a safe-reachability property , which states that with a probability above a certain threshold , it is possible to eventually reach a goal state while satisfying a safety requirement . the central challenge in our problem is that it requires reasoning over a vast space of probability distributions . what 's more , it has been shown that policy synthesis of pomdps with reachability objectives is undecidable in general . to address these challenges , we introduce the notion of a goal-constrained belief space , which only contains beliefs ( probability distributions over states ) reachable from the initial belief under desired executions . this constrained space is generally much smaller than the original belief space . our approach compactly represents this space over a bounded horizon using symbolic constraints , and employs an incremental satisfiability modulo theories ( smt ) solver to efficiently search for a valid policy over it . we evaluate our method using a case study involving a partially observable robotics domain with uncertain obstacles . our results suggest that it is possible to synthesize policies over large belief spaces with a small number of smt solver calls by focusing on goal-constrained belief space , and our method o ers a stronger guarantee of both safety and reachability than alternative unconstrained/constrained pomdp formulations .", "topics": ["autonomous car", "robot"]}
{"title": "confocalgn : a minimalistic confocal image simulator", "abstract": "summary : we developed a user-friendly software to generate synthetic confocal microscopy images from a ground truth specified as a 3d bitmap with pixels of arbitrary size . the software can analyze a real confocal stack to derivate noise parameters and will use them directly to generate new images with similar noise characteristics . such synthetic images can then be used to assert the quality and robustness of an image analysis pipeline , as well as be used to train machine-learning image analysis procedures . we illustrate the approach with closed curves corresponding to the microtubule ring present in blood platelet . availability and implementation : confocalgn is written in matlab but does not require any toolbox . the source code is distributed under the gpl 3.0 licence on https : //github.com/sergedmi/confocalgn .", "topics": ["synthetic data", "simulation"]}
{"title": "learning in the presence of corruption", "abstract": "in supervised learning one wishes to identify a pattern present in a joint distribution $ p $ , of instances , label pairs , by providing a function $ f $ from instances to labels that has low risk $ \\mathbb { e } _ { p } \\ell ( y , f ( x ) ) $ . to do so , the learner is given access to $ n $ iid samples drawn from $ p $ . in many real world problems clean samples are not available . rather , the learner is given access to samples from a corrupted distribution $ \\tilde { p } $ from which to learn , while the goal of predicting the clean pattern remains . there are many different types of corruption one can consider , and as of yet there is no general means to compare the relative ease of learning under these different corruption processes . in this paper we develop a general framework for tackling such problems as well as introducing upper and lower bounds on the risk for learning in the presence of corruption . our ultimate goal is to be able to make informed economic decisions in regards to the acquisition of data sets . for a certain subclass of corruption processes ( those that are \\emph { reconstructible } ) we achieve this goal in a particular sense . our lower bounds are in terms of the coefficient of ergodicity , a simple to calculate property of stochastic matrices . our upper bounds proceed via a generalization of the method of unbiased estimators appearing in recent work of natarajan et al and implicit in the earlier work of kearns .", "topics": ["supervised learning", "coefficient"]}
{"title": "unbiased online recurrent optimization", "abstract": "the novel unbiased online recurrent optimization ( uoro ) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models . it works in a streaming fashion and avoids backtracking through past activations and inputs . uoro is computationally as costly as truncated backpropagation through time ( truncated bptt ) , a widespread algorithm for online learning of recurrent networks . uoro is a modification of nobacktrack that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks , even for complex models . like nobacktrack , uoro provides unbiased gradient estimates ; unbiasedness is the core hypothesis in stochastic gradient descent theory , without which convergence to a local optimum is not guaranteed . on the contrary , truncated bptt does not provide this property , leading to possible divergence . on synthetic tasks where truncated bptt is shown to diverge , uoro converges . for instance , when a parameter has a positive short-term but negative long-term influence , truncated bptt diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions , while uoro performs well thanks to the unbiasedness of its gradients .", "topics": ["recurrent neural network", "gradient descent"]}
{"title": "projective reconstruction in algebraic vision", "abstract": "we discuss the geometry of rational maps from a projective space of an arbitrary dimension to the product of projective spaces of lower dimensions induced by linear projections . in particular , we give a purely algebro-geometric proof of the projective reconstruction theorem by hartley and schaffalitzky [ hs09 ] .", "topics": ["map"]}
{"title": "treeqn and atreec : differentiable tree-structured models for deep reinforcement learning", "abstract": "combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep rl . on-line planning with look-ahead trees has proven successful in environments where transition models are known a priori . however , in complex environments where transition models need to be learned from data , the deficiencies of learned models have limited their utility for planning . to address these challenges , we propose treeqn , a differentiable , recursive , tree-structured model that serves as a drop-in replacement for any value function network in deep rl with discrete actions . treeqn dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate q-values . we also propose atreec , an actor-critic variant that augments treeqn with a softmax layer to form a stochastic policy network . both approaches are trained end-to-end , such that the learned model is optimised for its actual use in the tree . we show that treeqn and atreec outperform n-step dqn and a2c on a box-pushing task , as well as n-step dqn and value prediction networks ( oh et al . 2017 ) on multiple atari games . furthermore , we present ablation studies that demonstrate the effect of different auxiliary losses on learning transition models .", "topics": ["reinforcement learning", "end-to-end principle"]}
{"title": "generic statistical relational entity resolution in knowledge graphs", "abstract": "entity resolution , the problem of identifying the underlying entity of references found in data , has been researched for many decades in many communities . a common theme in this research has been the importance of incorporating relational features into the resolution process . relational entity resolution is particularly important in knowledge graphs ( kgs ) , which have a regular structure capturing entities and their interrelationships . we identify three major problems in kg entity resolution : ( 1 ) intra-kg reference ambiguity ; ( 2 ) inter-kg reference ambiguity ; and ( 3 ) ambiguity when extending kgs with new facts . we implement a framework that generalizes across these three settings and exploits this regular structure of kgs . our framework has many advantages over custom solutions widely deployed in industry , including collective inference , scalability , and interpretability . we apply our framework to two real-world kg entity resolution problems , ambiguity in nell and merging data from freebase and musicbrainz , demonstrating the importance of relational features .", "topics": ["entity", "scalability"]}
{"title": "using simulation and domain adaptation to improve efficiency of deep robotic grasping", "abstract": "instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive . an appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically . unfortunately , models trained purely on simulated data often fail to generalize to the real world . we study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular rgb images . we extensively evaluate our approaches with a total of more than 25,000 physical test grasps , studying a range of simulation conditions and domain adaptation methods , including a novel extension of pixel-level domain adaptation that we term the graspgan . we show that , by using synthetic data and domain adaptation , we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times , using only randomly generated simulated objects . we also show that by using only unlabeled real-world data and our graspgan methodology , we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples .", "topics": ["synthetic data", "simulation"]}
{"title": "feedback-controlled sequential lasso screening", "abstract": "one way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features . prior research has shown that sequential screening methods offer the greatest promise in this endeavor . most existing work on sequential screening targets the context of tuning parameter selection , where one screens and solves a sequence of $ n $ lasso problems with a fixed grid of geometrically spaced regularization parameters . in contrast , we focus on the scenario where a target regularization parameter has already been chosen via cross-validated model selection , and we then need to solve many lasso instances using this fixed value . in this context , we propose and explore a feedback controlled sequential screening scheme . feedback is used at each iteration to select the next problem to be solved . this allows the sequence of problems to be adapted to the instance presented and the number of intermediate problems to be automatically selected . we demonstrate our feedback scheme using several datasets including a dictionary of approximate size 100,000 by 300,000 .", "topics": ["approximation algorithm", "matrix regularization"]}
{"title": "partially observable markov decision process for recommender systems", "abstract": "we report the `` recurrent deterioration '' ( rd ) phenomenon observed in online recommender systems . the rd phenomenon is reflected by the trend of performance degradation when the recommendation model is always trained based on users ' feedbacks of the previous recommendations . there are several reasons for the recommender systems to encounter the rd phenomenon , including the lack of negative training data and the evolution of users ' interests , etc . motivated to tackle the problems causing the rd phenomenon , we propose the pomdp-rec framework , which is a neural-optimized partially observable markov decision process algorithm for recommender systems . we show that the pomdp-rec framework effectively uses the accumulated historical data from real-world recommender systems and automatically achieves comparable results with those models fine-tuned exhaustively by domain exports on public datasets .", "topics": ["test set", "recurrent neural network"]}
{"title": "large-margin learning of submodular summarization methods", "abstract": "in this paper , we present a supervised learning approach to training submodular scoring functions for extractive multi-document summarization . by taking a structured predicition approach , we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure . the learning method applies to all submodular summarization methods , and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets . compared to state-of-the-art functions that were tuned manually , our method significantly improves performance and enables high-fidelity models with numbers of parameters well beyond what could reasonbly be tuned by hand .", "topics": ["supervised learning"]}
{"title": "generation of high dynamic range illumination from a single image for the enhancement of undesirably illuminated images", "abstract": "this paper presents an algorithm that enhances undesirably illuminated images by generating and fusing multi-level illuminations from a single image.the input image is first decomposed into illumination and reflectance components by using an edge-preserving smoothing filter . then the reflectance component is scaled up to improve the image details in bright areas . the illumination component is scaled up and down to generate several illumination images that correspond to certain camera exposure values different from the original . the virtual multi-exposure illuminations are blended into an enhanced illumination , where we also propose a method to generate appropriate weight maps for the tone fusion . finally , an enhanced image is obtained by multiplying the equalized illumination and enhanced reflectance . experiments show that the proposed algorithm produces visually pleasing output and also yields comparable objective results to the conventional enhancement methods , while requiring modest computational loads .", "topics": ["map"]}
{"title": "dynamic classifier chains for multi-label learning", "abstract": "in this paper , we deal with the task of building a dynamic ensemble of chain classifiers for multi-label classification . to do so , we proposed two concepts of classifier chains algorithms that are able to change label order of the chain without rebuilding the entire model . such modes allows anticipating the instance-specific chain order without a significant increase in computational burden . the proposed chain models are built using the naive bayes classifier and nearest neighbour approach as a base single-label classifiers . to take the benefits of the proposed algorithms , we developed a simple heuristic that allows the system to find relatively good label order . the heuristic sort labels according to the label-specific classification quality gained during the validation phase . the heuristic tries to minimise the phenomenon of error propagation in the chain . the experimental results showed that the proposed model based on naive bayes classifier the above-mentioned heuristic is an efficient tool for building dynamic chain classifiers .", "topics": ["heuristic"]}
{"title": "types of cognition and its implications for future high-level cognitive machines", "abstract": "this work summarizes part of current knowledge on high-level cognitive process and its relation with biological hardware . thus , it is possible to identify some paradoxes which could impact the development of future technologies and artificial intelligence : we may make a high-level cognitive machine , sacrificing the principal attribute of a machine , its accuracy .", "topics": ["artificial intelligence"]}
{"title": "detecting figures and part labels in patents : competition-based development of image processing algorithms", "abstract": "we report the findings of a month-long online competition in which participants developed algorithms for augmenting the digital version of patent documents published by the united states patent and trademark office ( uspto ) . the goal was to detect figures and part labels in u.s. patent drawing pages . the challenge drew 232 teams of two , of which 70 teams ( 30 % ) submitted solutions . collectively , teams submitted 1,797 solutions that were compiled on the competition servers . participants reported spending an average of 63 hours developing their solutions , resulting in a total of 5,591 hours of development time . a manually labeled dataset of 306 patents was used for training , online system tests , and evaluation . the design and performance of the top-5 systems are presented , along with a system developed after the competition which illustrates that winning teams produced near state-of-the-art results under strict time and computation constraints . for the 1st place system , the harmonic mean of recall and precision ( f-measure ) was 88.57 % for figure region detection , 78.81 % for figure regions with correctly recognized figure titles , and 70.98 % for part label detection and character recognition . data and software from the competition are available through the online uci machine learning repository to inspire follow-on work by the image processing community .", "topics": ["image processing", "computation"]}
{"title": "are you talking to me ? reasoned visual dialog generation through adversarial learning", "abstract": "the visual dialogue task requires an agent to engage in a conversation about an image with a human . it represents an extension of the visual question answering task in that the agent needs to answer a question about an image , but it needs to do so in light of the previous dialogue that has taken place . the key challenge in visual dialogue is thus maintaining a consistent , and natural dialogue while continuing to answer questions correctly . we present a novel approach that combines reinforcement learning and generative adversarial networks ( gans ) to generate more human-like responses to questions . the gan helps overcome the relative paucity of training data , and the tendency of the typical mle-based approach to generate overly terse answers . critically , the gan is tightly integrated into the attention mechanism that generates human-interpretable reasons for each answer . this means that the discriminative model of the gan has the task of assessing whether a candidate answer is generated by a human or not , given the provided reason . this is significant because it drives the generative model to produce high quality answers that are well supported by the associated reasoning . the method also generates the state-of-the-art results on the primary benchmark .", "topics": ["generative model", "test set"]}
{"title": "bayesian group nonnegative matrix factorization for eeg analysis", "abstract": "we propose a generative model of a group eeg analysis , based on appropriate kernel assumptions on eeg data . we derive the variational inference update rule using various approximation techniques . the proposed model outperforms the current state-of-the-art algorithms in terms of common pattern extraction . the validity of the proposed model is tested on the bci competition dataset .", "topics": ["generative model", "calculus of variations"]}
{"title": "empirical study on deep learning models for question answering", "abstract": "in this paper we explore deep learning models with memory component or attention mechanism for question answering task . we combine and compare three models , neural machine translation , neural turing machine , and memory networks for a simulated qa data set . this paper is the first one that uses neural machine translation and neural turing machines for solving qa tasks . our results suggest that the combination of attention and memory have potential to solve certain qa problem .", "topics": ["machine translation", "simulation"]}
{"title": "on sensitivity of the map bayesian network structure to the equivalent sample size parameter", "abstract": "bdeu marginal likelihood score is a popular model selection criterion for selecting a bayesian network structure based on sample data . this non-informative scoring criterion assigns same score for network structures that encode same independence statements . however , before applying the bdeu score , one must determine a single parameter , the equivalent sample size alpha . unfortunately no generally accepted rule for determining the alpha parameter has been suggested . this is disturbing , since in this paper we show through a series of concrete experiments that the solution of the network structure optimization problem is highly sensitive to the chosen alpha parameter value . based on these results , we are able to give explanations for how and why this phenomenon happens , and discuss ideas for solving this problem .", "topics": ["optimization problem", "bayesian network"]}
{"title": "multi-dimensional parametric mincuts for constrained map inference", "abstract": "in this paper , we propose novel algorithms for inferring the maximum a posteriori ( map ) solution of discrete pairwise random field models under multiple constraints . we show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its lagrangian dual , and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly . these multiple solutions enable us to even deal with `soft constraints ' ( higher order penalty functions ) . moreover , we propose two practical variants of our algorithm to solve problems with hard constraints . we also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation . experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods .", "topics": ["optimization problem", "image segmentation"]}
{"title": "macroblock classification method for video applications involving motions", "abstract": "in this paper , a macroblock classification method is proposed for various video processing applications involving motions . based on the analysis of the motion vector field in the compressed video , we propose to classify macroblocks of each video frame into different classes and use this class information to describe the frame content . we demonstrate that this low-computation-complexity method can efficiently catch the characteristics of the frame . based on the proposed macroblock classification , we further propose algorithms for different video processing applications , including shot change detection , motion discontinuity detection , and outlier rejection for global motion estimation . experimental results demonstrate that the methods based on the proposed approach can work effectively on these applications .", "topics": ["computation"]}
{"title": "relevant ensemble of trees", "abstract": "tree ensembles are flexible predictive models that can capture relevant variables and to some extent their interactions in a compact and interpretable manner . most algorithms for obtaining tree ensembles are based on versions of boosting or random forest . previous work showed that boosting algorithms exhibit a cyclic behavior of selecting the same tree again and again due to the way the loss is optimized . at the same time , random forest is not based on loss optimization and obtains a more complex and less interpretable model . in this paper we present a novel method for obtaining compact tree ensembles by growing a large pool of trees in parallel with many independent boosting threads and then selecting a small subset and updating their leaf weights by loss optimization . we allow for the trees in the initial pool to have different depths which further helps with generalization . experiments on real datasets show that the obtained model has usually a smaller loss than boosting , which is also reflected in a lower misclassification error on the test set .", "topics": ["test set", "mathematical optimization"]}
{"title": "probabilistic sensor fusion for ambient assisted living", "abstract": "there is a widely-accepted need to revise current forms of health-care provision , with particular interest in sensing systems in the home . given a multiple-modality sensor platform with heterogeneous network connectivity , as is under development in the sensor platform for healthcare in residential environment ( sphere ) interdisciplinary research collaboration ( irc ) , we face specific challenges relating to the fusion of the heterogeneous sensor modalities . we introduce bayesian models for sensor fusion , which aims to address the challenges of fusion of heterogeneous sensor modalities . using this approach we are able to identify the modalities that have most utility for each particular activity , and simultaneously identify which features within that activity are most relevant for a given activity . we further show how the two separate tasks of location prediction and activity recognition can be fused into a single model , which allows for simultaneous learning an prediction for both tasks . we analyse the performance of this model on data collected in the sphere house , and show its utility . we also compare against some benchmark models which do not have the full structure , and show how the proposed model compares favourably to these methods", "topics": ["sensor"]}
{"title": "nested nonnegative cone analysis", "abstract": "motivated by the analysis of nonnegative data objects , a novel nested nonnegative cone analysis ( nnca ) approach is proposed to overcome some drawbacks of existing methods . the application of traditional pca/svd method to nonnegative data often cause the approximation matrix leave the nonnegative cone , which leads to non-interpretable and sometimes nonsensical results . the nonnegative matrix factorization ( nmf ) approach overcomes this issue , however the nmf approximation matrices suffer several drawbacks : 1 ) the factorization may not be unique , 2 ) the resulting approximation matrix at a specific rank may not be unique , and 3 ) the subspaces spanned by the approximation matrices at different ranks may not be nested . these drawbacks will cause troubles in determining the number of components and in multi-scale ( in ranks ) interpretability . the nnca approach proposed in this paper naturally generates a nested structure , and is shown to be unique at each rank . simulations are used in this paper to illustrate the drawbacks of the traditional methods , and the usefulness of the nnca method .", "topics": ["simulation"]}
{"title": "`` let me convince you to buy my product ... `` : a case study of an automated persuasive system for fashion products", "abstract": "persuasivenes is a creative art aimed at making people believe in certain set of beliefs . many a times , such creativity is about adapting richness of one domain into another to strike a chord with the target audience . in this research , we present persuaide ! - a persuasive system based on linguistic creativity to transform given sentence to generate various forms of persuading sentences . these various forms cover multiple focus of persuasion such as memorability and sentiment . for a given simple product line , the algorithm is composed of several steps including : ( i ) select an appropriate well-known expression for the target domain to add memorability , ( ii ) identify keywords and entities in the given sentence and expression and transform it to produce creative persuading sentence , and ( iii ) adding positive or negative sentiment for further persuasion . the persuasive conversion were manually verified using qualitative results and the effectiveness of the proposed approach is empirically discussed .", "topics": ["entity"]}
{"title": "sequence modeling via segmentations", "abstract": "segmental structure is a common pattern in many types of sequences such as phrases in human languages . in this paper , we present a probabilistic model for sequences via their segmentations . the probability of a segmented sequence is calculated as the product of the probabilities of all its segments , where each segment is modeled using existing tools such as recurrent neural networks . since the segmentation of a sequence is usually unknown in advance , we sum over all valid segmentations to obtain the final probability for the sequence . an efficient dynamic programming algorithm is developed for forward and backward computations without resorting to any approximation . we demonstrate our approach on text segmentation and speech recognition tasks . in addition to quantitative results , we also show that our approach can discover meaningful segments in their respective application contexts .", "topics": ["image segmentation", "recurrent neural network"]}
{"title": "revisiting activation regularization for language rnns", "abstract": "recurrent neural networks ( rnns ) serve as a fundamental building block for many sequence tasks across natural language processing . recent research has focused on recurrent dropout techniques or custom rnn cells in order to improve performance . both of these can require substantial modifications to the machine learning model or to the underlying rnn configurations . we revisit traditional regularization techniques , specifically l2 regularization on rnn activations and slowness regularization over successive hidden states , to improve the performance of rnns on the task of language modeling . both of these techniques require minimal modification to existing rnn architectures and result in performance improvements comparable or superior to more complicated regularization techniques or custom cell architectures . these regularization techniques can be used without any modification on optimized lstm implementations such as the nvidia cudnn lstm .", "topics": ["recurrent neural network", "natural language processing"]}
{"title": "adabatch : adaptive batch sizes for training deep neural networks", "abstract": "training deep neural networks with stochastic gradient descent , or its variants , requires careful choice of both learning rate and batch size . while smaller batch sizes generally converge in fewer training epochs , larger batch sizes offer more parallelism and hence better computational efficiency . we have developed a new training approach that , rather than statically choosing a single batch size for all epochs , adaptively increases the batch size during the training process . our method delivers the convergence rate of small batch sizes while achieving performance similar to large batch sizes . we analyse our approach using the standard alexnet , resnet , and vgg networks operating on the popular cifar-10 , cifar-100 , and imagenet datasets . our results demonstrate that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 nvidia tesla p100 gpus while changing accuracy by less than 1 % relative to training with fixed batch sizes .", "topics": ["neural networks", "gradient descent"]}
{"title": "complex networks analysis of language complexity", "abstract": "methods from statistical physics , such as those involving complex networks , have been increasingly used in quantitative analysis of linguistic phenomena . in this paper , we represented pieces of text with different levels of simplification in co-occurrence networks and found that topological regularity correlated negatively with textual complexity . furthermore , in less complex texts the distance between concepts , represented as nodes , tended to decrease . the complex networks metrics were treated with multivariate pattern recognition techniques , which allowed us to distinguish between original texts and their simplified versions . for each original text , two simplified versions were generated manually with increasing number of simplification operations . as expected , distinction was easier for the strongly simplified versions , where the most relevant metrics were node strength , shortest paths and diversity . also , the discrimination of complex texts was improved with higher hierarchical network metrics , thus pointing to the usefulness of considering wider contexts around the concepts . though the accuracy rate in the distinction was not as high as in methods using deep linguistic knowledge , the complex network approach is still useful for a rapid screening of texts whenever assessing complexity is essential to guarantee accessibility to readers with limited reading ability", "topics": ["cluster analysis", "natural language processing"]}
{"title": "network dissection : quantifying interpretability of deep visual representations", "abstract": "we propose a general framework called network dissection for quantifying the interpretability of latent representations of cnns by evaluating the alignment between individual hidden units and a set of semantic concepts . given any cnn model , the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer . the units with semantics are given labels across a range of objects , parts , scenes , textures , materials , and colors . we use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units , then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks . we further analyze the effect of training iterations , compare networks trained with different initializations , examine the impact of network depth and width , and measure the effect of dropout and batch normalization on the interpretability of deep visual representations . we demonstrate that the proposed method can shed light on characteristics of cnn models and training methods that go beyond measurements of their discriminative power .", "topics": ["iteration"]}
{"title": "design of a p system based artificial graph chemistry", "abstract": "artificial chemistries ( acs ) are symbolic chemical metaphors for the exploration of artificial life , with specific focus on the origin of life . in this work we define a p system based artificial graph chemistry to understand the principles leading to the evolution of life-like structures in an ac set up and to develop a unified framework to characterize and classify symbolic artificial chemistries by devising appropriate formalism to capture semantic and organizational information . an extension of p system is considered by associating probabilities with the rules providing the topological framework for the evolution of a labeled undirected graph based molecular reaction semantics .", "topics": ["artificial intelligence"]}
{"title": "mean-field networks", "abstract": "the mean field algorithm is a widely used approximate inference algorithm for graphical models whose exact inference is intractable . in each iteration of mean field , the approximate marginals for each variable are updated by getting information from the neighbors . this process can be equivalently converted into a feedforward network , with each layer representing one iteration of mean field and with tied weights on all layers . this conversion enables a few natural extensions , e.g . untying the weights in the network . in this paper , we study these mean field networks ( mfns ) , and use them as inference tools as well as discriminative models . preliminary experiment results show that mfns can learn to do inference very efficiently and perform significantly better than mean field as discriminative models .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "wrpn : wide reduced-precision networks", "abstract": "for computer vision applications , prior works have shown the efficacy of reducing numeric precision of model parameters ( network weights ) in deep neural networks . activation maps , however , occupy a large memory footprint during both the training and inference step when using mini-batches of inputs . one way to reduce this large memory footprint is to reduce the precision of activations . however , past works have shown that reducing the precision of activations hurts model accuracy . we study schemes to train networks from scratch using reduced-precision activations without hurting accuracy . we reduce the precision of activation maps ( along with model parameters ) and increase the number of filter maps in a layer , and find that this scheme matches or surpasses the accuracy of the baseline full-precision network . as a result , one can significantly improve the execution efficiency ( e.g . reduce dynamic memory footprint , memory bandwidth and computational energy ) and speed up the training and inference process with appropriate hardware support . we call our scheme wrpn - wide reduced-precision networks . we report results and show that wrpn scheme is better than previously reported accuracies on ilsvrc-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks .", "topics": ["baseline ( configuration management )", "computer vision"]}
{"title": "exposure : a white-box photo post-processing framework", "abstract": "retouching can significantly elevate the visual appeal of photos , but many casual photographers lack the expertise to do this well . to address this problem , previous works have proposed automatic retouching systems based on supervised learning from paired training images acquired before and after manual editing . as it is difficult for users to acquire paired images that reflect their retouching preferences , we present in this paper a deep learning approach that is instead trained on unpaired data , namely a set of photographs that exhibits a retouching style the user likes , which is much easier to collect . our system is formulated using deep convolutional neural networks that learn to apply different retouching operations on an input image . network training with respect to various types of edits is enabled by modeling these retouching operations in a unified manner as resolution-independent differentiable filters . to apply the filters in a proper sequence and with suitable parameters , we employ a deep reinforcement learning approach that learns to make decisions on what action to take next , given the current state of the image . in contrast to many deep learning systems , ours provides users with an understandable solution in the form of conventional retouching edits , rather than just a `` black-box '' result . through quantitative comparisons and user studies , we show that this technique generates retouching results consistent with the provided photo set .", "topics": ["baseline ( configuration management )", "end-to-end principle"]}
{"title": "factored contextual policy search with bayesian optimization", "abstract": "scarce data is a major challenge to scaling robot learning to truly complex tasks , as we need to generalize locally learned policies over different `` contexts '' . bayesian optimization approaches to contextual policy search ( cps ) offer data-efficient policy learning that generalize over a context space . we propose to improve data- efficiency by factoring typically considered contexts into two components : target- type contexts that correspond to a desired outcome of the learned behavior , e.g . target position for throwing a ball ; and environment type contexts that correspond to some state of the environment , e.g . initial ball position or wind speed . our key observation is that experience can be directly generalized over target-type contexts . based on that we introduce factored contextual policy search with bayesian optimization for both passive and active learning settings . preliminary results show faster policy generalization on a simulated toy problem .", "topics": ["simulation"]}
{"title": "( bandit ) convex optimization with biased noisy gradient oracles", "abstract": "algorithms for bandit convex optimization and online learning often rely on constructing noisy gradient estimates , which are then used in appropriately adjusted first-order algorithms , replacing actual gradients . depending on the properties of the function to be optimized and the nature of `` noise '' in the bandit feedback , the bias and variance of gradient estimates exhibit various tradeoffs . in this paper we propose a novel framework that replaces the specific gradient estimation methods with an abstract oracle . with the help of the new framework we unify previous works , reproducing their results in a clean and concise fashion , while , perhaps more importantly , the framework also allows us to formally show that to achieve the optimal root- $ n $ rate either the algorithms that use existing gradient estimators , or the proof techniques used to analyze them have to go beyond what exists today .", "topics": ["gradient"]}
{"title": "efficient algorithms for bayesian network parameter learning from incomplete data", "abstract": "we propose an efficient family of algorithms to learn the parameters of a bayesian network from incomplete data . in contrast to textbook approaches such as em and the gradient method , our approach is non-iterative , yields closed form parameter estimates , and eliminates the need for inference in a bayesian network . our approach provides consistent parameter estimates for missing data problems that are mcar , mar , and in some cases , mnar . empirically , our approach is orders of magnitude faster than em ( as our approach requires no inference ) . given sufficient data , we learn parameters that can be orders of magnitude more accurate .", "topics": ["bayesian network"]}
{"title": "early prediction of the duration of protests using probabilistic latent dirichlet allocation and decision trees", "abstract": "protests and agitations are an integral part of every democratic civil society . in recent years , south africa has seen a large increase in its protests . the objective of this paper is to provide an early prediction of the duration of protests from its free flowing english text description . free flowing descriptions of the protests help us in capturing its various nuances such as multiple causes , courses of actions etc . next we use a combination of unsupervised learning ( topic modeling ) and supervised learning ( decision trees ) to predict the duration of the protests . our results show a high degree ( close to 90 % ) of accuracy in early prediction of the duration of protests.we expect the work to help police and other security services in planning and managing their resources in better handling protests in future .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "quick and energy-efficient bayesian computing of binocular disparity using stochastic digital signals", "abstract": "reconstruction of the tridimensional geometry of a visual scene using the binocular disparity information is an important issue in computer vision and mobile robotics , which can be formulated as a bayesian inference problem . however , computation of the full disparity distribution with an advanced bayesian model is usually an intractable problem , and proves computationally challenging even with a simple model . in this paper , we show how probabilistic hardware using distributed memory and alternate representation of data as stochastic bitstreams can solve that problem with high performance and energy efficiency . we put forward a way to express discrete probability distributions using stochastic data representations and perform bayesian fusion using those representations , and show how that approach can be applied to diparity computation . we evaluate the system using a simulated stochastic implementation and discuss possible hardware implementations of such architectures and their potential for sensorimotor processing and robotics .", "topics": ["computational complexity theory", "computer vision"]}
{"title": "a note on the inapproximability of correlation clustering", "abstract": "we consider inapproximability of the correlation clustering problem defined as follows : given a graph $ g = ( v , e ) $ where each edge is labeled either `` + '' ( similar ) or `` - '' ( dissimilar ) , correlation clustering seeks to partition the vertices into clusters so that the number of pairs correctly ( resp . incorrectly ) classified with respect to the labels is maximized ( resp . minimized ) . the two complementary problems are called maxagree and mindisagree , respectively , and have been studied on complete graphs , where every edge is labeled , and general graphs , where some edge might not have been labeled . natural edge-weighted versions of both problems have been studied as well . let s-maxagree denote the weighted problem where all weights are taken from set s , we show that s-maxagree with weights bounded by $ o ( |v|^ { 1/2-\\delta } ) $ essentially belongs to the same hardness class in the following sense : if there is a polynomial time algorithm that approximates s-maxagree within a factor of $ \\lambda = o ( \\log { |v| } ) $ with high probability , then for any choice of s ' , s'-maxagree can be approximated in polynomial time within a factor of $ ( \\lambda + \\epsilon ) $ , where $ \\epsilon > 0 $ can be arbitrarily small , with high probability . a similar statement also holds for $ s-mindisagree . this result implies it is hard ( assuming $ np \\neq rp $ ) to approximate unweighted maxagree within a factor of $ 80/79-\\epsilon $ , improving upon a previous known factor of $ 116/115-\\epsilon $ by charikar et . al . \\cite { chari05 } .", "topics": ["cluster analysis", "approximation algorithm"]}
{"title": "efficient localized inference for large graphical models", "abstract": "we propose a new localized inference algorithm for answering marginalization queries in large graphical models with the correlation decay property . given a query variable and a large graphical model , we define a much smaller model in a local region around the query variable in the target model so that the marginal distribution of the query variable can be accurately approximated . we introduce two approximation error bounds based on the dobrushin 's comparison theorem and apply our bounds to derive a greedy expansion algorithm that efficiently guides the selection of neighbor nodes for localized inference . we verify our theoretical bounds on various datasets and demonstrate that our localized inference algorithm can provide fast and accurate approximation for large graphical models .", "topics": ["graphical model", "approximation algorithm"]}
{"title": "continuous and simultaneous gesture and posture recognition for commanding a robotic wheelchair ; towards spotting the signal patterns", "abstract": "spotting signal patterns with varying lengths has been still an open problem in the literature . in this study , we describe a signal pattern recognition approach for continuous and simultaneous classification of a tracked hand 's posture and gestures and map them to steering commands for control of a robotic wheelchair . the developed methodology not only affords 100\\ % recognition accuracy on a streaming signal for continuous recognition , but also brings about a new perspective for building a training dictionary which eliminates human intervention to spot the gesture or postures on a training signal . in the training phase we employ a state of art subspace clustering method to find the most representative state samples . the recognition and training framework reveal boundaries of the patterns on the streaming signal with a successive decision tree structure intrinsically . we make use of the collaborative ans block sparse representation based classification methods for continuous gesture and posture recognition .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "cnns are globally optimal given multi-layer support", "abstract": "stochastic gradient descent ( sgd ) is the central workhorse for training modern cnns . although giving impressive empirical performance it can be slow to converge . in this paper we explore a novel strategy for training a cnn using an alternation strategy that offers substantial speedups during training . we make the following contributions : ( i ) replace the relu non-linearity within a cnn with positive hard-thresholding , ( ii ) reinterpret this non-linearity as a binary state vector making the entire cnn linear if the multi-layer support is known , and ( iii ) demonstrate that under certain conditions a global optima to the cnn can be found through local descent . we then employ a novel alternation strategy ( between weights and support ) for cnn training that leads to substantially faster convergence rates , nice theoretical properties , and achieving state of the art results across large scale datasets ( e.g . imagenet ) as well as other standard benchmarks .", "topics": ["nonlinear system", "gradient descent"]}
{"title": "data-driven feature sampling for deep hyperspectral classification and segmentation", "abstract": "the high dimensionality of hyperspectral imaging forces unique challenges in scope , size and processing requirements . motivated by the potential for an in-the-field cell sorting detector , we examine a $ \\textit { synechocystis sp . } $ pcc 6803 dataset wherein cells are grown alternatively in nitrogen rich or deplete cultures . we use deep learning techniques to both successfully classify cells and generate a mask segmenting the cells/condition from the background . further , we use the classification accuracy to guide a data-driven , iterative feature selection method , allowing the design neural networks requiring 90 % fewer input features with little accuracy degradation .", "topics": ["iteration"]}
{"title": "normalized gradient with adaptive stepsize method for deep neural network training", "abstract": "in this paper , we propose a generic and simple algorithmic framework for first order optimization . the framework essentially contains two consecutive steps in each iteration : 1 ) computing and normalizing the mini-batch stochastic gradient ; 2 ) selecting adaptive step size to update the decision variable ( parameter ) towards the negative of the normalized gradient . we show that the proposed approach , when customized to the popular adaptive stepsize methods , such as adagrad , can enjoy a sublinear convergence rate , if the objective is convex . we also conduct extensive empirical studies on various non-convex neural network optimization problems , including multi layer perceptron , convolution neural networks and recurrent neural networks . the results indicate the normalized gradient with adaptive step size can help accelerate the training of neural networks . in particular , significant speedup can be observed if the networks are deep or the dependencies are long .", "topics": ["recurrent neural network", "gradient descent"]}
{"title": "efficient clustering of correlated variables and variable selection in high-dimensional linear models", "abstract": "in this paper , we introduce adaptive cluster lasso ( acl ) method for variable selection in high dimensional sparse regression models with strongly correlated variables . to handle correlated variables , the concept of clustering or grouping variables and then pursuing model fitting is widely accepted . when the dimension is very high , finding an appropriate group structure is as difficult as the original problem . the acl is a three-stage procedure where , at the first stage , we use the lasso ( or its adaptive or thresholded version ) to do initial selection , then we also include those variables which are not selected by the lasso but are strongly correlated with the variables selected by the lasso . at the second stage we cluster the variables based on the reduced set of predictors and in the third stage we perform sparse estimation such as lasso on cluster representatives or the group lasso based on the structures generated by clustering procedure . we show that our procedure is consistent and efficient in finding true underlying population group structure ( under assumption of irrepresentable and beta-min conditions ) . we also study the group selection consistency of our method and we support the theory using simulated and pseudo-real dataset examples .", "topics": ["cluster analysis", "simulation"]}
{"title": "how to allocate resources for features acquisition ?", "abstract": "we study classification problems where features are corrupted by noise and where the magnitude of the noise in each feature is influenced by the resources allocated to its acquisition . this is the case , for example , when multiple sensors share a common resource ( power , bandwidth , attention , etc . ) . we develop a method for computing the optimal resource allocation for a variety of scenarios and derive theoretical bounds concerning the benefit that may arise by non-uniform allocation . we further demonstrate the effectiveness of the developed method in simulations .", "topics": ["simulation", "sensor"]}
{"title": "comparison of pca with ica from data distribution perspective", "abstract": "we performed an empirical comparison of ica and pca algorithms by applying them on two simulated noisy time series with varying distribution parameters and level of noise . in general , ica shows better results than pca because it takes into account higher moments of data distribution . on the other hand , pca remains quite sensitive to the level of correlations among signals .", "topics": ["nonlinear system", "optimization problem"]}
{"title": "deep simnets", "abstract": "we present a deep layered architecture that generalizes convolutional neural networks ( convnets ) . the architecture , called simnets , is driven by two operators : ( i ) a similarity function that generalizes inner-product , and ( ii ) a log-mean-exp function called mex that generalizes maximum and average . the two operators applied in succession give rise to a standard neuron but in `` feature space '' . the feature spaces realized by simnets depend on the choice of the similarity operator . the simplest setting , which corresponds to a convolution , realizes the feature space of the exponential kernel , while other settings realize feature spaces of more powerful kernels ( generalized gaussian , which includes as special cases rbf and laplacian ) , or even dynamically learned feature spaces ( generalized multiple kernel learning ) . as a result , the simnet contains a higher abstraction level compared to a traditional convnet . we argue that enhanced expressiveness is important when the networks are small due to run-time constraints ( such as those imposed by mobile applications ) . empirical evaluation validates the superior expressiveness of simnets , showing a significant gain in accuracy over convnets when computational resources at run-time are limited . we also show that in large-scale settings , where computational complexity is less of a concern , the additional capacity of simnets can be controlled with proper regularization , yielding accuracies comparable to state of the art convnets .", "topics": ["kernel ( operating system )", "feature vector"]}
{"title": "embedding words as distributions with a bayesian skip-gram model", "abstract": "we introduce a method for embedding words as probability densities in a low-dimensional space . rather than assuming that a word embedding is fixed across the entire text collection , as in standard word embedding methods , in our bayesian model we generate it from a word-specific prior density for each occurrence of a given word . intuitively , for each word , the prior density encodes the distribution of its potential 'meanings ' . these prior densities are conceptually similar to gaussian embeddings . interestingly , unlike the gaussian embeddings , we can also obtain context-specific densities : they encode uncertainty about the sense of a word given its context and correspond to posterior distributions within our model . the context-dependent densities have many potential applications : for example , we show that they can be directly used in the lexical substitution task . we describe an effective estimation method based on the variational autoencoding framework . we also demonstrate that our embeddings achieve competitive results on standard benchmarks .", "topics": ["natural language processing", "natural language"]}
{"title": "constrained 1-spectral clustering", "abstract": "an important form of prior information in clustering comes in form of can not -link and must-link constraints . we present a generalization of the popular spectral clustering technique which integrates such constraints . motivated by the recently proposed $ 1 $ -spectral clustering for the unconstrained problem , our method is based on a tight relaxation of the constrained normalized cut into a continuous optimization problem . opposite to all other methods which have been suggested for constrained spectral clustering , we can always guarantee to satisfy all constraints . moreover , our soft formulation allows to optimize a trade-off between normalized cut and the number of violated constraints . an efficient implementation is provided which scales to large datasets . we outperform consistently all other proposed methods in the experiments .", "topics": ["cluster analysis", "optimization problem"]}
{"title": "an empirical study of the l2-boost technique with echo state networks", "abstract": "a particular case of recurrent neural network ( rnn ) was introduced at the beginning of the 2000s under the name of echo state networks ( esns ) . the esn model overcomes the limitations during the training of the rnns while introducing no significant disadvantages . although the model presents some well-identified drawbacks when the parameters are not well initialised . the performance of an esn is highly dependent on its internal parameters and pattern of connectivity of the hidden-hidden weights often , the tuning of the network parameters can be hard and can impact in the accuracy of the models . in this work , we investigate the performance of a specific boosting technique ( called l2-boost ) with esns as single predictors . the l2-boost technique has been shown to be an effective tool to combine `` weak '' predictors in regression problems . in this study , we use an ensemble of random initialized esns ( without control their parameters ) as `` weak '' predictors of the boosting procedure . we evaluate our approach on five well-know time-series benchmark problems . additionally , we compare this technique with a baseline approach that consists of averaging the prediction of an ensemble of esns .", "topics": ["baseline ( configuration management )", "time series"]}
{"title": "sparse projections onto the simplex", "abstract": "most learning methods with rank or sparsity constraints use convex relaxations , which lead to optimization with the nuclear norm or the $ \\ell_1 $ -norm . however , several important learning applications can not benefit from this approach as they feature these convex norms as constraints in addition to the non-convex rank and sparsity constraints . in this setting , we derive efficient sparse projections onto the simplex and its extension , and illustrate how to use them to solve high-dimensional learning problems in quantum tomography , sparse density estimation and portfolio selection with non-convex constraints .", "topics": ["sparse matrix"]}
{"title": "efficiently using second order information in large l1 regularization problems", "abstract": "we propose a novel general algorithm lhac that efficiently uses second-order information to train a class of large-scale l1-regularized problems . our method executes cheap iterations while achieving fast local convergence rate by exploiting the special structure of a low-rank matrix , constructed via quasi-newton approximation of the hessian of the smooth loss function . a greedy active-set strategy , based on the largest violations in the dual constraints , is employed to maintain a working set that iteratively estimates the complement of the optimal active set . this allows for smaller size of subproblems and eventually identifies the optimal active set . empirical comparisons confirm that lhac is highly competitive with several recently proposed state-of-the-art specialized solvers for sparse logistic regression and sparse inverse covariance matrix selection .", "topics": ["matrix regularization", "loss function"]}
{"title": "unsupervised depth estimation , 3d face rotation and replacement", "abstract": "we present an unsupervised approach for learning to estimate three dimensional ( 3d ) facial structure from a single image while also predicting 3d viewpoint transformations that match a desired pose and facial geometry . we achieve this by inferring the depth of facial key-points in an input image in an unsupervised way . we show how it is possible to use these depths as intermediate computations within a new backpropable loss to predict the parameters of a 3d affine transformation matrix that maps inferred 3d key-points of an input face to corresponding 2d key-points on a desired target facial geometry or pose . our resulting approach can therefore be used to infer plausible 3d transformations from one face pose to another , allowing faces to be frontalized , trans- formed into 3d models or even warped to another pose and facial geometry . lastly , we identify certain shortcomings with our formulation , and explore adversarial image translation techniques as a post-processing step . correspondingly , we explore several adversarial image transformation methods which allow us to re-synthesize complete head shots for faces re-targeted to different poses as well as repair images resulting from face replacements across identities .", "topics": ["unsupervised learning", "map"]}
{"title": "playerank : multi-dimensional and role-aware rating of soccer player performance", "abstract": "the problem of rating the performance of soccer players is attracting the interest of many companies , websites , and the scientific community , thanks to the availability of massive data capturing all the events generated during a game ( e.g . , tackles , passes , shots , etc . ) . existing approaches fail to fully exploit the richness of the available data and lack of a proper validation . in this paper , we design and implement playerank , a data-driven framework that offers a principled multi-dimensional and role-aware evaluation of the performance of soccer players . we validate the framework through an experimental analysis advised by soccer experts , based on a massive dataset of millions of events pertaining four seasons of the five prominent european leagues . experiments show that playerank is robust in agreeing with the experts ' evaluation of players , significantly improving the state of the art . we also explore an application of playerank -- - i.e . searching players -- - by introducing a special form of spatial query on the soccer field . this shows its flexibility and efficiency , which makes it worth to be used in the design of a scalable platform for soccer analytics .", "topics": ["scalability"]}
{"title": "crowd saliency detection via global similarity structure", "abstract": "it is common for cctv operators to overlook inter- esting events taking place within the crowd due to large number of people in the crowded scene ( i.e . marathon , rally ) . thus , there is a dire need to automate the detection of salient crowd regions acquiring immediate attention for a more effective and proactive surveillance . this paper proposes a novel framework to identify and localize salient regions in a crowd scene , by transforming low-level features extracted from crowd motion field into a global similarity structure . the global similarity structure representation allows the discovery of the intrinsic manifold of the motion dynamics , which could not be captured by the low-level representation . ranking is then performed on the global similarity structure to identify a set of extrema . the proposed approach is unsupervised so learning stage is eliminated . experimental results on public datasets demonstrates the effectiveness of exploiting such extrema in identifying salient regions in various crowd scenarios that exhibit crowding , local irregular motion , and unique motion areas such as sources and sinks .", "topics": ["high- and low-level", "unsupervised learning"]}
{"title": "an iterative convolutional neural network algorithm improves electron microscopy image segmentation", "abstract": "to build the connectomics map of the brain , we developed a new algorithm that can automatically refine the membrane detection probability maps ( mdpm ) generated to perform automatic segmentation of electron microscopy ( em ) images . to achieve this , we executed supervised training of a convolutional neural network to recover the removed center pixel label of patches sampled from a mdpm . mdpm can be generated from other machine learning based algorithms recognizing whether a pixel in an image corresponds to the cell membrane . by iteratively applying this network over mdpm for multiple rounds , we were able to significantly improve membrane segmentation results .", "topics": ["image segmentation", "pixel"]}
{"title": "unsupervised grammar induction with depth-bounded pcfg", "abstract": "there has been recent interest in applying cognitively or empirically motivated bounds on recursion depth to limit the search space of grammar induction models ( ponvert et al . , 2011 ; noji and johnson , 2016 ; shain et al . , 2016 ) . this work extends this depth-bounding approach to probabilistic context-free grammar induction ( db-pcfg ) , which has a smaller parameter space than hierarchical sequence models , and therefore more fully exploits the space reductions of depth-bounding . results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy . moreover , gram- mars acquired from this model demonstrate a consistent use of category labels , something which has not been demonstrated by other acquisition models .", "topics": ["parsing"]}
{"title": "a convergent online single time scale actor critic algorithm", "abstract": "actor-critic based approaches were among the first to address reinforcement learning in a general setting . recently , these algorithms have gained renewed interest due to their generality , good convergence properties , and possible biological relevance . in this paper , we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward . linear function approximation is used by the critic in order estimate the value function , and the temporal difference signal , which is passed from the critic to the actor . the main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale , while in most current convergence proofs they are required to have very different time scales in order to converge . moreover , the same temporal difference signal is used to update the parameters of both the actor and the critic . a limitation of the proposed approach , compared to results available for two time scale convergence , is that convergence is guaranteed only to a neighborhood of an optimal value , rather to an optimal value itself . the single time scale and identical temporal difference signal used by the actor and the critic , may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain .", "topics": ["reinforcement learning", "relevance"]}
{"title": "multi-value rule sets", "abstract": "we present the multi-value rule set ( mars ) model for interpretable classification with feature efficient presentations . mars introduces a more generalized form of association rules that allows multiple values in a condition . rules of this form are more concise than traditional single-valued rules in capturing and describing patterns in data . mars mitigates the problem of dealing with continuous features and high-cardinality categorical features faced by rule-based models . our formulation also pursues a higher efficiency of feature utilization , which reduces the cognitive load to understand the decision process . we propose an efficient inference method for learning a maximum a posteriori model , incorporating theoretically grounded bounds to iteratively reduce the search space to improve search efficiency . experiments with synthetic and real-world data demonstrate that mars models have significantly smaller complexity and fewer features , providing better interpretability while being competitive in predictive accuracy . we conducted a usability study with human subjects and results show that mars is the easiest to use compared with other competing rule-based models , in terms of the correct rate and response time . overall , mars introduces a new approach to rule-based models that balance accuracy and interpretability with feature-efficient representations .", "topics": ["value ( ethics )", "synthetic data"]}
{"title": "optimization of distributions differences for classification", "abstract": "in this paper we introduce a new classification algorithm called optimization of distributions differences ( odd ) . the algorithm aims to find a transformation from the feature space to a new space where the instances in the same class are as close as possible to one another while the gravity centers of these classes are as far as possible from one another . this aim is formulated as a multiobjective optimization problem that is solved by a hybrid of an evolutionary strategy and the quasi-newton method . the choice of the transformation function is flexible and could be any continuous space function . we experiment with a linear and a non-linear transformation in this paper . we show that the algorithm can outperform 6 other state-of-the-art classification methods , namely naive bayes , support vector machines , linear discriminant analysis , multi-layer perceptrons , decision trees , and k-nearest neighbors , in 12 standard classification datasets . our results show that the method is less sensitive to the imbalanced number of instances comparing to these methods . we also show that odd maintains its performance better than other classification methods in these datasets , hence , offers a better generalization ability .", "topics": ["feature vector", "support vector machine"]}
{"title": "a joint model for question answering and question generation", "abstract": "we propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents . the proposed model uses a sequence-to-sequence framework that encodes the document and generates a question ( answer ) given an answer ( question ) . significant improvement in model performance is observed empirically on the squad corpus , confirming our hypothesis that the model benefits from jointly learning to perform both tasks . we believe the joint model 's novelty offers a new perspective on machine comprehension beyond architectural engineering , and serves as a first step towards autonomous information seeking .", "topics": ["autonomous car"]}
{"title": "reinforcement learning approach for parallelization in filters aggregation based feature selection algorithms", "abstract": "one of the classical problems in machine learning and data mining is feature selection . a feature selection algorithm is expected to be quick , and at the same time it should show high performance . melif algorithm effectively solves this problem using ensembles of ranking filters . this article describes two different ways to improve melif algorithm performance with parallelization . experiments show that proposed schemes significantly improves algorithm performance and increase feature selection quality .", "topics": ["data mining", "reinforcement learning"]}
{"title": "snapshot difference imaging using time-of-flight sensors", "abstract": "computational photography encompasses a diversity of imaging techniques , but one of the core operations performed by many of them is to compute image differences . an intuitive approach to computing such differences is to capture several images sequentially and then process them jointly . usually , this approach leads to artifacts when recording dynamic scenes . in this paper , we introduce a snapshot difference imaging approach that is directly implemented in the sensor hardware of emerging time-of-flight cameras . with a variety of examples , we demonstrate that the proposed snapshot difference imaging technique is useful for direct-global illumination separation , for direct imaging of spatial and temporal image gradients , for direct depth edge imaging , and more .", "topics": ["pixel"]}
{"title": "generating sequences with recurrent neural networks", "abstract": "this paper shows how long short-term memory recurrent neural networks can be used to generate complex sequences with long-range structure , simply by predicting one data point at a time . the approach is demonstrated for text ( where the data are discrete ) and online handwriting ( where the data are real-valued ) . it is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence . the resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "clustering based feature learning on variable stars", "abstract": "the success of automatic classification of variable stars strongly depends on the lightcurve representation . usually , lightcurves are represented as a vector of many statistical descriptors designed by astronomers called features . these descriptors commonly demand significant computational power to calculate , require substantial research effort to develop and do not guarantee good performance on the final classification task . today , lightcurve representation is not entirely automatic ; algorithms that extract lightcurve features are designed by humans and must be manually tuned up for every survey . the vast amounts of data that will be generated in future surveys like lsst mean astronomers must develop analysis pipelines that are both scalable and automated . recently , substantial efforts have been made in the machine learning community to develop methods that prescind from expert-designed and manually tuned features for features that are automatically learned from data . in this work we present what is , to our knowledge , the first unsupervised feature learning algorithm designed for variable stars . our method first extracts a large number of lightcurve subsequences from a given set of photometric data , which are then clustered to find common local patterns in the time series . representatives of these patterns , called exemplars , are then used to transform lightcurves of a labeled set into a new representation that can then be used to train an automatic classifier . the proposed algorithm learns the features from both labeled and unlabeled lightcurves , overcoming the bias generated when the learning process is done only with labeled data . we test our method on macho and ogle datasets ; the results show that the classification performance we achieve is as good and in some cases better than the performance achieved using traditional features , while the computational cost is significantly lower .", "topics": ["feature learning", "statistical classification"]}
{"title": "note on evolution and forecasting of requirements : communications example", "abstract": "combinatorial evolution and forecasting of system requirements is examined . the morphological model is used for a hierarchical requirements system ( i.e . , system parts , design alternatives for the system parts , ordinal estimates for the alternatives ) . a set of system changes involves changes of the system structure , component alternatives and their estimates . the composition process of the forecast is based on combinatorial synthesis ( knapsack problem , multiple choice problem , hierarchical morphological design ) . an illustrative numerical example for four-phase evolution and forecasting of requirements to communications is described .", "topics": ["numerical analysis"]}
{"title": "identifying cancer subtypes in glioblastoma by combining genomic , transcriptomic and epigenomic data", "abstract": "we present a nonparametric bayesian method for disease subtype discovery in multi-dimensional cancer data . our method can simultaneously analyse a wide range of data types , allowing for both agreement and disagreement between their underlying clustering structure . it includes feature selection and infers the most likely number of disease subtypes , given the data . we apply the method to 277 glioblastoma samples from the cancer genome atlas , for which there are gene expression , copy number variation , methylation and microrna data . we identify 8 distinct consensus subtypes and study their prognostic value for death , new tumour events , progression and recurrence . the consensus subtypes are prognostic of tumour recurrence ( log-rank p-value of $ 3.6 \\times 10^ { -4 } $ after correction for multiple hypothesis tests ) . this is driven principally by the methylation data ( log-rank p-value of $ 2.0 \\times 10^ { -3 } $ ) but the effect is strengthened by the other 3 data types , demonstrating the value of integrating multiple data types . of particular note is a subtype of 47 patients characterised by very low levels of methylation . this subtype has very low rates of tumour recurrence and no new events in 10 years of follow up . we also identify a small gene expression subtype of 6 patients that shows particularly poor survival outcomes . additionally , we note a consensus subtype that showly a highly distinctive data signature and suggest that it is therefore a biologically distinct subtype of glioblastoma . the code is available from https : //sites.google.com/site/multipledatafusion/", "topics": ["cluster analysis"]}
{"title": "non-local graph-based prediction for reversible data hiding in images", "abstract": "reversible data hiding ( rdh ) is desirable in applications where both the hidden message and the cover medium need to be recovered without loss . among many rdh approaches is prediction-error expansion ( pee ) , containing two steps : i ) prediction of a target pixel value , and ii ) embedding according to the value of prediction-error . in general , higher prediction performance leads to larger embedding capacity and/or lower signal distortion . leveraging on recent advances in graph signal processing ( gsp ) , we pose pixel prediction as a graph-signal restoration problem , where the appropriate edge weights of the underlying graph are computed using a similar patch searched in a semi-local neighborhood . specifically , for each candidate patch , we first examine eigenvalues of its structure tensor to estimate its local smoothness . if sufficiently smooth , we pose a maximum a posteriori ( map ) problem using either a quadratic laplacian regularizer or a graph total variation ( gtv ) term as signal prior . while the map problem using the first prior has a closed-form solution , we design an efficient algorithm for the second prior using alternating direction method of multipliers ( admm ) with nested proximal gradient descent . experimental results show that with better quality gsp-based prediction , at low capacity the visual quality of the embedded image exceeds state-of-the-art methods noticeably .", "topics": ["gradient descent", "gradient"]}
{"title": "a two-phase safe vehicle routing and scheduling problem : formulations and solution algorithms", "abstract": "we propose a two phase time dependent vehicle routing and scheduling optimization model that identifies the safest routes , as a substitute for the classical objectives given in the literature such as shortest distance or travel time , through ( 1 ) avoiding recurring congestions , and ( 2 ) selecting routes that have a lower probability of crash occurrences and non-recurring congestion caused by those crashes . in the first phase , we solve a mixed-integer programming model which takes the dynamic speed variations into account on a graph of roadway networks according to the time of day , and identify the routing of a fleet and sequence of nodes on the safest feasible paths . second phase considers each route as an independent transit path ( fixed route with fixed node sequences ) , and tries to avoid congestion by rescheduling the departure times of each vehicle from each node , and by adjusting the sub-optimal speed on each arc . a modified simulated annealing ( sa ) algorithm is formulated to solve both complex models iteratively , which is found to be capable of providing solutions in a considerably short amount of time .", "topics": ["simulation"]}
{"title": "boosting of image denoising algorithms", "abstract": "in this paper we propose a generic recursive algorithm for improving image denoising methods . given the initial denoised image , we suggest repeating the following `` sos '' procedure : ( i ) ( s ) trengthen the signal by adding the previous denoised image to the degraded input image , ( ii ) ( o ) perate the denoising method on the strengthened image , and ( iii ) ( s ) ubtract the previous denoised image from the restored signal-strengthened outcome . the convergence of this process is studied for the k-svd image denoising and related algorithms . still in the context of k-svd image denoising , we introduce an interesting interpretation of the sos algorithm as a technique for closing the gap between the local patch-modeling and the global restoration task , thereby leading to improved performance . in a quest for the theoretical origin of the sos algorithm , we provide a graph-based interpretation of our method , where the sos recursive update effectively minimizes a penalty function that aims to denoise the image , while being regularized by the graph laplacian . we demonstrate the sos boosting algorithm for several leading denoising methods ( k-svd , nlm , bm3d , and epll ) , showing tendency to further improve denoising performance .", "topics": ["noise reduction"]}
{"title": "towards end-to-end reinforcement learning of dialogue agents for information access", "abstract": "this paper proposes kb-infobot -- a multi-turn dialogue agent which helps users search knowledge bases ( kbs ) without composing complicated queries . such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge . previous systems achieved this by issuing a symbolic query to the kb to retrieve entries based on their attributes . however , such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents . in this paper , we address this limitation by replacing symbolic queries with an induced `` soft '' posterior distribution over the kb that indicates which entities the user is interested in . integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users . we also present a fully neural end-to-end agent , trained entirely from user feedback , and discuss its application towards personalized dialogue agents . the source code is available at https : //github.com/miulab/kb-infobot .", "topics": ["reinforcement learning", "simulation"]}
{"title": "recurrent computations for visual pattern completion", "abstract": "making inferences from partial information constitutes a critical aspect of cognition . during visual perception , pattern completion enables recognition of poorly visible or occluded objects . we combined psychophysics , physiology and computational models to test the hypothesis that pattern completion is implemented by recurrent computations and present three pieces of evidence that are consistent with this hypothesis . first , subjects robustly recognized objects even when rendered < 10 % visible , but recognition was largely impaired when processing was interrupted by backward masking . second , invasive physiological responses along the human ventral cortex exhibited visually selective responses to partially visible objects that were delayed compared to whole objects , suggesting the need for additional computations . these physiological delays were correlated with the effects of backward masking . third , state-of-the-art feed-forward computational architectures were not robust to partial visibility . however , recognition performance was recovered when the model was augmented with attractor-based recurrent connectivity . these results provide a strong argument of plausibility for the role of recurrent computations in making visual inferences from partial information .", "topics": ["recurrent neural network", "computation"]}
{"title": "deal or no deal ? end-to-end learning for negotiation dialogues", "abstract": "much of human dialogue occurs in semi-cooperative settings , where agents with different goals attempt to agree on common decisions . negotiations require complex communication and reasoning skills , but success is easy to measure , making this an interesting task for ai . we gather a large dataset of human-human negotiations on a multi-issue bargaining task , where agents who can not observe each other 's reward functions must reach an agreement ( or a deal ) via natural language dialogue . for the first time , we show it is possible to train end-to-end models for negotiation , which must learn both linguistic and reasoning skills with no annotated dialogue states . we also introduce dialogue rollouts , in which the model plans ahead by simulating possible complete continuations of the conversation , and find that this technique dramatically improves performance . our code and dataset are publicly available ( https : //github.com/facebookresearch/end-to-end-negotiator ) .", "topics": ["natural language", "simulation"]}
{"title": "open loop hyperparameter optimization and determinantal point processes", "abstract": "driven by the need for parallelizable hyperparameter optimization methods , this paper studies \\emph { open loop } search methods : sequences that are predetermined and can be generated before a single configuration is evaluated . examples include grid search , uniform random search , low discrepancy sequences , and other sampling distributions . in particular , we propose the use of $ k $ -determinantal point processes in hyperparameter optimization via random search . compared to conventional uniform random search where hyperparameter settings are sampled independently , a $ k $ -dpp promotes diversity . we describe an approach that transforms hyperparameter search spaces for efficient use with a $ k $ -dpp . in addition , we introduce a novel metropolis-hastings algorithm which can sample from $ k $ -dpps defined over any space from which uniform samples can be drawn , including spaces with a mixture of discrete and continuous dimensions or tree structure . our experiments show significant benefits in realistic scenarios with a limited budget for training supervised learners , whether in serial or parallel .", "topics": ["supervised learning"]}
{"title": "cern : confidence-energy recurrent network for group activity recognition", "abstract": "this work is about recognizing human activities occurring in videos at distinct semantic levels , including individual actions , interactions , and group activities . the recognition is realized using a two-level hierarchy of long short-term memory ( lstm ) networks , forming a feed-forward deep architecture , which can be trained end-to-end . in comparison with existing architectures of lstms , we make two key contributions giving the name to our approach as confidence-energy recurrent network -- cern . first , instead of using the common softmax layer for prediction , we specify a novel energy layer ( el ) for estimating the energy of our predictions . second , rather than finding the common minimum-energy class assignment , which may be numerically unstable under uncertainty , we specify that the el additionally computes the p-values of the solutions , and in this way estimates the most confident energy minimum . the evaluation on the collective activity and volleyball datasets demonstrates : ( i ) advantages of our two contributions relative to the common softmax and energy-minimization formulations and ( ii ) a superior performance relative to the state-of-the-art approaches .", "topics": ["recurrent neural network", "interaction"]}
{"title": "changing model behavior at test-time using reinforcement learning", "abstract": "machine learning models are often used at test-time subject to constraints and trade-offs not present at training-time . for example , a computer vision model operating on an embedded device may need to perform real-time inference , or a translation model operating on a cell phone may wish to bound its average compute time in order to be power-efficient . in this work we describe a mixture-of-experts model and show how to change its test-time resource-usage on a per-input basis using reinforcement learning . we test our method on a small mnist-based example .", "topics": ["reinforcement learning", "computer vision"]}
{"title": "an evolutionary computational based approach towards automatic image registration", "abstract": "image registration is a key component of various image processing operations which involve the analysis of different image data sets . automatic image registration domains have witnessed the application of many intelligent methodologies over the past decade ; however inability to properly model object shape as well as contextual information had limited the attainable accuracy . in this paper , we propose a framework for accurate feature shape modeling and adaptive resampling using advanced techniques such as vector machines , cellular neural network ( cnn ) , sift , coreset , and cellular automata . cnn has found to be effective in improving feature matching as well as resampling stages of registration and complexity of the approach has been considerably reduced using corset optimization the salient features of this work are cellular neural network approach based sift feature point optimisation , adaptive resampling and intelligent object modelling . developed methodology has been compared with contemporary methods using different statistical measures . investigations over various satellite images revealed that considerable success was achieved with the approach . system has dynamically used spectral and spatial information for representing contextual knowledge using cnn-prolog approach . methodology also illustrated to be effective in providing intelligent interpretation and adaptive resampling .", "topics": ["image processing", "mathematical optimization"]}
{"title": "representation stability as a regularizer for improved text analytics transfer learning", "abstract": "although neural networks are well suited for sequential transfer learning tasks , the catastrophic forgetting problem hinders proper integration of prior knowledge . in this work , we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training . we demonstrate our approach on a twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks . we show that our technique outperforms networks fine-tuned to the target task . additionally , we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning . surprisingly , we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance . our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation . in particular , for the semeval 2016 task 4 subtask a ( nakov et al . , 2016 ) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data .", "topics": ["recurrent neural network"]}
{"title": "gitgraph - architecture search space creation through frequent computational subgraph mining", "abstract": "the dramatic success of deep neural networks across multiple application areas often relies on experts painstakingly designing a network architecture specific to each task . to simplify this process and make it more accessible , an emerging research effort seeks to automate the design of neural network architectures , using e.g . evolutionary algorithms or reinforcement learning or simple search in a constrained space of neural modules . considering the typical size of the search space ( e.g . $ 10^ { 10 } $ candidates for a $ 10 $ -layer network ) and the cost of evaluating a single candidate , current architecture search methods are very restricted . they either rely on static pre-built modules to be recombined for the task at hand , or they define a static hand-crafted framework within which they can generate new architectures from the simplest possible operations . in this paper , we relax these restrictions , by capitalizing on the collective wisdom contained in the plethora of neural networks published in online code repositories . concretely , we ( a ) extract and publish gitgraph , a corpus of neural architectures and their descriptions ; ( b ) we create problem-specific neural architecture search spaces , implemented as a textual search mechanism over gitgraph ; ( c ) we propose a method of identifying unique common subgraphs within the architectures solving each problem ( e.g . , image processing , reinforcement learning ) , that can then serve as modules in the newly created problem specific neural search space .", "topics": ["image processing", "reinforcement learning"]}
{"title": "fast mle computation for the dirichlet multinomial", "abstract": "given a collection of categorical data , we want to find the parameters of a dirichlet distribution which maximizes the likelihood of that data . newton 's method is typically used for this purpose but current implementations require reading through the entire dataset on each iteration . in this paper , we propose a modification which requires only a single pass through the dataset and substantially decreases running time . furthermore we analyze both theoretically and empirically the performance of the proposed algorithm , and provide an open source implementation .", "topics": ["time complexity", "iteration"]}
{"title": "hi , how can i help you ? : automating enterprise it support help desks", "abstract": "question answering is one of the primary challenges of natural language understanding . in realizing such a system , providing complex long answers to questions is a challenging task as opposed to factoid answering as the former needs context disambiguation . the different methods explored in the literature can be broadly classified into three categories namely : 1 ) classification based , 2 ) knowledge graph based and 3 ) retrieval based . individually , none of them address the need of an enterprise wide assistance system for an it support and maintenance domain . in this domain the variance of answers is large ranging from factoid to structured operating procedures ; the knowledge is present across heterogeneous data sources like application specific documentation , ticket management systems and any single technique for a general purpose assistance is unable to scale for such a landscape . to address this , we have built a cognitive platform with capabilities adopted for this domain . further , we have built a general purpose question answering system leveraging the platform that can be instantiated for multiple products , technologies in the support domain . the system uses a novel hybrid answering model that orchestrates across a deep learning classifier , a knowledge graph based context disambiguation module and a sophisticated bag-of-words search system . this orchestration performs context switching for a provided question and also does a smooth hand-off of the question to a human expert if none of the automated techniques can provide a confident answer . this system has been deployed across 675 internal enterprise it support and maintenance projects .", "topics": ["natural language"]}
{"title": "reasoning about liquids via closed-loop simulation", "abstract": "simulators are powerful tools for reasoning about a robot 's interactions with its environment . however , when simulations diverge from reality , that reasoning becomes less useful . in this paper , we show how to close the loop between liquid simulation and real-time perception . we use observations of liquids to correct errors when tracking the liquid 's state in a simulator . our results show that closed-loop simulation is an effective way to prevent large divergence between the simulated and real liquid states . as a direct consequence of this , our method can enable reasoning about liquids that would otherwise be infeasible due to large divergences , such as reasoning about occluded liquid .", "topics": ["simulation", "interaction"]}
{"title": "robust feature selection by mutual information distributions", "abstract": "mutual information is widely used in artificial intelligence , in a descriptive way , to measure the stochastic dependence of discrete random variables . in order to address questions such as the reliability of the empirical value , one must consider sample-to-population inferential approaches . this paper deals with the distribution of mutual information , as obtained in a bayesian framework by a second-order dirichlet prior distribution . the exact analytical expression for the mean and an analytical approximation of the variance are reported . asymptotic approximations of the distribution are proposed . the results are applied to the problem of selecting features for incremental learning and classification of the naive bayes classifier . a fast , newly defined method is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets . finally , a theoretical development is reported that allows one to efficiently extend the above methods to incomplete samples in an easy and effective way .", "topics": ["approximation", "artificial intelligence"]}
{"title": "the promise of premise : harnessing question premises in visual question answering", "abstract": "in this paper , we make a simple observation that questions about images often contain premises - objects and relationships implied by the question - and that reasoning about premises can help visual question answering ( vqa ) models respond more intelligently to irrelevant or previously unseen questions . when presented with a question that is irrelevant to an image , state-of-the-art vqa models will still answer purely based on learned language biases , resulting in non-sensical or even misleading answers . we note that a visual question is irrelevant to an image if at least one of its premises is false ( i.e . not depicted in the image ) . we leverage this observation to construct a dataset for question relevance prediction and explanation ( qrpe ) by searching for false premises . we train novel question relevance detection models and show that models that reason about premises consistently outperform models that do not . we also find that forcing standard vqa models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning .", "topics": ["relevance"]}
{"title": "flexible deep neural network processing", "abstract": "the recent success of deep neural networks ( dnns ) has drastically improved the state of the art for many application domains . while achieving high accuracy performance , deploying state-of-the-art dnns is a challenge since they typically require billions of expensive arithmetic computations . in addition , dnns are typically deployed in ensemble to boost accuracy performance , which further exacerbates the system requirements . this computational overhead is an issue for many platforms , e.g . data centers and embedded systems , with tight latency and energy budgets . in this article , we introduce flexible dnns ensemble processing technique , which achieves large reduction in average inference latency while incurring small to negligible accuracy drop . our technique is flexible in that it allows for dynamic adaptation between quality of results ( qor ) and execution runtime . we demonstrate the effectiveness of the technique on alexnet and resnet-50 using the imagenet dataset . this technique can also easily handle other types of networks .", "topics": ["neural networks", "computation"]}
{"title": "domain adaptations for computer vision applications", "abstract": "a basic assumption of statistical learning theory is that train and test data are drawn from the same underlying distribution . unfortunately , this assumption does n't hold in many applications . instead , ample labeled data might exist in a particular `source ' domain while inference is needed in another , `target ' domain . domain adaptation methods leverage labeled data from both domains to improve classification on unseen data in the target domain . in this work we survey domain transfer learning methods for various application domains with focus on recent work in computer vision .", "topics": ["computer vision"]}
{"title": "on the convergence of sgd training of neural networks", "abstract": "neural networks are usually trained by some form of stochastic gradient descent ( sgd ) ) . a number of strategies are in common use intended to improve sgd optimization , such as learning rate schedules , momentum , and batching . these are motivated by ideas about the occurrence of local minima at different scales , valleys , and other phenomena in the objective function . empirical results presented here suggest that these phenomena are not significant factors in sgd optimization of mlp-related objective functions , and that the behavior of stochastic gradient descent in these problems is better described as the simultaneous convergence at different rates of many , largely non-interacting subproblems", "topics": ["optimization problem", "gradient descent"]}
{"title": "an online hierarchical algorithm for extreme clustering", "abstract": "many modern clustering methods scale well to a large number of data items , n , but not to a large number of clusters , k. this paper introduces perch , a new non-greedy algorithm for online hierarchical clustering that scales to both massive n and k -- a problem setting we term extreme clustering . our algorithm efficiently routes new data points to the leaves of an incrementally-built tree . motivated by the desire for both accuracy and speed , our approach performs tree rotations for the sake of enhancing subtree purity and encouraging balancedness . we prove that , under a natural separability assumption , our non-greedy algorithm will produce trees with perfect dendrogram purity regardless of online data arrival order . our experiments demonstrate that perch constructs more accurate trees than other tree-building clustering algorithms and scales well with both n and k , achieving a higher quality clustering than the strongest flat clustering competitor in nearly half the time .", "topics": ["cluster analysis"]}
{"title": "the feeling of success : does touch sensing help predict grasp outcomes ?", "abstract": "a successful grasp requires careful balancing of the contact forces . deducing whether a particular grasp will be successful from indirect measurements , such as vision , is therefore quite challenging , and direct sensing of contacts through touch sensing provides an appealing avenue toward more successful and consistent robotic grasping . however , in order to fully evaluate the value of touch sensing for grasp outcome prediction , we must understand how touch sensing can influence outcome prediction accuracy when combined with other modalities . doing so using conventional model-based techniques is exceptionally difficult . in this work , we investigate the question of whether touch sensing aids in predicting grasp outcomes within a multimodal sensing framework that combines vision and touch . to that end , we collected more than 9,000 grasping trials using a two-finger gripper equipped with gelsight high-resolution tactile sensors on each finger , and evaluated visuo-tactile deep neural network models to directly predict grasp outcomes from either modality individually , and from both modalities together . our experimental results indicate that incorporating tactile readings substantially improve grasping performance .", "topics": ["sensor", "robot"]}
{"title": "stabilizing dual-energy x-ray computed tomography reconstructions using patch-based regularization", "abstract": "recent years have seen growing interest in exploiting dual- and multi-energy measurements in computed tomography ( ct ) in order to characterize material properties as well as object shape . material characterization is performed by decomposing the scene into constitutive basis functions , such as compton scatter and photoelectric absorption functions . while well motivated physically , the joint recovery of the spatial distribution of photoelectric and compton properties is severely complicated by the fact that the data are several orders of magnitude more sensitive to compton scatter coefficients than to photoelectric absorption , so small errors in compton estimates can create large artifacts in the photoelectric estimate . to address these issues , we propose a model-based iterative approach which uses patch-based regularization terms to stabilize inversion of photoelectric coefficients , and solve the resulting problem though use of computationally attractive alternating direction method of multipliers ( admm ) solution techniques . using simulations and experimental data acquired on a commercial scanner , we demonstrate that the proposed processing can lead to more stable material property estimates which should aid materials characterization in future dual- and multi-energy ct systems .", "topics": ["matrix regularization", "simulation"]}
{"title": "temporal autoencoding improves generative models of time series", "abstract": "restricted boltzmann machines ( rbms ) are generative models which can learn useful representations from samples of a dataset in an unsupervised fashion . they have been widely employed as an unsupervised pre-training method in machine learning . rbms have been modified to model time series in two main ways : the temporal rbm stacks a number of rbms laterally and introduces temporal dependencies between the hidden layer units ; the conditional rbm , on the other hand , considers past samples of the dataset as a conditional bias and learns a representation which takes these into account . here we propose a new training method for both the trbm and the crbm , which enforces the dynamic structure of temporal datasets . we do so by treating the temporal models as denoising autoencoders , considering past frames of the dataset as corrupted versions of the present frame and minimizing the reconstruction error of the present data by the model . we call this approach temporal autoencoding . this leads to a significant improvement in the performance of both models in a filling-in-frames task across a number of datasets . the error reduction for motion capture data is 56\\ % for the crbm and 80\\ % for the trbm . taking the posterior mean prediction instead of single samples further improves the model 's estimates , decreasing the error by as much as 91\\ % for the crbm on motion capture data . we also trained the model to perform forecasting on a large number of datasets and have found ta pretraining to consistently improve the performance of the forecasts . furthermore , by looking at the prediction error across time , we can see that this improvement reflects a better representation of the dynamics of the data as opposed to a bias towards reconstructing the observed data on a short time scale .", "topics": ["time series", "unsupervised learning"]}
{"title": "expresso : a user-friendly gui for designing , training and exploring convolutional neural networks", "abstract": "with a view to provide a user-friendly interface for designing , training and developing deep learning frameworks , we have developed expresso , a gui tool written in python . expresso is built atop caffe , the open-source , prize-winning framework popularly used to develop convolutional neural networks . expresso provides a convenient wizard-like graphical interface which guides the user through various common scenarios -- data import , construction and training of deep networks , performing various experiments , analyzing and visualizing the results of these experiments . the multi-threaded nature of expresso enables concurrent execution and notification of events related to the aforementioned scenarios . the gui sub-components and inter-component interfaces in expresso have been designed with extensibility in mind . we believe expresso 's flexibility and ease of use will come in handy to researchers , newcomers and seasoned alike , in their explorations related to deep learning .", "topics": ["neural networks"]}
{"title": "gated recurrent neural tensor network", "abstract": "recurrent neural networks ( rnns ) , which are a powerful scheme for modeling temporal and sequential data need to capture long-term dependencies on datasets and represent them in hidden layers with a powerful model to capture more information from inputs . for modeling long-term dependencies in a dataset , the gating mechanism concept can help rnns remember and forget previous information . representing the hidden layers of an rnn with more expressive operations ( i.e . , tensor products ) helps it learn a more complex relationship between the current input and the previous hidden layer information . these ideas can generally improve rnn performances . in this paper , we proposed a novel rnn architecture that combine the concepts of gating mechanism and the tensor product into a single model . by combining these two concepts into a single rnn , our proposed models learn long-term dependencies by modeling with gating units and obtain more expressive and direct interaction between input and hidden layers using a tensor product on 3-dimensional array ( tensor ) weight parameters . we use long short term memory ( lstm ) rnn and gated recurrent unit ( gru ) rnn and combine them with a tensor product inside their formulations . our proposed rnns , which are called a long-short term memory recurrent neural tensor network ( lstmrntn ) and gated recurrent unit recurrent neural tensor network ( grurntn ) , are made by combining the lstm and gru rnn models with the tensor product . we conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models .", "topics": ["baseline ( configuration management )", "recurrent neural network"]}
{"title": "applications of sampling kantorovich operators to thermographic images for seismic engineering", "abstract": "in this paper , we present some applications of the multivariate sampling kantorovich operators $ s_w $ to seismic engineering . the mathematical theory of these operators , both in the space of continuous functions and in orlicz spaces , show how it is possible to approximate/reconstruct multivariate signals , such as images . in particular , to obtain applications for thermographic images a mathematical algorithm is developed using matlab and matrix calculus . the setting of orlicz spaces is important since allow us to reconstruct not necessarily continuous signals by means of $ s_w $ . the reconstruction of thermographic images of buildings by our sampling kantorovich algorithm allow us to obtain models for the simulation of the behavior of structures under seismic action . we analyze a real world case study in term of structural analysis and we compare the behavior of the building under seismic action using various models .", "topics": ["sampling ( signal processing )", "simulation"]}
{"title": "debugging machine learning tasks", "abstract": "unlike traditional programs ( such as operating systems or word processors ) which have large amounts of code , machine learning tasks use programs with relatively small amounts of code ( written in machine learning libraries ) , but voluminous amounts of data . just like developers of traditional programs debug errors in their code , developers of machine learning tasks debug and fix errors in their data . however , algorithms and tools for debugging and fixing errors in data are less common , when compared to their counterparts for detecting and fixing errors in code . in this paper , we consider classification tasks where errors in training data lead to misclassifications in test points , and propose an automated method to find the root causes of such misclassifications . our root cause analysis is based on pearl 's theory of causation , and uses pearl 's ps ( probability of sufficiency ) as a scoring metric . our implementation , psi , encodes the computation of ps as a probabilistic program , and uses recent work on probabilistic programs and transformations on probabilistic programs ( along with gray-box models of machine learning algorithms ) to efficiently compute ps . psi is able to identify root causes of data errors in interesting data sets .", "topics": ["test set", "computation"]}
{"title": "effects of images with different levels of familiarity on eeg", "abstract": "evaluating human brain potentials during watching different images can be used for memory evaluation , information retrieving , guilty-innocent identification and examining the brain response . in this study , the effects of watching images , with different levels of familiarity , on subjects ' electroencephalogram ( eeg ) have been studied . three different groups of images with three familiarity levels of `` unfamiliar '' , `` familiar '' and `` very familiar '' have been considered for this study . eeg signals of 21 subjects ( 14 men ) were recorded . after signal acquisition , pre-processing , including noise and artifact removal , were performed on epochs of data . features , including spatial-statistical , wavelet , frequency and harmonic parameters , and also correlation between recording channels , were extracted from the data . then , we evaluated the efficiency of the extracted features by using p-value and also an orthogonal feature selection method ( combination of gram-schmitt method and fisher discriminant ratio ) for feature dimensional reduction . as the final step of feature selection , we used 'add-r take-away l ' method for choosing the most discriminative features . for data classification , including all two-class and three-class cases , we applied support vector machine ( svm ) on the extracted features . the correct classification rates ( ccr ) for `` unfamiliar-familiar '' , `` unfamiliar-very familiar '' and `` familiar-very familiar '' cases were 85.6 % , 92.6 % , and 70.6 % , respectively . the best results of classifications were obtained in pre-frontal and frontal regions of brain . also , wavelet , frequency and harmonic features were among the most discriminative features . finally , in three-class case , the best ccr was 86.8 % .", "topics": ["support vector machine", "support vector machine"]}
{"title": "turing 's red flag", "abstract": "sometime in the future we will have to deal with the impact of ai 's being mistaken for humans . for this reason , i propose that any autonomous system should be designed so that it is unlikely to be mistaken for anything besides an autonomous sysem , and should identify itself at the start of any interaction with another agent .", "topics": ["artificial intelligence"]}
{"title": "unperturbed : spectral analysis beyond davis-kahan", "abstract": "classical matrix perturbation results , such as weyl 's theorem for eigenvalues and the davis-kahan theorem for eigenvectors , are general purpose . these classical bounds are tight in the worst case , but in many settings sub-optimal in the typical case . in this paper , we present perturbation bounds which consider the nature of the perturbation and its interaction with the unperturbed structure in order to obtain significant improvements over the classical theory in many scenarios , such as when the perturbation is random . we demonstrate the utility of these new results by analyzing perturbations in the stochastic blockmodel where we derive much tighter bounds than provided by the classical theory . we use our new perturbation theory to show that a very simple and natural clustering algorithm -- whose analysis was difficult using the classical tools -- nevertheless recovers the communities of the blockmodel exactly even in very sparse graphs .", "topics": ["cluster analysis", "sparse matrix"]}
{"title": "online representation learning in recurrent neural language models", "abstract": "we investigate an extension of continuous online learning in recurrent neural network language models . the model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction . the initial experiments give promising results , indicating that the method is able to increase language modelling accuracy , while also decreasing the parameters needed to store the model along with the computation required at each step .", "topics": ["recurrent neural network", "computation"]}
{"title": "on the exact relationship between the denoising function and the data distribution", "abstract": "we prove an exact relationship between the optimal denoising function and the data distribution in the case of additive gaussian noise , showing that denoising implicitly models the structure of data allowing it to be exploited in the unsupervised learning of representations . this result generalizes a known relationship [ 2 ] , which is valid only in the limit of small corruption noise .", "topics": ["noise reduction", "unsupervised learning"]}
{"title": "auto-encoding variational bayes", "abstract": "how can we perform efficient inference and learning in directed probabilistic models , in the presence of continuous latent variables with intractable posterior distributions , and large datasets ? we introduce a stochastic variational inference and learning algorithm that scales to large datasets and , under some mild differentiability conditions , even works in the intractable case . our contributions is two-fold . first , we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods . second , we show that for i.i.d . datasets with continuous latent variables per datapoint , posterior inference can be made especially efficient by fitting an approximate inference model ( also called a recognition model ) to the intractable posterior using the proposed lower bound estimator . theoretical advantages are reflected in experimental results .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "word sense disambiguation via high order of learning in complex networks", "abstract": "complex networks have been employed to model many real systems and as a modeling tool in a myriad of applications . in this paper , we use the framework of complex networks to the problem of supervised classification in the word disambiguation task , which consists in deriving a function from the supervised ( or labeled ) training data of ambiguous words . traditional supervised data classification takes into account only topological or physical features of the input data . on the other hand , the human ( animal ) brain performs both low- and high-level orders of learning and it has facility to identify patterns according to the semantic meaning of the input data . in this paper , we apply a hybrid technique which encompasses both types of learning in the field of word sense disambiguation and show that the high-level order of learning can really improve the accuracy rate of the model . this evidence serves to demonstrate that the internal structures formed by the words do present patterns that , generally , can not be correctly unveiled by only traditional techniques . finally , we exhibit the behavior of the model for different weights of the low- and high-level classifiers by plotting decision boundaries . this study helps one to better understand the effectiveness of the model .", "topics": ["test set", "supervised learning"]}
{"title": "sliced wasserstein generative models", "abstract": "in the paper , we introduce a model of sliced optimal transport ( sot ) , which measures the distribution affinity with sliced wasserstein distance ( swd ) . since swd enjoys the property of factorizing high-dimensional joint distributions into their multiple one-dimensional marginal distributions , its dual and primal forms can be approximated easier compared to wasserstein distance ( wd ) . thus , we propose two types of differentiable sot blocks to equip modern generative frameworks -- -auto-encoders ( aes ) and generative adversarial networks ( gans ) -- -with the primal and dual forms of swd . the superiority of our swae and swgan over the state-of-the-art generative models is studied both qualitatively and quantitatively on standard benchmarks .", "topics": ["generative model", "loss function"]}
{"title": "latent tree models for hierarchical topic detection", "abstract": "we present a novel method for hierarchical topic detection where topics are obtained by clustering documents in multiple ways . specifically , we model document collections using a class of graphical models called hierarchical latent tree models ( hltms ) . the variables at the bottom level of an hltm are observed binary variables that represent the presence/absence of words in a document . the variables at other levels are binary latent variables , with those at the lowest latent level representing word co-occurrence patterns and those at higher levels representing co-occurrence of patterns at the level below . each latent variable gives a soft partition of the documents , and document clusters in the partitions are interpreted as topics . latent variables at high levels of the hierarchy capture long-range word co-occurrence patterns and hence give thematically more general topics , while those at low levels of the hierarchy capture short-range word co-occurrence patterns and give thematically more specific topics . unlike lda-based topic models , hltms do not refer to a document generation process and use word variables instead of token variables . they use a tree structure to model the relationships between topics and words , which is conducive to the discovery of meaningful topics and topic hierarchies .", "topics": ["graphical model", "cluster analysis"]}
{"title": "stochastic downsampling for cost-adjustable inference and improved regularization in convolutional networks", "abstract": "it is desirable to train convolutional networks ( cnns ) to run more efficiently during inference . in many cases however , the computational budget that the system has for inference can not be known beforehand during training , or the inference budget is dependent on the changing real-time resource availability . thus , it is inadequate to train just inference-efficient cnns , whose inference costs are not adjustable and can not adapt to varied inference budgets . we propose a novel approach for cost-adjustable inference in cnns - stochastic downsampling point ( sdpoint ) . during training , sdpoint applies feature map downsampling to a random point in the layer hierarchy , with a random downsampling ratio . the different stochastic downsampling configurations known as sdpoint instances ( of the same model ) have computational costs different from each other , while being trained to minimize the same prediction loss . sharing network parameters across different instances provides significant regularization boost . during inference , one may handpick a sdpoint instance that best fits the inference budget . the effectiveness of sdpoint , as both a cost-adjustable inference approach and a regularizer , is validated through extensive experiments on image classification .", "topics": ["matrix regularization", "computer vision"]}
{"title": "generalized fisher score for feature selection", "abstract": "fisher score is one of the most widely used supervised feature selection methods . however , it selects each feature independently according to their scores under the fisher criterion , which leads to a suboptimal subset of features . in this paper , we present a generalized fisher score to jointly select features . it aims at finding an subset of features , which maximize the lower bound of traditional fisher score . the resulting feature selection problem is a mixed integer programming , which can be reformulated as a quadratically constrained linear programming ( qclp ) . it is solved by cutting plane algorithm , in each iteration of which a multiple kernel learning problem is solved alternatively by multivariate ridge regression and projected gradient descent . experiments on benchmark data sets indicate that the proposed method outperforms fisher score as well as many other state-of-the-art feature selection methods .", "topics": ["gradient descent", "iteration"]}
{"title": "astronomical image denoising using dictionary learning", "abstract": "astronomical images suffer a constant presence of multiple defects that are consequences of the intrinsic properties of the acquisition equipments , and atmospheric conditions . one of the most frequent defects in astronomical imaging is the presence of additive noise which makes a denoising step mandatory before processing data . during the last decade , a particular modeling scheme , based on sparse representations , has drawn the attention of an ever growing community of researchers . sparse representations offer a promising framework to many image and signal processing tasks , especially denoising and restoration applications . at first , the harmonics , wavelets , and similar bases and overcomplete representations have been considered as candidate domains to seek the sparsest representation . a new generation of algorithms , based on data-driven dictionaries , evolved rapidly and compete now with the off-the-shelf fixed dictionaries . while designing a dictionary beforehand leans on a guess of the most appropriate representative elementary forms and functions , the dictionary learning framework offers to construct the dictionary upon the data themselves , which provides us with a more flexible setup to sparse modeling and allows to build more sophisticated dictionaries . in this paper , we introduce the centered dictionary learning ( cdl ) method and we study its performances for astronomical image denoising . we show how cdl outperforms wavelet or classic dictionary learning denoising techniques on astronomical images , and we give a comparison of the effect of these different algorithms on the photometry of the denoised images .", "topics": ["noise reduction", "dictionary"]}
{"title": "robust subspace clustering via smoothed rank approximation", "abstract": "matrix rank minimizing subject to affine constraints arises in many application areas , ranging from signal processing to machine learning . nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions . however , for many real-world applications , nuclear norm approximation to the rank function can only produce a result far from the optimum . to seek a solution of higher accuracy than the nuclear norm , in this paper , we propose a rank approximation based on logarithm-determinant . we consider using this rank approximation for subspace clustering application . our framework can model different kinds of errors and noise . effective optimization strategy is developed with theoretical guarantee to converge to a stationary point . the proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms .", "topics": ["cluster analysis", "mathematical optimization"]}
{"title": "gaussian process domain experts for model adaptation in facial behavior analysis", "abstract": "we present a novel approach for supervised domain adaptation that is based upon the probabilistic framework of gaussian processes ( gps ) . specifically , we introduce domain-specific gps as local experts for facial expression classification from face images . the adaptation of the classifier is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts . furthermore , in contrast to existing adaptation approaches , we also learn a target expert from available target data solely . then , a single and confident classifier is obtained by combining the predictions from multiple experts based on their confidence . learning of the model is efficient and requires no retraining/reweighting of the source classifiers . we evaluate the proposed approach on two publicly available datasets for multi-class ( multipie ) and multi-label ( disfa ) facial expression classification . to this end , we perform adaptation of two contextual factors : 'where ' ( view ) and 'who ' ( subject ) . we show in our experiments that the proposed approach consistently outperforms both source and target classifiers , while using as few as 30 target examples . it also outperforms the state-of-the-art approaches for supervised domain adaptation .", "topics": ["statistical classification"]}
{"title": "combinatorial multi-armed bandit with probabilistically triggered arms : a case with bounded regret", "abstract": "in this paper , we study the combinatorial multi-armed bandit problem ( cmab ) with probabilistically triggered arms ( ptas ) . under the assumption that the arm triggering probabilities ( atps ) are positive for all arms , we prove that a class of upper confidence bound ( ucb ) policies , named combinatorial ucb with exploration rate $ \\kappa $ ( cucb- $ \\kappa $ ) , and combinatorial thompson sampling ( cts ) , which estimates the expected states of the arms via thompson sampling , achieve bounded regret . in addition , we prove that cucb- $ 0 $ and cts incur $ o ( \\sqrt { t } ) $ gap-independent regret . these results improve the results in previous works , which show $ o ( \\log t ) $ gap-dependent and $ o ( \\sqrt { t\\log t } ) $ gap-independent regrets , respectively , under no assumptions on the atps . then , we numerically evaluate the performance of cucb- $ \\kappa $ and cts in a real-world movie recommendation problem , where the actions correspond to recommending a set of movies , the arms correspond to the edges between the movies and the users , and the goal is to maximize the total number of users that are attracted by at least one movie . our numerical results complement our theoretical findings on bounded regret . apart from this problem , our results also directly apply to the online influence maximization ( oim ) problem studied in numerous prior works .", "topics": ["sampling ( signal processing )", "regret ( decision theory )"]}
{"title": "predicting clinical events by combining static and dynamic information using recurrent neural networks", "abstract": "in clinical data sets we often find static information ( e.g . patient gender , blood type , etc . ) combined with sequences of data that are recorded during multiple hospital visits ( e.g . medications prescribed , tests performed , etc . ) . recurrent neural networks ( rnns ) have proven to be very successful for modelling sequences of data in many areas of machine learning . in this work we present an approach based on rnns , specifically designed for the clinical domain , that combines static and dynamic information in order to predict future events . we work with a database collected in the charit\\ ' { e } hospital in berlin that contains complete information concerning patients that underwent a kidney transplantation . after the transplantation three main endpoints can occur : rejection of the kidney , loss of the kidney and death of the patient . our goal is to predict , based on information recorded in the electronic health record of each patient , whether any of those endpoints will occur within the next six or twelve months after each visit to the clinic . we compared different types of rnns that we developed for this work , with a model based on a feedforward neural network and a logistic regression model . we found that the rnn that we developed based on gated recurrent units provides the best performance for this task . we also used the same models for a second task , i.e . , next event prediction , and found that here the model based on a feedforward neural network outperformed the other models . our hypothesis is that long-term dependencies are not as relevant in this task .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "manipulation is harder with incomplete votes", "abstract": "the coalitional manipulation ( cm ) problem has been studied extensively in the literature for many voting rules . the cm problem , however , has been studied only in the complete information setting , that is , when the manipulators know the votes of the non-manipulators . a more realistic scenario is an incomplete information setting where the manipulators do not know the exact votes of the non- manipulators but may have some partial knowledge of the votes . in this paper , we study a setting where the manipulators know a partial order for each voter that is consistent with the vote of that voter . in this setting , we introduce and study two natural computational problems - ( 1 ) weak manipulation ( wm ) problem where the manipulators wish to vote in a way that makes their preferred candidate win in at least one extension of the partial votes of the non-manipulators ; ( 2 ) strong manipulation ( sm ) problem where the manipulators wish to vote in a way that makes their preferred candidate win in all possible extensions of the partial votes of the non-manipulators . we study the computational complexity of the wm and the sm problems for commonly used voting rules such as plurality , veto , k-approval , k-veto , maximin , copeland , and bucklin . our key finding is that , barring a few exceptions , manipulation becomes a significantly harder problem in the setting of incomplete votes .", "topics": ["computational complexity theory"]}
{"title": "pac-bayesian analysis of martingales and multiarmed bandits", "abstract": "we present two alternative ways to apply pac-bayesian analysis to sequences of dependent random variables . the first is based on a new lemma that enables to bound expectations of convex functions of certain dependent random variables by expectations of the same functions of independent bernoulli random variables . this lemma provides an alternative tool to hoeffding-azuma inequality to bound concentration of martingale values . our second approach is based on integration of hoeffding-azuma inequality with pac-bayesian analysis . we also introduce a way to apply pac-bayesian analysis in situation of limited feedback . we combine the new tools to derive pac-bayesian generalization and regret bounds for the multiarmed bandit problem . although our regret bound is not yet as tight as state-of-the-art regret bounds based on other well-established techniques , our results significantly expand the range of potential applications of pac-bayesian analysis and introduce a new analysis tool to reinforcement learning and many other fields , where martingales and limited feedback are encountered .", "topics": ["regret ( decision theory )", "reinforcement learning"]}
{"title": "memoisation : purely , left-recursively , and with ( continuation passing ) style", "abstract": "memoisation , or tabling , is a well-known technique that yields large improvements in the performance of some recursive computations . tabled resolution in prologs such as xsb and b-prolog can transform so called left-recursive predicates from non-terminating computations into finite and well-behaved ones . in the functional programming literature , memoisation has usually been implemented in a way that does not handle left-recursion , requiring supplementary mechanisms to prevent non-termination . a notable exception is johnson 's ( 1995 ) continuation passing approach in scheme . this , however , relies on mutation of a memo table data structure and coding in explicit continuation passing style . we show how johnson 's approach can be implemented purely functionally in a modern , strongly typed functional language ( ocaml ) , presented via a monadic interface that hides the implementation details , yet providing a way to return a compact represention of the memo tables at the end of the computation .", "topics": ["computation"]}
{"title": "enhanced perceptrons using contrastive biclusters", "abstract": "perceptrons are neuronal devices capable of fully discriminating linearly separable classes . although straightforward to implement and train , their applicability is usually hindered by non-trivial requirements imposed by real-world classification problems . therefore , several approaches , such as kernel perceptrons , have been conceived to counteract such difficulties . in this paper , we investigate an enhanced perceptron model based on the notion of contrastive biclusters . from this perspective , a good discriminative bicluster comprises a subset of data instances belonging to one class that show high coherence across a subset of features and high differentiation from nearest instances of the other class under the same features ( referred to as its contrastive bicluster ) . upon each local subspace associated with a pair of contrastive biclusters a perceptron is trained and the model with highest area under the receiver operating characteristic curve ( auc ) value is selected as the final classifier . experiments conducted on a range of data sets , including those related to a difficult biosignal classification problem , show that the proposed variant can be indeed very useful , prevailing in most of the cases upon standard and kernel perceptrons in terms of accuracy and auc measures .", "topics": ["kernel ( operating system )"]}
{"title": "beta-negative binomial process and poisson factor analysis", "abstract": "a beta-negative binomial ( bnb ) process is proposed , leading to a beta-gamma-poisson process , which may be viewed as a `` multi-scoop '' generalization of the beta-bernoulli process . the bnb process is augmented into a beta-gamma-gamma-poisson hierarchical structure , and applied as a nonparametric bayesian prior for an infinite poisson factor analysis model . a finite approximation for the beta process levy random measure is constructed for convenient implementation . efficient mcmc computations are performed with data augmentation and marginalization techniques . encouraging results are shown on document count matrix factorization .", "topics": ["computation"]}
{"title": "learning disentangled representations with semi-supervised deep generative models", "abstract": "variational autoencoders ( vaes ) learn representations of data by jointly training a probabilistic encoder and decoder network . typically these models encode all features of the data into a single variable . here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables . we propose to learn such representations using model architectures that generalise from standard vaes , employing a general graphical model structure in the encoder and decoder . this allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables . we further define a general objective for semi-supervised learning in this model class , which can be approximated using an importance sampling procedure . we evaluate our framework 's ability to learn disentangled representations , both by qualitative exploration of its generative capacity , and quantitative evaluation of its discriminative ability on a variety of models and datasets .", "topics": ["sampling ( signal processing )", "graphical model"]}
{"title": "a group theoretic perspective on unsupervised deep learning", "abstract": "why does deep learning work ? what representations does it capture ? how do higher-order representations emerge ? we study these questions from the perspective of group theory , thereby opening a new approach towards a theory of deep learning . one factor behind the recent resurgence of the subject is a key algorithmic step called { \\em pretraining } : first search for a good generative model for the input samples , and repeat the process one layer at a time . we show deeper implications of this simple principle , by establishing a connection with the interplay of orbits and stabilizers of group actions . although the neural networks themselves may not form groups , we show the existence of { \\em shadow } groups whose elements serve as close approximations . over the shadow groups , the pre-training step , originally introduced as a mechanism to better initialize a network , becomes equivalent to a search for features with minimal orbits . intuitively , these features are in a way the { \\em simplest } . which explains why a deep learning network learns simple features first . next , we show how the same principle , when repeated in the deeper layers , can capture higher order representations , and why representation complexity increases as the layers get deeper .", "topics": ["unsupervised learning"]}
{"title": "symmetry-invariant optimization in deep networks", "abstract": "recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the euclidean gradient based stochastic gradient descent optimization . in this work , we show that these and other commonly used deep networks , such as those which use a max-pooling and sub-sampling layer , possess more complex forms of symmetry arising from scaling based reparameterization of the network weights . we then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning . our empirical evidence based on the mnist dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates . we also show the results of training with one of the proposed weight updates on an image segmentation problem .", "topics": ["sampling ( signal processing )", "image segmentation"]}
{"title": "prosody based co-analysis for continuous recognition of coverbal gestures", "abstract": "although speech and gesture recognition has been studied extensively , all the successful attempts of combining them in the unified framework were semantically motivated , e.g . , keyword-gesture cooccurrence . such formulations inherited the complexity of natural language processing . this paper presents a bayesian formulation that uses a phenomenon of gesture and speech articulation for improving accuracy of automatic recognition of continuous coverbal gestures . the prosodic features from the speech signal were coanalyzed with the visual signal to learn the prior probability of co-occurrence of the prominent spoken segments with the particular kinematical phases of gestures . it was found that the above co-analysis helps in detecting and disambiguating visually small gestures , which subsequently improves the rate of continuous gesture recognition . the efficacy of the proposed approach was demonstrated on a large database collected from the weather channel broadcast . this formulation opens new avenues for bottom-up frameworks of multimodal integration .", "topics": ["natural language processing", "natural language"]}
{"title": "on the possible computational power of the human mind", "abstract": "the aim of this paper is to address the question : can an artificial neural network ( ann ) model be used as a possible characterization of the power of the human mind ? we will discuss what might be the relationship between such a model and its natural counterpart . a possible characterization of the different power capabilities of the mind is suggested in terms of the information contained ( in its computational complexity ) or achievable by it . such characterization takes advantage of recent results based on natural neural networks ( nnn ) and the computational power of arbitrary artificial neural networks ( ann ) . the possible acceptance of neural networks as the model of the human mind 's operation makes the aforementioned quite relevant .", "topics": ["computational complexity theory"]}
{"title": "line-circle : a geometric filter for single camera edge-based object detection", "abstract": "this paper presents a state-of-the-art approach in object detection for being applied in future slam problems . although , many slam methods are proposed to create suitable autonomy for mobile robots namely ground vehicles , they still face overconfidence and large computations during entrance to immense spaces with many landmarks . in particular , they suffer from impractical applications via sole reliance on the limited sensors like camera . proposed method claims that unmanned ground vehicles without having huge amount of database for object definition and highly advance prediction parameters can deal with incoming objects during straight motion of camera in real-time . line-circle ( lc ) filter tries to apply detection , tracking and learning to each defined experts to obtain more information for judging scene without over-calculation . in this filter , circle expert let us summarize edges in groups . the interactive feedback learning between each expert creates minimal error that fights against overwhelming landmark signs in crowded scenes without mapping . our experts basically are dependent on trust factors ' covariance with geometric definitions to ignore , emerge and compare detected landmarks . the experiment for validating the model is taken place utilizing a camera beside an imu sensor for location estimation .", "topics": ["object detection", "computation"]}
{"title": "optimistic initialization and greediness lead to polynomial time learning in factored mdps - extended version", "abstract": "in this paper we propose an algorithm for polynomial-time reinforcement learning in factored markov decision processes ( fmdps ) . the factored optimistic initial model ( foim ) algorithm , maintains an empirical model of the fmdp in a conventional way , and always follows a greedy policy with respect to its model . the only trick of the algorithm is that the model is initialized optimistically . we prove that with suitable initialization ( i ) foim converges to the fixed point of approximate value iteration ( avi ) ; ( ii ) the number of steps when the agent makes non-near-optimal decisions ( with respect to the solution of avi ) is polynomial in all relevant quantities ; ( iii ) the per-step costs of the algorithm are also polynomial . to our best knowledge , foim is the first algorithm with these properties . this extended version contains the rigorous proofs of the main theorem . a version of this paper appeared in icml'09 .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "spacetimes with semantics ( iii ) - the structure of functional knowledge representation and artificial reasoning", "abstract": "using the previously developed concepts of semantic spacetime , i explore the interpretation of knowledge representations , and their structure , as a semantic system , within the framework of promise theory . by assigning interpretations to phenomena , from observers to observed , we may approach a simple description of knowledge-based functional systems , with direct practical utility . the focus is especially on the interpretation of concepts , associative knowledge , and context awareness . the inference seems to be that most if not all of these concepts emerge from purely semantic spacetime properties , which opens the possibility for a more generalized understanding of what constitutes a learning , or even `intelligent ' system . some key principles emerge for effective knowledge representation : 1 ) separation of spacetime scales , 2 ) the recurrence of four irreducible types of association , by which intent propagates : aggregation , causation , cooperation , and similarity , 3 ) the need for discrimination of identities ( discrete ) , which is assisted by distinguishing timeline simultaneity from sequential events , and 4 ) the ability to learn ( memory ) . it is at least plausible that emergent knowledge abstraction capabilities have their origin in basic spacetime structures . these notes present a unified view of mostly well-known results ; they allow us to see information models , knowledge representations , machine learning , and semantic networking ( transport and information base ) in a common framework . the notion of `smart spaces ' thus encompasses artificial systems as well as living systems , across many different scales , e.g . smart cities and organizations .", "topics": ["artificial intelligence", "causality"]}
{"title": "noisy networks for exploration", "abstract": "we introduce noisynet , a deep reinforcement learning agent with parametric noise added to its weights , and show that the induced stochasticity of the agent 's policy can be used to aid efficient exploration . the parameters of the noise are learned with gradient descent along with the remaining network weights . noisynet is straightforward to implement and adds little computational overhead . we find that replacing the conventional exploration heuristics for a3c , dqn and dueling agents ( entropy reward and $ \\epsilon $ -greedy respectively ) with noisynet yields substantially higher scores for a wide range of atari games , in some cases advancing the agent from sub to super-human performance .", "topics": ["reinforcement learning", "gradient descent"]}
{"title": "labelbank : revisiting global perspectives for semantic segmentation", "abstract": "semantic segmentation requires a detailed labeling of image pixels by object category . information derived from local image patches is necessary to describe the detailed shape of individual objects . however , this information is ambiguous and can result in noisy labels . global inference of image content can instead capture the general semantic concepts present . we advocate that holistic inference of image concepts provides valuable information for detailed pixel labeling . we propose a generic framework to leverage holistic information in the form of a labelbank for pixel-level segmentation . we show the ability of our framework to improve semantic segmentation performance in a variety of settings . we learn models for extracting a holistic labelbank from visual cues , attributes , and/or textual descriptions . we demonstrate improvements in semantic segmentation accuracy on standard datasets across a range of state-of-the-art segmentation architectures and holistic inference approaches .", "topics": ["image segmentation", "pixel"]}
{"title": "strategies for online inference of model-based clustering in large and growing networks", "abstract": "in this paper we adapt online estimation strategies to perform model-based clustering on large networks . our work focuses on two algorithms , the first based on the saem algorithm , and the second on variational methods . these two strategies are compared with existing approaches on simulated and real data . we use the method to decipher the connexion structure of the political websphere during the us political campaign in 2008 . we show that our online em-based algorithms offer a good trade-off between precision and speed , when estimating parameters for mixture distributions in the context of random graphs .", "topics": ["calculus of variations", "cluster analysis"]}
{"title": "qualitatively characterizing neural network optimization problems", "abstract": "training neural networks involves solving large-scale non-convex optimization problems . this task has long been believed to be extremely difficult , with fear of local minima and other obstacles motivating a variety of schemes to improve optimization , such as unsupervised pretraining . however , modern neural networks are able to achieve negligible training error on complex tasks , using only direct training with stochastic gradient descent . we introduce a simple analysis technique to look for evidence that such networks are overcoming local optima . we find that , in fact , on a straight path from initialization to solution , a variety of state of the art neural networks never encounter any significant obstacles .", "topics": ["unsupervised learning", "gradient descent"]}
{"title": "compressive adaptive computational ghost imaging", "abstract": "compressive sensing is considered a huge breakthrough in signal acquisition . it allows recording an image consisting of $ n^2 $ pixels using much fewer than $ n^2 $ measurements if it can be transformed to a basis where most pixels take on negligibly small values . standard compressive sensing techniques suffer from the computational overhead needed to reconstruct an image with typical computation times between hours and days and are thus not optimal for applications in physics and spectroscopy . we demonstrate an adaptive compressive sampling technique that performs measurements directly in a sparse basis . it needs much fewer than $ n^2 $ measurements without any computational overhead , so the result is available instantly .", "topics": ["sampling ( signal processing )", "sparse matrix"]}
{"title": "neural random forests", "abstract": "given an ensemble of randomized regression trees , it is possible to restructure them as a collection of multilayered neural networks with particular connection weights . following this principle , we reformulate the random forest method of breiman ( 2001 ) into a neural network setting , and in turn propose two new hybrid procedures that we call neural random forests . both predictors exploit prior knowledge of regression trees for their architecture , have less parameters to tune than standard networks , and less restrictions on the geometry of the decision boundaries . consistency results are proved , and substantial numerical evidence is provided on both synthetic and real data sets to assess the excellent performance of our methods in a large variety of prediction problems .", "topics": ["numerical analysis", "synthetic data"]}
{"title": "graphlab : a new framework for parallel machine learning", "abstract": "designing and implementing efficient , provably correct parallel machine learning ( ml ) algorithms is challenging . existing high-level parallel abstractions like mapreduce are insufficiently expressive while low-level tools like mpi and pthreads leave ml experts repeatedly solving the same design challenges . by targeting common patterns in ml , we developed graphlab , which improves upon abstractions like mapreduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance . we demonstrate the expressiveness of the graphlab framework by designing and implementing parallel versions of belief propagation , gibbs sampling , co-em , lasso and compressed sensing . we show that using graphlab we can achieve excellent parallel performance on large scale real-world problems .", "topics": ["high- and low-level", "sparse matrix"]}
{"title": "distributed variational inference in sparse gaussian process regression and latent variable models", "abstract": "gaussian processes ( gps ) are a powerful tool for probabilistic inference over functions . they have been applied to both regression and non-linear dimensionality reduction , and offer desirable properties such as uncertainty estimates , robustness to over-fitting , and principled ways for tuning hyper-parameters . however the scalability of these models to big datasets remains an active topic of research . we introduce a novel re-parametrisation of variational inference for sparse gp regression and latent variable models that allows for an efficient distributed algorithm . this is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a map-reduce setting . we show that the inference scales well with data and computational resources , while preserving a balanced distribution of the load among the nodes . we further demonstrate the utility in scaling gaussian processes to big data . we show that gp performance improves with increasing amounts of data in regression ( on flight data with 2 million records ) and latent variable modelling ( on mnist ) . the results show that gps perform better than many common models often used for big data .", "topics": ["calculus of variations", "nonlinear system"]}
{"title": "gene-machine , a new search heuristic algorithm", "abstract": "this paper introduces gene-machine , an efficient and new search heuristic algorithm , based in the building-block hypothesis . it is inspired by natural evolution , but does not use some of the concepts present in genetic algorithms like population , mutation and generation . this heuristic exhibits good performance in comparison with genetic algorithms , and can be used to generate useful solutions to optimization and search problems .", "topics": ["heuristic"]}
{"title": "multidimensional data classification with artificial neural networks", "abstract": "multi-dimensional data classification is an important and challenging problem in many astro-particle experiments . neural networks have proved to be versatile and robust in multi-dimensional data classification . in this article we shall study the classification of gamma from the hadrons for the magic experiment . two neural networks have been used for the classification task . one is multi-layer perceptron based on supervised learning and other is self-organising map ( som ) , which is based on unsupervised learning technique . the results have been shown and the possible ways of combining these networks have been proposed to yield better and faster classification results .", "topics": ["supervised learning", "unsupervised learning"]}
{"title": "predicting organic reaction outcomes with weisfeiler-lehman network", "abstract": "the prediction of organic reaction outcomes is a fundamental problem in computational chemistry . since a reaction may involve hundreds of atoms , fully exploring the space of possible transformations is intractable . the current solution utilizes reaction templates to limit the space , but it suffers from coverage and efficiency issues . in this paper , we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur . since only a small number of atoms contribute to reaction center , we can directly enumerate candidate products . the generated candidates are scored by a weisfeiler-lehman difference network that models high-order interactions between changes occurring at nodes across the molecule . our framework outperforms the top-performing template-based approach with a 10\\ % margin , while running orders of magnitude faster . finally , we demonstrate that the model accuracy rivals the performance of domain experts .", "topics": ["interaction"]}
{"title": "exploration for multi-task reinforcement learning with deep generative models", "abstract": "exploration in multi-task reinforcement learning is critical in training agents to deduce the underlying mdp . many of the existing exploration frameworks such as $ e^3 $ , $ r_ { max } $ , thompson sampling assume a single stationary mdp and are not suitable for system identification in the multi-task setting . we present a novel method to facilitate exploration in multi-task reinforcement learning using deep generative models . we supplement our method with a low dimensional energy model to learn the underlying mdp distribution and provide a resilient and adaptive exploration signal to the agent . we evaluate our method on a new set of environments and provide intuitive interpretation of our results .", "topics": ["reinforcement learning"]}
{"title": "representational distance learning for deep neural networks", "abstract": "deep neural networks ( dnns ) provide useful models of visual representational transformations . we present a method that enables a dnn ( student ) to learn from the internal representational spaces of a reference model ( teacher ) , which could be another dnn or , in the future , a biological brain . representational spaces of the student and the teacher are characterized by representational distance matrices ( rdms ) . we propose representational distance learning ( rdl ) , a stochastic gradient descent method that drives the rdms of the student to approximate the rdms of the teacher . we demonstrate that rdl is competitive with other transfer learning techniques for two publicly available benchmark computer vision datasets ( mnist and cifar-100 ) , while allowing for architectural differences between student and teacher . by pulling the student 's rdms towards those of the teacher , rdl significantly improved visual classification performance when compared to baseline networks that did not use transfer learning . in the future , rdl may enable combined supervised training of deep neural networks using task constraints ( e.g . images and category labels ) and constraints from brain-activity measurements , so as to build models that replicate the internal representational spaces of biological brains .", "topics": ["baseline ( configuration management )", "neural networks"]}
{"title": "causal discovery in a binary exclusive-or skew acyclic model : bexsam", "abstract": "discovering causal relations among observed variables in a given data set is a major objective in studies of statistics and artificial intelligence . recently , some techniques to discover a unique causal model have been explored based on non-gaussianity of the observed data distribution . however , most of these are limited to continuous data . in this paper , we present a novel causal model for binary data and propose an efficient new approach to deriving the unique causal model governing a given binary data set under skew distributions of external binary noises . experimental evaluation shows excellent performance for both artificial and real world data sets .", "topics": ["artificial intelligence", "causality"]}
{"title": "linxgboost : extension of xgboost to generalized local linear models", "abstract": "xgboost is often presented as the algorithm that wins every ml competition . surprisingly , this is true even though predictions are piecewise constant . this might be justified in high dimensional input spaces , but when the number of features is low , a piecewise linear model is likely to perform better . xgboost was extended into linxgboost that stores at each leaf a linear model . this extension , equivalent to piecewise regularized least-squares , is particularly attractive for regression of functions that exhibits jumps or discontinuities . those functions are notoriously hard to regress . our extension is compared to the vanilla xgboost and random forest in experiments on both synthetic and real-world data sets .", "topics": ["synthetic data"]}
{"title": "learning piece-wise linear models from large scale data for ad click prediction", "abstract": "ctr prediction in real-world business is a difficult machine learning problem with large scale nonlinear sparse data . in this paper , we introduce an industrial strength solution with model named large scale piece-wise linear model ( ls-plm ) . we formulate the learning problem with $ l_1 $ and $ l_ { 2,1 } $ regularizers , leading to a non-convex and non-smooth optimization problem . then , we propose a novel algorithm to solve it efficiently , based on directional derivatives and quasi-newton method . in addition , we design a distributed system which can run on hundreds of machines parallel and provides us with the industrial scalability . ls-plm model can capture nonlinear patterns from massive sparse data , saving us from heavy feature engineering jobs . since 2012 , ls-plm has become the main ctr prediction model in alibaba 's online display advertising system , serving hundreds of millions users every day .", "topics": ["optimization problem", "nonlinear system"]}
{"title": "online learning of commission avoidant portfolio ensembles", "abstract": "we present a novel online ensemble learning strategy for portfolio selection . the new strategy controls and exploits any set of commission-oblivious portfolio selection algorithms . the strategy handles transaction costs using a novel commission avoidance mechanism . we prove a logarithmic regret bound for our strategy with respect to optimal mixtures of the base algorithms . numerical examples validate the viability of our method and show significant improvement over the state-of-the-art .", "topics": ["regret ( decision theory )", "numerical analysis"]}
{"title": "herding as a learning system with edge-of-chaos dynamics", "abstract": "herding defines a deterministic dynamical system at the edge of chaos . it generates a sequence of model states and parameters by alternating parameter perturbations with state maximizations , where the sequence of states can be interpreted as `` samples '' from an associated mrf model . herding differs from maximum likelihood estimation in that the sequence of parameters does not converge to a fixed point and differs from an mcmc posterior sampling approach in that the sequence of states is generated deterministically . herding may be interpreted as a '' perturb and map '' method where the parameter perturbations are generated using a deterministic nonlinear dynamical system rather than randomly from a gumbel distribution . this chapter studies the distinct statistical characteristics of the herding algorithm and shows that the fast convergence rate of the controlled moments may be attributed to edge of chaos dynamics . the herding algorithm can also be generalized to models with latent variables and to a discriminative learning setting . the perceptron cycling theorem ensures that the fast moment matching property is preserved in the more general framework .", "topics": ["sampling ( signal processing )", "nonlinear system"]}
{"title": "coarse to fine non-rigid registration : a chain of scale-specific neural networks for multimodal image alignment with application to remote sensing", "abstract": "we tackle here the problem of multimodal image non-rigid registration , which is of prime importance in remote sensing and medical imaging . the difficulties encountered by classical registration approaches include feature design and slow optimization by gradient descent . by analyzing these methods , we note the significance of the notion of scale . we design easy-to-train , fully-convolutional neural networks able to learn scale-specific features . once chained appropriately , they perform global registration in linear time , getting rid of gradient descent schemes by predicting directly the deformation.we show their performance in terms of quality and speed through various tasks of remote sensing multimodal image alignment . in particular , we are able to register correctly cadastral maps of buildings as well as road polylines onto rgb images , and outperform current keypoint matching methods .", "topics": ["time complexity", "gradient descent"]}
{"title": "learning an attention model in an artificial visual system", "abstract": "the human visual perception of the world is of a large fixed image that is highly detailed and sharp . however , receptor density in the retina is not uniform : a small central region called the fovea is very dense and exhibits high resolution , whereas a peripheral region around it has much lower spatial resolution . thus , contrary to our perception , we are only able to observe a very small region around the line of sight with high resolution . the perception of a complete and stable view is aided by an attention mechanism that directs the eyes to the numerous points of interest within the scene . the eyes move between these targets in quick , unconscious movements , known as `` saccades '' . once a target is centered at the fovea , the eyes fixate for a fraction of a second while the visual system extracts the necessary information . an artificial visual system was built based on a fully recurrent neural network set within a reinforcement learning protocol , and learned to attend to regions of interest while solving a classification task . the model is consistent with several experimentally observed phenomena , and suggests novel predictions .", "topics": ["recurrent neural network", "reinforcement learning"]}
{"title": "classification with ultrahigh-dimensional features", "abstract": "although much progress has been made in classification with high-dimensional features \\citep { fan_fan:2008 , jguo:2010 , caisun:2014 , prxu:2014 } , classification with ultrahigh-dimensional features , wherein the features much outnumber the sample size , defies most existing work . this paper introduces a novel and computationally feasible multivariate screening and classification method for ultrahigh-dimensional data . leveraging inter-feature correlations , the proposed method enables detection of marginally weak and sparse signals and recovery of the true informative feature set , and achieves asymptotic optimal misclassification rates . we also show that the proposed procedure provides more powerful discovery boundaries compared to those in \\citet { caisun:2014 } and \\citet { jjin:2009 } . the performance of the proposed procedure is evaluated using simulation studies and demonstrated via classification of patients with different post-transplantation renal functional types .", "topics": ["simulation", "sparse matrix"]}
{"title": "teaching a machine to read maps with deep reinforcement learning", "abstract": "the ability to use a 2d map to navigate a complex 3d environment is quite remarkable , and even difficult for many humans . localization and navigation is also an important problem in domains such as robotics , and has recently become a focus of the deep reinforcement learning community . in this paper we teach a reinforcement learning agent to read a map in order to find the shortest way out of a random maze it has never seen before . our system combines several state-of-the-art methods such as a3c and incorporates novel elements such as a recurrent localization cell . our agent learns to localize itself based on 3d first person images and an approximate orientation angle . the agent generalizes well to bigger mazes , showing that it learned useful localization and navigation capabilities .", "topics": ["reinforcement learning"]}
{"title": "learning with confident examples : rank pruning for robust classification with noisy labels", "abstract": "noisy pn learning is the problem of binary classification when training examples may be mislabeled ( flipped ) uniformly with noise rate rho1 for positive examples and rho0 for negative examples . we propose rank pruning ( rp ) to solve noisy pn learning and the open problem of estimating the noise rates , i.e . the fraction of wrong positive and negative labels . unlike prior solutions , rp is time-efficient and general , requiring o ( t ) for any unrestricted choice of probabilistic classifier with t fitting time . we prove rp has consistent noise estimation and equivalent expected risk as learning with uncorrupted labels in ideal conditions , and derive closed-form solutions when conditions are non-ideal . rp achieves state-of-the-art noise estimation and f1 , error , and auc-pr for both mnist and cifar datasets , regardless of the amount of noise and performs similarly impressively when a large portion of training examples are noise drawn from a third distribution . to highlight , rp with a cnn classifier can predict if an mnist digit is a `` one '' or `` not '' with only 0.25 % error , and 0.46 error across all digits , even when 50 % of positive examples are mislabeled and 50 % of observed positive labels are mislabeled negative examples .", "topics": ["mnist database"]}
{"title": "learning reporting dynamics during breaking news for rumour detection in social media", "abstract": "breaking news leads to situations of fast-paced reporting in social media , producing all kinds of updates related to news stories , albeit with the caveat that some of those early updates tend to be rumours , i.e . , information with an unverified status at the time of posting . flagging information that is unverified can be helpful to avoid the spread of information that may turn out to be false . detection of rumours can also feed a rumour tracking system that ultimately determines their veracity . in this paper we introduce a novel approach to rumour detection that learns from the sequential dynamics of reporting during breaking news in social media to detect rumours in new stories . using twitter datasets collected during five breaking news stories , we experiment with conditional random fields as a sequential classifier that leverages context learnt during an event for rumour detection , which we compare with the state-of-the-art rumour detection system as well as other baselines . in contrast to existing work , our classifier does not need to observe tweets querying a piece of information to deem it a rumour , but instead we detect rumours from the tweet alone by exploiting context learnt during the event . our classifier achieves competitive performance , beating the state-of-the-art classifier that relies on querying tweets with improved precision and recall , as well as outperforming our best baseline with nearly 40 % improvement in terms of f1 score . the scale and diversity of our experiments reinforces the generalisability of our classifier .", "topics": ["baseline ( configuration management )"]}
{"title": "deep tempering", "abstract": "restricted boltzmann machines ( rbms ) are one of the fundamental building blocks of deep learning . approximate maximum likelihood training of rbms typically necessitates sampling from these models . in many training scenarios , computationally efficient gibbs sampling procedures are crippled by poor mixing . in this work we propose a novel method of sampling from boltzmann machines that demonstrates a computationally efficient way to promote mixing . our approach leverages an under-appreciated property of deep generative models such as the deep belief network ( dbn ) , where gibbs sampling from deeper levels of the latent variable hierarchy results in dramatically increased ergodicity . our approach is thus to train an auxiliary latent hierarchical model , based on the dbn . when used in conjunction with parallel-tempering , the method is asymptotically guaranteed to simulate samples from the target rbm . experimental results confirm the effectiveness of this sampling strategy in the context of rbm training .", "topics": ["sampling ( signal processing )", "computational complexity theory"]}
{"title": "fast , accurate and fully parallelizable digital image correlation", "abstract": "digital image correlation ( dic ) is a widely used optical metrology for surface deformation measurements . dic relies on nonlinear optimization method . thus an initial guess is quite important due to its influence on the converge characteristics of the algorithm . in order to obtain a reliable , accurate initial guess , a reliability-guided digital image correlation ( rg-dic ) method , which is able to intelligently obtain a reliable initial guess without using time-consuming integer-pixel registration , was proposed . however , the rg-dic and its improved methods are path-dependent and can not be fully parallelized . besides , it is highly possible that rg-dic fails in the full-field analysis of deformation without manual intervention if the deformation fields contain large areas of discontinuous deformation . feature-based initial guess is highly robust while it is relatively time-consuming . recently , path-independent algorithm , fast fourier transform-based cross correlation ( fft-cc ) algorithm , was proposed to estimate the initial guess . complete parallelizability is the major advantage of the fft-cc algorithm , while it is sensitive to small deformation . wu et al proposed an efficient integer-pixel search scheme , but the parameters of this algorithm are set by the users empirically . in this technical note , a fully parallelizable dic method is proposed . different from rg-dic method , the proposed method divides dic algorithm into two parts : full-field initial guess estimation and sub-pixel registration . the proposed method has the following benefits : 1 ) providing a pre-knowledge of deformation fields ; 2 ) saving computational time ; 3 ) reducing error propagation ; 4 ) integratability with well-established dic algorithms ; 5 ) fully parallelizability .", "topics": ["time complexity", "nonlinear system"]}
{"title": "dynamic neural turing machine with soft and hard addressing schemes", "abstract": "we extend neural turing machine ( ntm ) model into a dynamic neural turing machine ( d-ntm ) by introducing a trainable memory addressing scheme . this addressing scheme maintains for each memory cell two separate vectors , content and address vectors . this allows the d-ntm to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones . we implement the d-ntm with both continuous , differentiable and discrete , non-differentiable read/write mechanisms . we investigate the mechanisms and effects of learning to read and write into a memory through experiments on facebook babi tasks using both a feedforward and grucontroller . the d-ntm is evaluated on a set of facebook babi tasks and shown to outperform ntm and lstm baselines . we have done extensive analysis of our model and different variations of ntm on babi task . we also provide further experimental results on sequential pmnist , stanford natural language inference , associative recall and copy tasks .", "topics": ["nonlinear system", "natural language"]}
{"title": "parameter-free spectral kernel learning", "abstract": "due to the growing ubiquity of unlabeled data , learning with unlabeled data is attracting increasing attention in machine learning . in this paper , we propose a novel semi-supervised kernel learning method which can seamlessly combine manifold structure of unlabeled data and regularized least-squares ( rls ) to learn a new kernel . interestingly , the new kernel matrix can be obtained analytically with the use of spectral decomposition of graph laplacian matrix . hence , the proposed algorithm does not require any numerical optimization solvers . moreover , by maximizing kernel target alignment on labeled data , we can also learn model parameters automatically with a closed-form solution . for a given graph laplacian matrix , our proposed method does not need to tune any model parameter including the tradeoff parameter in rls and the balance parameter for unlabeled data . extensive experiments on ten benchmark datasets show that our proposed two-stage parameter-free spectral kernel learning algorithm can obtain comparable performance with fine-tuned manifold regularization methods in transductive setting , and outperform multiple kernel learning in supervised setting .", "topics": ["kernel ( operating system )", "mathematical optimization"]}
{"title": "supervised metric learning with generalization guarantees", "abstract": "the crucial importance of metrics in machine learning algorithms has led to an increasing interest in optimizing distance and similarity functions , an area of research known as metric learning . when data consist of feature vectors , a large body of work has focused on learning a mahalanobis distance . less work has been devoted to metric learning from structured objects ( such as strings or trees ) , most of it focusing on optimizing a notion of edit distance . we identify two important limitations of current metric learning approaches . first , they allow to improve the performance of local algorithms such as k-nearest neighbors , but metric learning for global algorithms ( such as linear classifiers ) has not been studied so far . second , the question of the generalization ability of metric learning methods has been largely ignored . in this thesis , we propose theoretical and algorithmic contributions that address these limitations . our first contribution is the derivation of a new kernel function built from learned edit probabilities . our second contribution is a novel framework for learning string and tree edit similarities inspired by the recent theory of ( e , g , t ) -good similarity functions . using uniform stability arguments , we establish theoretical guarantees for the learned similarity that give a bound on the generalization error of a linear classifier built from that similarity . in our third contribution , we extend these ideas to metric learning from feature vectors by proposing a bilinear similarity learning method that efficiently optimizes the ( e , g , t ) -goodness . generalization guarantees are derived for our approach , highlighting that our method minimizes a tighter bound on the generalization error of the classifier . our last contribution is a framework for establishing generalization bounds for a large class of existing metric learning algorithms based on a notion of algorithmic robustness .", "topics": ["test set", "cluster analysis"]}
{"title": "a method of generating random weights and biases in feedforward neural networks with random hidden nodes", "abstract": "neural networks with random hidden nodes have gained increasing interest from researchers and practical applications . this is due to their unique features such as very fast training and universal approximation property . in these networks the weights and biases of hidden nodes determining the nonlinear feature mapping are set randomly and are not learned . appropriate selection of the intervals from which weights and biases are selected is extremely important . this topic has not yet been sufficiently explored in the literature . in this work a method of generating random weights and biases is proposed . this method generates the parameters of the hidden nodes in such a way that nonlinear fragments of the activation functions are located in the input space regions with data and can be used to construct the surface approximating a nonlinear target function . the weights and biases are dependent on the input data range and activation function type . the proposed methods allows us to control the generalization degree of the model . these all lead to improvement in approximation performance of the network . several experiments show very promising results .", "topics": ["approximation algorithm", "nonlinear system"]}
{"title": "on the consistency of multithreshold entropy linear classifier", "abstract": "multithreshold entropy linear classifier ( melc ) is a recent classifier idea which employs information theoretic concept in order to create a multithreshold maximum margin model . in this paper we analyze its consistency over multithreshold linear models and show that its objective function upper bounds the amount of misclassified points in a similar manner like hinge loss does in support vector machines . for further confirmation we also conduct some numerical experiments on five datasets .", "topics": ["support vector machine", "numerical analysis"]}
{"title": "complex structure leads to overfitting : a structure regularization decoding method for natural language processing", "abstract": "recent systems on structured prediction focus on increasing the level of structural dependencies within the model . however , our study suggests that complex structures entail high overfitting risks . to control the structure-based overfitting , we propose to conduct structure regularization decoding ( sr decoding ) . the decoding of the complex structure model is regularized by the additionally trained simple structure model . we theoretically analyze the quantitative relations between the structural complexity and the overfitting risk . the analysis shows that complex structure models are prone to the structure-based overfitting . empirical evaluations show that the proposed method improves the performance of the complex structure models by reducing the structure-based overfitting . on the sequence labeling tasks , the proposed method substantially improves the performance of the complex neural network models . the maximum f1 error rate reduction is 36.4 % for the third-order model . the proposed method also works for the parsing task . the maximum uas improvement is 5.5 % for the tri-sibling model . the results are competitive with or better than the state-of-the-art results .", "topics": ["natural language processing", "matrix regularization"]}
{"title": "second-order kernel online convex optimization with adaptive sketching", "abstract": "kernel online convex optimization ( koco ) is a framework combining the expressiveness of non-parametric kernel models with the regret guarantees of online learning . first-order koco methods such as functional gradient descent require only $ \\mathcal { o } ( t ) $ time and space per iteration , and , when the only information on the losses is their convexity , achieve a minimax optimal $ \\mathcal { o } ( \\sqrt { t } ) $ regret . nonetheless , many common losses in kernel problems , such as squared loss , logistic loss , and squared hinge loss posses stronger curvature that can be exploited . in this case , second-order koco methods achieve $ \\mathcal { o } ( \\log ( \\text { det } ( \\boldsymbol { k } ) ) ) $ regret , which we show scales as $ \\mathcal { o } ( d_ { \\text { eff } } \\log t ) $ , where $ d_ { \\text { eff } } $ is the effective dimension of the problem and is usually much smaller than $ \\mathcal { o } ( \\sqrt { t } ) $ . the main drawback of second-order methods is their much higher $ \\mathcal { o } ( t^2 ) $ space and time complexity . in this paper , we introduce kernel online newton step ( kons ) , a new second-order koco method that also achieves $ \\mathcal { o } ( d_ { \\text { eff } } \\log t ) $ regret . to address the computational complexity of second-order methods , we introduce a new matrix sketching algorithm for the kernel matrix $ \\boldsymbol { k } _t $ , and show that for a chosen parameter $ \\gamma \\leq 1 $ our sketched-kons reduces the space and time complexity by a factor of $ \\gamma^2 $ to $ \\mathcal { o } ( t^2\\gamma^2 ) $ space and time per iteration , while incurring only $ 1/\\gamma $ times more regret .", "topics": ["kernel ( operating system )", "regret ( decision theory )"]}
{"title": "3d vision guided robotic charging station for electric and plug-in hybrid vehicles", "abstract": "electric vehicles ( evs ) and plug-in hybrid vehicles ( phevs ) are rapidly gaining popularity on our roads . besides a comparatively high purchasing price , the main two problems limiting their use are the short driving range and inconvenient charging process . in this paper we address the following by presenting an automatic robot-based charging station with 3d vision guidance for plugging and unplugging the charger . first of all , the whole system concept consisting of a 3d vision system , an ur10 robot and a charging station is presented . then we show the shape-based matching methods used to successfully identify and get the exact pose of the charging port . the same approach is used to calibrate the camera-robot system by using just known structure of the connector plug and no additional markers . finally , a three-step robot motion planning procedure for plug-in is presented and functionality is demonstrated in a series of successful experiments .", "topics": ["robot"]}
{"title": "twitter as a lifeline : human-annotated twitter corpora for nlp of crisis-related messages", "abstract": "microblogging platforms such as twitter provide active communication channels during mass convergence and emergency events such as earthquakes , typhoons . during the sudden onset of a crisis situation , affected people post useful information on twitter that can be used for situational awareness and other humanitarian disaster response efforts , if processed timely and effectively . processing social media information pose multiple challenges such as parsing noisy , brief and informal messages , learning information categories from the incoming stream of messages and classifying them into different classes among others . one of the basic necessities of many of these tasks is the availability of data , in particular human-annotated data . in this paper , we present human-annotated twitter corpora collected during 19 different crises that took place between 2013 and 2015 . to demonstrate the utility of the annotations , we train machine learning classifiers . moreover , we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets . to deal with tweets language issues , we present human-annotated normalized lexical resources for different lexical variations .", "topics": ["natural language processing", "text corpus"]}
{"title": "object detection for comics using manga109 annotations", "abstract": "with the growth of digitized comics , image understanding techniques are becoming important . in this paper , we focus on object detection , which is a fundamental task of image understanding . although convolutional neural networks ( cnn ) -based methods archived good performance in object detection for naturalistic images , there are two problems in applying these methods to the comic object detection task . first , there is no large-scale annotated comics dataset . the cnn-based methods require large-scale annotations for training . secondly , the objects in comics are highly overlapped compared to naturalistic images . this overlap causes the assignment problem in the existing cnn-based methods . to solve these problems , we proposed a new annotation dataset and a new cnn model . we annotated an existing image dataset of comics and created the largest annotation dataset , named manga109-annotations . for the assignment problem , we proposed a new cnn-based detector , ssd300-fork . we compared ssd300-fork with other detection methods using manga109-annotations and confirmed that our model outperformed them based on the map score .", "topics": ["object detection", "computer vision"]}
{"title": "efficient convolutional auto-encoding via random convexification and frequency-domain minimization", "abstract": "the omnipresence of deep learning architectures such as deep convolutional neural networks ( cnn ) s is fueled by the synergistic combination of ever-increasing labeled datasets and specialized hardware . despite the indisputable success , the reliance on huge amounts of labeled data and specialized hardware can be a limiting factor when approaching new applications . to help alleviating these limitations , we propose an efficient learning strategy for layer-wise unsupervised training of deep cnns on conventional hardware in acceptable time . our proposed strategy consists of randomly convexifying the reconstruction contractive auto-encoding ( rcae ) learning objective and solving the resulting large-scale convex minimization problem in the frequency domain via coordinate descent ( cd ) . the main advantages of our proposed learning strategy are : ( 1 ) single tunable optimization parameter ; ( 2 ) fast and guaranteed convergence ; ( 3 ) possibilities for full parallelization . numerical experiments show that our proposed learning strategy scales ( in the worst case ) linearly with image size , number of filters and filter size .", "topics": ["unsupervised learning"]}
{"title": "a blockwise descent algorithm for group-penalized multiresponse and multinomial regression", "abstract": "in this paper we purpose a blockwise descent algorithm for group-penalized multiresponse regression . using a quasi-newton framework we extend this to group-penalized multinomial regression . we give a publicly available implementation for these in r , and compare the speed of this algorithm to a competing algorithm -- - we show that our implementation is an order of magnitude faster than its competitor , and can solve gene-expression-sized problems in real time .", "topics": ["simulation"]}
{"title": "multiple object recognition with visual attention", "abstract": "we present an attention-based model for recognizing multiple objects in images . the proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image . we show that the model learns to both localize and recognize multiple objects despite being given only class labels during training . we evaluate the model on the challenging task of transcribing house number sequences from google street view images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation .", "topics": ["recurrent neural network", "reinforcement learning"]}
{"title": "nonstochastic multi-armed bandits with graph-structured feedback", "abstract": "we present and study a partial-information model of online learning , where a decision maker repeatedly chooses from a finite set of actions , and observes some subset of the associated losses . this naturally models several situations where the losses of different actions are related , and knowing the loss of one action provides information on the loss of other actions . moreover , it generalizes and interpolates between the well studied full-information setting ( where all losses are revealed ) and the bandit setting ( where only the loss of the action chosen by the player is revealed ) . we provide several algorithms addressing different variants of our setting , and provide tight regret bounds depending on combinatorial properties of the information feedback structure .", "topics": ["regret ( decision theory )"]}
{"title": "gated-attention architectures for task-oriented language grounding", "abstract": "to perform tasks specified by natural language instructions , autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment . this problem is called task-oriented language grounding . we propose an end-to-end trainable neural architecture for task-oriented language grounding in 3d environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input . the proposed model combines the image and text representations using a gated-attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods . we show the effectiveness of the proposed model on unseen instructions as well as unseen maps , both quantitatively and qualitatively . we also introduce a novel environment based on a 3d game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states .", "topics": ["natural language", "map"]}
{"title": "advances in artificial intelligence : deep intentions , shallow achievements", "abstract": "over the past decade , ai has made a remarkable progress due to recently revived deep learning technology . deep learning enables to process large amounts of data using simplified neuron networks that simulate the way in which the brain works . at the same time , there is another point of view that posits that brain is processing information , not data . this duality hampered ai progress for years . to provide a remedy for this situation , i propose a new definition of information that considers it as a coupling between two separate entities - physical information ( that implies data processing ) and semantic information ( that provides physical information interpretation ) . in such a case , intelligence arises as a result of information processing . the paper points on the consequences of this turn for the ai design philosophy .", "topics": ["simulation", "entity"]}
{"title": "ds-mlr : exploiting double separability for scaling up distributed multinomial logistic regression", "abstract": "scaling multinomial logistic regression to datasets with very large number of data points and classes has not been trivial . this is primarily because one needs to compute the log-partition function on every data point . this makes distributing the computation hard . in this paper , we present a distributed stochastic gradient descent based optimization method ( ds-mlr ) for scaling up multinomial logistic regression problems to massive scale datasets without hitting any storage constraints on the data and model parameters . our algorithm exploits double-separability , an attractive property we observe in the objective functions of several models in machine learning , that allows us to achieve both data as well as model parallelism simultaneously . in addition to being parallelizable , our algorithm can also easily be made non-blocking and asynchronous . we demonstrate the effectiveness of ds-mlr empirically on several real-world datasets , the largest being a reddit dataset created out of 1.7 billion user comments , where the data and parameter sizes are 228 gb and 358 gb respectively .", "topics": ["gradient descent", "computation"]}
{"title": "satisficing in time-sensitive bandit learning", "abstract": "much of the recent literature on bandit learning focuses on algorithms that aim to converge on an optimal action . one shortcoming is that this orientation does not account for time sensitivity , which can play a crucial role when learning an optimal action requires much more information than near-optimal ones . indeed , popular approaches such as upper-confidence-bound methods and thompson sampling can fare poorly in such situations . we consider instead learning a satisficing action , which is near-optimal while requiring less information , and propose satisficing thompson sampling , an algorithm that serves this purpose . we establish a general bound on expected discounted regret and study the application of satisficing thompson sampling to linear and infinite-armed bandits , demonstrating arbitrarily large benefits over thompson sampling . we also discuss the relation between the notion of satisficing and the theory of rate distortion , which offers guidance on the selection of satisficing actions .", "topics": ["sampling ( signal processing )"]}
{"title": "pde-constrained optimization in medical image analysis", "abstract": "pde-constrained optimization problems find many applications in medical image analysis , for example , neuroimaging , cardiovascular imaging , and oncological imaging . we review related literature and give examples on the formulation , discretization , and numerical solution of pde-constrained optimization problems for medical imaging . we discuss three examples . the first one is image registration . the second one is data assimilation for brain tumor patients , and the third one data assimilation in cardiovascular imaging . the image registration problem is a classical task in medical image analysis and seeks to find pointwise correspondences between two or more images . the data assimilation problems use a pde-constrained formulation to link a biophysical model to patient-specific data obtained from medical images . the associated optimality systems turn out to be sets of nonlinear , multicomponent pdes that are challenging to solve in an efficient way . the ultimate goal of our work is the design of inversion methods that integrate complementary data , and rigorously follow mathematical and physical principles , in an attempt to support clinical decision making . this requires reliable , high-fidelity algorithms with a short time-to-solution . this task is complicated by model and data uncertainties , and by the fact that pde-constrained optimization problems are ill-posed in nature , and in general yield high-dimensional , severely ill-conditioned systems after discretization . these features make regularization , effective preconditioners , and iterative solvers that , in many cases , have to be implemented on distributed-memory architectures to be practical , a prerequisite . we showcase state-of-the-art techniques in scientific computing to tackle these challenges .", "topics": ["mathematical optimization", "numerical analysis"]}
{"title": "model compression via distillation and quantization", "abstract": "deep neural networks ( dnns ) continue to make significant advances , solving tasks from image classification to translation or reinforcement learning . one aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments , such as mobile or embedded devices . this paper focuses on this problem , and proposes two new compression methods , which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks . the first method we propose is called quantized distillation and leverages distillation during the training process , by incorporating distillation loss , expressed with respect to the teacher , into the training of a student network whose weights are quantized to a limited set of levels . the second method , differentiable quantization , optimizes the location of quantization points through stochastic gradient descent , to better fit the behavior of the teacher model . we validate both methods through experiments on convolutional and recurrent architectures . we show that quantized shallow students can reach similar accuracy levels to full-precision teacher models , while providing order of magnitude compression , and inference speedup that is linear in the depth reduction . in sum , our results enable dnns for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices .", "topics": ["reinforcement learning", "computer vision"]}
{"title": "feature selection algorithm based on catastrophe model to improve the performance of regression analysis", "abstract": "in this paper we introduce a new feature selection algorithm to remove the irrelevant or redundant features in the data sets . in this algorithm the importance of a feature is based on its fitting to the catastrophe model . akaike information crite- rion value is used for ranking the features in the data set . the proposed algorithm is compared with well-known relief feature selection algorithm . breast cancer , parkinson telemonitoring data and slice locality data sets are used to evaluate the model .", "topics": ["relevance", "eisenstein 's criterion"]}
{"title": "trainable back-propagated functional transfer matrices", "abstract": "connections between nodes of fully connected neural networks are usually represented by weight matrices . in this article , functional transfer matrices are introduced as alternatives to the weight matrices : instead of using real weights , a functional transfer matrix uses real functions with trainable parameters to represent connections between nodes . multiple functional transfer matrices are then stacked together with bias vectors and activations to form deep functional transfer neural networks . these neural networks can be trained within the framework of back-propagation , based on a revision of the delta rules and the error transmission rule for functional connections . in experiments , it is demonstrated that the revised rules can be used to train a range of functional connections : 20 different functions are applied to neural networks with up to 10 hidden layers , and most of them gain high test accuracies on the mnist database . it is also demonstrated that a functional transfer matrix with a memory function can roughly memorise a non-cyclical sequence of 400 digits .", "topics": ["mnist database"]}
{"title": "efficient multicore collaborative filtering", "abstract": "this paper describes the solution method taken by lebusishu team for track1 in acm kdd cup 2011 contest ( resulting in the 5th place ) . we identified two main challenges : the unique item taxonomy characteristics as well as the large data set size.to handle the item taxonomy , we present a novel method called matrix factorization item taxonomy regularization ( mfitr ) . mfitr obtained the 2nd best prediction result out of more then ten implemented algorithms . for rapidly computing multiple solutions of various algorithms , we have implemented an open source parallel collaborative filtering library on top of the graphlab machine learning framework . we report some preliminary performance results obtained using the blacklight supercomputer .", "topics": ["data mining", "matrix regularization"]}
{"title": "partitioning large scale deep belief networks using dropout", "abstract": "deep learning methods have shown great promise in many practical applications , ranging from speech recognition , visual object recognition , to text processing . however , most of the current deep learning methods suffer from scalability problems for large-scale applications , forcing researchers or users to focus on small-scale problems with fewer parameters . in this paper , we consider a well-known machine learning model , deep belief networks ( dbns ) that have yielded impressive classification performance on a large number of benchmark machine learning tasks . to scale up dbn , we propose an approach that can use the computing clusters in a distributed environment to train large models , while the dense matrix computations within a single machine are sped up using graphics processors ( gpu ) . when training a dbn , each machine randomly drops out a portion of neurons in each hidden layer , for each training case , making the remaining neurons only learn to detect features that are generally helpful for producing the correct answer . within our approach , we have developed four methods to combine outcomes from each machine to form a unified model . our preliminary experiment on the mnst handwritten digit database demonstrates that our approach outperforms the state of the art test error rate .", "topics": ["speech recognition", "sparse matrix"]}
{"title": "learning from noisy labels with distillation", "abstract": "the ability of learning from noisy labels is very useful in many visual recognition tasks , as a vast amount of data with noisy labels are relatively easy to obtain . traditionally , the label noises have been treated as statistical outliers , and approaches such as importance re-weighting and bootstrap have been proposed to alleviate the problem . according to our observation , the real-world noisy labels exhibit multi-mode characteristics as the true labels , rather than behaving like independent random outliers . in this work , we propose a unified distillation framework to use side information , including a small clean dataset and label relations in knowledge graph , to `` hedge the risk '' of learning from noisy labels . furthermore , unlike the traditional approaches evaluated based on simulated label noises , we propose a suite of new benchmark datasets , in sports , species and artifacts domains , to evaluate the task of learning from noisy labels in the practical setting . the empirical study demonstrates the effectiveness of our proposed method in all the domains .", "topics": ["computer vision", "simulation"]}
{"title": "a direct method for estimating a causal ordering in a linear non-gaussian acyclic model", "abstract": "structural equation models and bayesian networks have been widely used to analyze causal relations between continuous variables . in such frameworks , linear acyclic models are typically used to model the datagenerating process of variables . recently , it was shown that use of non-gaussianity identifies a causal ordering of variables in a linear acyclic model without using any prior knowledge on the network structure , which is not the case with conventional methods . however , existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps . in this paper , we propose a new direct method to estimate a causal ordering based on non-gaussianity . in contrast to the previous methods , our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model .", "topics": ["bayesian network", "causality"]}
{"title": "contour detection using cost-sensitive convolutional neural networks", "abstract": "we address the problem of contour detection via per-pixel classifications of edge point . to facilitate the process , the proposed approach leverages with densenet , an efficient implementation of multiscale convolutional neural networks ( cnns ) , to extract an informative feature vector for each pixel and uses an svm classifier to accomplish contour detection . the main challenge lies in adapting a pre-trained per-image cnn model for yielding per-pixel image features . we propose to base on the densenet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches . in the experiment of contour detection , we look into the effectiveness of combining per-pixel features from different cnn layers and obtain comparable performances to the state-of-the-art on bsds500 .", "topics": ["feature vector", "pixel"]}
{"title": "learning task grouping and overlap in multi-task learning", "abstract": "in the paradigm of multi-task learning , mul- tiple related prediction tasks are learned jointly , sharing information across the tasks . we propose a framework for multi-task learn- ing that enables one to selectively share the information across the tasks . we assume that each task parameter vector is a linear combi- nation of a finite number of underlying basis tasks . the coefficients of the linear combina- tion are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these . our model is based on on the assumption that task pa- rameters within a group lie in a low dimen- sional subspace but allows the tasks in differ- ent groups to overlap with each other in one or more bases . experimental results on four datasets show that our approach outperforms competing methods .", "topics": ["sparse matrix", "coefficient"]}
{"title": "a compare-propagate architecture with alignment factorization for natural language inference", "abstract": "this paper presents a new deep learning architecture for natural language inference ( nli ) . firstly , we introduce a new compare-propagate architecture where alignments pairs are compared and then propagated to upper layers for enhanced representation learning . secondly , we adopt novel factorization layers for efficient compression of alignment vectors into scalar valued features , which are then be used to augment the base word representations . the design of our approach is aimed to be conceptually simple , compact and yet powerful . we conduct experiments on three popular benchmarks , snli , multinli and scitail , achieving state-of-the-art performance on all . a lightweight parameterization of our model enjoys a $ \\approx 300\\ % $ reduction in parameter size compared to the esim and diin , while maintaining competitive performance . visual analysis shows that our propagated features are highly interpretable , opening new avenues to explainability in neural nli models .", "topics": ["feature learning", "natural language"]}
{"title": "riemannian multi-manifold modeling", "abstract": "this paper advocates a novel framework for segmenting a dataset in a riemannian manifold $ m $ into clusters lying around low-dimensional submanifolds of $ m $ . important examples of $ m $ , for which the proposed clustering algorithm is computationally efficient , are the sphere , the set of positive definite matrices , and the grassmannian . the clustering problem with these examples of $ m $ is already useful for numerous application domains such as action identification in video sequences , dynamic texture clustering , brain fiber segmentation in medical imaging , and clustering of deformed images . the proposed clustering algorithm constructs a data-affinity matrix by thoroughly exploiting the intrinsic geometry and then applies spectral clustering . the intrinsic local geometry is encoded by local sparse coding and more importantly by directional information of local tangent spaces and geodesics . theoretical guarantees are established for a simplified variant of the algorithm even when the clusters intersect . to avoid complication , these guarantees assume that the underlying submanifolds are geodesic . extensive validation on synthetic and real data demonstrates the resiliency of the proposed method against deviations from the theoretical model as well as its superior performance over state-of-the-art techniques .", "topics": ["cluster analysis", "computational complexity theory"]}
{"title": "techniques for learning binary stochastic feedforward neural networks", "abstract": "stochastic binary hidden units in a multi-layer perceptron ( mlp ) network give at least three potential benefits when compared to deterministic mlp networks . ( 1 ) they allow to learn one-to-many type of mappings . ( 2 ) they can be used in structured prediction problems , where modeling the internal structure of the output is important . ( 3 ) stochasticity has been shown to be an excellent regularizer , which makes generalization performance potentially better in general . however , training stochastic networks is considerably more difficult . we study training using m samples of hidden activations per input . we show that the case m=1 leads to a fundamentally different behavior where the network tries to avoid stochasticity . we propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms . our experiments confirm that training stochastic networks is difficult and show that the proposed two estimators perform favorably among all the five known estimators .", "topics": ["gradient"]}
{"title": "heavy-tailed analogues of the covariance matrix for ica", "abstract": "independent component analysis ( ica ) is the problem of learning a square matrix $ a $ , given samples of $ x=as $ , where $ s $ is a random vector with independent coordinates . most existing algorithms are provably efficient only when each $ s_i $ has finite and moderately valued fourth moment . however , there are practical applications where this assumption need not be true , such as speech and finance . algorithms have been proposed for heavy-tailed ica , but they are not practical , using random walks and the full power of the ellipsoid algorithm multiple times . the main contributions of this paper are : ( 1 ) a practical algorithm for heavy-tailed ica that we call htica . we provide theoretical guarantees and show that it outperforms other algorithms in some heavy-tailed regimes , both on real and synthetic data . like the current state-of-the-art , the new algorithm is based on the centroid body ( a first moment analogue of the covariance matrix ) . unlike the state-of-the-art , our algorithm is practically efficient . to achieve this , we use explicit analytic representations of the centroid body , which bypasses the use of the ellipsoid method and random walks . ( 2 ) we study how heavy tails affect different ica algorithms , including htica . somewhat surprisingly , we show that some algorithms that use the covariance matrix or higher moments can successfully solve a range of ica instances with infinite second moment . we study this theoretically and experimentally , with both synthetic and real-world heavy-tailed data .", "topics": ["synthetic data"]}
{"title": "clustering dynamic web usage data", "abstract": "most classification methods are based on the assumption that data conforms to a stationary distribution . the machine learning domain currently suffers from a lack of classification techniques that are able to detect the occurrence of a change in the underlying data distribution . ignoring possible changes in the underlying concept , also known as concept drift , may degrade the performance of the classification model . often these changes make the model inconsistent and regular updatings become necessary . taking the temporal dimension into account during the analysis of web usage data is a necessity , since the way a site is visited may indeed evolve due to modifications in the structure and content of the site , or even due to changes in the behavior of certain user groups . one solution to this problem , proposed in this article , is to update models using summaries obtained by means of an evolutionary approach based on an intelligent clustering approach . we carry out various clustering strategies that are applied on time sub-periods . to validate our approach we apply two external evaluation criteria which compare different partitions from the same data set . our experiments show that the proposed approach is efficient to detect the occurrence of changes .", "topics": ["cluster analysis"]}
{"title": "communication-efficient false discovery rate control via knockoff aggregation", "abstract": "the false discovery rate ( fdr ) -- -the expected fraction of spurious discoveries among all the discoveries -- -provides a popular statistical assessment of the reproducibility of scientific studies in various disciplines . in this work , we introduce a new method for controlling the fdr in meta-analysis of many decentralized linear models . our method targets the scenario where many research groups -- -possibly the number of which is random -- -are independently testing a common set of hypotheses and then sending summary statistics to a coordinating center in an online manner . built on the knockoffs framework introduced by barber and candes ( 2015 ) , our procedure starts by applying the knockoff filter to each linear model and then aggregates the summary statistics via one-shot communication in a novel way . this method gives exact fdr control non-asymptotically without any knowledge of the noise variances or making any assumption about sparsity of the signal . in certain settings , it has a communication complexity that is optimal up to a logarithmic factor .", "topics": ["sparse matrix"]}
{"title": "learning to understand by evolving theories", "abstract": "in this paper , we describe an approach that enables an autonomous system to infer the semantics of a command ( i.e . a symbol sequence representing an action ) in terms of the relations between changes in the observations and the action instances . we present a method of how to induce a theory ( i.e . a semantic description ) of the meaning of a command in terms of a minimal set of background knowledge . the only thing we have is a sequence of observations from which we extract what kinds of effects were caused by performing the command . this way , we yield a description of the semantics of the action and , hence , a definition .", "topics": ["autonomous car"]}
{"title": "is this a joke ? detecting humor in spanish tweets", "abstract": "while humor has been historically studied from a psychological , cognitive and linguistic standpoint , its study from a computational perspective is an area yet to be explored in computational linguistics . there exist some previous works , but a characterization of humor that allows its automatic recognition and generation is far from being specified . in this work we build a crowdsourced corpus of labeled tweets , annotated according to its humor value , letting the annotators subjectively decide which are humorous . a humor classifier for spanish tweets is assembled based on supervised learning , reaching a precision of 84 % and a recall of 69 % .", "topics": ["supervised learning"]}
{"title": "a new color feature extraction method based on dynamic color distribution entropy of neighborhoods", "abstract": "one of the important requirements in image retrieval , indexing , classification , clustering and etc . is extracting efficient features from images . the color feature is one of the most widely used visual features . use of color histogram is the most common way for representing color feature . one of disadvantage of the color histogram is that it does not take the color spatial distribution into consideration . in this paper dynamic color distribution entropy of neighborhoods method based on color distribution entropy is presented , which effectively describes the spatial information of colors . the image retrieval results in compare to improved color distribution entropy show the acceptable efficiency of this approach .", "topics": ["statistical classification", "feature extraction"]}
{"title": "semi-described and semi-supervised learning with gaussian processes", "abstract": "propagating input uncertainty through non-linear gaussian process ( gp ) mappings is intractable . this hinders the task of training gps using uncertain and partially observed inputs . in this paper we refer to this task as `` semi-described learning '' . we then introduce a gp framework that solves both , the semi-described and the semi-supervised learning problems ( where missing values occur in the outputs ) . auto-regressive state space simulation is also recognised as a special case of semi-described learning . to achieve our goal we develop variational methods for handling semi-described inputs in gps , and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled , bayesian manner . extensive experiments on simulated and real-world data study the problems of iterative forecasting and regression/classification with missing values . the results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks .", "topics": ["calculus of variations", "supervised learning"]}
{"title": "the unreasonable effectiveness of deep features as a perceptual metric", "abstract": "while it is nearly effortless for humans to quickly assess the perceptual similarity between two images , the underlying processes are thought to be quite complex . despite this , the most widely used perceptual metrics today , such as psnr and ssim , are simple , shallow functions , and fail to account for many nuances of human perception . recently , the deep learning community has found that features of the vgg network trained on the imagenet classification task has been remarkably useful as a training loss for image synthesis . but how perceptual are these so-called `` perceptual losses '' ? what elements are critical for their success ? to answer these questions , we introduce a new full reference image quality assessment ( fr-iqa ) dataset of perceptual human judgments , orders of magnitude larger than previous datasets . we systematically evaluate deep features across different architectures and tasks and compare them with classic metrics . we find that deep features outperform all previous metrics by huge margins . more surprisingly , this result is not restricted to imagenet-trained vgg features , but holds across different deep architectures and levels of supervision ( supervised , self-supervised , or even unsupervised ) . our results suggest that perceptual similarity is an emergent property shared across deep visual representations .", "topics": ["computer vision", "pixel"]}
{"title": "the peaking phenomenon in semi-supervised learning", "abstract": "for the supervised least squares classifier , when the number of training objects is smaller than the dimensionality of the data , adding more data to the training set may first increase the error rate before decreasing it . this , possibly counterintuitive , phenomenon is known as peaking . in this work , we observe that a similar but more pronounced version of this phenomenon also occurs in the semi-supervised setting , where instead of labeled objects , unlabeled objects are added to the training set . we explain why the learning curve has a more steep incline and a more gradual decline in this setting through simulation studies and by applying an approximation of the learning curve based on the work by raudys & duin .", "topics": ["supervised learning", "simulation"]}
{"title": "geometric gan", "abstract": "generative adversarial nets ( gans ) represent an important milestone for effective generative models , which has inspired numerous variants seemingly different from each other . one of the main contributions of this paper is to reveal a unified geometric structure in gan and its variants . specifically , we show that the adversarial generative model training can be decomposed into three geometric steps : separating hyperplane search , discriminator parameter update away from the separating hyperplane , and the generator update along the normal vector direction of the separating hyperplane . this geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric gan using svm separating hyperplane that maximizes the margin . our theoretical analysis shows that the geometric gan converges to a nash equilibrium between the discriminator and generator . in addition , extensive numerical results show that the superior performance of geometric gan .", "topics": ["generative model", "numerical analysis"]}
{"title": "enriching word vectors with subword information", "abstract": "continuous word representations , trained on large unlabeled corpora are useful for many natural language processing tasks . popular models that learn such representations ignore the morphology of words , by assigning a distinct vector to each word . this is a limitation , especially for languages with large vocabularies and many rare words . in this paper , we propose a new approach based on the skipgram model , where each word is represented as a bag of character $ n $ -grams . a vector representation is associated to each character $ n $ -gram ; words being represented as the sum of these representations . our method is fast , allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data . we evaluate our word representations on nine different languages , both on word similarity and analogy tasks . by comparing to recently proposed morphological word representations , we show that our vectors achieve state-of-the-art performance on these tasks .", "topics": ["natural language processing", "text corpus"]}
{"title": "distributed constraint optimization problems and applications : a survey", "abstract": "the field of multi-agent system ( mas ) is an active area of research within artificial intelligence , with an increasingly important impact in industrial and other real-world applications . within a mas , autonomous agents interact to pursue personal interests and/or to achieve common objectives . distributed constraint optimization problems ( dcops ) have emerged as one of the prominent agent architectures to govern the agents ' autonomous behavior , where both algorithms and communication models are driven by the structure of the specific problem . during the last decade , several extensions to the dcop model have enabled them to support mas in complex , real-time , and uncertain environments . this survey aims at providing an overview of the dcop model , giving a classification of its multiple extensions and addressing both resolution methods and applications that find a natural mapping within each class of dcops . the proposed classification suggests several future perspectives for dcop extensions , and identifies challenges in the design of efficient resolution algorithms , possibly through the adaptation of strategies from different areas .", "topics": ["artificial intelligence", "autonomous car"]}
{"title": "neural baby talk", "abstract": "we introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image . our approach reconciles classical slot filling approaches ( that are generally better grounded in images ) with modern neural captioning approaches ( that are generally more natural sounding and accurate ) . our approach first generates a sentence `template ' with slot locations explicitly tied to specific image regions . these slots are then filled in by visual concepts identified in the regions by object detectors . the entire architecture ( sentence template generation and slot filling with object detectors ) is end-to-end differentiable . we verify the effectiveness of our proposed model on different image captioning tasks . on standard image captioning and novel object captioning , our model reaches state-of-the-art on both coco and flickr30k datasets . we also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different . code has been made available at : https : //github.com/jiasenlu/neuralbabytalk", "topics": ["natural language", "entity"]}
{"title": "quantum predictive learning and communication complexity with single input", "abstract": "we define a new model of quantum learning that we call predictive quantum ( pq ) . this is a quantum analogue of pac , where during the testing phase the student is only required to answer a polynomial number of testing queries . we demonstrate a relational concept class that is efficiently learnable in pq , while in any `` reasonable '' classical model exponential amount of training data would be required . this is the first unconditional separation between quantum and classical learning . we show that our separation is the best possible in several ways ; in particular , there is no analogous result for a functional class , as well as for several weaker versions of quantum learning . in order to demonstrate tightness of our separation we consider a special case of one-way communication that we call single-input mode , where bob receives no input . somewhat surprisingly , this setting becomes nontrivial when relational communication tasks are considered . in particular , any problem with two-sided input can be transformed into a single-input relational problem of equal classical one-way cost . we show that the situation is different in the quantum case , where the same transformation can make the communication complexity exponentially larger . this happens if and only if the original problem has exponential gap between quantum and classical one-way communication costs . we believe that these auxiliary results might be of independent interest .", "topics": ["test set", "time complexity"]}
{"title": "on move pattern trends in a large go games corpus", "abstract": "we process a large corpus of game records of the board game of go and propose a way of extracting summary information on played moves . we then apply several basic data-mining methods on the summary information to identify the most differentiating features within the summary information , and discuss their correspondence with traditional go knowledge . we show statistically significant mappings of the features to player attributes such as playing strength or informally perceived `` playing style '' ( e.g . territoriality or aggressivity ) , describe accurate classifiers for these attributes , and propose applications including seeding real-work ranks of internet players , aiding in go study and tuning of go-playing programs , or contribution to go-theoretical discussion on the scope of `` playing style '' .", "topics": ["data mining", "text corpus"]}
{"title": "the wreath process : a totally generative model of geometric shape based on nested symmetries", "abstract": "we consider the problem of modelling noisy but highly symmetric shapes that can be viewed as hierarchies of whole-part relationships in which higher level objects are composed of transformed collections of lower level objects . to this end , we propose the stochastic wreath process , a fully generative probabilistic model of drawings . following leyton 's `` generative theory of shape '' , we represent shapes as sequences of transformation groups composed through a wreath product . this representation emphasizes the maximization of transfer -- - the idea that the most compact and meaningful representation of a given shape is achieved by maximizing the re-use of existing building blocks or parts . the proposed stochastic wreath process extends leyton 's theory by defining a probability distribution over geometric shapes in terms of noise processes that are aligned with the generative group structure of the shape . we propose an inference scheme for recovering the generative history of given images in terms of the wreath process using reversible jump markov chain monte carlo methods and approximate bayesian computation . in the context of sketching we demonstrate the feasibility and limitations of this approach on model-generated and real data .", "topics": ["generative model", "computation"]}
{"title": "a streaming accelerator for deep convolutional neural networks with image and feature decomposition for resource-limited system applications", "abstract": "deep convolutional neural networks ( cnn ) are widely used in modern artificial intelligence ( ai ) and smart vision systems but also limited by computation latency , throughput , and energy efficiency on a resource-limited scenario , such as mobile devices , internet of things ( iot ) , unmanned aerial vehicles ( uav ) , and so on . a hardware streaming architecture is proposed to accelerate convolution and pooling computations for state-of-the-art deep cnns . it is optimized for energy efficiency by maximizing local data reuse to reduce off-chip dram data access . in addition , image and feature decomposition techniques are introduced to optimize memory access pattern for an arbitrary size of image and number of features within limited on-chip sram capacity . a prototype accelerator was implemented in tsmc 65 nm cmos technology with 2.3 mm x 0.8 mm core area , which achieves 144 gops peak throughput and 0.8 tops/w peak energy efficiency .", "topics": ["computation", "artificial intelligence"]}
{"title": "a neural network decision tree for learning concepts from eeg data", "abstract": "to learn the multi-class conceptions from the electroencephalogram ( eeg ) data we developed a neural network decision tree ( dt ) , that performs the linear tests , and a new training algorithm . we found that the known methods fail inducting the classification models when the data are presented by the features some of them are irrelevant , and the classes are heavily overlapped . to train the dt , our algorithm exploits a bottom up search of the features that provide the best classification accuracy of the linear tests . we applied the developed algorithm to induce the dt from the large eeg dataset consisted of 65 patients belonging to 16 age groups . in these recordings each eeg segment was represented by 72 calculated features . the dt correctly classified 80.8 % of the training and 80.1 % of the testing examples . correspondingly it correctly classified 89.2 % and 87.7 % of the eeg recordings .", "topics": ["relevance"]}
{"title": "selective inference and learning mixed graphical models", "abstract": "this thesis studies two problems in modern statistics . first , we study selective inference , or inference for hypothesis that are chosen after looking at the data . the motiving application is inference for regression coefficients selected by the lasso . we present the condition-on-selection method that allows for valid selective inference , and study its application to the lasso , and several other selection algorithms . in the second part , we consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables . we present a new pairwise model for graphical models with both continuous and discrete variables that is amenable to structure learning . in previous work , authors have considered structure learning of gaussian graphical models and structure learning of discrete models . our approach is a natural generalization of these two lines of work to the mixed case . the penalization scheme involves a novel symmetric use of the group-lasso norm and follows naturally from a particular parametrization of the model . we provide conditions under which our estimator is model selection consistent in the high-dimensional regime .", "topics": ["graphical model", "coefficient"]}
{"title": "background subtraction via fast robust matrix completion", "abstract": "background subtraction is the primary task of the majority of video inspection systems . the most important part of the background subtraction which is common among different algorithms is background modeling . in this regard , our paper addresses the problem of background modeling in a computationally efficient way , which is important for current eruption of `` big data '' processing coming from high resolution multi-channel videos . our model is based on the assumption that background in natural images lies on a low-dimensional subspace . we formulated and solved this problem in a low-rank matrix completion framework . in modeling the background , we benefited from the in-face extended frank-wolfe algorithm for solving a defined convex optimization problem . we evaluated our fast robust matrix completion ( frmc ) method on both background models challenge ( bmc ) and stuttgart artificial background subtraction ( sabs ) datasets . the results were compared with the robust principle component analysis ( rpca ) and low-rank robust matrix completion ( rmc ) methods , both solved by inexact augmented lagrangian multiplier ( ialm ) . the results showed faster computation , at least twice as when ialm solver is used , while having a comparable accuracy even better in some challenges , in subtracting the backgrounds in order to detect moving objects in the scene .", "topics": ["optimization problem", "computational complexity theory"]}
{"title": "intrinsic motivation and mental replay enable efficient online adaptation in stochastic recurrent networks", "abstract": "autonomous robots need to interact with unknown , unstructured and changing environments , constantly facing novel challenges . therefore , continuous online adaptation for lifelong-learning and the need of sample-efficient mechanisms to adapt to changes in the environment , the constraints , the tasks , or the robot itself are crucial . in this work , we propose a novel framework for probabilistic online motion planning with online adaptation based on a bio-inspired stochastic recurrent neural network . by using learning signals which mimic the intrinsic motivation signal cognitive dissonance in addition with a mental replay strategy to intensify experiences , the stochastic recurrent network can learn from few physical interactions and adapts to novel environments in seconds . we evaluate our online planning and adaptation framework on an anthropomorphic kuka lwr arm . the rapid online adaptation is shown by learning unknown workspace constraints sample-efficiently from few physical interactions while following given via points .", "topics": ["recurrent neural network", "interaction"]}
{"title": "modeling multiple annotator expertise in the semi-supervised learning scenario", "abstract": "learning algorithms normally assume that there is at most one annotation or label per data point . however , in some scenarios , such as medical diagnosis and on-line collaboration , multiple annotations may be available . in either case , obtaining labels for data points can be expensive and time-consuming ( in some circumstances ground-truth may not exist ) . semi-supervised learning approaches have shown that utilizing the unlabeled data is often beneficial in these cases . this paper presents a probabilistic semi-supervised model and algorithm that allows for learning from both unlabeled and labeled data in the presence of multiple annotators . we assume that it is known what annotator labeled which data points . the proposed approach produces annotator models that allow us to provide ( 1 ) estimates of the true label and ( 2 ) annotator variable expertise for both labeled and unlabeled data . we provide numerical comparisons under various scenarios and with respect to standard semi-supervised learning . experiments showed that the presented approach provides clear advantages over multi-annotator methods that do not use the unlabeled data and over methods that do not use multi-labeler information .", "topics": ["supervised learning", "numerical analysis"]}
{"title": "learning distributed representations of sentences from unlabelled data", "abstract": "unsupervised methods for learning distributed representations of words are ubiquitous in today 's nlp research , but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data . this paper is a systematic comparison of models that learn such representations . we find that the optimal approach depends critically on the intended application . deeper , more complex models are preferable for representations to be used in supervised systems , but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics . we also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time , domain portability and performance .", "topics": ["natural language processing"]}
{"title": "crime incidents embedding using restricted boltzmann machines", "abstract": "we present a new approach for detecting related crime series , by unsupervised learning of the latent feature embeddings from narratives of crime record via the gaussian-bernoulli restricted boltzmann machines ( rbm ) . this is a drastically different approach from prior work on crime analysis , which typically considers only time and location and at most category information . after the embedding , related cases are closer to each other in the euclidean feature space , and the unrelated cases are far apart , which is a good property can enable subsequent analysis such as detection and clustering of related cases . experiments over several series of related crime incidents hand labeled by the atlanta police department reveal the promise of our embedding methods .", "topics": ["cluster analysis", "feature vector"]}
{"title": "meta-learning by adjusting priors based on extended pac-bayes theory", "abstract": "in meta-learning an agent extracts knowledge from observed tasks , aiming to facilitate learning of novel future tasks . under the assumption that future tasks are 'related ' to previous tasks , representations should be learned in a way which captures the common structure across learned tasks , while allowing the learner sufficient flexibility to adapt to novel aspects of new tasks . we present a framework for meta-learning that is based on generalization error bounds , allowing us to extend various pac-bayes bounds to meta-learning . learning takes place through the construction of a distribution over hypotheses based on the observed tasks , and its utilization for learning a new task . thus , prior knowledge is incorporated through setting an experience-dependent prior for novel tasks . we develop a gradient-based algorithm which minimizes an objective function derived from the bounds and demonstrate its effectiveness numerically with deep neural networks . in addition to establishing the improved performance available through meta-learning , we demonstrate the intuitive way by which prior information is manifested at different levels of the network .", "topics": ["optimization problem", "numerical analysis"]}
{"title": "safe : spectral evolution analysis feature extraction for non-stationary time series prediction", "abstract": "this paper presents a practical approach for detecting non-stationarity in time series prediction . this method is called safe and works by monitoring the evolution of the spectral contents of time series through a distance function . this method is designed to work in combination with state-of-the-art machine learning methods in real time by informing the online predictors to perform necessary adaptation when a non-stationarity presents . we also propose an algorithm to proportionally include some past data in the adaption process to overcome the catastrophic forgetting problem . to validate our hypothesis and test the effectiveness of our approach , we present comprehensive experiments in different elements of the approach involving artificial and real-world datasets . the experiments show that the proposed method is able to significantly save computational resources in term of processor or gpu cycles while maintaining high prediction performances .", "topics": ["time series", "feature extraction"]}
{"title": "appearance-based indoor localization : a comparison of patch descriptor performance", "abstract": "vision is one of the most important of the senses , and humans use it extensively during navigation . we evaluated different types of image and video frame descriptors that could be used to determine distinctive visual landmarks for localizing a person based on what is seen by a camera that they carry . to do this , we created a database containing over 3 km of video-sequences with ground-truth in the form of distance travelled along different corridors . using this database , the accuracy of localization - both in terms of knowing which route a user is on - and in terms of position along a certain route , can be evaluated . for each type of descriptor , we also tested different techniques to encode visual structure and to search between journeys to estimate a user 's position . the techniques include single-frame descriptors , those using sequences of frames , and both colour and achromatic descriptors . we found that single-frame indexing worked better within this particular dataset . this might be because the motion of the person holding the camera makes the video too dependent on individual steps and motions of one particular journey . our results suggest that appearance-based information could be an additional source of navigational data indoors , augmenting that provided by , say , radio signal strength indicators ( rssis ) . such visual information could be collected by crowdsourcing low-resolution video feeds , allowing journeys made by different users to be associated with each other , and location to be inferred without requiring explicit mapping . this offers a complementary approach to methods based on simultaneous localization and mapping ( slam ) algorithms .", "topics": ["ground truth"]}
{"title": "local similarities , global coding : an algorithm for feature coding and its applications", "abstract": "data coding as a building block of several image processing algorithms has been received great attention recently . indeed , the importance of the locality assumption in coding approaches is studied in numerous works and several methods are proposed based on this concept . we probe this assumption and claim that taking the similarity between a data point and a more global set of anchor points does not necessarily weaken the coding method as long as the underlying structure of the anchor points are taken into account . based on this fact , we propose to capture this underlying structure by assuming a random walker over the anchor points . we show that our method is a fast approximate learning algorithm based on the diffusion map kernel . the experiments on various datasets show that making different state-of-the-art coding algorithms aware of this structure boosts them in different learning tasks .", "topics": ["image processing", "approximation algorithm"]}
{"title": "multimodal prediction and personalization of photo edits with deep generative models", "abstract": "professional-grade software applications are powerful but complicated $ - $ expert users can achieve impressive results , but novices often struggle to complete even basic tasks . photo editing is a prime example : after loading a photo , the user is confronted with an array of cryptic sliders like `` clarity '' , `` temp '' , and `` highlights '' . an automatically generated suggestion could help , but there is no single `` correct '' edit for a given image $ - $ different experts may make very different aesthetic decisions when faced with the same image , and a single expert may make different choices depending on the intended use of the image ( or on a whim ) . we therefore want a system that can propose multiple diverse , high-quality edits while also learning from and adapting to a user 's aesthetic preferences . in this work , we develop a statistical model that meets these objectives . our model builds on recent advances in neural network generative modeling and scalable inference , and uses hierarchical structure to learn editing patterns across many diverse users . empirically , we find that our model outperforms other approaches on this challenging multimodal prediction task .", "topics": ["scalability"]}
{"title": "explicit knowledge-based reasoning for visual question answering", "abstract": "we describe a method for visual question answering which is capable of reasoning about contents of an image on the basis of information extracted from a large-scale knowledge base . the method not only answers natural language questions using concepts not contained in the image , but can provide an explanation of the reasoning by which it developed its answer . the method is capable of answering far more complex questions than the predominant long short-term memory-based approach , and outperforms it significantly in the testing . we also provide a dataset and a protocol by which to evaluate such methods , thus addressing one of the key issues in general visual ques- tion answering .", "topics": ["natural language"]}
{"title": "embed to control : a locally linear latent dynamics model for control from raw images", "abstract": "we introduce embed to control ( e2c ) , a method for model learning and control of non-linear dynamical systems from raw pixel images . e2c consists of a deep generative model , belonging to the family of variational autoencoders , that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear . our model is derived directly from an optimal control formulation in latent space , supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems .", "topics": ["generative model", "calculus of variations"]}
{"title": "a comparison of rule extraction for different recurrent neural network models and grammatical complexity", "abstract": "it has been shown that rules can be extracted from highly non-linear , recursive models such as recurrent neural networks ( rnns ) . the rnn models mostly investigated include both elman networks and second-order recurrent networks . recently , new types of rnns have demonstrated superior power in handling many machine learning tasks , especially when structural data is involved such as language modeling . here , we empirically evaluate different recurrent models on the task of learning deterministic finite automata ( dfa ) , the seven tomita grammars . we are interested in the capability of recurrent models with different architectures in learning and expressing regular grammars , which can be the building blocks for many applications dealing with structural data . our experiments show that a second-order rnn provides the best and stablest performance of extracting dfa over all tomita grammars and that other rnn models are greatly influenced by different tomita grammars . to better understand these results , we provide a theoretical analysis of the `` complexity '' of different grammars , by introducing the entropy and the averaged edit distance of regular grammars defined in this paper . through our analysis , we categorize all tomita grammars into different classes , which explains the inconsistency in the performance of extraction observed across all rnn models .", "topics": ["recurrent neural network", "nonlinear system"]}
{"title": "bayesian model selection methods for mutual and symmetric $ k $ -nearest neighbor classification", "abstract": "the $ k $ -nearest neighbor classification method ( $ k $ -nnc ) is one of the simplest nonparametric classification methods . the mutual $ k $ -nn classification method ( m $ k $ nnc ) is a variant of $ k $ -nnc based on mutual neighborship . we propose another variant of $ k $ -nnc , the symmetric $ k $ -nn classification method ( s $ k $ nnc ) based on both mutual neighborship and one-sided neighborship . the performance of m $ k $ nnc and s $ k $ nnc depends on the parameter $ k $ as the one of $ k $ -nnc does . we propose the ways how m $ k $ nn and s $ k $ nn classification can be performed based on bayesian mutual and symmetric $ k $ -nn regression methods with the selection schemes for the parameter $ k $ . bayesian mutual and symmetric $ k $ -nn regression methods are based on gaussian process models , and it turns out that they can do m $ k $ nn and s $ k $ nn classification with new encodings of target values ( class labels ) . the simulation results show that the proposed methods are better than or comparable to $ k $ -nnc , m $ k $ nnc and s $ k $ nnc with the parameter $ k $ selected by the leave-one-out cross validation method not only for an artificial data set but also for real world data sets .", "topics": ["simulation"]}
{"title": "flexible stereo : constrained , non-rigid , wide-baseline stereo vision for fixed-wing aerial platforms", "abstract": "this paper proposes a computationally efficient method to estimate the time-varying relative pose between two visual-inertial sensor rigs mounted on the flexible wings of a fixed-wing unmanned aerial vehicle ( uav ) . the estimated relative poses are used to generate highly accurate depth maps in real-time and can be employed for obstacle avoidance in low-altitude flights or landing maneuvers . the approach is structured as follows : initially , a wing model is identified by fitting a probability density function to measured deviations from the nominal relative baseline transformation . at run-time , the prior knowledge about the wing model is fused in an extended kalman filter~ ( ekf ) together with relative pose measurements obtained from solving a relative perspective n-point problem ( pnp ) , and the linear accelerations and angular velocities measured by the two inertial measurement units ( imu ) which are rigidly attached to the cameras . results obtained from extensive synthetic experiments demonstrate that our proposed framework is able to estimate highly accurate baseline transformations and depth maps .", "topics": ["baseline ( configuration management )", "computational complexity theory"]}
{"title": "nefi : network extraction from images", "abstract": "networks and network-like structures are amongst the central building blocks of many technological and biological systems . given a mathematical graph representation of a network , methods from graph theory enable a precise investigation of its properties . software for the analysis of graphs is widely available and has been applied to graphs describing large scale networks such as social networks , protein-interaction networks , etc . in these applications , graph acquisition , i.e . , the extraction of a mathematical graph from a network , is relatively simple . however , for many network-like structures , e.g . leaf venations , slime molds and mud cracks , data collection relies on images where graph extraction requires domain-specific solutions or even manual . here we introduce network extraction from images , nefi , a software tool that automatically extracts accurate graphs from images of a wide range of networks originating in various domains . while there is previous work on graph extraction from images , theoretical results are fully accessible only to an expert audience and ready-to-use implementations for non-experts are rarely available or insufficiently documented . nefi provides a novel platform allowing practitioners from many disciplines to easily extract graph representations from images by supplying flexible tools from image processing , computer vision and graph theory bundled in a convenient package . thus , nefi constitutes a scalable alternative to tedious and error-prone manual graph extraction and special purpose tools . we anticipate nefi to enable the collection of larger datasets by reducing the time spent on graph extraction . the analysis of these new datasets may open up the possibility to gain new insights into the structure and function of various types of networks . nefi is open source and available http : //nefi.mpi-inf.mpg.de .", "topics": ["image processing", "computer vision"]}
{"title": "pte : predictive text embedding through large-scale heterogeneous text networks", "abstract": "unsupervised text embedding methods , such as skip-gram and paragraph vector , have been attracting increasing attention due to their simplicity , scalability , and effectiveness . however , comparing to sophisticated deep learning architectures such as convolutional neural networks , these methods usually yield inferior results when applied to particular machine learning tasks . one possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way , without leveraging the labeled information available for the task . although the low dimensional representations learned are applicable to many different tasks , they are not particularly tuned for any task . in this paper , we fill this gap by proposing a semi-supervised representation learning method for text data , which we call the \\textit { predictive text embedding } ( pte ) . predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text . the labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network , which is then embedded into a low dimensional space through a principled and efficient algorithm . this low dimensional embedding not only preserves the semantic closeness of words and documents , but also has a strong predictive power for the particular task . compared to recent supervised approaches based on convolutional neural networks , predictive text embedding is comparable or more effective , much more efficient , and has fewer parameters to tune .", "topics": ["feature learning", "unsupervised learning"]}
{"title": "advancing bayesian optimization : the mixed-global-local ( mgl ) kernel and length-scale cool down", "abstract": "bayesian optimization ( bo ) has become a core method for solving expensive black-box optimization problems . while much research focussed on the choice of the acquisition function , we focus on online length-scale adaption and the choice of kernel function . instead of choosing hyperparameters in view of maximum likelihood on past data , we propose to use the acquisition function to decide on hyperparameter adaptation more robustly and in view of the future optimization progress . further , we propose a particular kernel function that includes non-stationarity and local anisotropy and thereby implicitly integrates the efficiency of local convex optimization with global bayesian optimization . comparisons to state-of-the art bo methods underline the efficiency of these mechanisms on global optimization benchmarks .", "topics": ["kernel ( operating system )"]}
{"title": "predicting drug interactions and mutagenicity with ensemble classifiers on subgraphs of molecules", "abstract": "in this study , we intend to solve a mutual information problem in interacting molecules of any type , such as proteins , nucleic acids , and small molecules . using machine learning techniques , we accurately predict pairwise interactions , which can be of medical and biological importance . graphs are are useful in this problem for their generality to all types of molecules , due to the inherent association of atoms through atomic bonds . subgraphs can represent different molecular domains . these domains can be biologically significant as most molecules only have portions that are of functional significance and can interact with other domains . thus , we use subgraphs as features in different machine learning algorithms to predict if two drugs interact and predict potential single molecule effects .", "topics": ["interaction"]}
{"title": "transfer learning with binary neural networks", "abstract": "previous work has shown that it is possible to train deep neural networks with low precision weights and activations . in the extreme case it is even possible to constrain the network to binary values . the costly floating point multiplications are then reduced to fast logical operations . high end smart phones such as google 's pixel 2 and apple 's iphone x are already equipped with specialised hardware for image processing and it is very likely that other future consumer hardware will also have dedicated accelerators for deep neural networks . binary neural networks are attractive in this case because the logical operations are very fast and efficient when implemented in hardware . we propose a transfer learning based architecture where we first train a binary network on imagenet and then retrain part of the network for different tasks while keeping most of the network fixed . the fixed binary part could be implemented in a hardware accelerator while the last layers of the network are evaluated in software . we show that a single binary neural network trained on the imagenet dataset can indeed be used as a feature extractor for other datasets .", "topics": ["image processing", "neural networks"]}
{"title": "local gaussian processes for efficient fine-grained traffic speed prediction", "abstract": "traffic speed is a key indicator for the efficiency of an urban transportation system . accurate modeling of the spatiotemporally varying traffic speed thus plays a crucial role in urban planning and development . this paper addresses the problem of efficient fine-grained traffic speed prediction using big traffic data obtained from static sensors . gaussian processes ( gps ) have been previously used to model various traffic phenomena , including flow and speed . however , gps do not scale with big traffic data due to their cubic time complexity . in this work , we address their efficiency issues by proposing local gps to learn from and make predictions for correlated subsets of data . the main idea is to quickly group speed variables in both spatial and temporal dimensions into a finite number of clusters , so that future and unobserved traffic speed queries can be heuristically mapped to one of such clusters . a local gp corresponding to that cluster can then be trained on the fly to make predictions in real-time . we call this method localization . we use non-negative matrix factorization for localization and propose simple heuristics for cluster mapping . we additionally leverage on the expressiveness of gp kernel functions to model road network topology and incorporate side information . extensive experiments using real-world traffic data collected in the two u.s. cities of pittsburgh and washington , d.c. , show that our proposed local gps significantly improve both runtime performances and prediction accuracies compared to the baseline global and local gps .", "topics": ["baseline ( configuration management )", "time complexity"]}
{"title": "a unified multi-faceted video summarization system", "abstract": "this paper addresses automatic summarization and search in visual data comprising of videos , live streams and image collections in a unified manner . in particular , we propose a framework for multi-faceted summarization which extracts key-frames ( image summaries ) , skims ( video summaries ) and entity summaries ( summarization at the level of entities like objects , scenes , humans and faces in the video ) . the user can either view these as extractive summarization , or query focused summarization . our approach first pre-processes the video or image collection once , to extract all important visual features , following which we provide an interactive mechanism to the user to summarize the video based on their choice . we investigate several diversity , coverage and representation models for all these problems , and argue the utility of these different mod- els depending on the application . while most of the prior work on submodular summarization approaches has focused on combining several models and learning weighted mixtures , we focus on the explain-ability of different the diversity , coverage and representation models and their scalability . most importantly , we also show that we can summarize hours of video data in a few seconds , and our system allows the user to generate summaries of various lengths and types interactively on the fly .", "topics": ["entity", "scalability"]}
{"title": "a bayesian encourages dropout", "abstract": "dropout is one of the key techniques to prevent the learning from overfitting . it is explained that dropout works as a kind of modified l2 regularization . here , we shed light on the dropout from bayesian standpoint . bayesian interpretation enables us to optimize the dropout rate , which is beneficial for learning of weight parameters and prediction after learning . the experiment result also encourages the optimization of the dropout .", "topics": ["matrix regularization", "bayesian network"]}
{"title": "fast stochastic variance reduced admm for stochastic composition optimization", "abstract": "we consider the stochastic composition optimization problem proposed in \\cite { wang2017stochastic } , which has applications ranging from estimation to statistical and machine learning . we propose the first admm-based algorithm named com-svr-admm , and show that com-svr-admm converges linearly for strongly convex and lipschitz smooth objectives , and has a convergence rate of $ o ( \\log s/s ) $ , which improves upon the $ o ( s^ { -4/9 } ) $ rate in \\cite { wang2016accelerating } when the objective is convex and lipschitz smooth . moreover , com-svr-admm possesses a rate of $ o ( 1/\\sqrt { s } ) $ when the objective is convex but without lipschitz smoothness . we also conduct experiments and show that it outperforms existing algorithms .", "topics": ["optimization problem"]}
{"title": "a modified construction for a support vector classifier to accommodate class imbalances", "abstract": "given a training set with binary classification , the support vector machine identifies the hyperplane maximizing the margin between the two classes of training data . this general formulation is useful in that it can be applied without regard to variance differences between the classes . ignoring these differences is not optimal , however , as the general svm will give the class with lower variance an unjustifiably wide berth . this increases the chance of misclassification of the other class and results in an overall loss of predictive performance . an alternate construction is proposed in which the margins of the separating hyperplane are different for each class , each proportional to the standard deviation of its class along the direction perpendicular to the hyperplane . the construction agrees with the svm in the case of equal class variances . this paper will then examine the impact to the dual representation of the modified constraint equations .", "topics": ["test set", "support vector machine"]}
{"title": "theory of dual-sparse regularized randomized reduction", "abstract": "in this paper , we study randomized reduction methods , which reduce high-dimensional features into low-dimensional space by randomized methods ( e.g . , random projection , random hashing ) , for large-scale high-dimensional classification . previous theoretical results on randomized reduction methods hinge on strong assumptions about the data , e.g . , low rank of the data matrix or a large separable margin of classification , which hinder their applications in broad domains . to address these limitations , we propose dual-sparse regularized randomized reduction methods that introduce a sparse regularizer into the reduced dual problem . under a mild condition that the original dual solution is a ( nearly ) sparse vector , we show that the resulting dual solution is close to the original dual solution and concentrates on its support set . in numerical experiments , we present an empirical study to support the analysis and we also present a novel application of the dual-sparse regularized randomized reduction methods to reducing the communication cost of distributed learning from large-scale high-dimensional data .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "filtering variational objectives", "abstract": "when used as a surrogate objective for maximum likelihood estimation in latent variable models , the evidence lower bound ( elbo ) produces state-of-the-art results . inspired by this , we consider the extension of the elbo to a family of lower bounds defined by a particle filter 's estimator of the marginal likelihood , the filtering variational objectives ( fivos ) . fivos take the same arguments as the elbo , but can exploit a model 's sequential structure to form tighter bounds . we present results that relate the tightness of fivo 's bound to the variance of the particle filter 's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators . experimentally , we show that training with fivo results in substantial improvements over training the same model architecture with the elbo on sequential data .", "topics": ["calculus of variations"]}
{"title": "exploring the entire regularization path for the asymmetric cost linear support vector machine", "abstract": "we propose an algorithm for exploring the entire regularization path of asymmetric-cost linear support vector machines . empirical evidence suggests the predictive power of support vector machines depends on the regularization parameters of the training algorithms . the algorithms exploring the entire regularization paths have been proposed for single-cost support vector machines thereby providing the complete knowledge on the behavior of the trained model over the hyperparameter space . considering the problem in two-dimensional hyperparameter space though enables our algorithm to maintain greater flexibility in dealing with special cases and sheds light on problems encountered by algorithms building the paths in one-dimensional spaces . we demonstrate two-dimensional regularization paths for linear support vector machines that we train on synthetic and real data .", "topics": ["support vector machine", "synthetic data"]}
{"title": "complex and holographic embeddings of knowledge graphs : a comparison", "abstract": "embeddings of knowledge graphs have received significant attention due to their excellent performance for tasks like link prediction and entity resolution . in this short paper , we are providing a comparison of two state-of-the-art knowledge graph embeddings for which their equivalence has recently been established , i.e . , complex and hole [ nickel , rosasco , and poggio , 2016 ; trouillon et al . , 2016 ; hayashi and shimbo , 2017 ] . first , we briefly review both models and discuss how their scoring functions are equivalent . we then analyze the discrepancy of results reported in the original articles , and show experimentally that they are likely due to the use of different loss functions . in further experiments , we evaluate the ability of both models to embed symmetric and antisymmetric patterns . finally , we discuss advantages and disadvantages of both models and under which conditions one would be preferable to the other .", "topics": ["loss function"]}
{"title": "approximate bayesian image interpretation using generative probabilistic graphics programs", "abstract": "the idea of computer vision as the bayesian inverse problem to computer graphics has a long history and an appealing elegance , but it has proved difficult to directly implement . instead , most vision tasks are approached via complex bottom-up processing pipelines . here we show that it is possible to write short , simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images . generative probabilistic graphics programs consist of a stochastic scene generator , a renderer based on graphics software , a stochastic likelihood model linking the renderer 's output and the data , and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model . representations and algorithms from computer graphics , originally designed to produce high-quality images , are instead used as the deterministic backbone for highly approximate and stochastic generative models . this formulation combines probabilistic programming , computer graphics , and approximate bayesian computation , and depends only on general-purpose , automatic inference techniques . we describe two applications : reading sequences of degraded and adversarially obscured alphanumeric characters , and inferring 3d road models from vehicle-mounted camera images . each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code , and supports accurate , approximately bayesian inferences about ambiguous real-world images .", "topics": ["generative model", "approximation algorithm"]}
{"title": "inference in hidden markov models with explicit state duration distributions", "abstract": "in this letter we borrow from the inference techniques developed for unbounded state-cardinality ( nonparametric ) variants of the hmm and use them to develop a tuning-parameter free , black-box inference procedure for explicit-state-duration hidden markov models ( edhmm ) . edhmms are hmms that have latent states consisting of both discrete state-indicator and discrete state-duration random variables . in contrast to the implicit geometric state duration distribution possessed by the standard hmm , edhmms allow the direct parameterisation and estimation of per-state duration distributions . as most duration distributions are defined over the positive integers , truncation or other approximations are usually required to perform edhmm inference .", "topics": ["approximation", "markov chain"]}
{"title": "efficient blind compressed sensing using sparsifying transforms with convergence guarantees and application to mri", "abstract": "natural signals and images are well-known to be approximately sparse in transform domains such as wavelets and dct . this property has been heavily exploited in various applications in image processing and medical imaging . compressed sensing exploits the sparsity of images or image patches in a transform domain or synthesis dictionary to reconstruct images from undersampled measurements . in this work , we focus on blind compressed sensing , where the underlying sparsifying transform is a priori unknown , and propose a framework to simultaneously reconstruct the underlying image as well as the sparsifying transform from highly undersampled measurements . the proposed block coordinate descent type algorithms involve highly efficient optimal updates . importantly , we prove that although the proposed blind compressed sensing formulations are highly nonconvex , our algorithms are globally convergent ( i.e . , they converge from any initialization ) to the set of critical points of the objectives defining the formulations . these critical points are guaranteed to be at least partial global and partial local minimizers . the exact point ( s ) of convergence may depend on initialization . we illustrate the usefulness of the proposed framework for magnetic resonance image reconstruction from highly undersampled k-space measurements . as compared to previous methods involving the synthesis dictionary model , our approach is much faster , while also providing promising reconstruction quality .", "topics": ["image processing", "sparse matrix"]}
{"title": "localized complexities for transductive learning", "abstract": "we show two novel concentration inequalities for suprema of empirical processes when sampling without replacement , which both take the variance of the functions into account . while these inequalities may potentially have broad applications in learning theory in general , we exemplify their significance by studying the transductive setting of learning theory . for which we provide the first excess risk bounds based on the localized complexity of the hypothesis class , which can yield fast rates of convergence also in the transductive learning setting . we give a preliminary analysis of the localized complexities for the prominent case of kernel classes .", "topics": ["sampling ( signal processing )"]}
{"title": "hierarchical mixture-of-experts model for large-scale gaussian process regression", "abstract": "we propose a practical and scalable gaussian process model for large-scale nonlinear probabilistic regression . our mixture-of-experts model is conceptually simple and hierarchically recombines computations for an overall approximation of a full gaussian process . closed-form and distributed computations allow for efficient and massive parallelisation while keeping the memory consumption small . given sufficient computing resources , our model can handle arbitrarily large data sets , without explicit sparse approximations . we provide strong experimental evidence that our model can be applied to large data sets of sizes far beyond millions . hence , our model has the potential to lay the foundation for general large-scale gaussian process research .", "topics": ["nonlinear system", "sparse matrix"]}
{"title": "multi-timescale nexting in a reinforcement learning robot", "abstract": "the term `` nexting '' has been used by psychologists to refer to the propensity of people and many other animals to continually predict what will happen next in an immediate , local , and personal sense . the ability to `` next '' constitutes a basic kind of awareness and knowledge of one 's environment . in this paper we present results with a robot that learns to next in real time , predicting thousands of features of the world 's state , including all sensory inputs , at timescales from 0.1 to 8 seconds . this was achieved by treating each state feature as a reward-like target and applying temporal-difference methods to learn a corresponding value function with a discount rate corresponding to the timescale . we show that two thousand predictions , each dependent on six thousand state features , can be learned and updated online at better than 10hz on a laptop computer , using the standard td ( lambda ) algorithm with linear function approximation . we show that this approach is efficient enough to be practical , with most of the learning complete within 30 minutes . we also show that a single tile-coded feature representation suffices to accurately predict many different signals at a significant range of timescales . finally , we show that the accuracy of our learned predictions compares favorably with the optimal off-line solution .", "topics": ["computational complexity theory", "reinforcement learning"]}
{"title": "multiple-view spectral clustering for group-wise functional community detection", "abstract": "functional connectivity analysis yields powerful insights into our understanding of the human brain . group-wise functional community detection aims to partition the brain into clusters , or communities , in which functional activity is inter-regionally correlated in a common manner across a group of subjects . in this article , we show how to use multiple-view spectral clustering to perform group-wise functional community detection . in a series of experiments on 291 subjects from the human connectome project , we compare three versions of multiple-view spectral clustering : mvsc ( uniform weights ) , mvscw ( weights based on subject-specific embedding quality ) , and aasc ( weights optimized along with the embedding ) with the competing technique of joint diagonalization of laplacians ( jdl ) . results show that multiple-view spectral clustering not only yields group-wise functional communities that are more consistent than jdl when using randomly selected subsets of individual brains , but it is several orders of magnitude faster than jdl .", "topics": ["cluster analysis"]}
{"title": "group sparse additive models", "abstract": "we consider the problem of sparse variable selection in nonparametric additive models , with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly . previous works either study the group sparsity in the parametric setting ( e.g . , group lasso ) , or address the problem in the non-parametric setting without exploiting the structural information ( e.g . , sparse additive models ) . in this paper , we present a new method , called group sparse additive models ( groupspam ) , which can handle group sparsity in additive models . we generalize the l1/l2 norm to hilbert spaces as the sparsity-inducing penalty in groupspam . moreover , we derive a novel thresholding condition for identifying the functional sparsity at the group level , and propose an efficient block coordinate descent algorithm for constructing the estimate . we demonstrate by simulation that groupspam substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models , and also conduct a comparative experiment on a real breast cancer dataset .", "topics": ["simulation", "sparse matrix"]}
{"title": "a regularization approach to blind deblurring and denoising of qr barcodes", "abstract": "qr bar codes are prototypical images for which part of the image is a priori known ( required patterns ) . open source bar code readers , such as zbar , are readily available . we exploit both these facts to provide and assess purely regularization-based methods for blind deblurring of qr bar codes in the presence of noise .", "topics": ["noise reduction", "matrix regularization"]}
{"title": "divide and conquer networks", "abstract": "we consider the learning of algorithmic tasks by mere observation of input-output pairs . rather than studying this as a black-box discrete regression problem with no assumption whatsoever on the input-output mapping , we concentrate on tasks that are amenable to the principle of divide and conquer , and study what are its implications in terms of learning . this principle creates a powerful inductive bias that we leverage with neural archi- tectures that are defined recursively and dynamically , by learning two scale-invariant atomic operations : how to split a given input into smaller sets , and how to merge two partially solved tasks into a larger partial solution . our model can be trained in weakly supervised environments , namely by just observing input-output pairs , and in even weaker environments , using a non-differentiable reward signal . moreover , thanks to the dynamic aspect of our architecture , we can incorporate the computational complexity as a regularization term that can be optimized by backpropagation . we demonstrate the flexibility and efficiency of the divide-and-conquer network on three combinatorial and geometric tasks : sorting , clustering and convex hulls . thanks to the dynamic program- ming nature of our model , we show significant improvements in terms of generalization error and computational complexity", "topics": ["cluster analysis", "supervised learning"]}
{"title": "approximate learning in complex dynamic bayesian networks", "abstract": "in this paper we extend the work of smith and papamichail ( 1999 ) and present fast approximate bayesian algorithms for learning in complex scenarios where at any time frame , the relationships between explanatory state space variables can be described by a bayesian network that evolve dynamically over time and the observations taken are not necessarily gaussian . it uses recent developments in approximate bayesian forecasting methods in combination with more familiar gaussian propagation algorithms on junction trees . the procedure for learning state parameters from data is given explicitly for common sampling distributions and the methodology is illustrated through a real application . the efficiency of the dynamic approximation is explored by using the hellinger divergence measure and theoretical bounds for the efficacy of such a procedure are discussed .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "toward deeper understanding of neural networks : the power of initialization and a dual view on expressivity", "abstract": "we develop a general duality between neural networks and compositional kernels , striving towards a better understanding of deep learning . we show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space . hence , though the training objective is hard to optimize in the worst case , the initial weights form a good starting point for optimization . our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power .", "topics": ["approximation algorithm", "nonlinear system"]}
{"title": "frequency domain tof : encoding object depth in modulation frequency", "abstract": "time of flight cameras may emerge as the 3-d sensor of choice . today , time of flight sensors use phase-based sampling , where the phase delay between emitted and received , high-frequency signals encodes distance . in this paper , we present a new time of flight architecture that relies only on frequency -- -we refer to this technique as frequency-domain time of flight ( fd-tof ) . inspired by optical coherence tomography ( oct ) , fd-tof excels when frequency bandwidth is high . with the increasing frequency of tof sensors , new challenges to time of flight sensing continue to emerge . at high frequencies , fd-tof offers several potential benefits over phase-based time of flight methods .", "topics": ["sampling ( signal processing )", "sensor"]}
{"title": "learning mid-level features and modeling neuron selectivity for image classification", "abstract": "we now know that mid-level features can greatly enhance the performance of image learning , but how to automatically learn the image features efficiently and in an unsupervised manner is still an open question . in this paper , we present a very efficient mid-level feature learning approach ( midfea ) , which only involves simple operations such as $ k $ -means clustering , convolution , pooling , vector quantization and random projection . we explain why this simple method generates the desired features , and argue that there is no need to spend much time in learning low-level feature extractors . furthermore , to boost the performance , we propose to model the neuron selectivity ( ns ) principle by building an additional layer over the mid-level features before feeding the features into the classifier . we show that the ns-layer learns category-specific neurons with both bottom-up inference and top-down analysis , and thus supports fast inference for a query image . we run extensive experiments on several public databases to demonstrate that our approach can achieve state-of-the-art performances for face recognition , gender classification , age estimation and object categorization . in particular , we demonstrate that our approach is more than an order of magnitude faster than some recently proposed sparse coding based methods .", "topics": ["feature learning", "cluster analysis"]}
{"title": "similarity learning for high-dimensional sparse data", "abstract": "a good measure of similarity between data points is crucial to many tasks in machine learning . similarity and metric learning methods learn such measures automatically from data , but they do not scale well respect to the dimensionality of the data . in this paper , we propose a method that can learn efficiently similarity measure from high-dimensional sparse data . the core idea is to parameterize the similarity measure as a convex combination of rank-one matrices with specific sparsity structures . the parameters are then optimized with an approximate frank-wolfe procedure to maximally satisfy relative similarity constraints on the training data . our algorithm greedily incorporates one pair of features at a time into the similarity measure , providing an efficient way to control the number of active features and thus reduce overfitting . it enjoys very appealing convergence guarantees and its time and memory complexity depends on the sparsity of the data instead of the dimension of the feature space . our experiments on real-world high-dimensional datasets demonstrate its potential for classification , dimensionality reduction and data exploration .", "topics": ["test set", "approximation algorithm"]}
{"title": "automatic representation for lifetime value recommender systems", "abstract": "many modern commercial sites employ recommender systems to propose relevant content to users . while most systems are focused on maximizing the immediate gain ( clicks , purchases or ratings ) , a better notion of success would be the lifetime value ( ltv ) of the user-system interaction . the ltv approach considers the future implications of the item recommendation , and seeks to maximize the cumulative gain over time . the reinforcement learning ( rl ) framework is the standard formulation for optimizing cumulative successes over time . however , rl is rarely used in practice due to its associated representation , optimization and validation techniques which can be complex . in this paper we propose a new architecture for combining rl with recommendation systems which obviates the need for hand-tuned features , thus automating the state-space representation construction process . we analyze the practical difficulties in this formulation and test our solutions on batch off-line real-world recommendation data .", "topics": ["reinforcement learning"]}
{"title": "gradient descent gan optimization is locally stable", "abstract": "despite the growing prominence of generative adversarial networks ( gans ) , optimization in gans is still a poorly understood topic . in this paper , we analyze the `` gradient descent '' form of gan optimization i.e . , the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters . we show that even though gan optimization does not correspond to a convex-concave game ( even for simple parameterizations ) , under proper conditions , equilibrium points of this optimization procedure are still \\emph { locally asymptotically stable } for the traditional gan formulation . on the other hand , we show that the recently proposed wasserstein gan can have non-convergent limit cycles near equilibrium . motivated by this stability analysis , we propose an additional regularization term for gradient descent gan updates , which \\emph { is } able to guarantee local stability for both the wgan and the traditional gan , and also shows practical promise in speeding up convergence and addressing mode collapse .", "topics": ["generative model", "gradient descent"]}
{"title": "an efficient character-level neural machine translation", "abstract": "neural machine translation aims at building a single large neural network that can be trained to maximize translation performance . the encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems on the task of english-to-french translation . however , the use of large vocabulary becomes the bottleneck in both training and improving the performance . in this paper , we propose an efficient architecture to train a deep character-level neural machine translation by introducing a decimator and an interpolator . the decimator is used to sample the source sequence before encoding while the interpolator is used to resample after decoding . such a deep model has two major advantages . it avoids the large vocabulary issue radically ; at the same time , it is much faster and more memory-efficient in training than conventional character-based models . more interestingly , our model is able to translate the misspelled word like human beings .", "topics": ["machine translation", "encoder"]}
{"title": "a unified approach for learning the parameters of sum-product networks", "abstract": "we present a unified approach for learning the parameters of sum-product networks ( spns ) . we prove that any complete and decomposable spn is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions . based on the mixture model perspective , we characterize the objective function when learning spns based on the maximum likelihood estimation ( mle ) principle and show that the optimization problem can be formulated as a signomial program . we construct two parameter learning algorithms for spns by using sequential monomial approximations ( sma ) and the concave-convex procedure ( cccp ) , respectively . the two proposed methods naturally admit multiplicative updates , hence effectively avoiding the projection operation . with the help of the unified framework , we also show that , in the case of spns , cccp leads to the same algorithm as expectation maximization ( em ) despite the fact that they are different in general .", "topics": ["optimization problem", "loss function"]}
{"title": "mapping unseen words to task-trained embedding spaces", "abstract": "we consider the supervised training setting in which we learn task-specific word embeddings . we assume that we start with initial embeddings learned from unlabelled data and update them to learn task-specific embeddings for words in the supervised training data . however , for new words in the test set , we must use either their initial embeddings or a single unknown embedding , which often leads to errors . we address this by learning a neural network to map from initial embeddings to the task-specific embedding space , via a multi-loss objective function . the technique is general , but here we demonstrate its use for improved dependency parsing ( especially for sentences with out-of-vocabulary words ) , as well as for downstream improvements on sentiment analysis .", "topics": ["test set", "optimization problem"]}
{"title": "least square error method robustness of computation : what is not usually considered and taught", "abstract": "there are many practical applications based on the least square error ( lse ) approximation . it is based on a square error minimization 'on a vertical ' axis . the lse method is simple and easy also for analytical purposes . however , if data span is large over several magnitudes or non-linear lse is used , severe numerical instability can be expected . the presented contribution describes a simple method for large span of data lse computation . it is especially convenient if large span of data are to be processed , when the 'standard ' pseudoinverse matrix is ill conditioned . it is actually based on a lse solution using orthogonal basis vectors instead of orthonormal basis vectors . the presented approach has been used for a linear regression as well as for approximation using radial basis functions .", "topics": ["numerical analysis", "nonlinear system"]}
{"title": "co-occurrence of the benford-like and zipf laws arising from the texts representing human and artificial languages", "abstract": "we demonstrate that large texts , representing human ( english , russian , ukrainian ) and artificial ( c++ , java ) languages , display quantitative patterns characterized by the benford-like and zipf laws . the frequency of a word following the zipf law is inversely proportional to its rank , whereas the total numbers of a certain word appearing in the text generate the uneven benford-like distribution of leading numbers . excluding the most popular words essentially improves the correlation of actual textual data with the zipfian distribution , whereas the benford distribution of leading numbers ( arising from the overall amount of a certain word ) is insensitive to the same elimination procedure . the calculated values of the moduli of slopes of double logarithmical plots for artificial languages ( c++ , java ) are markedly larger than those for human ones .", "topics": ["text corpus"]}
{"title": "diving deeper into mentee networks", "abstract": "modern computer vision is all about the possession of powerful image representations . deeper and deeper convolutional neural networks have been built using larger and larger datasets and are made publicly available . a large swath of computer vision scientists use these pre-trained networks with varying degrees of successes in various tasks . even though there is tremendous success in copying these networks , the representational space is not learnt from the target dataset in a traditional manner . one of the reasons for opting to use a pre-trained network over a network learnt from scratch is that small datasets provide less supervision and require meticulous regularization , smaller and careful tweaking of learning rates to even achieve stable learning without weight explosion . it is often the case that large deep networks are not portable , which necessitates the ability to learn mid-sized networks from scratch . in this article , we dive deeper into training these mid-sized networks on small datasets from scratch by drawing additional supervision from a large pre-trained network . such learning also provides better generalization accuracies than networks trained with common regularization techniques such as l2 , l1 and dropouts . we show that features learnt thus , are more general than those learnt independently . we studied various characteristics of such networks and found some interesting behaviors .", "topics": ["computer vision", "matrix regularization"]}
{"title": "sharp threshold for multivariate multi-response linear regression via block regularized lasso", "abstract": "in this paper , we investigate a multivariate multi-response ( mvmr ) linear regression problem , which contains multiple linear regression models with differently distributed design matrices , and different regression and output vectors . the goal is to recover the support union of all regression vectors using $ l_1/l_2 $ -regularized lasso . we characterize sufficient and necessary conditions on sample complexity \\emph { as a sharp threshold } to guarantee successful recovery of the support union . namely , if the sample size is above the threshold , then $ l_1/l_2 $ -regularized lasso correctly recovers the support union ; and if the sample size is below the threshold , $ l_1/l_2 $ -regularized lasso fails to recover the support union . in particular , the threshold precisely captures the impact of the sparsity of regression vectors and the statistical properties of the design matrices on sample complexity . therefore , the threshold function also captures the advantages of joint support union recovery using multi-task lasso over individual support recovery using single-task lasso .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "analysis of crowdsourced sampling strategies for hodgerank with sparse random graphs", "abstract": "crowdsourcing platforms are now extensively used for conducting subjective pairwise comparison studies . in this setting , a pairwise comparison dataset is typically gathered via random sampling , either \\emph { with } or \\emph { without } replacement . in this paper , we use tools from random graph theory to analyze these two random sampling methods for the hodgerank estimator . using the fiedler value of the graph as a measurement for estimator stability ( informativeness ) , we provide a new estimate of the fiedler value for these two random graph models . in the asymptotic limit as the number of vertices tends to infinity , we prove the validity of the estimate . based on our findings , for a small number of items to be compared , we recommend a two-stage sampling strategy where a greedy sampling method is used initially and random sampling \\emph { without } replacement is used in the second stage . when a large number of items is to be compared , we recommend random sampling with replacement as this is computationally inexpensive and trivially parallelizable . experiments on synthetic and real-world datasets support our analysis .", "topics": ["sampling ( signal processing )", "synthetic data"]}
{"title": "regularized laplacian estimation and fast eigenvector approximation", "abstract": "recently , mahoney and orecchia demonstrated that popular diffusion-based procedures to compute a quick \\emph { approximation } to the first nontrivial eigenvector of a data graph laplacian \\emph { exactly } solve certain regularized semi-definite programs ( sdps ) . in this paper , we extend that result by providing a statistical interpretation of their approximation procedure . our interpretation will be analogous to the manner in which $ \\ell_2 $ -regularized or $ \\ell_1 $ -regularized $ \\ell_2 $ -regression ( often called ridge regression and lasso regression , respectively ) can be interpreted in terms of a gaussian prior or a laplace prior , respectively , on the coefficient vector of the regression problem . our framework will imply that the solutions to the mahoney-orecchia regularized sdp can be interpreted as regularized estimates of the pseudoinverse of the graph laplacian . conversely , it will imply that the solution to this regularized estimation problem can be computed very quickly by running , e.g . , the fast diffusion-based pagerank procedure for computing an approximation to the first nontrivial eigenvector of the graph laplacian . empirical results are also provided to illustrate the manner in which approximate eigenvector computation \\emph { implicitly } performs statistical regularization , relative to running the corresponding exact algorithm .", "topics": ["approximation algorithm", "matrix regularization"]}
{"title": "sequence-to-sequence learning as beam-search optimization", "abstract": "sequence-to-sequence ( seq2seq ) modeling has rapidly become an important general-purpose nlp tool that has proven effective for many text-generation and sequence-labeling tasks . seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local , next-word distributions . in this work , we introduce a model and beam-search training scheme , based on the work of daume iii and marcu ( 2005 ) , that extends seq2seq to learn global sequence scores . this structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage , while preserving the proven model architecture of seq2seq and its efficient training approach . we show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks : word ordering , parsing , and machine translation .", "topics": ["natural language processing", "machine translation"]}
{"title": "from dependency to causality : a machine learning approach", "abstract": "the relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference . recent results in the chalearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in markov indistinguishable configurations thanks to data driven approaches . this paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with $ n > 2 $ variables . the approach relies on the asymmetry of some conditional ( in ) dependence relations between the members of the markov blankets of two variables causally connected . our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for $ n > 2 $ variate distributions .", "topics": ["supervised learning", "causality"]}
{"title": "ai2-thor : an interactive 3d environment for visual ai", "abstract": "we introduce the house of interactions ( thor ) , a framework for visual ai research , available at http : //ai2thor.allenai.org . ai2-thor consists of near photo-realistic 3d indoor scenes , where ai agents can navigate in the scenes and interact with objects to perform tasks . ai2-thor enables research in many different domains including but not limited to deep reinforcement learning , imitation learning , learning by interaction , planning , visual question answering , unsupervised representation learning , object detection and segmentation , and learning models of cognition . the goal of ai2-thor is to facilitate building visually intelligent models and push the research forward in this domain .", "topics": ["feature learning", "object detection"]}
{"title": "on clustering time series using euclidean distance and pearson correlation", "abstract": "for time series comparisons , it has often been observed that z-score normalized euclidean distances far outperform the unnormalized variant . in this paper we show that a z-score normalized , squared euclidean distance is , in fact , equal to a distance based on pearson correlation . this has profound impact on many distance-based classification or clustering methods . in addition to this theoretically sound result we also show that the often used k-means algorithm formally needs a mod ification to keep the interpretation as pearson correlation strictly valid . experimental results demonstrate that in many cases the standard k-means algorithm generally produces the same results .", "topics": ["cluster analysis", "time series"]}
{"title": "maximum entropy for collaborative filtering", "abstract": "within the task of collaborative filtering two challenges for computing conditional probabilities exist . first , the amount of training data available is typically sparse with respect to the size of the domain . thus , support for higher-order interactions is generally not present . second , the variables that we are conditioning upon vary for each query . that is , users label different variables during each query . for this reason , there is no consistent input to output mapping . to address these problems we purpose a maximum entropy approach using a non-standard measure of entropy . this approach can be simplified to solving a set of linear equations that can be efficiently solved .", "topics": ["test set", "interaction"]}
{"title": "adversarial patrolling with spatially uncertain alarm signals", "abstract": "when securing complex infrastructures or large environments , constant surveillance of every area is not affordable . to cope with this issue , a common countermeasure is the usage of cheap but wide-ranged sensors , able to detect suspicious events that occur in large areas , supporting patrollers to improve the effectiveness of their strategies . however , such sensors are commonly affected by uncertainty . in the present paper , we focus on spatially uncertain alarm signals . that is , the alarm system is able to detect an attack but it is uncertain on the exact position where the attack is taking place . this is common when the area to be secured is wide such as in border patrolling and fair site surveillance . we propose , to the best of our knowledge , the first patrolling security game model where a defender is supported by a spatially uncertain alarm system which non-deterministically generates signals once a target is under attack . we show that finding the optimal strategy in arbitrary graphs is apx-hard even in zero-sum games and we provide two ( exponential time ) exact algorithms and two ( polynomial time ) approximation algorithms . furthermore , we analyse what happens in environments with special topologies , showing that in linear and cycle graphs the optimal patrolling strategy can be found in polynomial time , de facto allowing our algorithms to be used in real-life scenarios , while in trees the problem is np-hard . finally , we show that without false positives and missed detections , the best patrolling strategy reduces to stay in a place , wait for a signal , and respond to it at best . this strategy is optimal even with non-negligible missed detection rates , which , unfortunately , affect every commercial alarm system . we evaluate our methods in simulation , assessing both quantitative and qualitative aspects .", "topics": ["approximation algorithm", "time complexity"]}
{"title": "a geometric framework for convolutional neural networks", "abstract": "in this paper , a geometric framework for neural networks is proposed . this framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component-based form , but in a coordinate-free manner . convolutional neural networks are described in this framework in a compact form , with the gradients of standard -- - and higher-order -- - loss functions calculated for each layer of the network . this approach can be applied to other network structures and provides a basis on which to create new networks .", "topics": ["neural networks", "loss function"]}
{"title": "natural language processing with small feed-forward networks", "abstract": "we show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models . motivated by resource-constrained environments like mobile phones , we showcase simple techniques for obtaining such small neural network models , and investigate different tradeoffs when deciding how to allocate a small memory budget .", "topics": ["natural language processing", "natural language"]}
{"title": "interpretable explanations of black boxes by meaningful perturbation", "abstract": "as machine learning algorithms are increasingly applied to high impact yet high risk tasks , such as medical diagnosis or autonomous driving , it is critical that researchers can explain how such algorithms arrived at their predictions . in recent years , a number of image saliency methods have been developed to summarize where highly complex neural networks `` look '' in an image for evidence for their predictions . however , these techniques are limited by their heuristic nature and architectural constraints . in this paper , we make two main contributions : first , we propose a general framework for learning different kinds of explanations for any black box algorithm . second , we specialise the framework to find the part of an image most responsible for a classifier decision . unlike previous works , our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations .", "topics": ["statistical classification", "heuristic"]}
{"title": "genetic algorithms in time-dependent environments", "abstract": "the influence of time-dependent fitnesses on the infinite population dynamics of simple genetic algorithms ( without crossover ) is analyzed . based on general arguments , a schematic phase diagram is constructed that allows one to characterize the asymptotic states in dependence on the mutation rate and the time scale of changes . furthermore , the notion of regular changes is raised for which the population can be shown to converge towards a generalized quasispecies . based on this , error thresholds and an optimal mutation rate are approximately calculated for a generational genetic algorithm with a moving needle-in-the-haystack landscape . the so found phase diagram is fully consistent with our general considerations .", "topics": ["interaction"]}
{"title": "the effects of memory replay in reinforcement learning", "abstract": "experience replay is a key technique behind many recent advances in deep reinforcement learning . allowing the agent to learn from earlier memories can speed up learning and break undesirable temporal correlations . despite its wide-spread application , very little is understood about the properties of experience replay . how does the amount of memory kept affect learning dynamics ? does it help to prioritize certain experiences ? in this paper , we address these questions by formulating a dynamical systems ode model of q-learning with experience replay . we derive analytic solutions of the ode for a simple setting . we show that even in this very simple setting , the amount of memory kept can substantially affect the agent 's performance . too much or too little memory both slow down learning . moreover , we characterize regimes where prioritized replay harms the agent 's learning . we show that our analytic solutions have excellent agreement with experiments . finally , we propose a simple algorithm for adaptively changing the memory buffer size which achieves consistently good empirical performance .", "topics": ["reinforcement learning"]}
{"title": "the predictron : end-to-end learning and planning", "abstract": "one of the key challenges of artificial intelligence is to learn models that are effective in the context of planning . in this document we introduce the predictron architecture . the predictron consists of a fully abstract model , represented by a markov reward process , that can be rolled forward multiple `` imagined '' planning steps . each forward pass of the predictron accumulates internal rewards and values over multiple planning depths . the predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function . we applied the predictron to procedurally generated random mazes and a simulator for the game of pool . the predictron yielded significantly more accurate predictions than conventional deep neural network architectures .", "topics": ["approximation algorithm", "value ( ethics )"]}
{"title": "preference completion : large-scale collaborative ranking from pairwise comparisons", "abstract": "in this paper we consider the collaborative ranking setting : a pool of users each provides a small number of pairwise preferences between $ d $ possible items ; from these we need to predict preferences of the users for items they have not yet seen . we do so by fitting a rank $ r $ score matrix to the pairwise data , and provide two main contributions : ( a ) we show that an algorithm based on convex optimization provides good generalization guarantees once each user provides as few as $ o ( r\\log^2 d ) $ pairwise comparisons -- essentially matching the sample complexity required in the related matrix completion setting ( which uses actual numerical as opposed to pairwise information ) , and ( b ) we develop a large-scale non-convex implementation , which we call altsvm , that trains a factored form of the matrix via alternating minimization ( which we show reduces to alternating svm problems ) , and scales and parallelizes very well to large problem settings . it also outperforms common baselines on many moderately large popular collaborative filtering datasets in both ndcg and in other measures of ranking performance .", "topics": ["numerical analysis"]}
{"title": "parsimonious topic models with salient word discovery", "abstract": "we propose a parsimonious topic model for text corpora . in related models such as latent dirichlet allocation ( lda ) , all words are modeled topic-specifically , even though many words occur with similar frequencies across different topics . our modeling determines salient words for each topic , which have topic-specific probabilities , with the rest explained by a universal shared model . further , in lda all topics are in principle present in every document . by contrast our model gives sparse topic representation , determining the ( small ) subset of relevant topics for each document . we derive a bayesian information criterion ( bic ) , balancing model complexity and goodness of fit . here , interestingly , we identify an effective sample size and corresponding penalty specific to each parameter type in our model . we minimize bic to jointly determine our entire model -- the topic-specific words , document-specific topics , all model parameter values , { \\it and } the total number of topics -- in a wholly unsupervised fashion . results on three text corpora and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels , compared to lda and to a model designed to incorporate sparsity .", "topics": ["test set", "value ( ethics )"]}
{"title": "latent intention dialogue models", "abstract": "developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research . traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability . in this paper , we propose a latent intention dialogue model ( lidm ) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference . in a goal-oriented dialogue scenario , these latent intentions can be interpreted as actions guiding the generation of machine responses , which can be further refined autonomously by reinforcement learning . the experimental evaluation of lidm shows that the model out-performs published benchmarks for both corpus-based and human evaluation , demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues .", "topics": ["calculus of variations", "reinforcement learning"]}
{"title": "unsupervised domain adaptation : from simulation engine to the realworld", "abstract": "large-scale labeled training datasets have enabled deep neural networks to excel on a wide range of benchmark vision tasks . however , in many applications it is prohibitively expensive or time-consuming to obtain large quantities of labeled data . to cope with limited labeled training data , many have attempted to directly apply models trained on a large-scale labeled source domain to another sparsely labeled target domain . unfortunately , direct transfer across domains often performs poorly due to domain shift and dataset bias . domain adaptation is the machine learning paradigm that aims to learn a model from a source domain that can perform well on a different ( but related ) target domain . in this paper , we summarize and compare the latest unsupervised domain adaptation methods in computer vision applications . we classify the non-deep approaches into sample re-weighting and intermediate subspace transformation categories , while the deep strategy includes discrepancy-based methods , adversarial generative models , adversarial discriminative models and reconstruction-based methods . we also discuss some potential directions .", "topics": ["test set", "computer vision"]}
{"title": "deep component analysis via alternating direction neural networks", "abstract": "despite a lack of theoretical understanding , deep neural networks have achieved unparalleled performance in a wide range of applications . on the other hand , shallow representation learning with component analysis is associated with rich intuition and theory , but smaller capacity often limits its usefulness . to bridge this gap , we introduce deep component analysis ( deepca ) , an expressive multilayer model formulation that enforces hierarchical structure through constraints on latent variables in each layer . for inference , we propose a differentiable optimization algorithm implemented using recurrent alternating direction neural networks ( adnns ) that enable parameter learning using standard backpropagation . by interpreting feed-forward networks as single-iteration approximations of inference in our model , we provide both a novel theoretical perspective for understanding them and a practical technique for constraining predictions with prior knowledge . experimentally , we demonstrate performance improvements on a variety of tasks , including single-image depth prediction with sparse output constraints .", "topics": ["feature learning", "neural networks"]}
{"title": "mesh learning for classifying cognitive processes", "abstract": "a relatively recent advance in cognitive neuroscience has been multi-voxel pattern analysis ( mvpa ) , which enables researchers to decode brain states and/or the type of information represented in the brain during a cognitive operation . mvpa methods utilize machine learning algorithms to distinguish among types of information or cognitive states represented in the brain , based on distributed patterns of neural activity . in the current investigation , we propose a new approach for representation of neural data for pattern analysis , namely a mesh learning model . in this approach , at each time instant , a star mesh is formed around each voxel , such that the voxel corresponding to the center node is surrounded by its p-nearest neighbors . the arc weights of each mesh are estimated from the voxel intensity values by least squares method . the estimated arc weights of all the meshes , called mesh arc descriptors ( mads ) , are then used to train a classifier , such as neural networks , k-nearest neighbor , na\\ '' ive bayes and support vector machines . the proposed mesh model was tested on neuroimaging data acquired via functional magnetic resonance imaging ( fmri ) during a recognition memory experiment using categorized word lists , employing a previously established experimental paradigm ( \\ '' oztekin & badre , 2011 ) . results suggest that the proposed mesh learning approach can provide an effective algorithm for pattern analysis of brain activity during cognitive processing .", "topics": ["support vector machine", "neural networks"]}
{"title": "a nonlinear orthogonal non-negative matrix factorization approach to subspace clustering", "abstract": "a recent theoretical analysis shows the equivalence between non-negative matrix factorization ( nmf ) and spectral clustering based approach to subspace clustering . as nmf and many of its variants are essentially linear , we introduce a nonlinear nmf with explicit orthogonality and derive general kernel-based orthogonal multiplicative update rules to solve the subspace clustering problem . in nonlinear orthogonal nmf framework , we propose two subspace clustering algorithms , named kernel-based non-negative subspace clustering knsc-ncut and knsc-rcut and establish their connection with spectral normalized cut and ratio cut clustering . we further extend the nonlinear orthogonal nmf framework and introduce a graph regularization to obtain a factorization that respects a local geometric structure of the data after the nonlinear mapping . the proposed nmf-based approach to subspace clustering takes into account the nonlinear nature of the manifold , as well as its intrinsic local geometry , which considerably improves the clustering performance when compared to the several recently proposed state-of-the-art methods .", "topics": ["cluster analysis", "nonlinear system"]}
{"title": "general vector machine", "abstract": "the support vector machine ( svm ) is an important class of learning machines for function approach , pattern recognition , and time-serious prediction , etc . it maps samples into the feature space by so-called support vectors of selected samples , and then feature vectors are separated by maximum margin hyperplane . the present paper presents the general vector machine ( gvm ) to replace the svm . the support vectors are replaced by general project vectors selected from the usual vector space , and a monte carlo ( mc ) algorithm is developed to find the general vectors . the general project vectors improves the feature-extraction ability , and the mc algorithm can control the width of the separation margin of the hyperplane . by controlling the separation margin , we show that the maximum margin hyperplane can usually induce the overlearning , and the best learning machine is achieved with a proper separation margin . applications in function approach , pattern recognition , and classification indicate that the developed method is very successful , particularly for small-set training problems . additionally , our algorithm may induce some particular applications , such as for the transductive inference .", "topics": ["feature vector", "support vector machine"]}
{"title": "learning to act greedily : polymatroid semi-bandits", "abstract": "many important optimization problems , such as the minimum spanning tree and minimum-cost flow , can be solved optimally by a greedy method . in this work , we study a learning variant of these problems , where the model of the problem is unknown and has to be learned by interacting repeatedly with the environment in the bandit setting . we formalize our learning problem quite generally , as learning how to maximize an unknown modular function on a known polymatroid . we propose a computationally efficient algorithm for solving our problem and bound its expected cumulative regret . our gap-dependent upper bound is tight up to a constant and our gap-free upper bound is tight up to polylogarithmic factors . finally , we evaluate our method on three problems and demonstrate that it is practical .", "topics": ["regret ( decision theory )", "mathematical optimization"]}
{"title": "chained gaussian processes", "abstract": "gaussian process models are flexible , bayesian non-parametric approaches to regression . properties of multivariate gaussians mean that they can be combined linearly in the manner of additive models and via a link function ( like in generalized linear models ) to handle non-gaussian data . however , the link function formalism is restrictive , link functions are always invertible and must convert a parameter of interest to a linear combination of the underlying processes . there are many likelihoods and models where a non-linear combination is more appropriate . we term these more general models chained gaussian processes : the transformation of the gps to the likelihood parameters will not generally be invertible , and that implies that linearisation would only be possible with multiple ( localized ) links , i.e . a chain . we develop an approximate inference procedure for chained gps that is scalable and applicable to any factorized likelihood . we demonstrate the approximation on a range of likelihood functions .", "topics": ["approximation algorithm", "nonlinear system"]}
{"title": "a comprehensive approach to universal piecewise nonlinear regression based on trees", "abstract": "in this paper , we investigate adaptive nonlinear regression and introduce tree based piecewise linear regression algorithms that are highly efficient and provide significantly improved performance with guaranteed upper bounds in an individual sequence manner . we use a tree notion in order to partition the space of regressors in a nested structure . the introduced algorithms adapt not only their regression functions but also the complete tree structure while achieving the performance of the `` best '' linear mixture of a doubly exponential number of partitions , with a computational complexity only polynomial in the number of nodes of the tree . while constructing these algorithms , we also avoid using any artificial `` weighting '' of models ( with highly data dependent parameters ) and , instead , directly minimize the final regression error , which is the ultimate performance goal . the introduced methods are generic such that they can readily incorporate different tree construction methods such as random trees in their framework and can use different regressor or partitioning functions as demonstrated in the paper .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "a simple reinforcement learning mechanism for resource allocation in lte-a networks with markov decision process and q-learning", "abstract": "resource allocation is still a difficult issue to deal with in wireless networks . the unstable channel condition and traffic demand for quality of service ( qos ) raise some barriers that interfere with the process . it is significant that an optimal policy takes into account some resources available to each traffic class while considering the spectral efficiency and other related channel issues . reinforcement learning is a dynamic and effective method to support the accomplishment of resource allocation properly maintaining qos levels for applications . the technique can track the system state as feedback to enhance the performance of a given task . herein , it is proposed a simple reinforcement learning mechanism introduced in lte-a networks and aimed to choose and limit the number of resources allocated for each traffic class , regarding the qos class identifier ( qci ) , at each transmission time interval ( tti ) along the scheduling procedure . the proposed mechanism implements a markov decision process ( mdp ) solved by the q-learning algorithm to find an optimal action-state decision policy . the results obtained from simulation exhibit good performance , especially for the real-time video application .", "topics": ["reinforcement learning", "simulation"]}
{"title": "automatic neuron detection in calcium imaging data using convolutional networks", "abstract": "calcium imaging is an important technique for monitoring the activity of thousands of neurons simultaneously . as calcium imaging datasets grow in size , automated detection of individual neurons is becoming important . here we apply a supervised learning approach to this problem and show that convolutional networks can achieve near-human accuracy and superhuman speed . accuracy is superior to the popular pca/ica method based on precision and recall relative to ground truth annotation by a human expert . these results suggest that convolutional networks are an efficient and flexible tool for the analysis of large-scale calcium imaging data .", "topics": ["supervised learning", "ground truth"]}
{"title": "active learning for crowd-sourced databases", "abstract": "crowd-sourcing has become a popular means of acquiring labeled data for a wide variety of tasks where humans are more accurate than computers , e.g . , labeling images , matching objects , or analyzing sentiment . however , relying solely on the crowd is often impractical even for data sets with thousands of items , due to time and cost constraints of acquiring human input ( which cost pennies and minutes per label ) . in this paper , we propose algorithms for integrating machine learning into crowd-sourced databases , with the goal of allowing crowd-sourcing applications to scale , i.e . , to handle larger datasets at lower costs . the key observation is that , in many of the above tasks , humans and machine learning algorithms can be complementary , as humans are often more accurate but slow and expensive , while algorithms are usually less accurate , but faster and cheaper . based on this observation , we present two new active learning algorithms to combine humans and algorithms together in a crowd-sourced database . our algorithms are based on the theory of non-parametric bootstrap , which makes our results applicable to a broad class of machine learning models . our results , on three real-life datasets collected with amazon 's mechanical turk , and on 15 well-known uci data sets , show that our methods on average ask humans to label one to two orders of magnitude fewer items to achieve the same accuracy as a baseline that labels random images , and two to eight times fewer questions than previous active learning schemes .", "topics": ["baseline ( configuration management )", "scalability"]}
{"title": "ups and downs : modeling the visual evolution of fashion trends with one-class collaborative filtering", "abstract": "building a successful recommender system depends on understanding both the dimensions of people 's preferences as well as their dynamics . in certain domains , such as fashion , modeling such preferences can be incredibly difficult , due to the need to simultaneously model the visual appearance of products as well as their evolution over time . the subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets . in this paper we build novel models for the one-class collaborative filtering setting , where our goal is to estimate users ' fashion-aware personalized ranking functions based on their past feedback . to uncover the complex and evolving visual factors that people consider when evaluating products , our method combines high-level visual features extracted from a deep convolutional neural network , users ' past feedback , as well as evolving trends within the community . experimentally we evaluate our method on two large real-world datasets from amazon.com , where we show it to outperform state-of-the-art personalized ranking measures , and also use it to visualize the high-level fashion trends across the 11-year span of our dataset .", "topics": ["high- and low-level", "nonlinear system"]}
{"title": "auc optimisation and collaborative filtering", "abstract": "in recommendation systems , one is interested in the ranking of the predicted items as opposed to other losses such as the mean squared error . although a variety of ways to evaluate rankings exist in the literature , here we focus on the area under the roc curve ( auc ) as it widely used and has a strong theoretical underpinning . in practical recommendation , only items at the top of the ranked list are presented to the users . with this in mind , we propose a class of objective functions over matrix factorisations which primarily represent a smooth surrogate for the real auc , and in a special case we show how to prioritise the top of the list . the objectives are differentiable and optimised through a carefully designed stochastic gradient-descent-based algorithm which scales linearly with the size of the data . in the special case of square loss we show how to improve computational complexity by leveraging previously computed measures . to understand theoretically the underlying matrix factorisation approaches we study both the consistency of the loss functions with respect to auc , and generalisation using rademacher theory . the resulting generalisation analysis gives strong motivation for the optimisation under study . finally , we provide computation results as to the efficacy of the proposed method using synthetic and real data .", "topics": ["computational complexity theory", "mathematical optimization"]}
{"title": "bayesian deep convolutional encoder-decoder networks for surrogate modeling and uncertainty quantification", "abstract": "we are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic pdes using a deep convolutional encoder-decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks . since normal neural networks are data intensive and can not provide predictive uncertainty , we propose a bayesian approach to convolutional neural nets . a recently introduced variational gradient descent algorithm based on stein 's method is scaled to deep convolutional networks to perform approximate bayesian inference on millions of uncertain network parameters . this approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in bayesian neural networks as well as techniques that include gaussian processes and ensemble methods even when the training data size is relatively small . to evaluate the performance of this approach , we consider standard uncertainty quantification benchmark problems including flow in heterogeneous media defined in terms of limited data-driven permeability realizations . the performance of the surrogate model developed is very good even though there is no underlying structure shared between the input ( permeability ) and output ( flow/pressure ) fields as is often the case in the image-to-image regression models used in computer vision problems . studies are performed with an underlying stochastic input dimensionality up to $ 4,225 $ where most other uncertainty quantification methods fail . uncertainty propagation tasks are considered and the predictive output bayesian statistics are compared to those obtained with monte carlo estimates .", "topics": ["test set", "calculus of variations"]}
{"title": "wordrank : learning word embeddings via robust ranking", "abstract": "embedding words in a vector space has gained a lot of attention in recent years . while state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding , their motivation is often left unclear . in this paper , we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics . then , based on this insight , we propose a novel framework wordrank that efficiently estimates word representations via robust ranking , in which the attention mechanism and robustness to noise are readily achieved via the dcg-like ranking losses . the performance of wordrank is measured in word similarity and word analogy benchmarks , and the results are compared to the state-of-the-art word embedding techniques . our algorithm is very competitive to the state-of-the- arts on large corpora , while outperforms them by a significant margin when the training set is limited ( i.e . , sparse and noisy ) . with 17 million tokens , wordrank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark . our multi-node distributed implementation of wordrank is publicly available for general usage .", "topics": ["sparse matrix", "text corpus"]}
{"title": "earth system modeling 2.0 : a blueprint for models that learn from observations and targeted high-resolution simulations", "abstract": "climate projections continue to be marred by large uncertainties , which originate in processes that need to be parameterized , such as clouds , convection , and ecosystems . but rapid progress is now within reach . new computational tools and methods from data assimilation and machine learning make it possible to integrate global observations and local high-resolution simulations in an earth system model ( esm ) that systematically learns from both . here we propose a blueprint for such an esm . we outline how parameterization schemes can learn from global observations and targeted high-resolution simulations , for example , of clouds and convection , through matching low-order statistics between esms , observations , and high-resolution simulations . we illustrate learning algorithms for esms with a simple dynamical system that shares characteristics of the climate system ; and we discuss the opportunities the proposed framework presents and the challenges that remain to realize it .", "topics": ["simulation"]}
{"title": "recent advances in neural program synthesis", "abstract": "in recent years , deep learning has made tremendous progress in a number of fields that were previously out of reach for artificial intelligence . the successes in these problems has led researchers to consider the possibilities for intelligent systems to tackle a problem that humans have only recently themselves considered : program synthesis . this challenge is unlike others such as object recognition and speech translation , since its abstract nature and demand for rigor make it difficult even for human minds to attempt . while it is still far from being solved or even competitive with most existing methods , neural program synthesis is a rapidly growing discipline which holds great promise if completely realized . in this paper , we start with exploring the problem statement and challenges of program synthesis . then , we examine the fascinating evolution of program induction models , along with how they have succeeded , failed and been reimagined since . finally , we conclude with a contrastive look at program synthesis and future research recommendations for the field .", "topics": ["artificial intelligence"]}
{"title": "tiny ssd : a tiny single-shot detection deep convolutional neural network for real-time embedded object detection", "abstract": "object detection is a major challenge in computer vision , involving both object classification and object localization within a scene . while deep neural networks have been shown in recent years to yield very powerful techniques for tackling the challenge of object detection , one of the biggest challenges with enabling such object detection networks for widespread deployment on embedded devices is high computational and memory requirements . recently , there has been an increasing focus in exploring small deep neural network architectures for object detection that are more suitable for embedded devices , such as tiny yolo and squeezedet . inspired by the efficiency of the fire microarchitecture introduced in squeezenet and the object detection performance of the single-shot detection macroarchitecture introduced in ssd , this paper introduces tiny ssd , a single-shot detection deep convolutional neural network for real-time embedded object detection that is composed of a highly optimized , non-uniform fire sub-network stack and a non-uniform sub-network stack of highly optimized ssd-based auxiliary convolutional feature layers designed specifically to minimize model size while maintaining object detection performance . the resulting tiny ssd possess a model size of 2.3mb ( ~26x smaller than tiny yolo ) while still achieving an map of 61.3 % on voc 2007 ( ~4.2 % higher than tiny yolo ) . these experimental results show that very small deep neural network architectures can be designed for real-time object detection that are well-suited for embedded scenarios .", "topics": ["object detection", "computer vision"]}
{"title": "using parameterized black-box priors to scale up model-based policy search for robotics", "abstract": "the most data-efficient algorithms for reinforcement learning in robotics are model-based policy search algorithms , which alternate between learning a dynamical model of the robot and optimizing a policy to maximize the expected return given the model and its uncertainties . among the few proposed approaches , the recently introduced black-drops algorithm exploits a black-box optimization algorithm to achieve both high data-efficiency and good computation times when several cores are used ; nevertheless , like all model-based policy search approaches , black-drops does not scale to high dimensional state/action spaces . in this paper , we introduce a new model learning procedure in black-drops that leverages parameterized black-box priors to ( 1 ) scale up to high-dimensional systems , and ( 2 ) be robust to large inaccuracies of the prior information . we demonstrate the effectiveness of our approach with the `` pendubot '' swing-up task in simulation and with a physical hexapod robot ( 48d state space , 18d action space ) that has to walk forward as fast as possible . the results show that our new algorithm is more data-efficient than previous model-based policy search algorithms ( with and without priors ) and that it can allow a physical 6-legged robot to learn new gaits in only 16 to 30 seconds of interaction time .", "topics": ["reinforcement learning", "simulation"]}
{"title": "fast k-means based on knn graph", "abstract": "in the era of big data , k-means clustering has been widely adopted as a basic processing tool in various contexts . however , its computational cost could be prohibitively high as the data size and the cluster number are large . it is well known that the processing bottleneck of k-means lies in the operation of seeking closest centroid in each iteration . in this paper , a novel solution towards the scalability issue of k-means is presented . in the proposal , k-means is supported by an approximate k-nearest neighbors graph . in the k-means iteration , each data sample is only compared to clusters that its nearest neighbors reside . since the number of nearest neighbors we consider is much less than k , the processing cost in this step becomes minor and irrelevant to k. the processing bottleneck is therefore overcome . the most interesting thing is that k-nearest neighbor graph is constructed by iteratively calling the fast $ k $ -means itself . comparing with existing fast k-means variants , the proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality . as it is tested on 10 million 512-dimensional data , it takes only 5.2 hours to produce 1 million clusters . in contrast , to fulfill the same scale of clustering , it would take 3 years for traditional k-means .", "topics": ["cluster analysis", "iteration"]}
{"title": "efficient gpu implementation for single block orthogonal dictionary learning", "abstract": "dictionary training for sparse representations involves dealing with large chunks of data and complex algorithms that determine time consuming implementations . sbo is an iterative dictionary learning algorithm based on constructing unions of orthonormal bases via singular value decomposition , that represents each data item through a single best fit orthobase . in this paper we present a gpgpu approach of implementing sbo in opencl . we provide a lock-free solution that ensures full-occupancy of the gpu by following the map-reduce model for the sparse-coding stage and by making use of the partitioned global address space ( pgas ) model for developing parallel dictionary updates . the resulting implementation achieves a favourable trade-off between algorithm complexity and data representation quality compared to pak-svd which is the standard overcomplete dictionary learning approach . we present and discuss numerical results showing a significant acceleration of the execution time for the dictionary learning process .", "topics": ["numerical analysis", "sparse matrix"]}
{"title": "scalable variational inference in log-supermodular models", "abstract": "we consider the problem of approximate bayesian inference in log-supermodular models . these models encompass regular pairwise mrfs with binary variables , but allow to capture high-order interactions , which are intractable for existing approximate inference techniques such as belief propagation , mean field , and variants . we show that a recently proposed variational approach to inference in log-supermodular models -l-field- reduces to the widely-studied minimum norm problem for submodular minimization . this insight allows to leverage powerful existing tools , and hence to solve the variational problem orders of magnitude more efficiently than previously possible . we then provide another natural interpretation of l-field , demonstrating that it exactly minimizes a specific type of r\\'enyi divergence measure . this insight sheds light on the nature of the variational approximations produced by l-field . furthermore , we show how to perform parallel inference as message passing in a suitable factor graph at a linear convergence rate , without having to sum up over all the configurations of the factor . finally , we apply our approach to a challenging image segmentation task . our experiments confirm scalability of our approach , high quality of the marginals , and the benefit of incorporating higher-order potentials .", "topics": ["approximation algorithm", "calculus of variations"]}
{"title": "document embedding with paragraph vectors", "abstract": "paragraph vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts . in their work , the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis . that proof of concept , while encouraging , was rather narrow . here we consider tasks other than sentiment analysis , provide a more thorough comparison of paragraph vectors to other document modelling algorithms such as latent dirichlet allocation , and evaluate performance of the method as we vary the dimensionality of the learned representation . we benchmarked the models on two document similarity data sets , one from wikipedia , one from arxiv . we observe that the paragraph vector method performs significantly better than other methods , and propose a simple improvement to enhance embedding quality . somewhat surprisingly , we also show that much like word embeddings , vector operations on paragraph vectors can perform useful semantic results .", "topics": ["unsupervised learning"]}
{"title": "boosting-like deep learning for pedestrian detection", "abstract": "this paper proposes boosting-like deep learning ( bdl ) framework for pedestrian detection . due to overtraining on the limited training samples , overfitting is a major problem of deep learning . we incorporate a boosting-like technique into deep learning to weigh the training samples , and thus prevent overtraining in the iterative process . we theoretically give the details of derivation of our algorithm , and report the experimental results on open data sets showing that bdl achieves a better stable performance than the state-of-the-arts . our approach achieves 15.85 % and 3.81 % reduction in the average miss rate compared with acf and jointdeep on the largest caltech benchmark dataset , respectively .", "topics": ["iteration"]}
{"title": "playing doom with slam-augmented deep reinforcement learning", "abstract": "a number of recent approaches to policy learning in 2d game domains have been successful going directly from raw input images to actions . however when employed in complex 3d environments , they typically suffer from challenges related to partial observability , combinatorial exploration spaces , path planning , and a scarcity of rewarding scenarios . inspired from prior work in human cognition that indicates how humans employ a variety of semantic concepts and abstractions ( object categories , localisation , etc . ) to reason about the world , we build an agent-model that incorporates such abstractions into its policy-learning framework . we augment the raw image input to a deep q-learning network ( dqn ) , by adding details of objects and structural elements encountered , along with the agent 's localisation . the different components are automatically extracted and composed into a topological representation using on-the-fly object detection and 3d-scene reconstruction.we evaluate the efficacy of our approach in doom , a 3d first-person combat game that exhibits a number of challenges discussed , and show that our augmented framework consistently learns better , more effective policies .", "topics": ["object detection", "reinforcement learning"]}
{"title": "approximation of the truncated zeta distribution and zipf 's law", "abstract": "zipf 's law appears in many application areas but does not have a closed form expression , which may make its use cumbersome . since it coincides with the truncated version of the zeta distribution , in this paper we propose three approximate closed form expressions for the truncated zeta distribution , which may be employed for zipf 's law as well . the three approximations are based on the replacement of the sum occurring in zipf 's law with an integral , and are named respectively the integral approximation , the average integral approximation , and the trapezoidal approximation . while the first one is shown to be of little use , the trapezoidal approximation exhibits an error which is typically lower than 1\\ % , but is as low as 0.1\\ % for the range of values of the zipf parameter below 1 .", "topics": ["approximation algorithm", "approximation"]}
{"title": "comparative study on generative adversarial networks", "abstract": "in recent years , there have been tremendous advancements in the field of machine learning . these advancements have been made through both academic as well as industrial research . lately , a fair amount of research has been dedicated to the usage of generative models in the field of computer vision and image classification . these generative models have been popularized through a new framework called generative adversarial networks . moreover , many modified versions of this framework have been proposed in the last two years . we study the original model proposed by goodfellow et al . as well as modifications over the original model and provide a comparative analysis of these models .", "topics": ["generative model", "computer vision"]}
{"title": "question answering via integer programming over semi-structured knowledge", "abstract": "answering science questions posed in natural language is an important ai challenge . answering such questions often requires non-trivial inference and knowledge that goes beyond factoid retrieval . yet , most systems for this task are based on relatively shallow information retrieval ( ir ) and statistical correlation techniques operating on large unstructured corpora . we propose a structured inference system for this task , formulated as an integer linear program ( ilp ) , that answers natural language questions using a semi-structured knowledge base derived from text , including questions requiring multi-step inference and a combination of multiple facts . on a dataset of real , unseen science questions , our system significantly outperforms ( +14 % ) the best previous attempt at structured reasoning for this task , which used markov logic networks ( mlns ) . it also improves upon a previous ilp formulation by 17.7 % . when combined with unstructured inference methods , the ilp system significantly boosts overall performance ( +10 % ) . finally , we show our approach is substantially more robust to a simple answer perturbation compared to statistical correlation methods .", "topics": ["natural language", "text corpus"]}
{"title": "learning representations using complex-valued nets", "abstract": "complex-valued neural networks ( cvnns ) are an emerging field of research in neural networks due to their potential representational properties for audio , image , and physiological signals . it is common in signal processing to transform sequences of real values to the complex domain via a set of complex basis functions , such as the fourier transform . we show how cvnns can be used to learn complex representations of real valued time-series data . we present methods and results using a framework that can compose holomorphic and non-holomorphic functions in a multi-layer network using a theoretical result called the wirtinger derivative . we test our methods on a representation learning task for real-valued signals , recurrent complex-valued networks and their real-valued counterparts . our results show that recurrent complex-valued networks can perform as well as their real-valued counterparts while learning filters that are representative of the domain of the data .", "topics": ["feature learning", "recurrent neural network"]}
{"title": "nonparametric bayesian estimation of periodic functions", "abstract": "many real world problems exhibit patterns that have periodic behavior . for example , in astrophysics , periodic variable stars play a pivotal role in understanding our universe . an important step when analyzing data from such processes is the problem of identifying the period : estimating the period of a periodic function based on noisy observations made at irregularly spaced time points . this problem is still a difficult challenge despite extensive study in different disciplines . the paper makes several contributions toward solving this problem . first , we present a nonparametric bayesian model for period finding , based on gaussian processes ( gp ) , that does not make strong assumptions on the shape of the periodic function . as our experiments demonstrate , the new model leads to significantly better results in period estimation when the target function is non-sinusoidal . second , we develop a new algorithm for parameter optimization for gp which is useful when the likelihood function is very sensitive to the setting of the hyper-parameters with numerous local minima , as in the case of period estimation . the algorithm combines gradient optimization with grid search and incorporates several mechanisms to overcome the high complexity of inference with gp . third , we develop a novel approach for using domain knowledge , in the form of a probabilistic generative model , and incorporate it into the period estimation algorithm . experimental results on astrophysics data validate our approach showing significant improvement over the state of the art in this domain .", "topics": ["generative model", "computational complexity theory"]}
{"title": "sim-ce : an advanced simulink platform for studying the brain of caenorhabditis elegans", "abstract": "we introduce sim-ce , an advanced , user-friendly modeling and simulation environment in simulink for performing multi-scale behavioral analysis of the nervous system of caenorhabditis elegans ( c. elegans ) . sim-ce contains an implementation of the mathematical models of c. elegans 's neurons and synapses , in simulink , which can be easily extended and particularized by the user . the simulink model is able to capture both complex dynamics of ion channels and additional biophysical detail such as intracellular calcium concentration . we demonstrate the performance of sim-ce by carrying out neuronal , synaptic and neural-circuit-level behavioral simulations . such environment enables the user to capture unknown properties of the neural circuits , test hypotheses and determine the origin of many behavioral plasticities exhibited by the worm .", "topics": ["simulation"]}
{"title": "model-consistent sparse estimation through the bootstrap", "abstract": "we consider the least-square linear regression problem with regularization by the $ \\ell^1 $ -norm , a problem usually referred to as the lasso . in this paper , we first present a detailed asymptotic analysis of model consistency of the lasso in low-dimensional settings . for various decays of the regularization parameter , we compute asymptotic equivalents of the probability of correct model selection . for a specific rate decay , we show that the lasso selects all the variables that should enter the model with probability tending to one exponentially fast , while it selects all other variables with strictly positive probability . we show that this property implies that if we run the lasso for several bootstrapped replications of a given sample , then intersecting the supports of the lasso bootstrap estimates leads to consistent model selection . this novel variable selection procedure , referred to as the bolasso , is extended to high-dimensional settings by a provably consistent two-step procedure .", "topics": ["matrix regularization", "sparse matrix"]}
{"title": "compressive spectral embedding : sidestepping the svd", "abstract": "spectral embedding based on the singular value decomposition ( svd ) is a widely used `` preprocessing '' step in many learning tasks , typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes ( by a predefined function of the singular value ) . however , the number of such vectors required to capture problem structure grows with problem size , and even partial svd computation becomes a bottleneck . in this paper , we propose a low-complexity it compressive spectral embedding algorithm , which employs random projections and finite order polynomial expansions to compute approximations to svd-based embedding . for an m times n matrix with t non-zeros , its time complexity is o ( ( t+m+n ) log ( m+n ) ) , and the embedding dimension is o ( log ( m+n ) ) , both of which are independent of the number of singular vectors whose effect we wish to capture . to the best of our knowledge , this is the first work to circumvent this dependence on the number of singular vectors for general svd-based embeddings . the key to sidestepping the svd is the observation that , for downstream inference tasks such as clustering and classification , we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the euclidean norm , rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial svd tries to do . our numerical results on network datasets demonstrate the efficacy of the proposed method , and motivate further exploration of its application to large-scale inference tasks .", "topics": ["cluster analysis", "time complexity"]}
{"title": "on the behavior of convolutional nets for feature extraction", "abstract": "deep neural networks are representation learning techniques . during training , a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning . extracting the descriptive language coded within a trained cnn model ( in the case of image data ) , and reusing it for other purposes is a field of interest , as it provides access to the visual descriptors previously learnt by the cnn after processing millions of images , without requiring an expensive training phase . contributions to this field ( commonly known as feature representation transfer or transfer learning ) have been purely empirical so far , extracting all cnn features from a single layer close to the output and testing their performance by feeding them to a classifier . this approach has provided consistent results , although its relevance is limited to classification tasks . in a completely different approach , in this paper we statistically measure the discriminative power of every single feature found within a deep cnn , when used for characterizing every class of 11 datasets . we seek to provide new insights into the behavior of cnn features , particularly the ones from convolutional layers , as this can be relevant for their application to knowledge representation and reasoning . our results confirm that low and middle level features may behave differently to high level features , but only under certain conditions . we find that all cnn features can be used for knowledge representation purposes both by their presence or by their absence , doubling the information a single cnn feature may provide . we also study how much noise these features may include , and propose a thresholding approach to discard most of it . all these insights have a direct application to the generation of cnn embedding spaces .", "topics": ["feature learning", "statistical classification"]}
{"title": "on the origin of long-range correlations in texts", "abstract": "the complexity of human interactions with social and natural phenomena is mirrored in the way we describe our experiences through natural language . in order to retain and convey such a high dimensional information , the statistical properties of our linguistic output has to be highly correlated in time . an example are the robust observations , still largely not understood , of correlations on arbitrary long scales in literary texts . in this paper we explain how long-range correlations flow from highly structured linguistic levels down to the building blocks of a text ( words , letters , etc.. ) . by combining calculations and data analysis we show that correlations take form of a bursty sequence of events once we approach the semantically relevant topics of the text . the mechanisms we identify are fairly general and can be equally applied to other hierarchical settings .", "topics": ["natural language", "interaction"]}
{"title": "finding statistically significant attribute interactions", "abstract": "in many data exploration tasks it is meaningful to identify groups of attribute interactions that are specific to a variable of interest . for instance , in a dataset where the attributes are medical markers and the variable of interest ( class variable ) is binary indicating presence/absence of disease , we would like to know which medical markers interact with respect to the binary class label . these interactions are useful in several practical applications , for example , to gain insight into the structure of the data , in feature selection , and in data anonymisation . we present a novel method , based on statistical significance testing , that can be used to verify if the data set has been created by a given factorised class-conditional joint distribution , where the distribution is parametrised by a partition of its attributes . furthermore , we provide a method , named astrid , for automatically finding a partition of attributes describing the distribution that has generated the data . state-of-the-art classifiers are utilised to capture the interactions present in the data by systematically breaking attribute interactions and observing the effect of this breaking on classifier performance . we empirically demonstrate the utility of the proposed method with examples using real and synthetic data .", "topics": ["synthetic data", "interaction"]}
{"title": "solving principal component pursuit in linear time via $ l_1 $ filtering", "abstract": "in the past decades , exactly recovering the intrinsic data structure from corrupted observations , which is known as robust principal component analysis ( rpca ) , has attracted tremendous interests and found many applications in computer vision . recently , this problem has been formulated as recovering a low-rank component and a sparse component from the observed data matrix . it is proved that under some suitable conditions , this problem can be exactly solved by principal component pursuit ( pcp ) , i.e . , minimizing a combination of nuclear norm and $ l_1 $ norm . most of the existing methods for solving pcp require singular value decompositions ( svd ) of the data matrix , resulting in a high computational complexity , hence preventing the applications of rpca to very large scale computer vision problems . in this paper , we propose a novel algorithm , called $ l_1 $ filtering , for \\emph { exactly } solving pcp with an $ o ( r^2 ( m+n ) ) $ complexity , where $ m\\times n $ is the size of data matrix and $ r $ is the rank of the matrix to recover , which is supposed to be much smaller than $ m $ and $ n $ . moreover , $ l_1 $ filtering is \\emph { highly parallelizable } . it is the first algorithm that can \\emph { exactly } solve a nuclear norm minimization problem in \\emph { linear time } ( with respect to the data size ) . experiments on both synthetic data and real applications testify to the great advantage of $ l_1 $ filtering in speed over state-of-the-art algorithms .", "topics": ["computational complexity theory", "time complexity"]}
{"title": "generate image descriptions based on deep rnn and memory cells for images features", "abstract": "generating natural language descriptions for images is a challenging task . the traditional way is to use the convolutional neural network ( cnn ) to extract image features , followed by recurrent neural network ( rnn ) to generate sentences . in this paper , we present a new model that added memory cells to gate the feeding of image features to the deep neural network . the intuition is enabling our model to memorize how much information from images should be fed at each stage of the rnn . experiments on flickr8k and flickr30k datasets showed that our model outperforms other state-of-the-art models with higher bleu scores .", "topics": ["recurrent neural network", "natural language"]}
{"title": "satellite image classification methods and landsat 5tm bands", "abstract": "this paper attempts to find the most accurate classification method among parallelepiped , minimum distance and chain methods . moreover , this study also challenges to find the suitable combination of bands , which can lead to better results in case combinations of bands occur . after comparing these three methods , the chain method over perform the other methods with 79 % overall accuracy . hence , it is more accurate than minimum distance with 67 % and parallelepiped with 65 % . on the other hand , based on bands features , and also by combining several researchers ' findings , a table was created which includes the main objects on the land and the suitable combination of the bands for accurately detecting of landcover objects . during this process , it was observed that band 4 ( out of 7 bands of landsat 5tm ) is the band , which can be used for increasing the accuracy of the combined bands in detecting objects on the land .", "topics": ["computer vision"]}
{"title": "efficiently bounding optimal solutions after small data modification in large-scale empirical risk minimization", "abstract": "we study large-scale classification problems in changing environments where a small part of the dataset is modified , and the effect of the data modification must be quickly incorporated into the classifier . when the entire dataset is large , even if the amount of the data modification is fairly small , the computational cost of re-training the classifier would be prohibitively large . in this paper , we propose a novel method for efficiently incorporating such a data modification effect into the classifier without actually re-training it . the proposed method provides bounds on the unknown optimal classifier with the cost only proportional to the size of the data modification . we demonstrate through numerical experiments that the proposed method provides sufficiently tight bounds with negligible computational costs , especially when a small part of the dataset is modified in a large-scale classification problem .", "topics": ["numerical analysis"]}
{"title": "classification with noisy labels by importance reweighting", "abstract": "in this paper , we study a classification problem in which sample labels are randomly corrupted . in this scenario , there is an unobservable sample with noise-free labels . however , before being observed , the true labels are independently flipped with a probability $ \\rho\\in [ 0,0.5 ) $ , and the random label noise can be class-conditional . here , we address two fundamental problems raised by this scenario . the first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise . we prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting , with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample . the other is the open problem of how to obtain the noise rate $ \\rho $ . we show that the rate is upper bounded by the conditional probability $ p ( y|x ) $ of the noisy sample . consequently , the rate can be estimated , because the upper bound can be easily reached in classification problems . experimental results on synthetic and real datasets confirm the efficiency of our methods .", "topics": ["statistical classification", "synthetic data"]}
{"title": "dimensionality reduction with subspace structure preservation", "abstract": "modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications . however , dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied . our key contribution is to show that $ 2k $ projection vectors are sufficient for the independence preservation of any $ k $ class data sampled from a union of independent subspaces . it is this non-trivial observation that we use for designing our dimensionality reduction technique . in this paper , we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset . we support our theoretical analysis with empirical results on both synthetic and real world data achieving \\textit { state-of-the-art } results compared to popular dimensionality reduction techniques .", "topics": ["synthetic data"]}
{"title": "fast graph construction using auction algorithm", "abstract": "in practical machine learning systems , graph based data representation has been widely used in various learning paradigms , ranging from unsupervised clustering to supervised classification . besides those applications with natural graph or network structure data , such as social network analysis and relational learning , many other applications often involve a critical step in converting data vectors to an adjacency graph . in particular , a sparse subgraph extracted from the original graph is often required due to both theoretic and practical needs . previous study clearly shows that the performance of different learning algorithms , e.g . , clustering and classification , benefits from such sparse subgraphs with balanced node connectivity . however , the existing graph construction methods are either computationally expensive or with unsatisfactory performance . in this paper , we utilize a scalable method called auction algorithm and its parallel extension to recover a sparse yet nearly balanced subgraph with significantly reduced computational cost . empirical study and comparison with the state-ofart approaches clearly demonstrate the superiority of the proposed method in both efficiency and accuracy .", "topics": ["cluster analysis", "supervised learning"]}
{"title": "variance reduction methods for sublinear reinforcement learning", "abstract": "this work considers the problem of provably optimal reinforcement learning for ( episodic ) finite horizon mdps , i.e . how an agent learns to maximize his/her ( long term ) reward in an uncertain environment . the main contribution is in providing a novel algorithm -- - variance-reduced upper confidence q-learning ( vucq ) -- - which enjoys a regret bound of $ \\widetilde { o } ( \\sqrt { hsat } + h^5sa ) $ , where the $ t $ is the number of time steps the agent acts in the mdp , $ s $ is the number of states , $ a $ is the number of actions , and $ h $ is the ( episodic ) horizon time . this is the first regret bound that is both sub-linear in the model size and asymptotically optimal . the algorithm is sub-linear in that the time to achieve $ \\epsilon $ -average regret ( for any constant $ \\epsilon $ ) is $ o ( sa ) $ , which is a number of samples that is far less than that required to learn any ( non-trivial ) estimate of the transition model ( the transition model is specified by $ o ( s^2a ) $ parameters ) . the importance of sub-linear algorithms is largely the motivation for algorithms such as $ q $ -learning and other `` model free '' approaches . vucq algorithm also enjoys minimax optimal regret in the long run , matching the $ \\omega ( \\sqrt { hsat } ) $ lower bound . variance-reduced upper confidence q-learning ( vucq ) is a successive refinement method in which the algorithm reduces the variance in $ q $ -value estimates and couples this estimation scheme with an upper confidence based algorithm . technically , the coupling of both of these techniques is what leads to the algorithm enjoying both the sub-linear regret property and the ( asymptotically ) optimal regret .", "topics": ["regret ( decision theory )", "reinforcement learning"]}
{"title": "circulant binary embedding", "abstract": "binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space . traditional binary coding methods often suffer from very high computation and storage costs in such a scenario . to address this problem , we propose circulant binary embedding ( cbe ) which generates binary codes by projecting the data with a circulant matrix . the circulant structure enables the use of fast fourier transformation to speed up the computation . compared to methods that use unstructured matrices , the proposed method improves the time complexity from $ \\mathcal { o } ( d^2 ) $ to $ \\mathcal { o } ( d\\log { d } ) $ , and the space complexity from $ \\mathcal { o } ( d^2 ) $ to $ \\mathcal { o } ( d ) $ where $ d $ is the input dimensionality . we also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections , which alternatively minimizes the objective in original and fourier domains . we show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time , and provides much faster computation with no performance degradation for fixed number of bits .", "topics": ["time complexity", "computation"]}
{"title": "deep fried convnets", "abstract": "the fully connected layers of a deep convolutional neural network typically contain over 90 % of the network parameters , and consume the majority of the memory required to store the network parameters . reducing the number of parameters while preserving essentially the same predictive performance is critically important for operating deep neural networks in memory constrained environments such as gpus or embedded devices . in this paper we show how kernel methods , in particular a single fastfood layer , can be used to replace all fully connected layers in a deep convolutional neural network . this novel fastfood layer is also end-to-end trainable in conjunction with convolutional layers , allowing us to combine them into a new architecture , named deep fried convolutional networks , which substantially reduces the memory footprint of convolutional networks trained on mnist and imagenet with no drop in predictive performance .", "topics": ["end-to-end principle", "mnist database"]}
{"title": "on the diffusion approximation of nonconvex stochastic gradient descent", "abstract": "we study the stochastic gradient descent ( sgd ) method in nonconvex optimization problems from the point of view of approximating diffusion processes . we prove rigorously that the diffusion process can approximate the sgd algorithm weakly using the weak form of master equation for probability evolution . in the small step size regime and the presence of omnidirectional noise , our weak approximating diffusion process suggests the following dynamics for the sgd iteration starting from a local minimizer ( resp.~saddle point ) : it escapes in a number of iterations exponentially ( resp.~almost linearly ) dependent on the inverse stepsize . the results are obtained using the theory for random perturbations of dynamical systems ( theory of large deviations for local minimizers and theory of exiting for unstable stationary points ) . in addition , we discuss the effects of batch size for the deep neural networks , and we find that small batch size is helpful for sgd algorithms to escape unstable stationary points and sharp minimizers . our theory indicates that one should increase the batch size at later stage for the sgd to be trapped in flat minimizers for better generalization .", "topics": ["approximation algorithm", "gradient descent"]}
{"title": "unsupervised feature extraction by time-contrastive learning and nonlinear ica", "abstract": "nonlinear independent component analysis ( ica ) provides an appealing framework for unsupervised feature learning , but the models proposed so far are not identifiable . here , we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data . our learning principle , time-contrastive learning ( tcl ) , finds a representation which allows optimal discrimination of time segments ( windows ) . surprisingly , we show how tcl can be related to a nonlinear ica model , when ica is redefined to include temporal nonstationarities . in particular , we show that tcl combined with linear ica estimates the nonlinear ica model up to point-wise transformations of the sources , and this solution is unique -- - thus providing the first identifiability result for nonlinear ica which is rigorous , constructive , as well as very general .", "topics": ["feature learning", "time series"]}
{"title": "variational approaches for auto-encoding generative adversarial networks", "abstract": "auto-encoding generative adversarial networks ( gans ) combine the standard gan algorithm , which discriminates between real and model-generated data , with a reconstruction loss given by an auto-encoder . such models aim to prevent mode collapse in the learned generative model by ensuring that it is grounded in all the available training data . in this paper , we develop a principle upon which auto-encoders can be combined with generative adversarial networks by exploiting the hierarchical structure of the generative model . the underlying principle shows that variational inference can be used a basic tool for learning , but with the in- tractable likelihood replaced by a synthetic likelihood , and the unknown posterior distribution replaced by an implicit distribution ; both synthetic likelihoods and implicit posterior distributions can be learned using discriminators . this allows us to develop a natural fusion of variational auto-encoders and generative adversarial networks , combining the best of both these methods . we describe a unified objective for optimization , discuss the constraints needed to guide learning , connect to the wide range of existing work , and use a battery of tests to systematically and quantitatively assess the performance of our method .", "topics": ["generative model", "test set"]}
{"title": "design and development of an expert system to help head of university departments", "abstract": "one of the basic tasks which is responded for head of each university department , is employing lecturers based on some default factors such as experience , evidences , qualifies and etc . in this respect , to help the heads , some automatic systems have been proposed until now using machine learning methods , decision support systems ( dss ) and etc . according to advantages and disadvantages of the previous methods , a full automatic system is designed in this paper using expert systems . the proposed system is included two main steps . in the first one , the human expert 's knowledge is designed as decision trees . the second step is included an expert system which is evaluated using extracted rules of these decision trees . also , to improve the quality of the proposed system , a majority voting algorithm is proposed as post processing step to choose the best lecturer which satisfied more expert 's decision trees for each course . the results are shown that the designed system average accuracy is 78.88 . low computational complexity , simplicity to program and are some of other advantages of the proposed system .", "topics": ["computational complexity theory"]}
{"title": "logarithmic time one-against-some", "abstract": "we create a new online reduction of multiclass classification to binary classification for which training and prediction time scale logarithmically with the number of classes . compared to previous approaches , we obtain substantially better statistical performance for two reasons : first , we prove a tighter and more complete boosting theorem , and second we translate the results more directly into an algorithm . we show that several simple techniques give rise to an algorithm that can compete with one-against-all in both space and predictive power while offering exponential improvements in speed when the number of classes is large .", "topics": ["time complexity"]}
{"title": "data-driven rank breaking for efficient rank aggregation", "abstract": "rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference . rank-breaking is a common practice to reduce the computational complexity of learning the global ranking . the individual preferences are broken into pairwise comparisons and applied to efficient algorithms tailored for independent paired comparisons . however , due to the ignored dependencies in the data , naive rank-breaking approaches can result in inconsistent estimates . the key idea to produce accurate and consistent estimates is to treat the pairwise comparisons unequally , depending on the topology of the collected data . in this paper , we provide the optimal rank-breaking estimator , which not only achieves consistency but also achieves the best error bound . this allows us to characterize the fundamental tradeoff between accuracy and complexity . further , the analysis identifies how the accuracy depends on the spectral gap of a corresponding comparison graph .", "topics": ["computational complexity theory"]}
{"title": "matroid bandits : fast combinatorial optimization with learning", "abstract": "a matroid is a notion of independence in combinatorial optimization which is closely related to computational efficiency . in particular , it is well known that the maximum of a constrained modular function can be found greedily if and only if the constraints are associated with a matroid . in this paper , we bring together the ideas of bandits and matroids , and propose a new class of combinatorial bandits , matroid bandits . the objective in these problems is to learn how to maximize a modular function on a matroid . this function is stochastic and initially unknown . we propose a practical algorithm for solving our problem , optimistic matroid maximization ( omm ) ; and prove two upper bounds , gap-dependent and gap-free , on its regret . both bounds are sublinear in time and at most linear in all other quantities of interest . the gap-dependent upper bound is tight and we prove a matching lower bound on a partition matroid bandit . finally , we evaluate our method on three real-world problems and show that it is practical .", "topics": ["regret ( decision theory )"]}
{"title": "deepsofa : a real-time continuous acuity score framework using deep learning", "abstract": "traditional methods for assessing illness severity and predicting in-hospital mortality among critically ill patients require manual , time-consuming , and error-prone calculations that are further hindered by the use of static variable thresholds derived from aggregate patient populations . these coarse frameworks do not capture time-sensitive individual physiological patterns and are not suitable for instantaneous assessment of patients ' acuity trajectories , a critical task for the icu where conditions often change rapidly . furthermore , they are ill-suited to capitalize on the emerging availability of streaming electronic health record data . we propose a novel acuity score framework ( deepsofa ) that leverages temporal patient measurements in conjunction with deep learning models to make accurate assessments of a patient 's illness severity at any point during their icu stay . we compare deepsofa with sofa baseline models using the same predictors and find that at any point during an icu admission , deepsofa yields more accurate predictions of in-hospital mortality .", "topics": ["baseline ( configuration management )"]}
{"title": "marrnet : 3d shape reconstruction via 2.5d sketches", "abstract": "3d object reconstruction from a single image is a highly under-determined problem , requiring strong prior knowledge of plausible 3d shapes . this introduces challenges for learning-based approaches , as 3d object annotations are scarce in real images . previous work chose to train on synthetic data with ground truth 3d information , but suffered from domain adaptation when tested on real data . in this work , we propose marrnet , an end-to-end trainable model that sequentially estimates 2.5d sketches and 3d object shape . our disentangled , two-step formulation has three advantages . first , compared to full 3d shape , 2.5d sketches are much easier to be recovered from a 2d image ; models that recover 2.5d sketches are also more likely to transfer from synthetic to real data . second , for 3d reconstruction from 2.5d sketches , systems can learn purely from synthetic data . this is because we can easily render realistic 2.5d sketches without modeling object appearance variations in real images , including lighting , texture , etc . this further relieves the domain adaptation problem . third , we derive differentiable projective functions from 3d shape to 2.5d sketches ; the framework is therefore end-to-end trainable on real images , requiring no human annotations . our model achieves state-of-the-art performance on 3d shape reconstruction .", "topics": ["synthetic data", "end-to-end principle"]}
{"title": "managing sparsity , time , and quality of inference in topic models", "abstract": "inference is an integral part of probabilistic topic models , but is often non-trivial to derive an efficient algorithm for a specific model . it is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents . in this article , we introduce a simple framework for inference in probabilistic topic models , denoted by fw . this framework is general and flexible enough to be easily adapted to mixture models . it has a linear convergence rate , offers an easy way to incorporate prior knowledge , and provides us an easy way to directly trade off sparsity against quality and time . we demonstrate the goodness and flexibility of fw over existing inference methods by a number of tasks . finally , we show how inference in topic models with nonconjugate priors can be done efficiently .", "topics": ["sparse matrix"]}
{"title": "steady state resource allocation analysis of the stochastic diffusion search", "abstract": "this article presents the long-term behaviour analysis of stochastic diffusion search ( sds ) , a distributed agent-based system for best-fit pattern matching . sds operates by allocating simple agents into different regions of the search space . agents independently pose hypotheses about the presence of the pattern in the search space and its potential distortion . assuming a compositional structure of hypotheses about pattern matching agents perform an inference on the basis of partial evidence from the hypothesised solution . agents posing mutually consistent hypotheses about the pattern support each other and inhibit agents with inconsistent hypotheses . this results in the emergence of a stable agent population identifying the desired solution . positive feedback via diffusion of information between the agents significantly contributes to the speed with which the solution population is formed . the formulation of the sds model in terms of interacting markov chains enables its characterisation in terms of the allocation of agents , or computational resources . the analysis characterises the stationary probability distribution of the activity of agents , which leads to the characterisation of the solution population in terms of its similarity to the target pattern .", "topics": ["markov chain"]}
{"title": "highly scalable , parallel and distributed adaboost algorithm using light weight threads and web services on a network of multi-core machines", "abstract": "adaboost is an important algorithm in machine learning and is being widely used in object detection . adaboost works by iteratively selecting the best amongst weak classifiers , and then combines several weak classifiers to obtain a strong classifier . even though adaboost has proven to be very effective , its learning execution time can be quite large depending upon the application e.g . , in face detection , the learning time can be several days . due to its increasing use in computer vision applications , the learning time needs to be drastically reduced so that an adaptive near real time object detection system can be incorporated . in this paper , we develop a hybrid parallel and distributed adaboost algorithm that exploits the multiple cores in a cpu via light weight threads , and also uses multiple machines via a web service software architecture to achieve high scalability . we present a novel hierarchical web services based distributed architecture and achieve nearly linear speedup up to the number of processors available to us . in comparison with the previously published work , which used a single level master-slave parallel and distributed implementation [ 1 ] and only achieved a speedup of 2.66 on four nodes , we achieve a speedup of 95.1 on 31 workstations each having a quad-core processor , resulting in a learning time of only 4.8 seconds per feature .", "topics": ["object detection", "computer vision"]}
{"title": "gogma : globally-optimal gaussian mixture alignment", "abstract": "gaussian mixture alignment is a family of approaches that are frequently used for robustly solving the point-set registration problem . however , since they use local optimisation , they are susceptible to local minima and can only guarantee local optimality . consequently , their accuracy is strongly dependent on the quality of the initialisation . this paper presents the first globally-optimal solution to the 3d rigid gaussian mixture alignment problem under the l2 distance between mixtures . the algorithm , named gogma , employs a branch-and-bound approach to search the space of 3d rigid motions se ( 3 ) , guaranteeing global optimality regardless of the initialisation . the geometry of se ( 3 ) was used to find novel upper and lower bounds for the objective function and local optimisation was integrated into the scheme to accelerate convergence without voiding the optimality guarantee . the evaluation empirically supported the optimality proof and showed that the method performed much more robustly on two challenging datasets than an existing globally-optimal registration solution .", "topics": ["optimization problem", "mathematical optimization"]}
{"title": "image denoising and restoration with cnn-lstm encoder decoder with direct attention", "abstract": "image denoising is always a challenging task in the field of computer vision and image processing . in this paper , we have proposed an encoder-decoder model with direct attention , which is capable of denoising and reconstruct highly corrupted images . our model consists of an encoder and a decoder , where the encoder is a convolutional neural network and decoder is a multilayer long short-term memory network . in the proposed model , the encoder reads an image and catches the abstraction of that image in a vector , where decoder takes that vector as well as the corrupted image to reconstruct a clean image . we have trained our model on mnist handwritten digit database after making lower half of every image as black as well as adding noise top of that . after a massive destruction of the images where it is hard for a human to understand the content of those images , our model can retrieve that image with minimal error . our proposed model has been compared with convolutional encoder-decoder , where our model has performed better at generating missing part of the images than convolutional autoencoder .", "topics": ["image processing", "noise reduction"]}
{"title": "learning mixtures of multi-output regression models by correlation clustering for multi-view data", "abstract": "in many datasets , different parts of the data may have their own patterns of correlation , a structure that can be modeled as a mixture of local linear correlation models . the task of finding these mixtures is known as correlation clustering . in this work , we propose a linear correlation clustering method for datasets whose features are pre-divided into two views . the method , called canonical least squares ( cls ) clustering , is inspired by multi-output regression and canonical correlation analysis . cls clusters can be interpreted as variations in the regression relationship between the two views . the method is useful for data mining and data interpretation . its utility is demonstrated on a synthetic dataset and stock market dataset .", "topics": ["cluster analysis", "data mining"]}
{"title": "end-to-end phoneme sequence recognition using convolutional neural networks", "abstract": "most phoneme recognition state-of-the-art systems rely on a classical neural network classifiers , fed with highly tuned features , such as mfcc or plp features . recent advances in `` deep learning '' approaches questioned such systems , but while some attempts were made with simpler features such as spectrograms , state-of-the-art systems still rely on mfccs . this might be viewed as a kind of failure from deep learning approaches , which are often claimed to have the ability to train with raw signals , alleviating the need of hand-crafted features . in this paper , we investigate a convolutional neural network approach for raw speech signals . while convolutional architectures got tremendous success in computer vision or text processing , they seem to have been let down in the past recent years in the speech processing field . we show that it is possible to learn an end-to-end phoneme sequence classifier system directly from raw signal , with similar performance on the timit and wsj datasets than existing systems based on mfcc , questioning the need of complex hand-crafted features on large datasets .", "topics": ["computer vision", "end-to-end principle"]}
{"title": "bounded optimal exploration in mdp", "abstract": "within the framework of probably approximately correct markov decision processes ( pac-mdp ) , much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration . however , practical concerns require the attainment of satisfactory behavior within a short period of time . in this paper , we relax the pac-mdp conditions to reconcile theoretically driven exploration methods and practical needs . we propose simple algorithms for discrete and continuous state spaces , and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples . our algorithms also maintain anytime error bounds and average loss bounds . our approach accommodates both bayesian and non-bayesian methods .", "topics": ["numerical analysis"]}
{"title": "competitive multi-scale convolution", "abstract": "in this paper , we introduce a new deep convolutional neural network ( convnet ) module that promotes competition among a set of multi-scale convolutional filters . this new module is inspired by the inception module , where we replace the original collaborative pooling stage ( consisting of a concatenation of the multi-scale filter outputs ) by a competitive pooling represented by a maxout activation unit . this extension has the following two objectives : 1 ) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model , which has been shown to facilitate the training of complex learning problems ; and 2 ) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters . we show that the use of our proposed module in typical deep convnets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets : mnist , cifar-10 , cifar-100 and svhn .", "topics": ["convolution", "mnist database"]}
{"title": "towards an automated method based on iterated local search optimization for tuning the parameters of support vector machines", "abstract": "we provide preliminary details and formulation of an optimization strategy under current development that is able to automatically tune the parameters of a support vector machine over new datasets . the optimization strategy is a heuristic based on iterated local search , a modification of classic hill climbing which iterates calls to a local search routine .", "topics": ["support vector machine", "heuristic"]}
{"title": "learning cost-effective treatment regimes using markov decision processes", "abstract": "decision makers , such as doctors and judges , make crucial decisions such as recommending treatments to patients , and granting bails to defendants on a daily basis . such decisions typically involve weighting the potential benefits of taking an action against the costs involved . in this work , we aim to automate this task of learning \\emph { cost-effective , interpretable and actionable treatment regimes } . we formulate this as a problem of learning a decision list -- a sequence of if-then-else rules -- which maps characteristics of subjects ( eg . , diagnostic test results of patients ) to treatments . we propose a novel objective to construct a decision list which maximizes outcomes for the population , and minimizes overall costs . we model the problem of learning such a list as a markov decision process ( mdp ) and employ a variant of the upper confidence bound for trees ( uct ) strategy which leverages customized checks for pruning the search space effectively . experimental results on real world observational data capturing judicial bail decisions and treatment recommendations for asthma patients demonstrate the effectiveness of our approach .", "topics": ["map"]}
{"title": "fast abc-boost for multi-class classification", "abstract": "abc-boost is a new line of boosting algorithms for multi-class classification , by utilizing the commonly used sum-to-zero constraint . to implement abc-boost , a base class must be identified at each boosting step . prior studies used a very expensive procedure based on exhaustive search for determining the base class at each boosting step . good testing performances of abc-boost ( implemented as abc-mart and abc-logitboost ) on a variety of datasets were reported . for large datasets , however , the exhaustive search strategy adopted in prior abc-boost algorithms can be too prohibitive . to overcome this serious limitation , this paper suggests a heuristic by introducing gaps when computing the base class during training . that is , we update the choice of the base class only for every $ g $ boosting steps ( i.e . , g=1 in prior studies ) . we test this idea on large datasets ( covertype and poker ) as well as datasets of moderate sizes . our preliminary results are very encouraging . on the large datasets , even with g=100 ( or larger ) , there is essentially no loss of test accuracy . on the moderate datasets , no obvious loss of test accuracy is observed when g < = 20~50 . therefore , aided by this heuristic , it is promising that abc-boost will be a practical tool for accurate multi-class classification .", "topics": ["heuristic"]}
{"title": "architectural complexity measures of recurrent neural networks", "abstract": "in this paper , we systematically analyze the connecting architectures of recurrent neural networks ( rnns ) . our main contribution is twofold : first , we present a rigorous graph-theoretic framework describing the connecting architectures of rnns in general . second , we propose three architecture complexity measures of rnns : ( a ) the recurrent depth , which captures the rnn 's over-time nonlinear complexity , ( b ) the feedforward depth , which captures the local input-output nonlinearity ( similar to the `` depth '' in feedforward neural networks ( fnns ) ) , and ( c ) the recurrent skip coefficient which captures how rapidly the information propagates over time . we rigorously prove each measure 's existence and computability . our experimental results show that rnns might benefit from larger recurrent depth and feedforward depth . we further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems .", "topics": ["recurrent neural network", "neural networks"]}
{"title": "the cramer distance as a solution to biased wasserstein gradients", "abstract": "the wasserstein probability metric has received much attention from the machine learning community . unlike the kullback-leibler divergence , which strictly measures change in probability , the wasserstein metric reflects the underlying geometry between outcomes . the value of being sensitive to this geometry has been demonstrated , among others , in ordinal regression and generative modelling . in this paper we describe three natural properties of probability divergences that reflect requirements from machine learning : sum invariance , scale sensitivity , and unbiased sample gradients . the wasserstein metric possesses the first two properties but , unlike the kullback-leibler divergence , does not possess the third . we provide empirical evidence suggesting that this is a serious issue in practice . leveraging insights from probabilistic forecasting we propose an alternative to the wasserstein metric , the cram\\'er distance . we show that the cram\\'er distance possesses all three desired properties , combining the best of the wasserstein and kullback-leibler divergences . to illustrate the relevance of the cram\\'er distance in practice we design a new algorithm , the cram\\'er generative adversarial network ( gan ) , and show that it performs significantly better than the related wasserstein gan .", "topics": ["relevance"]}
{"title": "faster algorithms for large-scale machine learning using simple sampling techniques", "abstract": "now a days , the major challenge in machine learning is the `big~data ' challenge . the big data problems due to large number of data points or large number of features in each data point , or both , the training of models have become very slow . the training time has two major components : time to access the data and time to process ( learn from ) the data . in this paper , we have proposed one possible solution to handle the big data problems in machine learning . the idea is to reduce the training time through reducing data access time by proposing systematic sampling and cyclic/sequential sampling to select mini-batches from the dataset . to prove the effectiveness of proposed sampling techniques , we have used empirical risk minimization , which is commonly used machine learning problem , for strongly convex and smooth case . the problem has been solved using sag , saga , svrg , saag-ii and mbsgd ( mini-batched sgd ) , each using two step determination techniques , namely , constant step size and backtracking line search method . theoretical results prove the same convergence for systematic sampling , cyclic sampling and the widely used random sampling technique , in expectation . experimental results with bench marked datasets prove the efficacy of the proposed sampling techniques .", "topics": ["sampling ( signal processing )"]}
{"title": "a greedy homotopy method for regression with nonconvex constraints", "abstract": "constrained least squares regression is an essential tool for high-dimensional data analysis . given a partition $ \\mathcal { g } $ of input variables , this paper considers a particular class of nonconvex constraint functions that encourage the linear model to select a small number of variables from a small number of groups in $ \\mathcal { g } $ . such constraints are relevant in many practical applications , such as genome-wide association studies ( gwas ) . motivated by the efficiency of the lasso homotopy method , we present replasso , a greedy homotopy algorithm that tries to solve the induced sequence of nonconvex problems by solving a sequence of suitably adapted convex surrogate problems . we prove that in some situations replasso recovers the global minima of the nonconvex problem . moreover , even if it does not recover global minima , we prove that in relevant cases it will still do no worse than the lasso in terms of support and signed support recovery , while in practice outperforming it . we show empirically that the strategy can also be used to improve over other lasso-style algorithms . finally , a gwas of ankylosing spondylitis highlights our method 's practical utility .", "topics": ["sparse matrix"]}
{"title": "non-stochastic best arm identification and hyperparameter optimization", "abstract": "motivated by the task of hyperparameter optimization , we introduce the non-stochastic best-arm identification problem . within the multi-armed bandit literature , the cumulative regret objective enjoys algorithms and analyses for both the non-stochastic and stochastic settings while to the best of our knowledge , the best-arm identification framework has only been considered in the stochastic setting . we introduce the non-stochastic setting under this framework , identify a known algorithm that is well-suited for this setting , and analyze its behavior . next , by leveraging the iterative nature of standard machine learning algorithms , we cast hyperparameter optimization as an instance of non-stochastic best-arm identification , and empirically evaluate our proposed algorithm on this task . our empirical results show that , by allocating more resources to promising hyperparameter settings , we typically achieve comparable test accuracies an order of magnitude faster than baseline methods .", "topics": ["baseline ( configuration management )", "iteration"]}
{"title": "abstractions for ai-based user interfaces and systems", "abstract": "novel user interfaces based on artificial intelligence , such as natural-language agents , present new categories of engineering challenges . these systems need to cope with uncertainty and ambiguity , interface with machine learning algorithms , and compose information from multiple users to make decisions . we propose to treat these challenges as language-design problems . we describe three programming language abstractions for three core problems in intelligent system design . first , hypothetical worlds support nondeterministic search over spaces of alternative actions . second , a feature type system abstracts the interaction between applications and learning algorithms . finally , constructs for collaborative execution extend hypothetical worlds across multiple machines while controlling access to private data . we envision these features as first steps toward a complete language for implementing ai-based interfaces and applications .", "topics": ["natural language", "artificial intelligence"]}
{"title": "pixel-wise deep learning for contour detection", "abstract": "we address the problem of contour detection via per-pixel classifications of edge point . to facilitate the process , the proposed approach leverages with densenet , an efficient implementation of multiscale convolutional neural networks ( cnns ) , to extract an informative feature vector for each pixel and uses an svm classifier to accomplish contour detection . in the experiment of contour detection , we look into the effectiveness of combining per-pixel features from different cnn layers and verify their performance on bsds500 .", "topics": ["feature vector", "pixel"]}
{"title": "spatial statistics , image analysis and percolation theory", "abstract": "we develop a novel method for detection of signals and reconstruction of images in the presence of random noise . the method uses results from percolation theory . we specifically address the problem of detection of multiple objects of unknown shapes in the case of nonparametric noise . the noise density is unknown and can be heavy-tailed . the objects of interest have unknown varying intensities . no boundary shape constraints are imposed on the objects , only a set of weak bulk conditions is required . we view the object detection problem as a multiple hypothesis testing for discrete statistical inverse problems . we present an algorithm that allows to detect greyscale objects of various shapes in noisy images . we prove results on consistency and algorithmic complexity of our procedures . applications to cryo-electron microscopy are presented .", "topics": ["computational complexity theory", "object detection"]}
{"title": "robust bayesian optimization with student-t likelihood", "abstract": "bayesian optimization has recently attracted the attention of the automatic machine learning community for its excellent results in hyperparameter tuning . bo is characterized by the sample efficiency with which it can optimize expensive black-box functions . the efficiency is achieved in a similar fashion to the learning to learn methods : surrogate models ( typically in the form of gaussian processes ) learn the target function and perform intelligent sampling . this surrogate model can be applied even in the presence of noise ; however , as with most regression methods , it is very sensitive to outlier data . this can result in erroneous predictions and , in the case of bo , biased and inefficient exploration . in this work , we present a gp model that is robust to outliers which uses a student-t likelihood to segregate outliers and robustly conduct bayesian optimization . we present numerical results evaluating the proposed method in both artificial functions and real problems .", "topics": ["sampling ( signal processing )", "numerical analysis"]}
{"title": "time series prediction for graphs in kernel and dissimilarity spaces", "abstract": "graph models are relevant in many fields , such as distributed computing , intelligent tutoring systems or social network analysis . in many cases , such models need to take changes in the graph structure into account , i.e . a varying number of nodes or edges . predicting such changes within graphs can be expected to yield important insight with respect to the underlying dynamics , e.g . with respect to user behaviour . however , predictive techniques in the past have almost exclusively focused on single edges or nodes . in this contribution , we attempt to predict the future state of a graph as a whole . we propose to phrase time series prediction as a regression problem and apply dissimilarity- or kernel-based regression techniques , such as 1-nearest neighbor , kernel regression and gaussian process regression , which can be applied to graphs via graph kernels . the output of the regression is a point embedded in a pseudo-euclidean space , which can be analyzed using subsequent dissimilarity- or kernel-based processing methods . we discuss strategies to speed up gaussian processes regression from cubic to linear time and evaluate our approach on two well-established theoretical models of graph evolution as well as two real data sets from the domain of intelligent tutoring systems . we find that simple regression methods , such as kernel regression , are sufficient to capture the dynamics in the theoretical models , but that gaussian process regression significantly improves the prediction error for real-world data .", "topics": ["kernel ( operating system )", "time series"]}
{"title": "dropout as a low-rank regularizer for matrix factorization", "abstract": "regularization for matrix factorization ( mf ) and approximation problems has been carried out in many different ways . due to its popularity in deep learning , dropout has been applied also for this class of problems . despite its solid empirical performance , the theoretical properties of dropout as a regularizer remain quite elusive for this class of problems . in this paper , we present a theoretical analysis of dropout for mf , where bernoulli random variables are used to drop columns of the factors . we demonstrate the equivalence between dropout and a fully deterministic model for mf in which the factors are regularized by the sum of the product of squared euclidean norms of the columns . additionally , we inspect the case of a variable sized factorization and we prove that dropout achieves the global minimum of a convex approximation problem with ( squared ) nuclear norm regularization . as a result , we conclude that dropout can be used as a low-rank regularizer with data dependent singular-value thresholding .", "topics": ["approximation algorithm", "matrix regularization"]}
{"title": "identifying the relevant nodes without learning the model", "abstract": "we propose a method to identify all the nodes that are relevant to compute all the conditional probability distributions for a given set of nodes . our method is simple , effcient , consistent , and does not require learning a bayesian network first . therefore , our method can be applied to high-dimensional databases , e.g . gene expression databases .", "topics": ["bayesian network", "database"]}
{"title": "embracing data abundance : booktest dataset for reading comprehension", "abstract": "there is a practically unlimited amount of natural language data available . still , recent work in text comprehension has focused on datasets which are small relative to current computing possibilities . this article is making a case for the community to move to larger data and as a step in that direction it is proposing the booktest , a new dataset similar to the popular children 's book test ( cbt ) , however more than 60 times larger . we show that training on the new data improves the accuracy of our attention-sum reader model on the original cbt test data by a much larger margin than many recent attempts to improve the model architecture . on one version of the dataset our ensemble even exceeds the human baseline provided by facebook . we then show in our own human study that there is still space for further improvement .", "topics": ["baseline ( configuration management )", "natural language"]}
{"title": "learning with abandonment", "abstract": "consider a platform that wants to learn a personalized policy for each user , but the platform faces the risk of a user abandoning the platform if she is dissatisfied with the actions of the platform . for example , a platform is interested in personalizing the number of newsletters it sends , but faces the risk that the user unsubscribes forever . we propose a general thresholded learning model for scenarios like this , and discuss the structure of optimal policies . we describe salient features of optimal personalization algorithms and how feedback the platform receives impacts the results . furthermore , we investigate how the platform can efficiently learn the heterogeneity across users by interacting with a population and provide performance guarantees .", "topics": ["reinforcement learning"]}
{"title": "compressive phase-only filtering at extreme compression rates", "abstract": "we introduce an efficient method for the reconstruction of the correlation between a compressively measured image and a phase-only filter . the proposed method is based on two properties of phase-only filtering : such filtering is a unitary circulant transform , and the correlation plane it produces is usually sparse . thanks to these properties , phase-only filters are perfectly compatible with the framework of compressive sensing . moreover , the lasso-based recovery algorithm is very fast when phase-only filtering is used as the compression matrix . the proposed method can be seen as a generalisation of the correlation-based pattern recognition technique , which is hereby applied directly to non-adaptively acquired compressed data . at the time of measurement , any prior knowledge of the target object for which the data will be scanned is not required . we show that images measured at extremely high compression rates may still contain sufficient information for target classification and localization , even if the compression rate is high enough , that visual recognition of the target in the reconstructed image is no longer possible . the method has been applied by us to highly undersampled measurements obtained from a single-pixel camera , with sampling based on randomly chosen walsh-hadamard patterns .", "topics": ["sampling ( signal processing )", "sparse matrix"]}
{"title": "counting and uniform sampling from markov equivalent dags", "abstract": "we propose an exact solution for the problem of finding the size of a markov equivalence class ( mec ) . for the bounded degree graphs , the proposed solution is capable of computing the size of the mec in polynomial time . our proposed approach is based on a recursive method for counting the number of the elements of the mec when a specific vertex is set as the source variable . we will further use the idea to design a sampler , which is capable of sampling from an mec uniformly in polynomial time .", "topics": ["sampling ( signal processing )", "time complexity"]}
{"title": "visualizing and understanding curriculum learning for long short-term memory networks", "abstract": "curriculum learning emphasizes the order of training instances in a computational learning setup . the core hypothesis is that simpler instances should be learned early as building blocks to learn more complex ones . despite its usefulness , it is still unknown how exactly the internal representation of models are affected by curriculum learning . in this paper , we study the effect of curriculum learning on long short-term memory ( lstm ) networks , which have shown strong competency in many natural language processing ( nlp ) problems . our experiments on sentiment analysis task and a synthetic task similar to sequence prediction tasks in nlp show that curriculum learning has a positive effect on the lstm 's internal states by biasing the model towards building constructive representations i.e . the internal representation at the previous timesteps are used as building blocks for the final prediction . we also find that smaller models significantly improves when they are trained with curriculum learning . lastly , we show that curriculum learning helps more when the amount of training data is limited .", "topics": ["natural language processing", "natural language"]}
{"title": "monostream : a minimal-hardware high accuracy device-free wlan localization system", "abstract": "device-free ( df ) localization is an emerging technology that allows the detection and tracking of entities that do not carry any devices nor participate actively in the localization process . typically , df systems require a large number of transmitters and receivers to achieve acceptable accuracy , which is not available in many scenarios such as homes and small businesses . in this paper , we introduce monostream as an accurate single-stream df localization system that leverages the rich channel state information ( csi ) as well as mimo information from the physical layer to provide accurate df localization with only one stream . to boost its accuracy and attain low computational requirements , monostream models the df localization problem as an object recognition problem and uses a novel set of csi-context features and techniques with proven accuracy and efficiency . experimental evaluation in two typical testbeds , with a side-by-side comparison with the state-of-the-art , shows that monostream can achieve an accuracy of 0.95m with at least 26 % enhancement in median distance error using a single stream only . this enhancement in accuracy comes with an efficient execution of less than 23ms per location update on a typical laptop . this highlights the potential of monostream usage for real-time df tracking applications .", "topics": ["entity"]}
{"title": "prior-free and prior-dependent regret bounds for thompson sampling", "abstract": "we consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions . we are interested in studying prior-free and prior-dependent regret bounds , very much in the same spirit as the usual distribution-free and distribution-dependent bounds for the non-bayesian stochastic bandit . building on the techniques of audibert and bubeck [ 2009 ] and russo and roy [ 2013 ] we first show that thompson sampling attains an optimal prior-free bound in the sense that for any prior distribution its bayesian regret is bounded from above by $ 14 \\sqrt { n k } $ . this result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a bayesian regret bounded from below by $ \\frac { 1 } { 20 } \\sqrt { n k } $ . we also study the case of priors for the setting of bubeck et al . [ 2013 ] ( where the optimal mean is known as well as a lower bound on the smallest gap ) and we show that in this case the regret of thompson sampling is in fact uniformly bounded over time , thus showing that thompson sampling can greatly take advantage of the nice properties of these priors .", "topics": ["regret ( decision theory )", "loss function"]}
{"title": "network analysis of a corpus of undeciphered indus civilization inscriptions indicates syntactic organization", "abstract": "archaeological excavations in the sites of the indus valley civilization ( 2500-1900 bce ) in pakistan and northwestern india have unearthed a large number of artifacts with inscriptions made up of hundreds of distinct signs . to date there is no generally accepted decipherment of these sign sequences and there have been suggestions that the signs could be non-linguistic . here we apply complex network analysis techniques to a database of available indus inscriptions , with the aim of detecting patterns indicative of syntactic organization . our results show the presence of patterns , e.g . , recursive structures in the segmentation trees of the sequences , that suggest the existence of a grammar underlying these inscriptions .", "topics": ["text corpus"]}
{"title": "lecture notes on spectral graph methods", "abstract": "these are lecture notes that are based on the lectures from a class i taught on the topic of spectral graph methods at uc berkeley during the spring 2015 semester .", "topics": ["cluster analysis", "supervised learning"]}
{"title": "estimation of the sample covariance matrix from compressive measurements", "abstract": "this paper focuses on the estimation of the sample covariance matrix from low-dimensional random projections of data known as compressive measurements . in particular , we present an unbiased estimator to extract the covariance structure from compressive measurements obtained by a general class of random projection matrices consisting of i.i.d . zero-mean entries and finite first four moments . in contrast to previous works , we make no structural assumptions about the underlying covariance matrix such as being low-rank . in fact , our analysis is based on a non-bayesian data setting which requires no distributional assumptions on the set of data samples . furthermore , inspired by the generality of the projection matrices , we propose an approach to covariance estimation that utilizes sparse rademacher matrices . therefore , our algorithm can be used to estimate the covariance matrix in applications with limited memory and computation power at the acquisition devices . experimental results demonstrate that our approach allows for accurate estimation of the sample covariance matrix on several real-world data sets , including video data .", "topics": ["sparse matrix", "computation"]}
{"title": "policy gradient methods for off-policy control", "abstract": "off-policy learning refers to the problem of learning the value function of a way of behaving , or policy , while following a different policy . gradient-based off-policy learning algorithms , such as gtd and tdc/gq , converge even when using function approximation and incremental updates . however , they have been developed for the case of a fixed behavior policy . in control problems , one would like to adapt the behavior policy over time to become more greedy with respect to the existing value function . in this paper , we present the first gradient-based learning algorithms for this problem , which rely on the framework of policy gradient in order to modify the behavior policy . we present derivations of the algorithms , a convergence theorem , and empirical evidence showing that they compare favorably to existing approaches .", "topics": ["gradient"]}
{"title": "multi-task policy search", "abstract": "learning policies that generalize across multiple tasks is an important and challenging research topic in reinforcement learning and robotics . training individual policies for every single potential task is often impractical , especially for continuous task variations , requiring more principled approaches to share and transfer knowledge among similar tasks . we present a novel approach for learning a nonlinear feedback policy that generalizes across multiple tasks . the key idea is to define a parametrized policy as a function of both the state and the task , which allows learning a single policy that generalizes across multiple known and unknown tasks . applications of our novel approach to reinforcement and imitation learning in real-robot experiments are shown .", "topics": ["reinforcement learning", "nonlinear system"]}
{"title": "towards a neural statistician", "abstract": "an efficient learner is one who reuses what they already know to tackle a new problem . for a machine learner , this means understanding the similarities amongst datasets . in order to do this , one must take seriously the idea of working with datasets , rather than datapoints , as the key objects to model . towards this goal , we demonstrate an extension of a variational autoencoder that can learn a method for computing representations , or statistics , of datasets in an unsupervised fashion . the network is trained to produce statistics that encapsulate a generative model for each dataset . hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks . we show that we are able to learn statistics that can be used for : clustering datasets , transferring generative models to new datasets , selecting representative samples of datasets and classifying previously unseen classes . we refer to our model as a neural statistician , and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision .", "topics": ["generative model", "supervised learning"]}
{"title": "large-scale reasoning with owl", "abstract": "with the growth of the semantic web in size and importance , more and more knowledge is stored in machine-readable formats such as the web ontology language owl . this paper outlines common approaches for efficient reasoning on large-scale data consisting of billions ( $ 10^9 $ ) of triples . therefore , owl and its sublanguages , as well as forward and backward chaining techniques are presented . the webpie reasoner is discussed in detail as an example for forward chaining using mapreduce for materialisation . moreover , the querypie reasoner is presented as a backward chaining/hybrid approach which uses query rewriting . furthermore , an overview on other reasoners is given such as owlim and trowl .", "topics": ["mathematical optimization"]}
{"title": "blackout : speeding up recurrent neural network language models with very large vocabularies", "abstract": "we propose blackout , an approximation algorithm to efficiently train massive recurrent neural network language models ( rnnlms ) with million word vocabularies . blackout is motivated by using a discriminative loss , and we describe a new sampling strategy which significantly reduces computation while improving stability , sample efficiency , and rate of convergence . one way to understand blackout is to view it as an extension of the dropout strategy to the output layer , wherein we use a discriminative training loss and a weighted sampling scheme . we also establish close connections between blackout , importance sampling , and noise contrastive estimation ( nce ) . our experiments , on the recently released one billion word language modeling benchmark , demonstrate scalability and accuracy of blackout ; we outperform the state-of-the art , and achieve the lowest perplexity scores on this dataset . moreover , unlike other established methods which typically require gpus or cpu clusters , we show that a carefully implemented version of blackout requires only 1-10 days on a single machine to train a rnnlm with a million word vocabulary and billions of parameters on one billion words . although we describe blackout in the context of rnnlm training , it can be used to any networks with large softmax output layers .", "topics": ["sampling ( signal processing )", "approximation algorithm"]}
{"title": "pairwise rotation hashing for high-dimensional features", "abstract": "binary hashing is widely used for effective approximate nearest neighbors search . even though various binary hashing methods have been proposed , very few methods are feasible for extremely high-dimensional features often used in visual tasks today . we propose a novel highly sparse linear hashing method based on pairwise rotations . the encoding cost of the proposed algorithm is $ \\mathrm { o } ( n \\log n ) $ for n-dimensional features , whereas that of the existing state-of-the-art method is typically $ \\mathrm { o } ( n^2 ) $ . the proposed method is also remarkably faster in the learning phase . along with the efficiency , the retrieval accuracy is comparable to or slightly outperforming the state-of-the-art . pairwise rotations used in our method are formulated from an analytical study of the trade-off relationship between quantization error and entropy of binary codes . although these hashing criteria are widely used in previous researches , its analytical behavior is rarely studied . all building blocks of our algorithm are based on the analytical solution , and it thus provides a fairly simple and efficient procedure .", "topics": ["computational complexity theory", "sparse matrix"]}
{"title": "how much does your data exploration overfit ? controlling bias via information usage", "abstract": "modern data is messy and high-dimensional , and it is often not clear a priori what are the right questions to ask . instead , the analyst typically needs to use the data to search for interesting analyses to perform and hypotheses to test . this is an adaptive process , where the choice of analysis to be performed next depends on the results of the previous analyses on the same data . ultimately , which results are reported can be heavily influenced by the data . it is widely recognized that this process , even if well-intentioned , can lead to biases and false discoveries , contributing to the crisis of reproducibility in science . but while % the adaptive nature of exploration any data-exploration renders standard statistical theory invalid , experience suggests that different types of exploratory analysis can lead to disparate levels of bias , and the degree of bias also depends on the particulars of the data set . in this paper , we propose a general information usage framework to quantify and provably bound the bias and other error metrics of an arbitrary exploratory analysis . we prove that our mutual information based bound is tight in natural settings , and then use it to give rigorous insights into when commonly used procedures do or do not lead to substantially biased estimation . through the lens of information usage , we analyze the bias of specific exploration procedures such as filtering , rank selection and clustering . our general framework also naturally motivates randomization techniques that provably reduces exploration bias while preserving the utility of the data analysis . we discuss the connections between our approach and related ideas from differential privacy and blinded data analysis , and supplement our results with illustrative simulations .", "topics": ["cluster analysis", "simulation"]}
{"title": "prediction of the yield of enzymatic synthesis of betulinic acid ester using artificial neural networks and support vector machine", "abstract": "3\\b { eta } -o-phthalic ester of betulinic acid is of great importance in anticancer studies . however , the optimization of its reaction conditions requires a large number of experimental works . to simplify the number of times of optimization in experimental works , here , we use artificial neural network ( ann ) and support vector machine ( svm ) models for the prediction of yields of 3\\b { eta } -o-phthalic ester of betulinic acid synthesized by betulinic acid and phthalic anhydride using lipase as biocatalyst . general regression neural network ( grnn ) , multilayer feed-forward neural network ( mlfn ) and the svm models were trained based on experimental data . four indicators were set as independent variables , including time ( h ) , temperature ( c ) , amount of enzyme ( mg ) and molar ratio , while the yield of the 3\\b { eta } -o-phthalic ester of betulinic acid was set as the dependent variable . results show that the grnn and svm models have the best prediction results during the testing process , with comparatively low rms errors ( 4.01 and 4.23respectively ) and short training times ( both 1s ) . the prediction accuracy of the grnn and svm are both 100 % in testing process , under the tolerance of 30 % .", "topics": ["support vector machine"]}
{"title": "opennmt : open-source toolkit for neural machine translation", "abstract": "we describe an open-source toolkit for neural machine translation ( nmt ) . the toolkit prioritizes efficiency , modularity , and extensibility with the goal of supporting nmt research into model architectures , feature representations , and source modalities , while maintaining competitive performance and reasonable training requirements . the toolkit consists of modeling and translation support , as well as detailed pedagogical documentation about the underlying techniques .", "topics": ["machine translation"]}
{"title": "scaling text with the class affinity model", "abstract": "probabilistic methods for classifying text form a rich tradition in machine learning and natural language processing . for many important problems , however , class prediction is uninteresting because the class is known , and instead the focus shifts to estimating latent quantities related to the text , such as affect or ideology . we focus on one such problem of interest , estimating the ideological positions of 55 irish legislators in the 1991 d\\'ail confidence vote . to solve the d\\'ail scaling problem and others like it , we develop a text modeling framework that allows actors to take latent positions on a `` gray '' spectrum between `` black '' and `` white '' polar opposites . we are able to validate results from this model by measuring the influences exhibited by individual words , and we are able to quantify the uncertainty in the scaling estimates by using a sentence-level block bootstrap . applying our method to the d\\'ail debate , we are able to scale the legislators between extreme pro-government and pro-opposition in a way that reveals nuances in their speeches not captured by their votes or party affiliations .", "topics": ["natural language processing"]}
{"title": "empirical study of proxtone and proxtone $ ^+ $ for fast learning of large scale sparse models", "abstract": "proxtone is a novel and fast method for optimization of large scale non-smooth convex problem \\cite { shi2015large } . in this work , we try to use proxtone method in solving large scale \\emph { non-smooth non-convex } problems , for example training of sparse deep neural network ( sparse dnn ) or sparse convolutional neural network ( sparse cnn ) for embedded or mobile device . proxtone converges much faster than first order methods , while first order method is easy in deriving and controlling the sparseness of the solutions . thus in some applications , in order to train sparse models fast , we propose to combine the merits of both methods , that is we use proxtone in the first several epochs to reach the neighborhood of an optimal solution , and then use the first order method to explore the possibility of sparsity in the following training . we call such method proxtone plus ( proxtone $ ^+ $ ) . both proxtone and proxtone $ ^+ $ are tested in our experiments , and which demonstrate both methods improved convergence speed twice as fast at least on diverse sparse model learning problems , and at the same time reduce the size to 0.5\\ % for dnn models . the source of all the algorithms is available upon request .", "topics": ["optimization problem", "sparse matrix"]}
{"title": "kernel alignment inspired linear discriminant analysis", "abstract": "kernel alignment measures the degree of similarity between two kernels . in this paper , inspired from kernel alignment , we propose a new linear discriminant analysis ( lda ) formulation , kernel alignment lda ( kalda ) . we first define two kernels , data kernel and class indicator kernel . the problem is to find a subspace to maximize the alignment between subspace-transformed data kernel and class indicator kernel . surprisingly , the kernel alignment induced kalda objective function is very similar to classical lda and can be expressed using between-class and total scatter matrices . this can be extended to multi-label data . we use a stiefel-manifold gradient descent algorithm to solve this problem . we perform experiments on 8 single-label and 6 multi-label data sets . results show that kalda has very good performance on many single-label and multi-label problems .", "topics": ["kernel ( operating system )", "loss function"]}
{"title": "stackgan++ : realistic image synthesis with stacked generative adversarial networks", "abstract": "although generative adversarial networks ( gans ) have shown remarkable success in various tasks , they still face challenges in generating high quality images . in this paper , we propose stacked generative adversarial networks ( stackgan ) aiming at generating high-resolution photo-realistic images . first , we propose a two-stage generative adversarial network architecture , stackgan-v1 , for text-to-image synthesis . the stage-i gan sketches the primitive shape and colors of the object based on given text description , yielding low-resolution images . the stage-ii gan takes stage-i results and text descriptions as inputs , and generates high-resolution images with photo-realistic details . second , an advanced multi-stage generative adversarial network architecture , stackgan-v2 , is proposed for both conditional and unconditional generative tasks . our stackgan-v2 consists of multiple generators and discriminators in a tree-like structure ; images at multiple scales corresponding to the same scene are generated from different branches of the tree . stackgan-v2 shows more stable training behavior than stackgan-v1 by jointly approximating multiple distributions . extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images .", "topics": ["approximation algorithm"]}
{"title": "reader-aware multi-document summarization via sparse coding", "abstract": "we propose a new mds paradigm called reader-aware multi-document summarization ( ra-mds ) . specifically , a set of reader comments associated with the news reports are also collected . the generated summaries from the reports for the event should be salient according to not only the reports but also the reader comments . to tackle this ra-mds problem , we propose a sparse-coding-based method that is able to calculate the salience of the text units by jointly considering news reports and reader comments . another reader-aware characteristic of our framework is to improve linguistic quality via entity rewriting . the rewriting consideration is jointly assessed together with other summarization requirements under a unified optimization model . to support the generation of compressive summaries via optimization , we explore a finer syntactic unit , namely , noun/verb phrase . in this work , we also generate a data set for conducting ra-mds . extensive experiments on this data set and some classical data sets demonstrate the effectiveness of our proposed approach .", "topics": ["sparse matrix"]}
