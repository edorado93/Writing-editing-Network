{"title": "sleeping beauty reconsidered : conditioning and reflection in asynchronous systems", "abstract": "a careful analysis of conditioning in the sleeping beauty problem is done , using the formal model for reasoning about knowledge and probability developed by halpern and tuttle . while the sleeping beauty problem has been viewed as revealing problems with conditioning in the presence of imperfect recall , the analysis done here reveals that the problems are not so much due to imperfect recall as to asynchrony . the implications of this analysis for van fraassen 's reflection principle and savage 's sure-thing principle are considered ."}
{"title": "mining combined causes in large data sets", "abstract": "in recent years , many methods have been developed for detecting causal relationships in observational data . some of them have the potential to tackle large data sets . however , these methods fail to discover a combined cause , i.e . a multi-factor cause consisting of two or more component variables which individually are not causes . a straightforward approach to uncovering a combined cause is to include both individual and combined variables in the causal discovery using existing methods , but this scheme is computationally infeasible due to the huge number of combined variables . in this paper , we propose a novel approach to address this practical causal discovery problem , i.e . mining combined causes in large data sets . the experiments with both synthetic and real world data sets show that the proposed method can obtain high-quality causal discoveries with a high computational efficiency ."}
{"title": "an evaluation of structural parameters for probabilistic reasoning : results on benchmark circuits", "abstract": "many algorithms for processing probabilistic networks are dependent on the topological properties of the problem 's structure . such algorithms ( e.g. , clustering , conditioning ) are effective only if the problem has a sparse graph captured by parameters such as tree width and cycle-cut set size . in this paper we initiate a study to determine the potential of structure-based algorithms in real-life applications . we analyze empirically the structural properties of problems coming from the circuit diagnosis domain . specifically , we locate those properties that capture the effectiveness of clustering and conditioning as well as of a family of conditioning+clustering algorithms designed to gradually trade space for time . we perform our analysis on 11 benchmark circuits widely used in the testing community . we also report on the effect of ordering heuristics on tree-clustering and show that , on our benchmarks , the well-known max-cardinality ordering is substantially inferior to an ordering called min-degree ."}
{"title": "stable-unstable semantics : beyond np with normal logic programs", "abstract": "standard answer set programming ( asp ) targets at solving search problems from the first level of the polynomial time hierarchy ( ph ) . tackling search problems beyond np using asp is less straightforward . the class of disjunctive logic programs offers the most prominent way of reaching the second level of the ph , but encoding respective hard problems as disjunctive programs typically requires sophisticated techniques such as saturation or meta-interpretation . the application of such techniques easily leads to encodings that are inaccessible to non-experts . furthermore , while disjunctive asp solvers often rely on calls to a ( co- ) np oracle , it may be difficult to detect from the input program where the oracle is being accessed . in other formalisms , such as quantified boolean formulas ( qbfs ) , the interface to the underlying oracle is more transparent as it is explicitly recorded in the quantifier prefix of a formula . on the other hand , asp has advantages over qbfs from the modeling perspective . the rich high-level languages such as asp-core-2 offer a wide variety of primitives that enable concise and natural encodings of search problems . in this paper , we present a novel logic programming -- based modeling paradigm that combines the best features of asp and qbfs . we develop so-called combined logic programs in which oracles are directly cast as ( normal ) logic programs themselves . recursive incarnations of this construction enable logic programming on arbitrarily high levels of the ph . we develop a proof-of-concept implementation for our new paradigm . this paper is under consideration for acceptance in tplp ."}
{"title": "the utility of hedged assertions in the emergence of shared categorical labels", "abstract": "we investigate the emergence of shared concepts in a community of language users using a multi-agent simulation . we extend results showing that negated assertions are of use in developing shared categories , to include assertions modified by linguistic hedges . results show that using hedged assertions positively affects the emergence of shared categories in two distinct ways . firstly , using contraction hedges like ` very ' gives better convergence over time . secondly , using expansion hedges such as ` quite ' reduces concept overlap . however , both these improvements come at a cost of slower speed of development ."}
{"title": "convergent actor-critic algorithms under off-policy training and function approximation", "abstract": "we present the first class of policy-gradient algorithms that work with both state-value and policy function-approximation , and are guaranteed to converge under off-policy training . our solution targets problems in reinforcement learning where the action representation adds to the-curse-of-dimensionality ; that is , with continuous or large action sets , thus making it infeasible to estimate state-action value functions ( q functions ) . using state-value functions helps to lift the curse and as a result naturally turn our policy-gradient solution into classical actor-critic architecture whose actor uses state-value function for the update . our algorithms , gradient actor-critic and emphatic actor-critic , are derived based on the exact gradient of averaged state-value function objective and thus are guaranteed to converge to its optimal solution , while maintaining all the desirable properties of classical actor-critic methods with no additional hyper-parameters . to our knowledge , this is the first time that convergent off-policy learning methods have been extended to classical actor-critic methods with function approximation ."}
{"title": "using a distributional semantic vector space with a knowledge base for reasoning in uncertain conditions", "abstract": "the inherent inflexibility and incompleteness of commonsense knowledge bases ( kb ) has limited their usefulness . we describe a system called displacer for performing kb queries extended with the analogical capabilities of the word2vec distributional semantic vector space ( dsvs ) . this allows the system to answer queries with information which was not contained in the original kb in any form . by performing analogous queries on semantically related terms and mapping their answers back into the context of the original query using displacement vectors , we are able to give approximate answers to many questions which , if posed to the kb alone , would return no results . we also show how the hand-curated knowledge in a kb can be used to increase the accuracy of a dsvs in solving analogy problems . in these ways , a kb and a dsvs can make up for each other 's weaknesses ."}
{"title": "context-aware generative adversarial privacy", "abstract": "preserving the utility of published datasets while simultaneously providing provable privacy guarantees is a well-known challenge . on the one hand , context-free privacy solutions , such as differential privacy , provide strong privacy guarantees , but often lead to a significant reduction in utility . on the other hand , context-aware privacy solutions , such as information theoretic privacy , achieve an improved privacy-utility tradeoff , but assume that the data holder has access to dataset statistics . we circumvent these limitations by introducing a novel context-aware privacy framework called generative adversarial privacy ( gap ) . gap leverages recent advancements in generative adversarial networks ( gans ) to allow the data holder to learn privatization schemes from the dataset itself . under gap , learning the privacy mechanism is formulated as a constrained minimax game between two players : a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks on the individuals ' private variables , and an adversary that tries to infer the private variables from the sanitized dataset . to evaluate gap 's performance , we investigate two simple ( yet canonical ) statistical dataset models : ( a ) the binary data model , and ( b ) the binary gaussian mixture model . for both models , we derive game-theoretically optimal minimax privacy mechanisms , and show that the privacy mechanisms learned from data ( in a generative adversarial fashion ) match the theoretically optimal ones . this demonstrates that our framework can be easily applied in practice , even in the absence of dataset statistics ."}
{"title": "embodied question answering", "abstract": "we present a new ai task -- embodied question answering ( embodiedqa ) -- where an agent is spawned at a random location in a 3d environment and asked a question ( `` what color is the car ? '' ) . in order to answer , the agent must first intelligently navigate to explore the environment , gather information through first-person ( egocentric ) vision , and then answer the question ( `` orange '' ) . this challenging task requires a range of ai skills -- active perception , language understanding , goal-driven navigation , commonsense reasoning , and grounding of language into actions . in this work , we develop the environments , end-to-end-trained reinforcement learning agents , and evaluation protocols for embodiedqa ."}
{"title": "visual learning of arithmetic operations", "abstract": "a simple neural network model is presented for end-to-end visual learning of arithmetic operations from pictures of numbers . the input consists of two pictures , each showing a 7-digit number . the output , also a picture , displays the number showing the result of an arithmetic operation ( e.g. , addition or subtraction ) on the two input numbers . the concepts of a number , or of an operator , are not explicitly introduced . this indicates that addition is a simple cognitive task , which can be learned visually using a very small number of neurons . other operations , e.g. , multiplication , were not learnable using this architecture . some tasks were not learnable end-to-end ( e.g. , addition with roman numerals ) , but were easily learnable once broken into two separate sub-tasks : a perceptual \\textit { character recognition } and cognitive \\textit { arithmetic } sub-tasks . this indicates that while some tasks may be easily learnable end-to-end , other may need to be broken into sub-tasks ."}
{"title": "interactive ant colony optimisation ( iaco ) for early lifecycle software design", "abstract": "software design is crucial to successful software development , yet is a demanding multi-objective problem for software engineers . in an attempt to assist the software designer , interactive ( i.e . human in-the-loop ) meta-heuristic search techniques such as evolutionary computing have been applied and show promising results . recent investigations have also shown that ant colony optimization ( aco ) can outperform evolutionary computing as a potential search engine for interactive software design . with a limited computational budget , aco produces superior candidate design solutions in a smaller number of iterations . building on these findings , we propose a novel interactive aco ( iaco ) approach to assist the designer in early lifecycle software design , in which the search is steered jointly by subjective designer evaluation as well as machine fitness functions relating the structural integrity and surrogate elegance of software designs . results show that iaco is speedy , responsive and highly effective in enabling interactive , dynamic multi-objective search in early lifecycle software design . study participants rate the iaco search experience as compelling . results of machine learning of fitness measure weightings indicate that software design elegance does indeed play a significant role in designer evaluation of candidate software design . we conclude that the evenness of the number of attributes and methods among classes ( nac ) is a significant surrogate elegance measure , which in turn suggests that this evenness of distribution , when combined with structural integrity , is an implicit but crucial component of effective early lifecycle software design ."}
{"title": "infinite-dimensional log-determinant divergences ii : alpha-beta divergences", "abstract": "this work presents a parametrized family of divergences , namely alpha-beta log- determinant ( log-det ) divergences , between positive definite unitized trace class operators on a hilbert space . this is a generalization of the alpha-beta log-determinant divergences between symmetric , positive definite matrices to the infinite-dimensional setting . the family of alpha-beta log-det divergences is highly general and contains many divergences as special cases , including the recently formulated infinite dimensional affine-invariant riemannian distance and the infinite-dimensional alpha log-det divergences between positive definite unitized trace class operators . in particular , it includes a parametrized family of metrics between positive definite trace class operators , with the affine-invariant riemannian distance and the square root of the symmetric stein divergence being special cases . for the alpha-beta log-det divergences between covariance operators on a reproducing kernel hilbert space ( rkhs ) , we obtain closed form formulas via the corresponding gram matrices ."}
{"title": "value-function approximations for partially observable markov decision processes", "abstract": "partially observable markov decision processes ( pomdps ) provide an elegant mathematical framework for modeling complex decision and planning problems in stochastic domains in which states of the system are observable only indirectly , via a set of imperfect or noisy observations . the modeling advantage of pomdps , however , comes at a price -- exact methods for solving them are computationally very expensive and thus applicable in practice only to very simple problems . we focus on efficient approximation ( heuristic ) methods that attempt to alleviate the computational problem and trade off accuracy for speed . we have two objectives here . first , we survey various approximation methods , analyze their properties and relations and provide some new insights into their differences . second , we present a number of new approximation methods and novel refinements of existing techniques . the theoretical results are supported by experiments on a problem from the agent navigation domain ."}
{"title": "learning fuzzy controllers in mobile robotics with embedded preprocessing", "abstract": "the automatic design of controllers for mobile robots usually requires two stages . in the first stage , sensorial data are preprocessed or transformed into high level and meaningful values of variables whichare usually defined from expert knowledge . in the second stage , a machine learning technique is applied toobtain a controller that maps these high level variables to the control commands that are actually sent tothe robot . this paper describes an algorithm that is able to embed the preprocessing stage into the learningstage in order to get controllers directly starting from sensorial raw data with no expert knowledgeinvolved . due to the high dimensionality of the sensorial data , this approach uses quantified fuzzy rules ( qfrs ) , that are able to transform low-level input variables into high-level input variables , reducingthe dimensionality through summarization . the proposed learning algorithm , called iterative quantifiedfuzzy rule learning ( iqfrl ) , is based on genetic programming . iqfrl is able to learn rules with differentstructures , and can manage linguistic variables with multiple granularities . the algorithm has been testedwith the implementation of the wall-following behavior both in several realistic simulated environmentswith different complexity and on a pioneer 3-at robot in two real environments . results have beencompared with several well-known learning algorithms combined with different data preprocessingtechniques , showing that iqfrl exhibits a better and statistically significant performance . moreover , three real world applications for which iqfrl plays a central role are also presented : path and objecttracking with static and moving obstacles avoidance ."}
{"title": "text compression for sentiment analysis via evolutionary algorithms", "abstract": "can textual data be compressed intelligently without losing accuracy in evaluating sentiment ? in this study , we propose a novel evolutionary compression algorithm , parsec ( parts-of-speech for sentiment compression ) , which makes use of parts-of-speech tags to compress text in a way that sacrifices minimal classification accuracy when used in conjunction with sentiment analysis algorithms . an analysis of parsec with eight commercial and non-commercial sentiment analysis algorithms on twelve english sentiment data sets reveals that accurate compression is possible with ( 0 % , 1.3 % , 3.3 % ) loss in sentiment classification accuracy for ( 20 % , 50 % , 75 % ) data compression with parsec using lingpipe , the most accurate of the sentiment algorithms . other sentiment analysis algorithms are more severely affected by compression . we conclude that significant compression of text data is possible for sentiment analysis depending on the accuracy demands of the specific application and the specific sentiment analysis algorithm used ."}
{"title": "manipulation of nanson 's and baldwin 's rules", "abstract": "nanson 's and baldwin 's voting rules select a winner by successively eliminating candidates with low borda scores . we show that these rules have a number of desirable computational properties . in particular , with unweighted votes , it is np-hard to manipulate either rule with one manipulator , whilst with weighted votes , it is np-hard to manipulate either rule with a small number of candidates and a coalition of manipulators . as only a couple of other voting rules are known to be np-hard to manipulate with a single manipulator , nanson 's and baldwin 's rules appear to be particularly resistant to manipulation from a theoretical perspective . we also propose a number of approximation methods for manipulating these two rules . experiments demonstrate that both rules are often difficult to manipulate in practice . these results suggest that elimination style voting rules deserve further study ."}
{"title": "an integrated framework for learning and reasoning", "abstract": "learning and reasoning are both aspects of what is considered to be intelligence . their studies within ai have been separated historically , learning being the topic of machine learning and neural networks , and reasoning falling under classical ( or symbolic ) ai . however , learning and reasoning are in many ways interdependent . this paper discusses the nature of some of these interdependencies and proposes a general framework called flare , that combines inductive learning using prior knowledge together with reasoning in a propositional setting . several examples that test the framework are presented , including classical induction , many important reasoning protocols and two simple expert systems ."}
{"title": "the digital synaptic neural substrate : a new approach to computational creativity", "abstract": "we introduce a new artificial intelligence ( ai ) approach called , the 'digital synaptic neural substrate ' ( dsns ) . it uses selected attributes from objects in various domains ( e.g . chess problems , classical music , renowned artworks ) and recombines them in such a way as to generate new attributes that can then , in principle , be used to create novel objects of creative value to humans relating to any one of the source domains . this allows some of the burden of creative content generation to be passed from humans to machines . the approach was tested in the domain of chess problem composition . we used it to automatically compose numerous sets of chess problems based on attributes extracted and recombined from chess problems and tournament games by humans , renowned paintings , computer-evolved abstract art , photographs of people , and classical music tracks . the quality of these generated chess problems was then assessed automatically using an existing and experimentally-validated computational chess aesthetics model . they were also assessed by human experts in the domain . the results suggest that attributes collected and recombined from chess and other domains using the dsns approach can indeed be used to automatically generate chess problems of reasonably high aesthetic quality . in particular , a low quality chess source ( i.e . tournament game sequences between weak players ) used in combination with actual photographs of people was able to produce three-move chess problems of comparable quality or better to those generated using a high quality chess source ( i.e . published compositions by human experts ) , and more efficiently as well . why information from a foreign domain can be integrated and functional in this way remains an open question for now . the dsns approach is , in principle , scalable and applicable to any domain in which objects have attributes that can be represented using real numbers ."}
{"title": "tech report a variational hem algorithm for clustering hidden markov models", "abstract": "the hidden markov model ( hmm ) is a generative model that treats sequential data under the assumption that each observation is conditioned on the state of a discrete hidden variable that evolves in time as a markov chain . in this paper , we derive a novel algorithm to cluster hmms through their probability distributions . we propose a hierarchical em algorithm that i ) clusters a given collection of hmms into groups of hmms that are similar , in terms of the distributions they represent , and ii ) characterizes each group by a `` cluster center '' , i.e. , a novel hmm that is representative for the group . we present several empirical studies that illustrate the benefits of the proposed algorithm ."}
{"title": "towards a general framework for actual causation using cp-logic", "abstract": "since pearl 's seminal work on providing a formal language for causality , the subject has garnered a lot of interest among philosophers and researchers in artificial intelligence alike . one of the most debated topics in this context regards the notion of actual causation , which concerns itself with specific - as opposed to general - causal claims . the search for a proper formal definition of actual causation has evolved into a controversial debate , that is pervaded with ambiguities and confusion . the goal of our research is twofold . first , we wish to provide a clear way to compare competing definitions . second , we also want to improve upon these definitions so they can be applied to a more diverse range of instances , including non-deterministic ones . to achieve these goals we will provide a general , abstract definition of actual causation , formulated in the context of the expressive language of cp-logic ( causal probabilistic logic ) . we will then show that three recent definitions by ned hall ( originally formulated for structural models ) and a definition of our own ( formulated for cp-logic directly ) can be viewed and directly compared as instantiations of this abstract definition , which allows them to deal with a broader range of examples ."}
{"title": "preliminary report on wasp 2.0", "abstract": "answer set programming ( asp ) is a declarative programming paradigm . the intrinsic complexity of the evaluation of asp programs makes the development of more effective and faster systems a challenging research topic . this paper reports on the recent improvements of the asp solver wasp . wasp is undergoing a refactoring process which will end up in the release of a new and more performant version of the software . in particular the paper focus on the improvements to the core evaluation algorithms working on normal programs . a preliminary experiment on benchmarks from the 3rd asp competition belonging to the np class is reported . the previous version of wasp was often not competitive with alternative solutions on this class . the new version of wasp shows a substantial increase in performance ."}
{"title": "abductive , causal , and counterfactual conditionals under incomplete probabilistic knowledge", "abstract": "we study abductive , causal , and non-causal conditionals in indicative and counterfactual formulations using probabilistic truth table tasks under incomplete probabilistic knowledge ( n = 80 ) . we frame the task as a probability-logical inference problem . the most frequently observed response type across all conditions was a class of conditional event interpretations of conditionals ; it was followed by conjunction interpretations . an interesting minority of participants neglected some of the relevant imprecision involved in the premises when inferring lower or upper probability bounds on the target conditional/counterfactual ( `` halfway responses '' ) . we discuss the results in the light of coherence-based probability logic and the new paradigm psychology of reasoning ."}
{"title": "developing knowledge-enhanced chronic disease risk prediction models from regional ehr repositories", "abstract": "precision medicine requires the precision disease risk prediction models . in literature , there have been a lot well-established ( inter- ) national risk models , but when applying them into the local population , the prediction performance becomes unsatisfactory . to address the localization issue , this paper exploits the way to develop knowledge-enhanced localized risk models . on the one hand , we tune models by learning from regional electronic health record ( ehr ) repositories , and on the other hand , we propose knowledge injection into the ehr data learning process . for experiments , we leverage the pooled cohort equations ( pce , as recommended in acc/aha guidelines to estimate the risk of ascvd ) to develop a localized ascvd risk prediction model in diabetes . the experimental results show that , if directly using the pce algorithm on our cohort , the auc is only 0.653 , while our knowledge-enhanced localized risk model can achieve higher prediction performance with auc of 0.723 ( improved by 10.7 % ) ."}
{"title": "directional consistency for continuous numerical constraints", "abstract": "bounds consistency is usually enforced on continuous constraints by first decomposing them into binary and ternary primitives . this decomposition has long been shown to drastically slow down the computation of solutions . to tackle this , benhamou et al . have introduced an algorithm that avoids formally decomposing constraints . its better efficiency compared to the former method has already been experimentally demonstrated . it is shown here that their algorithm implements a strategy to enforce on a continuous constraint a consistency akin to directional bounds consistency as introduced by dechter and pearl for discrete problems . the algorithm is analyzed in this framework , and compared with algorithms that enforce bounds consistency . these theoretical results are eventually contrasted with new experimental results on standard benchmarks from the interval constraint community ."}
{"title": "protocol requirements for self-organizing artifacts : towards an ambient intelligence", "abstract": "we discuss which properties common-use artifacts should have to collaborate without human intervention . we conceive how devices , such as mobile phones , pdas , and home appliances , could be seamlessly integrated to provide an `` ambient intelligence '' that responds to the user 's desires without requiring explicit programming or commands . while the hardware and software technology to build such systems already exists , as yet there is no standard protocol that can learn new meanings . we propose the first steps in the development of such a protocol , which would need to be adaptive , extensible , and open to the community , while promoting self-organization . we argue that devices , interacting through `` game-like '' moves , can learn to agree about how to communicate , with whom to cooperate , and how to delegate and coordinate specialized tasks . thus , they may evolve a distributed cognition or collective intelligence capable of tackling complex tasks ."}
{"title": "concept relation discovery and innovation enabling technology ( cordiet )", "abstract": "concept relation discovery and innovation enabling technology ( cordiet ) , is a toolbox for gaining new knowledge from unstructured text data . at the core of cordiet is the c-k theory which captures the essential elements of innovation . the tool uses formal concept analysis ( fca ) , emergent self organizing maps ( esom ) and hidden markov models ( hmm ) as main artifacts in the analysis process . the user can define temporal , text mining and compound attributes . the text mining attributes are used to analyze the unstructured text in documents , the temporal attributes use these document 's timestamps for analysis . the compound attributes are xml rules based on text mining and temporal attributes . the user can cluster objects with object-cluster rules and can chop the data in pieces with segmentation rules . the artifacts are optimized for efficient data analysis ; object labels in the fca lattice and esom map contain an url on which the user can click to open the selected document ."}
{"title": "incremental truncated lstd", "abstract": "balancing between computational efficiency and sample efficiency is an important goal in reinforcement learning . temporal difference ( td ) learning algorithms stochastically update the value function , with a linear time complexity in the number of features , whereas least-squares temporal difference ( lstd ) algorithms are sample efficient but can be quadratic in the number of features . in this work , we develop an efficient incremental low-rank lstd ( { \\lambda } ) algorithm that progresses towards the goal of better balancing computation and sample efficiency . the algorithm reduces the computation and storage complexity to the number of features times the chosen rank parameter while summarizing past samples efficiently to nearly obtain the sample complexity of lstd . we derive a simulation bound on the solution given by truncated low-rank approximation , illustrating a bias- variance trade-off dependent on the choice of rank . we demonstrate that the algorithm effectively balances computational complexity and sample efficiency for policy evaluation in a benchmark task and a high-dimensional energy allocation domain ."}
{"title": "an automatic water detection approach based on dempster-shafer theory for multi spectral images", "abstract": "detection of surface water in natural environment via multi-spectral imagery has been widely utilized in many fields , such land cover identification . however , due to the similarity of the spectra of water bodies , built-up areas , approaches based on high-resolution satellites sometimes confuse these features . a popular direction to detect water is spectral index , often requiring the ground truth to find appropriate thresholds manually . as for traditional machine learning methods , they identify water merely via differences of spectra of various land covers , without taking specific properties of spectral reflection into account . in this paper , we propose an automatic approach to detect water bodies based on dempster-shafer theory , combining supervised learning with specific property of water in spectral band in a fully unsupervised context . the benefits of our approach are twofold . on the one hand , it performs well in mapping principle water bodies , including little streams and branches . on the other hand , it labels all objects usually confused with water as ` ignorance ' , including half-dry watery areas , built-up areas and semi-transparent clouds and shadows . ` ignorance ' indicates not only limitations of the spectral properties of water and supervised learning itself but insufficiency of information from multi-spectral bands as well , providing valuable information for further land cover classification ."}
{"title": "on the real-time vehicle placement problem", "abstract": "motivated by ride-sharing platforms ' efforts to reduce their riders ' wait times for a vehicle , this paper introduces a novel problem of placing vehicles to fulfill real-time pickup requests in a spatially and temporally changing environment . the real-time nature of this problem makes it fundamentally different from other placement and scheduling problems , as it requires not only real-time placement decisions but also handling real-time request dynamics , which are influenced by human mobility patterns . we use a dataset of ten million ride requests from four major u.s. cities to show that the requests exhibit significant self-similarity . we then propose distributed online learning algorithms for the real-time vehicle placement problem and bound their expected performance under this observed self-similarity ."}
{"title": "identifying interaction sites in `` recalcitrant '' proteins : predicted protein and rna binding sites in rev proteins of hiv-1 and eiav agree with experimental data", "abstract": "protein-protein and protein nucleic acid interactions are vitally important for a wide range of biological processes , including regulation of gene expression , protein synthesis , and replication and assembly of many viruses . we have developed machine learning approaches for predicting which amino acids of a protein participate in its interactions with other proteins and/or nucleic acids , using only the protein sequence as input . in this paper , we describe an application of classifiers trained on datasets of well-characterized protein-protein and protein-rna complexes for which experimental structures are available . we apply these classifiers to the problem of predicting protein and rna binding sites in the sequence of a clinically important protein for which the structure is not known : the regulatory protein rev , essential for the replication of hiv-1 and other lentiviruses . we compare our predictions with published biochemical , genetic and partial structural information for hiv-1 and eiav rev and with our own published experimental mapping of rna binding sites in eiav rev . the predicted and experimentally determined binding sites are in very good agreement . the ability to predict reliably the residues of a protein that directly contribute to specific binding events - without the requirement for structural information regarding either the protein or complexes in which it participates - can potentially generate new disease intervention strategies ."}
{"title": "meta-learning within projective simulation", "abstract": "learning models of artificial intelligence can nowadays perform very well on a large variety of tasks . however , in practice different task environments are best handled by different learning models , rather than a single , universal , approach . most non-trivial models thus require the adjustment of several to many learning parameters , which is often done on a case-by-case basis by an external party . meta-learning refers to the ability of an agent to autonomously and dynamically adjust its own learning parameters , or meta-parameters . in this work we show how projective simulation , a recently developed model of artificial intelligence , can naturally be extended to account for meta-learning in reinforcement learning settings . the projective simulation approach is based on a random walk process over a network of clips . the suggested meta-learning scheme builds upon the same design and employs clip networks to monitor the agent 's performance and to adjust its meta-parameters `` on the fly '' . we distinguish between `` reflexive adaptation '' and `` adaptation through learning '' , and show the utility of both approaches . in addition , a trade-off between flexibility and learning-time is addressed . the extended model is examined on three different kinds of reinforcement learning tasks , in which the agent has different optimal values of the meta-parameters , and is shown to perform well , reaching near-optimal to optimal success rates in all of them , without ever needing to manually adjust any meta-parameter ."}
{"title": "counterexample guided abstraction refinement algorithm for propositional circumscription", "abstract": "circumscription is a representative example of a nonmonotonic reasoning inference technique . circumscription has often been studied for first order theories , but its propositional version has also been the subject of extensive research , having been shown equivalent to extended closed world assumption ( ecwa ) . moreover , entailment in propositional circumscription is a well-known example of a decision problem in the second level of the polynomial hierarchy . this paper proposes a new boolean satisfiability ( sat ) -based algorithm for entailment in propositional circumscription that explores the relationship of propositional circumscription to minimal models . the new algorithm is inspired by ideas commonly used in sat-based model checking , namely counterexample guided abstraction refinement . in addition , the new algorithm is refined to compute the theory closure for generalized close world assumption ( gcwa ) . experimental results show that the new algorithm can solve problem instances that other solutions are unable to solve ."}
{"title": "populations can be essential in tracking dynamic optima", "abstract": "real-world optimisation problems are often dynamic . previously good solutions must be updated or replaced due to changes in objectives and constraints . it is often claimed that evolutionary algorithms are particularly suitable for dynamic optimisation because a large population can contain different solutions that may be useful in the future . however , rigorous theoretical demonstrations for how populations in dynamic optimisation can be essential are sparse and restricted to special cases . this paper provides theoretical explanations of how populations can be essential in evolutionary dynamic optimisation in a general and natural setting . we describe a natural class of dynamic optimisation problems where a sufficiently large population is necessary to keep track of moving optima reliably . we establish a relationship between the population-size and the probability that the algorithm loses track of the optimum ."}
{"title": "approximations from anywhere and general rough sets", "abstract": "not all approximations arise from information systems . the problem of fitting approximations , subjected to some rules ( and related data ) , to information systems in a rough scheme of things is known as the \\emph { inverse problem } . the inverse problem is more general than the duality ( or abstract representation ) problems and was introduced by the present author in her earlier papers . from the practical perspective , a few ( as opposed to one ) theoretical frameworks may be suitable for formulating the problem itself . \\emph { granular operator spaces } have been recently introduced and investigated by the present author in her recent work in the context of antichain based and dialectical semantics for general rough sets . the nature of the inverse problem is examined from number-theoretic and combinatorial perspectives in a higher order variant of granular operator spaces and some necessary conditions are proved . the results and the novel approach would be useful in a number of unsupervised and semi supervised learning contexts and algorithms ."}
{"title": "generalized value iteration networks : life beyond lattices", "abstract": "in this paper , we introduce a generalized value iteration network ( gvin ) , which is an end-to-end neural network planning module . gvin emulates the value iteration algorithm by using a novel graph convolution operator , which enables gvin to learn and plan on irregular spatial graphs . we propose three novel differentiable kernels as graph convolution operators and show that the embedding based kernel achieves the best performance . we further propose episodic q-learning , an improvement upon traditional n-step q-learning that stabilizes training for networks that contain a planning module . lastly , we evaluate gvin on planning problems in 2d mazes , irregular graphs , and real-world street networks , showing that gvin generalizes well for both arbitrary graphs and unseen graphs of larger scale and outperforms a naive generalization of vin ( discretizing a spatial graph into a 2d image ) ."}
{"title": "towards an intelligent tutor for mathematical proofs", "abstract": "computer-supported learning is an increasingly important form of study since it allows for independent learning and individualized instruction . in this paper , we discuss a novel approach to developing an intelligent tutoring system for teaching textbook-style mathematical proofs . we characterize the particularities of the domain and discuss common its design models . our approach is motivated by phenomena found in a corpus of tutorial dialogs that were collected in a wizard-of-oz experiment . we show how an intelligent tutor for textbook-style mathematical proofs can be built on top of an adapted assertion-level proof assistant by reusing representations and proof search strategies originally developed for automated and interactive theorem proving . the resulting prototype was successfully evaluated on a corpus of tutorial dialogs and yields good results ."}
{"title": "feature subset selection for software cost modelling and estimation", "abstract": "feature selection has been recently used in the area of software engineering for improving the accuracy and robustness of software cost models . the idea behind selecting the most informative subset of features from a pool of available cost drivers stems from the hypothesis that reducing the dimensionality of datasets will significantly minimise the complexity and time required to reach to an estimation using a particular modelling technique . this work investigates the appropriateness of attributes , obtained from empirical project databases and aims to reduce the cost drivers used while preserving performance . finding suitable subset selections that may cater improved predictions may be considered as a pre-processing step of a particular technique employed for cost estimation ( filter or wrapper ) or an internal ( embedded ) step to minimise the fitting error . this paper compares nine relatively popular feature selection methods and uses the empirical values of selected attributes recorded in the isbsg and desharnais datasets to estimate software development effort ."}
{"title": "emotional responses in artificial agent-based systems : reflexivity and adaptation in artificial life", "abstract": "the current work addresses a virtual environment with self-replicating agents whose decisions are based on a form of `` somatic computation '' ( soma - body ) in which basic emotional responses , taken in parallelism to actual living organisms , are introduced as a way to provide the agents with greater reflexive abilities . the work provides a contribution to the field of artificial intelligence ( ai ) and artificial life ( alife ) in connection to a neurobiology-based cognitive framework for artificial systems and virtual environments ' simulations . the performance of the agents capable of emotional responses is compared with that of self-replicating automata , and the implications of research on emotions and ai , in connection to both virtual agents as well as robots , is addressed regarding possible future directions and applications ."}
{"title": "inductive inference and the representation of uncertainty", "abstract": "the form and justification of inductive inference rules depend strongly on the representation of uncertainty . this paper examines one generic representation , namely , incomplete information . the notion can be formalized by presuming that the relevant probabilities in a decision problem are known only to the extent that they belong to a class k of probability distributions . the concept is a generalization of a frequent suggestion that uncertainty be represented by intervals or ranges on probabilities . to make the representation useful for decision making , an inductive rule can be formulated which determines , in a well-defined manner , a best approximation to the unknown probability , given the set k. in addition , the knowledge set notion entails a natural procedure for updating -- modifying the set k given new evidence . several non-intuitive consequences of updating emphasize the differences between inference with complete and inference with incomplete information ."}
{"title": "conceptual spaces for cognitive architectures : a lingua franca for different levels of representation", "abstract": "during the last decades , many cognitive architectures ( cas ) have been realized adopting different assumptions about the organization and the representation of their knowledge level . some of them ( e.g . soar [ laird ( 2012 ) ] ) adopt a classical symbolic approach , some ( e.g . leabra [ o'reilly and munakata ( 2000 ) ] ) are based on a purely connectionist model , while others ( e.g . clarion [ sun ( 2006 ) ] adopt a hybrid approach combining connectionist and symbolic representational levels . additionally , some attempts ( e.g . bisoar ) trying to extend the representational capacities of cas by integrating diagrammatical representations and reasoning are also available [ kurup and chandrasekaran ( 2007 ) ] . in this paper we propose a reflection on the role that conceptual spaces , a framework developed by peter g\u007f\\ '' ardenfors [ g\u007f\\ '' ardenfors ( 2000 ) ] more than fifteen years ago , can play in the current development of the knowledge level in cognitive systems and architectures . in particular , we claim that conceptual spaces offer a lingua franca that allows to unify and generalize many aspects of the symbolic , sub-symbolic and diagrammatic approaches ( by overcoming some of their typical problems ) and to integrate them on a common ground . in doing so we extend and detail some of the arguments explored by g\u007f\\ '' ardenfors [ g\u007f\\ '' ardenfors ( 1997 ) ] for defending the need of a conceptual , intermediate , representation level between the symbolic and the sub-symbolic one ."}
{"title": "the complexity of repairing , adjusting , and aggregating of extensions in abstract argumentation", "abstract": "we study the computational complexity of problems that arise in abstract argumentation in the context of dynamic argumentation , minimal change , and aggregation . in particular , we consider the following problems where always an argumentation framework f and a small positive integer k are given . - the repair problem asks whether a given set of arguments can be modified into an extension by at most k elementary changes ( i.e. , the extension is of distance k from the given set ) . - the adjust problem asks whether a given extension can be modified by at most k elementary changes into an extension that contains a specified argument . - the center problem asks whether , given two extensions of distance k , whether there is a `` center '' extension that is a distance at most ( k-1 ) from both given extensions . we study these problems in the framework of parameterized complexity , and take the distance k as the parameter . our results covers several different semantics , including admissible , complete , preferred , semi-stable and stable semantics ."}
{"title": "a scalable method for solving high-dimensional continuous pomdps using local approximation", "abstract": "partially-observable markov decision processes ( pomdps ) are typically solved by finding an approximate global solution to a corresponding belief-mdp . in this paper , we offer a new planning algorithm for pomdps with continuous state , action and observation spaces . since such domains have an inherent notion of locality , we can find an approximate solution using local optimization methods . we parameterize the belief distribution as a gaussian mixture , and use the extended kalman filter ( ekf ) to approximate the belief update . since the ekf is a first-order filter , we can marginalize over the observations analytically . by using feedback control and state estimation during policy execution , we recover a behavior that is effectively conditioned on incoming observations despite the unconditioned planning . local optimization provides no guarantees of global optimality , but it allows us to tackle domains that are at least an order of magnitude larger than the current state-of-the-art . we demonstrate the scalability of our algorithm by considering a simulated hand-eye coordination domain with 16 continuous state dimensions and 6 continuous action dimensions ."}
{"title": "formal verification of piece-wise linear feed-forward neural networks", "abstract": "we present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function . such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory ( smt ) and integer linear programming ( ilp ) solvers . the starting point of our approach is the addition of a global linear approximation of the overall network behavior to the verification problem that helps with smt-like reasoning over the network behavior . we present a specialized verification algorithm that employs this approximation in a search process in which it infers additional node phases for the non-linear nodes in the network from partial node phase assignments , similar to unit propagation in classical sat solving . we also show how to infer additional conflict clauses and safe node fixtures from the results of the analysis steps performed during the search . the resulting approach is evaluated on collision avoidance and handwritten digit recognition case studies ."}
{"title": "eliminating unfounded set checking for hex-programs", "abstract": "hex-programs are an extension of the answer set programming ( asp ) paradigm incorporating external means of computation into the declarative programming language through so-called external atoms . their semantics is defined in terms of minimal models of the faber-leone-pfeifer ( flp ) reduct . developing native solvers for hex-programs based on an appropriate notion of unfounded sets has been subject to recent research for reasons of efficiency . although this has lead to an improvement over naive minimality checking using the flp reduct , testing for foundedness remains a computationally expensive task . in this work we improve on hex-program evaluation in this respect by identifying a syntactic class of programs , that can be efficiently recognized and allows to entirely skip the foundedness check . moreover , we develop criteria for decomposing a program into components , such that the search for unfounded sets can be restricted . observing that our results apply to many hex-program applications provides analytic evidence for the significance and effectiveness of our approach , which is complemented by a brief discussion of preliminary experimental validation ."}
{"title": "deliberation and its role in the formation of intentions", "abstract": "deliberation plays an important role in the design of rational agents embedded in the real-world . in particular , deliberation leads to the formation of intentions , i.e. , plans of action that the agent is committed to achieving . in this paper , we present a branching time possible-worlds model for representing and reasoning about , beliefs , goals , intentions , time , actions , probabilities , and payoffs . we compare this possible-worlds approach with the more traditional decision tree representation and provide a transformation from decision trees to possible worlds . finally , we illustrate how an agent can perform deliberation using a decision-tree representation and then use a possible-worlds model to form and reason about his intentions ."}
{"title": "detecting qualia in natural and artificial agents", "abstract": "the hard problem of consciousness has been dismissed as an illusion . by showing that computers are capable of experiencing , we show that they are at least rudimentarily conscious with potential to eventually reach superconsciousness . the main contribution of the paper is a test for confirming certain subjective experiences in a tested agent . we follow with analysis of benefits and problems with conscious machines and implications of such capability on future of computing , machine rights and artificial intelligence safety ."}
{"title": "a probabilistic methodology for multilabel classification", "abstract": "multilabel classification is a relatively recent subfield of machine learning . unlike to the classical approach , where instances are labeled with only one category , in multilabel classification , an arbitrary number of categories is chosen to label an instance . due to the problem complexity ( the solution is one among an exponential number of alternatives ) , a very common solution ( the binary method ) is frequently used , learning a binary classifier for every category , and combining them all afterwards . the assumption taken in this solution is not realistic , and in this work we give examples where the decisions for all the labels are not taken independently , and thus , a supervised approach should learn those existing relationships among categories to make a better classification . therefore , we show here a generic methodology that can improve the results obtained by a set of independent probabilistic binary classifiers , by using a combination procedure with a classifier trained on the co-occurrences of the labels . we show an exhaustive experimentation in three different standard corpora of labeled documents ( reuters-21578 , ohsumed-23 and rcv1 ) , which present noticeable improvements in all of them , when using our methodology , in three probabilistic base classifiers ."}
{"title": "local-search techniques for propositional logic extended with cardinality constraints", "abstract": "we study local-search satisfiability solvers for propositional logic extended with cardinality atoms , that is , expressions that provide explicit ways to model constraints on cardinalities of sets . adding cardinality atoms to the language of propositional logic facilitates modeling search problems and often results in concise encodings . we propose two `` native '' local-search solvers for theories in the extended language . we also describe techniques to reduce the problem to standard propositional satisfiability and allow us to use off-the-shelf sat solvers . we study these methods experimentally . our general finding is that native solvers designed specifically for the extended language perform better than indirect methods relying on sat solvers ."}
{"title": "informed heuristics for guiding stem-and-cycle ejection chains", "abstract": "the state of the art in local search for the traveling salesman problem is dominated by ejection chain methods utilising the stem-and-cycle reference structure . though effective such algorithms employ very little information in their successor selection strategy , typically seeking only to minimise the cost of a move . we propose an alternative approach inspired from the ai literature and show how an admissible heuristic can be used to guide successor selection . we undertake an empirical analysis and demonstrate that this technique often produces better results than less informed strategies albeit at the cost of running in higher polynomial time ."}
{"title": "machine generalization and human categorization : an information-theoretic view", "abstract": "in designing an intelligent system that must be able to explain its reasoning to a human user , or to provide generalizations that the human user finds reasonable , it may be useful to take into consideration psychological data on what types of concepts and categories people naturally use . the psychological literature on concept learning and categorization provides strong evidence that certain categories are more easily learned , recalled , and recognized than others . we show here how a measure of the informational value of a category predicts the results of several important categorization experiments better than standard alternative explanations . this suggests that information-based approaches to machine generalization may prove particularly useful and natural for human users of the systems ."}
{"title": "shaping influence and influencing shaping : a computational red teaming trust-based swarm intelligence model", "abstract": "sociotechnical systems are complex systems , where nonlinear interaction among different players can obscure causal relationships . the absence of mechanisms to help us understand how to create a change in the system makes it hard to manage these systems . influencing and shaping are social operators acting on sociotechnical systems to design a change . however , the two operators are usually discussed in an ad-hoc manner , without proper guiding models and metrics which assist in adopting these models successfully . moreover , both social operators rely on accurate understanding of the concept of trust . without such understanding , neither of these operators can create the required level to create a change in a desirable direction . in this paper , we define these concepts in a concise manner suitable for modelling the concepts and understanding their dynamics . we then introduce a model for influencing and shaping and use computational red teaming principles to design and demonstrate how this model operates . we validate the results computationally through a simulation environment to show social influencing and shaping in an artificial society ."}
{"title": "deep reinforcement learning for multi-domain dialogue systems", "abstract": "standard deep reinforcement learning methods such as deep q-networks ( dqn ) for multiple tasks ( domains ) face scalability problems . we propose a method for multi-domain dialogue policy learning -- -termed ndqn , and apply it to an information-seeking spoken dialogue system in the domains of restaurants and hotels . experimental results comparing dqn ( baseline ) versus ndqn ( proposed ) using simulations report that our proposed method exhibits better scalability and is promising for optimising the behaviour of multi-domain dialogue systems ."}
{"title": "from first-order logic to assertional logic", "abstract": "first-order logic ( fol ) is widely regarded as one of the most important foundations for knowledge representation . nevertheless , in this paper , we argue that fol has several critical issues for this purpose . instead , we propose an alternative called assertional logic , in which all syntactic objects are categorized as set theoretic constructs including individuals , concepts and operators , and all kinds of knowledge are formalized by equality assertions . we first present a primitive form of assertional logic that uses minimal assumed knowledge and constructs . then , we show how to extend it by definitions , which are special kinds of knowledge , i.e. , assertions . we argue that assertional logic , although simpler , is more expressive and extensible than fol . as a case study , we show how assertional logic can be used to unify logic and probability , and more building blocks in ai ."}
{"title": "toward an automaton constraint for local search", "abstract": "we explore the idea of using finite automata to implement new constraints for local search ( this is already a successful technique in constraint-based global search ) . we show how it is possible to maintain incrementally the violations of a constraint and its decision variables from an automaton that describes a ground checker for that constraint . we establish the practicality of our approach idea on real-life personnel rostering problems , and show that it is competitive with the approach of [ pralong , 2007 ] ."}
{"title": "on the robustness of most probable explanations", "abstract": "in bayesian networks , a most probable explanation ( mpe ) is a complete variable instantiation with a highest probability given the current evidence . in this paper , we discuss the problem of finding robustness conditions of the mpe under single parameter changes . specifically , we ask the question : how much change in a single network parameter can we afford to apply while keeping the mpe unchanged ? we will describe a procedure , which is the first of its kind , that computes this answer for each parameter in the bayesian network variable in time o ( n exp ( w ) ) , where n is the number of network variables and w is its treewidth ."}
{"title": "practical uses of belief functions", "abstract": "we present examples where the use of belief functions provided sound and elegant solutions to real life problems . these are essentially characterized by ? missing ' information . the examples deal with 1 ) discriminant analysis using a learning set where classes are only partially known ; 2 ) an information retrieval systems handling inter-documents relationships ; 3 ) the combination of data from sensors competent on partially overlapping frames ; 4 ) the determination of the number of sources in a multi-sensor environment by studying the inter-sensors contradiction . the purpose of the paper is to report on such applications where the use of belief functions provides a convenient tool to handle ? messy ' data problems ."}
{"title": "anytime coalition structure generation with worst case guarantees", "abstract": "coalition formation is a key topic in multiagent systems . one would prefer a coalition structure that maximizes the sum of the values of the coalitions , but often the number of coalition structures is too large to allow exhaustive search for the optimal one . but then , can the coalition structure found via a partial search be guaranteed to be within a bound from optimum ? we show that none of the previous coalition structure generation algorithms can establish any bound because they search fewer nodes than a threshold that we show necessary for establishing a bound . we present an algorithm that establishes a tight bound within this minimal amount of search , and show that any other algorithm would have to search strictly more . the fraction of nodes needed to be searched approaches zero as the number of agents grows . if additional time remains , our anytime algorithm searches further , and establishes a progressively lower tight bound . surprisingly , just searching one more node drops the bound in half . as desired , our algorithm lowers the bound rapidly early on , and exhibits diminishing returns to computation . it also drastically outperforms its obvious contenders . finally , we show how to distribute the desired search across self-interested manipulative agents ."}
{"title": "anomaly detection in xml-structured soap messages using tree-based association rule mining", "abstract": "web services are software systems designed for supporting interoperable dynamic cross-enterprise interactions . the result of attacks to web services can be catastrophic and causing the disclosure of enterprises ' confidential data . as new approaches of attacking arise every day , anomaly detection systems seem to be invaluable tools in this context . the aim of this work has been to target the attacks that reside in the web service layer and the extensible markup language ( xml ) -structured simple object access protocol ( soap ) messages . after studying the shortcomings of the existing solutions , a new approach for detecting anomalies in web services is outlined . more specifically , the proposed technique illustrates how to identify anomalies by employing mining methods on xml-structured soap messages . this technique also takes the advantages of tree-based association rule mining to extract knowledge in the training phase , which is used in the test phase to detect anomalies . in addition , this novel composition of techniques brings nearly low false alarm rate while maintaining the detection rate reasonably high , which is shown by a case study ."}
{"title": "recoverability of joint distribution from missing data", "abstract": "a probabilistic query may not be estimable from observed data corrupted by missing values if the data are not missing at random ( mar ) . it is therefore of theoretical interest and practical importance to determine in principle whether a probabilistic query is estimable from missing data or not when the data are not mar . we present an algorithm that systematically determines whether the joint probability is estimable from observed data with missing values , assuming that the data-generation model is represented as a bayesian network containing unobserved latent variables that not only encodes the dependencies among the variables but also explicitly portrays the mechanisms responsible for the missingness process . the result significantly advances the existing work ."}
{"title": "taxonomy induction using hypernym subsequences", "abstract": "we propose a novel , semi-supervised approach towards domain taxonomy induction from an input vocabulary of seed terms . unlike all previous approaches , which typically extract direct hypernym edges for terms , our approach utilizes a novel probabilistic framework to extract hypernym subsequences . taxonomy induction from extracted subsequences is cast as an instance of the minimumcost flow problem on a carefully designed directed graph . through experiments , we demonstrate that our approach outperforms stateof- the-art taxonomy induction approaches across four languages . importantly , we also show that our approach is robust to the presence of noise in the input vocabulary . to the best of our knowledge , no previous approaches have been empirically proven to manifest noise-robustness in the input vocabulary ."}
{"title": "learning to order bdd variables in verification", "abstract": "the size and complexity of software and hardware systems have significantly increased in the past years . as a result , it is harder to guarantee their correct behavior . one of the most successful methods for automated verification of finite-state systems is model checking . most of the current model-checking systems use binary decision diagrams ( bdds ) for the representation of the tested model and in the verification process of its properties . generally , bdds allow a canonical compact representation of a boolean function ( given an order of its variables ) . the more compact the bdd is , the better performance one gets from the verifier . however , finding an optimal order for a bdd is an np-complete problem . therefore , several heuristic methods based on expert knowledge have been developed for variable ordering . we propose an alternative approach in which the variable ordering algorithm gains 'ordering experience ' from training models and uses the learned knowledge for finding good orders . our methodology is based on offline learning of pair precedence classifiers from training models , that is , learning which variable pair permutation is more likely to lead to a good order . for each training model , a number of training sequences are evaluated . every training model variable pair permutation is then tagged based on its performance on the evaluated orders . the tagged permutations are then passed through a feature extractor and are given as examples to a classifier creation algorithm . given a model for which an order is requested , the ordering algorithm consults each precedence classifier and constructs a pair precedence table which is used to create the order . our algorithm was integrated with smv , which is one of the most widely used verification systems . preliminary empirical evaluation of our methodology , using real benchmark models , shows performance that is better than random ordering and is competitive with existing algorithms that use expert knowledge . we believe that in sub-domains of models ( alu , caches , etc . ) our system will prove even more valuable . this is because it features the ability to learn sub-domain knowledge , something that no other ordering algorithm does ."}
{"title": "polymorphic self-* agents for stigmergic fault mitigation in large-scale real-time embedded systems", "abstract": "organization and coordination of agents within large-scale , complex , distributed environments is one of the primary challenges in the field of multi-agent systems . a lot of interest has surfaced recently around self-* ( self-organizing , self-managing , self-optimizing , self-protecting ) agents . this paper presents polymorphic self-* agents that evolve a core set of roles and behavior based on environmental cues . the agents adapt these roles based on the changing demands of the environment , and are directly implementable in computer systems applications . the design combines strategies from game theory , stigmergy , and other biologically inspired models to address fault mitigation in large-scale , real-time , distributed systems . the agents are embedded within the individual digital signal processors of btev , a high energy physics experiment consisting of 2500 such processors . results obtained using a swarm simulation of the btev environment demonstrate the polymorphic character of the agents , and show how this design exceeds performance and reliability metrics obtained from comparable centralized , and even traditional decentralized approaches ."}
{"title": "parameterized complexity results for exact bayesian network structure learning", "abstract": "bayesian network structure learning is the notoriously difficult problem of discovering a bayesian network that optimally represents a given set of training data . in this paper we study the computational worst-case complexity of exact bayesian network structure learning under graph theoretic restrictions on the ( directed ) super-structure . the super-structure is an undirected graph that contains as subgraphs the skeletons of solution networks . we introduce the directed super-structure as a natural generalization of its undirected counterpart . our results apply to several variants of score-based bayesian network structure learning where the score of a network decomposes into local scores of its nodes . results : we show that exact bayesian network structure learning can be carried out in non-uniform polynomial time if the super-structure has bounded treewidth , and in linear time if in addition the super-structure has bounded maximum degree . furthermore , we show that if the directed super-structure is acyclic , then exact bayesian network structure learning can be carried out in quadratic time . we complement these positive results with a number of hardness results . we show that both restrictions ( treewidth and degree ) are essential and can not be dropped without loosing uniform polynomial time tractability ( subject to a complexity-theoretic assumption ) . similarly , exact bayesian network structure learning remains np-hard for `` almost acyclic '' directed super-structures . furthermore , we show that the restrictions remain essential if we do not search for a globally optimal network but aim to improve a given network by means of at most k arc additions , arc deletions , or arc reversals ( k-neighborhood local search ) ."}
{"title": "slice sampling for probabilistic programming", "abstract": "we introduce the first , general purpose , slice sampling inference engine for probabilistic programs . this engine is released as part of stocpy , a new turing-complete probabilistic programming language , available as a python library . we present a transdimensional generalisation of slice sampling which is necessary for the inference engine to work on traces with different numbers of random variables . we show that stocpy compares favourably to other ppls in terms of flexibility and usability , and that slice sampling can outperform previously introduced inference methods . our experiments include a logistic regression , hmm , and bayesian neural net ."}
{"title": "measuring relations between concepts in conceptual spaces", "abstract": "the highly influential framework of conceptual spaces provides a geometric way of representing knowledge . instances are represented by points in a high-dimensional space and concepts are represented by regions in this space . our recent mathematical formalization of this framework is capable of representing correlations between different domains in a geometric way . in this paper , we extend our formalization by providing quantitative mathematical definitions for the notions of concept size , subsethood , implication , similarity , and betweenness . this considerably increases the representational power of our formalization by introducing measurable ways of describing relations between concepts ."}
{"title": "learning to translate for multilingual question answering", "abstract": "in multilingual question answering , either the question needs to be translated into the document language , or vice versa . in addition to direction , there are multiple methods to perform the translation , four of which we explore in this paper : word-based , 10-best , context-based , and grammar-based . we build a feature for each combination of translation direction and method , and train a model that learns optimal feature weights . on a large forum dataset consisting of posts in english , arabic , and chinese , our novel learn-to-translate approach was more effective than a strong baseline ( p < 0.05 ) : translating all text into english , then training a classifier based only on english ( original or translated ) text ."}
{"title": "knowledge base approach for 3d objects detection in point clouds using 3d processing and specialists knowledge", "abstract": "this paper presents a knowledge-based detection of objects approach using the owl ontology language , the semantic web rule language , and 3d processing built-ins aiming at combining geometrical analysis of 3d point clouds and specialist 's knowledge . here , we share our experience regarding the creation of 3d semantic facility model out of unorganized 3d point clouds . thus , a knowledge-based detection approach of objects using the owl ontology language is presented . this knowledge is used to define swrl detection rules . in addition , the combination of 3d processing built-ins and topological built-ins in swrl rules allows a more flexible and intelligent detection , and the annotation of objects contained in 3d point clouds . the created widop prototype takes a set of 3d point clouds as input , and produces as output a populated ontology corresponding to an indexed scene visualized within vrml language . the context of the study is the detection of railway objects materialized within the deutsche bahn scene such as signals , technical cupboards , electric poles , etc . thus , the resulting enriched and populated ontology , that contains the annotations of objects in the point clouds , is used to feed a gis system or an ifc file for architecture purposes ."}
{"title": "tableaux for policy synthesis for mdps with pctl* constraints", "abstract": "markov decision processes ( mdps ) are the standard formalism for modelling sequential decision making in stochastic environments . policy synthesis addresses the problem of how to control or limit the decisions an agent makes so that a given specification is met . in this paper we consider pctl* , the probabilistic counterpart of ctl* , as the specification language . because in general the policy synthesis problem for pctl* is undecidable , we restrict to policies whose execution history memory is finitely bounded a priori . surprisingly , no algorithm for policy synthesis for this natural and expressive framework has been developed so far . we close this gap and describe a tableau-based algorithm that , given an mdp and a pctl* specification , derives in a non-deterministic way a system of ( possibly nonlinear ) equalities and inequalities . the solutions of this system , if any , describe the desired ( stochastic ) policies . our main result in this paper is the correctness of our method , i.e. , soundness , completeness and termination ."}
{"title": "planning in pomdps using multiplicity automata", "abstract": "planning and learning in partially observable mdps ( pomdps ) are among the most challenging tasks in both the ai and operation research communities . although solutions to these problems are intractable in general , there might be special cases , such as structured pomdps , which can be solved efficiently . a natural and possibly efficient way to represent a pomdp is through the predictive state representation ( psr ) - a representation which recently has been receiving increasing attention . in this work , we relate pomdps to multiplicity automata- showing that pomdps can be represented by multiplicity automata with no increase in the representation size . furthermore , we show that the size of the multiplicity automaton is equal to the rank of the predictive state representation . therefore , we relate both the predictive state representation and pomdps to the well-founded multiplicity automata literature . based on the multiplicity automata representation , we provide a planning algorithm which is exponential only in the multiplicity automata rank rather than the number of states of the pomdp . as a result , whenever the predictive state representation is logarithmic in the standard pomdp representation , our planning algorithm is efficient ."}
{"title": "less is more - genetic optimisation of nearest neighbour classifiers", "abstract": "the present paper deals with optimisation of nearest neighbour rule classifiers via genetic algorithms . the methodology consists on implement a genetic algorithm capable of search the input feature space used by the nnr classifier . results show that is adequate to perform feature reduction and simultaneous improve the recognition rate . some practical examples prove that is possible to recognise portuguese granites in 100 % , with only 3 morphological features ( from an original set of 117 features ) , which is well suited for real time applications . moreover , the present method represents a robust strategy to understand the proper nature of the images treated , and their discriminant features . keywords : feature reduction , genetic algorithms , nearest neighbour rule classifiers ( k-nnr ) ."}
{"title": "a note on nesting in dyadic deontic logic", "abstract": "the paper reports on some results concerning aqvist 's dyadic logic known as system g , which is one of the most influential logics for reasoning with dyadic obligations ( `` it ought to be the case that ... if it is the case that ... '' ) . although this logic has been known in the literature for a while , many of its properties still await in-depth consideration . in this short paper we show : that any formula in system g including nested modal operators is equivalent to some formula with no nesting ; that the universal modality introduced by aqvist in the first presentation of the system is definable in terms of the deontic modality ."}
{"title": "avoiding the bloat with stochastic grammar-based genetic programming", "abstract": "the application of genetic programming to the discovery of empirical laws is often impaired by the huge size of the search space , and consequently by the computer resources needed . in many cases , the extreme demand for memory and cpu is due to the massive growth of non-coding segments , the introns . the paper presents a new program evolution framework which combines distribution-based evolution in the pbil spirit , with grammar-based genetic programming ; the information is stored as a probability distribution on the gra mmar rules , rather than in a population . experiments on a real-world like problem show that this approach gives a practical solution to the problem of intron growth ."}
{"title": "on the high-dimensional power of linear-time kernel two-sample testing under mean-difference alternatives", "abstract": "nonparametric two sample testing deals with the question of consistently deciding if two distributions are different , given samples from both , without making any parametric assumptions about the form of the distributions . the current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ ( \\textit { general } alternatives ) , and those which are designed to specifically test easier alternatives , like a difference in means ( \\textit { mean-shift } alternatives ) . the main contribution of this paper is to explicitly characterize the power of a popular nonparametric two sample test , designed for general alternatives , under a mean-shift alternative in the high-dimensional setting . specifically , we explicitly derive the power of the linear-time maximum mean discrepancy statistic using the gaussian kernel , where the dimension and sample size can both tend to infinity at any rate , and the two distributions differ in their means . as a corollary , we find that if the signal-to-noise ratio is held constant , then the test 's power goes to one if the number of samples increases faster than the dimension increases . this is the first explicit power derivation for a general nonparametric test in the high-dimensional setting , and also the first analysis of how tests designed for general alternatives perform when faced with easier ones ."}
{"title": "artificial intelligence techniques for steam generator modelling", "abstract": "this paper investigates the use of different artificial intelligence methods to predict the values of several continuous variables from a steam generator . the objective was to determine how the different artificial intelligence methods performed in making predictions on the given dataset . the artificial intelligence methods evaluated were neural networks , support vector machines , and adaptive neuro-fuzzy inference systems . the types of neural networks investigated were multi-layer perceptions , and radial basis function . bayesian and committee techniques were applied to these neural networks . each of the ai methods considered was simulated in matlab . the results of the simulations showed that all the ai methods were capable of predicting the steam generator data reasonably accurately . however , the adaptive neuro-fuzzy inference system out performed the other methods in terms of accuracy and ease of implementation , while still achieving a fast execution time as well as a reasonable training time ."}
{"title": "some complexity considerations in the combination of belief networks", "abstract": "one topic that is likely to attract an increasing amount of attention within the knowledge-base systems research community is the coordination of information provided by multiple experts . we envision a situation in which several experts independently encode information as belief networks . a potential user must then coordinate the conclusions and recommendations of these networks to derive some sort of consensus . one approach to such a consensus is the fusion of the contributed networks into a single , consensus model prior to the consideration of any case-specific data ( specific observations , test results ) . this approach requires two types of combination procedures , one for probabilities , and one for graphs . since the combination of probabilities is relatively well understood , the key barriers to this approach lie in the realm of graph theory . this paper provides formal definitions of some of the operations necessary to effect the necessary graphical combinations , and provides complexity analyses of these procedures . the paper 's key result is that most of these operations are np-hard , and its primary message is that the derivation of ? good ? consensus networks must be done heuristically ."}
{"title": "ggp with advanced reasoning and board knowledge discovery", "abstract": "quality of general game playing ( ggp ) matches suffers from slow state-switching and weak knowledge modules . instantiation and propositional networks offer great performance gains over prolog-based reasoning , but do not scale well . in this publication mgdl , a variant of gdl stripped of function constants , has been defined as a basis for simple reasoning machines . mgdl allows to easily map rules to c++ functions . 253 out of 270 tested gdl rule sheets conformed to mgdl without any modifications ; the rest required minor changes . a revised ( m ) gdl to c++ translation scheme has been reevaluated ; it brought gains ranging from 28 % to 7300 % over yap prolog , managing to compile even demanding rule sheets under few seconds . for strengthening game knowledge , spatial features inspired by similar successful techniques from computer go have been proposed . for they required an euclidean metric , a small board extension to gdl has been defined through a set of ground atomic sentences . an sga-based genetic algorithm has been designed for tweaking game parameters and conducting self-plays , so the features could be mined from meaningful game records . the approach has been tested on a small cluster , giving performance gains up to 20 % more wins against the baseline uct player . implementations of proposed ideas constitutes the core of ggp spatium - a small c++/python ggp framework , created for developing compact ggp players and problem solvers ."}
{"title": "essentially no barriers in neural network energy landscape", "abstract": "training neural networks involves finding minima of a high-dimensional non-convex loss function . knowledge of the structure of this energy landscape is sparse . relaxing from linear interpolations , we construct continuous paths between minima of recent neural network architectures on cifar10 and cifar100 . surprisingly , the paths are essentially flat in both the training and test landscapes . this implies that neural networks have enough capacity for structural changes , or that these changes are small between minima . also , each minimum has at least one vanishing hessian eigenvalue in addition to those resulting from trivial invariance ."}
{"title": "generating redundant features with unsupervised multi-tree genetic programming", "abstract": "recently , feature selection has become an increasingly important area of research due to the surge in high-dimensional datasets in all areas of modern life . a plethora of feature selection algorithms have been proposed , but it is difficult to truly analyse the quality of a given algorithm . ideally , an algorithm would be evaluated by measuring how well it removes known bad features . acquiring datasets with such features is inherently difficult , and so a common technique is to add synthetic bad features to an existing dataset . while adding noisy features is an easy task , it is very difficult to automatically add complex , redundant features . this work proposes one of the first approaches to generating redundant features , using a novel genetic programming approach . initial experiments show that our proposed method can automatically create difficult , redundant features which have the potential to be used for creating high-quality feature selection benchmark datasets ."}
{"title": "aixijs : a software demo for general reinforcement learning", "abstract": "reinforcement learning is a general and powerful framework with which to study and implement artificial intelligence . recent advances in deep learning have enabled rl algorithms to achieve impressive performance in restricted domains such as playing atari video games ( mnih et al. , 2015 ) and , recently , the board game go ( silver et al. , 2016 ) . however , we are still far from constructing a generally intelligent agent . many of the obstacles and open questions are conceptual : what does it mean to be intelligent ? how does one explore and learn optimally in general , unknown environments ? what , in fact , does it mean to be optimal in the general sense ? the universal bayesian agent aixi ( hutter , 2005 ) is a model of a maximally intelligent agent , and plays a central role in the sub-field of general reinforcement learning ( grl ) . recently , aixi has been shown to be flawed in important ways ; it does n't explore enough to be asymptotically optimal ( orseau , 2010 ) , and it can perform poorly with certain priors ( leike and hutter , 2015 ) . several variants of aixi have been proposed to attempt to address these shortfalls : among them are entropy-seeking agents ( orseau , 2011 ) , knowledge-seeking agents ( orseau et al. , 2013 ) , bayes with bursts of exploration ( lattimore , 2013 ) , mdl agents ( leike , 2016a ) , thompson sampling ( leike et al. , 2016 ) , and optimism ( sunehag and hutter , 2015 ) . we present aixijs , a javascript implementation of these grl agents . this implementation is accompanied by a framework for running experiments against various environments , similar to openai gym ( brockman et al. , 2016 ) , and a suite of interactive demos that explore different properties of the agents , similar to reinforcejs ( karpathy , 2015 ) . we use aixijs to present numerous experiments illustrating fundamental properties of , and differences between , these agents ."}
{"title": "boosting variational inference : an optimization perspective", "abstract": "variational inference is a popular technique to approximate a possibly intractable bayesian posterior with a more tractable one . recently , boosting variational inference has been proposed as a new paradigm to approximate the posterior by a mixture of densities by greedily adding components to the mixture . in the present work , we study the convergence properties of this approach from a modern optimization viewpoint by establishing connections to the classic frank-wolfe algorithm . our analyses yields novel theoretical insights on the boosting of variational inference regarding the sufficient conditions for convergence , explicit sublinear/linear rates , and algorithmic simplifications ."}
{"title": "computing preferred extensions in abstract argumentation : a sat-based approach", "abstract": "this paper presents a novel sat-based approach for the computation of extensions in abstract argumentation , with focus on preferred semantics , and an empirical evaluation of its performances . the approach is based on the idea of reducing the problem of computing complete extensions to a sat problem and then using a depth-first search method to derive preferred extensions . the proposed approach has been tested using two distinct sat solvers and compared with three state-of-the-art systems for preferred extension computation . it turns out that the proposed approach delivers significantly better performances in the large majority of the considered cases ."}
{"title": "sentiment analysis in scholarly book reviews", "abstract": "so far different studies have tackled the sentiment analysis in several domains such as restaurant and movie reviews . but , this problem has not been studied in scholarly book reviews which is different in terms of review style and size . in this paper , we propose to combine different features in order to be presented to a supervised classifiers which extract the opinion target expressions and detect their polarities in scholarly book reviews . we construct a labeled corpus for training and evaluating our methods in french book reviews . we also evaluate them on english restaurant reviews in order to measure their robustness across the domains and languages . the evaluation shows that our methods are enough robust for english restaurant reviews and french book reviews ."}
{"title": "a study on the relation between linguistics-oriented and domain-specific semantics", "abstract": "in this paper we dealt with the comparison and linking between lexical resources with domain knowledge provided by ontologies . it is one of the issues for the combination of the semantic web ontologies and text mining . we investigated the relations between the linguistics oriented and domain-specific semantics , by associating the go biological process concepts to the framenet semantic frames . the result shows the gaps between the linguistics-oriented and domain-specific semantics on the classification of events and the grouping of target words . the result provides valuable information for the improvement of domain ontologies supporting for text mining systems . and also , it will result in benefits to language understanding technology ."}
{"title": "clusters of driving behavior from observational smartphone data", "abstract": "understanding driving behaviors is essential for improving safety and mobility of our transportation systems . data is usually collected via simulator-based studies or naturalistic driving studies . those techniques allow for understanding relations between demographics , road conditions and safety . on the other hand , they are very costly and time consuming . thanks to the ubiquity of smartphones , we have an opportunity to substantially complement more traditional data collection techniques with data extracted from phone sensors , such as gps , accelerometer gyroscope and camera . we developed statistical models that provided insight into driver behavior in the san francisco metro area based on tens of thousands of driver logs . we used novel data sources to support our work . we used cell phone sensor data drawn from five hundred drivers in san francisco to understand the speed of traffic across the city as well as the maneuvers of drivers in different areas . specifically , we clustered drivers based on their driving behavior . we looked at driver norms by street and flagged driving behaviors that deviated from the norm ."}
{"title": "cached sufficient statistics for efficient machine learning with large datasets", "abstract": "this paper introduces new algorithms and data structures for quick counting for machine learning datasets . we focus on the counting task of constructing contingency tables , but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries . subject to certain assumptions , the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table . we provide a very sparse data structure , the adtree , to minimize memory use . we provide analytical worst-case bounds for this structure for several models of data distribution . we empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by ( a ) using a sparse tree structure that never allocates memory for counts of zero , ( b ) never allocating memory for counts that can be deduced from other counts , and ( c ) not bothering to expand the tree fully near its leaves . we show how the adtree can be used to accelerate bayes net structure finding algorithms , rule learning algorithms , and feature selection algorithms , and we provide a number of empirical results comparing adtree methods against traditional direct counting approaches . we also discuss the possible uses of adtrees in other machine learning methods , and discuss the merits of adtrees in comparison with alternative representations such as kd-trees , r-trees and frequent sets ."}
{"title": "variational inference for policy gradient", "abstract": "inspired by the seminal work on stein variational inference and stein variational policy gradient , we derived a method to generate samples from the posterior variational parameter distribution by \\textit { explicitly } minimizing the kl divergence to match the target distribution in an amortize fashion . consequently , we applied this varational inference technique into vanilla policy gradient , trpo and ppo with bayesian neural network parameterizations for reinforcement learning problems ."}
{"title": "towards detection of bottlenecks in modular systems", "abstract": "the paper describes some basic approaches to detection of bottlenecks in composite ( modular ) systems . the following basic system bottlenecks detection problems are examined : ( 1 ) traditional quality management approaches ( pareto chart based method , multicriteria analysis as selection of pareto-efficient points , and/or multicriteria ranking ) , ( 2 ) selection of critical system elements ( critical components/modules , critical component interconnection ) , ( 3 ) selection of interconnected system components as composite system faults ( via clique-based fusion ) , ( 4 ) critical elements ( e.g. , nodes ) in networks , and ( 5 ) predictive detection of system bottlenecks ( detection of system components based on forecasting of their parameters ) . here , heuristic solving schemes are used . numerical examples illustrate the approaches ."}
{"title": "self-correcting models for model-based reinforcement learning", "abstract": "when an agent can not represent a perfectly accurate model of its environment 's dynamics , model-based reinforcement learning ( mbrl ) can fail catastrophically . planning involves composing the predictions of the model ; when flawed predictions are composed , even minor errors can compound and render the model useless for planning . hallucinated replay ( talvitie 2014 ) trains the model to `` correct '' itself when it produces errors , substantially improving mbrl with flawed models . this paper theoretically analyzes this approach , illuminates settings in which it is likely to be effective or ineffective , and presents a novel error bound , showing that a model 's ability to self-correct is more tightly related to mbrl performance than one-step prediction error . these results inspire an mbrl algorithm for deterministic mdps with performance guarantees that are robust to model class limitations ."}
{"title": "the partner units configuration problem : completing the picture", "abstract": "the partner units problem ( pup ) is an acknowledged hard benchmark problem for the logic programming community with various industrial application fields like surveillance , electrical engineering , computer networks or railway safety systems . however , computational complexity remained widely unclear so far . in this paper we provide all missing complexity results making the pup better exploitable for benchmark testing . furthermore , we present quickpup , a heuristic search algorithm for pup instances which outperforms all state-of-the-art solving approaches and which is already in use in real world industrial configuration environments ."}
{"title": "identifying and harnessing the building blocks of machine learning pipelines for sensible initialization of a data science automation tool", "abstract": "as data science continues to grow in popularity , there will be an increasing need to make data science tools more scalable , flexible , and accessible . in particular , automated machine learning ( automl ) systems seek to automate the process of designing and optimizing machine learning pipelines . in this chapter , we present a genetic programming-based automl system called tpot that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification problem . further , we analyze a large database of pipelines that were previously used to solve various supervised classification problems and identify 100 short series of machine learning operations that appear the most frequently , which we call the building blocks of machine learning pipelines . we harness these building blocks to initialize tpot with promising solutions , and find that this sensible initialization method significantly improves tpot 's performance on one benchmark at no cost of significantly degrading performance on the others . thus , sensible initialization with machine learning pipeline building blocks shows promise for gp-based automl systems , and should be further refined in future work ."}
{"title": "defaults and normality in causal structures", "abstract": "a serious defect with the halpern-pearl ( hp ) definition of causality is repaired by combining a theory of causality with a theory of defaults . in addition , it is shown that ( despite a claim to the contrary ) a cause according to the hp condition need not be a single conjunct . a definition of causality motivated by wright 's ness test is shown to always hold for a single conjunct . moreover , conditions that hold for all the examples considered by hp are given that guarantee that causality according to ( this version ) of the ness test is equivalent to the hp definition ."}
{"title": "learning to explain : an information-theoretic perspective on model interpretation", "abstract": "we introduce instancewise feature selection as a methodology for model interpretation . our method is based on learning a function to extract a subset of features that are most informative for each given example . this feature selector is trained to maximize the mutual information between selected features and the response variable , where the conditional distribution of the response variable given the input is the model to be explained . we develop an efficient variational approximation to the mutual information , and show that the resulting method compares favorably to other model explanation methods on a variety of synthetic and real data sets using both quantitative metrics and human evaluation ."}
{"title": "bpcmont : business process change management ontology", "abstract": "change management for evolving collaborative business process development is crucial when the business logic , transections and workflow change due to changes in business strategies or organizational and technical environment . during the change implementation , business processes are analyzed and improved ensuring that they capture the proposed change and they do not contain any undesired functionalities or change side-effects . this paper presents business process change management approach for the efficient and effective implementation of change in the business process . the key technology behind our approach is our proposed business process change management ontology ( bpcmont ) which is the main contribution of this paper . bpcmont , as a formalized change specification , helps to revert bp into a consistent state in case of system crash , intermediate conflicting stage or unauthorized change done , aid in change traceability in the new and old versions of business processes , change effects can be seen and estimated effectively , ease for stakeholders to validate and verify change implementation , etc ."}
{"title": "inference compilation and universal probabilistic programming", "abstract": "we introduce a method for using deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages , establishing a framework that combines the strengths of probabilistic programming and deep learning methods . we call what we do `` compilation of inference '' because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language . when at test time this neural network is fed observational data and executed , it performs approximate inference in the original model specified by the probabilistic program . our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine . we illustrate our method on mixture models and captcha solving and show significant speedups in the efficiency of inference ."}
{"title": "discovery of a missing disease spreader", "abstract": "this study presents a method to discover an outbreak of an infectious disease in a region for which data are missing , but which is at work as a disease spreader . node discovery for the spread of an infectious disease is defined as discriminating between the nodes which are neighboring to a missing disease spreader node , and the rest , given a dataset on the number of cases . the spread is described by stochastic differential equations . a perturbation theory quantifies the impact of the missing spreader on the moments of the number of cases . statistical discriminators examine the mid-body or tail-ends of the probability density function , and search for the disturbance from the missing spreader . they are tested with computationally synthesized datasets , and applied to the sars outbreak and flu pandemic ."}
{"title": "discussion on mechanical learning and learning machine", "abstract": "mechanical learning is a computing system that is based on a set of simple and fixed rules , and can learn from incoming data . a learning machine is a system that realizes mechanical learning . importantly , we emphasis that it is based on a set of simple and fixed rules , contrasting to often called machine learning that is sophisticated software based on very complicated mathematical theory , and often needs human intervene for software fine tune and manual adjustments . here , we discuss some basic facts and principles of such system , and try to lay down a framework for further study . we propose 2 directions to approach mechanical learning , just like church-turing pair : one is trying to realize a learning machine , another is trying to well describe the mechanical learning ."}
{"title": "doubly robust off-policy value evaluation for reinforcement learning", "abstract": "we study the problem of off-policy value evaluation in reinforcement learning ( rl ) , where one aims to estimate the value of a new policy based on data collected by a different policy . this problem is often a critical step when applying rl in real-world problems . despite its importance , existing general methods either have uncontrolled bias or suffer high variance . in this work , we extend the doubly robust estimator for bandits to sequential decision-making problems , which gets the best of both worlds : it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators . we demonstrate the estimator 's accuracy in several benchmark problems , and illustrate its use as a subroutine in safe policy improvement . we also provide theoretical results on the hardness of the problem , and show that our estimator can match the lower bound in certain scenarios ."}
{"title": "learning flexible and reusable locomotion primitives for a microrobot", "abstract": "the design of gaits for robot locomotion can be a daunting process which requires significant expert knowledge and engineering . this process is even more challenging for robots that do not have an accurate physical model , such as compliant or micro-scale robots . data-driven gait optimization provides an automated alternative to analytical gait design . in this paper , we propose a novel approach to efficiently learn a wide range of locomotion tasks with walking robots . this approach formalizes locomotion as a contextual policy search task to collect data , and subsequently uses that data to learn multi-objective locomotion primitives that can be used for planning . as a proof-of-concept we consider a simulated hexapod modeled after a recently developed microrobot , and we thoroughly evaluate the performance of this microrobot on different tasks and gaits . our results validate the proposed controller and learning scheme on single and multi-objective locomotion tasks . moreover , the experimental simulations show that without any prior knowledge about the robot used ( e.g. , dynamics model ) , our approach is capable of learning locomotion primitives within 250 trials and subsequently using them to successfully navigate through a maze ."}
{"title": "incremental sampling-based motion planners using policy iteration methods", "abstract": "recent progress in randomized motion planners has led to the development of a new class of sampling-based algorithms that provide asymptotic optimality guarantees , notably the rrt* and the prm* algorithms . careful analysis reveals that the so-called `` rewiring '' step in these algorithms can be interpreted as a local policy iteration ( pi ) step ( i.e. , a local policy evaluation step followed by a local policy improvement step ) so that asymptotically , as the number of samples tend to infinity , both algorithms converge to the optimal path almost surely ( with probability 1 ) . policy iteration , along with value iteration ( vi ) are common methods for solving dynamic programming ( dp ) problems . based on this observation , recently , the rrt $ ^ { \\ # } $ algorithm has been proposed , which performs , during each iteration , bellman updates ( aka `` backups '' ) on those vertices of the graph that have the potential of being part of the optimal path ( i.e. , the `` promising '' vertices ) . the rrt $ ^ { \\ # } $ algorithm thus utilizes dynamic programming ideas and implements them incrementally on randomly generated graphs to obtain high quality solutions . in this work , and based on this key insight , we explore a different class of dynamic programming algorithms for solving shortest-path problems on random graphs generated by iterative sampling methods . these class of algorithms utilize policy iteration instead of value iteration , and thus are better suited for massive parallelization . contrary to the rrt* algorithm , the policy improvement during the rewiring step is not performed only locally but rather on a set of vertices that are classified as `` promising '' during the current iteration . this tends to speed-up the whole process . the resulting algorithm , aptly named policy iteration-rrt $ ^ { \\ # } $ ( pi-rrt $ ^ { \\ # } $ ) is the first of a new class of dp-inspired algorithms for randomized motion planning that utilize pi methods ."}
{"title": "characterizations of decomposable dependency models", "abstract": "decomposable dependency models possess a number of interesting and useful properties . this paper presents new characterizations of decomposable models in terms of independence relationships , which are obtained by adding a single axiom to the well-known set characterizing dependency models that are isomorphic to undirected graphs . we also briefly discuss a potential application of our results to the problem of learning graphical models from data ."}
{"title": "emulator vs real phone : android malware detection using machine learning", "abstract": "the android operating system has become the most popular operating system for smartphones and tablets leading to a rapid rise in malware . sophisticated android malware employ detection avoidance techniques in order to hide their malicious activities from analysis tools . these include a wide range of anti-emulator techniques , where the malware programs attempt to hide their malicious activities by detecting the emulator . for this reason , countermeasures against antiemulation are becoming increasingly important in android malware detection . analysis and detection based on real devices can alleviate the problems of anti-emulation as well as improve the effectiveness of dynamic analysis . hence , in this paper we present an investigation of machine learning based malware detection using dynamic analysis on real devices . a tool is implemented to automatically extract dynamic features from android phones and through several experiments , a comparative analysis of emulator based vs. device based detection by means of several machine learning algorithms is undertaken . our study shows that several features could be extracted more effectively from the on-device dynamic analysis compared to emulators . it was also found that approximately 24 % more apps were successfully analysed on the phone . furthermore , all of the studied machine learning based detection performed better when applied to features extracted from the on-device dynamic analysis ."}
{"title": "hybridization of interval cp and evolutionary algorithms for optimizing difficult problems", "abstract": "the only rigorous approaches for achieving a numerical proof of optimality in global optimization are interval-based methods that interleave branching of the search-space and pruning of the subdomains that can not contain an optimal solution . state-of-the-art solvers generally integrate local optimization algorithms to compute a good upper bound of the global minimum over each subspace . in this document , we propose a cooperative framework in which interval methods cooperate with evolutionary algorithms . the latter are stochastic algorithms in which a population of candidate solutions iteratively evolves in the search-space to reach satisfactory solutions . within our cooperative solver charibde , the evolutionary algorithm and the interval-based algorithm run in parallel and exchange bounds , solutions and search-space in an advanced manner via message passing . a comparison of charibde with state-of-the-art interval-based solvers ( globsol , ibba , ibex ) and nlp solvers ( couenne , baron ) on a benchmark of difficult coconut problems shows that charibde is highly competitive against non-rigorous solvers and converges faster than rigorous solvers by an order of magnitude ."}
{"title": "fuzzy logic and probability", "abstract": "in this paper we deal with a new approach to probabilistic reasoning in a logical framework . nearly almost all logics of probability that have been proposed in the literature are based on classical two-valued logic . after making clear the differences between fuzzy logic and probability theory , here we propose a { em fuzzy } logic of probability for which completeness results ( in a probabilistic sense ) are provided . the main idea behind this approach is that probability values of crisp propositions can be understood as truth-values of some suitable fuzzy propositions associated to the crisp ones . moreover , suggestions and examples of how to extend the formalism to cope with conditional probabilities and with other uncertainty formalisms are also provided ."}
{"title": "local utility elicitation in gai models", "abstract": "structured utility models are essential for the effective representation and elicitation of complex multiattribute utility functions . generalized additive independence ( gai ) models provide an attractive structural model of user preferences , offering a balanced tradeoff between simplicity and applicability . while representation and inference with such models is reasonably well understood , elicitation of the parameters of such models has been studied less from a practical perspective . we propose a procedure to elicit gai model parameters using only `` local '' utility queries rather than `` global '' queries over full outcomes . our local queries take full advantage of gai structure and provide a sound framework for extending the elicitation procedure to settings where the uncertainty over utility parameters is represented probabilistically . we describe experiments using a myopic value-of-information approach to elicitation in a large gai model ."}
{"title": "the power of non-ground rules in answer set programming", "abstract": "answer set programming ( asp ) is a well-established logic programming language that offers an intuitive , declarative syntax for problem solving . in its traditional application , a fixed asp program for a given problem is designed and the actual instance of the problem is fed into the program as a set of facts . this approach typically results in programs with comparably short and simple rules . however , as is known from complexity analysis , such an approach limits the expressive power of asp ; in fact , an entire np-check can be encoded into a single large rule body of bounded arity that performs both a guess and a check within the same rule . here , we propose a novel paradigm for encoding hard problems in asp by making explicit use of large rules which depend on the actual instance of the problem . we illustrate how this new encoding paradigm can be used , providing examples of problems from the first , second , and even third level of the polynomial hierarchy . as state-of-the-art solvers are tuned towards short rules , rule decomposition is a key technique in the practical realization of our approach . we also provide some preliminary benchmarks which indicate that giving up the convenient way of specifying a fixed program can lead to a significant speed-up . this paper is under consideration for acceptance into tplp ."}
{"title": "neuromorphic robot dream", "abstract": "in this paper we present the next step in our approach to neurobiologically plausible implementation of emotional reactions and behaviors for real-time autonomous robotic systems . the working metaphor we use is the `` day '' and the `` night '' phases of mammalian life . during the `` day phase '' a robotic system stores the inbound information and is controlled by a light-weight rule-based system in real time . in contrast to that , during the `` night phase '' information that has been stored is transferred to a supercomputing system to update the realistic neural network : emotional and behavioral strategies ."}
{"title": "interactive learning of acyclic conditional preference networks", "abstract": "learning of user preferences , as represented by , for example , conditional preference networks ( cp-nets ) , has become a core issue in ai research . recent studies investigate learning of cp-nets from randomly chosen examples or from membership and equivalence queries . to assess the optimality of learning algorithms as well as to better understand the combinatorial structure of classes of cp-nets , it is helpful to calculate certain learning-theoretic information complexity parameters . this paper determines bounds on or exact values of some of the most central information complexity parameters , namely the vc dimension , the ( recursive ) teaching dimension , the self-directed learning complexity , and the optimal mistake bound , for classes of acyclic cp-nets . we further provide an algorithm that learns tree-structured cp-nets from membership queries . using our results on complexity parameters , we assess the optimality of our algorithm as well as that of another query learning algorithm for acyclic cp-nets presented in the literature . our algorithm is near-optimal , and can , under certain assumptions be adapted to the case when the membership oracle is faulty ."}
{"title": "global preferential consistency for the topological sorting-based maximal spanning tree problem", "abstract": "we introduce a new type of fully computable problems , for dss dedicated to maximal spanning tree problems , based on deduction and choice : preferential consistency problems . to show its interest , we describe a new compact representation of preferences specific to spanning trees , identifying an efficient maximal spanning tree sub-problem . next , we compare this problem with the pareto-based multiobjective one . and at last , we propose an efficient algorithm solving the associated preferential consistency problem ."}
{"title": "technical report : distribution temporal logic : combining correctness with quality of estimation", "abstract": "we present a new temporal logic called distribution temporal logic ( dtl ) defined over predicates of belief states and hidden states of partially observable systems . dtl can express properties involving uncertainty and likelihood that can not be described by existing logics . a co-safe formulation of dtl is defined and algorithmic procedures are given for monitoring executions of a partially observable markov decision process with respect to such formulae . a simulation case study of a rescue robotics application outlines our approach ."}
{"title": "avian influenza ( h5n1 ) expert system using dempster-shafer theory", "abstract": "based on cumulative number of confirmed human cases of avian influenza ( h5n1 ) reported to world health organization ( who ) in the 2011 from 15 countries , indonesia has the largest number death because avian influenza which 146 deaths . in this research , the researcher built an avian influenza ( h5n1 ) expert system for identifying avian influenza disease and displaying the result of identification process . in this paper , we describe five symptoms as major symptoms which include depression , combs , wattle , bluish face region , swollen face region , narrowness of eyes , and balance disorders . we use chicken as research object . research location is in the lampung province , south sumatera . the researcher reason to choose lampung province in south sumatera on the basis that has a high poultry population . dempster-shafer theory to quantify the degree of belief as inference engine in expert system , our approach uses dempster-shafer theory to combine beliefs under conditions of uncertainty and ignorance , and allows quantitative measurement of the belief and plausibility in our identification result . the result reveal that avian influenza ( h5n1 ) expert system has successfully identified the existence of avian influenza and displaying the result of identification process ."}
{"title": "accelerating dependency graph learning from heterogeneous categorical event streams via knowledge transfer", "abstract": "dependency graph , as a heterogeneous graph representing the intrinsic relationships between different pairs of system entities , is essential to many data analysis applications , such as root cause diagnosis , intrusion detection , etc . given a well-trained dependency graph from a source domain and an immature dependency graph from a target domain , how can we extract the entity and dependency knowledge from the source to enhance the target ? one way is to directly apply a mature dependency graph learned from a source domain to the target domain . but due to the domain variety problem , directly using the source dependency graph often can not achieve good performance . traditional transfer learning methods mainly focus on numerical data and are not applicable . in this paper , we propose acret , a knowledge transfer based model for accelerating dependency graph learning from heterogeneous categorical event streams . in particular , we first propose an entity estimation model to filter out irrelevant entities from the source domain based on entity embedding and manifold learning . only the entities with statistically high correlations are transferred to the target domain . on the surviving entities , we propose a dependency construction model for constructing the unbiased dependency relationships by solving a two-constraint optimization problem . the experimental results on synthetic and real-world datasets demonstrate the effectiveness and efficiency of acret . we also apply acret to a real enterprise security system for intrusion detection . our method is able to achieve superior detection performance at least 20 days lead lag time in advance with more than 70 % accuracy ."}
{"title": "separation properties of sets of probability measures", "abstract": "this paper analyzes independence concepts for sets of probability measures associated with directed acyclic graphs . the paper shows that epistemic independence and the standard markov condition violate desirable separation properties . the adoption of a contraction condition leads to d-separation but still fails to guarantee a belief separation property . to overcome this unsatisfactory situation , a strong markov condition is proposed , based on epistemic independence . the main result is that the strong markov condition leads to strong independence and does enforce separation properties ; this result implies that ( 1 ) separation properties of bayesian networks do extend to epistemic independence and sets of probability measures , and ( 2 ) strong independence has a clear justification based on epistemic independence and the strong markov condition ."}
{"title": "ares : adaptive receding-horizon synthesis of optimal plans", "abstract": "we introduce ares , an efficient approximation algorithm for generating optimal plans ( action sequences ) that take an initial state of a markov decision process ( mdp ) to a state whose cost is below a specified ( convergence ) threshold . ares uses particle swarm optimization , with adaptive sizing for both the receding horizon and the particle swarm . inspired by importance splitting , the length of the horizon and the number of particles are chosen such that at least one particle reaches a next-level state , that is , a state where the cost decreases by a required delta from the previous-level state . the level relation on states and the plans constructed by ares implicitly define a lyapunov function and an optimal policy , respectively , both of which could be explicitly generated by applying ares to all states of the mdp , up to some topological equivalence relation . we also assess the effectiveness of ares by statistically evaluating its rate of success in generating optimal plans . the ares algorithm resulted from our desire to clarify if flying in v-formation is a flocking policy that optimizes energy conservation , clear view , and velocity alignment . that is , we were interested to see if one could find optimal plans that bring a flock from an arbitrary initial state to a state exhibiting a single connected v-formation . for flocks with 7 birds , ares is able to generate a plan that leads to a v-formation in 95 % of the 8,000 random initial configurations within 63 seconds , on average . ares can also be easily customized into a model-predictive controller ( mpc ) with an adaptive receding horizon and statistical guarantees of convergence . to the best of our knowledge , our adaptive-sizing approach is the first to provide convergence guarantees in receding-horizon techniques ."}
{"title": "markov logic in infinite domains", "abstract": "combining first-order logic and probability has long been a goal of ai . markov logic ( richardson & domingos , 2006 ) accomplishes this by attaching weights to first-order formulas and viewing them as templates for features of markov networks . unfortunately , it does not have the full power of first-order logic , because it is only defined for finite domains . this paper extends markov logic to infinite domains , by casting it in the framework of gibbs measures ( georgii , 1988 ) . we show that a markov logic network ( mln ) admits a gibbs measure as long as each ground atom has a finite number of neighbors . many interesting cases fall in this category . we also show that an mln admits a unique measure if the weights of its non-unit clauses are small enough . we then examine the structure of the set of consistent measures in the non-unique case . many important phenomena , including systems with phase transitions , are represented by mlns with non-unique measures . we relate the problem of satisfiability in first-order logic to the properties of mln measures , and discuss how markov logic relates to previous infinite models ."}
{"title": "not only a lack of right definitions : arguments for a shift in information-processing paradigm", "abstract": "machine consciousness and machine intelligence are not simply new buzzwords that occupy our imagination . over the last decades , we witness an unprecedented rise in attempts to create machines with human-like features and capabilities . however , despite widespread sympathy and abundant funding , progress in these enterprises is far from being satisfactory . the reasons for this are twofold : first , the notions of cognition and intelligence ( usually borrowed from human behavior studies ) are notoriously blurred and ill-defined , and second , the basic concepts underpinning the whole discourse are by themselves either undefined or defined very vaguely . that leads to improper and inadequate research goals determination , which i will illustrate with some examples drawn from recent documents issued by darpa and the european commission . on the other hand , i would like to propose some remedies that , i hope , would improve the current state-of-the-art disgrace ."}
{"title": "d-cfpr : d numbers extended consistent fuzzy preference relations", "abstract": "how to express an expert 's or a decision maker 's preference for alternatives is an open issue . consistent fuzzy preference relation ( cfpr ) is with big advantages to handle this problem due to it can be construed via a smaller number of pairwise comparisons and satisfies additive transitivity property . however , the cfpr is incapable of dealing with the cases involving uncertain and incomplete information . in this paper , a d numbers extended consistent fuzzy preference relation ( d-cfpr ) is proposed to overcome the weakness . the d-cfpr extends the classical cfpr by using a new model of expressing uncertain information called d numbers . the d-cfpr inherits the merits of classical cfpr and can be totally reduced to the classical cfpr . this study can be integrated into our previous study about d-ahp ( d numbers extended ahp ) model to provide a systematic solution for multi-criteria decision making ( mcdm ) ."}
{"title": "learning for expertise matching with declination prediction", "abstract": "we study the problem of finding appropriate experts who are able to complete timely reviews and would not say `` no '' to the invitation . the problem is a central issue in many question-and-answer systems , but has received little research attention . different from most existing studies that focus on expertise matching , we want to further predict the expert 's response : given a question , how can we find the expert who is able to provide a quality review and will agree to do it . we formalize the problem as a ranking problem . we first present an embedding-based question-to-expert distance metric for expertise matching and propose a ranking factor graph ( rankfg ) model to predict expert response . for online evaluation , we developed a chrome extension for reviewer recommendation and deployed it in the google chrome web store , and then collected the reviewers ' feedback . we also used the review bidding of a cs conference for evaluation . in the experiments , the proposed method demonstrates its superiority ( +6.6-21.2 % by map ) over several state-of-the-art algorithms ."}
{"title": "bypassing captcha by machine a proof for passing the turing test", "abstract": "for the last ten years , captchas have been widely used by websites to prevent their data being automatically updated by machines . by supposedly allowing only humans to do so , captchas take advantage of the reverse turing test ( tt ) , knowing that humans are more intelligent than machines . generally , captchas have defeated machines , but things are changing rapidly as technology improves . hence , advanced research into optical character recognition ( ocr ) is overtaking attempts to strengthen captchas against machine-based attacks . this paper investigates the immunity of captcha , which was built on the failure of the tt . we show that some captchas are easily broken using a simple ocr machine built for the purpose of this study . by reviewing other techniques , we show that even more difficult captchas can be broken using advanced ocr machines . current advances in ocr should enable machines to pass the tt in the image recognition domain , which is exactly where machines are seeking to overcome captchas . we enhance traditional captchas by employing not only characters , but also natural language and multiple objects within the same captcha . the proposed captchas might be able to hold out against machines , at least until the advent of a machine that passes the tt completely ."}
{"title": "verification of inconsistency-aware knowledge and action bases ( extended version )", "abstract": "description logic knowledge and action bases ( kabs ) have been recently introduced as a mechanism that provides a semantically rich representation of the information on the domain of interest in terms of a dl kb and a set of actions to change such information over time , possibly introducing new objects . in this setting , decidability of verification of sophisticated temporal properties over kabs , expressed in a variant of first-order mu-calculus , has been shown . however , the established framework treats inconsistency in a simplistic way , by rejecting inconsistent states produced through action execution . we address this problem by showing how inconsistency handling based on the notion of repairs can be integrated into kabs , resorting to inconsistency-tolerant semantics . in this setting , we establish decidability and complexity of verification ."}
{"title": "creating scalable and interactive web applications using high performance latent variable models", "abstract": "in this project we outline a modularized , scalable system for comparing amazon products in an interactive and informative way using efficient latent variable models and dynamic visualization . we demonstrate how our system can build on the structure and rich review information of amazon products in order to provide a fast , multifaceted , and intuitive comparison . by providing a condensed per-topic comparison visualization to the user , we are able to display aggregate information from the entire set of reviews while providing an interface that is at least as compact as the `` most helpful reviews '' currently displayed by amazon , yet far more informative ."}
{"title": "stackinsights : cognitive learning for hybrid cloud readiness", "abstract": "hybrid cloud is an integrated cloud computing environment utilizing a mix of public cloud , private cloud , and on-premise traditional it infrastructures . workload awareness , defined as a detailed full range understanding of each individual workload , is essential in implementing the hybrid cloud . while it is critical to perform an accurate analysis to determine which workloads are appropriate for on-premise deployment versus which workloads can be migrated to a cloud off-premise , the assessment is mainly performed by rule or policy based approaches . in this paper , we introduce stackinsights , a novel cognitive system to automatically analyze and predict the cloud readiness of workloads for an enterprise . our system harnesses the critical metrics across the entire stack : 1 ) infrastructure metrics , 2 ) data relevance metrics , and 3 ) application taxonomy , to identify workloads that have characteristics of a ) low sensitivity with respect to business security , criticality and compliance , and b ) low response time requirements and access patterns . since the capture of the data relevance metrics involves an intrusive and in-depth scanning of the content of storage objects , a machine learning model is applied to perform the business relevance classification by learning from the meta level metrics harnessed across stack . in contrast to traditional methods , stackinsights significantly reduces the total time for hybrid cloud readiness assessment by orders of magnitude ."}
{"title": "toggling operators in computability logic", "abstract": "computability logic ( cl ) ( see http : //www.cis.upenn.edu/~giorgi/cl.html ) is a research program for redeveloping logic as a formal theory of computability , as opposed to the formal theory of truth which it has more traditionally been . formulas in cl stand for interactive computational problems , seen as games between a machine and its environment ; logical operators represent operations on such entities ; and `` truth '' is understood as existence of an effective solution . the formalism of cl is open-ended , and may undergo series of extensions as the studies of the subject advance . so far three -- parallel , sequential and choice -- sorts of conjunction and disjunction have been studied . the present paper adds one more natural kind to this collection , termed toggling . the toggling operations can be characterized as lenient versions of choice operations where choices are retractable , being allowed to be reconsidered any finite number of times . this way , they model trial-and-error style decision steps in interactive computation . the main technical result of this paper is constructing a sound and complete axiomatization for the propositional fragment of computability logic whose vocabulary , together with negation , includes all four -- parallel , toggling , sequential and choice -- kinds of conjunction and disjunction . along with toggling conjunction and disjunction , the paper also introduces the toggling versions of quantifiers and recurrence operations ."}
{"title": "learning robocup-keepaway with kernels", "abstract": "we apply kernel-based methods to solve the difficult reinforcement learning problem of 3vs2 keepaway in robocup simulated soccer . key challenges in keepaway are the high-dimensionality of the state space ( rendering conventional discretization-based function approximation like tilecoding infeasible ) , the stochasticity due to noise and multiple learning agents needing to cooperate ( meaning that the exact dynamics of the environment are unknown ) and real-time learning ( meaning that an efficient online implementation is required ) . we employ the general framework of approximate policy iteration with least-squares-based policy evaluation . as underlying function approximator we consider the family of regularization networks with subset of regressors approximation . the core of our proposed solution is an efficient recursive implementation with automatic supervised selection of relevant basis functions . simulation results indicate that the behavior learned through our approach clearly outperforms the best results obtained earlier with tilecoding by stone et al . ( 2005 ) ."}
{"title": "sample-efficient deep reinforcement learning for dialog control", "abstract": "representing a dialog policy as a recurrent neural network ( rnn ) is attractive because it handles partial observability , infers a latent representation of state , and can be optimized with supervised learning ( sl ) or reinforcement learning ( rl ) . for rl , a policy gradient approach is natural , but is sample inefficient . in this paper , we present 3 methods for reducing the number of dialogs required to optimize an rnn-based dialog policy with rl . the key idea is to maintain a second rnn which predicts the value of the current policy , and to apply experience replay to both networks . on two tasks , these methods reduce the number of dialogs/episodes required by about a third , vs. standard policy gradient methods ."}
{"title": "a distributed process infrastructure for a distributed data structure", "abstract": "the resource description framework ( rdf ) is continuing to grow outside the bounds of its initial function as a metadata framework and into the domain of general-purpose data modeling . this expansion has been facilitated by the continued increase in the capacity and speed of rdf database repositories known as triple-stores . high-end rdf triple-stores can hold and process on the order of 10 billion triples . in an effort to provide a seamless integration of the data contained in rdf repositories , the linked data community is providing specifications for linking rdf data sets into a universal distributed graph that can be traversed by both man and machine . while the seamless integration of rdf data sets is important , at the scale of the data sets that currently exist and will ultimately grow to become , the `` download and index '' philosophy of the world wide web will not so easily map over to the semantic web . this essay discusses the importance of adding a distributed rdf process infrastructure to the current distributed rdf data structure ."}
{"title": "variable neighborhood search for the university lecturer-student assignment problem", "abstract": "the paper presents a study of local search heuristics in general and variable neighborhood search in particular for the resolution of an assignment problem studied in the practical work of universities . here , students have to be assigned to scientific topics which are proposed and supported by members of staff . the problem involves the optimization under given preferences of students which may be expressed when applying for certain topics . it is possible to observe that variable neighborhood search leads to superior results for the tested problem instances . one instance is taken from an actual case , while others have been generated based on the real world data to support the analysis with a deeper analysis . an extension of the problem has been formulated by integrating a second objective function that simultaneously balances the workload of the members of staff while maximizing utility of the students . the algorithmic approach has been prototypically implemented in a computer system . one important aspect in this context is the application of the research work to problems of other scientific institutions , and therefore the provision of decision support functionalities ."}
{"title": "the automated mapping of plans for plan recognition", "abstract": "to coordinate with other agents in its environment , an agent needs models of what the other agents are trying to do . when communication is impossible or expensive , this information must be acquired indirectly via plan recognition . typical approaches to plan recognition start with a specification of the possible plans the other agents may be following , and develop special techniques for discriminating among the possibilities . perhaps more desirable would be a uniform procedure for mapping plans to general structures supporting inference based on uncertain and incomplete observations . in this paper , we describe a set of methods for converting plans represented in a flexible procedural language to observation models represented as probabilistic belief networks ."}
{"title": "generalizing prototype theory : a formal quantum framework", "abstract": "theories of natural language and concepts have been unable to model the flexibility , creativity , context-dependence , and emergence , exhibited by words , concepts and their combinations . the mathematical formalism of quantum theory has instead been successful in capturing these phenomena such as graded membership , situational meaning , composition of categories , and also more complex decision making situations , which can not be modeled in traditional probabilistic approaches . we show how a formal quantum approach to concepts and their combinations can provide a powerful extension of prototype theory . we explain how prototypes can interfere in conceptual combinations as a consequence of their contextual interactions , and provide an illustration of this using an intuitive wave-like diagram . this quantum-conceptual approach gives new life to original prototype theory , without however making it a privileged concept theory , as we explain at the end of our paper ."}
{"title": "augmenting supervised emotion recognition with rule-based decision model", "abstract": "the aim of this research is development of rule based decision model for emotion recognition . this research also proposes using the rules for augmenting inter-corporal recognition accuracy in multimodal systems that use supervised learning techniques . the classifiers for such learning based recognition systems are susceptible to over fitting and only perform well on intra-corporal data . to overcome the limitation this research proposes using rule based model as an additional modality . the rules were developed using raw feature data from visual channel , based on human annotator agreement and existing studies that have attributed movement and postures to emotions . the outcome of the rule evaluations was combined during the decision phase of emotion recognition system . the results indicate rule based emotion recognition augment recognition accuracy of learning based systems and also provide better recognition rate across inter corpus emotion test data ."}
{"title": "the complexity of reasoning with global constraints", "abstract": "constraint propagation is one of the techniques central to the success of constraint programming . to reduce search , fast algorithms associated with each constraint prune the domains of variables . with global ( or non-binary ) constraints , the cost of such propagation may be much greater than the quadratic cost for binary constraints . we therefore study the computational complexity of reasoning with global constraints . we first characterise a number of important questions related to constraint propagation . we show that such questions are intractable in general , and identify dependencies between the tractability and intractability of the different questions . we then demonstrate how the tools of computational complexity can be used in the design and analysis of specific global constraints . in particular , we illustrate how computational complexity can be used to determine when a lesser level of local consistency should be enforced , when constraints can be safely generalized , when decomposing constraints will reduce the amount of pruning , and when combining constraints is tractable ."}
{"title": "axioms in model-based planners", "abstract": "axioms can be used to model derived predicates in domain- independent planning models . formulating models which use axioms can sometimes result in problems with much smaller search spaces and shorter plans than the original model . previous work on axiom-aware planners focused solely on state- space search planners . we propose axiom-aware planners based on answer set programming and integer programming . we evaluate them on pddl domains with axioms and show that they can exploit additional expressivity of axioms ."}
{"title": "introduction to formal concept analysis and its applications in information retrieval and related fields", "abstract": "this paper is a tutorial on formal concept analysis ( fca ) and its applications . fca is an applied branch of lattice theory , a mathematical discipline which enables formalisation of concepts as basic units of human thinking and analysing data in the object-attribute form . originated in early 80s , during the last three decades , it became a popular human-centred tool for knowledge representation and data analysis with numerous applications . since the tutorial was specially prepared for russir 2014 , the covered fca topics include information retrieval with a focus on visualisation aspects , machine learning , data mining and knowledge discovery , text mining and several others ."}
{"title": "conformant planning as a case study of incremental qbf solving", "abstract": "we consider planning with uncertainty in the initial state as a case study of incremental quantified boolean formula ( qbf ) solving . we report on experiments with a workflow to incrementally encode a planning instance into a sequence of qbfs . to solve this sequence of incrementally constructed qbfs , we use our general-purpose incremental qbf solver depqbf . since the generated qbfs have many clauses and variables in common , our approach avoids redundancy both in the encoding phase and in the solving phase . experimental results show that incremental qbf solving outperforms non-incremental qbf solving . our results are the first empirical study of incremental qbf solving in the context of planning and motivate its use in other application domains ."}
{"title": "implementing evidential reasoning in expert systems", "abstract": "the dempster-shafer theory has been extended recently for its application to expert systems . however , implementing the extended d-s reasoning model in rule-based systems greatly complicates the task of generating informative explanations . by implementing gertis , a prototype system for diagnosing rheumatoid arthritis , we show that two kinds of knowledge are essential for explanation generation : ( l ) taxonomic class relationships between hypotheses and ( 2 ) pointers to the rules that significantly contribute to belief in the hypothesis . as a result , the knowledge represented in gertis is richer and more complex than that of conventional rule-based systems . gertis not only demonstrates the feasibility of rule-based evidential-reasoning systems , but also suggests ways to generate better explanations , and to explicitly represent various useful relationships among hypotheses and rules ."}
{"title": "elastic solver : balancing solution time and energy consumption", "abstract": "combinatorial decision problems arise in many different domains such as scheduling , routing , packing , bioinformatics , and many more . despite recent advances in developing scalable solvers , there are still many problems which are often very hard to solve . typically the most advanced solvers include elements which are stochastic in nature . if a same instance is solved many times using different seeds then depending on the inherent characteristics of a problem instance and the solver , one can observe a highly-variant distribution of times spanning multiple orders of magnitude . therefore , to solve a problem instance efficiently it is often useful to solve the same instance in parallel with different seeds . with the proliferation of cloud computing , it is natural to think about an elastic solver which can scale up by launching searches in parallel on thousands of machines ( or cores ) . however , this could result in consuming a lot of energy . moreover , not every instance would require thousands of machines . the challenge is to resolve the tradeoff between solution time and energy consumption optimally for a given problem instance . we analyse the impact of the number of machines ( or cores ) on not only solution time but also on energy consumption . we highlight that although solution time always drops as the number of machines increases , the relation between the number of machines and energy consumption is more complicated . in many cases , the optimal energy consumption may be achieved by a middle ground , we analyse this relationship in detail . the tradeoff between solution time and energy consumption is studied further , showing that the energy consumption of a solver can be reduced drastically if we increase the solution time marginally . we also develop a prediction model , demonstrating that such insights can be exploited to achieve faster solutions times in a more energy efficient manor ."}
{"title": "is there a physically universal cellular automaton or hamiltonian ?", "abstract": "it is known that both quantum and classical cellular automata ( ca ) exist that are computationally universal in the sense that they can simulate , after appropriate initialization , any quantum or classical computation , respectively . here we introduce a different notion of universality : a ca is called physically universal if every transformation on any finite region can be ( approximately ) implemented by the autonomous time evolution of the system after the complement of the region has been initialized in an appropriate way . we pose the question of whether physically universal cas exist . such cas would provide a model of the world where the boundary between a physical system and its controller can be consistently shifted , in analogy to the heisenberg cut for the quantum measurement problem . we propose to study the thermodynamic cost of computation and control within such a model because implementing a cyclic process on a microsystem may require a non-cyclic process for its controller , whereas implementing a cyclic process on system and controller may require the implementation of a non-cyclic process on a `` meta '' -controller , and so on . physically universal cas avoid this infinite hierarchy of controllers and the cost of implementing cycles on a subsystem can be described by mixing properties of the ca dynamics . we define a physical prior on the ca configurations by applying the dynamics to an initial state where half of the ca is in the maximum entropy state and half of it is in the all-zero state ( thus reflecting the fact that life requires non-equilibrium states like the boundary between a hold and a cold reservoir ) . as opposed to solomonoff 's prior , our prior does not only account for the kolmogorov complexity but also for the cost of isolating the system during the state preparation if the preparation process is not robust ."}
{"title": "optimal weighting for exam composition", "abstract": "a problem faced by many instructors is that of designing exams that accurately assess the abilities of the students . typically these exams are prepared several days in advance , and generic question scores are used based on rough approximation of the question difficulty and length . for example , for a recent class taught by the author , there were 30 multiple choice questions worth 3 points , 15 true/false with explanation questions worth 4 points , and 5 analytical exercises worth 10 points . we describe a novel framework where algorithms from machine learning are used to modify the exam question weights in order to optimize the exam scores , using the overall class grade as a proxy for a student 's true ability . we show that significant error reduction can be obtained by our approach over standard weighting schemes , and we make several new observations regarding the properties of the `` good '' and `` bad '' exam questions that can have impact on the design of improved future evaluation methods ."}
{"title": "on-the-job learning with bayesian decision theory", "abstract": "our goal is to deploy a high-accuracy system starting with zero training examples . we consider an `` on-the-job '' setting , where as inputs arrive , we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident . as the model improves over time , the reliance on crowdsourcing queries decreases . we cast our setting as a stochastic game based on bayesian decision theory , which allows us to balance latency , cost , and accuracy objectives in a principled way . computing the optimal policy is intractable , so we develop an approximation based on monte carlo tree search . we tested our approach on three datasets -- -named-entity recognition , sentiment classification , and image classification . on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation , while boosting performance relative to the expert provided labels . we also achieve a 8 % f1 improvement over having a single human label the whole set , and a 28 % f1 improvement over online learning ."}
{"title": "plan recognition in stories and in life", "abstract": "plan recognition does not work the same way in stories and in `` real life '' ( people tend to jump to conclusions more in stories ) . we present a theory of this , for the particular case of how objects in stories ( or in life ) influence plan recognition decisions . we provide a bayesian network formalization of a simple first-order theory of plans , and show how a particular network parameter seems to govern the difference between `` life-like '' and `` story-like '' response . we then show why this parameter would be influenced ( in the desired way ) by a model of speaker ( or author ) topic selection which assumes that facts in stories are typically `` relevant '' ."}
{"title": "effective multi-robot spatial task allocation using model approximations", "abstract": "real-world multi-agent planning problems can not be solved using decision-theoretic planning methods due to the exponential complexity . we approximate firefighting in rescue simulation as a spatially distributed task and model with multi-agent markov decision process . we use recent approximation methods for spatial task problems to reduce the model complexity . our approximations are single-agent , static task , shortest path pruning , dynamic planning horizon , and task clustering . we create scenarios from robocup rescue simulation maps and evaluate our methods on these graph worlds . the results show that our approach is faster and better than comparable methods and has negligible performance loss compared to the optimal policy . we also show that our method has a similar performance as dcop methods on example rcrs scenarios ."}
{"title": "proactive decision support using automated planning", "abstract": "proactive decision support ( pds ) helps in improving the decision making experience of human decision makers in human-in-the-loop planning environments . here both the quality of the decisions and the ease of making them are enhanced . in this regard , we propose a pds framework , named radar , based on the research in automated planning in ai , that aids the human decision maker with her plan to achieve her goals by providing alerts on : whether such a plan can succeed at all , whether there exist any resource constraints that may foil her plan , etc . this is achieved by generating and analyzing the landmarks that must be accomplished by any successful plan on the way to achieving the goals . note that , this approach also supports naturalistic decision making which is being acknowledged as a necessary element in proactive decision support , since it only aids the human decision maker through suggestions and alerts rather than enforcing fixed plans or decisions . we demonstrate the utility of the proposed framework through search-and-rescue examples in a fire-fighting domain ."}
{"title": "solving linear constraints in elementary abelian p-groups of symmetries", "abstract": "symmetries occur naturally in csp or sat problems and are not very difficult to discover , but using them to prune the search space tends to be very challenging . indeed , this usually requires finding specific elements in a group of symmetries that can be huge , and the problem of their very existence is np-hard . we formulate such an existence problem as a constraint problem on one variable ( the symmetry to be used ) ranging over a group , and try to find restrictions that may be solved in polynomial time . by considering a simple form of constraints ( restricted by a cardinality k ) and the class of groups that have the structure of fp-vector spaces , we propose a partial algorithm based on linear algebra . this polynomial algorithm always applies when k=p=2 , but may fail otherwise as we prove the problem to be np-hard for all other values of k and p. experiments show that this approach though restricted should allow for an efficient use of at least some groups of symmetries . we conclude with a few directions to be explored to efficiently solve this problem on the general case ."}
{"title": "learning amp chain graphs under faithfulness", "abstract": "this paper deals with chain graphs under the alternative andersson-madigan-perlman ( amp ) interpretation . in particular , we present a constraint based algorithm for learning an amp chain graph a given probability distribution is faithful to . we also show that the extension of meek 's conjecture to amp chain graphs does not hold , which compromises the development of efficient and correct score+search learning algorithms under assumptions weaker than faithfulness ."}
{"title": "men also like shopping : reducing gender bias amplification using corpus-level constraints", "abstract": "language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web . structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora . in this work , we study data and models associated with multilabel object classification and visual semantic role labeling . we find that ( a ) datasets for these tasks contain significant gender bias and ( b ) models trained on these datasets further amplify existing bias . for example , the activity cooking is over 33 % more likely to involve females than males in a training set , and a trained model further amplifies the disparity to 68 % at test time . we propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on lagrangian relaxation for collective inference . our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5 % and 40.5 % for multilabel classification and visual semantic role labeling , respectively ."}
{"title": "computability logic : a formal theory of interaction", "abstract": "computability logic is a formal theory of ( interactive ) computability in the same sense as classical logic is a formal theory of truth . this approach was initiated very recently in `` introduction to computability logic '' ( annals of pure and applied logic 123 ( 2003 ) , pp.1-99 ) . the present paper reintroduces computability logic in a more compact and less technical way . it is written in a semitutorial style with a general computer science , logic or mathematics audience in mind . an internet source on the subject is available at http : //www.cis.upenn.edu/~giorgi/cl.html , and additional material at http : //www.csc.villanova.edu/~japaridz/cl/gsoll.html ."}
{"title": "optimal target assignment and path finding for teams of agents", "abstract": "we study the tapf ( combined target-assignment and path-finding ) problem for teams of agents in known terrain , which generalizes both the anonymous and non-anonymous multi-agent path-finding problems . each of the teams is given the same number of targets as there are agents in the team . each agent has to move to exactly one target given to its team such that all targets are visited . the tapf problem is to first assign agents to targets and then plan collision-free paths for the agents to their targets in a way such that the makespan is minimized . we present the cbm ( conflict-based min-cost-flow ) algorithm , a hierarchical algorithm that solves tapf instances optimally by combining ideas from anonymous and non-anonymous multi-agent path-finding algorithms . on the low level , cbm uses a min-cost max-flow algorithm on a time-expanded network to assign all agents in a single team to targets and plan their paths . on the high level , cbm uses conflict-based search to resolve collisions among agents in different teams . theoretically , we prove that cbm is correct , complete and optimal . experimentally , we show the scalability of cbm to tapf instances with dozens of teams and hundreds of agents and adapt it to a simulated warehouse system ."}
{"title": "new ideas for brain modelling", "abstract": "this paper describes some biologically-inspired processes that could be used to build the sort of networks that we associate with the human brain . new to this paper , a 'refined ' neuron will be proposed . this is a group of neurons that by joining together can produce a more analogue system , but with the same level of control and reliability that a binary neuron would have . with this new structure , it will be possible to think of an essentially binary system in terms of a more variable set of values . the paper also shows how recent research associated with the new model , can be combined with established theories , to produce a more complete picture . the propositions are largely in line with conventional thinking , but possibly with one or two more radical suggestions . an earlier cognitive model can be filled in with more specific details , based on the new research results , where the components appear to fit together almost seamlessly . the intention of the research has been to describe plausible 'mechanical ' processes that can produce the appropriate brain structures and mechanisms , but that could be used without the magical 'intelligence ' part that is still not fully understood . there are also some important updates from an earlier version of this paper ."}
{"title": "recursive decomposition for nonconvex optimization", "abstract": "continuous optimization is an important problem in many areas of ai , including vision , robotics , probabilistic inference , and machine learning . unfortunately , most real-world optimization problems are nonconvex , causing standard convex techniques to find only local optima , even with extensions like random restarts and simulated annealing . we observe that , in many cases , the local modes of the objective function have combinatorial structure , and thus ideas from combinatorial optimization can be brought to bear . based on this , we propose a problem-decomposition approach to nonconvex optimization . similarly to dpll-style sat solvers and recursive conditioning in probabilistic inference , our algorithm , rdis , recursively sets variables so as to simplify and decompose the objective function into approximately independent sub-functions , until the remaining functions are simple enough to be optimized by standard techniques like gradient descent . the variables to set are chosen by graph partitioning , ensuring decomposition whenever possible . we show analytically that rdis can solve a broad class of nonconvex optimization problems exponentially faster than gradient descent with random restarts . experimentally , rdis outperforms standard techniques on problems like structure from motion and protein folding ."}
{"title": "rooting opinions in the minds : a cognitive model and a formal account of opinions and their dynamics", "abstract": "the study of opinions , their formation and change , is one of the defining topics addressed by social psychology , but in recent years other disciplines , like computer science and complexity , have tried to deal with this issue . despite the flourishing of different models and theories in both fields , several key questions still remain unanswered . the understanding of how opinions change and the way they are affected by social influence are challenging issues requiring a thorough analysis of opinion per se but also of the way in which they travel between agents ' minds and are modulated by these exchanges . to account for the two-faceted nature of opinions , which are mental entities undergoing complex social processes , we outline a preliminary model in which a cognitive theory of opinions is put forward and it is paired with a formal description of them and of their spreading among minds . furthermore , investigating social influence also implies the necessity to account for the way in which people change their minds , as a consequence of interacting with other people , and the need to explain the higher or lower persistence of such changes ."}
{"title": "optimizing the cvar via sampling", "abstract": "conditional value at risk ( cvar ) is a prominent risk measure that is being used extensively in various domains . we develop a new formula for the gradient of the cvar in the form of a conditional expectation . based on this formula , we propose a novel sampling-based estimator for the cvar gradient , in the spirit of the likelihood-ratio method . we analyze the bias of the estimator , and prove the convergence of a corresponding stochastic gradient descent algorithm to a local cvar optimum . our method allows to consider cvar optimization in new domains . as an example , we consider a reinforcement learning application , and learn a risk-sensitive controller for the game of tetris ."}
{"title": "exploiting resolution-based representations for maxsat solving", "abstract": "most recent maxsat algorithms rely on a succession of calls to a sat solver in order to find an optimal solution . in particular , several algorithms take advantage of the ability of sat solvers to identify unsatisfiable subformulas . usually , these maxsat algorithms perform better when small unsatisfiable subformulas are found early . however , this is not the case in many problem instances , since the whole formula is given to the sat solver in each call . in this paper , we propose to partition the maxsat formula using a resolution-based graph representation . partitions are then iteratively joined by using a proximity measure extracted from the graph representation of the formula . the algorithm ends when only one partition remains and the optimal solution is found . experimental results show that this new approach further enhances a state of the art maxsat solver to optimally solve a larger set of industrial problem instances ."}
{"title": "automatic fado music classification", "abstract": "in late 2011 , fado was elevated to the oral and intangible heritage of humanity by unesco . this study aims to develop a tool for automatic detection of fado music based on the audio signal . to do this , frequency spectrum-related characteristics were captured form the audio signal : in addition to the mel frequency cepstral coefficients ( mfccs ) and the energy of the signal , the signal was further analysed in two frequency ranges , providing additional information . tests were run both in a 10-fold cross-validation setup ( 97.6 % accuracy ) , and in a traditional train/test setup ( 95.8 % accuracy ) . the good results reflect the fact that fado is a very distinctive musical style ."}
{"title": "combinatorial multi-armed bandits for real-time strategy games", "abstract": "games with large branching factors pose a significant challenge for game tree search algorithms . in this paper , we address this problem with a sampling strategy for monte carlo tree search ( mcts ) algorithms called { \\em na\\ '' { i } ve sampling } , based on a variant of the multi-armed bandit problem called { \\em combinatorial multi-armed bandits } ( cmab ) . we analyze the theoretical properties of several variants of { \\em na\\ '' { i } ve sampling } , and empirically compare it against the other existing strategies in the literature for cmabs . we then evaluate these strategies in the context of real-time strategy ( rts ) games , a genre of computer games characterized by their very large branching factors . our results show that as the branching factor grows , { \\em na\\ '' { i } ve sampling } outperforms the other sampling strategies ."}
{"title": "name strategy : its existence and implications", "abstract": "it is argued that colour name strategy , object name strategy , and chunking strategy in memory are all aspects of the same general phenomena , called stereotyping . it is pointed out that the berlin-kay universal partial ordering of colours and the frequency of traffic accidents classified by colour are surprisingly similar . some consequences of the existence of a name strategy for the philosophy of language and mathematics are discussed . it is argued that real valued quantities occur { \\it ab initio } . the implication of real valued truth quantities is that the { \\bf continuum hypothesis } of pure mathematics is side-stepped . the existence of name strategy shows that thought/sememes and talk/phonemes can be separate , and this vindicates the assumption of thought occurring before talk used in psycholinguistic speech production models ."}
{"title": "verifying properties of binarized deep neural networks", "abstract": "understanding properties of deep neural networks is an important challenge in deep learning . in this paper , we take a step in this direction by proposing a rigorous way of verifying properties of a popular class of neural networks , binarized neural networks , using the well-developed means of boolean satisfiability . our main contribution is a construction that creates a representation of a binarized neural network as a boolean formula . our encoding is the first exact boolean representation of a deep neural network . using this encoding , we leverage the power of modern sat solvers along with a proposed counterexample-guided search procedure to verify various properties of these networks . a particular focus will be on the critical property of robustness to adversarial perturbations . for this property , our experimental results demonstrate that our approach scales to medium-size deep neural networks used in image classification tasks . to the best of our knowledge , this is the first work on verifying properties of deep neural networks using an exact boolean encoding of the network ."}
{"title": "simple regret optimization in online planning for markov decision processes", "abstract": "we consider online planning in markov decision processes ( mdps ) . in online planning , the agent focuses on its current state only , deliberates about the set of possible policies from that state onwards and , when interrupted , uses the outcome of that exploratory deliberation to choose what action to perform next . the performance of algorithms for online planning is assessed in terms of simple regret , which is the agent 's expected performance loss when the chosen action , rather than an optimal one , is followed . to date , state-of-the-art algorithms for online planning in general mdps are either best effort , or guarantee only polynomial-rate reduction of simple regret over time . here we introduce a new monte-carlo tree search algorithm , brue , that guarantees exponential-rate reduction of simple regret and error probability . this algorithm is based on a simple yet non-standard state-space sampling scheme , mcts2e , in which different parts of each sample are dedicated to different exploratory objectives . our empirical evaluation shows that brue not only provides superior performance guarantees , but is also very effective in practice and favorably compares to state-of-the-art . we then extend brue with a variant of `` learning by forgetting . '' the resulting set of algorithms , brue ( alpha ) , generalizes brue , improves the exponential factor in the upper bound on its reduction rate , and exhibits even more attractive empirical performance ."}
{"title": "a polynomial algorithm for computing the optimal repair strategy in a system with independent component failures", "abstract": "the goal of diagnosis is to compute good repair strategies in response to anomalous system behavior . in a decision theoretic framework , a good repair strategy has low expected cost . in a general formulation of the problem , the computation of the optimal ( lowest expected cost ) repair strategy for a system with multiple faults is intractable . in this paper , we consider an interesting and natural restriction on the behavior of the system being diagnosed : ( a ) the system exhibits faulty behavior if and only if one or more components is malfunctioning . ( b ) the failures of the system components are independent . given this restriction on system behavior , we develop a polynomial time algorithm for computing the optimal repair strategy . we then go on to introduce a system hierarchy and the notion of inspecting ( testing ) components before repair . we develop a linear time algorithm for computing an optimal repair strategy for the hierarchical system which includes both repair and inspection ."}
{"title": "closing the learning-planning loop with predictive state representations", "abstract": "a central problem in artificial intelligence is that of planning to maximize future reward under uncertainty in a partially observable environment . in this paper we propose and demonstrate a novel algorithm which accurately learns a model of such an environment directly from sequences of action-observation pairs . we then close the loop from observations to actions by planning in the learned model and recovering a policy which is near-optimal in the original environment . specifically , we present an efficient and statistically consistent spectral algorithm for learning the parameters of a predictive state representation ( psr ) . we demonstrate the algorithm by learning a model of a simulated high-dimensional , vision-based mobile robot planning task , and then perform approximate point-based planning in the learned psr . analysis of our results shows that the algorithm learns a state space which efficiently captures the essential features of the environment . this representation allows accurate prediction with a small number of parameters , and enables successful and efficient planning ."}
{"title": "formal concept analysis and resolution in algebraic domains", "abstract": "we relate two formerly independent areas : formal concept analysis and logic of domains . we will establish a correspondene between contextual attribute logic on formal contexts resp . concept lattices and a clausal logic on coherent algebraic cpos . we show how to identify the notion of formal concept in the domain theoretic setting . in particular , we show that a special instance of the resolution rule from the domain logic coincides with the concept closure operator from formal concept analysis . the results shed light on the use of contexts and domains for knowledge representation and reasoning purposes ."}
{"title": "on the definition of a confounder", "abstract": "the causal inference literature has provided a clear formal definition of confounding expressed in terms of counterfactual independence . the literature has not , however , come to any consensus on a formal definition of a confounder , as it has given priority to the concept of confounding over that of a confounder . we consider a number of candidate definitions arising from various more informal statements made in the literature . we consider the properties satisfied by each candidate definition , principally focusing on ( i ) whether under the candidate definition control for all `` confounders '' suffices to control for `` confounding '' and ( ii ) whether each confounder in some context helps eliminate or reduce confounding bias . several of the candidate definitions do not have these two properties . only one candidate definition of those considered satisfies both properties . we propose that a `` confounder '' be defined as a pre-exposure covariate c for which there exists a set of other covariates x such that effect of the exposure on the outcome is unconfounded conditional on ( x , c ) but such that for no proper subset of ( x , c ) is the effect of the exposure on the outcome unconfounded given the subset . we also provide a conditional analogue of the above definition ; and we propose a variable that helps reduce bias but not eliminate bias be referred to as a `` surrogate confounder . '' these definitions are closely related to those given by robins and morgenstern [ comput . math . appl . 14 ( 1987 ) 869-916 ] . the implications that hold among the various candidate definitions are discussed ."}
{"title": "towards arrow-theoretic semantics of ontologies : conceptories", "abstract": "in context of efforts of composing category-theoretic and logical methods in the area of knowledge representation we propose the notion of conceptory . we consider intersection/union and other constructions in conceptories as expressive alternative to category-theoretic ( co ) limits and show they have features similar to ( pro- , in- ) jections . then we briefly discuss approaches to development of formal systems built on the base of conceptories and describe possible application of such system to the specific ontology ."}
{"title": "roborobo ! a fast robot simulator for swarm and collective robotics", "abstract": "roborobo ! is a multi-platform , highly portable , robot simulator for large-scale collective robotics experiments . roborobo ! is coded in c++ , and follows the kiss guideline ( `` keep it simple '' ) . therefore , its external dependency is solely limited to the widely available sdl library for fast 2d graphics . roborobo ! is based on a khepera/epuck model . it is targeted for fast single and multi-robots simulation , and has already been used in more than a dozen published research mainly concerned with evolutionary swarm robotics , including environment-driven self-adaptation and distributed evolutionary optimization , as well as online onboard embodied evolution and embodied morphogenesis ."}
{"title": "resource-optimal planning for an autonomous planetary vehicle", "abstract": "autonomous planetary vehicles , also known as rovers , are small autonomous vehicles equipped with a variety of sensors used to perform exploration and experiments on a planet 's surface . rovers work in a partially unknown environment , with narrow energy/time/movement constraints and , typically , small computational resources that limit the complexity of on-line planning and scheduling , thus they represent a great challenge in the field of autonomous vehicles . indeed , formal models for such vehicles usually involve hybrid systems with nonlinear dynamics , which are difficult to handle by most of the current planning algorithms and tools . therefore , when offline planning of the vehicle activities is required , for example for rovers that operate without a continuous earth supervision , such planning is often performed on simplified models that are not completely realistic . in this paper we show how the upmurphi model checking based planning tool can be used to generate resource-optimal plans to control the engine of an autonomous planetary vehicle , working directly on its hybrid model and taking into account several safety constraints , thus achieving very accurate results ."}
{"title": "a deep network model for paraphrase detection in short text messages", "abstract": "this paper is concerned with paraphrase detection . the ability to detect similar sentences written in natural language is crucial for several applications , such as text mining , text summarization , plagiarism detection , authorship authentication and question answering . given two sentences , the objective is to detect whether they are semantically identical . an important insight from this work is that existing paraphrase systems perform well when applied on clean texts , but they do not necessarily deliver good performance against noisy texts . challenges with paraphrase detection on user generated short texts , such as twitter , include language irregularity and noise . to cope with these challenges , we propose a novel deep neural network-based approach that relies on coarse-grained sentence modeling using a convolutional neural network and a long short-term memory model , combined with a specific fine-grained word-level similarity matching model . our experimental results show that the proposed approach outperforms existing state-of-the-art approaches on user-generated noisy social media data , such as twitter texts , and achieves highly competitive performance on a cleaner corpus ."}
{"title": "quantifying mental health from social media with neural user embeddings", "abstract": "mental illnesses adversely affect a significant proportion of the population worldwide . however , the methods traditionally used for estimating and characterizing the prevalence of mental health conditions are time-consuming and expensive . consequently , best-available estimates concerning the prevalence of mental health conditions are often years out of date . automated approaches to supplement these survey methods with broad , aggregated information derived from social media content provides a potential means for near real-time estimates at scale . these may , in turn , provide grist for supporting , evaluating and iteratively improving upon public health programs and interventions . we propose a novel model for automated mental health status quantification that incorporates user embeddings . this builds upon recent work exploring representation learning methods that induce embeddings by leveraging social media post histories . such embeddings capture latent characteristics of individuals ( e.g. , political leanings ) and encode a soft notion of homophily . in this paper , we investigate whether user embeddings learned from twitter post histories encode information that correlates with mental health statuses . to this end , we estimated user embeddings for a set of users known to be affected by depression and post-traumatic stress disorder ( ptsd ) , and for a set of demographically matched ` control ' users . we then evaluated these embeddings with respect to : ( i ) their ability to capture homophilic relations with respect to mental health status ; and ( ii ) the performance of downstream mental health prediction models based on these features . our experimental results demonstrate that the user embeddings capture similarities between users with respect to mental conditions , and are predictive of mental health ."}
{"title": "adaptive submodular optimization under matroid constraints", "abstract": "many important problems in discrete optimization require maximization of a monotonic submodular function subject to matroid constraints . for these problems , a simple greedy algorithm is guaranteed to obtain near-optimal solutions . in this article , we extend this classic result to a general class of adaptive optimization problems under partial observability , where each choice can depend on observations resulting from past choices . specifically , we prove that a natural adaptive greedy algorithm provides a $ 1/ ( p+1 ) $ approximation for the problem of maximizing an adaptive monotone submodular function subject to $ p $ matroid constraints , and more generally over arbitrary $ p $ -independence systems . we illustrate the usefulness of our result on a complex adaptive match-making application ."}
{"title": "model counting in product configuration", "abstract": "we describe how to use propositional model counting for a quantitative analysis of product configuration data . our approach computes valuable meta information such as the total number of valid configurations or the relative frequency of components . this information can be used to assess the severity of documentation errors or to measure documentation quality . as an application example we show how we apply these methods to product documentation formulas of the mercedes-benz line of vehicles . in order to process these large formulas we developed and implemented a new model counter for non-cnf formulas . our model counter can process formulas , whose cnf representations could not be processed up till now ."}
{"title": "fgpga : an efficient genetic approach for producing feasible graph partitions", "abstract": "graph partitioning , a well studied problem of parallel computing has many applications in diversified fields such as distributed computing , social network analysis , data mining and many other domains . in this paper , we introduce fgpga , an efficient genetic approach for producing feasible graph partitions . our method takes into account the heterogeneity and capacity constraints of the partitions to ensure balanced partitioning . such approach has various applications in mobile cloud computing that include feasible deployment of software applications on the more resourceful infrastructure in the cloud instead of mobile hand set . our proposed approach is light weight and hence suitable for use in cloud architecture . we ensure feasibility of the partitions generated by not allowing over-sized partitions to be generated during the initialization and search . our proposed method tested on standard benchmark datasets significantly outperforms the state-of-the-art methods in terms of quality of partitions and feasibility of the solutions ."}
{"title": "intelligent indoor mobile robot navigation using stereo vision", "abstract": "majority of the existing robot navigation systems , which facilitate the use of laser range finders , sonar sensors or artificial landmarks , has the ability to locate itself in an unknown environment and then build a map of the corresponding environment . stereo vision , while still being a rapidly developing technique in the field of autonomous mobile robots , are currently less preferable due to its high implementation cost . this paper aims at describing an experimental approach for the building of a stereo vision system that helps the robots to avoid obstacles and navigate through indoor environments and at the same time remaining very much cost effective . this paper discusses the fusion techniques of stereo vision and ultrasound sensors which helps in the successful navigation through different types of complex environments . the data from the sensor enables the robot to create the two dimensional topological map of unknown environments and stereo vision systems models the three dimension model of the same environment ."}
{"title": "a neural network approach to context-sensitive generation of conversational responses", "abstract": "we present a novel response generation system that can be trained end to end on large quantities of unstructured twitter conversations . a neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models , allowing the system to take into account previous dialog utterances . our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive machine translation and information retrieval baselines ."}
{"title": "efficient tabling mechanisms for transaction logic programs", "abstract": "in this paper we present efficient evaluation algorithms for the horn transaction logic ( a generalization of the regular horn logic programs with state updates ) . we present two complementary methods for optimizing the implementation of transaction logic . the first method is based on tabling and we modified the proof theory to table calls and answers on states ( practically , equivalent to dynamic programming ) . the call-answer table is indexed on the call and a signature of the state in which the call was made . the answer columns contain the answer unification and a signature of the state after the call was executed . the states are signed efficiently using a technique based on tries and counting . the second method is based on incremental evaluation and it applies when the data oracle contains derived relations . the deletions and insertions ( executed in the transaction oracle ) change the state of the database . using the heuristic of inertia ( only a part of the state changes in response to elementary updates ) , most of the time it is cheaper to compute only the changes in the state than to recompute the entire state from scratch . the two methods are complementary by the fact that the first method optimizes the evaluation when a call is repeated in the same state , and the second method optimizes the evaluation of a new state when a call-state pair is not found by the tabling mechanism ( i.e . the first method ) . the proof theory of transaction logic with the application of tabling and incremental evaluation is sound and complete with respect to its model theory ."}
{"title": "a general methodology for the determination of 2d bodies elastic deformation invariants . application to the automatic identification of parasites", "abstract": "a novel methodology is introduced here that exploits 2d images of arbitrary elastic body deformation instances , so as to quantify mechano-elastic characteristics that are deformation invariant . determination of such characteristics allows for developing methods offering an image of the undeformed body . general assumptions about the mechano-elastic properties of the bodies are stated , which lead to two different approaches for obtaining bodies ' deformation invariants . one was developed to spot deformed body 's neutral line and its cross sections , while the other solves deformation pdes by performing a set of equivalent image operations on the deformed body images . both these processes may furnish a body undeformed version from its deformed image . this was confirmed by obtaining the undeformed shape of deformed parasites , cells ( protozoa ) , fibers and human lips . in addition , the method has been applied to the important problem of parasite automatic classification from their microscopic images . to achieve this , we first apply the previous method to straighten the highly deformed parasites and then we apply a dedicated curve classification method to the straightened parasite contours . it is demonstrated that essentially different deformations of the same parasite give rise to practically the same undeformed shape , thus confirming the consistency of the introduced methodology . finally , the developed pattern recognition method classifies the unwrapped parasites into 6 families , with an accuracy rate of 97.6 % ."}
{"title": "data granulation by the principles of uncertainty", "abstract": "researches in granular modeling produced a variety of mathematical models , such as intervals , ( higher-order ) fuzzy sets , rough sets , and shadowed sets , which are all suitable to characterize the so-called information granules . modeling of the input data uncertainty is recognized as a crucial aspect in information granulation . moreover , the uncertainty is a well-studied concept in many mathematical settings , such as those of probability theory , fuzzy set theory , and possibility theory . this fact suggests that an appropriate quantification of the uncertainty expressed by the information granule model could be used to define an invariant property , to be exploited in practical situations of information granulation . in this perspective , a procedure of information granulation is effective if the uncertainty conveyed by the synthesized information granule is in a monotonically increasing relation with the uncertainty of the input data . in this paper , we present a data granulation framework that elaborates over the principles of uncertainty introduced by klir . being the uncertainty a mesoscopic descriptor of systems and data , it is possible to apply such principles regardless of the input data type and the specific mathematical setting adopted for the information granules . the proposed framework is conceived ( i ) to offer a guideline for the synthesis of information granules and ( ii ) to build a groundwork to compare and quantitatively judge over different data granulation procedures . to provide a suitable case study , we introduce a new data granulation technique based on the minimum sum of distances , which is designed to generate type-2 fuzzy sets . we analyze the procedure by performing different experiments on two distinct data types : feature vectors and labeled graphs . results show that the uncertainty of the input data is suitably conveyed by the generated type-2 fuzzy set models ."}
{"title": "gated-attention architectures for task-oriented language grounding", "abstract": "to perform tasks specified by natural language instructions , autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment . this problem is called task-oriented language grounding . we propose an end-to-end trainable neural architecture for task-oriented language grounding in 3d environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input . the proposed model combines the image and text representations using a gated-attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods . we show the effectiveness of the proposed model on unseen instructions as well as unseen maps , both quantitatively and qualitatively . we also introduce a novel environment based on a 3d game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states ."}
{"title": "abductive reasoning as the basis to reproduce expert criteria in ecg atrial fibrillation identification", "abstract": "objective : this work aims at providing a new method for the automatic detection of atrial fibrillation , other arrhythmia and noise on short single lead ecg signals , emphasizing the importance of the interpretability of the classification results . approach : a morphological and rhythm description of the cardiac behavior is obtained by a knowledge-based interpretation of the signal using the \\textit { construe } abductive framework . then , a set of meaningful features are extracted for each individual heartbeat and as a summary of the full record . the feature distributions were used to elucidate the expert criteria underlying the labeling of the 2017 physionet/cinc challenge dataset , enabling a manual partial relabeling to improve the consistency of the classification rules . finally , state-of-the-art machine learning methods are combined to provide an answer on the basis of the feature values . main results : the proposal tied for the first place in the official stage of the challenge , with a combined $ f_1 $ score of 0.83 , and was even improved in the follow-up stage to 0.85 with a significant simplification of the model . significance : this approach demonstrates the potential of \\textit { construe } to provide robust and valuable descriptions of temporal data even with significant amounts of noise and artifacts . also , we discuss the importance of a consistent classification criteria in manually labeled training datasets , and the fundamental advantages of knowledge-based approaches to formalize and validate that criteria ."}
{"title": "the initial conditions of the universe from constrained simulations", "abstract": "i present a new approach to recover the primordial density fluctuations and the cosmic web structure underlying a galaxy distribution . the method is based on sampling gaussian fields which are compatible with a galaxy distribution and a structure formation model . this is achieved by splitting the inversion problem into two gibbs-sampling steps : the first being a gaussianisation step transforming a distribution of point sources at lagrangian positions -which are not a priori given- into a linear alias-free gaussian field . this step is based on hamiltonian sampling with a gaussian-poisson model . the second step consists on a likelihood comparison in which the set of matter tracers at the initial conditions is constrained on the galaxy distribution and the assumed structure formation model . for computational reasons second order lagrangian perturbation theory is used . however , the presented approach is flexible to adopt any structure formation model . a semi-analytic halo-model based galaxy mock catalog is taken to demonstrate that the recovered initial conditions are closely unbiased with respect to the actual ones from the corresponding n-body simulation down to scales of a ~ 5 mpc/h . the cross-correlation between them shows a substantial gain of information , being at k ~ 0.3 h/mpc more than doubled . in addition the initial conditions are extremely well gaussian distributed and the power-spectra follow the shape of the linear power-spectrum being very close to the actual one from the simulation down to scales of k ~ 1 h/mpc ."}
{"title": "an evolutionary algorithm for error-driven learning via reinforcement", "abstract": "although different learning systems are coordinated to afford complex behavior , little is known about how this occurs . this article describes a theoretical framework that specifies how complex behaviors that might be thought to require error-driven learning might instead be acquired through simple reinforcement . this framework includes specific assumptions about the mechanisms that contribute to the evolution of ( artificial ) neural networks to generate topologies that allow the networks to learn large-scale complex problems using only information about the quality of their performance . the practical and theoretical implications of the framework are discussed , as are possible biological analogs of the approach ."}
{"title": "performing bayesian risk aggregation using discrete approximation algorithms with graph factorization", "abstract": "risk aggregation is a popular method used to estimate the sum of a collection of financial assets or events , where each asset or event is modelled as a random variable . applications , in the financial services industry , include insurance , operational risk , stress testing , and sensitivity analysis , but the problem is widely encountered in many other application domains . this thesis has contributed two algorithms to perform bayesian risk aggregation when model exhibit hybrid dependency and high dimensional inter-dependency . the first algorithm operates on a subset of the general problem , with an emphasis on convolution problems , in the presence of continuous and discrete variables ( so called hybrid models ) and the second algorithm offer a universal method for general purpose inference over much wider classes of bayesian network models ."}
{"title": "multiobjective optimization of solar powered irrigation system with fuzzy type-2 noise modelling", "abstract": "optimization is becoming a crucial element in industrial applications involving sustainable alternative energy systems . during the design of such systems , the engineer/decision maker would often encounter noise factors ( e.g . solar insolation and ambient temperature fluctuations ) when their system interacts with the environment . in this chapter , the sizing and design optimization of the solar powered irrigation system was considered . this problem is multivariate , noisy , nonlinear and multiobjective . this design problem was tackled by first using the fuzzy type ii approach to model the noise factors . consequently , the bacterial foraging algorithm ( bfa ) ( in the context of a weighted sum framework ) was employed to solve this multiobjective fuzzy design problem . this method was then used to construct the approximate pareto frontier as well as to identify the best solution option in a fuzzy setting . comprehensive analyses and discussions were performed on the generated numerical results with respect to the implemented solution methods ."}
{"title": "set constraint model and automated encoding into sat : application to the social golfer problem", "abstract": "on the one hand , constraint satisfaction problems allow one to declaratively model problems . on the other hand , propositional satisfiability problem ( sat ) solvers can handle huge sat instances . we thus present a technique to declaratively model set constraint problems and to encode them automatically into sat instances . we apply our technique to the social golfer problem and we also use it to break symmetries of the problem . our technique is simpler , more declarative , and less error-prone than direct and improved hand modeling . the sat instances that we automatically generate contain less clauses than improved hand-written instances such as in [ 20 ] , and with unit propagation they also contain less variables . moreover , they are well-suited for sat solvers and they are solved faster as shown when solving difficult instances of the social golfer problem ."}
{"title": "storm - a novel information fusion and cluster interpretation technique", "abstract": "analysis of data without labels is commonly subject to scrutiny by unsupervised machine learning techniques . such techniques provide more meaningful representations , useful for better understanding of a problem at hand , than by looking only at the data itself . although abundant expert knowledge exists in many areas where unlabelled data is examined , such knowledge is rarely incorporated into automatic analysis . incorporation of expert knowledge is frequently a matter of combining multiple data sources from disparate hypothetical spaces . in cases where such spaces belong to different data types , this task becomes even more challenging . in this paper we present a novel immune-inspired method that enables the fusion of such disparate types of data for a specific set of problems . we show that our method provides a better visual understanding of one hypothetical space with the help of data from another hypothetical space . we believe that our model has implications for the field of exploratory data analysis and knowledge discovery ."}
{"title": "funnel libraries for real-time robust feedback motion planning", "abstract": "we consider the problem of generating motion plans for a robot that are guaranteed to succeed despite uncertainty in the environment , parametric model uncertainty , and disturbances . furthermore , we consider scenarios where these plans must be generated in real-time , because constraints such as obstacles in the environment may not be known until they are perceived ( with a noisy sensor ) at runtime . our approach is to pre-compute a library of `` funnels '' along different maneuvers of the system that the state is guaranteed to remain within ( despite bounded disturbances ) when the feedback controller corresponding to the maneuver is executed . we leverage powerful computational machinery from convex optimization ( sums-of-squares programming in particular ) to compute these funnels . the resulting funnel library is then used to sequentially compose motion plans at runtime while ensuring the safety of the robot . a major advantage of the work presented here is that by explicitly taking into account the effect of uncertainty , the robot can evaluate motion plans based on how vulnerable they are to disturbances . we demonstrate and validate our method using extensive hardware experiments on a small fixed-wing airplane avoiding obstacles at high speed ( ~12 mph ) , along with thorough simulation experiments of ground vehicle and quadrotor models navigating through cluttered environments . to our knowledge , these demonstrations constitute one of the first examples of provably safe and robust control for robotic systems with complex nonlinear dynamics that need to plan in real-time in environments with complex geometric constraints ."}
{"title": "a comparative study of meta-heuristic algorithms for solving quadratic assignment problem", "abstract": "quadratic assignment problem ( qap ) is an np-hard combinatorial optimization problem , therefore , solving the qap requires applying one or more of the meta-heuristic algorithms . this paper presents a comparative study between meta-heuristic algorithms : genetic algorithm , tabu search , and simulated annealing for solving a real-life ( qap ) and analyze their performance in terms of both runtime efficiency and solution quality . the results show that genetic algorithm has a better solution quality while tabu search has a faster execution time in comparison with other meta-heuristic algorithms for solving qap ."}
{"title": "reaching unanimous agreements within agent-based negotiation teams with linear and monotonic utility functions", "abstract": "in this article , an agent-based negotiation model for negotiation teams that negotiate a deal with an opponent is presented . agent-based negotiation teams are groups of agents that join together as a single negotiation party because they share an interest that is related to the negotiation process . the model relies on a trusted mediator that coordinates and helps team members in the decisions that they have to take during the negotiation process : which offer is sent to the opponent , and whether the offers received from the opponent are accepted . the main strength of the proposed negotiation model is the fact that it guarantees unanimity within team decisions since decisions report a utility to team members that is greater than or equal to their aspiration levels at each negotiation round . this work analyzes how unanimous decisions are taken within the team and the robustness of the model against different types of manipulations . an empirical evaluation is also performed to study the impact of the different parameters of the model ."}
{"title": "the n-tuple bandit evolutionary algorithm for automatic game improvement", "abstract": "this paper describes a new evolutionary algorithm that is especially well suited to ai-assisted game design . the approach adopted in this paper is to use observations of ai agents playing the game to estimate the game 's quality . some of best agents for this purpose are general video game ai agents , since they can be deployed directly on a new game without game-specific tuning ; these agents tend to be based on stochastic algorithms which give robust but noisy results and tend to be expensive to run . this motivates the main contribution of the paper : the development of the novel n-tuple bandit evolutionary algorithm , where a model is used to estimate the fitness of unsampled points and a bandit approach is used to balance exploration and exploitation of the search space . initial results on optimising a space battle game variant suggest that the algorithm offers far more robust results than the random mutation hill climber and a biased mutation variant , which are themselves known to offer competitive performance across a range of problems . subjective observations are also given by human players on the nature of the evolved games , which indicate a preference towards games generated by the n-tuple algorithm ."}
{"title": "neural programming by example", "abstract": "programming by example ( pbe ) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output . in this paper , we propose a deep neural networks ( dnn ) based pbe model called neural programming by example ( npbe ) , which can learn from input-output strings and induce programs that solve the string manipulation problems . our npbe model has four neural network based components : a string encoder , an input-output analyzer , a program generator , and a symbol selector . we demonstrate the effectiveness of npbe by training it end-to-end to solve some common string manipulation problems in spreadsheet systems . the results show that our model can induce string manipulation programs effectively . our work is one step towards teaching dnn to generate computer programs ."}
{"title": "on quantum decision trees", "abstract": "quantum decision systems are being increasingly considered for use in artificial intelligence applications . classical and quantum nodes can be distinguished based on certain correlations in their states . this paper investigates some properties of the states obtained in a decision tree structure . how these correlations may be mapped to the decision tree is considered . classical tree representations and approximations to quantum states are provided ."}
{"title": "double deep machine learning", "abstract": "very important breakthroughs in data-centric machine learning algorithms led to impressive performance in transactional point applications such as detecting anger in speech , alerts from a face recognition system , or ekg interpretation . non-transactional applications , e.g . medical diagnosis beyond the ekg results , require ai algorithms that integrate deeper and broader knowledge in their problem-solving capabilities , e.g . integrating knowledge about anatomy and physiology of the heart with ekg results and additional patient findings . similarly , for military aerial interpretation , where knowledge about enemy doctrines on force composition and spread helps immensely in situation assessment beyond image recognition of individual objects . the double deep learning approach advocates integrating data-centric machine self-learning techniques with machine-teaching techniques to leverage the power of both and overcome their corresponding limitations . to take ai to the next level , it is essential that we rebalance the roles of data and knowledge . data is important but knowledge- deep and commonsense- are equally important . an initiative is proposed to build wikipedia for smart machines , meaning target readers are not human , but rather smart machines . named rekopedia , the goal is to develop methodologies , tools , and automatic algorithms to convert humanity knowledge that we all learn in schools , universities and during our professional life into reusable knowledge structures that smart machines can use in their inference algorithms . ideally , rekopedia would be an open source shared knowledge repository similar to the well-known shared open source software code repositories . examples in the article are based on- or inspired by- real-life non-transactional ai systems i deployed over decades of ai career that benefit hundreds of millions of people around the globe ."}
{"title": "budget-constrained multi-armed bandits with multiple plays", "abstract": "we study the multi-armed bandit problem with multiple plays and a budget constraint for both the stochastic and the adversarial setting . at each round , exactly $ k $ out of $ n $ possible arms have to be played ( with $ 1\\leq k \\leq n $ ) . in addition to observing the individual rewards for each arm played , the player also learns a vector of costs which has to be covered with an a-priori defined budget $ b $ . the game ends when the sum of current costs associated with the played arms exceeds the remaining budget . firstly , we analyze this setting for the stochastic case , for which we assume each arm to have an underlying cost and reward distribution with support $ [ c_ { \\min } , 1 ] $ and $ [ 0 , 1 ] $ , respectively . we derive an upper confidence bound ( ucb ) algorithm which achieves $ o ( nk^4 \\log b ) $ regret . secondly , for the adversarial case in which the entire sequence of rewards and costs is fixed in advance , we derive an upper bound on the regret of order $ o ( \\sqrt { nb\\log ( n/k ) } ) $ utilizing an extension of the well-known $ \\texttt { exp3 } $ algorithm . we also provide upper bounds that hold with high probability and a lower bound of order $ \\omega ( ( 1 - k/n ) ^2 \\sqrt { nb/k } ) $ ."}
{"title": "fuzzy logic classification of imaging laser desorption fourier transform mass spectrometry data", "abstract": "a fuzzy logic based classification engine has been developed for classifying mass spectra obtained with an imaging internal source fourier transform mass spectrometer ( i^2ld-ftms ) . traditionally , an operator uses the relative abundance of ions with specific mass-to-charge ( m/z ) ratios to categorize spectra . an operator does this by comparing the spectrum of m/z versus abundance of an unknown sample against a library of spectra from known samples . automated positioning and acquisition allow i^2ld-ftms to acquire data from very large grids , this would require classification of up to 3600 spectrum per hour to keep pace with the acquisition . the tedious job of classifying numerous spectra generated in an i^2ld-ftms imaging application can be replaced by a fuzzy rule base if the cues an operator uses can be encapsulated . we present the translation of linguistic rules to a fuzzy classifier for mineral phases in basalt . this paper also describes a method for gathering statistics on ions , which are not currently used in the rule base , but which may be candidates for making the rule base more accurate and complete or to form new rule bases based on data obtained from known samples . a spatial method for classifying spectra with low membership values , based on neighboring sample classifications , is also presented ."}
{"title": "learning low-density separators", "abstract": "we define a novel , basic , unsupervised learning problem - learning the lowest density homogeneous hyperplane separator of an unknown probability distribution . this task is relevant to several problems in machine learning , such as semi-supervised learning and clustering stability . we investigate the question of existence of a universally consistent algorithm for this problem . we propose two natural learning paradigms and prove that , on input unlabeled random samples generated by any member of a rich family of distributions , they are guaranteed to converge to the optimal separator for that distribution . we complement this result by showing that no learning algorithm for our task can achieve uniform learning rates ( that are independent of the data generating distribution ) ."}
{"title": "when you must forget : beyond strong persistence when forgetting in answer set programming", "abstract": "among the myriad of desirable properties discussed in the context of forgetting in answer set programming ( asp ) , strong persistence naturally captures its essence . recently , it has been shown that it is not always possible to forget a set of atoms from a program while obeying this property , and a precise criterion regarding what can be forgotten has been presented , accompanied by a class of forgetting operators that return the correct result when forgetting is possible . however , it is an open question what to do when we have to forget a set of atoms , but can not without violating this property . in this paper , we address this issue and investigate three natural alternatives to forget when forgetting without violating strong persistence is not possible , which turn out to correspond to the different possible relaxations of the characterization of strong persistence . additionally , we discuss their preferable usage , shed light on the relation between forgetting and notions of relativized equivalence established earlier in the context of asp , and present a detailed study on their computational complexity ."}
{"title": "path planning with kinematic constraints for robot groups", "abstract": "path planning for multiple robots is well studied in the ai and robotics communities . for a given discretized environment , robots need to find collision-free paths to a set of specified goal locations . robots can be fully anonymous , non-anonymous , or organized in groups . although powerful solvers for this abstract problem exist , they make simplifying assumptions by ignoring kinematic constraints , making it difficult to use the resulting plans on actual robots . in this paper , we present a solution which takes kinematic constraints , such as maximum velocities , into account , while guaranteeing a user-specified minimum safety distance between robots . we demonstrate our approach in simulation and on real robots in 2d and 3d environments ."}
{"title": "symphony from synapses : neocortex as a universal dynamical systems modeller using hierarchical temporal memory", "abstract": "reverse engineering the brain is proving difficult , perhaps impossible . while many believe that this is just a matter of time and effort , a different approach might help . here , we describe a very simple idea which explains the power of the brain as well as its structure , exploiting complex dynamics rather than abstracting it away . just as a turing machine is a universal digital computer operating in a world of symbols , we propose that the brain is a universal dynamical systems modeller , evolved bottom-up ( itself using nested networks of interconnected , self-organised dynamical systems ) to prosper in a world of dynamical systems . recent progress in applied mathematics has produced startling evidence of what happens when abstract dynamical systems interact . key latent information describing system a can be extracted by system b from very simple signals , and signals can be used by one system to control and manipulate others . using these facts , we show how a region of the neocortex uses its dynamics to intrinsically `` compute '' about the external and internal world . building on an existing `` static '' model of cortical computation ( hawkins ' hierarchical temporal memory - htm ) , we describe how a region of neocortex can be viewed as a network of components which together form a dynamical systems modelling module , connected via sensory and motor pathways to the external world , and forming part of a larger dynamical network in the brain . empirical modelling and simulations of dynamical htm are possible with simple extensions and combinations of currently existing open source software . we list a number of relevant projects ."}
{"title": "parameterized complexity results for a model of theory of mind based on dynamic epistemic logic", "abstract": "in this paper we introduce a computational-level model of theory of mind ( tom ) based on dynamic epistemic logic ( del ) , and we analyze its computational complexity . the model is a special case of del model checking . we provide a parameterized complexity analysis , considering several aspects of del ( e.g. , number of agents , size of preconditions , etc . ) as parameters . we show that model checking for del is pspace-hard , also when restricted to single-pointed models and s5 relations , thereby solving an open problem in the literature . our approach is aimed at formalizing current intractability claims in the cognitive science literature regarding computational models of tom ."}
{"title": "peek arc consistency", "abstract": "this paper studies peek arc consistency , a reasoning technique that extends the well-known arc consistency technique for constraint satisfaction . in contrast to other more costly extensions of arc consistency that have been studied in the literature , peek arc consistency requires only linear space and quadratic time and can be parallelized in a straightforward way such that it runs in linear time with a linear number of processors . we demonstrate that for various constraint languages , peek arc consistency gives a polynomial-time decision procedure for the constraint satisfaction problem . we also present an algebraic characterization of those constraint languages that can be solved by peek arc consistency , and study the robustness of the algorithm ."}
{"title": "parametric prediction from parametric agents", "abstract": "we consider a problem of prediction based on opinions elicited from heterogeneous rational agents with private information . making an accurate prediction with a minimal cost requires a joint design of the incentive mechanism and the prediction algorithm . such a problem lies at the nexus of statistical learning theory and game theory , and arises in many domains such as consumer surveys and mobile crowdsourcing . in order to elicit heterogeneous agents ' private information and incentivize agents with different capabilities to act in the principal 's best interest , we design an optimal joint incentive mechanism and prediction algorithm called cope ( cost and prediction elicitation ) , the analysis of which offers several valuable engineering insights . first , when the costs incurred by the agents are linear in the exerted effort , cope corresponds to a `` crowd contending '' mechanism , where the principal only employs the agent with the highest capability . second , when the costs are quadratic , cope corresponds to a `` crowd-sourcing '' mechanism that employs multiple agents with different capabilities at the same time . numerical simulations show that cope improves the principal 's profit and the network profit significantly ( larger than 30 % in our simulations ) , comparing to those mechanisms that assume all agents have equal capabilities ."}
{"title": "how , what and why to test an ontology", "abstract": "ontology development relates to software development in that they both involve the production of formal computational knowledge . it is possible , therefore , that some of the techniques used in software engineering could also be used for ontologies ; for example , in software engineering testing is a well-established process , and part of many different methodologies . the application of testing to ontologies , therefore , seems attractive . the karyotype ontology is developed using the novel tawny-owl library . this provides a fully programmatic environment for ontology development , which includes a complete test harness . in this paper , we describe how we have used this harness to build an extensive series of tests as well as used a commodity continuous integration system to link testing deeply into our development process ; this environment , is applicable to any owl ontology whether written using tawny-owl or not . moreover , we present a novel analysis of our tests , introducing a new classification of what our different tests are . for each class of test , we describe why we use these tests , also by comparison to software tests . we believe that this systematic comparison between ontology and software development will help us move to a more agile form of ontology development ."}
{"title": "epistemic protocols for distributed gossiping", "abstract": "gossip protocols aim at arriving , by means of point-to-point or group communications , at a situation in which all the agents know each other 's secrets . we consider distributed gossip protocols which are expressed by means of epistemic logic . we provide an operational semantics of such protocols and set up an appropriate framework to argue about their correctness . then we analyze specific protocols for complete graphs and for directed rings ."}
{"title": "incremental tradeoff resolution in qualitative probabilistic networks", "abstract": "qualitative probabilistic reasoning in a bayesian network often reveals tradeoffs : relationships that are ambiguous due to competing qualitative influences . we present two techniques that combine qualitative and numeric probabilistic reasoning to resolve such tradeoffs , inferring the qualitative relationship between nodes in a bayesian network . the first approach incrementally marginalizes nodes that contribute to the ambiguous qualitative relationships . the second approach evaluates approximate bayesian networks for bounds of probability distributions , and uses these bounds to determinate qualitative relationships in question . this approach is also incremental in that the algorithm refines the state spaces of random variables for tighter bounds until the qualitative relationships are resolved . both approaches provide systematic methods for tradeoff resolution at potentially lower computational cost than application of purely numeric methods ."}
{"title": "the self-organization of interaction networks for nature-inspired optimization", "abstract": "over the last decade , significant progress has been made in understanding complex biological systems , however there have been few attempts at incorporating this knowledge into nature inspired optimization algorithms . in this paper , we present a first attempt at incorporating some of the basic structural properties of complex biological systems which are believed to be necessary preconditions for system qualities such as robustness . in particular , we focus on two important conditions missing in evolutionary algorithm populations ; a self-organized definition of locality and interaction epistasis . we demonstrate that these two features , when combined , provide algorithm behaviors not observed in the canonical evolutionary algorithm or in evolutionary algorithms with structured populations such as the cellular genetic algorithm . the most noticeable change in algorithm behavior is an unprecedented capacity for sustainable coexistence of genetically distinct individuals within a single population . this capacity for sustained genetic diversity is not imposed on the population but instead emerges as a natural consequence of the dynamics of the system ."}
{"title": "using options and covariance testing for long horizon off-policy policy evaluation", "abstract": "evaluating a policy by deploying it in the real world can be risky and costly . off-policy policy evaluation ( ope ) algorithms use historical data collected from running a previous policy to evaluate a new policy , which provides a means for evaluating a policy without requiring it to ever be deployed . importance sampling is a popular ope method because it is robust to partial observability and works with continuous states and actions . however , the amount of historical data required by importance sampling can scale exponentially with the horizon of the problem : the number of sequential decisions that are made . we propose using policies over temporally extended actions , called options , and show that combining these policies with importance sampling can significantly improve performance for long-horizon problems . in addition , we can take advantage of special cases that arise due to options-based policies to further improve the performance of importance sampling . we further generalize these special cases to a general covariance testing rule that can be used to decide which weights to drop in an is estimate , and derive a new is algorithm called incremental importance sampling that can provide significantly more accurate estimates for a broad class of domains ."}
{"title": "combining answer set programming and pomdps for knowledge representation and reasoning on mobile robots", "abstract": "for widespread deployment in domains characterized by partial observability , non-deterministic actions and unforeseen changes , robots need to adapt sensing , processing and interaction with humans to the tasks at hand . while robots typically can not process all sensor inputs or operate without substantial domain knowledge , it is a challenge to provide accurate domain knowledge and humans may not have the time and expertise to provide elaborate and accurate feedback . the architecture described in this paper combines declarative programming and probabilistic reasoning to address these challenges , enabling robots to : ( a ) represent and reason with incomplete domain knowledge , resolving ambiguities and revising existing knowledge using sensor inputs and minimal human feedback ; and ( b ) probabilistically model the uncertainty in sensor input processing and navigation . specifically , answer set programming ( asp ) , a declarative programming paradigm , is combined with hierarchical partially observable markov decision processes ( pomdps ) , using domain knowledge to revise probabilistic beliefs , and using positive and negative observations for early termination of tasks that can no longer be pursued . all algorithms are evaluated in simulation and on mobile robots locating target objects in indoor domains ."}
{"title": "a framework for intelligent medical diagnosis using rough set with formal concept analysis", "abstract": "medical diagnosis process vary in the degree to which they attempt to deal with different complicating aspects of diagnosis such as relative importance of symptoms , varied symptom pattern and the relation between diseases them selves . based on decision theory , in the past many mathematical models such as crisp set , probability distribution , fuzzy set , intuitionistic fuzzy set were developed to deal with complicating aspects of diagnosis . but , many such models are failed to include important aspects of the expert decisions . therefore , an effort has been made to process inconsistencies in data being considered by pawlak with the introduction of rough set theory . though rough set has major advantages over the other methods , but it generates too many rules that create many difficulties while taking decisions . therefore , it is essential to minimize the decision rules . in this paper , we use two processes such as pre process and post process to mine suitable rules and to explore the relationship among the attributes . in pre process we use rough set theory to mine suitable rules , whereas in post process we use formal concept analysis from these suitable rules to explore better knowledge and most important factors affecting the decision making ."}
{"title": "unsupervised diverse colorization via generative adversarial networks", "abstract": "colorization of grayscale images has been a hot topic in computer vision . previous research mainly focuses on producing a colored image to match the original one . however , since many colors share the same gray value , an input grayscale image could be diversely colored while maintaining its reality . in this paper , we design a novel solution for unsupervised diverse colorization . specifically , we leverage conditional generative adversarial networks to model the distribution of real-world item colors , in which we develop a fully convolutional generator with multi-layer noise to enhance diversity , with multi-layer condition concatenation to maintain reality , and with stride 1 to keep spatial information . with such a novel network architecture , the model yields highly competitive performance on the open lsun bedroom dataset . the turing test of 80 humans further indicates our generated color schemes are highly convincible ."}
{"title": "designing neural networks that process mean values of random variables", "abstract": "we introduce a class of neural networks derived from probabilistic models in the form of bayesian networks . by imposing additional assumptions about the nature of the probabilistic models represented in the networks , we derive neural networks with standard dynamics that require no training to determine the synaptic weights , that perform accurate calculation of the mean values of the random variables , that can pool multiple sources of evidence , and that deal cleanly and consistently with inconsistent or contradictory evidence . the presented neural networks capture many properties of bayesian networks , providing distributed versions of probabilistic models ."}
{"title": "book : storing algorithm-invariant episodes for deep reinforcement learning", "abstract": "we introduce a novel method to train agents of reinforcement learning ( rl ) by sharing knowledge in a way similar to the concept of using a book . the recorded information in the form of a book is the main means by which humans learn knowledge . nevertheless , the conventional deep rl methods have mainly focused either on experiential learning where the agent learns through interactions with the environment from the start or on imitation learning that tries to mimic the teacher . contrary to these , our proposed book learning shares key information among different agents in a book-like manner by delving into the following two characteristic features : ( 1 ) by defining the linguistic function , input states can be clustered semantically into a relatively small number of core clusters , which are forwarded to other rl agents in a prescribed manner . ( 2 ) by defining state priorities and the contents for recording , core experiences can be selected and stored in a small container . we call this container as ` book ' . our method learns hundreds to thousand times faster than the conventional methods by learning only a handful of core cluster information , which shows that deep rl agents can effectively learn through the shared knowledge from other agents ."}
{"title": "a heuristic scheme for the cooperative team orienteering problem with time windows", "abstract": "the cooperative orienteering problem with time windows ( coptw ) is a class of problems with some important applications and yet has received relatively little attention . in the coptw a certain number of team members are required to collect the associated reward from each customer simultaneously and cooperatively . this requirement to have one or more team members simultaneously available at a vertex to collect the reward , poses a challenging or task . exact methods are not able to handle large scale instances of the coptw and no heuristic schemes have been developed for this problem so far . in this paper , a new modification to the classical clarke and wright saving heuristic is proposed to handle this problem . a new benchmark set generated by adding the resource requirement attribute to the existing benchmarks . the heuristic algorithm followed by boosting operators achieves optimal solutions for 64.5 % of instances for which the optimal results are known . the proposed solution approach attains an optimality gap of 2.61 % for the same instances and solves benchmarks with realistic size within short computational times ."}
{"title": "on rational closure in description logics of typicality", "abstract": "we define the notion of rational closure in the context of description logics extended with a tipicality operator . we start from alc+t , an extension of alc with a typicality operator t : intuitively allowing to express concepts of the form t ( c ) , meant to select the `` most normal '' instances of a concept c. the semantics we consider is based on rational model . but we further restrict the semantics to minimal models , that is to say , to models that minimise the rank of domain elements . we show that this semantics captures exactly a notion of rational closure which is a natural extension to description logics of lehmann and magidor 's original one . we also extend the notion of rational closure to the abox component . we provide an exptime algorithm for computing the rational closure of an abox and we show that it is sound and complete with respect to the minimal model semantics ."}
{"title": "deep learning for video game playing", "abstract": "in this article , we review recent deep learning advances in the context of how they have been applied to play different types of video games such as first-person shooters , arcade games , and real-time strategy games . we analyze the unique requirements that different game genres pose to a deep learning system and highlight important open challenges in the context of applying these machine learning methods to video games , such as general game playing , dealing with extremely large decision spaces and sparse rewards ."}
{"title": "consistency management of normal logic program by top-down abductive proof procedure", "abstract": "this paper presents a method of computing a revision of a function-free normal logic program . if an added rule is inconsistent with a program , that is , if it leads to a situation such that no stable model exists for a new program , then deletion and addition of rules are performed to avoid inconsistency . we specify a revision by translating a normal logic program into an abductive logic program with abducibles to represent deletion and addition of rules . to compute such deletion and addition , we propose an adaptation of our top-down abductive proof procedure to compute a relevant abducibles to an added rule . we compute a minimally revised program , by choosing a minimal set of abducibles among all the sets of abducibles computed by a top-down proof procedure ."}
{"title": "lattice embeddings between types of fuzzy sets . closed-valued fuzzy sets", "abstract": "in this paper we deal with the problem of extending zadeh 's operators on fuzzy sets ( fss ) to interval-valued ( ivfss ) , set-valued ( svfss ) and type-2 ( t2fss ) fuzzy sets . namely , it is known that seeing fss as svfss , or t2fss , whose membership degrees are singletons is not order-preserving . we then describe a family of lattice embeddings from fss to svfss . alternatively , if the former singleton viewpoint is required , we reformulate the intersection on hesitant fuzzy sets and introduce what we have called closed-valued fuzzy sets . this new type of fuzzy sets extends standard union and intersection on fss . in addition , it allows handling together membership degrees of different nature as , for instance , closed intervals and finite sets . finally , all these constructions are viewed as t2fss forming a chain of lattices ."}
{"title": "trend filtering on graphs", "abstract": "we introduce a family of adaptive estimators on graphs , based on penalizing the $ \\ell_1 $ norm of discrete graph differences . this generalizes the idea of trend filtering [ kim et al . ( 2009 ) , tibshirani ( 2014 ) ] , used for univariate nonparametric regression , to graphs . analogous to the univariate case , graph trend filtering exhibits a level of local adaptivity unmatched by the usual $ \\ell_2 $ -based graph smoothers . it is also defined by a convex minimization problem that is readily solved ( e.g. , by fast admm or newton algorithms ) . we demonstrate the merits of graph trend filtering through examples and theory ."}
{"title": "highly comparative feature-based time-series classification", "abstract": "a highly comparative , feature-based approach to time series classification is introduced that uses an extensive database of algorithms to extract thousands of interpretable features from time series . these features are derived from across the scientific time-series analysis literature , and include summaries of time series in terms of their correlation structure , distribution , entropy , stationarity , scaling properties , and fits to a range of time-series models . after computing thousands of features for each time series in a training set , those that are most informative of the class structure are selected using greedy forward feature selection with a linear classifier . the resulting feature-based classifiers automatically learn the differences between classes using a reduced number of time-series properties , and circumvent the need to calculate distances between time series . representing time series in this way results in orders of magnitude of dimensionality reduction , allowing the method to perform well on very large datasets containing long time series or time series of different lengths . for many of the datasets studied , classification performance exceeded that of conventional instance-based classifiers , including one nearest neighbor classifiers using euclidean distances and dynamic time warping and , most importantly , the features selected provide an understanding of the properties of the dataset , insight that can guide further scientific investigation ."}
{"title": "on macroscopic complexity and perceptual coding", "abstract": "the theoretical limits of 'lossy ' data compression algorithms are considered . the complexity of an object as seen by a macroscopic observer is the size of the perceptual code which discards all information that can be lost without altering the perception of the specified observer . the complexity of this macroscopically observed state is the simplest description of any microstate comprising that macrostate . inference and pattern recognition based on macrostate rather than microstate complexities will take advantage of the complexity of the macroscopic observer to ignore irrelevant noise ."}
{"title": "a workflow for visual diagnostics of binary classifiers using instance-level explanations", "abstract": "human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions . to this end , we propose a visual analytics workflow to help data scientists and domain experts explore , diagnose , and understand the decisions made by a binary classifier . the approach leverages `` instance-level explanations '' , measures of local feature relevance that explain single instances , and uses them to build a set of visual representations that guide the users in their investigation . the workflow is based on three main visual representations and steps : one based on aggregate statistics to see how data distributes across correct / incorrect decisions ; one based on explanations to understand which features are used to make these decisions ; and one based on raw data , to derive insights on potential root causes for the observed patterns . the workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed . the case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes , thus experts can generate useful hypotheses on how a model can be improved ."}
{"title": "abc-sg : a new artificial bee colony algorithm-based distance of sequential data using sigma grams", "abstract": "the problem of similarity search is one of the main problems in computer science . this problem has many applications in text-retrieval , web search , computational biology , bioinformatics and others . similarity between two data objects can be depicted using a similarity measure or a distance metric . there are numerous distance metrics in the literature , some are used for a particular data type , and others are more general . in this paper we present a new distance metric for sequential data which is based on the sum of n-grams . the novelty of our distance is that these n-grams are weighted using artificial bee colony ; a recent optimization algorithm based on the collective intelligence of a swarm of bees on their search for nectar . this algorithm has been used in optimizing a large number of numerical problems . we validate the new distance experimentally ."}
{"title": "rockit : exploiting parallelism and symmetry for map inference in statistical relational models", "abstract": "rockit is a maximum a-posteriori ( map ) query engine for statistical relational models . map inference in graphical models is an optimization problem which can be compiled to integer linear programs ( ilps ) . we describe several advances in translating map queries to ilp instances and present the novel meta-algorithm cutting plane aggregation ( cpa ) . cpa exploits local context-specific symmetries and bundles up sets of linear constraints . the resulting counting constraints lead to more compact ilps and make the symmetry of the ground model more explicit to state-of-the-art ilp solvers . moreover , rockit parallelizes most parts of the map inference pipeline taking advantage of ubiquitous shared-memory multi-core architectures . we report on extensive experiments with markov logic network ( mln ) benchmarks showing that rockit outperforms the state-of-the-art systems alchemy , markov thebeast , and tuffy both in terms of efficiency and quality of results ."}
{"title": "the distributed ontology language ( dol ) : use cases , syntax , and extensibility", "abstract": "the distributed ontology language ( dol ) is currently being standardized within the ontoiop ( ontology integration and interoperability ) activity of iso/tc 37/sc 3. it aims at providing a unified framework for ( 1 ) ontologies formalized in heterogeneous logics , ( 2 ) modular ontologies , ( 3 ) links between ontologies , and ( 4 ) annotation of ontologies . this paper presents the current state of dol 's standardization . it focuses on use cases where distributed ontologies enable interoperability and reusability . we demonstrate relevant features of the dol syntax and semantics and explain how these integrate into existing knowledge engineering environments ."}
{"title": "a novel method for speech segmentation based on speakers ' characteristics", "abstract": "speech segmentation is the process change point detection for partitioning an input audio stream into regions each of which corresponds to only one audio source or one speaker . one application of this system is in speaker diarization systems . there are several methods for speaker segmentation ; however , most of the speaker diarization systems use bic-based segmentation methods . the main goal of this paper is to propose a new method for speaker segmentation with higher speed than the current methods - e.g . bic - and acceptable accuracy . our proposed method is based on the pitch frequency of the speech . the accuracy of this method is similar to the accuracy of common speaker segmentation methods . however , its computation cost is much less than theirs . we show that our method is about 2.4 times faster than the bic-based method , while the average accuracy of pitch-based method is slightly higher than that of the bic-based method ."}
{"title": "nonparametric general reinforcement learning", "abstract": "reinforcement learning ( rl ) problems are often phrased in terms of markov decision processes ( mdps ) . in this thesis we go beyond mdps and consider rl in environments that are non-markovian , non-ergodic and only partially observable . our focus is not on practical algorithms , but rather on the fundamental underlying problems : how do we balance exploration and exploitation ? how do we explore optimally ? when is an agent optimal ? we follow the nonparametric realizable paradigm . we establish negative results on bayesian rl agents , in particular aixi . we show that unlucky or adversarial choices of the prior cause the agent to misbehave drastically . therefore legg-hutter intelligence and balanced pareto optimality , which depend crucially on the choice of the prior , are entirely subjective . moreover , in the class of all computable environments every policy is pareto optimal . this undermines all existing optimality properties for aixi . however , there are bayesian approaches to general rl that satisfy objective optimality guarantees : we prove that thompson sampling is asymptotically optimal in stochastic environments in the sense that its value converges to the value of the optimal policy . we connect asymptotic optimality to regret given a recoverability assumption on the environment that allows the agent to recover from mistakes . hence thompson sampling achieves sublinear regret in these environments . our results culminate in a formal solution to the grain of truth problem : a bayesian agent acting in a multi-agent environment learns to predict the other agents ' policies if its prior assigns positive probability to them ( the prior contains a grain of truth ) . we construct a large but limit computable class containing a grain of truth and show that agents based on thompson sampling over this class converge to play nash equilibria in arbitrary unknown computable multi-agent environments ."}
{"title": "fleet size and mix split-delivery vehicle routing", "abstract": "in the classic vehicle routing problem ( vrp ) a fleet of of vehicles has to visit a set of customers while minimising the operations ' costs . we study a rich variant of the vrp featuring split deliveries , an heterogeneous fleet , and vehicle-commodity incompatibility constraints . our goal is twofold : define the cheapest routing and the most adequate fleet . to do so , we split the problem into two interdependent components : a fleet design component and a routing component . first , we define two mixed integer programming ( mip ) formulations for each component . then we discuss several improvements in the form of valid cuts and symmetry breaking constraints . the main contribution of this paper is a comparison of the four resulting models for this rich vrp . we highlight their strengths and weaknesses with extensive experiments . finally , we explore a lightweight integration with constraint programming ( cp ) . we use a fast cp model which gives good solutions and use the solution to warm-start our models ."}
{"title": "factorization of discrete probability distributions", "abstract": "we formulate necessary and sufficient conditions for an arbitrary discrete probability distribution to factor according to an undirected graphical model , or a log-linear model , or other more general exponential models . this result generalizes the well known hammersley-clifford theorem ."}
{"title": "effects of coupling in human-virtual agent body interaction", "abstract": "this paper presents a study of the dynamic coupling between a user and a virtual character during body interaction . coupling is directly linked with other dimensions , such as co-presence , engagement , and believability , and was measured in an experiment that allowed users to describe their subjective feelings about those dimensions of interest . the experiment was based on a theatrical game involving the imitation of slow upper-body movements and the proposal of new movements by the user and virtual agent . the agent 's behaviour varied in autonomy : the agent could limit itself to imitating the user 's movements only , initiate new movements , or combine both behaviours . after the game , each participant completed a questionnaire regarding their engagement in the interaction , their subjective feeling about the co-presence of the agent , etc . based on four main dimensions of interest , we tested several hypotheses against our experimental results , which are discussed here ."}
{"title": "hypertree decompositions and tractable queries", "abstract": "several important decision problems on conjunctive queries ( cqs ) are np-complete in general but become tractable , and actually highly parallelizable , if restricted to acyclic or nearly acyclic queries . examples are the evaluation of boolean cqs and query containment . these problems were shown tractable for conjunctive queries of bounded treewidth and of bounded degree of cyclicity . the so far most general concept of nearly acyclic queries was the notion of queries of bounded query-width introduced by chekuri and rajaraman ( 1997 ) . while cqs of bounded query width are tractable , it remained unclear whether such queries are efficiently recognizable . chekuri and rajaraman stated as an open problem whether for each constant k it can be determined in polynomial time if a query has query width less than or equal to k. we give a negative answer by proving this problem np-complete ( specifically , for k=4 ) . in order to circumvent this difficulty , we introduce the new concept of hypertree decomposition of a query and the corresponding notion of hypertree width . we prove : ( a ) for each k , the class of queries with query width bounded by k is properly contained in the class of queries whose hypertree width is bounded by k ; ( b ) unlike query width , constant hypertree-width is efficiently recognizable ; ( c ) boolean queries of constant hypertree width can be efficiently evaluated ."}
{"title": "reliable uncertain evidence modeling in bayesian networks by credal networks", "abstract": "a reliable modeling of uncertain evidence in bayesian networks based on a set-valued quantification is proposed . both soft and virtual evidences are considered . we show that evidence propagation in this setup can be reduced to standard updating in an augmented credal network , equivalent to a set of consistent bayesian networks . a characterization of the computational complexity for this task is derived together with an efficient exact procedure for a subclass of instances . in the case of multiple uncertain evidences over the same variable , the proposed procedure can provide a set-valued version of the geometric approach to opinion pooling ."}
{"title": "crowdsourcing control : moving beyond multiple choice", "abstract": "to ensure quality results from crowdsourced tasks , requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses . however , all current models assume prior knowledge of all possible outcomes of the task . while not an unreasonable assumption for tasks that can be posited as multiple-choice questions ( e.g . n-ary classification ) , we observe that many tasks do not naturally fit this paradigm , but instead demand a free-response formulation where the outcome space is of infinite size ( e.g . audio transcription ) . we model such tasks with a novel probabilistic graphical model , and design and implement lazysusan , a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks . we also design an em algorithm to jointly learn the parameters of our model while inferring the correct answers to multiple tasks at a time . live experiments on amazon mechanical turk demonstrate the superiority of lazysusan at solving sat math questions , eliminating 83.2 % of the error and achieving greater net utility compared to the state-ofthe-art strategy , majority-voting . we also show in live experiments that our em algorithm outperforms majority-voting on a visualization task that we design ."}
{"title": "evidence and plausibility in neighborhood structures", "abstract": "the intuitive notion of evidence has both semantic and syntactic features . in this paper , we develop an { \\em evidence logic } for epistemic agents faced with possibly contradictory evidence from different sources . the logic is based on a neighborhood semantics , where a neighborhood $ n $ indicates that the agent has reason to believe that the true state of the world lies in $ n $ . further notions of relative plausibility between worlds and beliefs based on the latter ordering are then defined in terms of this evidence structure , yielding our intended models for evidence-based beliefs . in addition , we also consider a second more general flavor , where belief and plausibility are modeled using additional primitive relations , and we prove a representation theorem showing that each such general model is a $ p $ -morphic image of an intended one . this semantics invites a number of natural special cases , depending on how uniform we make the evidence sets , and how coherent their total structure . we give a structural study of the resulting ` uniform ' and ` flat ' models . our main result are sound and complete axiomatizations for the logics of all four major model classes with respect to the modal language of evidence , belief and safe belief . we conclude with an outlook toward logics for the dynamics of changing evidence , and the resulting language extensions and connections with logics of plausibility change ."}
{"title": "scalable exact parent sets identification in bayesian networks learning with apache spark", "abstract": "in machine learning , the parent set identification problem is to find a set of random variables that best explain selected variable given the data and some predefined scoring function . this problem is a critical component to structure learning of bayesian networks and markov blankets discovery , and thus has many practical applications , ranging from fraud detection to clinical decision support . in this paper , we introduce a new distributed memory approach to the exact parent sets assignment problem . to achieve scalability , we derive theoretical bounds to constraint the search space when mdl scoring function is used , and we reorganize the underlying dynamic programming such that the computational density is increased and fine-grain synchronization is eliminated . we then design efficient realization of our approach in the apache spark platform . through experimental results , we demonstrate that the method maintains strong scalability on a 500-core standalone spark cluster , and it can be used to efficiently process data sets with 70 variables , far beyond the reach of the currently available solutions ."}
{"title": "top rank optimization in linear time", "abstract": "bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances . recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list . most existing approaches are either to optimize task specific metrics or to extend the ranking loss by emphasizing more on the error associated with the top ranked instances , leading to a high computational cost that is super-linear in the number of training instances . we propose a highly efficient approach , titled toppush , for optimizing accuracy at the top that has computational complexity linear in the number of training instances . we present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach . empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster ."}
{"title": "on local regret", "abstract": "online learning aims to perform nearly as well as the best hypothesis in hindsight . for some hypothesis classes , though , even finding the best hypothesis offline is challenging . in such offline cases , local search techniques are often employed and only local optimality guaranteed . for online decision-making with such hypothesis classes , we introduce local regret , a generalization of regret that aims to perform nearly as well as only nearby hypotheses . we then present a general algorithm to minimize local regret with arbitrary locality graphs . we also show how the graph structure can be exploited to drastically speed learning . these algorithms are then demonstrated on a diverse set of online problems : online disjunct learning , online max-sat , and online decision tree learning ."}
{"title": "learning spatiotemporal features for infrared action recognition with 3d convolutional neural networks", "abstract": "infrared ( ir ) imaging has the potential to enable more robust action recognition systems compared to visible spectrum cameras due to lower sensitivity to lighting conditions and appearance variability . while the action recognition task on videos collected from visible spectrum imaging has received much attention , action recognition in ir videos is significantly less explored . our objective is to exploit imaging data in this modality for the action recognition task . in this work , we propose a novel two-stream 3d convolutional neural network ( cnn ) architecture by introducing the discriminative code layer and the corresponding discriminative code loss function . the proposed network processes ir image and the ir-based optical flow field sequences . we pretrain the 3d cnn model on the visible spectrum sports-1m action dataset and finetune it on the infrared action recognition ( infar ) dataset . to our best knowledge , this is the first application of the 3d cnn to action recognition in the ir domain . we conduct an elaborate analysis of different fusion schemes ( weighted average , single and double-layer neural nets ) applied to different 3d cnn outputs . experimental results demonstrate that our approach can achieve state-of-the-art average precision ( ap ) performances on the infar dataset : ( 1 ) the proposed two-stream 3d cnn achieves the best reported 77.5 % ap , and ( 2 ) our 3d cnn model applied to the optical flow fields achieves the best reported single stream 75.42 % ap ."}
{"title": "the case for meta-cognitive machine learning : on model entropy and concept formation in deep learning", "abstract": "machine learning is usually defined in behaviourist terms , where external validation is the primary mechanism of learning . in this paper , i argue for a more holistic interpretation in which finding more probable , efficient and abstract representations is as central to learning as performance . in other words , machine learning should be extended with strategies to reason over its own learning process , leading to so-called meta-cognitive machine learning . as such , the de facto definition of machine learning should be reformulated in these intrinsically multi-objective terms , taking into account not only the task performance but also internal learning objectives . to this end , we suggest a `` model entropy function '' to be defined that quantifies the efficiency of the internal learning processes . it is conjured that the minimization of this model entropy leads to concept formation . besides philosophical aspects , some initial illustrations are included to support the claims ."}
{"title": "location-based reasoning about complex multi-agent behavior", "abstract": "recent research has shown that surprisingly rich models of human activity can be learned from gps ( positional ) data . however , most effort to date has concentrated on modeling single individuals or statistical properties of groups of people . moreover , prior work focused solely on modeling actual successful executions ( and not failed or attempted executions ) of the activities of interest . we , in contrast , take on the task of understanding human interactions , attempted interactions , and intentions from noisy sensor data in a fully relational multi-agent setting . we use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities . we model the domain using markov logic , a statistical-relational language , and learn a theory that jointly denoises the data and infers occurrences of high-level activities , such as a player capturing an enemy . our unified model combines constraints imposed by the geometry of the game area , the motion model of the players , and by the rules and dynamics of the game in a probabilistically and logically sound fashion . we show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction , the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it . further , we show that given a model of successfully performed multi-agent activities , along with a set of examples of failed attempts at the same activities , our system automatically learns an augmented model that is capable of recognizing success and failure , as well as goals of peoples actions with high accuracy . we compare our approach with other alternatives and show that our unified model , which takes into account not only relationships among individual players , but also relationships among activities over the entire length of a game , although more computationally costly , is significantly more accurate . finally , we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks ."}
{"title": "tweeting ai : perceptions of lay vs expert twitterati", "abstract": "with the recent advancements in artificial intelligence ( ai ) , various organizations and individuals are debating about the progress of ai as a blessing or a curse for the future of the society . this paper conducts an investigation on how the public perceives the progress of ai by utilizing the data shared on twitter . specifically , this paper performs a comparative analysis on the understanding of users belonging to two categories -- general ai-tweeters ( ait ) and expert ai-tweeters ( eait ) who share posts about ai on twitter . our analysis revealed that users from both the categories express distinct emotions and interests towards ai . users from both the categories regard ai as positive and are optimistic about the progress of ai but the experts are more negative than the general ai-tweeters . expert ai-tweeters share relatively large percentage of tweets about their personal news compared to technical aspects of ai . however , the effects of automation on the future are of primary concern to ait than to eait . when the expert category is sub-categorized , the emotion analysis revealed that students and industry professionals have more insights in their tweets about ai than academicians ."}
{"title": "yggdrasil - a statistical package for learning split models", "abstract": "there are two main objectives of this paper . the first is to present a statistical framework for models with context specific independence structures , i.e. , conditional independences holding only for sepcific values of the conditioning variables . this framework is constituted by the class of split models . split models are extension of graphical models for contigency tables and allow for a more sophisticiated modelling than graphical models . the treatment of split models include estimation , representation and a markov property for reading off those independencies holding in a specific context . the second objective is to present a software package named yggdrasil which is designed for statistical inference in split models , i.e. , for learning such models on the basis of data ."}
{"title": "from dependency to causality : a machine learning approach", "abstract": "the relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference . recent results in the chalearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in markov indistinguishable configurations thanks to data driven approaches . this paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with $ n > 2 $ variables . the approach relies on the asymmetry of some conditional ( in ) dependence relations between the members of the markov blankets of two variables causally connected . our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for $ n > 2 $ variate distributions ."}
{"title": "quantified conditional logics are fragments of hol", "abstract": "a semantic embedding of ( constant domain ) quantified conditional logic in classical higher-order logic is presented ."}
{"title": "diagnosing client faults using svm-based intelligent inference from tcp packet traces", "abstract": "we present the intelligent automated client diagnostic ( iacd ) system , which only relies on inference from transmission control protocol ( tcp ) packet traces for rapid diagnosis of client device problems that cause network performance issues . using soft-margin support vector machine ( svm ) classifiers , the system ( i ) distinguishes link problems from client problems , and ( ii ) identifies characteristics unique to client faults to report the root cause of the client device problem . experimental evaluation demonstrated the capability of the iacd system to distinguish between faulty and healthy links and to diagnose the client faults with 98 % accuracy in healthy links . the system can perform fault diagnosis independent of the client 's specific tcp implementation , enabling diagnosis capability on diverse range of client computers ."}
{"title": "metatheory of actions : beyond consistency", "abstract": "consistency check has been the only criterion for theory evaluation in logic-based approaches to reasoning about actions . this work goes beyond that and contributes to the metatheory of actions by investigating what other properties a good domain description in reasoning about actions should have . we state some metatheoretical postulates concerning this sore spot . when all postulates are satisfied together we have a modular action theory . besides being easier to understand and more elaboration tolerant in mccarthy 's sense , modular theories have interesting properties . we point out the problems that arise when the postulates about modularity are violated and propose algorithmic checks that can help the designer of an action theory to overcome them ."}
{"title": "multimodal biometric systems - study to improve accuracy and performance", "abstract": "biometrics is the science and technology of measuring and analyzing biological data of human body , extracting a feature set from the acquired data , and comparing this set against to the template set in the database . experimental studies show that unimodal biometric systems had many disadvantages regarding performance and accuracy . multimodal biometric systems perform better than unimodal biometric systems and are popular even more complex also . we examine the accuracy and performance of multimodal biometric authentication systems using state of the art commercial off- the-shelf ( cots ) products . here we discuss fingerprint and face biometric systems , decision and fusion techniques used in these systems . we also discuss their advantage over unimodal biometric systems ."}
{"title": "short portfolio training for csp solving", "abstract": "many different approaches for solving constraint satisfaction problems ( csps ) and related constraint optimization problems ( cops ) exist . however , there is no single solver ( nor approach ) that performs well on all classes of problems and many portfolio approaches for selecting a suitable solver based on simple syntactic features of the input csp instance have been developed . in this paper we first present a simple portfolio method for csp based on k-nearest neighbors method . then , we propose a new way of using portfolio systems -- - training them shortly in the exploitation time , specifically for the set of instances to be solved and using them on that set . thorough evaluation has been performed and has shown that the approach yields good results . we evaluated several machine learning techniques for our portfolio . due to its simplicity and efficiency , the selected k-nearest neighbors method is especially suited for our short training approach and it also yields the best results among the tested methods . we also confirm that our approach yields good results on sat domain ."}
{"title": "variational gaussian process dynamical systems", "abstract": "high dimensional time series are endemic in applications of machine learning such as robotics ( sensor data ) , computational biology ( gene expression data ) , vision ( video sequences ) and graphics ( motion capture data ) . practical nonlinear probabilistic approaches to this data are required . in this paper we introduce the variational gaussian process dynamical system . our work builds on recent variational approximations for gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space . the approach also allows for the appropriate dimensionality of the latent space to be automatically determined . we demonstrate the model on a human motion capture data set and a series of high resolution video sequences ."}
{"title": "a modified physarum-inspired model for the user equilibrium traffic assignment problem", "abstract": "the user equilibrium traffic assignment principle is very important in the traffic assignment problem . mathematical programming models are designed to solve the user equilibrium problem in traditional algorithms . recently , the physarum shows the ability to address the user equilibrium and system optimization traffic assignment problems . however , the physarum model are not efficient in real traffic networks with two-way traffic characteristics and multiple origin-destination pairs . in this article , a modified physarum-inspired model for the user equilibrium problem is proposed . by decomposing traffic flux based on origin nodes , the traffic flux from different origin-destination pairs can be distinguished in the proposed model . the physarum can obtain the equilibrium traffic flux when no shorter path can be discovered between each origin-destination pair . finally , numerical examples use the sioux falls network to demonstrate the rationality and convergence properties of the proposed model ."}
{"title": "a sufficiently fast algorithm for finding close to optimal junction trees", "abstract": "an algorithm is developed for finding a close to optimal junction tree of a given graph g. the algorithm has a worst case complexity o ( c^k n^a ) where a and c are constants , n is the number of vertices , and k is the size of the largest clique in a junction tree of g in which this size is minimized . the algorithm guarantees that the logarithm of the size of the state space of the heaviest clique in the junction tree produced is less than a constant factor off the optimal value . when k = o ( log n ) , our algorithm yields a polynomial inference algorithm for bayesian networks ."}
{"title": "learning invariant feature spaces to transfer skills with reinforcement learning", "abstract": "people can learn a wide range of tasks from their own experience , but can also learn from observing other creatures . this can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology . in this paper , we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents ( e.g. , different robots ) . we introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information . our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another . the process of learning these invariant feature spaces can be viewed as a kind of `` analogy making '' , or implicit learning of partial correspondences between two distinct domains . we evaluate our transfer learning algorithm in two simulated robotic manipulation skills , and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links , as well as simulated arms with different actuation mechanisms , where one robot is torque-driven while the other is tendon-driven ."}
{"title": "seraph : semi-supervised metric learning paradigm with hyper sparsity", "abstract": "we propose a general information-theoretic approach called seraph ( semi-supervised metric learning paradigm with hyper-sparsity ) for metric learning that does not rely upon the manifold assumption . given the probability parameterized by a mahalanobis distance , we maximize the entropy of that probability on labeled data and minimize it on unlabeled data following entropy regularization , which allows the supervised and unsupervised parts to be integrated in a natural and meaningful way . furthermore , seraph is regularized by encouraging a low-rank projection induced from the metric . the optimization of seraph is solved efficiently and stably by an em-like scheme with the analytical e-step and convex m-step . experiments demonstrate that seraph compares favorably with many well-known global and local metric learning methods ."}
{"title": "learning bayesian network structure from massive datasets : the `` sparse candidate '' algorithm", "abstract": "learning bayesian networks is often cast as an optimization problem , where the computational task is to find a structure that maximizes a statistically motivated score . by and large , existing learning tools address this optimization problem using standard heuristic search techniques . since the search space is extremely large , such search procedures can spend most of the time examining candidates that are extremely unreasonable . this problem becomes critical when we deal with data sets that are large either in the number of instances , or the number of attributes . in this paper , we introduce an algorithm that achieves faster learning by restricting the search space . this iterative algorithm restricts the parents of each variable to belong to a small subset of candidates . we then search for a network that satisfies these constraints . the learned network is then used for selecting better candidates for the next iteration . we evaluate this algorithm both on synthetic and real-life data . our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures ."}
{"title": "strategyproof peer selection using randomization , partitioning , and apportionment", "abstract": "peer review , evaluation , and selection is a fundamental aspect of modern science . funding bodies the world over employ experts to review and select the best proposals of those submitted for funding . the problem of peer selection , however , is much more general : a professional society may want to give a subset of its members awards based on the opinions of all members ; an instructor for a mooc or online course may want to crowdsource grading ; or a marketing company may select ideas from group brainstorming sessions based on peer evaluation . we make three fundamental contributions to the study of procedures or mechanisms for peer selection , a specific type of group decision-making problem , studied in computer science , economics , and political science . first , we propose a novel mechanism that is strategyproof , i.e. , agents can not benefit by reporting insincere valuations . second , we demonstrate the effectiveness of our mechanism by a comprehensive simulation-based comparison with a suite of mechanisms found in the literature . finally , our mechanism employs a randomized rounding technique that is of independent interest , as it solves the apportionment problem that arises in various settings where discrete resources such as parliamentary representation slots need to be divided proportionally ."}
{"title": "proceedings of the fourth conference on uncertainty in artificial intelligence ( 1988 )", "abstract": "this is the proceedings of the fourth conference on uncertainty in artificial intelligence , which was held in minneapolis , mn , july 10-12 , 1988"}
{"title": "proceedings of the workshop on data mining for oil and gas", "abstract": "the process of exploring and exploiting oil and gas ( o & g ) generates a lot of data that can bring more efficiency to the industry . the opportunities for using data mining techniques in the `` digital oil-field '' remain largely unexplored or uncharted . with the high rate of data expansion , companies are scrambling to develop ways to develop near-real-time predictive analytics , data mining and machine learning capabilities , and are expanding their data storage infrastructure and resources . with these new goals , come the challenges of managing data growth , integrating intelligence tools , and analyzing the data to glean useful insights . oil and gas companies need data solutions to economically extract value from very large volumes of a wide variety of data generated from exploration , well drilling and production devices and sensors . data mining for oil and gas industry throughout the lifecycle of the reservoir includes the following roles : locating hydrocarbons , managing geological data , drilling and formation evaluation , well construction , well completion , and optimizing production through the life of the oil field . for each of these phases during the lifecycle of oil field , data mining play a significant role . based on which phase were talking about , knowledge creation through scientific models , data analytics and machine learning , a effective , productive , and on demand data insight is critical for decision making within the organization . the significant challenges posed by this complex and economically vital field justify a meeting of data scientists that are willing to share their experience and knowledge . thus , the worskhop on data mining for oil and gas ( dm4og ) aims to provide a quality forum for researchers that work on the significant challenges arising from the synergy between data science , machine learning , and the modeling and optimization problems in the o & g industry ."}
{"title": "action-driven object detection with top-down visual attentions", "abstract": "a dominant paradigm for deep learning based object detection relies on a `` bottom-up '' approach using `` passive '' scoring of class agnostic proposals . these approaches are efficient but lack of holistic analysis of scene-level context . in this paper , we present an `` action-driven '' detection mechanism using our `` top-down '' visual attention model . we localize an object by taking sequential actions that the attention model provides . the attention model conditioned with an image region provides required actions to get closer toward a target object . an action at each time step is weak itself but an ensemble of the sequential actions makes a bounding-box accurately converge to a target object boundary . this attention model we call attentionnet is composed of a convolutional neural network . during our whole detection procedure , we only utilize the actions from a single attentionnet without any modules for object proposals nor post bounding-box regression . we evaluate our top-down detection mechanism over the pascal voc series and ilsvrc cls-loc dataset , and achieve state-of-the-art performances compared to the major bottom-up detection methods . in particular , our detection mechanism shows a strong advantage in elaborate localization by outperforming faster r-cnn with a margin of +7.1 % over pascal voc 2007 when we increase the iou threshold for positive detection to 0.7 ."}
{"title": "action branching architectures for deep reinforcement learning", "abstract": "discrete-action algorithms have been central to numerous recent successes of deep reinforcement learning . however , applying these algorithms to high-dimensional action tasks requires tackling the combinatorial increase of the number of possible actions with the number of action dimensions . this problem is further exacerbated for continuous-action tasks that require fine control of actions via discretization . in this paper , we propose a novel neural architecture featuring a shared decision module followed by several network branches , one for each action dimension . this approach achieves a linear increase of the number of network outputs with the number of degrees of freedom by allowing a level of independence for each individual action dimension . to illustrate the approach , we present a novel agent , called branching dueling q-network ( bdq ) , as a branching variant of the dueling double deep q-network ( dueling ddqn ) . we evaluate the performance of our agent on a set of challenging continuous control tasks . the empirical results show that the proposed agent scales gracefully to environments with increasing action dimensionality and indicate the significance of the shared decision module in coordination of the distributed action branches . furthermore , we show that the proposed agent performs competitively against a state-of-the-art continuous control algorithm , deep deterministic policy gradient ( ddpg ) ."}
{"title": "phoenix : a self-optimizing chess engine", "abstract": "since the advent of computers , many tasks which required humans to spend a lot of time and energy have been trivialized by the computers ' ability to perform repetitive tasks extremely quickly . playing chess is one such task . it was one of the first games which was ` solved ' using ai . with the advent of deep learning , chess playing agents can surpass human ability with relative ease . however algorithms using deep learning must learn millions of parameters . this work looks at the game of chess through the lens of genetic algorithms . we train a genetic player from scratch using only a handful of learnable parameters . we use multi-niche crowding to optimize positional value tables ( pvts ) which are used extensively in chess engines to evaluate the goodness of a position . with a very simple setup and after only 1000 generations of evolution , the player reaches the level of an international master ."}
{"title": "deep reinforcement learning with macro-actions", "abstract": "deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks , such as the atari domain . in this paper , we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches . we concentrate on macro-actions , and evaluate these on different atari 2600 games , where we show that they yield significant improvements in learning speed . additionally , we show that they can even achieve better scores than dqn . we offer analysis and explanation for both convergence and final results , revealing a problem deep rl approaches have with sparse reward signals ."}
{"title": "targeted advertising based on browsing history", "abstract": "audience interest , demography , purchase behavior and other possible classifications are ex- tremely important factors to be carefully studied in a targeting campaign . this information can help advertisers and publishers deliver advertisements to the right audience group . how- ever , it is not easy to collect such information , especially for the online audience with whom we have limited interaction and minimum deterministic knowledge . in this paper , we pro- pose a predictive framework that can estimate online audience demographic attributes based on their browsing histories . under the proposed framework , first , we retrieve the content of the websites visited by audience , and represent the content as website feature vectors ; second , we aggregate the vectors of websites that audience have visited and arrive at feature vectors representing the users ; finally , the support vector machine is exploited to predict the audience demographic attributes . the key to achieving good prediction performance is preparing representative features of the audience . word embedding , a widely used tech- nique in natural language processing tasks , together with term frequency-inverse document frequency weighting scheme is used in the proposed method . this new representation ap- proach is unsupervised and very easy to implement . the experimental results demonstrate that the new audience feature representation method is more powerful than existing baseline methods , leading to a great improvement in prediction accuracy ."}
{"title": "pattern recognition and memory mapping using mirroring neural networks", "abstract": "in this paper , we present a new kind of learning implementation to recognize the patterns using the concept of mirroring neural network ( mnn ) which can extract information from distinct sensory input patterns and perform pattern recognition tasks . it is also capable of being used as an advanced associative memory wherein image data is associated with voice inputs in an unsupervised manner . since the architecture is hierarchical and modular it has the potential of being used to devise learning engines of ever increasing complexity ."}
{"title": "e-valuate : a two-player game on arithmetic expressions -- an update", "abstract": "e-valuate is a game on arithmetic expressions . the players have contrasting roles of maximizing and minimizing the given expression . the maximizer proposes values and the minimizer substitutes them for variables of his choice . when the expression is fully instantiated , its value is compared with a certain minimax value that would result if the players played to their optimal strategies . the winner is declared based on this comparison . we use a game tree to represent the state of the game and show how the minimax value can be computed efficiently using backward induction and alpha-beta pruning . the efficacy of alpha-beta pruning depends on the order in which the nodes are evaluated . further improvements can be obtained by using transposition tables to prevent reevaluation of the same nodes . we propose a heuristic for node ordering . we show how the use of the heuristic and transposition tables lead to improved performance by comparing the number of nodes pruned by each method . we describe some domain-specific variants of this game . the first is a graph theoretic formulation wherein two players share a set of elements of a graph by coloring a related set with each player looking to maximize his share . the set being shared could be either the set of vertices , edges or faces ( for a planar graph ) . an application of this is the sharing of regions enclosed by a planar graph where each player 's aim is to maximize the area of his share . another variant is a tiling game where the players alternately place dominoes on a $ 8 \\times 8 $ checkerboard to construct a maximal partial tiling . we show that the size of the tiling $ x $ satisfies $ 22 \\le x \\le 32 $ by proving that any maximal partial tiling requires at least $ 22 $ dominoes ."}
{"title": "probabilistic reasoning about ship images", "abstract": "one of the most important aspects of current expert systems technology is the ability to make causal inferences about the impact of new evidence . when the domain knowledge and problem knowledge are uncertain and incomplete bayesian reasoning has proven to be an effective way of forming such inferences [ 3,4,8 ] . while several reasoning schemes have been developed based on bayes rule , there has been very little work examining the comparative effectiveness of these schemes in a real application . this paper describes a knowledge based system for ship classification [ 1 ] , originally developed using the prospector updating method [ 2 ] , that has been reimplemented to use the inference procedure developed by pearl and kim [ 4,5 ] . we discuss our reasons for making this change , the implementation of the new inference engine , and the comparative performance of the two versions of the system ."}
{"title": "temporal decision trees : model-based diagnosis of dynamic systems on-board", "abstract": "the automatic generation of decision trees based on off-line reasoning on models of a domain is a reasonable compromise between the advantages of using a model-based approach in technical domains and the constraints imposed by embedded applications . in this paper we extend the approach to deal with temporal information . we introduce a notion of temporal decision tree , which is designed to make use of relevant information as long as it is acquired , and we present an algorithm for compiling such trees from a model-based reasoning system ."}
{"title": "confounding equivalence in causal inference", "abstract": "the paper provides a simple test for deciding , from a given causal diagram , whether two sets of variables have the same bias-reducing potential under adjustment . the test requires that one of the following two conditions holds : either ( 1 ) both sets are admissible ( i.e. , satisfy the back-door criterion ) or ( 2 ) the markov boundaries surrounding the manipulated variable ( s ) are identical in both sets . applications to covariate selection and model testing are discussed ."}
{"title": "distributed deep reinforcement learning : learn how to play atari games in 21 minutes", "abstract": "we present a study in distributed deep reinforcement learning ( ddrl ) focused on scalability of a state-of-the-art deep reinforcement learning algorithm known as batch asynchronous advantage actorcritic ( ba3c ) . we show that using the adam optimization algorithm with a batch size of up to 2048 is a viable choice for carrying out large scale machine learning computations . this , combined with careful reexamination of the optimizer 's hyperparameters , using synchronous training on the node level ( while keeping the local , single node part of the algorithm asynchronous ) and minimizing the memory footprint of the model , allowed us to achieve linear scaling for up to 64 cpu nodes . this corresponds to a training time of 21 minutes on 768 cpu cores , as opposed to 10 hours when using a single node with 24 cores achieved by a baseline single-node implementation ."}
{"title": "solution of rectangular fuzzy games by principle of dominance using lr-type trapezoidal fuzzy numbers", "abstract": "fuzzy set theory has been applied in many fields such as operations research , control theory , and management sciences etc . in particular , an application of this theory in managerial decision making problems has a remarkable significance . in this paper , we consider a solution of rectangular fuzzy game with pay-off as imprecise numbers instead of crisp numbers viz. , interval and lr-type trapezoidal fuzzy numbers . the solution of such fuzzy games with pure strategies by minimax-maximin principle is discussed . the algebraic method to solve fuzzy games without saddle point by using mixed strategies is also illustrated . here , pay-off matrix is reduced to pay-off matrix by dominance method . this fact is illustrated by means of numerical example ."}
{"title": "automated construction of sparse bayesian networks from unstructured probabilistic models and domain information", "abstract": "an algorithm for automated construction of a sparse bayesian network given an unstructured probabilistic model and causal domain information from an expert has been developed and implemented . the goal is to obtain a network that explicitly reveals as much information regarding conditional independence as possible . the network is built incrementally adding one node at a time . the expert 's information and a greedy heuristic that tries to keep the number of arcs added at each step to a minimum are used to guide the search for the next node to add . the probabilistic model is a predicate that can answer queries about independencies in the domain . in practice the model can be implemented in various ways . for example , the model could be a statistical independence test operating on empirical data or a deductive prover operating on a set of independence statements about the domain ."}
{"title": "relative entropy , probabilistic inference and ai", "abstract": "various properties of relative entropy have led to its widespread use in information theory . these properties suggest that relative entropy has a role to play in systems that attempt to perform inference in terms of probability distributions . in this paper , i will review some basic properties of relative entropy as well as its role in probabilistic inference . i will also mention briefly a few existing and potential applications of relative entropy to so-called artificial intelligence ( ai ) ."}
{"title": "improving drug sensitivity predictions in precision medicine through active expert knowledge elicitation", "abstract": "predicting the efficacy of a drug for a given individual , using high-dimensional genomic measurements , is at the core of precision medicine . however , identifying features on which to base the predictions remains a challenge , especially when the sample size is small . incorporating expert knowledge offers a promising alternative to improve a prediction model , but collecting such knowledge is laborious to the expert if the number of candidate features is very large . we introduce a probabilistic model that can incorporate expert feedback about the impact of genomic measurements on the sensitivity of a cancer cell for a given drug . we also present two methods to intelligently collect this feedback from the expert , using experimental design and multi-armed bandit models . in a multiple myeloma blood cancer data set ( n=51 ) , expert knowledge decreased the prediction error by 8 % . furthermore , the intelligent approaches can be used to reduce the workload of feedback collection to less than 30 % on average compared to a naive approach ."}
{"title": "secured wireless communication using fuzzy logic based high speed public-key cryptography ( flhspkc )", "abstract": "in this paper secured wireless communication using fuzzy logic based high speed public key cryptography ( flhspkc ) has been proposed by satisfying the major issues likes computational safety , power management and restricted usage of memory in wireless communication . wireless sensor network ( wsn ) has several major constraints likes inadequate source of energy , restricted computational potentiality and limited memory . though conventional elliptic curve cryptography ( ecc ) which is a sort of public key cryptography used in wireless communication provides equivalent level of security like other existing public key algorithm using smaller parameters than other but this traditional ecc does not take care of all these major limitations in wsn . in conventional ecc consider elliptic curve point p , an arbitrary integer k and modulus m , ecc carry out scalar multiplication kp mod m , which takes about 80 % of key computation time on wsn . in this paper proposed flhspkc scheme provides some novel strategy including novel soft computing based strategy to speed up scalar multiplication in conventional ecc and which in turn takes shorter computational time and also satisfies power consumption restraint , limited usage of memory without hampering the security level . performance analysis of the different strategies under flhspkc scheme and comparison study with existing conventional ecc methods has been done ."}
{"title": "on the computability of aixi", "abstract": "how could we solve the machine learning and the artificial intelligence problem if we had infinite computation ? solomonoff induction and the reinforcement learning agent aixi are proposed answers to this question . both are known to be incomputable . in this paper , we quantify this using the arithmetical hierarchy , and prove upper and corresponding lower bounds for incomputability . we show that aixi is not limit computable , thus it can not be approximated using finite computation . our main result is a limit-computable { \\epsilon } -optimal version of aixi with infinite horizon that maximizes expected rewards ."}
{"title": "proceedings of the twenty-ninth conference on uncertainty in artificial intelligence ( 2013 )", "abstract": "this is the proceedings of the twenty-ninth conference on uncertainty in artificial intelligence , which was held in bellevue , wa , august 11-15 , 2013"}
{"title": "intelligent self-repairable web wrappers", "abstract": "the amount of information available on the web grows at an incredible high rate . systems and procedures devised to extract these data from web sources already exist , and different approaches and techniques have been investigated during the last years . on the one hand , reliable solutions should provide robust algorithms of web data mining which could automatically face possible malfunctioning or failures . on the other , in literature there is a lack of solutions about the maintenance of these systems . procedures that extract web data may be strictly interconnected with the structure of the data source itself ; thus , malfunctioning or acquisition of corrupted data could be caused , for example , by structural modifications of data sources brought by their owners . nowadays , verification of data integrity and maintenance are mostly manually managed , in order to ensure that these systems work correctly and reliably . in this paper we propose a novel approach to create procedures able to extract data from web sources -- the so called web wrappers -- which can face possible malfunctioning caused by modifications of the structure of the data source , and can automatically repair themselves ."}
{"title": "unweighted stochastic local search can be effective for random csp benchmarks", "abstract": "we present ulsa , a novel stochastic local search algorithm for random binary constraint satisfaction problems ( csp ) . ulsa is many times faster than the prior state of the art on a widely-studied suite of random csp benchmarks . unlike the best previous methods for these benchmarks , ulsa is a simple unweighted method that does not require dynamic adaptation of weights or penalties . ulsa obtains new record best solutions satisfying 99 of 100 variables in the challenging frb100-40 benchmark instance ."}
{"title": "desirability and the birth of incomplete preferences", "abstract": "we establish an equivalence between two seemingly different theories : one is the traditional axiomatisation of incomplete preferences on horse lotteries based on the mixture independence axiom ; the other is the theory of desirable gambles developed in the context of imprecise probability . the equivalence allows us to revisit incomplete preferences from the viewpoint of desirability and through the derived notion of coherent lower previsions . on this basis , we obtain new results and insights : in particular , we show that the theory of incomplete preferences can be developed assuming only the existence of a worst act -- -no best act is needed -- - , and that a weakened archimedean axiom suffices too ; this axiom allows us also to address some controversy about the regularity assumption ( that probabilities should be positive -- -they need not ) , which enables us also to deal with uncountable possibility spaces ; we show that it is always possible to extend in a minimal way a preference relation to one with a worst act , and yet the resulting relation is never archimedean , except in a trivial case ; we show that the traditional notion of state independence coincides with the notion called strong independence in imprecise probability -- -this leads us to give much a weaker definition of state independence than the traditional one ; we rework and uniform the notions of complete preferences , beliefs , values ; we argue that archimedeanity does not capture all the problems that can be modelled with sets of expected utilities and we provide a new notion that does precisely that . perhaps most importantly , we argue throughout that desirability is a powerful and natural setting to model , and work with , incomplete preferences , even in case of non-archimedean problems . this leads us to suggest that desirability , rather than preference , should be the primitive notion at the basis of decision-theoretic axiomatisations ."}
{"title": "a quantum dynamic belief model to explain the interference effects of categorization on decision making", "abstract": "categorization is necessary for many decision making tasks . however , the categorization process may interfere the decision making result and the law of total probability can be violated in some situations . to predict the interference effect of categorization , some model based on quantum probability has been proposed . in this paper , a new quantum dynamic belief ( qdb ) model is proposed . considering the precise decision may not be made during the process , the concept of uncertainty is introduced in our model to simulate real human thinking process . then the interference effect categorization can be predicted by handling the uncertain information . the proposed model is applied to a categorization decision-making experiment to explain the interference effect of categorization . compared with other models , our model is relatively more succinct and the result shows the correctness and effectiveness of our model ."}
{"title": "hyper-heuristic algorithm for finding efficient features in diagnose of lung cancer disease", "abstract": "background : lung cancer was known as primary cancers and the survival rate of cancer is about 15 % . early detection of lung cancer is the leading factor in survival rate . all symptoms ( features ) of lung cancer do not appear until the cancer spreads to other areas . it needs an accurate early detection of lung cancer , for increasing the survival rate . for accurate detection , it need characterizes efficient features and delete redundancy features among all features . feature selection is the problem of selecting informative features among all features . materials and methods : lung cancer database consist of 32 patient records with 57 features . this database collected by hong and youngand indexed in the university of california irvine repository . experimental contents include the extracted from the clinical data and x-ray data , etc . the data described 3 types of pathological lung cancers and all features are taking an integer value 0-3. in our study , new method is proposed for identify efficient features of lung cancer . it is based on hyper-heuristic . results : we obtained an accuracy of 80.63 % using reduced 11 feature set . the proposed method compare to the accuracy of 5 machine learning feature selections . the accuracy of these 5 methods are 60.94 , 57.81 , 68.75 , 60.94 and 68.75. conclusions : the proposed method has better performance with the highest level of accuracy . therefore , the proposed model is recommended for identifying an efficient symptom of disease . these finding are very important in health research , particularly in allocation of medical resources for patients who predicted as high-risks"}
{"title": "toward the coevolution of novel vertical-axis wind turbines", "abstract": "the production of renewable and sustainable energy is one of the most important challenges currently facing mankind . wind has made an increasing contribution to the world 's energy supply mix , but still remains a long way from reaching its full potential . in this paper , we investigate the use of artificial evolution to design vertical-axis wind turbine prototypes that are physically instantiated and evaluated under fan generated wind conditions . initially a conventional evolutionary algorithm is used to explore the design space of a single wind turbine and later a cooperative coevolutionary algorithm is used to explore the design space of an array of wind turbines . artificial neural networks are used throughout as surrogate models to assist learning and found to reduce the number of fabrications required to reach a higher aerodynamic efficiency . unlike in other approaches , such as computational fluid dynamics simulations , no mathematical formulations are used and no model assumptions are made ."}
{"title": "a probabilistic perspective on gaussian filtering and smoothing", "abstract": "we present a general probabilistic perspective on gaussian filtering and smoothing . this allows us to show that common approaches to gaussian filtering/smoothing can be distinguished solely by their methods of computing/approximating the means and covariances of joint probabilities . this implies that novel filters and smoothers can be derived straightforwardly by providing methods for computing these moments . based on this insight , we derive the cubature kalman smoother and propose a novel robust filtering and smoothing algorithm based on gibbs sampling ."}
{"title": "pose-selective max pooling for measuring similarity", "abstract": "in this paper , we deal with two challenges for measuring the similarity of the subject identities in practical video-based face recognition - the variation of the head pose in uncontrolled environments and the computational expense of processing videos . since the frame-wise feature mean is unable to characterize the pose diversity among frames , we define and preserve the overall pose diversity and closeness in a video . then , identity will be the only source of variation across videos since the pose varies even within a single video . instead of simply using all the frames , we select those faces whose pose point is closest to the centroid of the k-means cluster containing that pose point . then , we represent a video as a bag of frame-wise deep face features while the number of features has been reduced from hundreds to k. since the video representation can well represent the identity , now we measure the subject similarity between two videos as the max correlation among all possible pairs in the two bags of features . on the official 5,000 video-pairs of the youtube face dataset for face verification , our algorithm achieves a comparable performance with vgg-face that averages over deep features of all frames . other vision tasks can also benefit from the generic idea of employing geometric cues to improve the descriptiveness of deep features ."}
{"title": "provably optimal algorithms for generalized linear contextual bandits", "abstract": "contextual bandits are widely used in internet services from news recommendation to advertising , and to web search . generalized linear models ( logistical regression in particular ) have demonstrated stronger performance than linear models in many applications where rewards are binary . however , most theoretical analyses on contextual bandits so far are on linear bandits . in this work , we propose an upper confidence bound based algorithm for generalized linear contextual bandits , which achieves an $ \\tilde { o } ( \\sqrt { dt } ) $ regret over $ t $ rounds with $ d $ dimensional feature vectors . this regret matches the minimax lower bound , up to logarithmic terms , and improves on the best previous result by a $ \\sqrt { d } $ factor , assuming the number of arms is fixed . a key component in our analysis is to establish a new , sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models , which may be of independent interest . we also analyze a simpler upper confidence bound algorithm , which is useful in practice , and prove it to have optimal regret for certain cases ."}
{"title": "fine-grained word sense disambiguation based on parallel corpora , word alignment , word clustering and aligned wordnets", "abstract": "the paper presents a method for word sense disambiguation based on parallel corpora . the method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus . the wordnets are aligned to the princeton wordnet , according to the principles established by eurowordnet . the evaluation of the wsd system , implementing the method described herein showed very encouraging results . the same system used in a validation mode , can be used to check and spot alignment errors in multilingually aligned wordnets as balkanet and eurowordnet ."}
{"title": "hybrid code networks : practical and efficient end-to-end dialog control with supervised and reinforcement learning", "abstract": "end-to-end learning of recurrent neural networks ( rnns ) is an attractive solution for dialog systems ; however , current techniques are data-intensive and require thousands of dialogs to learn simple behaviors . we introduce hybrid code networks ( hcns ) , which combine an rnn with domain-specific knowledge encoded as software and system action templates . compared to existing end-to-end approaches , hcns considerably reduce the amount of training data required , while retaining the key benefit of inferring a latent representation of dialog state . in addition , hcns can be optimized with supervised learning , reinforcement learning , or a mixture of both . hcns attain state-of-the-art performance on the babi dialog dataset , and outperform two commercially deployed customer-facing dialog systems ."}
{"title": "non-adaptive policies for 20 questions target localization", "abstract": "the problem of target localization with noise is addressed . the target is a sample from a continuous random variable with known distribution and the goal is to locate it with minimum mean squared error distortion . the localization scheme or policy proceeds by queries , or questions , weather or not the target belongs to some subset as it is addressed in the 20-question framework . these subsets are not constrained to be intervals and the answers to the queries are noisy . while this situation is well studied for adaptive querying , this paper is focused on the non adaptive querying policies based on dyadic questions . the asymptotic minimum achievable distortion under such policies is derived . furthermore , a policy named the aurelian1 is exhibited which achieves asymptotically this distortion ."}
{"title": "estimating the margin of victory of an election using sampling", "abstract": "the margin of victory of an election is a useful measure to capture the robustness of an election outcome . it also plays a crucial role in determining the sample size of various algorithms in post election audit , polling etc . in this work , we present efficient sampling based algorithms for estimating the margin of victory of elections . more formally , we introduce the \\textsc { $ ( c , \\epsilon , \\delta ) $ -- margin of victory } problem , where given an election $ \\mathcal { e } $ on $ n $ voters , the goal is to estimate the margin of victory $ m ( \\mathcal { e } ) $ of $ \\mathcal { e } $ within an additive factor of $ c mov ( \\mathcal { e } ) +\\epsilon n $ . we study the \\textsc { $ ( c , \\epsilon , \\delta ) $ -- margin of victory } problem for many commonly used voting rules including scoring rules , approval , bucklin , maximin , and copeland $ ^ { \\alpha } . $ we observe that even for the voting rules for which computing the margin of victory is np-hard , there may exist efficient sampling based algorithms , as observed in the cases of maximin and copeland $ ^ { \\alpha } $ voting rules ."}
{"title": "an ordinal view of independence with application to plausible reasoning", "abstract": "an ordinal view of independence is studied in the framework of possibility theory . we investigate three possible definitions of dependence , of increasing strength . one of them is the counterpart to the multiplication law in probability theory , and the two others are based on the notion of conditional possibility . these two have enough expressive power to support the whole possibility theory , and a complete axiomatization is provided for the strongest one . moreover we show that weak independence is well-suited to the problems of belief change and plausible reasoning , especially to address the problem of blocking of property inheritance in exception-tolerant taxonomic reasoning ."}
{"title": "a comprehensive implementation of conceptual spaces", "abstract": "the highly influential framework of conceptual spaces provides a geometric way of representing knowledge . instances are represented by points and concepts are represented by regions in a ( potentially ) high-dimensional space . based on our recent formalization , we present a comprehensive implementation of the conceptual spaces framework that is not only capable of representing concepts with inter-domain correlations , but that also offers a variety of operations on these concepts ."}
{"title": "computational aspects of the calculus of structure", "abstract": "logic is the science of correct inferences and a logical system is a tool to prove assertions in a certain logic in a correct way . there are many logical systems , and many ways of formalizing them , e.g. , using natural deduction or sequent calculus . calculus of structures ( cos ) is a new formalism proposed by alessio guglielmi in 2004 that generalizes sequent calculus in the sense that inference rules can be applied at any depth inside a formula , rather than only to the main connective . with this feature , proofs in cos are shorter than in any other formalism supporting analytical proofs . although it is great to have the freedom and expressiveness of cos , under the point of view of proof search more freedom means a larger search space . and that should be restricted when looking for complete automation of deductive systems . some efforts were made to reduce this non-determinism , but they are all basically operational approaches , and no solid theoretical result regarding the computational behaviour of cos has been achieved so far . the main focus of this thesis is to discuss ways to propose a proof search strategy for cos suitable to implementation . this strategy should be theoretical instead of purely operational . we introduce the concept of incoherence number of substructures inside structures and we use this concept to achieve our main result : there is an algorithm that , according to our conjecture , corresponds to a proof search strategy to every provable structure in the subsystem of fbv ( the multiplicative linear logic mll plus the rule mix ) containing only pairwise distinct atoms . our algorithm is implemented and we believe our strategy is a good starting point to exploit the computational aspects of cos in more general systems , like bv itself ."}
{"title": "a complete framework for ambush avoidance in realistic environments", "abstract": "operating vehicles in adversarial environments between a recurring origin-destination pair requires new planning techniques . a two players zero-sum game is introduced . the goal of the first player is to minimize the expected casualties undergone by a convoy . the goal of the second player is to maximize this damage . the outcome of the game is obtained via a linear program that solves the corresponding minmax optimization problem over this outcome . different environment models are defined in order to compute routing strategies over unstructured environments . to compare these methods for increasingly accurate representations of the environment , a grid-based model is chosen to represent the environment and the existence of a sufficient network size is highlighted . a global framework for the generation of realistic routing strategies between any two points is described . this framework requires a good assessment of the potential casualties at any location , therefore the most important parameters are identified . finally the framework is tested on real world environments ."}
{"title": "relational reasoning in the region connection calculus", "abstract": "this paper is mainly concerned with the relation-algebraical aspects of the well-known region connection calculus ( rcc ) . we show that the contact relation algebra ( cra ) of certain rcc model is not atomic complete and hence infinite . so in general an extensional composition table for the rcc can not be obtained by simply refining the rcc8 relations . after having shown that each rcc model is a consistent model of the rcc11 ct , we give an exhaustive investigation about extensional interpretation of the rcc11 ct. more important , we show the complemented closed disk algebra is a representation for the relation algebra determined by the rcc11 table . the domain of this algebra contains two classes of regions , the closed disks and closures of their complements in the real plane ."}
{"title": "ultrafast photonic reinforcement learning based on laser chaos", "abstract": "reinforcement learning involves decision making in dynamic and uncertain environments , and constitutes one important element of artificial intelligence ( ai ) . in this paper , we experimentally demonstrate that the ultrafast chaotic oscillatory dynamics of lasers efficiently solve the multi-armed bandit problem ( mab ) , which requires decision making concerning a class of difficult trade-offs called the exploration-exploitation dilemma . to solve the mab , a certain degree of randomness is required for exploration purposes . however , pseudo-random numbers generated using conventional electronic circuitry encounter severe limitations in terms of their data rate and the quality of randomness due to their algorithmic foundations . we generate laser chaos signals using a semiconductor laser sampled at a maximum rate of 100 gsample/s , and combine it with a simple decision-making principle called tug-of-war with a variable threshold , to ensure ultrafast , adaptive and accurate decision making at a maximum adaptation speed of 1 ghz . we found that decision-making performance was maximized with an optimal sampling interval , and we highlight the exact coincidence between the negative autocorrelation inherent in laser chaos and decision-making performance . this study paves the way for a new realm of ultrafast photonics in the age of ai , where the ultrahigh bandwidth of photons can provide new value ."}
{"title": "interpreting finite automata for sequential data", "abstract": "automaton models are often seen as interpretable models . interpretability itself is not well defined : it remains unclear what interpretability means without first explicitly specifying objectives or desired attributes . in this paper , we identify the key properties used to interpret automata and propose a modification of a state-merging approach to learn variants of finite state automata . we apply the approach to problems beyond typical grammar inference tasks . additionally , we cover several use-cases for prediction , classification , and clustering on sequential data in both supervised and unsupervised scenarios to show how the identified key properties are applicable in a wide range of contexts ."}
{"title": "speeding-up the decision making of a learning agent using an ion trap quantum processor", "abstract": "we report a proof-of-principle experimental demonstration of the quantum speed-up for learning agents utilizing a small-scale quantum information processor based on radiofrequency-driven trapped ions . the decision-making process of a quantum learning agent within the projective simulation paradigm for machine learning is implemented in a system of two qubits . the latter are realized using hyperfine states of two frequency-addressed atomic ions exposed to a static magnetic field gradient . we show that the deliberation time of this quantum learning agent is quadratically improved with respect to comparable classical learning agents . the performance of this quantum-enhanced learning agent highlights the potential of scalable quantum processors taking advantage of machine learning ."}
{"title": "vision-based road detection in automotive systems : a real-time expectation-driven approach", "abstract": "the main aim of this work is the development of a vision-based road detection system fast enough to cope with the difficult real-time constraints imposed by moving vehicle applications . the hardware platform , a special-purpose massively parallel system , has been chosen to minimize system production and operational costs . this paper presents a novel approach to expectation-driven low-level image segmentation , which can be mapped naturally onto mesh-connected massively parallel simd architectures capable of handling hierarchical data structures . the input image is assumed to contain a distorted version of a given template ; a multiresolution stretching process is used to reshape the original template in accordance with the acquired image content , minimizing a potential function . the distorted template is the process output ."}
{"title": "separate training for conditional random fields using co-occurrence rate factorization", "abstract": "the standard training method of conditional random fields ( crfs ) is very slow for large-scale applications . as an alternative , piecewise training divides the full graph into pieces , trains them independently , and combines the learned weights at test time . in this paper , we present \\emph { separate } training for undirected models based on the novel co-occurrence rate factorization ( cr-f ) . separate training is a local training method . in contrast to memms , separate training is unaffected by the label bias problem . experiments show that separate training ( i ) is unaffected by the label bias problem ; ( ii ) reduces the training time from weeks to seconds ; and ( iii ) obtains competitive results to the standard and piecewise training on linear-chain crfs ."}
{"title": "business process deviance mining : review and evaluation", "abstract": "business process deviance refers to the phenomenon whereby a subset of the executions of a business process deviate , in a negative or positive way , with respect to its expected or desirable outcomes . deviant executions of a business process include those that violate compliance rules , or executions that undershoot or exceed performance targets . deviance mining is concerned with uncovering the reasons for deviant executions by analyzing business process event logs . this article provides a systematic review and comparative evaluation of deviance mining approaches based on a family of data mining techniques known as sequence classification . using real-life logs from multiple domains , we evaluate a range of feature types and classification methods in terms of their ability to accurately discriminate between normal and deviant executions of a process . we also analyze the interestingness of the rule sets extracted using different methods . we observe that feature sets extracted using pattern mining techniques only slightly outperform simpler feature sets based on counts of individual activity occurrences in a trace ."}
{"title": "visualization of collaborative data", "abstract": "collaborative data consist of ratings relating two distinct sets of objects : users and items . much of the work with such data focuses on filtering : predicting unknown ratings for pairs of users and items . in this paper we focus on the problem of visualizing the information . given all of the ratings , our task is to embed all of the users and items as points in the same euclidean space . we would like to place users near items that they have rated ( or would rate ) high , and far away from those they would give a low rating . we pose this problem as a real-valued non-linear bayesian network and employ markov chain monte carlo and expectation maximization to find an embedding . we present a metric by which to judge the quality of a visualization and compare our results to local linear embedding and eigentaste on three real-world datasets ."}
{"title": "holstep : a machine learning dataset for higher-order logic theorem proving", "abstract": "large computer-understandable proofs consist of millions of intermediate logical steps . the vast majority of such steps originate from manually selected and manually guided heuristics applied to intermediate goals . so far , machine learning has generally not been used to filter or generate these steps . in this paper , we introduce a new dataset based on higher-order logic ( hol ) proofs , for the purpose of developing new machine learning-based theorem-proving strategies . we make this dataset publicly available under the bsd license . we propose various machine learning tasks that can be performed on this dataset , and discuss their significance for theorem proving . we also benchmark a set of simple baseline machine learning models suited for the tasks ( including logistic regression , convolutional neural networks and recurrent neural networks ) . the results of our baseline models show the promise of applying machine learning to hol theorem proving ."}
{"title": "belief hidden markov model for speech recognition", "abstract": "speech recognition searches to predict the spoken words automatically . these systems are known to be very expensive because of using several pre-recorded hours of speech . hence , building a model that minimizes the cost of the recognizer will be very interesting . in this paper , we present a new approach for recognizing speech based on belief hmms instead of proba-bilistic hmms . experiments shows that our belief recognizer is insensitive to the lack of the data and it can be trained using only one exemplary of each acoustic unit and it gives a good recognition rates . consequently , using the belief hmm recognizer can greatly minimize the cost of these systems ."}
{"title": "allsat compressed with wildcards . part 1 : converting cnf 's to orthogonal dnf 's", "abstract": "for most branching algorithms in boolean logic `` branching '' means `` variable-wise branching '' . we present the apparently novel technique of clause-wise branching , which is used to solve the allsat problem for arbitrary boolean functions in cnf format . specifically , it converts a cnf into an orthogonal dnf , i.e . into an exclusive sum of products . our method is enhanced by two ingredients : the use of a good sat-solver and wildcards beyond the common don't-care symbol ."}
{"title": "efficient probabilistic inference with partial ranking queries", "abstract": "distributions over rankings are used to model data in various settings such as preference analysis and political elections . the factorial size of the space of rankings , however , typically forces one to make structural assumptions , such as smoothness , sparsity , or probabilistic independence about these underlying distributions . we approach the modeling problem from the computational principle that one should make structural assumptions which allow for efficient calculation of typical probabilistic queries . for ranking models , `` typical '' queries predominantly take the form of partial ranking queries ( e.g. , given a user 's top-k favorite movies , what are his preferences over remaining movies ? ) . in this paper , we argue that riffled independence factorizations proposed in recent literature [ 7 , 8 ] are a natural structural assumption for ranking distributions , allowing for particularly efficient processing of partial ranking queries ."}
{"title": "filtered fictitious play for perturbed observation potential games and decentralised pomdps", "abstract": "potential games and decentralised partially observable mdps ( dec-pomdps ) are two commonly used models of multi-agent interaction , for static optimisation and sequential decisionmaking settings , respectively . in this paper we introduce filtered fictitious play for solving repeated potential games in which each player 's observations of others ' actions are perturbed by random noise , and use this algorithm to construct an online learning method for solving dec-pomdps . specifically , we prove that noise in observations prevents standard fictitious play from converging to nash equilibrium in potential games , which also makes fictitious play impractical for solving dec-pomdps . to combat this , we derive filtered fictitious play , and provide conditions under which it converges to a nash equilibrium in potential games with noisy observations . we then use filtered fictitious play to construct a solver for dec-pomdps , and demonstrate our new algorithm 's performance in a box pushing problem . our results show that we consistently outperform the state-of-the-art dec-pomdp solver by an average of 100 % across the range of noise in the observation function ."}
{"title": "hybrid probabilistic programs : algorithms and complexity", "abstract": "hybrid probabilistic programs ( hpps ) are logic programs that allow the programmer to explicitly encode his knowledge of the dependencies between events being described in the program . in this paper , we classify hpps into three classes called hpp_1 , hpp_2 and hpp_r , r > = 3. for these classes , we provide three types of results for hpps . first , we develop algorithms to compute the set of all ground consequences of an hpp . then we provide algorithms and complexity results for the problems of entailment ( `` given an hpp p and a query q as input , is q a logical consequence of p ? '' ) and consistency ( `` given an hpp p as input , is p consistent ? '' ) . our results provide a fine characterization of when polynomial algorithms exist for the above problems , and when these problems become intractable ."}
{"title": "foundations for uniform interpolation and forgetting in expressive description logics", "abstract": "we study uniform interpolation and forgetting in the description logic alc . our main results are model-theoretic characterizations of uniform inter- polants and their existence in terms of bisimula- tions , tight complexity bounds for deciding the existence of uniform interpolants , an approach to computing interpolants when they exist , and tight bounds on their size . we use a mix of model- theoretic and automata-theoretic methods that , as a by-product , also provides characterizations of and decision procedures for conservative extensions ."}
{"title": "mixed-membership stochastic block-models for transactional networks", "abstract": "transactional network data can be thought of as a list of one-to-many communications ( e.g. , email ) between nodes in a social network . most social network models convert this type of data into binary relations between pairs of nodes . we develop a latent mixed membership model capable of modeling richer forms of transactional network data , including relations between more than two nodes . the model can cluster nodes and predict transactions . the block-model nature of the model implies that groups can be characterized in very general ways . this flexible notion of group structure enables discovery of rich structure in transactional networks . estimation and inference are accomplished via a variational em algorithm . simulations indicate that the learning algorithm can recover the correct generative model . interesting structure is discovered in the enron email dataset and another dataset extracted from the reddit website . analysis of the reddit data is facilitated by a novel performance measure for comparing two soft clusterings . the new model is superior at discovering mixed membership in groups and in predicting transactions ."}
{"title": "the alldifferent constraint with precedences", "abstract": "we propose alldiffprecedence , a new global constraint that combines together an alldifferent constraint with precedence constraints that strictly order given pairs of variables . we identify a number of applications for this global constraint including instruction scheduling and symmetry breaking . we give an efficient propagation algorithm that enforces bounds consistency on this global constraint . we show how to implement this propagator using a decomposition that extends the bounds consistency enforcing decomposition proposed for the alldifferent constraint . finally , we prove that enforcing domain consistency on this global constraint is np-hard in general ."}
{"title": "learning cooperative visual dialog agents with deep reinforcement learning", "abstract": "we introduce the first goal-driven training for visual question answering and dialog agents . specifically , we pose a cooperative 'image guessing ' game between two agents -- qbot and abot -- who communicate in natural language dialog so that qbot can select an unseen image from a lineup of images . we use deep reinforcement learning ( rl ) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward . we demonstrate two experimental results . first , as a 'sanity check ' demonstration of pure rl ( from scratch ) , we show results on a synthetic world , where the agents communicate in ungrounded vocabulary , i.e. , symbols with no pre-specified meanings ( x , y , z ) . we find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes ( shape/color/style ) . thus , we demonstrate the emergence of grounded language and communication among 'visual ' dialog agents with no human supervision . second , we conduct large-scale real-image experiments on the visdial dataset , where we pretrain with supervised dialog data and show that the rl 'fine-tuned ' agents significantly outperform sl agents . interestingly , the rl qbot learns to ask questions that abot is good at , ultimately resulting in more informative dialog and a better team ."}
{"title": "identifying the relevant nodes without learning the model", "abstract": "we propose a method to identify all the nodes that are relevant to compute all the conditional probability distributions for a given set of nodes . our method is simple , effcient , consistent , and does not require learning a bayesian network first . therefore , our method can be applied to high-dimensional databases , e.g . gene expression databases ."}
{"title": "semantic folding theory and its application in semantic fingerprinting", "abstract": "human language is recognized as a very complex domain since decades . no computer system has been able to reach human levels of performance so far . the only known computational system capable of proper language processing is the human brain . while we gather more and more data about the brain , its fundamental computational processes still remain obscure . the lack of a sound computational brain theory also prevents the fundamental understanding of natural language processing . as always when science lacks a theoretical foundation , statistical modeling is applied to accommodate as many sampled real-world data as possible . an unsolved fundamental issue is the actual representation of language ( data ) within the brain , denoted as the representational problem . starting with jeff hawkins ' hierarchical temporal memory ( htm ) theory , a consistent computational theory of the human cortex , we have developed a corresponding theory of language data representation : the semantic folding theory . the process of encoding words , by using a topographic semantic space as distributional reference frame into a sparse binary representational vector is called semantic folding and is the central topic of this document . semantic folding describes a method of converting language from its symbolic representation ( text ) into an explicit , semantically grounded representation that can be generically processed by hawkins ' htm networks . as it turned out , this change in representation , by itself , can solve many complex nlp problems by applying boolean operators and a generic similarity function like the euclidian distance . many practical problems of statistical nlp systems , like the high cost of computation , the fundamental incongruity of precision and recall , the complex tuning procedures etc. , can be elegantly overcome by applying semantic folding ."}
{"title": "a formalism for causal explanations with an answer set programming translation", "abstract": "we examine the practicality for a user of using answer set programming ( asp ) for representing logical formalisms . our example is a formalism aiming at capturing causal explanations from causal information . we show the naturalness and relative efficiency of this translation job . we are interested in the ease for writing an asp program . limitations of the earlier systems made that in practice , the `` declarative aspect '' was more theoretical than practical . we show how recent improvements in working asp systems facilitate the translation ."}
{"title": "deep reinforcement learning for event-driven multi-agent decision processes", "abstract": "the incorporation of macro-actions ( temporally extended actions ) into multi-agent decision problems has the potential to address the curse of dimensionality associated with such decision problems . since macro-actions last for stochastic durations , multiple agents executing decentralized policies in cooperative environments must act asynchronously . we present an algorithm that modifies generalized advantage estimation for temporally extended actions , allowing a state-of-the-art policy optimization algorithm to optimize policies in dec-pomdps in which agents act asynchronously . we show that our algorithm is capable of learning optimal policies in two cooperative domains , one involving real-time bus holding control and one involving wildfire fighting with unmanned aircraft . our algorithm works by framing problems as `` event-driven decision processes , '' which are scenarios where the sequence and timing of actions and events are random and governed by an underlying stochastic process . in addition to optimizing policies with continuous state and action spaces , our algorithm also facilitates the use of event-driven simulators , which do not require time to be discretized into time-steps . we demonstrate the benefit of using event-driven simulation in the context of multiple agents taking asynchronous actions . we show that fixed time-step simulation risks obfuscating the sequence in which closely-separated events occur , adversely affecting the policies learned . additionally , we show that arbitrarily shrinking the time-step scales poorly with the number of agents ."}
{"title": "entity embeddings with conceptual subspaces as a basis for plausible reasoning", "abstract": "conceptual spaces are geometric representations of conceptual knowledge , in which entities correspond to points , natural properties correspond to convex regions , and the dimensions of the space correspond to salient features . while conceptual spaces enable elegant models of various cognitive phenomena , the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence . to address this issue , we propose a method which learns a vector-space embedding of entities from wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace . we experimentally demonstrate the usefulness of these subspaces as ( approximate ) conceptual space representations by showing , among others , that important features can be modelled as directions and that natural properties tend to correspond to convex regions ."}
{"title": "best-first and depth-first minimax search in practice", "abstract": "most practitioners use a variant of the alpha-beta algorithm , a simple depth-first pro- cedure , for searching minimax trees . sss* , with its best-first search strategy , reportedly offers the potential for more efficient search . however , the complex formulation of the al- gorithm and its alleged excessive memory requirements preclude its use in practice . for two decades , the search efficiency of `` smart '' best-first sss* has cast doubt on the effectiveness of `` dumb '' depth-first alpha-beta . this paper presents a simple framework for calling alpha-beta that allows us to create a variety of algorithms , including sss* and dual* . in effect , we formulate a best-first algorithm using depth-first search . expressed in this framework sss* is just a special case of alpha-beta , solving all of the perceived drawbacks of the algorithm . in practice , alpha-beta variants typically evaluate less nodes than sss* . a new instance of this framework , mtd ( f ) , out-performs sss* and negascout , the alpha-beta variant of choice by practitioners ."}
{"title": "ordered { and , or } -decomposition and binary-decision diagram", "abstract": "in the context of knowledge compilation ( kc ) , we study the effect of augmenting ordered binary decision diagrams ( obdd ) with two kinds of decomposition nodes , i.e. , and-vertices and or-vertices which denote conjunctive and disjunctive decomposition of propositional knowledge bases , respectively . the resulting knowledge compilation language is called ordered { and , or } -decomposition and binary-decision diagram ( oaodd ) . roughly speaking , several previous languages can be seen as special types of oaodd , including obdd , and/or binary decision diagram ( aobdd ) , obdd with implied literals ( obdd-l ) , multi-level decomposition diagrams ( mldd ) . on the one hand , we propose some families of algorithms which can convert some fragments of oaodd into others ; on the other hand , we present a rich set of polynomial-time algorithms that perform logical operations . according to these algorithms , as well as theoretical analysis , we characterize the space efficiency and tractability of oaodd and its some fragments with respect to the evaluating criteria in the kc map . finally , we present a compilation algorithm which can convert formulas in negative normal form into oaodd ."}
{"title": "multimodal sentiment analysis with word-level fusion and reinforcement learning", "abstract": "with the increasing popularity of video sharing websites such as youtube and facebook , multimodal sentiment analysis has received increasing attention from the scientific community . contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity , we develop a novel deep architecture for multimodal sentiment analysis that performs modality fusion at the word level . in this paper , we propose the gated multimodal embedding lstm with temporal attention ( gme-lstm ( a ) ) model that is composed of 2 modules . the gated multimodal embedding alleviates the difficulties of fusion when there are noisy modalities . the lstm with temporal attention performs word level fusion at a finer fusion resolution between input modalities and attends to the most important time steps . as a result , the gme-lstm ( a ) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension . we demonstrate the effectiveness of this approach on the publicly-available multimodal corpus of sentiment intensity and subjectivity analysis ( cmu-mosi ) dataset by achieving state-of-the-art sentiment classification and regression results . qualitative analysis on our model emphasizes the importance of the temporal attention layer in sentiment prediction because the additional acoustic and visual modalities are noisy . we also demonstrate the effectiveness of the gated multimodal embedding in selectively filtering these noisy modalities out . our results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion ."}
{"title": "a novel hierarchical ant based qos aware intelligent routing scheme for manets", "abstract": "manet is a collection of mobile devices with no centralized control and no pre-existing infrastructures . due to the nodal mobility , supporting qos during routing in this type of networks is a very challenging task . to tackle this type of overhead many routing algorithms with clustering approach have been proposed . clustering is an effective method for resource management regarding network performance , routing protocol design , qos etc . most of the flat network architecture contains homogeneous capacity of nodes but in real time nodes are with heterogeneous capacity and transmission power . hierarchical routing provides routing through this kind of heterogeneous nodes . here , routes can be recorded hierarchically , across clusters to increase routing flexibility . besides this , it increases scalability and robustness of routes . in this paper , a novel ant based qos aware routing is proposed on a three level hierarchical cluster based topology in manet which will be more scalable and efficient compared to flat architecture and will give better throughput ."}
{"title": "the dynamic controllability of conditional stns with uncertainty", "abstract": "recent attempts to automate business processes and medical-treatment processes have uncovered the need for a formal framework that can accommodate not only temporal constraints , but also observations and actions with uncontrollable durations . to meet this need , this paper defines a conditional simple temporal network with uncertainty ( cstnu ) that combines the simple temporal constraints from a simple temporal network ( stn ) with the conditional nodes from a conditional simple temporal problem ( cstp ) and the contingent links from a simple temporal network with uncertainty ( stnu ) . a notion of dynamic controllability for a cstnu is defined that generalizes the dynamic consistency of a ctp and the dynamic controllability of an stnu . the paper also presents some sound constraint-propagation rules for dynamic controllability that are expected to form the backbone of a dynamic-controllability-checking algorithm for cstnus ."}
{"title": "a forgetting-based approach to merging knowledge bases", "abstract": "this paper presents a novel approach based on variable forgetting , which is a useful tool in resolving contradictory by filtering some given variables , to merging multiple knowledge bases . this paper first builds a relationship between belief merging and variable forgetting by using dilation . variable forgetting is applied to capture belief merging operation . finally , some new merging operators are developed by modifying candidate variables to amend the shortage of traditional merging operators . different from model selection of traditional merging operators , as an alternative approach , variable selection in those new operators could provide intuitive information about an atom variable among whole knowledge bases ."}
{"title": "macro-ff : improving ai planning with automatically learned macro-operators", "abstract": "despite recent progress in ai planning , many benchmarks remain challenging for current planners . in many domains , the performance of a planner can greatly be improved by discovering and exploiting information about the domain structure that is not explicitly encoded in the initial pddl formulation . in this paper we present and compare two automated methods that learn relevant information from previous experience in a domain and use it to solve new problem instances . our methods share a common four-step strategy . first , a domain is analyzed and structural information is extracted , then macro-operators are generated based on the previously discovered structure . a filtering and ranking procedure selects the most useful macro-operators . finally , the selected macros are used to speed up future searches . we have successfully used such an approach in the fourth international planning competition ipc-4 . our system , macro-ff , extends hoffmanns state-of-the-art planner ff 2.3 with support for two kinds of macro-operators , and with engineering enhancements . we demonstrate the effectiveness of our ideas on benchmarks from international planning competitions . our results indicate a large reduction in search effort in those complex domains where structural information can be inferred ."}
{"title": "some epistemological problems with the knowledge level in cognitive architectures", "abstract": "this article addresses an open problem in the area of cognitive systems and architectures : namely the problem of handling ( in terms of processing and reasoning capabilities ) complex knowledge structures that can be at least plausibly comparable , both in terms of size and of typology of the encoded information , to the knowledge that humans process daily for executing everyday activities . handling a huge amount of knowledge , and selectively retrieve it ac- cording to the needs emerging in different situational scenarios , is an important aspect of human intelligence . for this task , in fact , humans adopt a wide range of heuristics ( gigerenzer and todd ) due to their bounded rationality ( simon , 1957 ) . in this perspective , one of the re- quirements that should be considered for the design , the realization and the evaluation of intelligent cognitively inspired systems should be represented by their ability of heuristically identify and retrieve , from the general knowledge stored in their artificial long term memory ( ltm ) , that one which is synthetically and contextually relevant . this require- ment , however , is often neglected . currently , artificial cognitive systems and architectures are not able , de facto , to deal with complex knowledge structures that can be even slightly comparable to the knowledge heuris- tically managed by humans . in this paper i will argue that this is not only a technological problem but also an epistemological one and i will briefly sketch a proposal for a possible solution ."}
{"title": "a concurrent fuzzy-neural network approach for decision support systems", "abstract": "decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved . the past few years have witnessed a growing recognition of soft computing technologies that underlie the conception , design and utilization of intelligent systems . several works have been done where engineers and scientists have applied intelligent techniques and heuristics to obtain optimal decisions from imprecise information . in this paper , we present a concurrent fuzzy-neural network approach combining unsupervised and supervised learning techniques to develop the tactical air combat decision support system ( tacdss ) . experiment results clearly demonstrate the efficiency of the proposed technique ."}
{"title": "chemgan challenge for drug discovery : can ai reproduce natural chemical diversity ?", "abstract": "generating molecules with desired chemical properties is important for drug discovery . the use of generative neural networks is promising for this task . however , from visual inspection , it often appears that generated samples lack diversity . in this paper , we quantify this internal chemical diversity , and we raise the following challenge : can a nontrivial ai model reproduce natural chemical diversity for desired molecules ? to illustrate this question , we consider two generative models : a reinforcement learning model and the recently introduced organ . both fail at this challenge . we hope this challenge will stimulate research in this direction ."}
{"title": "compositional stochastic modeling and probabilistic programming", "abstract": "probabilistic programming is related to a compositional approach to stochastic modeling by switching from discrete to continuous time dynamics . in continuous time , an operator-algebra semantics is available in which processes proceeding in parallel ( and possibly interacting ) have summed time-evolution operators . from this foundation , algorithms for simulation , inference and model reduction may be systematically derived . the useful consequences are potentially far-reaching in computational science , machine learning and beyond . hybrid compositional stochastic modeling/probabilistic programming approaches may also be possible ."}
{"title": "modeling the experience of emotion", "abstract": "affective computing has proven to be a viable field of research comprised of a large number of multidisciplinary researchers resulting in work that is widely published . the majority of this work consists of computational models of emotion recognition , computational modeling of causal factors of emotion and emotion expression through rendered and robotic faces . a smaller part is concerned with modeling the effects of emotion , formal modeling of cognitive appraisal theory and models of emergent emotions . part of the motivation for affective computing as a field is to better understand emotional processes through computational modeling . one of the four major topics in affective computing is computers that have emotions ( the others are recognizing , expressing and understanding emotions ) . a critical and neglected aspect of having emotions is the experience of emotion ( barrett , mesquita , ochsner , and gross , 2007 ) : what does the content of an emotional episode look like , how does this content change over time and when do we call the episode emotional . few modeling efforts have these topics as primary focus . the launch of a journal on synthetic emotions should motivate research initiatives in this direction , and this research should have a measurable impact on emotion research in psychology . i show that a good way to do so is to investigate the psychological core of what an emotion is : an experience . i present ideas on how the experience of emotion could be modeled and provide evidence that several computational models of emotion are already addressing the issue ."}
{"title": "an evolutionary approach to function", "abstract": "background : understanding the distinction between function and role is vexing and difficult . while it appears to be useful , in practice this distinction is hard to apply , particularly within biology . results : i take an evolutionary approach , considering a series of examples , to develop and generate definitions for these concepts . i test them in practice against the ontology for biomedical investigations ( obi ) . finally , i give an axiomatisation and discuss methods for applying these definitions in practice . conclusions : the definitions in this paper are applicable , formalizing current practice . as such , they make a significant contribution to the use of these concepts within biomedical ontologies ."}
{"title": "validation of nonlinear pca", "abstract": "linear principal component analysis ( pca ) can be extended to a nonlinear pca by using artificial neural networks . but the benefit of curved components requires a careful control of the model complexity . moreover , standard techniques for model selection , including cross-validation and more generally the use of an independent test set , fail when applied to nonlinear pca because of its inherent unsupervised characteristics . this paper presents a new approach for validating the complexity of nonlinear pca models by using the error in missing data estimation as a criterion for model selection . it is motivated by the idea that only the model of optimal complexity is able to predict missing values with the highest accuracy . while standard test set validation usually favours over-fitted nonlinear pca models , the proposed model validation approach correctly selects the optimal model complexity ."}
{"title": "near-optimal bayesian active learning with correlated and noisy tests", "abstract": "we consider the bayesian active learning and experimental design problem , where the goal is to learn the value of some unknown target variable through a sequence of informative , noisy tests . in contrast to prior work , we focus on the challenging , yet practically relevant setting where test outcomes can be conditionally dependent given the hidden target variable . under such assumptions , common heuristics , such as greedily performing tests that maximize the reduction in uncertainty of the target , often perform poorly . in this paper , we propose eced , a novel , computationally efficient active learning algorithm , and prove strong theoretical guarantees that hold with correlated , noisy tests . rather than directly optimizing the prediction error , at each step , eced picks the test that maximizes the gain in a surrogate objective , which takes into account the dependencies between tests . our analysis relies on an information-theoretic auxiliary function to track the progress of eced , and utilizes adaptive submodularity to attain the near-optimal bound . we demonstrate strong empirical performance of eced on two problem instances , including a bayesian experimental design task intended to distinguish among economic theories of how people make risky decisions , and an active preference learning task via pairwise comparisons ."}
{"title": "inducing probabilistic programs by bayesian program merging", "abstract": "this report outlines an approach to learning generative models from data . we express models as probabilistic programs , which allows us to capture abstract patterns within the examples . by choosing our language for programs to be an extension of the algebraic data type of the examples , we can begin with a program that generates all and only the examples . we then introduce greater abstraction , and hence generalization , incrementally to the extent that it improves the posterior probability of the examples given the program . motivated by previous approaches to model merging and program induction , we search for such explanatory abstractions using program transformations . we consider two types of transformation : abstraction merges common subexpressions within a program into new functions ( a form of anti-unification ) . deargumentation simplifies functions by reducing the number of arguments . we demonstrate that this approach finds key patterns in the domain of nested lists , including parameterized sub-functions and stochastic recursion ."}
{"title": "mitre at semeval-2016 task 6 : transfer learning for stance detection", "abstract": "we describe mitre 's submission to the semeval-2016 task 6 , detecting stance in tweets . this effort achieved the top score in task a on supervised stance detection , producing an average f1 score of 67.8 when assessing whether a tweet author was in favor or against a topic . we employed a recurrent neural network initialized with features learned via distant supervision on two large unlabeled datasets . we trained embeddings of words and phrases with the word2vec skip-gram method , then used those features to learn sentence representations via a hashtag prediction auxiliary task . these sentence vectors were then fine-tuned for stance detection on several hundred labeled examples . the result was a high performing system that used transfer learning to maximize the value of the available training data ."}
{"title": "a formally verified proof of the prime number theorem", "abstract": "the prime number theorem , established by hadamard and de la vall'ee poussin independently in 1896 , asserts that the density of primes in the positive integers is asymptotic to 1 / ln x. whereas their proofs made serious use of the methods of complex analysis , elementary proofs were provided by selberg and erd '' os in 1948. we describe a formally verified version of selberg 's proof , obtained using the isabelle proof assistant ."}
{"title": "multi-granular perspectives on covering", "abstract": "covering model provides a general framework for granular computing in that overlapping among granules are almost indispensable . for any given covering , both intersection and union of covering blocks containing an element are exploited as granules to form granular worlds at different abstraction levels , respectively , and transformations among these different granular worlds are also discussed . as an application of the presented multi-granular perspective on covering , relational interpretation and axiomization of four types of covering based rough upper approximation operators are investigated , which can be dually applied to lower ones ."}
{"title": "towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning", "abstract": "developing a safe and efficient collision avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generate its paths without observing other robots ' states and intents . while other distributed multi-robot collision avoidance systems exist , they often require extracting agent-level features to plan a local collision-free action , which can be computationally prohibitive and not robust . more importantly , in practice the performance of these methods are much lower than their centralized counterparts . we present a decentralized sensor-level collision avoidance policy for multi-robot systems , which directly maps raw sensor measurements to an agent 's steering commands in terms of movement velocity . as a first step toward reducing the performance gap between decentralized and centralized methods , we present a multi-scenario multi-stage training framework to find an optimal policy which is trained over a large number of robots on rich , complex environments simultaneously using a policy gradient based reinforcement learning algorithm . we validate the learned sensor-level collision avoidance policy in a variety of simulated scenarios with thorough performance evaluations and show that the final learned policy is able to find time efficient , collision-free paths for a large-scale robot system . we also demonstrate that the learned policy can be well generalized to new scenarios that do not appear in the entire training period , including navigating a heterogeneous group of robots and a large-scale scenario with 100 robots . videos are available at https : //sites.google.com/view/drlmaca"}
{"title": "preferred well-founded semantics for logic programming by alternating fixpoints : preliminary report", "abstract": "we analyze the problem of defining well-founded semantics for ordered logic programs within a general framework based on alternating fixpoint theory . we start by showing that generalizations of existing answer set approaches to preference are too weak in the setting of well-founded semantics . we then specify some informal yet intuitive criteria and propose a semantical framework for preference handling that is more suitable for defining well-founded semantics for ordered logic programs . the suitability of the new approach is convinced by the fact that many attractive properties are satisfied by our semantics . in particular , our semantics is still correct with respect to various existing answer sets semantics while it successfully overcomes the weakness of their generalization to well-founded semantics . finally , we indicate how an existing preferred well-founded semantics can be captured within our semantical framework ."}
{"title": "upal : unbiased pool based active learning", "abstract": "in this paper we address the problem of pool based active learning , and provide an algorithm , called upal , that works by minimizing the unbiased estimator of the risk of a hypothesis in a given hypothesis space . for the space of linear classifiers and the squared loss we show that upal is equivalent to an exponentially weighted average forecaster . exploiting some recent results regarding the spectra of random matrices allows us to establish consistency of upal when the true hypothesis is a linear hypothesis . empirical comparison with an active learner implementation in vowpal wabbit , and a previously proposed pool based active learner implementation show good empirical performance and better scalability ."}
{"title": "linking makinson and kraus-lehmann-magidor preferential entailments", "abstract": "about ten years ago , various notions of preferential entailment have been introduced . the main reference is a paper by kraus , lehmann and magidor ( klm ) , one of the main competitor being a more general version defined by makinson ( mak ) . these two versions have already been compared , but it is time to revisit these comparisons . here are our three main results : ( 1 ) these two notions are equivalent , provided that we restrict our attention , as done in klm , to the cases where the entailment respects logical equivalence ( on the left and on the right ) . ( 2 ) a serious simplification of the description of the fundamental cases in which mak is equivalent to klm , including a natural passage in both ways . ( 3 ) the two previous results are given for preferential entailments more general than considered in some of the original texts , but they apply also to the original definitions and , for this particular case also , the models can be simplified ."}
{"title": "morphologic for knowledge dynamics : revision , fusion , abduction", "abstract": "several tasks in artificial intelligence require to be able to find models about knowledge dynamics . they include belief revision , fusion and belief merging , and abduction . in this paper we exploit the algebraic framework of mathematical morphology in the context of propositional logic , and define operations such as dilation or erosion of a set of formulas . we derive concrete operators , based on a semantic approach , that have an intuitive interpretation and that are formally well behaved , to perform revision , fusion and abduction . computation and tractability are addressed , and simple examples illustrate the typical results that can be obtained ."}
{"title": "distributed pharaoh system for network routing", "abstract": "in this paper it is introduced a biobjective ant algorithm for constructing low cost routing networks . the new algorithm is called the distributed pharaoh system ( dps ) . dps is based on antnet algorithm . the algorithm is using pharaoh ant system ( pas ) with an extra-exploration phase and a 'no-entry ' condition in order to improve the solutions for the low cost network routing problem . additionally it is used a cost model for overlay network construction that includes network traffic demands . the pharaoh ants ( monomorium pharaonis ) includes negative pheromones with signals concentrated at decision points where trails fork . the negative pheromones may complement positive pheromone or could help ants to escape from an unnecessarily long route to food that is being reinforced by attractive signals . numerical experiments were made for a random 10-node network . the average node degree of the network tested was 4.0. the results are encouraging . the algorithm converges to the shortest path while converging on a low cost overlay routing network topology ."}
{"title": "reasoning with intervals on granules", "abstract": "the formalizations of periods of time inside a linear model of time are usually based on the notion of intervals , that may contain or may not their endpoints . this is not enought when the periods are written in terms of coarse granularities with respect to the event taken into account . for instance , how to express the inter-war period in terms of a { \\em years } interval ? this paper presents a new type of intervals , neither open , nor closed or open-closed and the extension of operations on intervals of this new type , in order to reduce the gap between the discourse related to temporal relationship and its translation into a discretized model of time ."}
{"title": "towards music captioning : generating music playlist descriptions", "abstract": "descriptions are often provided along with recommendations to help users ' discovery . recommending automatically generated music playlists ( e.g . personalised playlists ) introduces the problem of generating descriptions . in this paper , we propose a method for generating music playlist descriptions , which is called as music captioning . in the proposed method , audio content analysis and natural language processing are adopted to utilise the information of each track ."}
{"title": "hilbert 's epsilon as an operator of indefinite committed choice", "abstract": "paul bernays and david hilbert carefully avoided overspecification of hilbert 's epsilon-operator and axiomatized only what was relevant for their proof-theoretic investigations . semantically , this left the epsilon-operator underspecified . in the meanwhile , there have been several suggestions for semantics of the epsilon as a choice operator . after reviewing the literature on semantics of hilbert 's epsilon operator , we propose a new semantics with the following features : we avoid overspecification ( such as right-uniqueness ) , but admit indefinite choice , committed choice , and classical logics . moreover , our semantics for the epsilon supports proof search optimally and is natural in the sense that it does not only mirror some cases of referential interpretation of indefinite articles in natural language , but may also contribute to philosophy of language . finally , we ask the question whether our epsilon within our free-variable framework can serve as a paradigm useful in the specification and computation of semantics of discourses in natural language ."}
{"title": "speeding up sor solvers for constraint-based guis with a warm-start strategy", "abstract": "many computer programs have graphical user interfaces ( guis ) , which need good layout to make efficient use of the available screen real estate . most guis do not have a fixed layout , but are resizable and able to adapt themselves . constraints are a powerful tool for specifying adaptable gui layouts : they are used to specify a layout in a general form , and a constraint solver is used to find a satisfying concrete layout , e.g.\\ for a specific gui size . the constraint solver has to calculate a new layout every time a gui is resized or changed , so it needs to be efficient to ensure a good user experience . one approach for constraint solvers is based on the gauss-seidel algorithm and successive over-relaxation ( sor ) . our observation is that a solution after resizing or changing is similar in structure to a previous solution . thus , our hypothesis is that we can increase the computational performance of an sor-based constraint solver if we reuse the solution of a previous layout to warm-start the solving of a new layout . in this paper we report on experiments to test this hypothesis experimentally for three common use cases : big-step resizing , small-step resizing and constraint change . in our experiments , we measured the solving time for randomly generated gui layout specifications of various sizes . for all three cases we found that the performance is improved if an existing solution is used as a starting solution for a new layout ."}
{"title": "video captioning via hierarchical reinforcement learning", "abstract": "video captioning is the task of automatically generating a textual description of the actions in a video . although previous work ( e.g . sequence-to-sequence model ) has shown promising results in abstracting a coarse description of a short video , it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description . this paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning , where a high-level manager module learns to design sub-goals and a low-level worker module recognizes the primitive actions to fulfill the sub-goal . with this compositional framework to reinforce video captioning at different levels , our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning . furthermore , our non-ensemble model has already achieved the state-of-the-art results on the widely-used msr-vtt dataset ."}
{"title": "bayesian gan", "abstract": "generative adversarial networks ( gans ) can implicitly learn rich distributions over images , audio , and data which are hard to model with an explicit likelihood . we present a practical bayesian formulation for unsupervised and semi-supervised learning with gans . within this framework , we use stochastic gradient hamiltonian monte carlo to marginalize the weights of the generator and discriminator networks . the resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching , or mini-batch discrimination . by exploring an expressive posterior over the parameters of the generator , the bayesian gan avoids mode-collapse , produces interpretable and diverse candidate samples , and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including svhn , celeba , and cifar-10 , outperforming dcgan , wasserstein gans , and dcgan ensembles ."}
{"title": "using memristor crossbar structure to implement a novel adaptive real time fuzzy modeling algorithm", "abstract": "although fuzzy techniques promise fast meanwhile accurate modeling and control abilities for complicated systems , different difficulties have been re-vealed in real situation implementations . usually there is no escape of it-erative optimization based on crisp domain algorithms . recently memristor structures appeared promising to implement neural network structures and fuzzy algorithms . in this paper a novel adaptive real-time fuzzy modeling algorithm is proposed which uses active learning method concept to mimic recent understandings of right brain processing techniques . the developed method is based on processing fuzzy numbers to provide the ability of being sensitive to each training data point to expand the knowledge tree leading to plasticity while used defuzzification technique guaranties enough stability . an outstanding characteristic of the proposed algorithm is its consistency to memristor crossbar hardware processing concepts . an analog implemen-tation of the proposed algorithm on memristor crossbars structure is also introduced in this paper . the effectiveness of the proposed algorithm in modeling and pattern recognition tasks is verified by means of computer simulations"}
{"title": "quantum structure in cognition : fundamentals and applications", "abstract": "experiments in cognitive science and decision theory show that the ways in which people combine concepts and make decisions can not be described by classical logic and probability theory . this has serious implications for applied disciplines such as information retrieval , artificial intelligence and robotics . inspired by a mathematical formalism that generalizes quantum mechanics the authors have constructed a contextual framework for both concept representation and decision making , together with quantum models that are in strong alignment with experimental data . the results can be interpreted by assuming the existence in human thought of a double-layered structure , a 'classical logical thought ' and a 'quantum conceptual thought ' , the latter being responsible of the above paradoxes and nonclassical effects . the presence of a quantum structure in cognition is relevant , for it shows that quantum mechanics provides not only a useful modeling tool for experimental data but also supplies a structural model for human and artificial thought processes . this approach has strong connections with theories formalizing meaning , such as semantic analysis , and has also a deep impact on computer science , information retrieval and artificial intelligence . more specifically , the links with information retrieval are discussed in this paper ."}
{"title": "integrating human-provided information into belief state representation using dynamic factorization", "abstract": "in partially observed environments , it can be useful for a human to provide the robot with declarative information that augments its direct sensory observations . for instance , given a robot on a search-and-rescue mission , a human operator might suggest locations of interest . we provide a representation for the robot 's internal knowledge that supports efficient combination of raw sensory information with high-level declarative information presented in a formal language . computational efficiency is achieved by dynamically selecting an appropriate factoring of the belief state , combining aspects of the belief when they are correlated through information and separating them when they are not . this strategy works in open domains , in which the set of possible objects is not known in advance , and provides significant improvements in inference time , leading to more efficient planning for complex partially observable tasks . we validate our approach experimentally in two open-domain planning problems : a 2d discrete gridworld task and a 3d continuous cooking task ."}
{"title": "deciding morality of graphs is np-complete", "abstract": "in order to find a causal explanation for data presented in the form of covariance and concentration matrices it is necessary to decide if the graph formed by such associations is a projection of a directed acyclic graph ( dag ) . we show that the general problem of deciding whether such a dag exists is np-complete ."}
{"title": "dd-eba : an algorithm for determining the number of neighbors in cost estimation by analogy using distance distributions", "abstract": "case based reasoning and particularly estimation by analogy , has been used in a number of problem-solving areas , such as cost estimation . conventional methods , despite the lack of a sound criterion for choosing nearest projects , were based on estimation using a fixed and predetermined number of neighbors from the entire set of historical instances . this approach puts boundaries to the estimation ability of such algorithms , for they do not take into consideration that every project under estimation is unique and requires different handling . the notion of distributions of distances together with a distance metric for distributions help us to adapt the proposed method ( we call it dd-eba ) each time to a specific case that is to be estimated without loosing in prediction power or computational cost . the results of this paper show that the proposed technique achieves the above idea in a very efficient way ."}
{"title": "deep reinforcement learning in parameterized action space", "abstract": "recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces . however , to the best of our knowledge no previous work has succeeded at using deep neural networks in structured ( parameterized ) continuous action spaces . to fill this gap , this paper focuses on learning within the domain of simulated robocup soccer , which features a small set of discrete action types , each of which is parameterized with continuous variables . the best learned agent can score goals more reliably than the 2012 robocup champion agent . as such , this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space mdps ."}
{"title": "a modelling approach based on fuzzy agents", "abstract": "modelling of complex systems is mainly based on the decomposition of these systems in autonomous elements , and the identification and definitio9n of possible interactions between these elements . for this , the agent-based approach is a modelling solution often proposed . complexity can also be due to external events or internal to systems , whose main characteristics are uncertainty , imprecision , or whose perception is subjective ( i.e . interpreted ) . insofar as fuzzy logic provides a solution for modelling uncertainty , the concept of fuzzy agent can model both the complexity and uncertainty . this paper focuses on introducing the concept of fuzzy agent : a classical architecture of agent is redefined according to a fuzzy perspective . a pedagogical illustration of fuzzy agentification of a smart watering system is then proposed ."}
{"title": "learning periodic human behaviour models from sparse data for crowdsourcing aid delivery in developing countries", "abstract": "in many developing countries , half the population lives in rural locations , where access to essentials such as school materials , mosquito nets , and medical supplies is restricted . we propose an alternative method of distribution ( to standard road delivery ) in which the existing mobility habits of a local population are leveraged to deliver aid , which raises two technical challenges in the areas optimisation and learning . for optimisation , a standard markov decision process applied to this problem is intractable , so we provide an exact formulation that takes advantage of the periodicities in human location behaviour . to learn such behaviour models from sparse data ( i.e. , cell tower observations ) , we develop a bayesian model of human mobility . using real cell tower data of the mobility behaviour of 50,000 individuals in ivory coast , we find that our model outperforms the state of the art approaches in mobility prediction by at least 25 % ( in held-out data likelihood ) . furthermore , when incorporating mobility prediction with our mdp approach , we find a 81.3 % reduction in total delivery time versus routine planning that minimises just the number of participants in the solution path ."}
{"title": "psique : next sequence prediction of satellite images using a convolutional sequence-to-sequence network", "abstract": "predicting unseen weather phenomena is an important issue for disaster management . in this paper , we suggest a model for a convolutional sequence-to-sequence autoencoder for predicting undiscovered weather situations from previous satellite images . we also propose a symmetric skip connection between encoder and decoder modules to produce more comprehensive image predictions . to examine our model performance , we conducted experiments for each suggested model to predict future satellite images from historical satellite images . a specific combination of skip connection and sequence-to-sequence autoencoder was able to generate closest prediction from the ground truth image ."}
{"title": "differentiable learning of logical rules for knowledge base reasoning", "abstract": "we study the problem of learning probabilistic first-order logical rules for knowledge base reasoning . this learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space . we propose a framework , neural logic programming , that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model . this approach is inspired by a recently-developed differentiable logic called tensorlog , where inference tasks can be compiled into sequences of differentiable operations . we design a neural controller system that learns to compose these operations . empirically , our method outperforms prior work on multiple knowledge base benchmark datasets , including freebase and wikimovies ."}
{"title": "reinforcement learning approach for real time strategy games battle city and s3", "abstract": "in this paper we proposed reinforcement learning algorithms with the generalized reward function . in our proposed method we use q-learning and sarsa algorithms with generalised reward function to train the reinforcement learning agent . we evaluated the performance of our proposed algorithms on two real-time strategy games called battlecity and s3 . there are two main advantages of having such an approach as compared to other works in rts . ( 1 ) we can ignore the concept of a simulator which is often game specific and is usually hard coded in any type of rts games ( 2 ) our system can learn from interaction with any opponents and quickly change the strategy according to the opponents and do not need any human traces as used in previous works . keywords : reinforcement learning , machine learning , real time strategy , artificial intelligence ."}
{"title": "a simple neural attentive meta-learner", "abstract": "deep neural networks excel in regimes with large amounts of data , but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task . in response , recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks , in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve . however , many recent meta-learning approaches are extensively hand-designed , either using architectures specialized to a particular application , or hard-coding algorithmic components that constrain how the meta-learner solves the task . we propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention ; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information . in the most extensive set of meta-learning experiments to date , we evaluate the resulting simple neural attentive learner ( or snail ) on several heavily-benchmarked tasks . on all tasks , in both supervised and reinforcement learning , snail attains state-of-the-art performance by significant margins ."}
{"title": "qualitative order of magnitude energy-flow-based failure modes and effects analysis", "abstract": "this paper presents a structured power and energy-flow-based qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow . the modelling is split into two parts . power flow is a global phenomenon and is therefore naturally represented and analysed by a network comprised of the relevant structural elements from the components of a system . the power flow analysis is a platform for higher-level behaviour prediction of energy related aspects using local component behaviour models to capture a state-based representation with a global time . the primary application is failure modes and effects analysis ( fmea ) and a form of exaggeration reasoning is used , combined with an order of magnitude representation to derive the worst case failure modes . the novel aspects of the work are an order of magnitude ( om ) qualitative network analyser to represent any power domain and topology , including multiple power sources , a feature that was not required for earlier specialised electrical versions of the approach . secondly , the representation of generalised energy related behaviour as state-based local models is presented as a modelling strategy that can be more vivid and intuitive for a range of topologically complex applications than qualitative equation-based representations.the two-level modelling strategy allows the broad system behaviour coverage of qualitative simulation to be exploited for the fmea task , while limiting the difficulties of qualitative ambiguity explanation that can arise from abstracted numerical models . we have used the method to support an automated fmea system with examples of an aircraft fuel system and domestic a heating system discussed in this paper ."}
{"title": "growing a tree in the forest : constructing folksonomies by integrating structured metadata", "abstract": "many social web sites allow users to annotate the content with descriptive metadata , such as tags , and more recently to organize content hierarchically . these types of structured metadata provide valuable evidence for learning how a community organizes knowledge . for instance , we can aggregate many personal hierarchies into a common taxonomy , also known as a folksonomy , that will aid users in visualizing and browsing social content , and also to help them in organizing their own content . however , learning from social metadata presents several challenges , since it is sparse , shallow , ambiguous , noisy , and inconsistent . we describe an approach to folksonomy learning based on relational clustering , which exploits structured metadata contained in personal hierarchies . our approach clusters similar hierarchies using their structure and tag statistics , then incrementally weaves them into a deeper , bushier tree . we study folksonomy learning using social metadata extracted from the photo-sharing site flickr , and demonstrate that the proposed approach addresses the challenges . moreover , comparing to previous work , the approach produces larger , more accurate folksonomies , and in addition , scales better ."}
{"title": "ashacl : alternative shapes constraint language", "abstract": "ashacl , a variant of the w3c shapes constraint language , is designed to determine whether an rdf graph meets some conditions . these conditions are grouped into shapes , which validate whether particular rdf terms each meet the constraints of the shape . shapes are themselves expressed as rdf triples in an rdf graph , called a shapes graph ."}
{"title": "skill analysis with time series image data", "abstract": "we present a skill analysis with time series image data using data mining methods , focused on table tennis . we do not use body model , but use only hi-speed movies , from which time series data are obtained and analyzed using data mining methods such as c4.5 and so on . we identify internal models for technical skills as evaluation skillfulness for the forehand stroke of table tennis , and discuss mono and meta-functional skills for improving skills ."}
{"title": "lexical disambiguation in natural language questions ( nlqs )", "abstract": "question processing is a fundamental step in a question answering ( qa ) application , and its quality impacts the performance of qa application . the major challenging issue in processing question is how to extract semantic of natural language questions ( nlqs ) . a human language is ambiguous . ambiguity may occur at two levels ; lexical and syntactic . in this paper , we propose a new approach for resolving lexical ambiguity problem by integrating context knowledge and concepts knowledge of a domain , into shallow natural language processing ( snlp ) techniques . concepts knowledge is modeled using ontology , while context knowledge is obtained from wordnet , and it is determined based on neighborhood words in a question . the approach will be applied to a university qa system ."}
{"title": "generating conditional probabilities for bayesian networks : easing the knowledge acquisition problem", "abstract": "the number of probability distributions required to populate a conditional probability table ( cpt ) in a bayesian network , grows exponentially with the number of parent-nodes associated with that table . if the table is to be populated through knowledge elicited from a domain expert then the sheer magnitude of the task forms a considerable cognitive barrier . in this paper we devise an algorithm to populate the cpt while easing the extent of knowledge acquisition . the input to the algorithm consists of a set of weights that quantify the relative strengths of the influences of the parent-nodes on the child-node , and a set of probability distributions the number of which grows only linearly with the number of associated parent-nodes . these are elicited from the domain expert . the set of probabilities are obtained by taking into consideration the heuristics that experts use while arriving at probabilistic estimations . the algorithm is used to populate the cpt by computing appropriate weighted sums of the elicited distributions . we invoke the methods of information geometry to demonstrate how these weighted sums capture the expert 's judgemental strategy ."}
{"title": "geracao automatica de paineis de controle para analise de mobilidade urbana utilizando redes complexas", "abstract": "in this paper we describe an automatic generator to support the data scientist to construct , in a user-friendly way , dashboards from data represented as networks . the generator called sbinet ( semantic for business intelligence from networks ) has a semantic layer that , through ontologies , describes the data that represents a network as well as the possible metrics to be calculated in the network . thus , with sbinet , the stages of the dashboard constructing process that uses complex network metrics are facilitated and can be done by users who do not necessarily know about complex networks ."}
{"title": "enhancing qpns for trade-off resolution", "abstract": "qualitative probabilistic networks have been introduced as qualitative abstractions of bayesian belief networks . one of the major drawbacks of these qualitative networks is their coarse level of detail , which may lead to unresolved trade-offs during inference . we present an enhanced formalism for qualitative networks with a finer level of detail . an enhanced qualitative probabilistic network differs from a regular qualitative network in that it distinguishes between strong and weak influences . enhanced qualitative probabilistic networks are purely qualitative in nature , as regular qualitative networks are , yet allow for efficiently resolving trade-offs during inference ."}
{"title": "exact and approximate inference in graphical models : variable elimination and beyond", "abstract": "probabilistic graphical models offer a powerful framework to account for the dependence structure between variables , which can be represented as a graph . the dependence between variables may render inference tasks such as computing normalizing constant , marginalization or optimization intractable . the objective of this paper is to review techniques exploiting the graph structure for exact inference borrowed from optimization and computer science . they are not yet standard in the statistician toolkit , and we specify under which conditions they are efficient in practice . they are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated in the graph . the so-called treewidth of the graph characterizes this algorithmic complexity : low-treewidth graphs can be processed efficiently . algorithmic solutions derived from variable elimination and the notion of treewidth are illustrated on problems of treewidth computation and inference in challenging benchmarks from optimization competitions . we also review how efficient techniques for approximate inference such as loopy belief propagation and variational approaches can be linked to variable elimination and we illustrate them in the context of expectation-maximisation procedures for parameter estimation in coupled hidden markov models ."}
{"title": "a csp implementation of the bigraph embedding problem", "abstract": "a crucial problem for many results and tools about bigraphs and bigraphical reactive systems is bigraph embedding . an embedding is more informative than a bigraph matching , since it keeps track of the correspondence between the various components of the redex ( guest ) within the agent ( host ) . in this paper , we present an algorithm for computing embeddings based on a reduction to a constraint satisfaction problem . this algorithm , that we prove to be sound and complete , has been successfully implemented in libbig , a library for manipulating bigraphical reactive systems . this library can be used for implementing a wide range of tools , and it can be adapted to various extensions of bigraphs ."}
{"title": "from truth to computability i", "abstract": "the recently initiated approach called computability logic is a formal theory of interactive computation . see a comprehensive online source on the subject at http : //www.cis.upenn.edu/~giorgi/cl.html . the present paper contains a soundness and completeness proof for the deductive system cl3 which axiomatizes the most basic first-order fragment of computability logic called the finite-depth , elementary-base fragment . among the potential application areas for this result are the theory of interactive computation , constructive applied theories , knowledgebase systems , systems for resource-bound planning and action . this paper is self-contained as it reintroduces all relevant definitions as well as main motivations ."}
{"title": "emergent complexity via multi-agent competition", "abstract": "reinforcement learning algorithms can train agents that solve problems in complex , interesting environments . normally , the complexity of the trained agent is closely related to the complexity of the environment . this suggests that a highly capable agent requires a complex environment for training . in this paper , we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself . we also point out that such environments come with a natural curriculum , because for any skill level , an environment full of agents of this level will have the right level of difficulty . this work introduces several competitive multi-agent environments where agents compete in a 3d world with simulated physics . the trained agents learn a wide variety of complex and interesting skills , even though the environment themselves are relatively simple . the skills include behaviors such as running , blocking , ducking , tackling , fooling opponents , kicking , and defending using both arms and legs . a highlight of the learned behaviors can be found here : https : //goo.gl/er7fbx"}
{"title": "building the signature of set theory using the mathsem program", "abstract": "knowledge representation is a popular research field in it . as mathematical knowledge is most formalized , its representation is important and interesting . mathematical knowledge consists of various mathematical theories . in this paper we consider a deductive system that derives mathematical notions , axioms and theorems . all these notions , axioms and theorems can be considered as the part of elementary set theory . this theory will be represented as a semantic net ."}
{"title": "utcnn : a deep learning model of stance classificationon on social media text", "abstract": "most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms . in this paper , we classify post stance on social media channels and develop utcnn , a neural network model that incorporates user tastes , topic tastes , and user comments on posts . utcnn not only works on social media texts , but also analyzes texts in forums and message boards . experiments performed on chinese facebook data and english online debate forum data show that utcnn achieves a 0.755 macro-average f-score for supportive , neutral , and unsupportive stance classes on facebook data , which is significantly better than models in which either user , topic , or comment information is withheld . this model design greatly mitigates the lack of data for the minor class without the use of oversampling . in addition , utcnn yields a 0.842 accuracy on english online debate forum data , which also significantly outperforms results from previous work as well as other deep learning models , showing that utcnn performs well regardless of language or platform ."}
{"title": "message-passing algorithms for quadratic programming formulations of map estimation", "abstract": "computing maximum a posteriori ( map ) estimation in graphical models is an important inference problem with many applications . we present message-passing algorithms for quadratic programming ( qp ) formulations of map estimation for pairwise markov random fields . in particular , we use the concave-convex procedure ( cccp ) to obtain a locally optimal algorithm for the non-convex qp formulation . a similar technique is used to derive a globally convergent algorithm for the convex qp relaxation of map . we also show that a recently developed expectation-maximization ( em ) algorithm for the qp formulation of map can be derived from the cccp perspective . experiments on synthetic and real-world problems confirm that our new approach is competitive with max-product and its variations . compared with cplex , we achieve more than an order-of-magnitude speedup in solving optimally the convex qp relaxation ."}
{"title": "hamiltonian maker-breaker games on small graphs", "abstract": "we look at the unbiased maker-breaker hamiltonicity game played on the edge set of a complete graph $ k_n $ , where maker 's goal is to claim a hamiltonian cycle . first , we prove that , independent of who starts , maker can win the game for $ n = 8 $ and $ n = 9 $ . then we use an inductive argument to show that , independent of who starts , maker can win the game if and only if $ n \\geq 8 $ . this , in particular , resolves in the affirmative the long-standing conjecture of papaioannou . we also study two standard positional games related to hamiltonicity game . for hamiltonian path game , we show that maker can claim a hamiltonian path if and only if $ n \\geq 5 $ , independent of who starts . next , we look at fixed hamiltonian path game , where the goal of maker is to claim a hamiltonian path between two predetermined vertices . we prove that if maker starts the game , he wins if and only if $ n \\geq 7 $ , and if breaker starts , maker wins if and only if $ n \\geq 8 $ . using this result , we are able to improve the previously best upper bound on the smallest number of edges a graph on $ n $ vertices can have , knowing that maker can win the maker-breaker hamiltonicity game played on its edges . to resolve the outcomes of the mentioned games on small ( finite ) boards , we devise algorithms for efficiently searching game trees and then obtain our results with the help of a computer ."}
{"title": "distributed power allocation with sinr constraints using trial and error learning", "abstract": "in this paper , we address the problem of global transmit power minimization in a self-congiguring network where radio devices are subject to operate at a minimum signal to interference plus noise ratio ( sinr ) level . we model the network as a parallel gaussian interference channel and we introduce a fully decentralized algorithm ( based on trial and error ) able to statistically achieve a congiguration where the performance demands are met . contrary to existing solutions , our algorithm requires only local information and can learn stable and efficient working points by using only one bit feedback . we model the network under two different game theoretical frameworks : normal form and satisfaction form . we show that the converging points correspond to equilibrium points , namely nash and satisfaction equilibrium . similarly , we provide sufficient conditions for the algorithm to converge in both formulations . moreover , we provide analytical results to estimate the algorithm 's performance , as a function of the network parameters . finally , numerical results are provided to validate our theoretical conclusions . keywords : learning , power control , trial and error , nash equilibrium , spectrum sharing ."}
{"title": "intrinsically motivated multimodal structure learning", "abstract": "we present a long-term intrinsically motivated structure learning method for modeling transition dynamics during controlled interactions between a robot and semi-permanent structures in the world . in particular , we discuss how partially-observable state is represented using distributions over a markovian state and build models of objects that predict how state distributions change in response to interactions with such objects . these structures serve as the basis for a number of possible future tasks defined as markov decision processes ( mdps ) . the approach is an example of a structure learning technique applied to a multimodal affordance representation that yields a population of forward models for use in planning . we evaluate the approach using experiments on a bimanual mobile manipulator ( ubot-6 ) that show the performance of model acquisition as the number of transition actions increases ."}
{"title": "compressed constraints in probabilistic logic and their revision", "abstract": "in probabilistic logic entailments , even moderate size problems can yield linear constraint systems with so many variables that exact methods are impractical . this difficulty can be remedied in many cases of interest by introducing a three valued logic ( true , false , and `` do n't care '' ) . the three-valued approach allows the construction of `` compressed '' constraint systems which have the same solution sets as their two-valued counterparts , but which may involve dramatically fewer variables . techniques to calculate point estimates for the posterior probabilities of entailed sentences are discussed ."}
{"title": "lstm-based mixture-of-experts for knowledge-aware dialogues", "abstract": "we introduce an lstm-based method for dynamically integrating several word-prediction experts to obtain a conditional language model which can be good simultaneously at several subtasks . we illustrate this general approach with an application to dialogue where we integrate a neural chat model , good at conversational aspects , with a neural question-answering model , good at retrieving precise information from a knowledge-base , and show how the integration combines the strengths of the independent components . we hope that this focused contribution will attract attention on the benefits of using such mixtures of experts in nlp ."}
{"title": "ontoana : domain ontology for human anatomy", "abstract": "today , we can find many search engines which provide us with information which is more operational in nature . none of the search engines provide domain specific information . this becomes very troublesome to a novice user who wishes to have information in a particular domain . in this paper , we have developed an ontology which can be used by a domain specific search engine . we have developed an ontology on human anatomy , which captures information regarding cardiovascular system , digestive system , skeleton and nervous system . this information can be used by people working in medical and health care domain ."}
{"title": "game theory models for communication between agents : a review", "abstract": "in the real world , agents or entities are in a continuous state of interactions . these inter- actions lead to various types of complexity dynamics . one key difficulty in the study of complex agent interactions is the difficulty of modeling agent communication on the basis of rewards . game theory offers a perspective of analysis and modeling these interactions . previously , while a large amount of literature is available on game theory , most of it is from specific domains and does not cater for the concepts from an agent- based perspective . here in this paper , we present a comprehensive multidisciplinary state-of-the-art review and taxonomy of game theory models of complex interactions between agents ."}
{"title": "computational curiosity ( a book draft )", "abstract": "this book discusses computational curiosity , from the psychology of curiosity to the computational models of curiosity , and then showcases several interesting applications of computational curiosity . a brief overview of the book is given as follows . chapter 1 discusses the underpinnings of curiosity in human beings , including the major categories of curiosity , curiosity-related emotions and behaviors , and the benefits of curiosity . chapter 2 reviews the arousal theories of curiosity in psychology and summarizes a general two-step process model for computational curiosity . base on the perspective of the two-step process model , chapter 3 reviews and analyzes some of the traditional computational models of curiosity . chapter 4 introduces a novel generic computational model of curiosity , which is developed based on the arousal theories of curiosity . after the discussion of computational models of curiosity , we outline the important applications where computational curiosity may bring significant impacts in chapter 5. chapter 6 discusses the application of the generic computational model of curiosity in a machine learning framework . chapter 7 discusses the application of the generic computational model of curiosity in a recommender system . in chapter 8 and chapter 9 , the generic computational model of curiosity is studied in two types of pedagogical agents . in chapter 8 , a curious peer learner is studied . it is a non-player character that aims to provide a believable virtual learning environment for users . in chapter 9 , a curious learning companion is studied . it aims to enhance users ' learning experience through providing meaningful interactions with them . chapter 10 discusses open questions in the research field of computation curiosity ."}
{"title": "validation rules for assessing and improving skos mapping quality", "abstract": "the simple knowledge organization system ( skos ) is popular for expressing controlled vocabularies , such as taxonomies , classifications , etc. , for their use in semantic web applications . using skos , concepts can be linked to other concepts and organized into hierarchies inside a single terminology system . meanwhile , expressing mappings between concepts in different terminology systems is also possible . this paper discusses potential quality issues in using skos to express these terminology mappings . problematic patterns are defined and corresponding rules are developed to automatically detect situations where the mappings either result in 'skos vocabulary hijacking ' to the source vocabularies or cause conflicts . an example of using the rules to validate sample mappings between two clinical terminologies is given . the validation rules , expressed in n3 format , are available as open source ."}
{"title": "generative topic embedding : a continuous representation of documents ( extended version with proofs )", "abstract": "word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window . on the other hand , topic modeling maps documents onto a low-dimensional topic space , by utilizing the global word collocation patterns in the same document . these two types of patterns are complementary . in this paper , we propose a generative topic embedding model to combine the two types of patterns . in our model , topics are represented by embedding vectors , and are shared across documents . the probability of each word is influenced by both its local context and its topic . a variational inference method yields the topic embeddings as well as the topic mixing proportions for each document . jointly they represent the document in a low-dimensional continuous space . in two document classification tasks , our method performs better than eight existing methods , with fewer features . in addition , we illustrate with an example that our method can generate coherent topics even based on only one document ."}
{"title": "multiscale probability transformation of basic probability assignment", "abstract": "decision making is still an open issue in the application of dempster-shafer evidence theory . a lot of works have been presented for it . in the transferable belief model ( tbm ) , pignistic probabilities based on the basic probability as- signments are used for decision making . in this paper , multiscale probability transformation of basic probability assignment based on the belief function and the plausibility function is proposed , which is a generalization of the pignistic probability transformation . in the multiscale probability function , a factor q based on the tsallis entropy is used to make the multiscale prob- abilities diversified . an example is shown that the multiscale probability transformation is more reasonable in the decision making ."}
{"title": "general combination rules for qualitative and quantitative beliefs", "abstract": "martin and osswald \\cite { martin07 } have recently proposed many generalizations of combination rules on quantitative beliefs in order to manage the conflict and to consider the specificity of the responses of the experts . since the experts express themselves usually in natural language with linguistic labels , smarandache and dezert \\cite { li07 } have introduced a mathematical framework for dealing directly also with qualitative beliefs . in this paper we recall some element of our previous works and propose the new combination rules , developed for the fusion of both qualitative or quantitative beliefs ."}
{"title": "bootstrapping skills", "abstract": "the monolithic approach to policy representation in markov decision processes ( mdps ) looks for a single policy that can be represented as a function from states to actions . for the monolithic approach to succeed ( and this is not always possible ) , a complex feature representation is often necessary since the policy is a complex object that has to prescribe what actions to take all over the state space . this is especially true in large domains with complicated dynamics . it is also computationally inefficient to both learn and plan in mdps using a complex monolithic approach . we present a different approach where we restrict the policy space to policies that can be represented as combinations of simpler , parameterized skills -- -a type of temporally extended action , with a simple policy representation . we introduce learning skills via bootstrapping ( lsb ) that can use a broad family of reinforcement learning ( rl ) algorithms as a `` black box '' to iteratively learn parametrized skills . initially , the learned skills are short-sighted but each iteration of the algorithm allows the skills to bootstrap off one another , improving each skill in the process . we prove that this bootstrapping process returns a near-optimal policy . furthermore , our experiments demonstrate that lsb can solve mdps that , given the same representational power , could not be solved by a monolithic approach . thus , planning with learned skills results in better policies without requiring complex policy representations ."}
{"title": "mining biclusters of similar values with triadic concept analysis", "abstract": "biclustering numerical data became a popular data-mining task in the beginning of 2000 's , especially for analysing gene expression data . a bicluster reflects a strong association between a subset of objects and a subset of attributes in a numerical object/attribute data-table . so called biclusters of similar values can be thought as maximal sub-tables with close values . only few methods address a complete , correct and non redundant enumeration of such patterns , which is a well-known intractable problem , while no formal framework exists . in this paper , we introduce important links between biclustering and formal concept analysis . more specifically , we originally show that triadic concept analysis ( tca ) , provides a nice mathematical framework for biclustering . interestingly , existing algorithms of tca , that usually apply on binary data , can be used ( directly or with slight modifications ) after a preprocessing step for extracting maximal biclusters of similar values ."}
{"title": "learning to represent programs with graphs", "abstract": "learning tasks on source code ( i.e. , formal languages ) have been considered recently , but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code 's known syntax . for example , long-range dependencies induced by using the same variable or function in distant locations are often not considered . we propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures . in this work , we present how to construct graphs from source code and how to scale gated graph neural networks training to such large graphs . we evaluate our method on two tasks : varnaming , in which a network attempts to predict the name of a variable given its usage , and varmisuse , in which the network learns to reason about selecting the correct variable that should be used at a given program location . our comparison to methods that use less structured program representations shows the advantages of modeling known structure , and suggests that our models learn to infer meaningful names and to solve the varmisuse task in many cases . additionally , our testing showed that varmisuse identifies a number of bugs in mature open-source projects ."}
{"title": "a short note on the axiomatic requirements of uncertainty measure", "abstract": "in this note , we argue that the axiomatic requirement of range to the measure of aggregated total uncertainty ( atu ) in dempster-shafer theory is not reasonable ."}
{"title": "consistent transformations of belief functions", "abstract": "consistent belief functions represent collections of coherent or non-contradictory pieces of evidence , but most of all they are the counterparts of consistent knowledge bases in belief calculus . the use of consistent transformations cs [ . ] in a reasoning process to guarantee coherence can therefore be desirable , and generalizes similar techniques in classical logic . transformations can be obtained by minimizing an appropriate distance measure between the original belief function and the collection of consistent ones . we focus here on the case in which distances are measured using classical lp norms , in both the `` mass space '' and the `` belief space '' representation of belief functions . while mass consistent approximations reassign the mass not focussed on a chosen element of the frame either to the whole frame or to all supersets of the element on an equal basis , approximations in the belief space do distinguish these focal elements according to the `` focussed consistent transformation '' principle . the different approximations are interpreted and compared , with the help of examples ."}
{"title": "t-skirt : online estimation of student proficiency in an adaptive learning system", "abstract": "we develop t-skirt : a temporal , structured-knowledge , irt-based method for predicting student responses online . by explicitly accounting for student learning and employing a structured , multidimensional representation of student proficiencies , the model outperforms standard irt-based methods on an online response prediction task when applied to real responses collected from students interacting with diverse pools of educational content ."}
{"title": "compact representations of extended causal models", "abstract": "judea pearl was the first to propose a definition of actual causation using causal models . a number of authors have suggested that an adequate account of actual causation must appeal not only to causal structure , but also to considerations of normality . in earlier work , we provided a definition of actual causation using extended causal models , which include information about both causal structure and normality . extended causal models are potentially very complex . in this paper , we show how it is possible to achieve a compact representation of extended causal models ."}
{"title": "a theory of goal-oriented mdps with dead ends", "abstract": "stochastic shortest path ( ssp ) mdps is a problem class widely studied in ai , especially in probabilistic planning . they describe a wide range of scenarios but make the restrictive assumption that the goal is reachable from any state , i.e. , that dead-end states do not exist . because of this , ssps are unable to model various scenarios that may have catastrophic events ( e.g. , an airplane possibly crashing if it flies into a storm ) . even though mdp algorithms have been used for solving problems with dead ends , a principled theory of ssp extensions that would allow dead ends , including theoretically sound algorithms for solving such mdps , has been lacking . in this paper , we propose three new mdp classes that admit dead ends under increasingly weaker assumptions . we present value iteration-based as well as the more efficient heuristic search algorithms for optimally solving each class , and explore theoretical relationships between these classes . we also conduct a preliminary empirical study comparing the performance of our algorithms on different mdp classes , especially on scenarios with unavoidable dead ends ."}
{"title": "implementing feedback in creative systems : a workshop approach", "abstract": "one particular challenge in ai is the computational modelling and simulation of creativity . feedback and learning from experience are key aspects of the creative process . here we investigate how we could implement feedback in creative systems using a social model . from the field of creative writing we borrow the concept of a writers workshop as a model for learning through feedback . the writers workshop encourages examination , discussion and debates of a piece of creative work using a prescribed format of activities . we propose a computational model of the writers workshop as a roadmap for incorporation of feedback in artificial creativity systems . we argue that the writers workshop setting describes the anatomy of the creative process . we support our claim with a case study that describes how to implement the writers workshop model in a computational creativity system . we present this work using patterns other people can follow to implement similar designs in their own systems . we conclude by discussing the broader relevance of this model to other aspects of ai ."}
{"title": "population sizing for genetic programming based upon decision making", "abstract": "this paper derives a population sizing relationship for genetic programming ( gp ) . following the population-sizing derivation for genetic algorithms in goldberg , deb , and clark ( 1992 ) , it considers building block decision making as a key facet . the analysis yields a gp-unique relationship because it has to account for bloat and for the fact that gp solutions often use subsolution multiple times . the population-sizing relationship depends upon tree size , solution complexity , problem difficulty and building block expression probability . the relationship is used to analyze and empirically investigate population sizing for three model gp problems named order , on-off and loud . these problems exhibit bloat to differing extents and differ in whether their solutions require the use of a building block multiple times ."}
{"title": "ultrametric model of mind , ii : application to text content analysis", "abstract": "in a companion paper , murtagh ( 2012 ) , we discussed how matte blanco 's work linked the unrepressed unconscious ( in the human ) to symmetric logic and thought processes . we showed how ultrametric topology provides a most useful representational and computational framework for this . now we look at the extent to which we can find ultrametricity in text . we use coherent and meaningful collections of nearly 1000 texts to show how we can measure inherent ultrametricity . on the basis of our findings we hypothesize that inherent ultrametricty is a basis for further exploring unconscious thought processes ."}
{"title": "bilateral multi-perspective matching for natural language sentences", "abstract": "natural language sentence matching is a fundamental technology for a variety of tasks . previous approaches either match sentences from a single direction or only apply single granular ( word-by-word or sentence-by-sentence ) matching . in this work , we propose a bilateral multi-perspective matching ( bimpm ) model under the `` matching-aggregation '' framework . given two sentences $ p $ and $ q $ , our model first encodes them with a bilstm encoder . next , we match the two encoded sentences in two directions $ p \\rightarrow q $ and $ p \\leftarrow q $ . in each matching direction , each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives . then , another bilstm layer is utilized to aggregate the matching results into a fix-length matching vector . finally , based on the matching vector , the decision is made through a fully connected layer . we evaluate our model on three tasks : paraphrase identification , natural language inference and answer sentence selection . experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks ."}
{"title": "z specification for the w3c editor 's draft core shacl semantics", "abstract": "this article provides a formalization of the w3c draft core shacl semantics specification using z notation . this formalization exercise has identified a number of quality issues in the draft . it has also established that the recursive definitions in the draft are well-founded . further formal validation of the draft will require the use of an executable specification technology ."}
{"title": "finetuning randomized heuristic search for 2d path planning : finding the best input parameters for r* algorithm through series of experiments", "abstract": "path planning is typically considered in artificial intelligence as a graph searching problem and r* is state-of-the-art algorithm tailored to solve it . the algorithm decomposes given path finding task into the series of subtasks each of which can be easily ( in computational sense ) solved by well-known methods ( such as a* ) . parameterized random choice is used to perform the decomposition and as a result r* performance largely depends on the choice of its input parameters . in our work we formulate a range of assumptions concerning possible upper and lower bounds of r* parameters , their interdependency and their influence on r* performance . then we evaluate these assumptions by running a large number of experiments . as a result we formulate a set of heuristic rules which can be used to initialize the values of r* parameters in a way that leads to algorithm 's best performance ."}
{"title": "an\u00e1lisis e implementaci\u00f3n de algoritmos evolutivos para la optimizaci\u00f3n de simulaciones en ingenier\u00eda civil . ( draft )", "abstract": "this paper studies the applicability of evolutionary algorithms , particularly , the evolution strategies family in order to estimate a degradation parameter in the shear design of reinforced concrete members . this problem represents a great computational task and is highly relevant in the framework of the structural engineering that for the first time is solved using genetic algorithms . you are viewing a draft , the authors appreciate corrections , comments and suggestions to this work ."}
{"title": "improved evolutionary generation of xslt stylesheets", "abstract": "this paper introduces a procedure based on genetic programming to evolve xslt programs ( usually called stylesheets or logicsheets ) . xslt is a general purpose , document-oriented functional language , generally used to transform xml documents ( or , in general , solve any problem that can be coded as an xml document ) . the proposed solution uses a tree representation for the stylesheets as well as diverse specific operators in order to obtain , in the studied cases and a reasonable time , a xslt stylesheet that performs the transformation . several types of representation have been compared , resulting in different performance and degree of success ."}
{"title": "stackgan++ : realistic image synthesis with stacked generative adversarial networks", "abstract": "although generative adversarial networks ( gans ) have shown remarkable success in various tasks , they still face challenges in generating high quality images . in this paper , we propose stacked generative adversarial networks ( stackgan ) aiming at generating high-resolution photo-realistic images . first , we propose a two-stage generative adversarial network architecture , stackgan-v1 , for text-to-image synthesis . the stage-i gan sketches the primitive shape and colors of the object based on given text description , yielding low-resolution images . the stage-ii gan takes stage-i results and text descriptions as inputs , and generates high-resolution images with photo-realistic details . second , an advanced multi-stage generative adversarial network architecture , stackgan-v2 , is proposed for both conditional and unconditional generative tasks . our stackgan-v2 consists of multiple generators and discriminators in a tree-like structure ; images at multiple scales corresponding to the same scene are generated from different branches of the tree . stackgan-v2 shows more stable training behavior than stackgan-v1 by jointly approximating multiple distributions . extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images ."}
{"title": "methods for applying the neural engineering framework to neuromorphic hardware", "abstract": "we review our current software tools and theoretical methods for applying the neural engineering framework to state-of-the-art neuromorphic hardware . these methods can be used to implement linear and nonlinear dynamical systems that exploit axonal transmission time-delays , and to fully account for nonideal mixed-analog-digital synapses that exhibit higher-order dynamics with heterogeneous time-constants . this summarizes earlier versions of these methods that have been discussed in a more biological context ( voelker & eliasmith , 2017 ) or regarding a specific neuromorphic architecture ( voelker et al. , 2017 ) ."}
{"title": "comparing heterogeneous entities using artificial neural networks of trainable weighted structural components and machine-learned activation functions", "abstract": "to compare entities of differing types and structural components , the artificial neural network paradigm was used to cross-compare structural components between heterogeneous documents . trainable weighted structural components were input into machine-learned activation functions of the neurons . the model was used for matching news articles and videos , where the inputs and activation functions respectively consisted of term vectors and cosine similarity measures between the weighted structural components . the model was tested with different weights , achieving as high as 59.2 % accuracy for matching videos to news articles . a mobile application user interface for recommending related videos for news articles was developed to demonstrate consumer value , including its potential usefulness for cross-selling products from unrelated categories ."}
{"title": "decision support systems using intelligent paradigms", "abstract": "decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved . the past few years have witnessed a growing recognition of soft computing ( sc ) technologies that underlie the conception , design and utilization of intelligent systems . in this paper , we present different sc paradigms involving an artificial neural network trained using the scaled conjugate gradient algorithm , two different fuzzy inference methods optimised using neural network learning/evolutionary algorithms and regression trees for developing intelligent decision support systems . we demonstrate the efficiency of the different algorithms by developing a decision support system for a tactical air combat environment ( tace ) . some empirical comparisons between the different algorithms are also provided ."}
{"title": "methods for finding leader -- follower equilibria with multiple followers", "abstract": "the concept of leader -- follower ( or stackelberg ) equilibrium plays a central role in a number of real -- world applications of game theory . while the case with a single follower has been thoroughly investigated , results with multiple followers are only sporadic and the problem of designing and evaluating computationally tractable equilibrium-finding algorithms is still largely open . in this work , we focus on the fundamental case where multiple followers play a nash equilibrium once the leader has committed to a strategy -- -as we illustrate , the corresponding equilibrium finding problem can be easily shown to be $ \\mathcal { fnp } $ -- hard and not in poly -- $ \\mathcal { apx } $ unless $ \\mathcal { p } = \\mathcal { np } $ and therefore it is one among the hardest problems to solve and approximate . we propose nonconvex mathematical programming formulations and global optimization methods to find both exact and approximate equilibria , as well as a heuristic black box algorithm . all the methods and formulations that we introduce are thoroughly evaluated computationally ."}
{"title": "from one point to a manifold : knowledge graph embedding for precise link prediction", "abstract": "knowledge graph embedding aims at offering a numerical knowledge representation paradigm by transforming the entities and relations into continuous vector space . however , existing methods could not characterize the knowledge graph in a fine degree to make a precise prediction . there are two reasons : being an ill-posed algebraic system and applying an overstrict geometric form . as precise prediction is critical , we propose an manifold-based embedding principle ( \\textbf { manifolde } ) which could be treated as a well-posed algebraic system that expands the position of golden triples from one point in current models to a manifold in ours . extensive experiments show that the proposed models achieve substantial improvements against the state-of-the-art baselines especially for the precise prediction task , and yet maintain high efficiency ."}
{"title": "expert and non-expert opinion about technological unemployment", "abstract": "there is significant concern that technological advances , especially in robotics and artificial intelligence ( ai ) , could lead to high levels of unemployment in the coming decades . studies have estimated that around half of all current jobs are at risk of automation . to look into this issue in more depth , we surveyed experts in robotics and ai about the risk , and compared their views with those of non-experts . whilst the experts predicted a significant number of occupations were at risk of automation in the next two decades , they were more cautious than people outside the field in predicting occupations at risk . their predictions were consistent with their estimates for when computers might be expected to reach human level performance across a wide range of skills . these estimates were typically decades later than those of the non-experts . technological barriers may therefore provide society with more time to prepare for an automated future than the public fear . in addition , public expectations may need to be dampened about the speed of progress to be expected in robotics and ai ."}
{"title": "a unified view of causal and non-causal feature selection", "abstract": "in this paper , we unify causal and non-causal feature selection methods based on the bayesian network framework . we first show that the objectives of causal and non-causal feature selection methods are equal and are to find the markov blanket of a class attribute , the theoretically optimal feature set for classification . we demonstrate that causal and non-causal feature selection take different assumptions of dependency among features to find markov blanket , and their algorithms are shown different level of approximation for finding markov blanket . in this framework , we are able to analyze the sample and error bounds of casual and non-causal methods . we conducted extensive experiments to show the correctness of our theoretical analysis ."}
{"title": "an existing , ecologically-successful genus of collectively intelligent artificial creatures", "abstract": "people sometimes worry about the singularity [ vinge , 1993 ; kurzweil , 2005 ] , or about the world being taken over by artificially intelligent robots . i believe the risks of these are very small . however , few people recognize that we already share our world with artificial creatures that participate as intelligent agents in our society : corporations . our planet is inhabited by two distinct kinds of intelligent beings -- - individual humans and corporate entities -- - whose natures and interests are intimately linked . to co-exist well , we need to find ways to define the rights and responsibilities of both individual humans and corporate entities , and to find ways to ensure that corporate entities behave as responsible members of society ."}
{"title": "model enumeration in propositional circumscription via unsatisfiable core analysis", "abstract": "many practical problems are characterized by a preference relation over admissible solutions , where preferred solutions are minimal in some sense . for example , a preferred diagnosis usually comprises a minimal set of reasons that is sufficient to cause the observed anomaly . alternatively , a minimal correction subset comprises a minimal set of reasons whose deletion is sufficient to eliminate the observed anomaly . circumscription formalizes such preference relations by associating propositional theories with minimal models . the resulting enumeration problem is addressed here by means of a new algorithm taking advantage of unsatisfiable core analysis . empirical evidence of the efficiency of the algorithm is given by comparing the performance of the resulting solver , circumscriptino , with hclasp , camus mcs , lbx and mcsls on the enumeration of minimal models for problems originating from practical applications . this paper is under consideration for acceptance in tplp ."}
{"title": "inverse reinforcement learning via nonparametric spatio-temporal subgoal modeling", "abstract": "recent advances in the field of inverse reinforcement learning ( irl ) have yielded sophisticated frameworks which relax the original modeling assumption that the behavior of an observed agent reflects only a single intention . instead , the demonstration data is typically divided into parts , to account for the fact that different trajectories may correspond to different intentions , e.g. , because they were generated by different domain experts . in this work , we go one step further : using the intuitive concept of subgoals , we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally , enabling a more compact representation of the observed behavior . based on this assumption , we build an implicit intentional model of the agent 's goals to forecast its behavior in unobserved situations . the result is an integrated bayesian prediction framework which provides smooth policy estimates that are consistent with the expert 's plan and significantly outperform existing irl solutions . most notably , our framework naturally handles situations where the intentions of the agent change with time and classical irl algorithms fail . in addition , due to its probabilistic nature , the model can be straightforwardly applied in an active learning setting to guide the demonstration process of the expert ."}
{"title": "logic programming with social features", "abstract": "in everyday life it happens that a person has to reason about what other people think and how they behave , in order to achieve his goals . in other words , an individual may be required to adapt his behaviour by reasoning about the others ' mental state . in this paper we focus on a knowledge representation language derived from logic programming which both supports the representation of mental states of individual communities and provides each with the capability of reasoning about others ' mental states and acting accordingly . the proposed semantics is shown to be translatable into stable model semantics of logic programs with aggregates ."}
{"title": "speeding up permutation testing in neuroimaging", "abstract": "multiple hypothesis testing is a significant problem in nearly all neuroimaging studies . in order to correct for this phenomena , we require a reliable estimate of the family-wise error rate ( fwer ) . the well known bonferroni correction method , while simple to implement , is quite conservative , and can substantially under-power a study because it ignores dependencies between test statistics . permutation testing , on the other hand , is an exact , non-parametric method of estimating the fwer for a given $ \\alpha $ -threshold , but for acceptably low thresholds the computational burden can be prohibitive . in this paper , we show that permutation testing in fact amounts to populating the columns of a very large matrix $ { \\bf p } $ . by analyzing the spectrum of this matrix , under certain conditions , we see that $ { \\bf p } $ has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub -- sampled -- - on the order of $ 0.5\\ % $ -- - matrix completion methods . based on this observation , we propose a novel permutation testing methodology which offers a large speedup , without sacrificing the fidelity of the estimated fwer . our evaluations on four different neuroimaging datasets show that a computational speedup factor of roughly $ 50\\times $ can be achieved while recovering the fwer distribution up to very high accuracy . further , we show that the estimated $ \\alpha $ -threshold is also recovered faithfully , and is stable ."}
{"title": "compatibility family learning for item recommendation and generation", "abstract": "compatibility between items , such as clothes and shoes , is a major factor among customer 's purchasing decisions . however , learning `` compatibility '' is challenging due to ( 1 ) broader notions of compatibility than those of similarity , ( 2 ) the asymmetric nature of compatibility , and ( 3 ) only a small set of compatible and incompatible items are observed . we propose an end-to-end trainable system to embed each item into a latent vector and project a query item into k compatible prototypes in the same space . these prototypes reflect the broad notions of compatibility . we refer to both the embedding and prototypes as `` compatibility family '' . in our learned space , we introduce a novel projected compatibility distance ( pcd ) function which is differentiable and ensures diversity by aiming for at least one prototype to be close to a compatible item , whereas none of the prototypes are close to an incompatible item . we evaluate our system on a toy dataset , two amazon product datasets , and polyvore outfit dataset . our method consistently achieves state-of-the-art performance . finally , we show that we can visualize the candidate compatible prototypes using a metric-regularized conditional generative adversarial network ( mrcgan ) , where the input is a projected prototype and the output is a generated image of a compatible item . we ask human evaluators to judge the relative compatibility between our generated images and images generated by cgans conditioned directly on query items . our generated images are significantly preferred , with roughly twice the number of votes as others ."}
{"title": "an asp-based architecture for autonomous uavs in dynamic environments : progress report", "abstract": "traditional ai reasoning techniques have been used successfully in many domains , including logistics , scheduling and game playing . this paper is part of a project aimed at investigating how such techniques can be extended to coordinate teams of unmanned aerial vehicles ( uavs ) in dynamic environments . specifically challenging are real-world environments where uavs and other network-enabled devices must communicate to coordinate -- -and communication actions are neither reliable nor free . such network-centric environments are common in military , public safety and commercial applications , yet most research ( even multi-agent planning ) usually takes communications among distributed agents as a given . we address this challenge by developing an agent architecture and reasoning algorithms based on answer set programming ( asp ) . asp has been chosen for this task because it enables high flexibility of representation , both of knowledge and of reasoning tasks . although asp has been used successfully in a number of applications , and asp-based architectures have been studied for about a decade , to the best of our knowledge this is the first practical application of a complete asp-based agent architecture . it is also the first practical application of asp involving a combination of centralized reasoning , decentralized reasoning , execution monitoring , and reasoning about network communications . this work has been empirically validated using a distributed network-centric software evaluation testbed and the results provide guidance to designers in how to understand and control intelligent systems that operate in these environments ."}
{"title": "multi-agent only knowing", "abstract": "levesque introduced a notion of `` only knowing '' , with the goal of capturing certain types of nonmonotonic reasoning . levesque 's logic dealt with only the case of a single agent . recently , both halpern and lakemeyer independently attempted to extend levesque 's logic to the multi-agent case . although there are a number of similarities in their approaches , there are some significant differences . in this paper , we reexamine the notion of only knowing , going back to first principles . in the process , we simplify levesque 's completeness proof , and point out some problems with the earlier definitions . this leads us to reconsider what the properties of only knowing ought to be . we provide an axiom system that captures our desiderata , and show that it has a semantics that corresponds to it . the axiom system has an added feature of interest : it includes a modal operator for satisfiability , and thus provides a complete axiomatization for satisfiability in the logic k45 ."}
{"title": "pivotal pruning of trade-offs in qpns", "abstract": "qualitative probabilistic networks have been designed for probabilistic reasoning in a qualitative way . due to their coarse level of representation detail , qualitative probabilistic networks do not provide for resolving trade-offs and typically yield ambiguous results upon inference . we present an algorithm for computing more insightful results for unresolved trade-offs . the algorithm builds upon the idea of using pivots to zoom in on the trade-offs and identifying the information that would serve to resolve them ."}
{"title": "argumentation as a general framework for uncertain reasoning", "abstract": "argumentation is the process of constructing arguments about propositions , and the assignment of statements of confidence to those propositions based on the nature and relative strength of their supporting arguments . the process is modelled as a labelled deductive system , in which propositions are doubly labelled with the grounds on which they are based and a representation of the confidence attached to the argument . argument construction is captured by a generalized argument consequence relation based on the ^ , -- fragment of minimal logic . arguments can be aggregated by a variety of numeric and symbolic flattening functions . this approach appears to shed light on the common logical structure of a variety of quantitative , qualitative and defeasible uncertainty calculi ."}
{"title": "applying maxi-adjustment to adaptive information filtering agents", "abstract": "learning and adaptation is a fundamental property of intelligent agents . in the context of adaptive information filtering , a filtering agent 's beliefs about a user 's information needs have to be revised regularly with reference to the user 's most current information preferences . this learning and adaptation process is essential for maintaining the agent 's filtering performance . the agm belief revision paradigm provides a rigorous foundation for modelling rational and minimal changes to an agent 's beliefs . in particular , the maxi-adjustment method , which follows the agm rationale of belief change , offers a sound and robust computational mechanism to develop adaptive agents so that learning autonomy of these agents can be enhanced . this paper describes how the maxi-adjustment method is applied to develop the learning components of adaptive information filtering agents , and discusses possible difficulties of applying such a framework to these agents ."}
{"title": "phased exploration with greedy exploitation in stochastic combinatorial partial monitoring games", "abstract": "partial monitoring games are repeated games where the learner receives feedback that might be different from adversary 's move or even the reward gained by the learner . recently , a general model of combinatorial partial monitoring ( cpm ) games was proposed \\cite { lincombinatorial2014 } , where the learner 's action space can be exponentially large and adversary samples its moves from a bounded , continuous space , according to a fixed distribution . the paper gave a confidence bound based algorithm ( gcb ) that achieves $ o ( t^ { 2/3 } \\log t ) $ distribution independent and $ o ( \\log t ) $ distribution dependent regret bounds . the implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner . adopting their cpm model , our first contribution is a phased exploration with greedy exploitation ( pege ) algorithmic framework for the problem . different algorithms within the framework achieve $ o ( t^ { 2/3 } \\sqrt { \\log t } ) $ distribution independent and $ o ( \\log^2 t ) $ distribution dependent regret respectively . crucially , our framework needs only the simpler `` argmax '' oracle from gcb and the distribution dependent regret does not require existence of a unique optimal action . our second contribution is another algorithm , pege2 , which combines gap estimation with a pege algorithm , to achieve an $ o ( \\log t ) $ regret bound , matching the gcb guarantee but removing the dependence on size of the learner 's action space . however , like gcb , pege2 requires access to both offline oracles and the existence of a unique optimal action . finally , we discuss how our algorithm can be efficiently applied to a cpm problem of practical interest : namely , online ranking with feedback at the top ."}
{"title": "latent relational metric learning via memory-based attention for collaborative ranking", "abstract": "this paper proposes a new neural architecture for collaborative ranking with implicit feedback . our model , lrml ( \\textit { latent relational metric learning } ) is a novel metric learning approach for recommendation . more specifically , instead of simple push-pull mechanisms between user and item pairs , we propose to learn latent relations that describe each user item interaction . this helps to alleviate the potential geometric inflexibility of existing metric learing approaches . this enables not only better performance but also a greater extent of modeling capability , allowing our model to scale to a larger number of interactions . in order to do so , we employ a augmented memory module and learn to attend over these memory blocks to construct latent relations . the memory-based attention module is controlled by the user-item interaction , making the learned relation vector specific to each user-item pair . hence , this can be interpreted as learning an exclusive and optimal relational translation for each user-item interaction . the proposed architecture demonstrates the state-of-the-art performance across multiple recommendation benchmarks . lrml outperforms other metric learning models by $ 6\\ % -7.5\\ % $ in terms of hits @ 10 and ndcg @ 10 on large datasets such as netflix and movielens20m . moreover , qualitative studies also demonstrate evidence that our proposed model is able to infer and encode explicit sentiment , temporal and attribute information despite being only trained on implicit feedback . as such , this ascertains the ability of lrml to uncover hidden relational structure within implicit datasets ."}
{"title": "possibilistic networks : parameters learning from imprecise data and evaluation strategy", "abstract": "there has been an ever-increasing interest in multidisciplinary research on representing and reasoning with imperfect data . possibilistic networks present one of the powerful frameworks of interest for representing uncertain and imprecise information . this paper covers the problem of their parameters learning from imprecise datasets , i.e. , containing multi-valued data . we propose in the rst part of this paper a possibilistic networks sampling process . in the second part , we propose a likelihood function which explores the link between random sets theory and possibility theory . this function is then deployed to parametrize possibilistic networks ."}
{"title": "ontological representations of software patterns", "abstract": "this paper is based on and advocates the trend in software engineering of extending the use of software patterns as means of structuring solutions to software development problems ( be they motivated by best practice or by company interests and policies ) . the paper argues that , on the one hand , this development requires tools for automatic organisation , retrieval and explanation of software patterns . on the other hand , that the existence of such tools itself will facilitate the further development and employment of patterns in the software development process . the paper analyses existing pattern representations and concludes that they are inadequate for the kind of automation intended here . adopting a standpoint similar to that taken in the semantic web , the paper proposes that feasible solutions can be built on the basis of ontological representations ."}
{"title": "a necessary and sufficient condition for two relations to induce the same definable set family", "abstract": "in pawlak rough sets , the structure of the definable set families is simple and clear , but in generalizing rough sets , the structure of the definable set families is a bit more complex . there has been much research work focusing on this topic . however , as a fundamental issue in relation based rough sets , under what condition two relations induce the same definable set family has not been discussed . in this paper , based on the concept of the closure of relations , we present a necessary and sufficient condition for two relations to induce the same definable set family ."}
{"title": "a flexible framework for defeasible logics", "abstract": "logics for knowledge representation suffer from over-specialization : while each logic may provide an ideal representation formalism for some problems , it is less than optimal for others . a solution to this problem is to choose from several logics and , when necessary , combine the representations . in general , such an approach results in a very difficult problem of combination . however , if we can choose the logics from a uniform framework then the problem of combining them is greatly simplified . in this paper , we develop such a framework for defeasible logics . it supports all defeasible logics that satisfy a strong negation principle . we use logic meta-programs as the basis for the framework ."}
{"title": "a theory of multiclass boosting", "abstract": "boosting combines weak classifiers to form highly accurate predictors . although the case of binary classification is well understood , in the multiclass setting , the `` correct '' requirements on the weak classifier , or the notion of the most efficient boosting algorithms are missing . in this paper , we create a broad and general framework , within which we make precise and identify the optimal requirements on the weak-classifier , as well as design the most effective , in a certain sense , boosting algorithms that assume such requirements ."}
{"title": "robots that can adapt like animals", "abstract": "as robots leave the controlled environments of factories to autonomously function in more complex , natural environments , they will have to respond to the inevitable fact that they will become damaged . however , while animals can quickly adapt to a wide variety of injuries , current robots can not `` think outside the box '' to find a compensatory behavior when damaged : they are limited to their pre-specified self-sensing abilities , can diagnose only anticipated failure modes , and require a pre-programmed contingency plan for every type of potential damage , an impracticality for complex robots . here we introduce an intelligent trial and error algorithm that allows robots to adapt to damage in less than two minutes , without requiring self-diagnosis or pre-specified contingency plans . before deployment , a robot exploits a novel algorithm to create a detailed map of the space of high-performing behaviors : this map represents the robot 's intuitions about what behaviors it can perform and their value . if the robot is damaged , it uses these intuitions to guide a trial-and-error learning algorithm that conducts intelligent experiments to rapidly discover a compensatory behavior that works in spite of the damage . experiments reveal successful adaptations for a legged robot injured in five different ways , including damaged , broken , and missing legs , and for a robotic arm with joints broken in 14 different ways . this new technique will enable more robust , effective , autonomous robots , and suggests principles that animals may use to adapt to injury ."}
{"title": "altaltp : online parallelization of plans with heuristic state search", "abstract": "despite their near dominance , heuristic state search planners still lag behind disjunctive planners in the generation of parallel plans in classical planning . the reason is that directly searching for parallel solutions in state space planners would require the planners to branch on all possible subsets of parallel actions , thus increasing the branching factor exponentially . we present a variant of our heuristic state search planner altalt , called altaltp which generates parallel plans by using greedy online parallelization of partial plans . the greedy approach is significantly informed by the use of novel distance heuristics that altaltp derives from a graphplan-style planning graph for the problem . while this approach is not guaranteed to provide optimal parallel plans , empirical results show that altaltp is capable of generating good quality parallel plans at a fraction of the cost incurred by the disjunctive planners ."}
{"title": "using bayesian network representations for effective sampling from generative network models", "abstract": "bayesian networks ( bns ) are used for inference and sampling by exploiting conditional independence among random variables . context specific independence ( csi ) is a property of graphical models where additional independence relations arise in the context of particular values of random variables ( rvs ) . identifying and exploiting csi properties can simplify inference . some generative network models ( models that generate social/information network samples from a network distribution p ( g ) ) , with complex interactions among a set of rvs , can be represented with probabilistic graphical models , in particular with bns . in the present work we show one such a case . we discuss how a mixed kronecker product graph model can be represented as a bn , and study its bn properties that can be used for efficient sampling . specifically , we show that instead of exhibiting csi properties , the model has deterministic context-specific dependence ( dcsd ) . exploiting this property focuses the sampling method on a subset of the sampling space that improves efficiency ."}
{"title": "ordinal conditional functions for nearly counterfactual revision", "abstract": "we are interested in belief revision involving conditional statements where the antecedent is almost certainly false . to represent such problems , we use ordinal conditional functions that may take infinite values . we model belief change in this context through simple arithmetical operations that allow us to capture the intuition that certain antecedents can not be validated by any number of observations . we frame our approach as a form of finite belief improvement , and we propose a model of conditional belief revision in which only the `` right '' hypothetical levels of implausibility are revised ."}
{"title": "the effect of discrete vs. continuous-valued ratings on reputation and ranking systems", "abstract": "when users rate objects , a sophisticated algorithm that takes into account ability or reputation may produce a fairer or more accurate aggregation of ratings than the straightforward arithmetic average . recently a number of authors have proposed different co-determination algorithms where estimates of user and object reputation are refined iteratively together , permitting accurate measures of both to be derived directly from the rating data . however , simulations demonstrating these methods ' efficacy assumed a continuum of rating values , consistent with typical physical modelling practice , whereas in most actual rating systems only a limited range of discrete values ( such as a 5-star system ) is employed . we perform a comparative test of several co-determination algorithms with different scales of discrete ratings and show that this seemingly minor modification in fact has a significant impact on algorithms ' performance . paradoxically , where rating resolution is low , increased noise in users ' ratings may even improve the overall performance of the system ."}
{"title": "inducing a semantically annotated lexicon via em-based clustering", "abstract": "we present a technique for automatic induction of slot annotations for subcategorization frames , based on induction of hidden classes in the em framework of statistical estimation . the models are empirically evalutated by a general decision test . induction of slot labeling for subcategorization frames is accomplished by a further application of em , and applied experimentally on frame observations derived from parsing large corpora . we outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries ."}
{"title": "reasoning about minimal belief and negation as failure", "abstract": "we investigate the problem of reasoning in the propositional fragment of mbnf , the logic of minimal belief and negation as failure introduced by lifschitz , which can be considered as a unifying framework for several nonmonotonic formalisms , including default logic , autoepistemic logic , circumscription , epistemic queries , and logic programming . we characterize the complexity and provide algorithms for reasoning in propositional mbnf . in particular , we show that entailment in propositional mbnf lies at the third level of the polynomial hierarchy , hence it is harder than reasoning in all the above mentioned propositional formalisms for nonmonotonic reasoning . we also prove the exact correspondence between negation as failure in mbnf and negative introspection in moore 's autoepistemic logic ."}
{"title": "compositional planning using optimal option models", "abstract": "in this paper we introduce a framework for option model composition . option models are temporal abstractions that , like macro-operators in classical planning , jump directly from a start state to an end state . prior work has focused on constructing option models from primitive actions , by intra-option model learning ; or on using option models to construct a value function , by inter-option planning . we present a unified view of intra- and inter-option model learning , based on a major generalisation of the bellman equation . our fundamental operation is the recursive composition of option models into other option models . this key idea enables compositional planning over many levels of abstraction . we illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals , and also searches over those option models to provide rapid progress towards other subgoals ."}
{"title": "argumentation-based security for social good", "abstract": "the increase of connectivity and the impact it has in every day life is raising new and existing security problems that are becoming important for social good . we introduce two particular problems : cyber attack attribution and regulatory data sharing . for both problems , decisions about which rules to apply , should be taken under incomplete and context dependent information . the solution we propose is based on argumentation reasoning , that is a well suited technique for implementing decision making mechanisms under conflicting and incomplete information . our proposal permits us to identify the attacker of a cyber attack and decide the regulation rule that should be used while using and sharing data . we illustrate our solution through concrete examples ."}
{"title": "predicate logic with definitions", "abstract": "predicate logic with definitions ( pld or d-logic ) is a modification of first-order logic intended mostly for practical formalization of mathematics . the main syntactic constructs of d-logic are terms , formulas and definitions . a definition is a definition of variables , a definition of constants , or a composite definition ( d-logic has also abbreviation definitions called abbreviations ) . definitions can be used inside terms and formulas . this possibility alleviates introducing new quantifier-like names . composite definitions allow constructing new definitions from existing ones ."}
{"title": "a minimum relative entropy controller for undiscounted markov decision processes", "abstract": "adaptive control problems are notoriously difficult to solve even in the presence of plant-specific controllers . one way to by-pass the intractable computation of the optimal policy is to restate the adaptive control as the minimization of the relative entropy of a controller that ignores the true plant dynamics from an informed controller . the solution is given by the bayesian control rule-a set of equations characterizing a stochastic adaptive controller for the class of possible plant dynamics . here , the bayesian control rule is applied to derive bcr-mdp , a controller to solve undiscounted markov decision processes with finite state and action spaces and unknown dynamics . in particular , we derive a non-parametric conjugate prior distribution over the policy space that encapsulates the agent 's whole relevant history and we present a gibbs sampler to draw random policies from this distribution . preliminary results show that bcr-mdp successfully avoids sub-optimal limit cycles due to its built-in mechanism to balance exploration versus exploitation ."}
{"title": "some experiments with real-time decision algorithms", "abstract": "real-time decision algorithms are a class of incremental resource-bounded [ horvitz , 89 ] or anytime [ dean , 93 ] algorithms for evaluating influence diagrams . we present a test domain for real-time decision algorithms , and the results of experiments with several real-time decision algorithms in this domain . the results demonstrate high performance for two algorithms , a decision-evaluation variant of incremental probabilisitic inference [ d'ambrosio 93 ] and a variant of an algorithm suggested by goldszmidt , [ goldszmidt , 95 ] , pk-reduced . we discuss the implications of these experimental results and explore the broader applicability of these algorithms ."}
{"title": "the intriguing properties of model explanations", "abstract": "linear approximations to the decision boundary of a complex model have become one of the most popular tools for interpreting predictions . in this paper , we study such linear explanations produced either post-hoc by a few recent methods or generated along with predictions with contextual explanation networks ( cens ) . we focus on two questions : ( i ) whether linear explanations are always consistent or can be misleading , and ( ii ) when integrated into the prediction process , whether and how explanations affect the performance of the model . our analysis sheds more light on certain properties of explanations produced by different methods and suggests that learning models that explain and predict jointly is often advantageous ."}
{"title": "rapid randomized restarts for multi-agent path finding solvers", "abstract": "multi-agent path finding ( mapf ) is an np-hard problem well studied in artificial intelligence and robotics . it has many real-world applications for which existing mapf solvers use various heuristics . however , these solvers are deterministic and perform poorly on `` hard '' instances typically characterized by many agents interfering with each other in a small region . in this paper , we enhance mapf solvers with randomization and observe that they exhibit heavy-tailed distributions of runtimes on hard instances . this leads us to develop simple rapid randomized restart ( rrr ) strategies with the intuition that , given a hard instance , multiple short runs have a better chance of solving it compared to one long run . we validate this intuition through experiments and show that our rrr strategies indeed boost the performance of state-of-the-art mapf solvers such as iecbs and m* ."}
{"title": "robust planning in uncertain environments", "abstract": "this paper describes a novel approach to planning which takes advantage of decision theory to greatly improve robustness in an uncertain environment . we present an algorithm which computes conditional plans of maximum expected utility . this algorithm relies on a representation of the search space as an and/or tree and employs a depth-limit to control computation costs . a numeric robustness factor , which parameterizes the utility function , allows the user to modulate the degree of risk-aversion employed by the planner . via a look-ahead search , the planning algorithm seeks to find an optimal plan using expected utility as its optimization criterion . we present experimental results obtained by applying our algorithm to a non-deterministic extension of the blocks world domain . our results demonstrate that the robustness factor governs the degree of risk embodied in the conditional plans computed by our algorithm ."}
{"title": "adaptive learning with binary neurons", "abstract": "a efficient incremental learning algorithm for classification tasks , called netlines , well adapted for both binary and real-valued input patterns is presented . it generates small compact feedforward neural networks with one hidden layer of binary units and binary output units . a convergence theorem ensures that solutions with a finite number of hidden units exist for both binary and real-valued input patterns . an implementation for problems with more than two classes , valid for any binary classifier , is proposed . the generalization error and the size of the resulting networks are compared to the best published results on well-known classification benchmarks . early stopping is shown to decrease overfitting , without improving the generalization performance ."}
{"title": "explicit learning curves for transduction and application to clustering and compression algorithms", "abstract": "inductive learning is based on inferring a general rule from a finite data set and using it to label new data . in transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points , which are given to the learner prior to learning . although transduction seems at the outset to be an easier task than induction , there have not been many provably useful algorithms for transduction . moreover , the precise relation between induction and transduction has not yet been determined . the main theoretical developments related to transduction were presented by vapnik more than twenty years ago . one of vapnik 's basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail . while tight , this bound is given implicitly via a computational routine . our first contribution is a somewhat looser but explicit characterization of a slightly extended pac-bayesian version of vapnik 's transductive bound . this characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement . we then derive error bounds for compression schemes such as ( transductive ) support vector machines and for transduction algorithms based on clustering . the main observation used for deriving these new error bounds and algorithms is that the unlabeled test points , which in the transductive setting are known in advance , can be used in order to construct useful data dependent prior distributions over the hypothesis space ."}
{"title": "a predictive analytics approach to reducing avoidable hospital readmission", "abstract": "hospital readmission has become a critical metric of quality and cost of healthcare . medicare anticipates that nearly $ 17 billion is paid out on the 20 % of patients who are readmitted within 30 days of discharge . although several interventions such as transition care management and discharge reengineering have been practiced in recent years , the effectiveness and sustainability depends on how well they can identify and target patients at high risk of rehospitalization . based on the literature , most current risk prediction models fail to reach an acceptable accuracy level ; none of them considers patient 's history of readmission and impacts of patient attribute changes over time ; and they often do not discriminate between planned and unnecessary readmissions . tackling such drawbacks , we develop a new readmission metric based on administrative data that can identify potentially avoidable readmissions from all other types of readmission . we further propose a tree based classification method to estimate the predicted probability of readmission that can directly incorporate patient 's history of readmission and risk factors changes over time . the proposed methods are validated with 2011-12 veterans health administration data from inpatients hospitalized for heart failure , acute myocardial infarction , pneumonia , or chronic obstructive pulmonary disease in the state of michigan . results shows improved discrimination power compared to the literature ( c-statistics > 80 % ) and good calibration ."}
{"title": "consistency and random constraint satisfaction models", "abstract": "in this paper , we study the possibility of designing non-trivial random csp models by exploiting the intrinsic connection between structures and typical-case hardness . we show that constraint consistency , a notion that has been developed to improve the efficiency of csp algorithms , is in fact the key to the design of random csp models that have interesting phase transition behavior and guaranteed exponential resolution complexity without putting much restriction on the parameter of constraint tightness or the domain size of the problem . we propose a very flexible framework for constructing problem instances withinteresting behavior and develop a variety of concrete methods to construct specific random csp models that enforce different levels of constraint consistency . a series of experimental studies with interesting observations are carried out to illustrate the effectiveness of introducing structural elements in random instances , to verify the robustness of our proposal , and to investigate features of some specific models based on our framework that are highly related to the behavior of backtracking search algorithms ."}
{"title": "towards ultra rapid restarts", "abstract": "we observe a trend regarding restart strategies used in sat solvers . a few years ago , most state-of-the-art solvers restarted on average after a few thousands of backtracks . currently , restarting after a dozen backtracks results in much better performance . the main reason for this trend is that heuristics and data structures have become more restart-friendly . we expect further continuation of this trend , so future sat solvers will restart even more rapidly . additionally , we present experimental results to support our observations ."}
{"title": "artificial neural networks for beginners", "abstract": "the scope of this teaching package is to make a brief induction to artificial neural networks ( anns ) for people who have no previous knowledge of them . we first make a brief introduction to models of networks , for then describing in general terms anns . as an application , we explain the backpropagation algorithm , since it is widely used and many other algorithms are derived from it . the user should know algebra and the handling of functions and vectors . differential calculus is recommendable , but not necessary . the contents of this package should be understood by people with high school education . it would be useful for people who are just curious about what are anns , or for people who want to become familiar with them , so when they study them more fully , they will already have clear notions of anns . also , people who only want to apply the backpropagation algorithm without a detailed and formal explanation of it will find this material useful . this work should not be seen as `` nets for dummies '' , but of course it is not a treatise . much of the formality is skipped for the sake of simplicity . detailed explanations and demonstrations can be found in the referred readings . the included exercises complement the understanding of the theory . the on-line resources are highly recommended for extending this brief induction ."}
{"title": "how users explore ontologies on the web : a study of ncbo 's bioportal usage logs", "abstract": "ontologies in the biomedical domain are numerous , highly specialized and very expensive to develop . thus , a crucial prerequisite for ontology adoption and reuse is effective support for exploring and finding existing ontologies . towards that goal , the national center for biomedical ontology ( ncbo ) has developed bioportal -- -an online repository designed to support users in exploring and finding more than 500 existing biomedical ontologies . in 2016 , bioportal represents one of the largest portals for exploration of semantic biomedical vocabularies and terminologies , which is used by many researchers and practitioners . while usage of this portal is high , we know very little about how exactly users search and explore ontologies and what kind of usage patterns or user groups exist in the first place . deeper insights into user behavior on such portals can provide valuable information to devise strategies for a better support of users in exploring and finding existing ontologies , and thereby enable better ontology reuse . to that end , we study and group users according to their browsing behavior on bioportal using data mining techniques . additionally , we use the obtained groups to characterize and compare exploration strategies across ontologies . in particular , we were able to identify seven distinct browsing-behavior types , which all make use of different functionality provided by bioportal . for example , search explorers make extensive use of the search functionality while ontology tree explorers mainly rely on the class hierarchy to explore ontologies . further , we show that specific characteristics of ontologies influence the way users explore and interact with the website . our results may guide the development of more user-oriented systems for ontology exploration on the web ."}
{"title": "reinforcement learning-based thermal comfort control for vehicle cabins", "abstract": "vehicle climate control systems aim to keep passengers thermally comfortable . however , current systems control temperature rather than thermal comfort and tend to be energy hungry , which is of particular concern when considering electric vehicles . this paper poses energy-efficient vehicle comfort control as a markov decision process , which is then solved numerically using sarsa ( { \\lambda } ) and an empirically validated , single-zone , 1d thermal model of the cabin . the resulting controller was tested in simulation using 200 randomly selected scenarios and found to exceed the performance of bang-bang , proportional , simple fuzzy logic , and commercial controllers with 23 % , 43 % , 40 % , 56 % increase , respectively . compared to the next best performing controller , energy consumption is reduced by 13 % while the proportion of time spent thermally comfortable is increased by 23 % . these results indicate that this is a viable approach that promises to translate into substantial comfort and energy improvements in the car ."}
{"title": "inter-causal independence and heterogeneous factorization", "abstract": "it is well known that conditional independence can be used to factorize a joint probability into a multiplication of conditional probabilities . this paper proposes a constructive definition of inter-causal independence , which can be used to further factorize a conditional probability . an inference algorithm is developed , which makes use of both conditional independence and inter-causal independence to reduce inference complexity in bayesian networks ."}
{"title": "inheritance in object-oriented knowledge representation", "abstract": "this paper contains the consideration of inheritance mechanism in such knowledge representation models as object-oriented programming , frames and object-oriented dynamic networks . in addition , inheritance within representation of vague and imprecise knowledge are also discussed . new types of inheritance , general classification of all known inheritance types and approach , which allows avoiding in many cases problems with exceptions , redundancy and ambiguity within object-oriented dynamic networks and their fuzzy extension , are introduced in the paper . the proposed approach bases on conception of homogeneous and inhomogeneous or heterogeneous class of objects , which allow building of inheritance hierarchy more flexibly and efficiently ."}
{"title": "new results of ant algorithms for the linear ordering problem", "abstract": "ant-based algorithms are successful tools for solving complex problems . one of these problems is the linear ordering problem ( lop ) . the paper shows new results on some lop instances , using ant colony system ( acs ) and the step-back sensitive ant model ( sb-sam ) ."}
{"title": "analogy-based and case-based reasoning : two sides of the same coin", "abstract": "analogy-based ( or analogical ) and case-based reasoning ( abr and cbr ) are two similar problem solving processes based on the adaptation of the solution of past problems for use with a new analogous problem . in this paper we review these two processes and we give some real world examples with emphasis to the field of medicine , where one can find some of the most common and useful cbr applications . we also underline the differences between cbr and the classical rule-induction algorithms , we discuss the criticism for cbr methods and we focus on the future trends of research in the area of cbr ."}
{"title": "workshop notes of the 6th international workshop on acquisition , representation and reasoning about context with logic ( arcoe-logic 2014 )", "abstract": "arcoe-logic 2014 , the 6th international workshop on acquisition , representation and reasoning about context with logic , was held in co-location with the 19th international conference on knowledge engineering and knowledge management ( ekaw 2014 ) on november 25 , 2014 in link\\ '' oping , sweden . these notes contain the five papers which were accepted and presented at the workshop ."}
{"title": "probabilistic model-based approach for heart beat detection", "abstract": "nowadays , hospitals are ubiquitous and integral to modern society . patients flow in and out of a veritable whirlwind of paperwork , consultations , and potential inpatient admissions , through an abstracted system that is not without flaws . one of the biggest flaws in the medical system is perhaps an unexpected one : the patient alarm system . one longitudinal study reported an 88.8 % rate of false alarms , with other studies reporting numbers of similar magnitudes . these false alarm rates lead to a number of deleterious effects that manifest in a significantly lower standard of care across clinics . this paper discusses a model-based probabilistic inference approach to identifying variables at a detection level . we design a generative model that complies with an overview of human physiology and perform approximate bayesian inference . one primary goal of this paper is to justify a bayesian modeling approach to increasing robustness in a physiological domain . we use three data sets provided by physionet , a research resource for complex physiological signals , in the form of the physionet 2014 challenge set-p1 and set-p2 , as well as the mgh/mf waveform database . on the extended data set our algorithm is on par with the other top six submissions to the physionet 2014 challenge ."}
{"title": "a prototype of a knowledge-based programming environment", "abstract": "in this paper we present a proposal for a knowledge-based programming environment . in such an environment , declarative background knowledge , procedures , and concrete data are represented in suitable languages and combined in a flexible manner . this leads to a highly declarative programming style . we illustrate our approach on an example and report about our prototype implementation ."}
{"title": "using behavior objects to manage complexity in virtual worlds", "abstract": "the quality of high-level ai of non-player characters ( npcs ) in commercial open-world games ( owgs ) has been increasing during the past years . however , due to constraints specific to the game industry , this increase has been slow and it has been driven by larger budgets rather than adoption of new complex ai techniques . most of the contemporary ai is still expressed as hard-coded scripts . the complexity and manageability of the script codebase is one of the key limiting factors for further ai improvements . in this paper we address this issue . we present behavior objects - a general approach to development of npc behaviors for large owgs . behavior objects are inspired by object-oriented programming and extend the concept of smart objects . our approach promotes encapsulation of data and code for multiple related behaviors in one place , hiding internal details and embedding intelligence in the environment . behavior objects are a natural abstraction of five different techniques that we have implemented to manage ai complexity in an upcoming aaa owg . we report the details of the implementations in the context of behavior trees and the lessons learned during development . our work should serve as inspiration for ai architecture designers from both the academia and the industry ."}
{"title": "towards deep symbolic reinforcement learning", "abstract": "deep reinforcement learning ( drl ) brings the power of deep neural networks to bear on the generic task of trial-and-error learning , and its effectiveness has been convincingly demonstrated on tasks such as atari video games and the game of go . however , contemporary drl systems inherit a number of shortcomings from the current generation of deep learning techniques . for example , they require very large datasets to work effectively , entailing that they are slow to learn even when such datasets are available . moreover , they lack the ability to reason on an abstract level , which makes it difficult to implement high-level cognitive functions such as transfer learning , analogical reasoning , and hypothesis-based reasoning . finally , their operation is largely opaque to humans , rendering them unsuitable for domains in which verifiability is important . in this paper , we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings . as proof-of-concept , we present a preliminary implementation of the architecture and apply it to several variants of a simple video game . we show that the resulting system -- though just a prototype -- learns effectively , and , by acquiring a set of symbolic rules that are easily comprehensible to humans , dramatically outperforms a conventional , fully neural drl system on a stochastic variant of the game ."}
{"title": "independently controllable factors", "abstract": "it has been postulated that a good representation is one that disentangles the underlying explanatory factors of variation . however , it remains an open question what kind of training framework could potentially achieve that . whereas most previous work focuses on the static setting ( e.g. , with images ) , we postulate that some of the causal factors could be discovered if the learner is allowed to interact with its environment . the agent can experiment with different actions and observe their effects . more specifically , we hypothesize that some of these factors correspond to aspects of the environment which are independently controllable , i.e. , that there exists a policy and a learnable feature for each such aspect of the environment , such that this policy can yield changes in that feature with minimal changes to other features that explain the statistical variations in the observed data . we propose a specific objective function to find such factors and verify experimentally that it can indeed disentangle independently controllable aspects of the environment without any extrinsic reward signal ."}
{"title": "qualitative decision theory with sugeno integrals", "abstract": "this paper presents an axiomatic framework for qualitative decision under uncertainty in a finite setting . the corresponding utility is expressed by a sup-min expression , called sugeno ( or fuzzy ) integral . technically speaking , sugeno integral is a median , which is indeed a qualitative counterpart to the averaging operation underlying expected utility . the axiomatic justification of sugeno integral-based utility is expressed in terms of preference between acts as in savage decision theory . pessimistic and optimistic qualitative utilities , based on necessity and possibility measures , previously introduced by two of the authors , can be retrieved in this setting by adding appropriate axioms ."}
{"title": "dcs : an implementation of datalog with constraints", "abstract": "answer-set programming ( asp ) has emerged recently as a viable programming paradigm . we describe here an asp system , datalog with constraints or dc , based on non-monotonic logic . informally , dc theories consist of propositional clauses ( constraints ) and of horn rules . the semantics is a simple and natural extension of the semantics of the propositional logic . however , thanks to the presence of horn rules in the system , modeling of transitive closure becomes straightforward . we describe the syntax , use and implementation of dc and provide experimental results ."}
{"title": "adapting heuristic mastermind strategies to evolutionary algorithms", "abstract": "the art of solving the mastermind puzzle was initiated by donald knuth and is already more than 30 years old ; despite that , it still receives much attention in operational research and computer games journals , not to mention the nature-inspired stochastic algorithm literature . in this paper we try to suggest a strategy that will allow nature-inspired algorithms to obtain results as good as those based on exhaustive search strategies ; in order to do that , we first review , compare and improve current approaches to solving the puzzle ; then we test one of these strategies with an estimation of distribution algorithm . finally , we try to find a strategy that falls short of being exhaustive , and is then amenable for inclusion in nature inspired algorithms ( such as evolutionary or particle swarm algorithms ) . this paper proves that by the incorporation of local entropy into the fitness function of the evolutionary algorithm it becomes a better player than a random one , and gives a rule of thumb on how to incorporate the best heuristic strategies to evolutionary algorithms without incurring in an excessive computational cost ."}
{"title": "querying geometric figures using a controlled language , ontological graphs and dependency lattices", "abstract": "dynamic geometry systems ( dgs ) have become basic tools in many areas of geometry as , for example , in education . geometry automated theorem provers ( gatp ) are an active area of research and are considered as being basic tools in future enhanced educational software as well as in a next generation of mechanized mathematics assistants . recently emerged web repositories of geometric knowledge , like tgtp and intergeo , are an attempt to make the already vast data set of geometric knowledge widely available . considering the large amount of geometric information already available , we face the need of a query mechanism for descriptions of geometric constructions . in this paper we discuss two approaches for describing geometric figures ( declarative and procedural ) , and present algorithms for querying geometric figures in declaratively and procedurally described corpora , by using a dgs or a dedicated controlled natural language for queries ."}
{"title": "time and activity sequence prediction of business process instances", "abstract": "the ability to know in advance the trend of running process instances , with respect to different features , such as the expected completion time , would allow business managers to timely counteract to undesired situations , in order to prevent losses . therefore , the ability to accurately predict future features of running business process instances would be a very helpful aid when managing processes , especially under service level agreement constraints . however , making such accurate forecasts is not easy : many factors may influence the predicted features . many approaches have been proposed to cope with this problem but all of them assume that the underling process is stationary . however , in real cases this assumption is not always true . in this work we present new methods for predicting the remaining time of running cases . in particular we propose a method , assuming process stationarity , which outperforms the state-of-the-art and two other methods which are able to make predictions even with non-stationary processes . we also describe an approach able to predict the full sequence of activities that a running case is going to take . all these methods are extensively evaluated on two real case studies ."}
{"title": "query answering over contextualized rdf/owl knowledge with forall-existential bridge rules : decidable finite extension classes ( post print )", "abstract": "the proliferation of contextualized knowledge in the semantic web ( sw ) has led to the popularity of knowledge formats such as \\emph { quads } in the sw community . a quad is an extension of an rdf triple with contextual information of the triple . in this paper , we study the problem of query answering over quads augmented with forall-existential bridge rules that enable interoperability of reasoning between triples in various contexts . we call a set of quads together with such expressive bridge rules , a quad-system . query answering over quad-systems is undecidable , in general . we derive decidable classes of quad-systems , for which query answering can be done using forward chaining . sound , complete and terminating procedures , which are adaptations of the well known chase algorithm , are provided for these classes for deciding query entailment . safe , msafe , and csafe class of quad-systems restrict the structure of blank nodes generated during the chase computation process to be directed acyclic graphs ( dags ) of bounded depth . rr and restricted rr classes do not allow the generation of blank nodes during the chase computation process . both data and combined complexity of query entailment has been established for the classes derived . we further show that quad-systems are equivalent to forall-existential rules whose predicates are restricted to ternary arity , modulo polynomial time translations . we subsequently show that the technique of safety , strictly subsumes in expressivity , some of the well known and expressive techniques , such as joint acyclicity and model faithful acyclicity , used for decidability guarantees in the realm of forall-existential rules ."}
{"title": "flica : a framework for leader identification in coordinated activity", "abstract": "leadership is an important aspect of social organization that affects the processes of group formation , coordination , and decision-making in human societies , as well as in the social system of many other animal species . the ability to identify leaders based on their behavior and the subsequent reactions of others opens opportunities to explore how group decisions are made . understanding who exerts influence provides key insights into the structure of social organizations . in this paper , we propose a simple yet powerful leadership inference framework extracting group coordination periods and determining leadership based on the activity of individuals within a group . we are able to not only identify a leader or leaders but also classify the type of leadership model that is consistent with observed patterns of group decision-making . the framework performs well in differentiating a variety of leadership models ( e.g . dictatorship , linear hierarchy , or local influence ) . we propose five simple features that can be used to categorize characteristics of each leadership model , and thus make model classification possible . the proposed approach automatically ( 1 ) identifies periods of coordinated group activity , ( 2 ) determines the identities of leaders , and ( 3 ) classifies the likely mechanism by which the group coordination occurred . we demonstrate our framework on both simulated and real-world data : gps tracks of a baboon troop and video-tracking of fish schools , as well as stock market closing price data of the nasdaq index . the results of our leadership model are consistent with ground-truthed biological data and the framework finds many known events in financial data which are not otherwise reflected in the aggregate nasdaq index . our approach is easily generalizable to any coordinated activity data from interacting entities ."}
{"title": "on bayesian network approximation by edge deletion", "abstract": "we consider the problem of deleting edges from a bayesian network for the purpose of simplifying models in probabilistic inference . in particular , we propose a new method for deleting network edges , which is based on the evidence at hand . we provide some interesting bounds on the kl-divergence between original and approximate networks , which highlight the impact of given evidence on the quality of approximation and shed some light on good and bad candidates for edge deletion . we finally demonstrate empirically the promise of the proposed edge deletion technique as a basis for approximate inference ."}
{"title": "a new selection strategy for selective cluster ensemble based on diversity and independency", "abstract": "this research introduces a new strategy in cluster ensemble selection by using independency and diversity metrics . in recent years , diversity and quality , which are two metrics in evaluation procedure , have been used for selecting basic clustering results in the cluster ensemble selection . although quality can improve the final results in cluster ensemble , it can not control the procedures of generating basic results , which causes a gap in prediction of the generated basic results ' accuracy . instead of quality , this paper introduces independency as a supplementary method to be used in conjunction with diversity . therefore , this paper uses a heuristic metric , which is based on the procedure of converting code to graph in software testing , in order to calculate the independency of two basic clustering algorithms . moreover , a new modeling language , which we called as `` clustering algorithms independency language '' ( cail ) , is introduced in order to generate graphs which depict independency of algorithms . also , uniformity , which is a new similarity metric , has been introduced for evaluating the diversity of basic results . as a credential , our experimental results on varied different standard data sets show that the proposed framework improves the accuracy of final results dramatically in comparison with other cluster ensemble methods ."}
{"title": "mathematical programming strategies for solving the minimum common string partition problem", "abstract": "the minimum common string partition problem is an np-hard combinatorial optimization problem with applications in computational biology . in this work we propose the first integer linear programming model for solving this problem . moreover , on the basis of the integer linear programming model we develop a deterministic 2-phase heuristic which is applicable to larger problem instances . the results show that provenly optimal solutions can be obtained for problem instances of small and medium size from the literature by solving the proposed integer linear programming model with cplex . furthermore , new best-known solutions are obtained for all considered problem instances from the literature . concerning the heuristic , we were able to show that it outperforms heuristic competitors from the related literature ."}
{"title": "representing first-order causal theories by logic programs", "abstract": "nonmonotonic causal logic , introduced by norman mccain and hudson turner , became a basis for the semantics of several expressive action languages . mccain 's embedding of definite propositional causal theories into logic programming paved the way to the use of answer set solvers for answering queries about actions described in such languages . in this paper we extend this embedding to nondefinite theories and to first-order causal logic ."}
{"title": "evolutionary algorithms for reinforcement learning", "abstract": "there are two distinct approaches to solving reinforcement learning problems , namely , searching in value function space and searching in policy space . temporal difference methods and evolutionary algorithms are well-known examples of these approaches . kaelbling , littman and moore recently provided an informative survey of temporal difference methods . this article focuses on the application of evolutionary algorithms to the reinforcement learning problem , emphasizing alternative policy representations , credit assignment methods , and problem-specific genetic operators . strengths and weaknesses of the evolutionary approach to reinforcement learning are presented , along with a survey of representative applications ."}
{"title": "uncertainty management for fuzzy decision support systems", "abstract": "a new approach for uncertainty management for fuzzy , rule based decision support systems is proposed : the domain expert 's knowledge is expressed by a set of rules that frequently refer to vague and uncertain propositions . the certainty of propositions is represented using intervals [ a , b ] expressing that the proposition 's probability is at least a and at most b. methods and techniques for computing the overall certainty of fuzzy compound propositions that have been defined by using logical connectives 'and ' , 'or ' and 'not ' are introduced . different inference schemas for applying fuzzy rules by using modus ponens are discussed . different algorithms for combining evidence that has been received from different rules for the same proposition are provided . the relationship of the approach to other approaches is analyzed and its problems of knowledge acquisition and knowledge representation are discussed in some detail . the basic concepts of a rule-based programming language called picasso , for which the approach is a theoretical foundation , are outlined ."}
{"title": "machine learning for the geosciences : challenges and opportunities", "abstract": "geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet . as geosciences enters the era of big data , machine learning ( ml ) -- that has been widely successful in commercial domains -- offers immense potential to contribute to problems in geosciences . however , problems in geosciences have several unique challenges that are seldom found in traditional applications , requiring novel problem formulations and methodologies in machine learning . this article introduces researchers in the machine learning ( ml ) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences . we first highlight typical sources of geoscience data and describe their properties that make it challenging to use traditional machine learning techniques . we then describe some of the common categories of geoscience problems where machine learning can play a role , and discuss some of the existing efforts and promising directions for methodological development in machine learning . we conclude by discussing some of the emerging research themes in machine learning that are applicable across all problems in the geosciences , and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines ."}
{"title": "iterated variable neighborhood search for the resource constrained multi-mode multi-project scheduling problem", "abstract": "the resource constrained multi-mode multi-project scheduling problem ( rcmmmpsp ) is a notoriously difficult combinatorial optimization problem . for a given set of activities , feasible execution mode assignments and execution starting times must be found such that some optimization function , e.g . the makespan , is optimized . when determining an optimal ( or at least feasible ) assignment of decision variable values , a set of side constraints , such as resource availabilities , precedence constraints , etc. , has to be respected . in 2013 , the mista 2013 challenge stipulated research in the rcmmmpsp . it 's goal was the solution of a given set of instances under running time restrictions . we have contributed to this challenge with the here presented approach ."}
{"title": "a large-scale car dataset for fine-grained categorization and verification", "abstract": "updated on 24/09/2015 : this update provides preliminary experiment results for fine-grained classification on the surveillance data of compcars . the train/test splits are provided in the updated dataset . see details in section 6 ."}
{"title": "lifted probabilistic inference for asymmetric graphical models", "abstract": "lifted probabilistic inference algorithms have been successfully applied to a large number of symmetric graphical models . unfortunately , the majority of real-world graphical models is asymmetric . this is even the case for relational representations when evidence is given . therefore , more recent work in the community moved to making the models symmetric and then applying existing lifted inference algorithms . however , this approach has two shortcomings . first , all existing over-symmetric approximations require a relational representation such as markov logic networks . second , the induced symmetries often change the distribution significantly , making the computed probabilities highly biased . we present a framework for probabilistic sampling-based inference that only uses the induced approximate symmetries to propose steps in a metropolis-hastings style markov chain . the framework , therefore , leads to improved probability estimates while remaining unbiased . experiments demonstrate that the approach outperforms existing mcmc algorithms ."}
{"title": "clustering without ( thinking about ) triangulation", "abstract": "the undirected technique for evaluating belief networks [ jensen , et.al. , 1990 , lauritzen and spiegelhalter , 1988 ] requires clustering the nodes in the network into a junction tree . in the traditional view , the junction tree is constructed from the cliques of the moralized and triangulated belief network : triangulation is taken to be the primitive concept , the goal towards which any clustering algorithm ( e.g . node elimination ) is directed . in this paper , we present an alternative conception of clustering , in which clusters and the junction tree property play the role of primitives : given a graph ( not a tree ) of clusters which obey ( a modified version of ) the junction tree property , we transform this graph until we have obtained a tree . there are several advantages to this approach : it is much clearer and easier to understand , which is important for humans who are constructing belief networks ; it admits a wider range of heuristics which may enable more efficient or superior clustering algorithms ; and it serves as the natural basis for an incremental clustering scheme , which we describe ."}
{"title": "norm-based capacity control in neural networks", "abstract": "we investigate the capacity , convexity and characterization of a general family of norm-constrained feed-forward networks ."}
{"title": "a logic of graded possibility and certainty coping with partial inconsistency", "abstract": "a semantics is given to possibilistic logic , a logic that handles weighted classical logic formulae , and where weights are interpreted as lower bounds on degrees of certainty or possibility , in the sense of zadeh 's possibility theory . the proposed semantics is based on fuzzy sets of interpretations . it is tolerant to partial inconsistency . satisfiability is extended from interpretations to fuzzy sets of interpretations , each fuzzy set representing a possibility distribution describing what is known about the state of the world . a possibilistic knowledge base is then viewed as a set of possibility distributions that satisfy it . the refutation method of automated deduction in possibilistic logic , based on previously introduced generalized resolution principle is proved to be sound and complete with respect to the proposed semantics , including the case of partial inconsistency ."}
{"title": "multiple decision trees", "abstract": "this paper describes experiments , on two domains , to investigate the effect of averaging over predictions of multiple decision trees , instead of using a single tree . other authors have pointed out theoretical and commonsense reasons for preferring the multiple tree approach . ideally , we would like to consider predictions from all trees , weighted by their probability . however , there is a vast number of different trees , and it is difficult to estimate the probability of each tree . we sidestep the estimation problem by using a modified version of the id3 algorithm to build good trees , and average over only these trees . our results are encouraging . for each domain , we managed to produce a small number of good trees . we find that it is best to average across sets of trees with different structure ; this usually gives better performance than any of the constituent trees , including the id3 tree ."}
{"title": "understanding the complexity of # sat using knowledge compilation", "abstract": "two main techniques have been used so far to solve the # p-hard problem # sat . the first one , used in practice , is based on an extension of dpll for model counting called exhaustive dpll . the second approach , more theoretical , exploits the structure of the input to compute the number of satisfying assignments by usually using a dynamic programming scheme on a decomposition of the formula . in this paper , we make a first step toward the separation of these two techniques by exhibiting a family of formulas that can be solved in polynomial time with the first technique but needs an exponential time with the second one . we show this by observing that both techniques implicitely construct a very specific boolean circuit equivalent to the input formula . we then show that every beta-acyclic formula can be represented by a polynomial size circuit corresponding to the first method and exhibit a family of beta-acyclic formulas which can not be represented by polynomial size circuits corresponding to the second method . this result shed a new light on the complexity of # sat and related problems on beta-acyclic formulas . as a byproduct , we give new handy tools to design algorithms on beta-acyclic hypergraphs ."}
{"title": "extracting features from ratings : the role of factor models", "abstract": "performing effective preference-based data retrieval requires detailed and preferentially meaningful structurized information about the current user as well as the items under consideration . a common problem is that representations of items often only consist of mere technical attributes , which do not resemble human perception . this is particularly true for integral items such as movies or songs . it is often claimed that meaningful item features could be extracted from collaborative rating data , which is becoming available through social networking services . however , there is only anecdotal evidence supporting this claim ; but if it is true , the extracted information could very valuable for preference-based data retrieval . in this paper , we propose a methodology to systematically check this common claim . we performed a preliminary investigation on a large collection of movie ratings and present initial evidence ."}
{"title": "a neutrosophic description logic", "abstract": "description logics ( dls ) are appropriate , widely used , logics for managing structured knowledge . they allow reasoning about individuals and concepts , i.e . set of individuals with common properties . typically , dls are limited to dealing with crisp , well defined concepts . that is , concepts for which the problem whether an individual is an instance of it is yes/no question . more often than not , the concepts encountered in the real world do not have a precisely defined criteria of membership : we may say that an individual is an instance of a concept only to a certain degree , depending on the individual 's properties . the dls that deal with such fuzzy concepts are called fuzzy dls . in order to deal with fuzzy , incomplete , indeterminate and inconsistent concepts , we need to extend the fuzzy dls , combining the neutrosophic logic with a classical dl . in particular , concepts become neutrosophic ( here neutrosophic means fuzzy , incomplete , indeterminate , and inconsistent ) , thus reasoning about neutrosophic concepts is supported . we 'll define its syntax , its semantics , and describe its properties ."}
{"title": "selecting computations : theory and applications", "abstract": "sequential decision problems are often approximately solvable by simulating possible future action sequences . metalevel decision procedures have been developed for selecting which action sequences to simulate , based on estimating the expected improvement in decision quality that would result from any particular simulation ; an example is the recent work on using bandit algorithms to control monte carlo tree search in the game of go . in this paper we develop a theoretical basis for metalevel decisions in the statistical framework of bayesian selection problems , arguing ( as others have done ) that this is more appropriate than the bandit framework . we derive a number of basic results applicable to monte carlo selection problems , including the first finite sampling bounds for optimal policies in certain cases ; we also provide a simple counterexample to the intuitive conjecture that an optimal policy will necessarily reach a decision in all cases . we then derive heuristic approximations in both bayesian and distribution-free settings and demonstrate their superiority to bandit-based heuristics in one-shot decision problems and in go ."}
{"title": "formalizing scenario analysis", "abstract": "we propose a formal treatment of scenarios in the context of a dialectical argumentation formalism for qualitative reasoning about uncertain propositions . our formalism extends prior work in which arguments for and against uncertain propositions were presented and compared in interaction spaces called agoras . we now define the notion of a scenario in this framework and use it to define a set of qualitative uncertainty labels for propositions across a collection of scenarios . this work is intended to lead to a formal theory of scenarios and scenario analysis ."}
{"title": "probabilistic model checking for complex cognitive tasks -- a case study in human-robot interaction", "abstract": "this paper proposes to use probabilistic model checking to synthesize optimal robot policies in multi-tasking autonomous systems that are subject to human-robot interaction . given the convincing empirical evidence that human behavior can be related to reinforcement models , we take as input a well-studied q-table model of the human behavior for flexible scenarios . we first describe an automated procedure to distill a markov decision process ( mdp ) for the human in an arbitrary but fixed scenario . the distinctive issue is that -- in contrast to existing models -- under-specification of the human behavior is included . probabilistic model checking is used to predict the human 's behavior . finally , the mdp model is extended with a robot model . optimal robot policies are synthesized by analyzing the resulting two-player stochastic game . experimental results with a prototypical implementation using prism show promising results ."}
{"title": "alternating optimisation and quadrature for robust control", "abstract": "bayesian optimisation has been successfully applied to a variety of reinforcement learning problems . however , the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables : state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator . this paper considers the problem of finding a robust policy while taking into account the impact of environment variables . we present alternating optimisation and quadrature ( aloq ) , which uses bayesian optimisation and bayesian quadrature to address such settings . aloq is robust to the presence of significant rare events , which may not be observable under random sampling , but play a substantial role in determining the optimal policy . experimental results across different domains show that aloq can learn more efficiently and robustly than existing methods ."}
{"title": "encoding reality : prediction-assisted cortical learning algorithm in hierarchical temporal memory", "abstract": "in the decade since jeff hawkins proposed hierarchical temporal memory ( htm ) as a model of neocortical computation , the theory and the algorithms have evolved dramatically . this paper presents a detailed description of htm 's cortical learning algorithm ( cla ) , including for the first time a rigorous mathematical formulation of all aspects of the computations . prediction assisted cla ( pacla ) , a refinement of the cla is presented , which is both closer to the neuroscience and adds significantly to the computational power . finally , we summarise the key functions of neocortex which are expressed in pacla implementations ."}
{"title": "object manipulation learning by imitation", "abstract": "we aim to enable robot to learn object manipulation by imitation . given external observations of demonstrations on object manipulations , we believe that two underlying problems to address in learning by imitation is 1 ) segment a given demonstration into skills that can be individually learned and reused , and 2 ) formulate the correct rl ( reinforcement learning ) problem that only considers the relevant aspects of each skill so that the policy for each skill can be effectively learned . previous works made certain progress in this direction , but none has taken private information into account . the public information is the information that is available in the external observations of demonstration , and the private information is the information that are only available to the agent that executes the actions , such as tactile sensations . our contribution is that we provide a method for the robot to automatically segment the demonstration of object manipulations into multiple skills , and formulate the correct rl problem for each skill , and automatically decide whether the private information is an important aspect of each skill based on interaction with the world . our experiment shows that our robot learns to pick up a block , and stack it onto another block by imitating an observed demonstration . the evaluation is based on 1 ) whether the demonstration is reasonably segmented , 2 ) whether the correct rl problems are formulated , 3 ) and whether a good policy is learned ."}
{"title": "role of ontology in semantic web development", "abstract": "world wide web ( www ) is the most popular global information sharing and communication system consisting of three standards .i.e. , uniform resource identifier ( url ) , hypertext transfer protocol ( http ) and hypertext mark-up language ( html ) . information is provided in text , image , audio and video formats over the web by using html which is considered to be unconventional in defining and formalizing the meaning of the context ..."}
{"title": "myopic value of information in influence diagrams", "abstract": "we present a method for calculation of myopic value of information in influence diagrams ( howard & matheson , 1981 ) based on the strong junction tree framework ( jensen , jensen & dittmer , 1994 ) . the difference in instantiation order in the influence diagrams is reflected in the corresponding junction trees by the order in which the chance nodes are marginalized . this order of marginalization can be changed by table expansion and in effect the same junction tree with expanded tables may be used for calculating the expected utility for scenarios with different instantiation order . we also compare our method to the classic method of modeling different instantiation orders in the same influence diagram ."}
{"title": "reasoning about actions with temporal answer sets", "abstract": "in this paper we combine answer set programming ( asp ) with dynamic linear time temporal logic ( dltl ) to define a temporal logic programming language for reasoning about complex actions and infinite computations . dltl extends propositional temporal logic of linear time with regular programs of propositional dynamic logic , which are used for indexing temporal modalities . the action language allows general dltl formulas to be included in domain descriptions to constrain the space of possible extensions . we introduce a notion of temporal answer set for domain descriptions , based on the usual notion of answer set . also , we provide a translation of domain descriptions into standard asp and we use bounded model checking techniques for the verification of dltl constraints ."}
{"title": "the generalized smallest grammar problem", "abstract": "the smallest grammar problem -- the problem of finding the smallest context-free grammar that generates exactly one given sequence -- has never been successfully applied to grammatical inference . we investigate the reasons and propose an extended formulation that seeks to minimize non-recursive grammars , instead of straight-line programs . in addition , we provide very efficient algorithms that approximate the minimization problem of this class of grammars . our empirical evaluation shows that we are able to find smaller models than the current best approximations to the smallest grammar problem on standard benchmarks , and that the inferred rules capture much better the syntactic structure of natural language ."}
{"title": "the cyborg astrobiologist : scouting red beds for uncommon features with geological significance", "abstract": "the ` cyborg astrobiologist ' ( ca ) has undergone a second geological field trial , at a red sandstone site in northern guadalajara , spain , near riba de santiuste . the cyborg astrobiologist is a wearable computer and video camera system that has demonstrated a capability to find uncommon interest points in geological imagery in real-time in the field . the first ( of three ) geological structures that we studied was an outcrop of nearly homogeneous sandstone , which exhibits oxidized-iron impurities in red and and an absence of these iron impurities in white . the white areas in these `` red beds '' have turned white because the iron has been removed by chemical reduction , perhaps by a biological agent . the computer vision system found in one instance several ( iron-free ) white spots to be uncommon and therefore interesting , as well as several small and dark nodules . the second geological structure contained white , textured mineral deposits on the surface of the sandstone , which were found by the ca to be interesting . the third geological structure was a 50 cm thick paleosol layer , with fossilized root structures of some plants , which were found by the ca to be interesting . a quasi-blind comparison of the cyborg astrobiologist 's interest points for these images with the interest points determined afterwards by a human geologist shows that the cyborg astrobiologist concurred with the human geologist 68 % of the time ( true positive rate ) , with a 32 % false positive rate and a 32 % false negative rate . ( abstract has been abridged ) ."}
{"title": "multiwinner approval rules as apportionment methods", "abstract": "we establish a link between multiwinner elections and apportionment problems by showing how approval-based multiwinner election rules can be interpreted as methods of apportionment . we consider several multiwinner rules and observe that they induce apportionment methods that are well-established in the literature on proportional representation . for instance , we show that proportional approval voting induces the d'hondt method and that monroe 's rule induces the largest reminder method . we also consider properties of apportionment methods and exhibit multiwinner rules that induce apportionment methods satisfying these properties ."}
{"title": "knowledge representation for high-level norms and violation inference in logic programming", "abstract": "most of the knowledge representation formalisms developed for representing prescriptive norms can be categorized as either suitable for representing either low level or high level norms.we argue that low level norm representations do not advance the cause of autonomy in agents in the sense that it is not the agent itself that determines the normative position it should be at a particular time , on the account of a more general rule . in other words an agent on some external system for a nitty gritty prescriptions of its obligations and prohibitions . on the other hand , high level norms which have an explicit description of a norm 's precondition and have some form of implication , do not as they exist in the literature do not support generalized inferences about violation like low level norm representations do . this paper presents a logical formalism for the representation of high level norms in open societies that enable violation inferences that detail the situation in which the norm violation took place and the identity of the norm violation . norms are formalized as logic programs whose heads specify what an agent is obliged or permitted to do when a situation arises and within what time constraint of the situation.each norm is also assigned an identity using some reification scheme . the body of each logic program describes the nature of the situation in which the agent is expected to act or desist from acting . this kind of violation is novel in the literature ."}
{"title": "causal effect identification in acyclic directed mixed graphs and gated models", "abstract": "we introduce a new family of graphical models that consists of graphs with possibly directed , undirected and bidirected edges but without directed cycles . we show that these models are suitable for representing causal models with additive error terms . we provide a set of sufficient graphical criteria for the identification of arbitrary causal effects when the new models contain directed and undirected edges but no bidirected edge . we also provide a necessary and sufficient graphical criterion for the identification of the causal effect of a single variable on the rest of the variables . moreover , we develop an exact algorithm for learning the new models from observational and interventional data via answer set programming . finally , we introduce gated models for causal effect identification , a new family of graphical models that exploits context specific independences to identify additional causal effects ."}
{"title": "thoughts on an unified framework for artificial chemistries", "abstract": "artificial chemistries ( acs ) are symbolic chemical metaphors for the exploration of artificial life , with specific focus on the problem of biogenesis or the origin of life . this paper presents authors thoughts towards defining a unified framework to characterize and classify symbolic artificial chemistries by devising appropriate formalism to capture semantic and organizational information . we identify three basic high level abstractions in initial proposal for this framework viz. , information , computation , and communication . we present an analysis of two important notions of information , namely , shannon 's entropy and algorithmic information , and discuss inductive and deductive approaches for defining the framework ."}
{"title": "flexible interpretations : a computational model for dynamic uncertainty assessment", "abstract": "the investigations reported in this paper center on the process of dynamic uncertainty assessment during interpretation tasks in real domain . in particular , we are interested here in the nature of the control structure of computer programs that can support multiple interpretation and smooth transitions between them , in real time . each step of the processing involves the interpretation of one input item and the appropriate re-establishment of the system 's confidence of the correctness of its interpretation ( s ) ."}
{"title": "a two-phase safe vehicle routing and scheduling problem : formulations and solution algorithms", "abstract": "we propose a two phase time dependent vehicle routing and scheduling optimization model that identifies the safest routes , as a substitute for the classical objectives given in the literature such as shortest distance or travel time , through ( 1 ) avoiding recurring congestions , and ( 2 ) selecting routes that have a lower probability of crash occurrences and non-recurring congestion caused by those crashes . in the first phase , we solve a mixed-integer programming model which takes the dynamic speed variations into account on a graph of roadway networks according to the time of day , and identify the routing of a fleet and sequence of nodes on the safest feasible paths . second phase considers each route as an independent transit path ( fixed route with fixed node sequences ) , and tries to avoid congestion by rescheduling the departure times of each vehicle from each node , and by adjusting the sub-optimal speed on each arc . a modified simulated annealing ( sa ) algorithm is formulated to solve both complex models iteratively , which is found to be capable of providing solutions in a considerably short amount of time ."}
{"title": "research on several key technologies in practical speech emotion recognition", "abstract": "in this dissertation the practical speech emotion recognition technology is studied , including several cognitive related emotion types , namely fidgetiness , confidence and tiredness . the high quality of naturalistic emotional speech data is the basis of this research . the following techniques are used for inducing practical emotional speech : cognitive task , computer game , noise stimulation , sleep deprivation and movie clips . a practical speech emotion recognition system is studied based on gaussian mixture model . a two-class classifier set is adopted for performance improvement under the small sample case . considering the context information in continuous emotional speech , a gaussian mixture model embedded with markov networks is proposed . a further study is carried out for system robustness analysis . first , noise reduction algorithm based on auditory masking properties is fist introduced to the practical speech emotion recognition . second , to deal with the complicated unknown emotion types under real situation , an emotion recognition method with rejection ability is proposed , which enhanced the system compatibility against unknown emotion samples . third , coping with the difficulties brought by a large number of unknown speakers , an emotional feature normalization method based on speaker-sensitive feature clustering is proposed . fourth , by adding the electrocardiogram channel , a bi-modal emotion recognition system based on speech signals and electrocardiogram signals is first introduced . the speech emotion recognition methods studied in this dissertation may be extended into the cross-language speech emotion recognition and the whispered speech emotion recognition ."}
{"title": "reduced space and faster convergence in imperfect-information games via regret-based pruning", "abstract": "counterfactual regret minimization ( cfr ) is the most popular iterative algorithm for solving zero-sum imperfect-information games . regret-based pruning ( rbp ) is an improvement that allows poorly-performing actions to be temporarily pruned , thus speeding up cfr . we introduce total rbp , a new form of rbp that reduces the space requirements of cfr as actions are pruned . we prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some nash equilibrium . this leads to provably faster convergence and lower space requirements . experiments show that total rbp results in an order of magnitude reduction in space , and the reduction factor increases with game size ."}
{"title": "the power of modeling - a response to pddl2.1", "abstract": "in this commentary i argue that although pddl is a very useful standard for the planning competition , its design does not properly consider the issue of domain modeling . hence , i would not advocate its use in specifying planning domains outside of the context of the planning competition . rather , the field needs to explore different approaches and grapple more directly with the problem of effectively modeling and utilizing all of the diverse pieces of knowledge we typically have about planning domains ."}
{"title": "easy monotonic policy iteration", "abstract": "a key problem in reinforcement learning for control with general function approximators ( such as deep neural networks and other nonlinear functions ) is that , for many algorithms employed in practice , updates to the policy or $ q $ -function may fail to improve performance -- -or worse , actually cause the policy performance to degrade . prior work has addressed this for policy iteration by deriving tight policy improvement bounds ; by optimizing the lower bound on policy improvement , a better policy is guaranteed . however , existing approaches suffer from bounds that are hard to optimize in practice because they include sup norm terms which can not be efficiently estimated or differentiated . in this work , we derive a better policy improvement bound where the sup norm of the policy divergence has been replaced with an average divergence ; this leads to an algorithm , easy monotonic policy iteration , that generates sequences of policies with guaranteed non-decreasing returns and is easy to implement in a sample-based framework ."}
{"title": "all-relevant feature selection using multidimensional filters with exhaustive search", "abstract": "this paper describes a method for identification of the informative variables in the information system with discrete decision variables . it is targeted specifically towards discovery of the variables that are non-informative when considered alone , but are informative when the synergistic interactions between multiple variables are considered . to this end , the mutual entropy of all possible k-tuples of variables with decision variable is computed . then , for each variable the maximal information gain due to interactions with other variables is obtained . for non-informative variables this quantity conforms to the well known statistical distributions . this allows for discerning truly informative variables from non-informative ones . for demonstration of the approach , the method is applied to several synthetic datasets that involve complex multidimensional interactions between variables . it is capable of identifying most important informative variables , even in the case when the dimensionality of the analysis is smaller than the true dimensionality of the problem . what is more , the high sensitivity of the algorithm allows for detection of the influence of nuisance variables on the response variable ."}
{"title": "mining generalized patterns from large databases using ontologies", "abstract": "formal concept analysis ( fca ) is a mathematical theory based on the formalization of the notions of concept and concept hierarchies . it has been successfully applied to several computer science fields such as data mining , software engineering , and knowledge engineering , and in many domains like medicine , psychology , linguistics and ecology . for instance , it has been exploited for the design , mapping and refinement of ontologies . in this paper , we show how fca can benefit from a given domain ontology by analyzing the impact of a taxonomy ( on objects and/or attributes ) on the resulting concept lattice . we willmainly concentrate on the usage of a taxonomy to extract generalized patterns ( i.e. , knowledge generated from data when elements of a given domain ontology are used ) in the form of concepts and rules , and improve navigation through these patterns . to that end , we analyze three generalization cases and show their impact on the size of the generalized pattern set . different scenarios of simultaneous generalizations on both objects and attributes are also discussed"}
{"title": "a survey of methods for explaining black box models", "abstract": "in the last years many accurate decision support systems have been constructed as black boxes , that is as systems that hide their internal logic to the user . this lack of explanation constitutes both a practical and an ethical issue . the literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability . the applications in which black box decision systems can be used are various , and each approach is typically developed to provide a solution for a specific problem and , as a consequence , delineating explicitly or implicitly its own definition of interpretability and explanation . the aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system . given a problem definition , a black box type , and a desired explanation this survey should help the researcher to find the proposals more useful for his own work . the proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective ."}
{"title": "houdini : fooling deep structured prediction models", "abstract": "generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines . so far , most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand . we introduce a novel flexible approach named houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered , be it combinatorial and non-decomposable . we successfully apply houdini to a range of applications such as speech recognition , pose estimation and semantic segmentation . in all cases , the attacks based on houdini achieve higher success rate than those based on the traditional surrogates used to train the models while using a less perceptible adversarial perturbation ."}
{"title": "hybrid gradient boosting trees and neural networks for forecasting operating room data", "abstract": "time series data constitutes a distinct and growing problem in machine learning . as the corpus of time series data grows larger , deep models that simultaneously learn features and classify with these features can be intractable or suboptimal . in this paper , we present feature learning via long short term memory ( lstm ) networks and prediction via gradient boosting trees ( xgb ) . focusing on the consequential setting of electronic health record data , we predict the occurrence of hypoxemia five minutes into the future based on past features . we make two observations : 1 ) long short term memory networks are effective at capturing long term dependencies based on a single feature and 2 ) gradient boosting trees are capable of tractably combining a large number of features including static features like height and weight . with these observations in mind , we generate features by performing `` supervised '' representation learning with lstm networks . augmenting the original xgb model with these features gives significantly better performance than either individual method ."}
{"title": "polymetric rhythmic feel for a cognitive drum computer", "abstract": "this paper addresses a question about music cognition : how do we derive polymetric structures . a preference rule system is presented which is implemented into a drum computer . the preference rule system allows inferring local polymetric structures , like two-over-three and three-over-two . by analyzing the micro-timing of west african percussion music a timing pattern consisting of six pulses was discovered . it integrates binary and ternary rhythmic feels . the presented drum computer integrates the discovered superimposed polymetric swing ( timing and velocity ) appropriate to the rhythmic sequence the user inputs . for binary sequences , the amount of binary swing is increased and for ternary sequences , the ternary swing is increased ."}
{"title": "perception driven texture generation", "abstract": "this paper investigates a novel task of generating texture images from perceptual descriptions . previous work on texture generation focused on either synthesis from examples or generation from procedural models . generating textures from perceptual attributes have not been well studied yet . meanwhile , perceptual attributes , such as directionality , regularity and roughness are important factors for human observers to describe a texture . in this paper , we propose a joint deep network model that combines adversarial training and perceptual feature regression for texture generation , while only random noise and user-defined perceptual attributes are required as input . in this model , a preliminary trained convolutional neural network is essentially integrated with the adversarial framework , which can drive the generated textures to possess given perceptual attributes . an important aspect of the proposed model is that , if we change one of the input perceptual features , the corresponding appearance of the generated textures will also be changed . we design several experiments to validate the effectiveness of the proposed method . the results show that the proposed method can produce high quality texture images with desired perceptual properties ."}
{"title": "model-based debugging using multiple abstract models", "abstract": "this paper introduces an automatic debugging framework that relies on model-based reasoning techniques to locate faults in programs . in particular , model-based diagnosis , together with an abstract interpretation based conflict detection mechanism is used to derive diagnoses , which correspond to possible faults in programs . design information and partial specifications are applied to guide a model revision process , which allows for automatic detection and correction of structural faults ."}
{"title": "allocating indivisible items in categorized domains", "abstract": "we formulate a general class of allocation problems called categorized domain allocation problems ( cdaps ) , where indivisible items from multiple categories are allocated to agents without monetary transfer and each agent gets at least one item per category . we focus on basic cdaps , where the number of items in each category is equal to the number of agents . we characterize serial dictatorships for basic cdaps by a minimal set of three axiomatic properties : strategy-proofness , non-bossiness , and category-wise neutrality . then , we propose a natural extension of serial dictatorships called categorial sequential allocation mechanisms ( csams ) , which allocate the items in multiple rounds : in each round , the active agent chooses an item from a designated category . we fully characterize the worst-case rank efficiency of csams for optimistic and pessimistic agents , and provide a bound for strategic agents . we also conduct experiments to compare expected rank efficiency of various csams w.r.t . random generated data ."}
{"title": "phishing detection in ims using domain ontology and cba - an innovative rule generation approach", "abstract": "user ignorance towards the use of communication services like instant messengers , emails , websites , social networks etc . is becoming the biggest advantage for phishers . it is required to create technical awareness in users by educating them to create a phishing detection application which would generate phishing alerts for the user so that phishing messages are not ignored . the lack of basic security features to detect and prevent phishing has had a profound effect on the im clients , as they lose their faith in e-banking and e-commerce transactions , which will have a disastrous impact on the corporate and banking sectors and businesses which rely heavily on the internet . very little research contributions were available in for phishing detection in instant messengers . a context based , dynamic and intelligent phishing detection methodology in ims is proposed , to analyze and detect phishing in instant messages with relevance to domain ontology ( obie ) and utilizes the classification based on association ( cba ) for generating phishing rules and alerting the victims . a pds monitoring system algorithm is used to identify the phishing activity during exchange of messages in ims , with high ratio of precision and recall . the results have shown improvement by the increased percentage of precision and recall when compared to the existing methods ."}
{"title": "learning bayesian networks with restricted causal interactions", "abstract": "a major problem for the learning of bayesian networks ( bns ) is the exponential number of parameters needed for conditional probability tables . recent research reduces this complexity by modeling local structure in the probability tables . we examine the use of log-linear local models . while log-linear models in this context are not new ( whittaker , 1990 ; buntine , 1991 ; neal , 1992 ; heckerman and meek , 1997 ) , for structure learning they are generally subsumed under a naive bayes model . we describe an alternative interpretation , and use a minimum message length ( mml ) ( wallace , 1987 ) metric for structure learning of networks exhibiting causal independence , which we term first-order networks ( fons ) . we also investigate local model selection on a node-by-node basis ."}
{"title": "dynamic move tables and long branches with backtracking in computer chess", "abstract": "the idea of dynamic move chains has been described in a preceding paper [ 10 ] . re-using an earlier piece of search allows the tree to be forward-pruned , which is known to be dangerous , because it can potentially remove new information that would only be realised through a more exhaustive search process . the justification is the integrity in the position and small changes between positions make it more likely that an earlier result still applies . larger problems where exhaustive search is not possible would also like a method that can guess accurately . this paper has added to the forward-pruning technique by using 'move tables ' that can act in the same way as transposition tables , but for moves not positions . they use an efficient memory structure and have put the design into the context of short or long-term memories . the long-term memory includes simply rote-learning of other players ' games . the forward-pruning technique can also be fortified to help to remove some potential errors . another idea is 'long branches ' . this plays a short move sequence , before returning to a full search at the resulting leaf nodes . therefore , with some configuration the dynamic tables can be reliably used and relatively independently of the position . this has advanced some of the future work theory of the earlier paper , and made more explicit where logical plans and more knowledge-based approaches might be applied . the author would argue that the process is a very human approach to searching for chess moves ."}
{"title": "reading comprehension using entity-based memory network", "abstract": "this paper introduces a novel neural network model for question answering , the \\emph { entity-based memory network } . it enhances neural networks ' ability of representing and calculating information over a long period by keeping records of entities contained in text . the core component is a memory pool which comprises entities ' states . these entities ' states are continuously updated according to the input text . questions with regard to the input text are used to search the memory pool for related entities and answers are further predicted based on the states of retrieved entities . compared with previous memory network models , the proposed model is capable of handling fine-grained information and more sophisticated relations based on entities . we formulated several different tasks as question answering problems and tested the proposed model . experiments reported satisfying results ."}
{"title": "identifying player\u015b strategies in no limit texas hold\u00e9m poker through the analysis of individual moves", "abstract": "the development of competitive artificial poker playing agents has proven to be a challenge , because agents must deal with unreliable information and deception which make it essential to model the opponents in order to achieve good results . this paper presents a methodology to develop opponent modeling techniques for poker agents . the approach is based on applying clustering algorithms to a poker game database in order to identify player types based on their actions . first , common game moves were identified by clustering all players\\ ' moves . then , player types were defined by calculating the frequency with which the players perform each type of movement . with the given dataset , 7 different types of players were identified with each one having at least one tactic that characterizes him . the identification of player types may improve the overall performance of poker agents , because it helps the agents to predict the opponent\\ 's moves , by associating each opponent to a distinct cluster ."}
{"title": "information compression by multiple alignment , unification and search as a unifying principle in computing and cognition", "abstract": "this article presents an overview of the idea that `` information compression by multiple alignment , unification and search '' ( icmaus ) may serve as a unifying principle in computing ( including mathematics and logic ) and in such aspects of human cognition as the analysis and production of natural language , fuzzy pattern recognition and best-match information retrieval , concept hierarchies with inheritance of attributes , probabilistic reasoning , and unsupervised inductive learning . the icmaus concepts are described together with an outline of the sp61 software model in which the icmaus concepts are currently realised . a range of examples is presented , illustrated with output from the sp61 model ."}
{"title": "local subspace-based outlier detection using global neighbourhoods", "abstract": "outlier detection in high-dimensional data is a challenging yet important task , as it has applications in , e.g. , fraud detection and quality control . state-of-the-art density-based algorithms perform well because they 1 ) take the local neighbourhoods of data points into account and 2 ) consider feature subspaces . in highly complex and high-dimensional data , however , existing methods are likely to overlook important outliers because they do not explicitly take into account that the data is often a mixture distribution of multiple components . we therefore introduce gloss , an algorithm that performs local subspace outlier detection using global neighbourhoods . experiments on synthetic data demonstrate that gloss more accurately detects local outliers in mixed data than its competitors . moreover , experiments on real-world data show that our approach identifies relevant outliers overlooked by existing methods , confirming that one should keep an eye on the global perspective even when doing local outlier detection ."}
{"title": "integrating existing cone-shaped and projection-based cardinal direction relations and a tcsp-like decidable generalisation", "abstract": "we consider the integration of existing cone-shaped and projection-based calculi of cardinal direction relations , well-known in qsr . the more general , integrating language we consider is based on convex constraints of the qualitative form $ r ( x , y ) $ , $ r $ being a cone-shaped or projection-based cardinal direction atomic relation , or of the quantitative form $ ( \\alpha , \\beta ) ( x , y ) $ , with $ \\alpha , \\beta\\in [ 0,2\\pi ) $ and $ ( \\beta -\\alpha ) \\in [ 0 , \\pi ] $ : the meaning of the quantitative constraint , in particular , is that point $ x $ belongs to the ( convex ) cone-shaped area rooted at $ y $ , and bounded by angles $ \\alpha $ and $ \\beta $ . the general form of a constraint is a disjunction of the form $ [ r_1\\vee ... \\vee r_ { n_1 } \\vee ( \\alpha_1 , \\beta_1 ) \\vee ... \\vee ( \\alpha _ { n_2 } , \\beta_ { n_2 } ) ] ( x , y ) $ , with $ r_i ( x , y ) $ , $ i=1 ... n_1 $ , and $ ( \\alpha _i , \\beta_i ) ( x , y ) $ , $ i=1 ... n_2 $ , being convex constraints as described above : the meaning of such a general constraint is that , for some $ i=1 ... n_1 $ , $ r_i ( x , y ) $ holds , or , for some $ i=1 ... n_2 $ , $ ( \\alpha_i , \\beta_i ) ( x , y ) $ holds . a conjunction of such general constraints is a $ \\tcsp $ -like csp , which we will refer to as an $ \\scsp $ ( spatial constraint satisfaction problem ) . an effective solution search algorithm for an $ \\scsp $ will be described , which uses ( 1 ) constraint propagation , based on a composition operation to be defined , as the filtering method during the search , and ( 2 ) the simplex algorithm , guaranteeing completeness , at the leaves of the search tree . the approach is particularly suited for large-scale high-level vision , such as , e.g. , satellite-like surveillance of a geographic area ."}
{"title": "map estimation via agreement on ( hyper ) trees : message-passing and linear programming", "abstract": "we develop and analyze methods for computing provably optimal { \\em maximum a posteriori } ( map ) configurations for a subclass of markov random fields defined on graphs with cycles . by decomposing the original distribution into a convex combination of tree-structured distributions , we obtain an upper bound on the optimal value of the original problem ( i.e. , the log probability of the map assignment ) in terms of the combined optimal values of the tree problems . we prove that this upper bound is tight if and only if all the tree distributions share an optimal configuration in common . an important implication is that any such shared configuration must also be a map configuration for the original distribution . next we develop two approaches to attempting to obtain tight upper bounds : ( a ) a { \\em tree-relaxed linear program } ( lp ) , which is derived from the lagrangian dual of the upper bounds ; and ( b ) a { \\em tree-reweighted max-product message-passing algorithm } that is related to but distinct from the max-product algorithm . in this way , we establish a connection between a certain lp relaxation of the mode-finding problem , and a reweighted form of the max-product ( min-sum ) message-passing algorithm ."}
{"title": "penta and hexa valued representation of neutrosophic information", "abstract": "starting from the primary representation of neutrosophic information , namely the degree of truth , degree of indeterminacy and degree of falsity , we define a nuanced representation in a penta valued fuzzy space , described by the index of truth , index of falsity , index of ignorance , index of contradiction and index of hesitation . also , it was constructed an associated penta valued logic and then using this logic , it was defined for the proposed penta valued structure the following operators : union , intersection , negation , complement and dual . then , the penta valued representation is extended to a hexa valued one , adding the sixth component , namely the index of ambiguity ."}
{"title": "a quantitative assessment of the effect of different algorithmic schemes to the task of learning the structure of bayesian networks", "abstract": "one of the most challenging tasks when adopting bayesian networks ( bns ) is the one of learning their structure from data . this task is complicated by the huge search space of possible solutions and turned out to be a well-known np-hard problem and , hence , approximations are required . however , to the best of our knowledge , a quantitative analysis of the performance and characteristics of the different heuristics to solve this problem has never been done before . for this reason , in this work , we provide a detailed study of the different state-of-the-arts methods for structural learning on simulated data considering both bns with discrete and continuous variables , and with different rates of noise in the data . in particular , we investigate the characteristics of different widespread scores proposed for the inference and the statistical pitfalls within them ."}
{"title": "integrating multiple sources to answer questions in algebraic topology", "abstract": "we present in this paper an evolution of a tool from a user interface for a concrete computer algebra system for algebraic topology ( the kenzo system ) , to a front-end allowing the interoperability among different sources for computation and deduction . the architecture allows the system not only to interface several systems , but also to make them cooperate in shared calculations ."}
{"title": "data-driven sequential monte carlo in probabilistic programming", "abstract": "most of markov chain monte carlo ( mcmc ) and sequential monte carlo ( smc ) algorithms in existing probabilistic programming systems suboptimally use only model priors as proposal distributions . in this work , we describe an approach for training a discriminative model , namely a neural network , in order to approximate the optimal proposal by using posterior estimates from previous runs of inference . we show an example that incorporates a data-driven proposal for use in a non-parametric model in the anglican probabilistic programming system . our results show that data-driven proposals can significantly improve inference performance so that considerably fewer particles are necessary to perform a good posterior estimation ."}
{"title": "an integrated optimization + learning approach to optimal dynamic pricing for the retailer with multi-type customers in smart grids", "abstract": "in this paper , we consider a realistic and meaningful scenario in the context of smart grids where an electricity retailer serves three different types of customers , i.e. , customers with an optimal home energy management system embedded in their smart meters ( c-hems ) , customers with only smart meters ( c-sm ) , and customers without smart meters ( c-none ) . the main objective of this paper is to support the retailer to make optimal day-ahead dynamic pricing decisions in such a mixed customer pool . to this end , we propose a two-level decision-making framework where the retailer acting as upper-level agent firstly announces its electricity prices of next 24 hours and customers acting as lower-level agents subsequently schedule their energy usages accordingly . for the lower level problem , we model the price responsiveness of different customers according to their unique characteristics . for the upper level problem , we optimize the dynamic prices for the retailer to maximize its profit subject to realistic market constraints . the above two-level model is tackled by genetic algorithms ( gas ) based distributed optimization methods while its feasibility and effectiveness are confirmed via simulation results ."}
{"title": "efficient approximation for triangulation of minimum treewidth", "abstract": "we present four novel approximation algorithms for finding triangulation of minimum treewidth . two of the algorithms improve on the running times of algorithms by robertson and seymour , and becker and geiger that approximate the optimum by factors of 4 and 3 2/3 , respectively . a third algorithm is faster than those but gives an approximation factor of 4 1/2 . the last algorithm is yet faster , producing factor-o ( lg/k ) approximations in polynomial time . finding triangulations of minimum treewidth for graphs is central to many problems in computer science . real-world problems in artificial intelligence , vlsi design and databases are efficiently solvable if we have an efficient approximation algorithm for them . we report on experimental results confirming the effectiveness of our algorithms for large graphs associated with real-world problems ."}
{"title": "`` dave ... i can assure you ... that it 's going to be all right ... '' -- a definition , case for , and survey of algorithmic assurances in human-autonomy trust relationships", "abstract": "as technology becomes more advanced , those who design , use and are otherwise affected by it want to know that it will perform correctly , and understand why it does what it does , and how to use it appropriately . in essence they want to be able to trust the systems that are being designed . in this survey we present assurances that are the method by which users can understand how to trust autonomous systems . trust between humans and autonomy is reviewed , and the implications for the design of assurances are highlighted . a survey of existing research related to assurances is presented . much of the surveyed research originates from fields such as interpretable , comprehensible , transparent , and explainable machine learning , as well as human-computer interaction , human-robot interaction , and e-commerce . several key ideas are extracted from this work in order to refine the definition of assurances . the design of assurances is found to be highly dependent not only on the capabilities of the autonomous system , but on the characteristics of the human user , and the appropriate trust-related behaviors . several directions for future research are identified and discussed ."}
{"title": "efficient planning under uncertainty with macro-actions", "abstract": "deciding how to act in partially observable environments remains an active area of research . identifying good sequences of decisions is particularly challenging when good control performance requires planning multiple steps into the future in domains with many states . towards addressing this challenge , we present an online , forward-search algorithm called the posterior belief distribution ( pbd ) . pbd leverages a novel method for calculating the posterior distribution over beliefs that result after a sequence of actions is taken , given the set of observation sequences that could be received during this process . this method allows us to efficiently evaluate the expected reward of a sequence of primitive actions , which we refer to as macro-actions . we present a formal analysis of our approach , and examine its performance on two very large simulation experiments : scientific exploration and a target monitoring domain . we also demonstrate our algorithm being used to control a real robotic helicopter in a target monitoring experiment , which suggests that our approach has practical potential for planning in real-world , large partially observable domains where a multi-step lookahead is required to achieve good performance ."}
{"title": "an event grouping based algorithm for university course timetabling problem", "abstract": "this paper presents the study of an event grouping based algorithm for a university course timetabling problem . several publications which discuss the problem and some approaches for its solution are analyzed . the grouping of events in groups with an equal number of events in each group is not applicable to all input data sets . for this reason , a universal approach to all possible groupings of events in commensurate in size groups is proposed here . also , an implementation of an algorithm based on this approach is presented . the methodology , conditions and the objectives of the experiment are described . the experimental results are analyzed and the ensuing conclusions are stated . the future guidelines for further research are formulated ."}
{"title": "belief propagation and beyond for particle tracking", "abstract": "we describe a novel approach to statistical learning from particles tracked while moving in a random environment . the problem consists in inferring properties of the environment from recorded snapshots . we consider here the case of a fluid seeded with identical passive particles that diffuse and are advected by a flow . our approach rests on efficient algorithms to estimate the weighted number of possible matchings among particles in two consecutive snapshots , the partition function of the underlying graphical model . the partition function is then maximized over the model parameters , namely diffusivity and velocity gradient . a belief propagation ( bp ) scheme is the backbone of our algorithm , providing accurate results for the flow parameters we want to learn . the bp estimate is additionally improved by incorporating loop series ( ls ) contributions . for the weighted matching problem , ls is compactly expressed as a cauchy integral , accurately estimated by a saddle point approximation . numerical experiments show that the quality of our improved bp algorithm is comparable to the one of a fully polynomial randomized approximation scheme , based on the markov chain monte carlo ( mcmc ) method , while the bp-based scheme is substantially faster than the mcmc scheme ."}
{"title": "asynchronous partial overlay : a new algorithm for solving distributed constraint satisfaction problems", "abstract": "distributed constraint satisfaction ( dcsp ) has long been considered an important problem in multi-agent systems research . this is because many real-world problems can be represented as constraint satisfaction and these problems often present themselves in a distributed form . in this article , we present a new complete , distributed algorithm called asynchronous partial overlay ( apo ) for solving dcsps that is based on a cooperative mediation process . the primary ideas behind this algorithm are that agents , when acting as a mediator , centralize small , relevant portions of the dcsp , that these centralized subproblems overlap , and that agents increase the size of their subproblems along critical paths within the dcsp as the problem solving unfolds . we present empirical evidence that shows that apo outperforms other known , complete dcsp techniques ."}
{"title": "cognitive discriminative mappings for rapid learning", "abstract": "humans can learn concepts or recognize items from just a handful of examples , while machines require many more samples to perform the same task . in this paper , we build a computational model to investigate the possibility of this kind of rapid learning . the proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory . we present a simple and intuitive technique called cognitive discriminative mappings ( cdm ) to explore the cognitive problem . first , cdm separates and clusters the data instances retrieved from long-term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm . cdm then maps each sensory data instance to be as close as possible to the median point of the data group with the same class . the experimental results demonstrate that the cdm approach is effective for learning the discriminative features of supervised classifications with few training sensory input instances ."}
{"title": "parameterized complexity of problems in coalitional resource games", "abstract": "coalition formation is a key topic in multi-agent systems . coalitions enable agents to achieve goals that they may not have been able to achieve on their own . previous work has shown problems in coalitional games to be computationally hard . wooldridge and dunne ( artificial intelligence 2006 ) studied the classical computational complexity of several natural decision problems in coalitional resource games ( crg ) - games in which each agent is endowed with a set of resources and coalitions can bring about a set of goals if they are collectively endowed with the necessary amount of resources . the input of coalitional resource games bundles together several elements , e.g. , the agent set ag , the goal set g , the resource set r , etc . shrot , aumann and kraus ( aamas 2009 ) examine coalition formation problems in the crg model using the theory of parameterized complexity . their refined analysis shows that not all parts of input act equal - some instances of the problem are indeed tractable while others still remain intractable . we answer an important question left open by shrot , aumann and kraus by showing that the sc problem ( checking whether a coalition is successful ) is w [ 1 ] -hard when parameterized by the size of the coalition . then via a single theme of reduction from sc , we are able to show that various problems related to resources , resource bounds and resource conflicts introduced by wooldridge et al are 1. w [ 1 ] -hard or co-w [ 1 ] -hard when parameterized by the size of the coalition . 2. para-np-hard or co-para-np-hard when parameterized by |r| . 3. fpt when parameterized by either |g| or |ag|+|r| ."}
{"title": "machine learned learning machines", "abstract": "there are two common approaches for optimizing the performance of a machine : genetic algorithms and machine learning . a genetic algorithm is applied over many generations whereas machine learning works by applying feedback until the system meets a performance threshold . though these are methods that typically operate separately , we combine evolutionary adaptation and machine learning into one approach . our focus is on machines that can learn during their lifetime , but instead of equipping them with a machine learning algorithm we aim to let them evolve their ability to learn by themselves . we use evolvable networks of probabilistic and deterministic logic gates , known as markov brains , as our computational model organism . the ability of markov brains to learn is augmented by a novel adaptive component that can change its computational behavior based on feedback . we show that markov brains can indeed evolve to incorporate these feedback gates to improve their adaptability to variable environments . by combining these two methods , we now also implemented a computational model that can be used to study the evolution of learning ."}
{"title": "policy recognition in the abstract hidden markov model", "abstract": "in this paper , we present a method for recognising an agent 's behaviour in dynamic , noisy , uncertain domains , and across multiple levels of abstraction . we term this problem on-line plan recognition under uncertainty and view it generally as probabilistic inference on the stochastic process representing the execution of the agent 's plan . our contributions in this paper are twofold . in terms of probabilistic inference , we introduce the abstract hidden markov model ( ahmm ) , a novel type of stochastic processes , provide its dynamic bayesian network ( dbn ) structure and analyse the properties of this network . we then describe an application of the rao-blackwellised particle filter to the ahmm which allows us to construct an efficient , hybrid inference method for this model . in terms of plan recognition , we propose a novel plan recognition framework based on the ahmm as the plan execution model . the rao-blackwellised hybrid inference for ahmm can take advantage of the independence properties inherent in a model of plan execution , leading to an algorithm for online probabilistic plan recognition that scales well with the number of levels in the plan hierarchy . this illustrates that while stochastic models for plan execution can be complex , they exhibit special structures which , if exploited , can lead to efficient plan recognition algorithms . we demonstrate the usefulness of the ahmm framework via a behaviour recognition system in a complex spatial environment using distributed video surveillance data ."}
{"title": "reduction of computational complexity in bayesian networks through removal of weak dependencies", "abstract": "the paper presents a method for reducing the computational complexity of bayesian networks through identification and removal of weak dependencies ( removal of links from the ( moralized ) independence graph ) . the removal of a small number of links may reduce the computational complexity dramatically , since several fill-ins and moral links may be rendered superfluous by the removal . the method is described in terms of impact on the independence graph , the junction tree , and the potential functions associated with these . an empirical evaluation of the method using large real-world networks demonstrates the applicability of the method . further , the method , which has been implemented in hugin , complements the approximation method suggested by jensen & andersen ( 1990 ) ."}
{"title": "similarit\u00e9 en intension vs en extension : \u00e0 la crois\u00e9e de l'informatique et du th\u00e9\u00e2tre", "abstract": "traditional staging is based on a formal approach of similarity leaning on dramaturgical ontologies and instanciation variations . inspired by interactive data mining , that suggests different approaches , we give an overview of computer science and theater researches using computers as partners of the actor to escape the a priori specification of roles ."}
{"title": "combining symmetry breaking and global constraints", "abstract": "we propose a new family of constraints which combine together lexicographical ordering constraints for symmetry breaking with other common global constraints . we give a general purpose propagator for this family of constraints , and show how to improve its complexity by exploiting properties of the included global constraints ."}
{"title": "click carving : segmenting objects in video with point clicks", "abstract": "we present a novel form of interactive video object segmentation where a few clicks by the user helps the system produce a full spatio-temporal segmentation of the object of interest . whereas conventional interactive pipelines take the user 's initialization as a starting point , we show the value in the system taking the lead even in initialization . in particular , for a given video frame , the system precomputes a ranked list of thousands of possible segmentation hypotheses ( also referred to as object region proposals ) using image and motion cues . then , the user looks at the top ranked proposals , and clicks on the object boundary to carve away erroneous ones . this process iterates ( typically 2-3 times ) , and each time the system revises the top ranked proposal set , until the user is satisfied with a resulting segmentation mask . finally , the mask is propagated across the video to produce a spatio-temporal object tube . on three challenging datasets , we provide extensive comparisons with both existing work and simpler alternative methods . in all , the proposed click carving approach strikes an excellent balance of accuracy and human effort . it outperforms all similarly fast methods , and is competitive or better than those requiring 2 to 12 times the effort ."}
{"title": "model-based test generation for robotic software : automata versus belief-desire-intention agents", "abstract": "robotic code needs to be verified to ensure its safety and functional correctness , especially when the robot is interacting with people . testing real code in simulation is a viable option . however , generating tests that cover rare scenarios , as well as exercising most of the code , is a challenge amplified by the complexity of the interactions between the environment and the software . model-based test generation methods can automate otherwise manual processes and facilitate reaching rare scenarios during testing . in this paper , we compare using belief-desire-intention ( bdi ) agents as models for test generation with more conventional automata-based techniques that exploit model checking , in terms of practicality , performance , transferability to different scenarios , and exploration ( ` coverage ' ) , through two case studies : a cooperative manufacturing task , and a home care scenario . the results highlight the advantages of using bdi agents for test generation . bdi agents naturally emulate the agency present in human-robot interactions ( hris ) , and are thus more expressive than automata . the performance of the bdi-based test generation is at least as high , and the achieved coverage is higher or equivalent , compared to test generation based on model checking automata ."}
{"title": "semantic similarity in a taxonomy : an information-based measure and its application to problems of ambiguity in natural language", "abstract": "this article presents a measure of semantic similarity in an is-a taxonomy based on the notion of shared information content . experimental evaluation against a benchmark set of human similarity judgments demonstrates that the measure performs better than the traditional edge-counting approach . the article presents algorithms that take advantage of taxonomic similarity in resolving syntactic and semantic ambiguity , along with experimental results demonstrating their effectiveness ."}
{"title": "a study on artificial intelligence iq and standard intelligent model", "abstract": "currently , potential threats of artificial intelligence ( ai ) to human have triggered a large controversy in society , behind which , the nature of the issue is whether the artificial intelligence ( ai ) system can be evaluated quantitatively . this article analyzes and evaluates the challenges that the ai development level is facing , and proposes that the evaluation methods for the human intelligence test and the ai system are not uniform ; and the key reason for which is that none of the models can uniformly describe the ai system and the beings like human . aiming at this problem , a standard intelligent system model is established in this study to describe the ai system and the beings like human uniformly . based on the model , the article makes an abstract mathematical description , and builds the standard intelligent machine mathematical model ; expands the von neumann architecture and proposes the liufeng - shiyong architecture ; gives the definition of the artificial intelligence iq , and establishes the artificial intelligence scale and the evaluation method ; conduct the test on 50 search engines and three human subjects at different ages across the world , and finally obtains the ranking of the absolute iq and deviation iq ranking for artificial intelligence iq 2014 ."}
{"title": "pac-learning recursive logic programs : negative results", "abstract": "in a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable . in this paper we present negative results showing that any natural generalization of this class is hard to learn in valiant 's model of pac-learnability . in particular , we show that the following program classes are cryptographically hard to learn : programs with an unbounded number of constant-depth linear recursive clauses ; programs with one constant-depth determinate clause containing an unbounded number of recursive calls ; and programs with one linear recursive clause of constant locality . these results immediately imply the non-learnability of any more general class of programs . we also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean dnf . together with positive results from the companion paper , these negative results establish a boundary of efficient learnability for recursive function-free clauses ."}
{"title": "the good , the bad , and the odd : cycles in answer-set programs", "abstract": "backdoors of answer-set programs are sets of atoms that represent clever reasoning shortcuts through the search space . assignments to backdoor atoms reduce the given program to several programs that belong to a tractable target class . previous research has considered target classes based on notions of acyclicity where various types of cycles ( good and bad cycles ) are excluded from graph representations of programs . we generalize the target classes by taking the parity of the number of negative edges on bad cycles into account and consider backdoors for such classes . we establish new hardness results and non-uniform polynomial-time tractability relative to directed or undirected cycles ."}
{"title": "multi-goal reinforcement learning : challenging robotics environments and request for research", "abstract": "the purpose of this technical report is two-fold . first of all , it introduces a suite of challenging continuous control tasks ( integrated with openai gym ) based on currently existing robotics hardware . the tasks include pushing , sliding and pick & place with a fetch robotic arm as well as in-hand object manipulation with a shadow dexterous hand . all tasks have sparse binary rewards and follow a multi-goal reinforcement learning ( rl ) framework in which an agent is told what to do using an additional input . the second part of the paper presents a set of concrete research ideas for improving rl algorithms , most of which are related to multi-goal rl and hindsight experience replay ."}
{"title": "extensible knowledge representation : the case of description reasoners", "abstract": "this paper offers an approach to extensible knowledge representation and reasoning for a family of formalisms known as description logics . the approach is based on the notion of adding new concept constructors , and includes a heuristic methodology for specifying the desired extensions , as well as a modularized software architecture that supports implementing extensions . the architecture detailed here falls in the normalize-compared paradigm , and supports both intentional reasoning ( subsumption ) involving concepts , and extensional reasoning involving individuals after incremental updates to the knowledge base . the resulting approach can be used to extend the reasoner with specialized notions that are motivated by specific problems or application areas , such as reasoning about dates , plans , etc . in addition , it provides an opportunity to implement constructors that are not currently yet sufficiently well understood theoretically , but are needed in practice . also , for constructors that are provably hard to reason with ( e.g. , ones whose presence would lead to undecidability ) , it allows the implementation of incomplete reasoners where the incompleteness is tailored to be acceptable for the application at hand ."}
{"title": "self-organizing neural networks in classification and image recognition", "abstract": "self-organizing neural networks are used for brick finding in opera experiment . self-organizing neural networks and wavelet analysis used for recognition and extraction of car numbers from images ."}
{"title": "a new method for knowledge representation in expert system 's ( xmlkr )", "abstract": "knowledge representation it is an essential section of a expert systems , because in this section we have a framework to establish an expert system then we can modeling and use by this to design an expert system . many method it is exist for knowledge representation but each method have problems , in this paper we introduce a new method of object oriented by xml language as xmlkr to knowledge representation , and we want to discuss advantage and disadvantage of this method ."}
{"title": "a uniform framework for concept definitions in description logics", "abstract": "most modern formalisms used in databases and artificial intelligence for describing an application domain are based on the notions of class ( or concept ) and relationship among classes . one interesting feature of such formalisms is the possibility of defining a class , i.e. , providing a set of properties that precisely characterize the instances of the class . many recent articles point out that there are several ways of assigning a meaning to a class definition containing some sort of recursion . in this paper , we argue that , instead of choosing a single style of semantics , we achieve better results by adopting a formalism that allows for different semantics to coexist . we demonstrate the feasibility of our argument , by presenting a knowledge representation formalism , the description logic mualcq , with the above characteristics . in addition to the constructs for conjunction , disjunction , negation , quantifiers , and qualified number restrictions , mualcq includes special fixpoint constructs to express ( suitably interpreted ) recursive definitions . these constructs enable the usual frame-based descriptions to be combined with definitions of recursive data structures such as directed acyclic graphs , lists , streams , etc . we establish several properties of mualcq , including the decidability and the computational complexity of reasoning , by formulating a correspondence with a particular modal logic of programs called the modal mu-calculus ."}
{"title": "a parallel and efficient algorithm for learning to match", "abstract": "many tasks in data mining and related fields can be formalized as matching between objects in two heterogeneous domains , including collaborative filtering , link prediction , image tagging , and web search . machine learning techniques , referred to as learning-to-match in this paper , have been successfully applied to the problems . among them , a class of state-of-the-art methods , named feature-based matrix factorization , formalize the task as an extension to matrix factorization by incorporating auxiliary features into the model . unfortunately , making those algorithms scale to real world problems is challenging , and simple parallelization strategies fail due to the complex cross talking patterns between sub-tasks . in this paper , we tackle this challenge with a novel parallel and efficient algorithm for feature-based matrix factorization . our algorithm , based on coordinate descent , can easily handle hundreds of millions of instances and features on a single machine . the key recipe of this algorithm is an iterative relaxation of the objective to facilitate parallel updates of parameters , with guaranteed convergence on minimizing the original objective function . experimental results demonstrate that the proposed method is effective on a wide range of matching problems , with efficiency significantly improved upon the baselines while accuracy retained unchanged ."}
{"title": "a primer on answer set programming", "abstract": "a introduction to the syntax and semantics of answer set programming intended as an handout to [ under ] graduate students taking artificial intlligence or logic programming classes ."}
{"title": "long-term memory networks for question answering", "abstract": "question answering is an important and difficult task in the natural language processing domain , because many basic natural language processing tasks can be cast into a question answering task . several deep neural network architectures have been developed recently , which employ memory and inference components to memorize and reason over text information , and generate answers to questions . however , a major drawback of many such models is that they are capable of only generating single-word answers . in addition , they require large amount of training data to generate accurate answers . in this paper , we introduce the long-term memory network ( ltmn ) , which incorporates both an external memory module and a long short-term memory ( lstm ) module to comprehend the input data and generate multi-word answers . the ltmn model can be trained end-to-end using back-propagation and requires minimal supervision . we test our model on two synthetic data sets ( based on facebook 's babi data set ) and the real-world stanford question answering data set , and show that it can achieve state-of-the-art performance ."}
{"title": "cocopf : an algorithm portfolio framework", "abstract": "algorithm portfolios represent a strategy of composing multiple heuristic algorithms , each suited to a different class of problems , within a single general solver that will choose the best suited algorithm for each input . this approach recently gained popularity especially for solving combinatoric problems , but optimization applications are still emerging . the coco platform of the bbob workshop series is the current standard way to measure performance of continuous black-box optimization algorithms . as an extension to the coco platform , we present the python-based cocopf framework that allows composing portfolios of optimization algorithms and running experiments with different selection strategies . in our framework , we focus on black-box algorithm portfolio and online adaptive selection . as a demonstration , we measure the performance of stock scipy optimization algorithms and the popular cma algorithm alone and in a portfolio with two simple selection strategies . we confirm that even a naive selection strategy can provide improved performance across problem classes ."}
{"title": "unfounded sets and well-founded semantics of answer set programs with aggregates", "abstract": "logic programs with aggregates ( lpa ) are one of the major linguistic extensions to logic programming ( lp ) . in this work , we propose a generalization of the notions of unfounded set and well-founded semantics for programs with monotone and antimonotone aggregates ( lpama programs ) . in particular , we present a new notion of unfounded set for lpama programs , which is a sound generalization of the original definition for standard ( aggregate-free ) lp . on this basis , we define a well-founded operator for lpama programs , the fixpoint of which is called well-founded model ( or well-founded semantics ) for lpama programs . the most important properties of unfounded sets and the well-founded semantics for standard lp are retained by this generalization , notably existence and uniqueness of the well-founded model , together with a strong relationship to the answer set semantics for lpama programs . we show that one of the d-well-founded semantics , defined by pelov , denecker , and bruynooghe for a broader class of aggregates using approximating operators , coincides with the well-founded model as defined in this work on lpama programs . we also discuss some complexity issues , most importantly we give a formal proof of tractable computation of the well-founded model for lpa programs . moreover , we prove that for general lpa programs , which may contain aggregates that are neither monotone nor antimonotone , deciding satisfaction of aggregate expressions with respect to partial interpretations is conp-complete . as a consequence , a well-founded semantics for general lpa programs that allows for tractable computation is unlikely to exist , which justifies the restriction on lpama programs . finally , we present a prototype system extending dlv , which supports the well-founded semantics for lpama programs , at the time of writing the only implemented system that does so . experiments with this prototype show significant computational advantages of aggregate constructs over equivalent aggregate-free encodings ."}
{"title": "first-improvement vs. best-improvement local optima networks of nk landscapes", "abstract": "this paper extends a recently proposed model for combinatorial landscapes : local optima networks ( lon ) , to incorporate a first-improvement ( greedy-ascent ) hill-climbing algorithm , instead of a best-improvement ( steepest-ascent ) one , for the definition and extraction of the basins of attraction of the landscape optima . a statistical analysis comparing best and first improvement network models for a set of nk landscapes , is presented and discussed . our results suggest structural differences between the two models with respect to both the network connectivity , and the nature of the basins of attraction . the impact of these differences in the behavior of search heuristics based on first and best improvement local search is thoroughly discussed ."}
{"title": "determining the consistency factor of autopilot using rough set theory", "abstract": "autopilot is a system designed to guide a vehicle without aid . due to increase in flight hours and complexity of modern day flight it has become imperative to equip the aircrafts with autopilot . thus reliability and consistency of an autopilot system becomes a crucial role in a flight . but the increased complexity and demand for better accuracy has made the process of evaluating the autopilot for consistency a difficult process .a vast amount of imprecise data has been involved . rough sets can be a potent tool for such kind of applications containing vague data . this paper proposes an approach towards consistency factor determination using rough set theory . the seventeen basic factors , that are crucial in determining the consistency of an autopilot system , are grouped into five payloads based on their functionality . consistency factor is evaluated through these payloads , using rough set theory . consistency factor determines the consistency and reliability of an autopilot system and the conditions under which manual override becomes imperative . using rough set theory the most and the least influential factors towards autopilot system are also determined ."}
{"title": "the new approach on fuzzy decision trees", "abstract": "decision trees have been widely used in machine learning . however , due to some reasons , data collecting in real world contains a fuzzy and uncertain form . the decision tree should be able to handle such fuzzy data . this paper presents a method to construct fuzzy decision tree . it proposes a fuzzy decision tree induction method in iris flower data set , obtaining the entropy from the distance between an average value and a particular value . it also presents an experiment result that shows the accuracy compared to former id3 ."}
{"title": "playerank : multi-dimensional and role-aware rating of soccer player performance", "abstract": "the problem of rating the performance of soccer players is attracting the interest of many companies , websites , and the scientific community , thanks to the availability of massive data capturing all the events generated during a game ( e.g. , tackles , passes , shots , etc. ) . existing approaches fail to fully exploit the richness of the available data and lack of a proper validation . in this paper , we design and implement playerank , a data-driven framework that offers a principled multi-dimensional and role-aware evaluation of the performance of soccer players . we validate the framework through an experimental analysis advised by soccer experts , based on a massive dataset of millions of events pertaining four seasons of the five prominent european leagues . experiments show that playerank is robust in agreeing with the experts ' evaluation of players , significantly improving the state of the art . we also explore an application of playerank -- - i.e . searching players -- - by introducing a special form of spatial query on the soccer field . this shows its flexibility and efficiency , which makes it worth to be used in the design of a scalable platform for soccer analytics ."}
{"title": "rl $ ^2 $ : fast reinforcement learning via slow reinforcement learning", "abstract": "deep reinforcement learning ( deep rl ) has been successful in learning sophisticated behaviors automatically ; however , the learning process requires a huge number of trials . in contrast , animals can learn new tasks in just a few trials , benefiting from their prior knowledge about the world . this paper seeks to bridge this gap . rather than designing a `` fast '' reinforcement learning algorithm , we propose to represent it as a recurrent neural network ( rnn ) and learn it from data . in our proposed method , rl $ ^2 $ , the algorithm is encoded in the weights of the rnn , which are learned slowly through a general-purpose ( `` slow '' ) rl algorithm . the rnn receives all information a typical rl algorithm would receive , including observations , actions , rewards , and termination flags ; and it retains its state across episodes in a given markov decision process ( mdp ) . the activations of the rnn store the state of the `` fast '' rl algorithm on the current ( previously unseen ) mdp . we evaluate rl $ ^2 $ experimentally on both small-scale and large-scale problems . on the small-scale side , we train it to solve randomly generated multi-arm bandit problems and finite mdps . after rl $ ^2 $ is trained , its performance on new mdps is close to human-designed algorithms with optimality guarantees . on the large-scale side , we test rl $ ^2 $ on a vision-based navigation task and show that it scales up to high-dimensional problems ."}
{"title": "i probe , therefore i am : designing a virtual journalist with human emotions", "abstract": "by utilizing different communication channels , such as verbal language , gestures or facial expressions , virtually embodied interactive humans hold a unique potential to bridge the gap between human-computer interaction and actual interhuman communication . the use of virtual humans is consequently becoming increasingly popular in a wide range of areas where such a natural communication might be beneficial , including entertainment , education , mental health research and beyond . behind this development lies a series of technological advances in a multitude of disciplines , most notably natural language processing , computer vision , and speech synthesis . in this paper we discuss a virtual human journalist , a project employing a number of novel solutions from these disciplines with the goal to demonstrate their viability by producing a humanoid conversational agent capable of naturally eliciting and reacting to information from a human user . a set of qualitative and quantitative evaluation sessions demonstrated the technical feasibility of the system whilst uncovering a number of deficits in its capacity to engage users in a way that would be perceived as natural and emotionally engaging . we argue that naturalness should not always be seen as a desirable goal and suggest that deliberately suppressing the naturalness of virtual human interactions , such as by altering its personality cues , might in some cases yield more desirable results ."}
{"title": "exploiting generalization in the subspaces for faster model-based learning", "abstract": "due to the lack of enough generalization in the state-space , common methods in reinforcement learning ( rl ) suffer from slow learning speed especially in the early learning trials . this paper introduces a model-based method in discrete state-spaces for increasing learning speed in terms of required experience ( but not required computational time ) by exploiting generalization in the experiences of the subspaces . a subspace is formed by choosing a subset of features in the original state representation ( full-space ) . generalization and faster learning in a subspace are due to many-to-one mapping of experiences from the full-space to each state in the subspace . nevertheless , due to inherent perceptual aliasing in the subspaces , the policy suggested by each subspace does not generally converge to the optimal policy . our approach , called model based learning with subspaces ( mobles ) , calculates confidence intervals of the estimated q-values in the full-space and in the subspaces . these confidence intervals are used in the decision making , such that the agent benefits the most from the possible generalization while avoiding from detriment of the perceptual aliasing in the subspaces . convergence of mobles to the optimal policy is theoretically investigated . additionally , we show through several experiments that mobles improves the learning speed in the early trials ."}
{"title": "towards resolving unidentifiability in inverse reinforcement learning", "abstract": "we consider a setting for inverse reinforcement learning ( irl ) where the learner is extended with the ability to actively select multiple environments , observing an agent 's behavior on each environment . we first demonstrate that if the learner can experiment with any transition dynamics on some fixed set of states and actions , then there exists an algorithm that reconstructs the agent 's reward function to the fullest extent theoretically possible , and that requires only a small ( logarithmic ) number of experiments . we contrast this result to what is known about irl in single fixed environments , namely that the true reward function is fundamentally unidentifiable . we then extend this setting to the more realistic case where the learner may not select any transition dynamic , but rather is restricted to some fixed set of environments that it may try . we connect the problem of maximizing the information derived from experiments to submodular function maximization and demonstrate that a greedy algorithm is near optimal ( up to logarithmic factors ) . finally , we empirically validate our algorithm on an environment inspired by behavioral psychology ."}
{"title": "an integrated and scalable platform for proactive event-driven traffic management", "abstract": "traffic on freeways can be managed by means of ramp meters from road traffic control rooms . human operators can not efficiently manage a network of ramp meters . to support them , we present an intelligent platform for traffic management which includes a new ramp metering coordination scheme in the decision making module , an efficient dashboard for interacting with human operators , machine learning tools for learning event definitions and complex event processing tools able to deal with uncertainties inherent to the traffic use case . unlike the usual approach , the devised event-driven platform is able to predict a congestion up to 4 minutes before it really happens . proactive decision making can then be established leading to significant improvement of traffic conditions ."}
{"title": "supply restoration in power distribution systems - a case study in integrating model-based diagnosis and repair planning", "abstract": "integrating diagnosis and repair is particularly crucial when gaining sufficient information to discriminate between several candidate diagnoses requires carrying out some repair actions . a typical case is supply restoration in a faulty power distribution system . this problem , which is a major concern for electricity distributors , features partial observability , and stochastic repair actions which are more elaborate than simple replacement of components . this paper analyses the difficulties in applying existing work on integrating model-based diagnosis and repair and on planning in partially observable stochastic domains to this real-world problem , and describes the pragmatic approach we have retained so far ."}
{"title": "using synchronous boolean networks to model several phenomena of collective behavior", "abstract": "in this paper , we propose an approach for modeling and analysis of a number of phenomena of collective behavior . by collectives we mean multi-agent systems that transition from one state to another at discrete moments of time . the behavior of a member of a collective ( agent ) is called conforming if the opinion of this agent at current time moment conforms to the opinion of some other agents at the previous time moment . we presume that at each moment of time every agent makes a decision by choosing from the set { 0,1 } ( where 1-decision corresponds to action and 0-decision corresponds to inaction ) . in our approach we model collective behavior with synchronous boolean networks . we presume that in a network there can be agents that act at every moment of time . such agents are called instigators . also there can be agents that never act . such agents are called loyalists . agents that are neither instigators nor loyalists are called simple agents . we study two combinatorial problems . the first problem is to find a disposition of instigators that in several time moments transforms a network from a state where a majority of simple agents are inactive to a state with a majority of active agents . the second problem is to find a disposition of loyalists that returns the network to a state with a majority of inactive agents . similar problems are studied for networks in which simple agents demonstrate the contrary to conforming behavior that we call anticonforming . we obtained several theoretical results regarding the behavior of collectives of agents with conforming or anticonforming behavior . in computational experiments we solved the described problems for randomly generated networks with several hundred vertices . we reduced corresponding combinatorial problems to the boolean satisfiability problem ( sat ) and used modern sat solvers to solve the instances obtained ."}
{"title": "learning to act by predicting the future", "abstract": "we present an approach to sensorimotor control in immersive environments . our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream . the cotemporal structure of these streams provides a rich supervisory signal , which enables training a sensorimotor control model by interacting with the environment . the model is trained using supervised learning techniques , but without extraneous supervision . it learns to act based on raw sensory input from a complex three-dimensional environment . the presented formulation enables learning without a fixed goal at training time , and pursuing dynamically changing goals at test time . we conduct extensive experiments in three-dimensional simulations based on the classical first-person game doom . the results demonstrate that the presented approach outperforms sophisticated prior formulations , particularly on challenging tasks . the results also show that trained models successfully generalize across environments and goals . a model trained using the presented approach won the full deathmatch track of the visual doom ai competition , which was held in previously unseen environments ."}
{"title": "exploring strategy-proofness , uniqueness , and pareto optimality for the stable matching problem with couples", "abstract": "the stable matching problem with couples ( smp-c ) is a ubiquitous real-world extension of the stable matching problem ( smp ) involving complementarities . although smp can be solved in polynomial time , smp-c is np-complete . hence , it is not clear which , if any , of the theoretical results surrounding the canonical smp problem apply in this setting . in this paper , we use a recently-developed sat encoding to solve smp-c exactly . this allows us to enumerate all stable matchings for any given instance of smp-c. with this tool , we empirically evaluate some of the properties that have been hypothesized to hold for smp-c. we take particular interest in investigating if , as the size of the market grows , the percentage of instances with unique stable matchings also grows . while we did not find this trend among the random problem instances we sampled , we did find that the percentage of instances with an resident optimal matching seems to more closely follow the trends predicted by previous conjectures . we also define and investigate resident pareto optimal stable matchings , finding that , even though this is important desideratum for the deferred acceptance style algorithms previously designed to solve smp-c , they do not always find one . we also investigate strategy-proofness for smp-c , showing that even if only one stable matching exists , residents still have incentive to misreport their preferences . however , if a problem has a resident optimal stable matching , we show that residents can not manipulate via truncation ."}
{"title": "self-organizing maps and generalization : an algorithmic description of numerosity and variability effects", "abstract": "category , or property generalization is a central function in the human cognition . it plays a crucial role in a variety of domains , such as learning , everyday reasoning , specialized reasoning , and decision making . judging the content of a dish as edible , a hormone level as healthy , a building as belonging to the same architectural style as previously seen buildings , are examples of category generalization . in this paper , we propose self-organizing maps as candidates to explain the psychological mechanisms underlying category generalization . self-organizing maps are psychologically and biologically plausible neural network models that learn after limited exposure to positive category examples , without any need of contrastive information . just like humans . they reproduce human behavior in category generalization , in particular for what concerns the well-known numerosity and variability effects , which are usually explained with bayesian tools . where category generalization is concerned , self-organizing maps are good candidates to bridge the gap between the computational level of analysis in marr 's hierarchy ( where bayesian models are situated ) and the algorithmic level of aanalysis in marr 's hierarchy ( where bayesian models are situated ) and the algorithmic level of analysis in which plausible mechanisms are described ."}
{"title": "software agents with concerns of their own", "abstract": "we claim that it is possible to have artificial software agents for which their actions and the world they inhabit have first-person or intrinsic meanings . the first-person or intrinsic meaning of an entity to a system is defined as its relation with the system 's goals and capabilities , given the properties of the environment in which it operates . therefore , for a system to develop first-person meanings , it must see itself as a goal-directed actor , facing limitations and opportunities dictated by its own capabilities , and by the properties of the environment . the first part of the paper discusses this claim in the context of arguments against and proposals addressing the development of computer programs with first-person meanings . a set of definitions is also presented , most importantly the concepts of cold and phenomenal first-person meanings . the second part of the paper presents preliminary proposals and achievements , resulting of actual software implementations , within a research approach that aims to develop software agents that intrinsically understand their actions and what happens to them . as a result , an agent with no a priori notion of its goals and capabilities , and of the properties of its environment acquires all these notions by observing itself in action . the cold first-person meanings of the agent 's actions and of what happens to it are defined using these acquired notions . although not solving the full problem of first-person meanings , the proposed approach and preliminary results allow us some confidence to address the problems yet to be considered , in particular the phenomenal aspect of first-person meanings ."}
{"title": "learning possibilistic logic theories from default rules", "abstract": "we introduce a setting for learning possibilistic logic theories from defaults of the form `` if alpha then typically beta '' . we first analyse this problem from the point of view of machine learning theory , determining the vc dimension of possibilistic stratifications as well as the complexity of the associated learning problems , after which we present a heuristic learning algorithm that can easily scale to thousands of defaults . an important property of our approach is that it is inherently able to handle noisy and conflicting sets of defaults . among others , this allows us to learn possibilistic logic theories from crowdsourced data and to approximate propositional markov logic networks using heuristic map solvers . we present experimental results that demonstrate the effectiveness of this approach ."}
{"title": "a machine learning approach to air traffic route choice modelling", "abstract": "air traffic flow and capacity management ( atfcm ) is one of the constituent parts of air traffic management ( atm ) . the goal of atfcm is to make airport and airspace capacity meet traffic demand and , when capacity opportunities are exhausted , optimise traffic flows to meet the available capacity . one of the key enablers of atfcm is the accurate estimation of future traffic demand . the available information ( schedules , flight plans , etc . ) and its associated level of uncertainty differ across the different atfcm planning phases , leading to qualitative differences between the types of forecasting that are feasible at each time horizon . while abundant research has been conducted on tactical trajectory prediction ( i.e. , during the day of operations ) , trajectory prediction in the pre-tactical phase , when few or no flight plans are available , has received much less attention . as a consequence , the methods currently in use for pre-tactical traffic forecast are still rather rudimentary , often resulting in suboptimal atfcm decision making . this paper proposes a machine learning approach for the prediction of airlines route choices between two airports as a function of route characteristics , such as flight efficiency , air navigation charges and expected level of congestion . different predictive models based on multinomial logistic regression and decision trees are formulated and calibrated with historical traffic data , and a critical evaluation of each model is conducted . we analyse the predictive power of each model in terms of its ability to forecast traffic volumes at the level of charging zones , proving significant potential to enhance pre-tactical traffic forecast . we conclude by discussing the limitations and room for improvement of the proposed approach , as well as the future developments required to produce reliable traffic forecasts at a higher spatial and temporal resolution ."}
{"title": "training a fully convolutional neural network to route integrated circuits", "abstract": "we present a deep , fully convolutional neural network that learns to route a circuit layout net with appropriate choice of metal tracks and wire class combinations . inputs to the network are the encoded layouts containing spatial location of pins to be routed . after 15 fully convolutional stages followed by a score comparator , the network outputs 8 layout layers ( corresponding to 4 route layers , 3 via layers and an identity-mapped pin layer ) which are then decoded to obtain the routed layouts . we formulate this as a binary segmentation problem on a per-pixel per-layer basis , where the network is trained to correctly classify pixels in each layout layer to be 'on ' or 'off ' . to demonstrate learnability of layout design rules , we train the network on a dataset of 50,000 train and 10,000 validation samples that we generate based on certain pre-defined layout constraints . precision , recall and $ f_1 $ score metrics are used to track the training progress . our network achieves $ f_1\\approx97\\ % $ on the train set and $ f_1\\approx92\\ % $ on the validation set . we use pytorch for implementing our model . code is made publicly available at https : //github.com/sjain-stanford/deep-route ."}
{"title": "the information-theoretic and algorithmic approach to human , animal and artificial cognition", "abstract": "we survey concepts at the frontier of research connecting artificial , animal and human cognition to computation and information processing -- -from the turing test to searle 's chinese room argument , from integrated information theory to computational and algorithmic complexity . we start by arguing that passing the turing test is a trivial computational problem and that its pragmatic difficulty sheds light on the computational nature of the human mind more than it does on the challenge of artificial intelligence . we then review our proposed algorithmic information-theoretic measures for quantifying and characterizing cognition in various forms . these are capable of accounting for known biases in human behavior , thus vindicating a computational algorithmic view of cognition as first suggested by turing , but this time rooted in the concept of algorithmic probability , which in turn is based on computational universality while being independent of computational model , and which has the virtue of being predictive and testable as a model theory of cognitive behavior ."}
{"title": "disintegration and bayesian inversion , both abstractly and concretely", "abstract": "the notions of disintegration and bayesian inversion are fundamental in conditional probability theory . they produce channels , as conditional probabilities , from a joint state , or from an already given channel ( in opposite direction ) . these notions exist in the literature , in concrete situations , but are presented here in abstract graphical formulations . the resulting abstract descriptions are used for proving basic results in conditional probability theory . the existence of disintegration and bayesian inversion is discussed for discrete probability , and also for measure-theoretic probability -- - via standard borel spaces and via likelihoods . finally , the usefulness of disintegration and bayesian inversion is illustrated in several non-trivial examples ."}
{"title": "constructing folksonomies from user-specified relations on flickr", "abstract": "many social web sites allow users to publish content and annotate with descriptive metadata . in addition to flat tags , some social web sites have recently began to allow users to organize their content and metadata hierarchically . the social photosharing site flickr , for example , allows users to group related photos in sets , and related sets in collections . the social bookmarking site del.icio.us similarly lets users group related tags into bundles . although the sites themselves do n't impose any constraints on how these hierarchies are used , individuals generally use them to capture relationships between concepts , most commonly the broader/narrower relations . collective annotation of content with hierarchical relations may lead to an emergent classification system , called a folksonomy . while some researchers have explored using tags as evidence for learning folksonomies , we believe that hierarchical relations described above offer a high-quality source of evidence for this task . we propose a simple approach to aggregate shallow hierarchies created by many distinct flickr users into a common folksonomy . our approach uses statistics to determine if a particular relation should be retained or discarded . the relations are then woven together into larger hierarchies . although we have not carried out a detailed quantitative evaluation of the approach , it looks very promising since it generates very reasonable , non-trivial hierarchies ."}
{"title": "coarse-to-fine sequential monte carlo for probabilistic programs", "abstract": "many practical techniques for probabilistic inference require a sequence of distributions that interpolate between a tractable distribution and an intractable distribution of interest . usually , the sequences used are simple , e.g. , based on geometric averages between distributions . when models are expressed as probabilistic programs , the models themselves are highly structured objects that can be used to derive annealing sequences that are more sensitive to domain structure . we propose an algorithm for transforming probabilistic programs to coarse-to-fine programs which have the same marginal distribution as the original programs , but generate the data at increasing levels of detail , from coarse to fine . we apply this algorithm to an ising model , its depth-from-disparity variation , and a factorial hidden markov model . we show preliminary evidence that the use of coarse-to-fine models can make existing generic inference algorithms more efficient ."}
{"title": "analysis of cause-effect inference via regression errors", "abstract": "we address the problem of inferring the causal relation between two variables by comparing the least-squares errors of the predictions in both possible causal directions . under the assumption of an independence between the function relating cause and effect , the conditional noise distribution , and the distribution of the cause , we show that the errors are smaller in causal direction if both variables are equally scaled and the causal relation is close to deterministic . based on this , we provide an easily applicable algorithm that only requires a regression in both possible causal directions and a comparison of the errors . the performance of the algorithm is compared with different related causal inference methods in various artificial and real-world data sets ."}
{"title": "modular multi-objective deep reinforcement learning with decision values", "abstract": "in this work we present a method for using deep q-networks ( dqns ) in multi-objective environments . deep q-networks provide remarkable performance in single objective problems learning from high-level visual state representations . however , in many scenarios ( e.g in robotics , games ) , the agent needs to pursue multiple objectives simultaneously . we propose an architecture in which separate dqns are used to control the agent 's behaviour with respect to particular objectives . in this architecture we introduce decision values to improve the scalarization of multiple dqns into a single action . our architecture enables the decomposition of the agent 's behaviour into controllable and replaceable sub-behaviours learned by distinct modules . moreover , it allows to change the priorities of particular objectives post-learning , while preserving the overall performance of the agent . to evaluate our solution we used a game-like simulator in which an agent - provided with high-level visual input - pursues multiple objectives in a 2d world ."}
{"title": "building smart communities with cyber-physical systems", "abstract": "there is a growing trend towards the convergence of cyber-physical systems ( cps ) and social computing , which will lead to the emergence of smart communities composed of various objects ( including both human individuals and physical things ) that interact and cooperate with each other . these smart communities promise to enable a number of innovative applications and services that will improve the quality of life . this position paper addresses some opportunities and challenges of building smart communities characterized by cyber-physical and social intelligence ."}
{"title": "difficulty rating of sudoku puzzles : an overview and evaluation", "abstract": "how can we predict the difficulty of a sudoku puzzle ? we give an overview of difficulty rating metrics and evaluate them on extensive dataset on human problem solving ( more then 1700 sudoku puzzles , hundreds of solvers ) . the best results are obtained using a computational model of human solving activity . using the model we show that there are two sources of the problem difficulty : complexity of individual steps ( logic operations ) and structure of dependency among steps . we also describe metrics based on analysis of solutions under relaxed constraints -- a novel approach inspired by phase transition phenomenon in the graph coloring problem . in our discussion we focus not just on the performance of individual metrics on the sudoku puzzle , but also on their generalizability and applicability to other problems ."}
{"title": "algorithms and limits for compact plan representations", "abstract": "compact representations of objects is a common concept in computer science . automated planning can be viewed as a case of this concept : a planning instance is a compact implicit representation of a graph and the problem is to find a path ( a plan ) in this graph . while the graphs themselves are represented compactly as planning instances , the paths are usually represented explicitly as sequences of actions . some cases are known where the plans always have compact representations , for example , using macros . we show that these results do not extend to the general case , by proving a number of bounds for compact representations of plans under various criteria , like efficient sequential or random access of actions . in addition to this , we show that our results have consequences for what can be gained from reformulating planning into some other problem . as a contrast to this we also prove a number of positive results , demonstrating restricted cases where plans do have useful compact representations , as well as proving that macro plans have favourable access properties . our results are finally discussed in relation to other relevant contexts ."}
{"title": "towards a benchmark of natural language arguments", "abstract": "the connections among natural language processing and argumentation theory are becoming stronger in the latest years , with a growing amount of works going in this direction , in different scenarios and applying heterogeneous techniques . in this paper , we present two datasets we built to cope with the combination of the textual entailment framework and bipolar abstract argumentation . in our approach , such datasets are used to automatically identify through a textual entailment system the relations among the arguments ( i.e. , attack , support ) , and then the resulting bipolar argumentation graphs are analyzed to compute the accepted arguments ."}
{"title": "convergence of online mirror descent algorithms", "abstract": "in this paper we consider online mirror descent ( omd ) algorithms , a class of scalable online learning algorithms exploiting data geometric structures through mirror maps . necessary and sufficient conditions are presented in terms of the step size sequence $ \\ { \\eta_t\\ } _ { t } $ for the convergence of an omd algorithm with respect to the expected bregman distance induced by the mirror map . the condition is $ \\lim_ { t\\to\\infty } \\eta_t=0 , \\sum_ { t=1 } ^ { \\infty } \\eta_t=\\infty $ in the case of positive variances . it is reduced to $ \\sum_ { t=1 } ^ { \\infty } \\eta_t=\\infty $ in the case of zero variances for which the linear convergence may be achieved by taking a constant step size sequence . a sufficient condition on the almost sure convergence is also given . we establish tight error bounds under mild conditions on the mirror map , the loss function , and the regularizer . our results are achieved by some novel analysis on the one-step progress of the omd algorithm using smoothness and strong convexity of the mirror map and the loss function ."}
{"title": "icon challenge on algorithm selection", "abstract": "we present the results of the icon challenge on algorithm selection ."}
{"title": "cluster-based kriging approximation algorithms for complexity reduction", "abstract": "kriging or gaussian process regression is applied in many fields as a non-linear regression model as well as a surrogate model in the field of evolutionary computation . however , the computational and space complexity of kriging , that is cubic and quadratic in the number of data points respectively , becomes a major bottleneck with more and more data available nowadays . in this paper , we propose a general methodology for the complexity reduction , called cluster kriging , where the whole data set is partitioned into smaller clusters and multiple kriging models are built on top of them . in addition , four kriging approximation algorithms are proposed as candidate algorithms within the new framework . each of these algorithms can be applied to much larger data sets while maintaining the advantages and power of kriging . the proposed algorithms are explained in detail and compared empirically against a broad set of existing state-of-the-art kriging approximation methods on a well-defined testing framework . according to the empirical study , the proposed algorithms consistently outperform the existing algorithms . moreover , some practical suggestions are provided for using the proposed algorithms ."}
{"title": "autodetection and classification of hidden cultural city districts from yelp reviews", "abstract": "topic models are a way to discover underlying themes in an otherwise unstructured collection of documents . in this study , we specifically used the latent dirichlet allocation ( lda ) topic model on a dataset of yelp reviews to classify restaurants based off of their reviews . furthermore , we hypothesize that within a city , restaurants can be grouped into similar `` clusters '' based on both location and similarity . we used several different clustering methods , including k-means clustering and a probabilistic mixture model , in order to uncover and classify districts , both well-known and hidden ( i.e . cultural areas like chinatown or hearsay like `` the best street for italian restaurants '' ) within a city . we use these models to display and label different clusters on a map . we also introduce a topic similarity heatmap that displays the similarity distribution in a city to a new restaurant ."}
{"title": "sift-based ear recognition by fusion of detected keypoints from color similarity slice regions", "abstract": "ear biometric is considered as one of the most reliable and invariant biometrics characteristics in line with iris and fingerprint characteristics . in many cases , ear biometrics can be compared with face biometrics regarding many physiological and texture characteristics . in this paper , a robust and efficient ear recognition system is presented , which uses scale invariant feature transform ( sift ) as feature descriptor for structural representation of ear images . in order to make it more robust to user authentication , only the regions having color probabilities in a certain ranges are considered for invariant sift feature extraction , where the k-l divergence is used for keeping color consistency . ear skin color model is formed by gaussian mixture model and clustering the ear color pattern using vector quantization . finally , k-l divergence is applied to the gmm framework for recording the color similarity in the specified ranges by comparing color similarity between a pair of reference model and probe ear images . after segmentation of ear images in some color slice regions , sift keypoints are extracted and an augmented vector of extracted sift features are created for matching , which is accomplished between a pair of reference model and probe ear images . the proposed technique has been tested on the iitk ear database and the experimental results show improvements in recognition accuracy while invariant features are extracted from color slice regions to maintain the robustness of the system ."}
{"title": "reasoning with topological and directional spatial information", "abstract": "current research on qualitative spatial representation and reasoning mainly focuses on one single aspect of space . in real world applications , however , multiple spatial aspects are often involved simultaneously . this paper investigates problems arising in reasoning with combined topological and directional information . we use the rcc8 algebra and the rectangle algebra ( ra ) for expressing topological and directional information respectively . we give examples to show that the bipath-consistency algorithm bipath is incomplete for solving even basic rcc8 and ra constraints . if topological constraints are taken from some maximal tractable subclasses of rcc8 , and directional constraints are taken from a subalgebra , termed dir49 , of ra , then we show that bipath is able to separate topological constraints from directional ones . this means , given a set of hybrid topological and directional constraints from the above subclasses of rcc8 and ra , we can transfer the joint satisfaction problem in polynomial time to two independent satisfaction problems in rcc8 and ra . for general ra constraints , we give a method to compute solutions that satisfy all topological constraints and approximately satisfy each ra constraint to any prescribed precision ."}
{"title": "efficient learning of generalized linear and single index models with isotonic regression", "abstract": "generalized linear models ( glms ) and single index models ( sims ) provide powerful generalizations of linear regression , where the target variable is assumed to be a ( possibly unknown ) 1-dimensional function of a linear predictor . in general , these problems entail non-convex estimation procedures , and , in practice , iterative local search heuristics are often used . kalai and sastry ( 2009 ) recently provided the first provably efficient method for learning sims and glms , under the assumptions that the data are in fact generated under a glm and under certain monotonicity and lipschitz constraints . however , to obtain provable performance , the method requires a fresh sample every iteration . in this paper , we provide algorithms for learning glms and sims , which are both computationally and statistically efficient . we also provide an empirical study , demonstrating their feasibility in practice ."}
{"title": "interacting attention-gated recurrent networks for recommendation", "abstract": "capturing the temporal dynamics of user preferences over items is important for recommendation . existing methods mainly assume that all time steps in user-item interaction history are equally relevant to recommendation , which however does not apply in real-world scenarios where user-item interactions can often happen accidentally . more importantly , they learn user and item dynamics separately , thus failing to capture their joint effects on user-item interactions . to better model user and item dynamics , we present the interacting attention-gated recurrent network ( iarn ) which adopts the attention model to measure the relevance of each time step . in particular , we propose a novel attention scheme to learn the attention scores of user and item history in an interacting way , thus to account for the dependencies between user and item dynamics in shaping user-item interactions . by doing so , iarn can selectively memorize different time steps of a user 's history when predicting her preferences over different items . our model can therefore provide meaningful interpretations for recommendation results , which could be further enhanced by auxiliary features . extensive validation on real-world datasets shows that iarn consistently outperforms state-of-the-art methods ."}
{"title": "on the scope of the universal-algebraic approach to constraint satisfaction", "abstract": "the universal-algebraic approach has proved a powerful tool in the study of the complexity of csps . this approach has previously been applied to the study of csps with finite or ( infinite ) omega-categorical templates , and relies on two facts . the first is that in finite or omega-categorical structures a , a relation is primitive positive definable if and only if it is preserved by the polymorphisms of a. the second is that every finite or omega-categorical structure is homomorphically equivalent to a core structure . in this paper , we present generalizations of these facts to infinite structures that are not necessarily omega-categorical . ( this abstract has been severely curtailed by the space constraints of arxiv -- please read the full abstract in the article . ) finally , we present applications of our general results to the description and analysis of the complexity of csps . in particular , we give general hardness criteria based on the absence of polymorphisms that depend on more than one argument , and we present a polymorphism-based description of those csps that are first-order definable ( and therefore can be solved in polynomial time ) ."}
{"title": "approximate counting of graphical models via mcmc revisited", "abstract": "in pe\\~na ( 2007 ) , mcmc sampling is applied to approximately calculate the ratio of essential graphs ( egs ) to directed acyclic graphs ( dags ) for up to 20 nodes . in the present paper , we extend that work from 20 to 31 nodes . we also extend that work by computing the approximate ratio of connected egs to connected dags , of connected egs to egs , and of connected dags to dags . furthermore , we prove that the latter ratio is asymptotically 1. we also discuss the implications of these results for learning dags from data ."}
{"title": "deceptive games", "abstract": "deceptive games are games where the reward structure or other aspects of the game are designed to lead the agent away from a globally optimal policy . while many games are already deceptive to some extent , we designed a series of games in the video game description language ( vgdl ) implementing specific types of deception , classified by the cognitive biases they exploit . vgdl games can be run in the general video game artificial intelligence ( gvgai ) framework , making it possible to test a variety of existing ai agents that have been submitted to the gvgai competition on these deceptive games . our results show that all tested agents are vulnerable to several kinds of deception , but that different agents have different weaknesses . this suggests that we can use deception to understand the capabilities of a game-playing algorithm , and game-playing algorithms to characterize the deception displayed by a game ."}
{"title": "parameter sharing deep deterministic policy gradient for cooperative multi-agent reinforcement learning", "abstract": "deep reinforcement learning for multi-agent cooperation and competition has been a hot topic recently . this paper focuses on cooperative multi-agent problem based on actor-critic methods under local observations settings . multi agent deep deterministic policy gradient obtained state of art results for some multi-agent games , whereas , it can not scale well with growing amount of agents . in order to boost scalability , we propose a parameter sharing deterministic policy gradient method with three variants based on neural networks , including actor-critic sharing , actor sharing and actor sharing with partially shared critic . benchmarks from rllab show that the proposed method has advantages in learning speed and memory efficiency , well scales with growing amount of agents , and moreover , it can make full use of reward sharing and exchangeability if possible ."}
{"title": "modelling office energy consumption : an agent based approach", "abstract": "in this paper , we develop an agent-based model which integrates four important elements , i.e . organisational energy management policies/regulations , energy management technologies , electric appliances and equipment , and human behaviour , based on a case study , to simulate the energy consumption in office buildings . with the model , we test the effectiveness of different energy management strategies , and solve practical office energy consumption problems . this paper theoretically contributes to an integration of four elements involved in the complex organisational issue of office energy consumption , and practically contributes to an application of agent-based approach for office building energy consumption study ."}
{"title": "a resolution prover for coalition logic", "abstract": "we present a prototype tool for automated reasoning for coalition logic , a non-normal modal logic that can be used for reasoning about cooperative agency . the theorem prover clprover is based on recent work on a resolution-based calculus for coalition logic that operates on coalition problems , a normal form for coalition logic . we provide an overview of coalition problems and of the resolution-based calculus for coalition logic . we then give details of the implementation of clprover and present the results for a comparison with an existing tableau-based solver ."}
{"title": "a heuristic routing mechanism using a new addressing scheme", "abstract": "current methods of routing are based on network information in the form of routing tables , in which routing protocols determine how to update the tables according to the network changes . despite the variability of data in routing tables , node addresses are constant . in this paper , we first introduce the new concept of variable addresses , which results in a novel framework to cope with routing problems using heuristic solutions . then we propose a heuristic routing mechanism based on the application of genes for determination of network addresses in a variable address network and describe how this method flexibly solves different problems and induces new ideas in providing integral solutions for variety of problems . the case of ad-hoc networks is where simulation results are more supportive and original solutions have been proposed for issues like mobility ."}
{"title": "information integration and computational logic", "abstract": "information integration is a young and exciting field with enormous research and commercial significance in the new world of the information society . it stands at the crossroad of databases and artificial intelligence requiring novel techniques that bring together different methods from these fields . information from disparate heterogeneous sources often with no a-priori common schema needs to be synthesized in a flexible , transparent and intelligent way in order to respond to the demands of a query thus enabling a more informed decision by the user or application program . the field although relatively young has already found many practical applications particularly for integrating information over the world wide web . this paper gives a brief introduction of the field highlighting some of the main current and future research issues and application areas . it attempts to evaluate the current and potential role of computational logic in this and suggests some of the problems where logic-based techniques could be used ."}
{"title": "learning how to learn : an adaptive dialogue agent for incrementally learning visually grounded word meanings", "abstract": "we present an optimised multi-modal dialogue agent for interactive learning of visually grounded word meanings from a human tutor , trained on real human-human tutoring data . within a life-long interactive learning period , the agent , trained using reinforcement learning ( rl ) , must be able to handle natural conversations with human users and achieve good learning performance ( accuracy ) while minimising human effort in the learning process . we train and evaluate this system in interaction with a simulated human tutor , which is built on the burchak corpus -- a human-human dialogue dataset for the visual learning task . the results show that : 1 ) the learned policy can coherently interact with the simulated user to achieve the goal of the task ( i.e . learning visual attributes of objects , e.g . colour and shape ) ; and 2 ) it finds a better trade-off between classifier accuracy and tutoring costs than hand-crafted rule-based policies , including ones with dynamic policies ."}
{"title": "computational understanding and manipulation of symmetries", "abstract": "for natural and artificial systems with some symmetry structure , computational understanding and manipulation can be achieved without learning by exploiting the algebraic structure . here we describe this algebraic coordinatization method and apply it to permutation puzzles . coordinatization yields a structural understanding , not just solutions for the puzzles ."}
{"title": "adaptive candidate generation for scalable edge-discovery tasks on data graphs", "abstract": "several ` edge-discovery ' applications over graph-based data models are known to have worst-case quadratic time complexity in the nodes , even if the discovered edges are sparse . one example is the generic link discovery problem between two graphs , which has invited research interest in several communities . specific versions of this problem include link prediction in social networks , ontology alignment between metadata-rich rdf data , approximate joins , and entity resolution between instance-rich data . as large datasets continue to proliferate , reducing quadratic complexity to make the task practical is an important research problem . within the entity resolution community , the problem is commonly referred to as blocking . a particular class of learnable blocking schemes is known as disjunctive normal form ( dnf ) blocking schemes , and has emerged as state-of-the art for homogeneous ( i.e . same-schema ) tabular data . despite the promise of these schemes , a formalism or learning framework has not been developed for them when input data instances are generic , attributed graphs possessing both node and edge heterogeneity . with such a development , the complexity-reducing scope of dnf schemes becomes applicable to a variety of problems , including entity resolution and type alignment between heterogeneous graphs , and link prediction in networks represented as attributed graphs . this paper presents a graph-theoretic formalism for dnf schemes , and investigates their learnability in an optimization framework . we also briefly describe an empirical case study encapsulating some of the principles in this paper ."}
{"title": "approximate bayesian image interpretation using generative probabilistic graphics programs", "abstract": "the idea of computer vision as the bayesian inverse problem to computer graphics has a long history and an appealing elegance , but it has proved difficult to directly implement . instead , most vision tasks are approached via complex bottom-up processing pipelines . here we show that it is possible to write short , simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images . generative probabilistic graphics programs consist of a stochastic scene generator , a renderer based on graphics software , a stochastic likelihood model linking the renderer 's output and the data , and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model . representations and algorithms from computer graphics , originally designed to produce high-quality images , are instead used as the deterministic backbone for highly approximate and stochastic generative models . this formulation combines probabilistic programming , computer graphics , and approximate bayesian computation , and depends only on general-purpose , automatic inference techniques . we describe two applications : reading sequences of degraded and adversarially obscured alphanumeric characters , and inferring 3d road models from vehicle-mounted camera images . each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code , and supports accurate , approximately bayesian inferences about ambiguous real-world images ."}
{"title": "application of advanced record linkage techniques for complex population reconstruction", "abstract": "record linkage is the process of identifying records that refer to the same entities from several databases . this process is challenging because commonly no unique entity identifiers are available . linkage therefore has to rely on partially identifying attributes , such as names and addresses of people . recent years have seen the development of novel techniques for linking data from diverse application areas , where a major focus has been on linking complex data that contain records about different types of entities . advanced approaches that exploit both the similarities between record attributes as well as the relationships between entities to identify clusters of matching records have been developed . in this application paper we study the novel problem where rather than different types of entities we have databases where the same entity can have different roles , and where these roles change over time . we specifically develop novel techniques for linking historical birth , death , marriage and census records with the aim to reconstruct the population covered by these records over a period of several decades . our experimental evaluation on real scottish data shows that even with advanced linkage techniques that consider group , relationship , and temporal aspects it is challenging to achieve high quality linkage from such complex data ."}
{"title": "language-constraint reachability learning in probabilistic graphs", "abstract": "the probabilistic graphs framework models the uncertainty inherent in real-world domains by means of probabilistic edges whose value quantifies the likelihood of the edge existence or the strength of the link it represents . the goal of this paper is to provide a learning method to compute the most likely relationship between two nodes in a framework based on probabilistic graphs . in particular , given a probabilistic graph we adopted the language-constraint reachability method to compute the probability of possible interconnections that may exists between two nodes . each of these connections may be viewed as feature , or a factor , between the two nodes and the corresponding probability as its weight . each observed link is considered as a positive instance for its corresponding link label . given the training set of observed links a l2-regularized logistic regression has been adopted to learn a model able to predict unobserved link labels . the experiments on a real world collaborative filtering problem proved that the proposed approach achieves better results than that obtained adopting classical methods ."}
{"title": "knowledge sharing : a model", "abstract": "we know anything because we learn about it , there is anything we ever share about it , but now a lot of media that can represent how it happened as infrastructure of the knowledge sharing . this paper aims to introduce a model for understanding a problem in knowledge sharing based on interaction ."}
{"title": "do reichenbachian common cause systems of arbitrary finite size exist ?", "abstract": "the principle of common cause asserts that positive correlations between causally unrelated events ought to be explained through the action of some shared causal factors . reichenbachian common cause systems are probabilistic structures aimed at accounting for cases where correlations of the aforesaid sort can not be explained through the action of a single common cause . the existence of reichenbachian common cause systems of arbitrary finite size for each pair of non-causally correlated events was allegedly demonstrated by hofer-szab\\'o and r\\'edei in 2006. this paper shows that their proof is logically deficient , and we propose an improved proof ."}
{"title": "self organizing maps whose topologies can be learned with adaptive binary search trees using conditional rotations", "abstract": "numerous variants of self-organizing maps ( soms ) have been proposed in the literature , including those which also possess an underlying structure , and in some cases , this structure itself can be defined by the user although the concepts of growing the som and updating it have been studied , the whole issue of using a self-organizing adaptive data structure ( ads ) to further enhance the properties of the underlying som , has been unexplored . in an earlier work , we impose an arbitrary , user-defined , tree-like topology onto the codebooks , which consequently enforced a neighborhood phenomenon and the so-called tree-based bubble of activity . in this paper , we consider how the underlying tree itself can be rendered dynamic and adaptively transformed . to do this , we present methods by which a som with an underlying binary search tree ( bst ) structure can be adaptively re-structured using conditional rotations ( conrot ) . these rotations on the nodes of the tree are local , can be done in constant time , and performed so as to decrease the weighted path length ( wpl ) of the entire tree . in doing this , we introduce the pioneering concept referred to as neural promotion , where neurons gain prominence in the neural network ( nn ) as their significance increases . we are not aware of any research which deals with the issue of neural promotion . the advantages of such a scheme is that the user need not be aware of any of the topological peculiarities of the stochastic data distribution . rather , the algorithm , referred to as the ttosom with conditional rotations ( ttoconrot ) , converges in such a manner that the neurons are ultimately placed in the input space so as to represent its stochastic distribution , and additionally , the neighborhood properties of the neurons suit the best bst that represents the data . these properties have been confirmed by our experimental results on a variety of data sets ."}
{"title": "discretization-free knowledge gradient methods for bayesian optimization", "abstract": "this paper studies bayesian ranking and selection ( r & s ) problems with correlated prior beliefs and continuous domains , i.e . bayesian optimization ( bo ) . knowledge gradient methods [ frazier et al. , 2008 , 2009 ] have been widely studied for discrete r & s problems , which sample the one-step bayes-optimal point . when used over continuous domains , previous work on the knowledge gradient [ scott et al. , 2011 , wu and frazier , 2016 , wu et al. , 2017 ] often rely on a discretized finite approximation . however , the discretization introduces error and scales poorly as the dimension of domain grows . in this paper , we develop a fast discretization-free knowledge gradient method for bayesian optimization . our method is not restricted to the fully sequential setting , but useful in all settings where knowledge gradient can be used over continuous domains . we show how our method can be generalized to handle ( i ) batch of points suggestion ( parallel knowledge gradient ) ; ( ii ) the setting where derivative information is available in the optimization process ( derivative-enabled knowledge gradient ) . in numerical experiments , we demonstrate that the discretization-free knowledge gradient method finds global optima significantly faster than previous bayesian optimization algorithms on both synthetic test functions and real-world applications , especially when function evaluations are noisy ; and derivative-enabled knowledge gradient can further improve the performances , even outperforming the gradient-based optimizer such as bfgs when derivative information is available ."}
{"title": "integration of the dolce top-level ontology into the ontospec methodology", "abstract": "this report describes a new version of the ontospec methodology for ontology building . defined by the laria knowledge engineering team ( university of picardie jules verne , amiens , france ) , ontospec aims at helping builders to model ontological knowledge ( upstream of formal representation ) . the methodology relies on a set of rigorously-defined modelling primitives and principles . its application leads to the elaboration of a semi-informal ontology , which is independent of knowledge representation languages . we recently enriched the ontospec methodology by endowing it with a new resource , the dolce top-level ontology defined at the loa ( ist-cnr , trento , italy ) . the goal of this integration is to provide modellers with additional help in structuring application ontologies , while maintaining independence vis-\\ ` { a } -vis formal representation languages . in this report , we first provide an overview of the ontospec methodology 's general principles and then describe the dolce re-engineering process . a complete version of dolce-os ( i.e . a specification of dolce in the semi-informal ontospec language ) is presented in an appendix ."}
{"title": "local approximate inference algorithms", "abstract": "we present a new local approximation algorithm for computing maximum a posteriori ( map ) and log-partition function for arbitrary exponential family distribution represented by a finite-valued pair-wise markov random field ( mrf ) , say $ g $ . our algorithm is based on decomposition of $ g $ into { \\em appropriately } chosen small components ; then computing estimates locally in each of these components and then producing a { \\em good } global solution . we show that if the underlying graph $ g $ either excludes some finite-sized graph as its minor ( e.g . planar graph ) or has low doubling dimension ( e.g . any graph with { \\em geometry } ) , then our algorithm will produce solution for both questions within { \\em arbitrary accuracy } . we present a message-passing implementation of our algorithm for map computation using self-avoiding walk of graph . in order to evaluate the computational cost of this implementation , we derive novel tight bounds on the size of self-avoiding walk tree for arbitrary graph . as a consequence of our algorithmic result , we show that the normalized log-partition function ( also known as free-energy ) for a class of { \\em regular } mrfs will converge to a limit , that is computable to an arbitrary accuracy ."}
{"title": "automating change of representation for proofs in discrete mathematics", "abstract": "representation determines how we can reason about a specific problem . sometimes one representation helps us find a proof more easily than others . most current automated reasoning tools focus on reasoning within one representation . there is , therefore , a need for the development of better tools to mechanise and automate formal and logically sound changes of representation . in this paper we look at examples of representational transformations in discrete mathematics , and show how we have used isabelle 's transfer tool to automate the use of these transformations in proofs . we give a brief overview of a general theory of transformations that we consider appropriate for thinking about the matter , and we explain how it relates to the transfer package . we show our progress towards developing a general tactic that incorporates the automatic search for representation within the proving process ."}
{"title": "revisiting causality inference in memory-less transition networks", "abstract": "several methods exist to infer causal networks from massive volumes of observational data . however , almost all existing methods require a considerable length of time series data to capture cause and effect relationships . in contrast , memory-less transition networks or markov chain data , which refers to one-step transitions to and from an event , have not been explored for causality inference even though such data is widely available . we find that causal network can be inferred from characteristics of four unique distribution zones around each event . we call this composition of transitions and show that cause , effect , and random events exhibit different behavior in their compositions . we applied machine learning models to learn these different behaviors and to infer causality . we name this new method causality inference using composition of transitions ( cict ) . to evaluate cict , we used an administrative inpatient healthcare dataset to set up a network of patients transitions between different diagnoses . we show that cict is highly accurate in inferring whether the transition between a pair of events is causal or random and performs well in identifying the direction of causality in a bi-directional association ."}
{"title": "oriented straight line segment algebra : qualitative spatial reasoning about oriented objects", "abstract": "nearly 15 years ago , a set of qualitative spatial relations between oriented straight line segments ( dipoles ) was suggested by schlieder . this work received substantial interest amongst the qualitative spatial reasoning community . however , it turned out to be difficult to establish a sound constraint calculus based on these relations . in this paper , we present the results of a new investigation into dipole constraint calculi which uses algebraic methods to derive sound results on the composition of relations and other properties of dipole calculi . our results are based on a condensed semantics of the dipole relations . in contrast to the points that are normally used , dipoles are extended and have an intrinsic direction . both features are important properties of natural objects . this allows for a straightforward representation of prototypical reasoning tasks for spatial agents . as an example , we show how to generate survey knowledge from local observations in a street network . the example illustrates the fast constraint-based reasoning capabilities of the dipole calculus . we integrate our results into two reasoning tools which are publicly available ."}
{"title": "dkp-aom : results for oaei 2015", "abstract": "in this paper , we present the results obtained by our dkp-aom system within the oaei 2015 campaign . dkp-aom is an ontology merging tool designed to merge heterogeneous ontologies . in oaei , we have participated with its ontology mapping component which serves as a basic module capable of matching large scale ontologies before their merging . this is our first successful participation in the conference , oa4qa and anatomy track of oaei . dkp-aom is participating with two versions ( dkp-aom and dkp-aom_lite ) , dkp-aom performs coherence analysis . in oa4qa track , dkpaom out-performed in the evaluation and generated accurate alignments allowed to answer all the queries of the evaluation . we can also see its competitive results for the conference track in the evaluation initiative among other reputed systems . in the anatomy track , it has produced alignments within an allocated time and appeared in the list of systems which produce coherent results . finally , we discuss some future work towards the development of dkp-aom ."}
{"title": "a data and model-parallel , distributed and scalable framework for training of deep networks in apache spark", "abstract": "training deep networks is expensive and time-consuming with the training period increasing with data size and growth in model parameters . in this paper , we provide a framework for distributed training of deep networks over a cluster of cpus in apache spark . the framework implements both data parallelism and model parallelism making it suitable to use for deep networks which require huge training data and model parameters which are too big to fit into the memory of a single machine . it can be scaled easily over a cluster of cheap commodity hardware to attain significant speedup and obtain better results making it quite economical as compared to farm of gpus and supercomputers . we have proposed a new algorithm for training of deep networks for the case when the network is partitioned across the machines ( model parallelism ) along with detailed cost analysis and proof of convergence of the same . we have developed implementations for fully-connected feedforward networks , convolutional neural networks , recurrent neural networks and long short-term memory architectures . we present the results of extensive simulations demonstrating the speedup and accuracy obtained by our framework for different sizes of the data and model parameters with variation in the number of worker cores/partitions ; thereby showing that our proposed framework can achieve significant speedup ( upto 11x for cnn ) and is also quite scalable ."}
{"title": "webapirec : recommending web apis to software projects via personalized ranking", "abstract": "application programming interfaces ( apis ) offer a plethora of functionalities for developers to reuse without reinventing the wheel . identifying the appropriate apis given a project requirement is critical for the success of a project , as many functionalities can be reused to achieve faster development . however , the massive number of apis would often hinder the developers ' ability to quickly find the right apis . in this light , we propose a new , automated approach called webapirec that takes as input a project profile and outputs a ranked list of { web } apis that can be used to implement the project . at its heart , webapirec employs a personalized ranking model that ranks web apis specific ( personalized ) to a project . based on the historical data of { web } api usages , webapirec learns a model that minimizes the incorrect ordering of web apis , i.e. , when a used { web } api is ranked lower than an unused ( or a not-yet-used ) web api . we have evaluated our approach on a dataset comprising 9,883 web apis and 4,315 web application projects from programmableweb with promising results . for 84.0 % of the projects , webapirec is able to successfully return correct apis that are used to implement the projects in the top-5 positions . this is substantially better than the recommendations provided by programmableweb 's native search functionality . webapirec also outperforms mcmillan et al . 's application search engine and popularity-based recommendation ."}
{"title": "explaining aviation safety incidents using deep temporal multiple instance learning", "abstract": "although aviation accidents are rare , safety incidents occur more frequently and require a careful analysis to detect and mitigate risks in a timely manner . analyzing safety incidents using operational data and producing event-based explanations is invaluable to airline companies as well as to governing organizations such as the federal aviation administration ( faa ) in the united states . however , this task is challenging because of the complexity involved in mining multi-dimensional heterogeneous time series data , the lack of time-step-wise annotation of events in a flight , and the lack of scalable tools to perform analysis over a large number of events . in this work , we propose a precursor mining algorithm that identifies events in the multidimensional time series that are correlated with the safety incident . precursors are valuable to systems health and safety monitoring and in explaining and forecasting safety incidents . current methods suffer from poor scalability to high dimensional time series data and are inefficient in capturing temporal behavior . we propose an approach by combining multiple-instance learning ( mil ) and deep recurrent neural networks ( drnn ) to take advantage of mil 's ability to learn using weakly supervised data and drnn 's ability to model temporal behavior . we describe the algorithm , the data , the intuition behind taking a mil approach , and a comparative analysis of the proposed algorithm with baseline models . we also discuss the application to a real-world aviation safety problem using data from a commercial airline company and discuss the model 's abilities and shortcomings , with some final remarks about possible deployment directions ."}
{"title": "ideal : a software package for analysis of influence diagrams", "abstract": "ideal ( influence diagram evaluation and analysis in lisp ) is a software environment for creation and evaluation of belief networks and influence diagrams . ideal is primarily a research tool and provides an implementation of many of the latest developments in belief network and influence diagram evaluation in a unified framework . this paper describes ideal and some lessons learned during its development ."}
{"title": "inter-session modeling for session-based recommendation", "abstract": "in recent years , research has been done on applying recurrent neural networks ( rnns ) as recommender systems . results have been promising , especially in the session-based setting where rnns have been shown to outperform state-of-the-art models . in many of these experiments , the rnn could potentially improve the recommendations by utilizing information about the user 's past sessions , in addition to its own interactions in the current session . a problem for session-based recommendation , is how to produce accurate recommendations at the start of a session , before the system has learned much about the user 's current interests . we propose a novel approach that extends a rnn recommender to be able to process the user 's recent sessions , in order to improve recommendations . this is done by using a second rnn to learn from recent sessions , and predict the user 's interest in the current session . by feeding this information to the original rnn , it is able to improve its recommendations . our experiments on two different datasets show that the proposed approach can significantly improve recommendations throughout the sessions , compared to a single rnn working only on the current session . the proposed model especially improves recommendations at the start of sessions , and is therefore able to deal with the cold start problem within sessions ."}
{"title": "should terminology principles be re-examined ?", "abstract": "operationalization of terminology for it applications has revived the wusterian approach . the conceptual dimension once more prevails after taking back seat to specialised lexicography . this is demonstrated by the emergence of ontology in terminology . while the terminology principles as defined in felber manual and the iso standards remain at the core of traditional terminology , their computational implementation raises some issues . in this article , while reiterating their importance , we will be re-examining these principles from a dual perspective : that of logic in the mathematical sense of the term and that of epistemology as in the theory of knowledge . we will thus be clarifying and describing some of them so as to take into account advances in knowledge engineering ( ontology ) and formal systems ( logic ) . the notion of ontoterminology , terminology whose conceptual system is a formal ontology , results from this approach ."}
{"title": "building computer network attacks", "abstract": "in this work we start walking the path to a new perspective for viewing cyberwarfare scenarios , by introducing conceptual tools ( a formal model ) to evaluate the costs of an attack , to describe the theater of operations , targets , missions , actions , plans and assets involved in cyberwarfare attacks . we also describe two applications of this model : autonomous planning leading to automated penetration tests , and attack simulations , allowing a system administrator to evaluate the vulnerabilities of his network ."}
{"title": "divisive-agglomerative algorithm and complexity of automatic classification problems", "abstract": "an algorithm of solution of the automatic classification ( ac for brevity ) problem is set forth in the paper . in the ac problem , it is required to find one or several artitions , starting with the given pattern matrix or dissimilarity , similarity matrix ."}
{"title": "stable model counting and its application in probabilistic logic programming", "abstract": "model counting is the problem of computing the number of models that satisfy a given propositional theory . it has recently been applied to solving inference tasks in probabilistic logic programming , where the goal is to compute the probability of given queries being true provided a set of mutually independent random variables , a model ( a logic program ) and some evidence . the core of solving this inference task involves translating the logic program to a propositional theory and using a model counter . in this paper , we show that for some problems that involve inductive definitions like reachability in a graph , the translation of logic programs to sat can be expensive for the purpose of solving inference tasks . for such problems , direct implementation of stable model semantics allows for more efficient solving . we present two implementation techniques , based on unfounded set detection , that extend a propositional model counter to a stable model counter . our experiments show that for particular problems , our approach can outperform a state-of-the-art probabilistic logic programming solver by several orders of magnitude in terms of running time and space requirements , and can solve instances of significantly larger sizes on which the current solver runs out of time or memory ."}
{"title": "development of a vo registry subject ontology using automated methods", "abstract": "we report on our initial work to automate the generation of a domain ontology using subject fields of resources held in the virtual observatory registry . preliminary results are comparable to more generalized ontology learning software currently in use . we expect to be able to refine our solution to improve both the depth and breadth of the generated ontology ."}
{"title": "efficient computation of exact irv margins", "abstract": "the margin of victory is easy to compute for many election schemes but difficult for instant runoff voting ( irv ) . this is important because arguments about the correctness of an election outcome usually rely on the size of the electoral margin . for example , risk-limiting audits require a knowledge of the margin of victory in order to determine how much auditing is necessary . this paper presents a practical branch-and-bound algorithm for exact irv margin computation that substantially improves on the current best-known approach . although exponential in the worst case , our algorithm runs efficiently in practice on all the real examples we could find . we can efficiently discover exact margins on election instances that can not be solved by the current state-of-the-art ."}
{"title": "decision support with belief functions theory for seabed characterization", "abstract": "the seabed characterization from sonar images is a very hard task because of the produced data and the unknown environment , even for an human expert . in this work we propose an original approach in order to combine binary classifiers arising from different kinds of strategies such as one-versus-one or one-versus-rest , usually used in the svm-classification . the decision functions coming from these binary classifiers are interpreted in terms of belief functions in order to combine these functions with one of the numerous operators of the belief functions theory . moreover , this interpretation of the decision function allows us to propose a process of decisions by taking into account the rejected observations too far removed from the learning data , and the imprecise decisions given in unions of classes . this new approach is illustrated and evaluated with a svm in order to classify the different kinds of sediment on image sonar ."}
{"title": "context-aware sentiment word identification : sentiword2vec", "abstract": "traditional sentiment analysis often uses sentiment dictionary to extract sentiment information in text and classify documents . however , emerging informal words and phrases in user generated content call for analysis aware to the context . usually , they have special meanings in a particular context . because of its great performance in representing inter-word relation , we use sentiment word vectors to identify the special words . based on the distributed language model word2vec , in this paper we represent a novel method about sentiment representation of word under particular context , to be detailed , to identify the words with abnormal sentiment polarity in long answers . result shows the improved model shows better performance in representing the words with special meaning , while keep doing well in representing special idiomatic pattern . finally , we will discuss the meaning of vectors representing in the field of sentiment , which may be different from general object-based conditions ."}
{"title": "artificial agents and speculative bubbles", "abstract": "pertaining to agent-based computational economics ( ace ) , this work presents two models for the rise and downfall of speculative bubbles through an exchange price fixing based on double auction mechanisms . the first model is based on a finite time horizon context , where the expected dividends decrease along time . the second model follows the { \\em greater fool } hypothesis ; the agent behaviour depends on the comparison of the estimated risk with the greater fool 's . simulations shed some light on the influent parameters and the necessary conditions for the apparition of speculative bubbles in an asset market within the considered framework ."}
{"title": "curiosity-driven exploration by self-supervised prediction", "abstract": "in many real-world scenarios , rewards extrinsic to the agent are extremely sparse , or absent altogether . in such cases , curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life . we formulate curiosity as the error in an agent 's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model . our formulation scales to high-dimensional continuous state spaces like images , bypasses the difficulties of directly predicting pixels , and , critically , ignores the aspects of the environment that can not affect the agent . the proposed approach is evaluated in two environments : vizdoom and super mario bros. three broad settings are investigated : 1 ) sparse extrinsic reward , where curiosity allows for far fewer interactions with the environment to reach the goal ; 2 ) exploration with no extrinsic reward , where curiosity pushes the agent to explore more efficiently ; and 3 ) generalization to unseen scenarios ( e.g . new levels of the same game ) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch . demo video and code available at https : //pathak22.github.io/noreward-rl/"}
{"title": "music and vocal separation using multi-band modulation based features", "abstract": "the potential use of non-linear speech features has not been investigated for music analysis although other commonly used speech features like mel frequency ceptral coefficients ( mfcc ) and pitch have been used extensively . in this paper , we assume an audio signal to be a sum of modulated sinusoidal and then use the energy separation algorithm to decompose the audio into amplitude and frequency modulation components using the non-linear teager-kaiser energy operator . we first identify the distribution of these non-linear features for music only and voice only segments in the audio signal in different mel spaced frequency bands and show that they have the ability to discriminate . the proposed method based on kullback-leibler divergence measure is evaluated using a set of indian classical songs from three different artists . experimental results show that the discrimination ability is evident in certain low and mid frequency bands ( 200 - 1500 hz ) ."}
{"title": "a model for combination of external and internal stimuli in the action selection of an autonomous agent", "abstract": "this paper proposes a model for combination of external and internal stimuli for the action selection in an autonomous agent , based in an action selection mechanism previously proposed by the authors . this combination model includes additive and multiplicative elements , which allows to incorporate new properties , which enhance the action selection . a given parameter a , which is part of the proposed model , allows to regulate the degree of dependence of the observed external behaviour from the internal states of the entity ."}
{"title": "engineering crowdsourced stream processing systems", "abstract": "a crowdsourced stream processing system ( csp ) is a system that incorporates crowdsourced tasks in the processing of a data stream . this can be seen as enabling crowdsourcing work to be applied on a sample of large-scale data at high speed , or equivalently , enabling stream processing to employ human intelligence . it also leads to a substantial expansion of the capabilities of data processing systems . engineering a csp system requires the combination of human and machine computation elements . from a general systems theory perspective , this means taking into account inherited as well as emerging properties from both these elements . in this paper , we position csp systems within a broader taxonomy , outline a series of design principles and evaluation metrics , present an extensible framework for their design , and describe several design patterns . we showcase the capabilities of csp systems by performing a case study that applies our proposed framework to the design and analysis of a real system ( aidr ) that classifies social media messages during time-critical crisis events . results show that compared to a pure stream processing system , aidr can achieve a higher data classification accuracy , while compared to a pure crowdsourcing solution , the system makes better use of human workers by requiring much less manual work effort ."}
{"title": "a state-based regression formulation for domains with sensing actions < br > and incomplete information", "abstract": "we present a state-based regression function for planning domains where an agent does not have complete information and may have sensing actions . we consider binary domains and employ a three-valued characterization of domains with sensing actions to define the regression function . we prove the soundness and completeness of our regression formulation with respect to the definition of progression . more specifically , we show that ( i ) a plan obtained through regression for a planning problem is indeed a progression solution of that planning problem , and that ( ii ) for each plan found through progression , using regression one obtains that plan or an equivalent one ."}
{"title": "crdt : correlation ratio based decision tree model for healthcare data mining", "abstract": "the phenomenal growth in the healthcare data has inspired us in investigating robust and scalable models for data mining . for classification problems information gain ( ig ) based decision tree is one of the popular choices . however , depending upon the nature of the dataset , ig based decision tree may not always perform well as it prefers the attribute with more number of distinct values as the splitting attribute . healthcare datasets generally have many attributes and each attribute generally has many distinct values . in this paper , we have tried to focus on this characteristics of the datasets while analysing the performance of our proposed approach which is a variant of decision tree model and uses the concept of correlation ratio ( cr ) . unlike ig based approach , this cr based approach has no biasness towards the attribute with more number of distinct values . we have applied our model on some benchmark healthcare datasets to show the effectiveness of the proposed technique ."}
{"title": "evaluating causal models by comparing interventional distributions", "abstract": "the predominant method for evaluating the quality of causal models is to measure the graphical accuracy of the learned model structure . we present an alternative method for evaluating causal models that directly measures the accuracy of estimated interventional distributions . we contrast such distributional measures with structural measures , such as structural hamming distance and structural intervention distance , showing that structural measures often correspond poorly to the accuracy of estimated interventional distributions . we use a number of real and synthetic datasets to illustrate various scenarios in which structural measures provide misleading results with respect to algorithm selection and parameter tuning , and we recommend that distributional measures become the new standard for evaluating causal models ."}
{"title": "the alvis format for linguistically annotated documents", "abstract": "the paper describes the alvis annotation format designed for the indexing of large collections of documents in topic-specific search engines . this paper is exemplified on the biological domain and on medline abstracts , as developing a specialized search engine for biologists is one of the alvis case studies . the alvis principle for linguistic annotations is based on existing works and standard propositions . we made the choice of stand-off annotations rather than inserted mark-up . annotations are encoded as xml elements which form the linguistic subsection of the document record ."}
{"title": "a compilation target for probabilistic programming languages", "abstract": "forward inference techniques such as sequential monte carlo and particle markov chain monte carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes , forking , mutexes , and shared memory . exploiting this we have defined , developed , and tested a probabilistic programming language intermediate representation language we call probabilistic c , which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient , scalable , portable probabilistic programming compilation target . this opens up a new hardware and systems research path for optimizing probabilistic programming systems ."}
{"title": "submodular optimization with submodular cover and submodular knapsack constraints", "abstract": "we investigate two new optimization problems -- minimizing a submodular function subject to a submodular lower bound constraint ( submodular cover ) and maximizing a submodular function subject to a submodular upper bound constraint ( submodular knapsack ) . we are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection , which require maximizing a certain submodular function ( like coverage or diversity ) while simultaneously minimizing another ( like cooperative cost ) . these problems are often posed as minimizing the difference between submodular functions [ 14 , 35 ] which is in the worst case inapproximable . we show , however , that by phrasing these problems as constrained optimization , which is more natural for many applications , we achieve a number of bounded approximation guarantees . we also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other . we provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors . finally , we empirically demonstrate the performance and good scalability properties of our algorithms ."}
{"title": "the darpa twitter bot challenge", "abstract": "a number of organizations ranging from terrorist groups such as isis to politicians and nation states reportedly conduct explicit campaigns to influence opinion on social media , posing a risk to democratic processes . there is thus a growing need to identify and eliminate `` influence bots '' - realistic , automated identities that illicitly shape discussion on sites like twitter and facebook - before they get too influential . spurred by such events , darpa held a 4-week competition in february/march 2015 in which multiple teams supported by the darpa social media in strategic communications program competed to identify a set of previously identified `` influence bots '' serving as ground truth on a specific topic within twitter . past work regarding influence bots often has difficulty supporting claims about accuracy , since there is limited ground truth ( though some exceptions do exist [ 3,7 ] ) . however , with the exception of [ 3 ] , no past work has looked specifically at identifying influence bots on a specific topic . this paper describes the darpa challenge and describes the methods used by the three top-ranked teams ."}
{"title": "skopus : mining top-k sequential patterns under leverage", "abstract": "this paper presents a framework for exact discovery of the top-k sequential patterns under leverage . it combines ( 1 ) a novel definition of the expected support for a sequential pattern - a concept on which most interestingness measures directly rely - with ( 2 ) skopus : a new branch-and-bound algorithm for the exact discovery of top-k sequential patterns under a given measure of interest . our interestingness measure employs the partition approach . a pattern is interesting to the extent that it is more frequent than can be explained by assuming independence between any of the pairs of patterns from which it can be composed . the larger the support compared to the expectation under independence , the more interesting is the pattern . we build on these two elements to exactly extract the k sequential patterns with highest leverage , consistent with our definition of expected support . we conduct experiments on both synthetic data with known patterns and real-world datasets ; both experiments confirm the consistency and relevance of our approach with regard to the state of the art . this article was published in data mining and knowledge discovery and is accessible at http : //dx.doi.org/10.1007/s10618-016-0467-9 ."}
{"title": "discovering structure in high-dimensional data through correlation explanation", "abstract": "we introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective . intuitively , the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information . the method is unsupervised , requires no model assumptions , and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems . we demonstrate that correlation explanation ( corex ) automatically discovers meaningful structure for data from diverse sources including personality tests , dna , and human language ."}
{"title": "edward : a library for probabilistic modeling , inference , and criticism", "abstract": "probabilistic modeling is a powerful approach for analyzing empirical information . we describe edward , a library for probabilistic modeling . edward 's design reflects an iterative process pioneered by george box : build a model of a phenomenon , make inferences about the model given data , and criticize the model 's fit to the data . edward supports a broad class of probabilistic models , efficient algorithms for inference , and many techniques for model criticism . the library builds on top of tensorflow to support distributed training and hardware such as gpus . edward enables the development of complex probabilistic models and their algorithms at a massive scale ."}
{"title": "ensembles of multiple models and architectures for robust brain tumour segmentation", "abstract": "deep learning approaches such as convolutional neural nets have consistently outperformed previous methods on challenging tasks such as dense , semantic segmentation . however , the various proposed networks perform differently , with behaviour largely influenced by architectural choices and training settings . this paper explores ensembles of multiple models and architectures ( emma ) for robust performance through aggregation of predictions from a wide range of methods . the approach reduces the influence of the meta-parameters of individual models and the risk of overfitting the configuration to a particular database . emma can be seen as an unbiased , generic deep learning model which is shown to yield excellent performance , winning the first position in the brats 2017 competition among 50+ participating teams ."}
{"title": "pop music highlighter : marking the emotion keypoints", "abstract": "the goal of music highlight extraction is to get a short consecutive segment of a piece of music that provides an effective representation of the whole piece . in a previous work , we introduced an attention-based convolutional recurrent neural network that uses music emotion classification as a surrogate task for music highlight extraction , for pop songs . the rationale behind that approach is that the highlight of a song is usually the most emotional part . this paper extends our previous work in the following two aspects . first , methodology-wise we experiment with a new architecture that does not need any recurrent layers , making the training process faster . moreover , we compare a late-fusion variant and an early-fusion variant to study which one better exploits the attention mechanism . second , we conduct and report an extensive set of experiments comparing the proposed attention-based methods against a heuristic energy-based method , a structural repetition-based method , and a few other simple feature-based methods for this task . due to the lack of public-domain labeled data for highlight extraction , following our previous work we use the rwc pop 100-song data set to evaluate how the detected highlights overlap with any chorus sections of the songs . the experiments demonstrate the effectiveness of our methods over competing methods . for reproducibility , we open source the code and pre-trained model at https : //github.com/remyhuang/pop-music-highlighter/ ."}
{"title": "vector field based neural networks", "abstract": "a novel neural network architecture is proposed using the mathematically and physically rich idea of vector fields as hidden layers to perform nonlinear transformations in the data . the data points are interpreted as particles moving along a flow defined by the vector field which intuitively represents the desired movement to enable classification . the architecture moves the data points from their original configuration to anew one following the streamlines of the vector field with the objective of achieving a final configuration where classes are separable . an optimization problem is solved through gradient descent to learn this vector field ."}
{"title": "visual dialog", "abstract": "we introduce the task of visual dialog , which requires an ai agent to hold a meaningful dialog with humans in natural , conversational language about visual content . specifically , given an image , a dialog history , and a question about the image , the agent has to ground the question in image , infer context from history , and answer the question accurately . visual dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence , while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress . we develop a novel two-person chat data-collection protocol to curate a large-scale visual dialog dataset ( visdial ) . visdial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from coco , with a total of ~1.2m dialog question-answer pairs . we introduce a family of neural encoder-decoder models for visual dialog with 3 encoders -- late fusion , hierarchical recurrent encoder and memory network -- and 2 decoders ( generative and discriminative ) , which outperform a number of sophisticated baselines . we propose a retrieval-based evaluation protocol for visual dialog where the ai agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response . we quantify gap between machine and human performance on the visual dialog task via human studies . putting it all together , we demonstrate the first 'visual chatbot ' ! our dataset , code , trained models and visual chatbot are available on https : //visualdialog.org"}
{"title": "predicting runtime distributions using deep neural networks", "abstract": "many state-of-the-art algorithms for solving hard combinatorial problems include elements of stochasticity that lead to high variations in runtime , even for a fixed problem instance , across runs with different pseudo-random number seeds . knowledge about the runtime distributions ( rtds ) of algorithms on given problem instances can be exploited in various meta-algorithmic procedures , such as algorithm selection , portfolios , and randomized restarts . previous work has shown that machine learning can be used to individually predict mean , median and variance of rtds . to establish a new state-of-the-art in predicting rtds , we demonstrate that the parameters of an rtd should be learned jointly and that neural networks can do this well by directly optimizing the likelihood of an rtd given runtime observations . in an empirical study involving four algorithms for sat solving and ai planning , we show that our neural networks predict the true rtds of unseen instances better than previous methods . as an exemplary application of rtd predictions , we show that our rtd models also yield good predictions of running these algorithms in parallel ."}
{"title": "ensemble relational learning based on selective propositionalization", "abstract": "dealing with structured data needs the use of expressive representation formalisms that , however , puts the problem to deal with the computational complexity of the machine learning process . furthermore , real world domains require tools able to manage their typical uncertainty . many statistical relational learning approaches try to deal with these problems by combining the construction of relevant relational features with a probabilistic tool . when the combination is static ( static propositionalization ) , the constructed features are considered as boolean features and used offline as input to a statistical learner ; while , when the combination is dynamic ( dynamic propositionalization ) , the feature construction and probabilistic tool are combined into a single process . in this paper we propose a selective propositionalization method that search the optimal set of relational features to be used by a probabilistic learner in order to minimize a loss function . the new propositionalization approach has been combined with the random subspace ensemble method . experiments on real-world datasets shows the validity of the proposed method ."}
{"title": "learning sparse representations in reinforcement learning with sparse coding", "abstract": "a variety of representation learning approaches have been investigated for reinforcement learning ; much less attention , however , has been given to investigating the utility of sparse coding . outside of reinforcement learning , sparse coding representations have been widely used , with non-convex objectives that result in discriminative representations . in this work , we develop a supervised sparse coding objective for policy evaluation . despite the non-convexity of this objective , we prove that all local minima are global minima , making the approach amenable to simple optimization strategies . we empirically show that it is key to use a supervised objective , rather than the more straightforward unsupervised sparse coding approach . we compare the learned representations to a canonical fixed sparse representation , called tile-coding , demonstrating that the sparse coding representation outperforms a wide variety of tilecoding representations ."}
{"title": "on the choice of regions for generalized belief propagation", "abstract": "generalized belief propagation ( gbp ) has proven to be a promising technique for approximate inference tasks in ai and machine learning . however , the choice of a good set of clusters to be used in gbp has remained more of an art then a science until this day . this paper proposes a sequential approach to adding new clusters of nodes and their interactions ( i.e . `` regions '' ) to the approximation . we first review and analyze the recently introduced region graphs and find that three kinds of operations ( `` split '' , `` merge '' and `` death '' ) leave the free energy and ( under some conditions ) the fixed points of gbp invariant . this leads to the notion of `` weakly irreducible '' regions as the natural candidates to be added to the approximation . computational complexity of the gbp algorithm is controlled by restricting attention to regions with small `` region-width '' . combining the above with an efficient ( i.e . local in the graph ) measure to predict the improved accuracy of gbp leads to the sequential `` region pursuit '' algorithm for adding new regions bottom-up to the region graph . experiments show that this algorithm can indeed perform close to optimally ."}
{"title": "simulation leagues : analysis of competition formats", "abstract": "the selection of an appropriate competition format is critical for both the success and credibility of any competition , both real and simulated . in this paper , the automated parallelism offered by the robocupsoccer 2d simulation league is leveraged to conduct a 28,000 game round-robin between the top 8 teams from robocup 2012 and 2013. a proposed new competition format is found to reduce variation from the resultant statistically significant team performance rankings by 75 % and 67 % , when compared to the actual competition results from robocup 2012 and 2013 respectively . these results are statistically validated by generating 10,000 random tournaments for each of the three considered formats and comparing the respective distributions of ranking discrepancy ."}
{"title": "automated timetabling for small colleges and high schools using huge integer programs", "abstract": "we formulate an integer program to solve a highly constrained academic timetabling problem at the united states merchant marine academy . the ip instance that results from our real case study has approximately both 170,000 rows and columns and solves to optimality in 4 -- 24 hours using a commercial solver on a portable computer ( near optimal feasible solutions were often found in 4 -- 12 hours ) . our model is applicable to both high schools and small colleges who wish to deviate from group scheduling . we also solve a necessary preprocessing student subgrouping problem , which breaks up big groups of students into small groups so they can optimally fit into small capacity classes ."}
{"title": "adaptive online sequential elm for concept drift tackling", "abstract": "a machine learning method needs to adapt to over time changes in the environment . such changes are known as concept drift . in this paper , we propose concept drift tackling method as an enhancement of online sequential extreme learning machine ( os-elm ) and constructive enhancement os-elm ( ceos-elm ) by adding adaptive capability for classification and regression problem . the scheme is named as adaptive os-elm ( aos-elm ) . it is a single classifier scheme that works well to handle real drift , virtual drift , and hybrid drift . the aos-elm also works well for sudden drift and recurrent context change type . the scheme is a simple unified method implemented in simple lines of code . we evaluated aos-elm on regression and classification problem by using concept drift public data set ( sea and stagger ) and other public data sets such as mnist , usps , and ids . experiments show that our method gives higher kappa value compared to the multiclassifier elm ensemble . even though aos-elm in practice does not need hidden nodes increase , we address some issues related to the increasing of the hidden nodes such as error condition and rank values . we propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect underfitting condition ."}
{"title": "some reflections on the set-based and the conditional-based interpretations of statements in syllogistic reasoning", "abstract": "two interpretations about syllogistic statements are described in this paper . one is the so-called set-based interpretation , which assumes that quantified statements and syllogisms talk about quantity-relationships between sets . the other one , the so-called conditional interpretation , assumes that quantified propositions talk about conditional propositions and how strong are the links between the antecedent and the consequent . both interpretations are compared attending to three different questions ( existential import , singular statements and non-proportional quantifiers ) from the point of view of their impact on the further development of this type of reasoning ."}
{"title": "skipflow : incorporating neural coherence features for end-to-end automatic text scoring", "abstract": "deep learning has demonstrated tremendous potential for automatic text scoring ( ats ) tasks . in this paper , we describe a new neural architecture that enhances vanilla neural network models with auxiliary neural coherence features . our new method proposes a new \\textsc { skipflow } mechanism that models relationships between snapshots of the hidden representations of a long short-term memory ( lstm ) network as it reads . subsequently , the semantic relationships between multiple snapshots are used as auxiliary features for prediction . this has two main benefits . firstly , essays are typically long sequences and therefore the memorization capability of the lstm network may be insufficient . implicit access to multiple snapshots can alleviate this problem by acting as a protection against vanishing gradients . the parameters of the \\textsc { skipflow } mechanism also acts as an auxiliary memory . secondly , modeling relationships between multiple positions allows our model to learn features that represent and approximate textual coherence . in our model , we call this \\textit { neural coherence } features . overall , we present a unified deep learning architecture that generates neural coherence features as it reads in an end-to-end fashion . our approach demonstrates state-of-the-art performance on the benchmark asap dataset , outperforming not only feature engineering baselines but also other deep learning models ."}
{"title": "learning bayesian networks : the combination of knowledge and statistical data", "abstract": "we describe algorithms for learning bayesian networks from a combination of user knowledge and statistical data . the algorithms have two components : a scoring metric and a search procedure . the scoring metric takes a network structure , statistical data , and a user 's prior knowledge , and returns a score proportional to the posterior probability of the network structure given the data . the search procedure generates networks for evaluation by the scoring metric . our contributions are threefold . first , we identify two important properties of metrics , which we call event equivalence and parameter modularity . these properties have been mostly ignored , but when combined , greatly simplify the encoding of a user 's prior knowledge . in particular , a user can express her knowledge-for the most part-as a single prior bayesian network for the domain . second , we describe local search and annealing algorithms to be used in conjunction with scoring metrics . in the special case where each node has at most one parent , we show that heuristic search can be replaced with a polynomial algorithm to identify the networks with the highest score . third , we describe a methodology for evaluating bayesian-network learning algorithms . we apply this approach to a comparison of metrics and search procedures ."}
{"title": "ocrapose ii : an ocr-based indoor positioning system using mobile phone images", "abstract": "in this paper , we propose an ocr ( optical character recognition ) -based localization system called ocrapose ii , which is applicable in a number of indoor scenarios including office buildings , parkings , airports , grocery stores , etc . in these scenarios , characters ( i.e . texts or numbers ) can be used as suitable distinctive landmarks for localization . the proposed system takes advantage of ocr to read these characters in the query still images and provides a rough location estimate using a floor plan . then , it finds depth and angle-of-view of the query using the information provided by the ocr engine in order to refine the location estimate . we derive novel formulas for the query angle-of-view and depth estimation using image line segments and the ocr box information . we demonstrate the applicability and effectiveness of the proposed system through experiments in indoor scenarios . it is shown that our system demonstrates better performance compared to the state-of-the-art benchmarks in terms of location recognition rate and average localization error specially under sparse database condition ."}
{"title": "ontology-based queries over cancer data", "abstract": "the ever-increasing amount of data in biomedical research , and in cancer research in particular , needs to be managed to support efficient data access , exchange and integration . existing software infrastructures , such cagrid , support access to distributed information annotated with a domain ontology . however , cagrid 's current querying functionality depends on the structure of individual data resources without exploiting the semantic annotations . in this paper , we present the design and development of an ontology-based querying functionality that consists of : the generation of owl2 ontologies from the underlying data resources metadata and a query rewriting and translation process based on reasoning , which converts a query at the domain ontology level into queries at the software infrastructure level . we present a detailed analysis of our approach as well as an extensive performance evaluation . while the implementation and evaluation was performed for the cagrid infrastructure , the approach could be applicable to other model and metadata-driven environments for data sharing ."}
{"title": "summable reparameterizations of wasserstein critics in the one-dimensional setting", "abstract": "generative adversarial networks ( gans ) are an exciting alternative to algorithms for solving density estimation problems -- -using data to assess how likely samples are to be drawn from the same distribution . instead of explicitly computing these probabilities , gans learn a generator that can match the given probabilistic source . this paper looks particularly at this matching capability in the context of problems with one-dimensional outputs . we identify a class of function decompositions with properties that make them well suited to the critic role in a leading approach to gans known as wasserstein gans . we show that taylor and fourier series decompositions belong to our class , provide examples of these critics outperforming standard gan approaches , and suggest how they can be scaled to higher dimensional problems in the future ."}
{"title": "robust rigid point registration based on convolution of adaptive gaussian mixture models", "abstract": "matching 3d rigid point clouds in complex environments robustly and accurately is still a core technique used in many applications . this paper proposes a new architecture combining error estimation from sample covariances and dual global probability alignment based on the convolution of adaptive gaussian mixture models ( gmm ) from point clouds . firstly , a novel adaptive gmm is defined using probability distributions from the corresponding points . then rigid point cloud alignment is performed by maximizing the global probability from the convolution of dual adaptive gmms in the whole 2d or 3d space , which can be efficiently optimized and has a large zone of accurate convergence . thousands of trials have been conducted on 200 models from public 2d and 3d datasets to demonstrate superior robustness and accuracy in complex environments with unpredictable noise , outliers , occlusion , initial rotation , shape and missing points ."}
{"title": "hierarchical multiclass decompositions with application to authorship determination", "abstract": "this paper is mainly concerned with the question of how to decompose multiclass classification problems into binary subproblems . we extend known jensen-shannon bounds on the bayes risk of binary problems to hierarchical multiclass problems and use these bounds to develop a heuristic procedure for constructing hierarchical multiclass decomposition for multinomials . we test our method and compare it to the well known `` all-pairs '' decomposition . our tests are performed using a new authorship determination benchmark test of machine learning authors . the new method consistently outperforms the all-pairs decomposition when the number of classes is small and breaks even on larger multiclass problems . using both methods , the classification accuracy we achieve , using an svm over a feature set consisting of both high frequency single tokens and high frequency token-pairs , appears to be exceptionally high compared to known results in authorship determination ."}
{"title": "existential rule languages with finite chase : complexity and expressiveness", "abstract": "finite chase , or alternatively chase termination , is an important condition to ensure the decidability of existential rule languages . in the past few years , a number of rule languages with finite chase have been studied . in this work , we propose a novel approach for classifying the rule languages with finite chase . using this approach , a family of decidable rule languages , which extend the existing languages with the finite chase property , are naturally defined . we then study the complexity of these languages . although all of them are tractable for data complexity , we show that their combined complexity can be arbitrarily high . furthermore , we prove that all the rule languages with finite chase that extend the weakly acyclic language are of the same expressiveness as the weakly acyclic one , while rule languages with higher combined complexity are in general more succinct than those with lower combined complexity ."}
{"title": "des : a challenge problem for nonmonotonic reasoning systems", "abstract": "the us data encryption standard , des for short , is put forward as an interesting benchmark problem for nonmonotonic reasoning systems because ( i ) it provides a set of test cases of industrial relevance which shares features of randomly generated problems and real-world problems , ( ii ) the representation of des using normal logic programs with the stable model semantics is simple and easy to understand , and ( iii ) this subclass of logic programs can be seen as an interesting special case for many other formalizations of nonmonotonic reasoning . in this paper we present two encodings of des as logic programs : a direct one out of the standard specifications and an optimized one extending the work of massacci and marraro . the computational properties of the encodings are studied by using them for des key search with the smodels system as the implementation of the stable model semantics . results indicate that the encodings and smodels are quite competitive : they outperform state-of-the-art sat-checkers working with an optimized encoding of des into sat and are comparable with a sat-checker that is customized and tuned for the optimized sat encoding ."}
{"title": "constraint propagation as information maximization", "abstract": "this paper draws on diverse areas of computer science to develop a unified view of computation : ( 1 ) optimization in operations research , where a numerical objective function is maximized under constraints , is generalized from the numerical total order to a non-numerical partial order that can be interpreted in terms of information . ( 2 ) relations are generalized so that there are relations of which the constituent tuples have numerical indexes , whereas in other relations these indexes are variables . the distinction is essential in our definition of constraint satisfaction problems . ( 3 ) constraint satisfaction problems are formulated in terms of semantics of conjunctions of atomic formulas of predicate logic . ( 4 ) approximation structures , which are available for several important domains , are applied to solutions of constraint satisfaction problems . as application we treat constraint satisfaction problems over reals . these cover a large part of numerical analysis , most significantly nonlinear equations and inequalities . the chaotic algorithm analyzed in the paper combines the efficiency of floating-point computation with the correctness guarantees of arising from our logico-mathematical model of constraint-satisfaction problems ."}
{"title": "nesta , the nicta energy system test case archive", "abstract": "in recent years the power systems research community has seen an explosion of work applying operations research techniques to challenging power network optimization problems . regardless of the application under consideration , all of these works rely on power system test cases for evaluation and validation . however , many of the well established power system test cases were developed as far back as the 1960s with the aim of testing ac power flow algorithms . it is unclear if these power flow test cases are suitable for power system optimization studies . this report surveys all of the publicly available ac transmission system test cases , to the best of our knowledge , and assess their suitability for optimization tasks . it finds that many of the traditional test cases are missing key network operation constraints , such as line thermal limits and generator capability curves . to incorporate these missing constraints , data driven models are developed from a variety of publicly available data sources . the resulting extended test cases form a compressive archive , nesta , for the evaluation and validation of power system optimization algorithms ."}
{"title": "automated reasoning and presentation support for formalizing mathematics in mizar", "abstract": "this paper presents a combination of several automated reasoning and proof presentation tools with the mizar system for formalization of mathematics . the combination forms an online service called mizar , similar to the systemontptp service for first-order automated reasoning . the main differences to systemontptp are the use of the mizar language that is oriented towards human mathematicians ( rather than the pure first-order logic used in systemontptp ) , and setting the service in the context of the large mizar mathematical library of previous theorems , definitions , and proofs ( rather than the isolated problems that are solved in systemontptp ) . these differences poses new challenges and new opportunities for automated reasoning and for proof presentation tools . this paper describes the overall structure of mizar , and presents the automated reasoning systems and proof presentation tools that are combined to make mizar a useful mathematical service ."}
{"title": "3d-r2n2 : a unified approach for single and multi-view 3d object reconstruction", "abstract": "inspired by the recent success of methods that employ shape priors to achieve robust 3d reconstructions , we propose a novel recurrent neural network architecture that we call the 3d recurrent reconstruction neural network ( 3d-r2n2 ) . the network learns a mapping from images of objects to their underlying 3d shapes from a large collection of synthetic data . our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3d occupancy grid . unlike most of the previous works , our network does not require any image annotations or object class labels for training or testing . our extensive experimental analysis shows that our reconstruction framework i ) outperforms the state-of-the-art methods for single view reconstruction , and ii ) enables the 3d reconstruction of objects in situations when traditional sfm/slam methods fail ( because of lack of texture and/or wide baseline ) ."}
{"title": "transfer from simulation to real world through learning deep inverse dynamics model", "abstract": "developing control policies in simulation is often more practical and safer than directly running experiments in the real world . this applies to policies obtained from planning and optimization , and even more so to policies obtained from reinforcement learning , which is often very data demanding . however , a policy that succeeds in simulation often does n't work when deployed on a real robot . nevertheless , often the overall gist of what the policy does in simulation remains valid in the real world . in this paper we investigate such settings , where the sequence of states traversed in simulation remains reasonable for the real world , even if the details of the controls are not , as could be the case when the key differences lie in detailed friction , contact , mass and geometry properties . during execution , at each time step our approach computes what the simulation-based control policy would do , but then , rather than executing these controls on the real robot , our approach computes what the simulation expects the resulting next state ( s ) will be , and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states . deep models are only as good as their training data , and we also propose an approach for data collection to ( incrementally ) learn the deep inverse dynamics model . our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy , including output error control and gaussian dynamics adaptation ."}
{"title": "conflict management in information fusion with belief functions", "abstract": "in information fusion , the conflict is an important concept . indeed , combining several imperfect experts or sources allows conflict . in the theory of belief functions , this notion has been discussed a lot . the mass appearing on the empty set during the conjunctive combination rule is generally considered as conflict , but that is not really a conflict . some measures of conflict have been proposed and some approaches have been proposed in order to manage this conflict or to decide with conflicting mass functions . we recall in this chapter some of them and we propose a discussion to consider the conflict in information fusion with the theory of belief functions ."}
{"title": "pattern-based constraint satisfaction and logic puzzles", "abstract": "pattern-based constraint satisfaction and logic puzzles develops a pure logic , pattern-based perspective of solving the finite constraint satisfaction problem ( csp ) , with emphasis on finding the `` simplest '' solution . different ways of reasoning with the constraints are formalised by various families of `` resolution rules '' , each of them carrying its own notion of simplicity . a large part of the book illustrates the power of the approach by applying it to various popular logic puzzles . it provides a unified view of how to model and solve them , even though they involve very different types of constraints : obvious symmetric ones in sudoku , non-symmetric but transitive ones ( inequalities ) in futoshiki , topological and geometric ones in map colouring , numbrix and hidato , and even much more complex non-binary arithmetic ones in kakuro ( or cross sums ) . it also shows that the most familiar techniques for these puzzles can indeed be understood as mere application-specific presentations of the general rules . sudoku is used as the main example throughout the book , making it also an advanced level sequel to `` the hidden logic of sudoku '' ( another book by the same author ) , with : many examples of relationships among different rules and of exceptional situations ; comparisons of the resolution potential of various families of rules ; detailed statistics of puzzles hardness ; analysis of extreme instances ."}
{"title": "kara : a system for visualising and visual editing of interpretations for answer-set programs", "abstract": "in answer-set programming ( asp ) , the solutions of a problem are encoded in dedicated models , called answer sets , of a logical theory . these answer sets are computed from the program that represents the theory by means of an asp solver and returned to the user as sets of ground first-order literals . as this type of representation is often cumbersome for the user to interpret , tools like aspviz and idpdraw were developed that allow for visualising answer sets . the tool kara , introduced in this paper , follows these approaches , using asp itself as a language for defining visualisations of interpretations . unlike existing tools that position graphic primitives according to static coordinates only , kara allows for more high-level specifications , supporting graph structures , grids , and relative positioning of graphical elements . moreover , generalising the functionality of previous tools , kara provides modifiable visualisations such that interpretations can be manipulated by graphically editing their visualisations . this is realised by resorting to abductive reasoning techniques . kara is part of sealion , a forthcoming integrated development environment ( ide ) for asp ."}
{"title": "algebras of measurements : the logical structure of quantum mechanics", "abstract": "in quantum physics , a measurement is represented by a projection on some closed subspace of a hilbert space . we study algebras of operators that abstract from the algebra of projections on closed subspaces of a hilbert space . the properties of such operators are justified on epistemological grounds . commutation of measurements is a central topic of interest . classical logical systems may be viewed as measurement algebras in which all measurements commute . keywords : quantum measurements , measurement algebras , quantum logic . pacs : 02.10.-v ."}
{"title": "exhaustive and efficient constraint propagation : a semi-supervised learning perspective and its applications", "abstract": "this paper presents a novel pairwise constraint propagation approach by decomposing the challenging constraint propagation problem into a set of independent semi-supervised learning subproblems which can be solved in quadratic time using label propagation based on k-nearest neighbor graphs . considering that this time cost is proportional to the number of all possible pairwise constraints , our approach actually provides an efficient solution for exhaustively propagating pairwise constraints throughout the entire dataset . the resulting exhaustive set of propagated pairwise constraints are further used to adjust the similarity matrix for constrained spectral clustering . other than the traditional constraint propagation on single-source data , our approach is also extended to more challenging constraint propagation on multi-source data where each pairwise constraint is defined over a pair of data points from different sources . this multi-source constraint propagation has an important application to cross-modal multimedia retrieval . extensive results have shown the superior performance of our approach ."}
{"title": "alternating directions dual decomposition", "abstract": "we propose ad3 , a new algorithm for approximate maximum a posteriori ( map ) inference on factor graphs based on the alternating directions method of multipliers . like dual decomposition algorithms , ad3 uses worker nodes to iteratively solve local subproblems and a controller node to combine these local solutions into a global update . the key characteristic of ad3 is that each local subproblem has a quadratic regularizer , leading to a faster consensus than subgradient-based dual decomposition , both theoretically and in practice . we provide closed-form solutions for these ad3 subproblems for binary pairwise factors and factors imposing first-order logic constraints . for arbitrary factors ( large or combinatorial ) , we introduce an active set method which requires only an oracle for computing a local map configuration , making ad3 applicable to a wide range of problems . experiments on synthetic and realworld problems show that ad3 compares favorably with the state-of-the-art ."}
{"title": "optimization with parity constraints : from binary codes to discrete integration", "abstract": "many probabilistic inference tasks involve summations over exponentially large sets . recently , it has been shown that these problems can be reduced to solving a polynomial number of map inference queries for a model augmented with randomly generated parity constraints . by exploiting a connection with max-likelihood decoding of binary codes , we show that these optimizations are computationally hard . inspired by iterative message passing decoding algorithms , we propose an integer linear programming ( ilp ) formulation for the problem , enhanced with new sparsification techniques to improve decoding performance . by solving the ilp through a sequence of lp relaxations , we get both lower and upper bounds on the partition function , which hold with high probability and are much tighter than those obtained with variational methods ."}
{"title": "reinforcement learning applied to single neuron", "abstract": "this paper extends the reinforcement learning ideas into the multi-agents system , which is far more complicated than the previously studied single-agent system . we studied two different multi-agents systems . one is the fully-connected neural network consists of multiple single neurons . another one is the simplified mechanical arm system which is controlled by multiple neurons . we suppose that each neuron is like an agent and it can do gibbs sampling of the posterior probability of stimulus features . the policy is optimized in a way that the cumulative global rewards are maximized . the algorithm for the second system is based on the same idea but we incorporate the physics model into the constraints . the simulation results show that for the first system our algorithm converges well . for the second system it does not converge well in a reasonable simulation time length . in summary , we took the initial endeavor to study the reinforcement learning for multi-agents system . the computational complexity is always an issue and significant amount of works have to be done in order to better understand the problem ."}
{"title": "a comparative quantitative analysis of contemporary big data clustering algorithms for market segmentation in hospitality industry", "abstract": "the hospitality industry is one of the data-rich industries that receives huge volumes of data streaming at high velocity with considerably variety , veracity , and variability . these properties make the data analysis in the hospitality industry a big data problem . meeting the customers ' expectations is a key factor in the hospitality industry to grasp the customers ' loyalty . to achieve this goal , marketing professionals in this industry actively look for ways to utilize their data in the best possible manner and advance their data analytic solutions , such as identifying a unique market segmentation clustering and developing a recommendation system . in this paper , we present a comprehensive literature review of existing big data clustering algorithms and their advantages and disadvantages for various use cases . we implement the existing big data clustering algorithms and provide a quantitative comparison of the performance of different clustering algorithms for different scenarios . we also present our insights and recommendations regarding the suitability of different big data clustering algorithms for different use cases . these recommendations will be helpful for hoteliers in selecting the appropriate market segmentation clustering algorithm for different clustering datasets to improve the customer experience and maximize the hotel revenue ."}
{"title": "venture : a higher-order probabilistic programming platform with programmable inference", "abstract": "we describe venture , an interactive virtual machine for probabilistic programming that aims to be sufficiently expressive , extensible , and efficient for general-purpose use . like church , probabilistic models and inference problems in venture are specified via a turing-complete , higher-order probabilistic language descended from lisp . unlike church , venture also provides a compositional language for custom inference strategies built out of scalable exact and approximate techniques . we also describe four key aspects of venture 's implementation that build on ideas from probabilistic graphical models . first , we describe the stochastic procedure interface ( spi ) that specifies and encapsulates primitive random variables . the spi supports custom control flow , higher-order probabilistic procedures , partially exchangeable sequences and `` likelihood-free '' stochastic simulators . it also supports external models that do inference over latent variables hidden from venture . second , we describe probabilistic execution traces ( pets ) , which represent execution histories of venture programs . pets capture conditional dependencies , existential dependencies and exchangeable coupling . third , we describe partitions of execution histories called scaffolds that factor global inference problems into coherent sub-problems . finally , we describe a family of stochastic regeneration algorithms for efficiently modifying pet fragments contained within scaffolds . stochastic regeneration linear runtime scaling in cases where many previous approaches scaled quadratically . we show how to use stochastic regeneration and the spi to implement general-purpose inference strategies such as metropolis-hastings , gibbs sampling , and blocked proposals based on particle markov chain monte carlo and mean-field variational inference techniques ."}
{"title": "now playing : continuous low-power music recognition", "abstract": "existing music recognition applications require a connection to a server that performs the actual recognition . in this paper we present a low-power music recognizer that runs entirely on a mobile device and automatically recognizes music without user interaction . to reduce battery consumption , a small music detector runs continuously on the mobile device 's dsp chip and wakes up the main application processor only when it is confident that music is present . once woken , the recognizer on the application processor is provided with a few seconds of audio which is fingerprinted and compared to the stored fingerprints in the on-device fingerprint database of tens of thousands of songs . our presented system , now playing , has a daily battery usage of less than 1 % on average , respects user privacy by running entirely on-device and can passively recognize a wide range of music ."}
{"title": "bounded policy synthesis for pomdps with safe-reachability objectives", "abstract": "planning robust executions under uncertainty is a fundamental challenge for building autonomous robots . partially observable markov decision processes ( pomdps ) provide a standard framework for modeling uncertainty in many robot applications . a key algorithmic problem for pomdps is policy synthesis . while this problem has traditionally been posed w.r.t . optimality objectives , many robot applications are better modeled by pomdps where the objective is a boolean requirement . in this paper , we study the latter problem in a setting where the requirement is a safe-reachability property , which states that with a probability above a certain threshold , it is possible to eventually reach a goal state while satisfying a safety requirement . the central challenge in our problem is that it requires reasoning over a vast space of probability distributions . what 's more , it has been shown that policy synthesis of pomdps with reachability objectives is undecidable in general . to address these challenges , we introduce the notion of a goal-constrained belief space , which only contains beliefs ( probability distributions over states ) reachable from the initial belief under desired executions . this constrained space is generally much smaller than the original belief space . our approach compactly represents this space over a bounded horizon using symbolic constraints , and employs an incremental satisfiability modulo theories ( smt ) solver to efficiently search for a valid policy over it . we evaluate our method using a case study involving a partially observable robotics domain with uncertain obstacles . our results suggest that it is possible to synthesize policies over large belief spaces with a small number of smt solver calls by focusing on goal-constrained belief space , and our method o ers a stronger guarantee of both safety and reachability than alternative unconstrained/constrained pomdp formulations ."}
{"title": "a deeper look at experience replay", "abstract": "experience replay plays an important role in the success of deep reinforcement learning ( rl ) by helping stabilize the neural networks . it has become a new norm in deep rl algorithms . in this paper , however , we showcase that varying the size of the experience replay buffer can hurt the performance even in very simple tasks . the size of the replay buffer is actually a hyper-parameter which needs careful tuning . moreover , our study of experience replay leads to the formulation of the combined dqn algorithm , which can significantly outperform primitive dqn in some tasks ."}
{"title": "deep reinforcement learning-based image captioning with embedding reward", "abstract": "image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language . recent advances in deep neural networks have substantially improved the performance of this task . most state-of-the-art approaches follow an encoder-decoder framework , which generates captions using a sequential recurrent prediction model . however , in this paper , we introduce a novel decision-making framework for image captioning . we utilize a `` policy network '' and a `` value network '' to collaboratively generate captions . the policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state . additionally , the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state . in essence , it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions . we train both networks using an actor-critic reinforcement learning model , with a novel reward defined by visual-semantic embedding . extensive experiments and analyses on the microsoft coco dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics ."}
{"title": "scalable parallel numerical constraint solver using global load balancing", "abstract": "we present a scalable parallel solver for numerical constraint satisfaction problems ( ncsps ) . our parallelization scheme consists of homogeneous worker solvers , each of which runs on an available core and communicates with others via the global load balancing ( glb ) method . the parallel solver is implemented with x10 that provides an implementation of glb as a library . in experiments , several ncsps from the literature were solved and attained up to 516-fold speedup using 600 cores of the tsubame2.5 supercomputer ."}
{"title": "minimizing maximum regret in commitment constrained sequential decision making", "abstract": "in cooperative multiagent planning , it can often be beneficial for an agent to make commitments about aspects of its behavior to others , allowing them in turn to plan their own behaviors without taking the agent 's detailed behavior into account . extending previous work in the bayesian setting , we consider instead a worst-case setting in which the agent has a set of possible environments ( mdps ) it could be in , and develop a commitment semantics that allows for probabilistic guarantees on the agent 's behavior in any of the environments it could end up facing . crucially , an agent receives observations ( of reward and state transitions ) that allow it to potentially eliminate possible environments and thus obtain higher utility by adapting its policy to the history of observations . we develop algorithms and provide theory and some preliminary empirical results showing that they ensure an agent meets its commitments with history-dependent policies while minimizing maximum regret over the possible environments ."}
{"title": "three iqs of ai systems and their testing methods", "abstract": "the rapid development of artificial intelligence has brought the artificial intelligence threat theory as well as the problem about how to evaluate the intelligence level of intelligent products . both need to find a quantitative method to evaluate the intelligence level of intelligence systems , including human intelligence . based on the standard intelligence system and the extended von neumann architecture , this paper proposes general iq , service iq and value iq evaluation methods for intelligence systems , depending on different evaluation purposes . among them , the general iq of intelligence systems is to answer the question of whether the artificial intelligence can surpass the human intelligence , which is reflected in putting the intelligence systems on an equal status and conducting the unified evaluation . the service iq and value iq of intelligence systems are used to answer the question of how the intelligent products can better serve the human , reflecting the intelligence and required cost of each intelligence system as a product in the process of serving human ."}
{"title": "constraint exploration and envelope of simulation trajectories", "abstract": "the implicit theory that a simulation represents is precisely not in the individual choices but rather in the 'envelope ' of possible trajectories - what is important is the shape of the whole envelope . typically a huge amount of computation is required when experimenting with factors bearing on the dynamics of a simulation to tease out what affects the shape of this envelope . in this paper we present a methodology aimed at systematically exploring this envelope . we propose a method for searching for tendencies and proving their necessity relative to a range of parameterisations of the model and agents ' choices , and to the logic of the simulation language . the exploration consists of a forward chaining generation of the trajectories associated to and constrained by such a range of parameterisations and choices . additionally , we propose a computational procedure that helps implement this exploration by translating a multi agent system simulation into a constraint-based search over possible trajectories by 'compiling ' the simulation rules into a more specific form , namely by partitioning the simulation rules using appropriate modularity in the simulation . an example of this procedure is exhibited . keywords : constraint search , constraint logic programming , proof , emergence , tendencies"}
{"title": "integration of navigation and action selection functionalities in a computational model of cortico-basal ganglia-thalamo-cortical loops", "abstract": "this article describes a biomimetic control architecture affording an animat both action selection and navigation functionalities . it satisfies the survival constraint of an artificial metabolism and supports several complementary navigation strategies . it builds upon an action selection model based on the basal ganglia of the vertebrate brain , using two interconnected cortico-basal ganglia-thalamo-cortical loops : a ventral one concerned with appetitive actions and a dorsal one dedicated to consummatory actions . the performances of the resulting model are evaluated in simulation . the experiments assess the prolonged survival permitted by the use of high level navigation strategies and the complementarity of navigation strategies in dynamic environments . the correctness of the behavioral choices in situations of antagonistic or synergetic internal states are also tested . finally , the modelling choices are discussed with regard to their biomimetic plausibility , while the experimental results are estimated in terms of animat adaptivity ."}
{"title": "taste or addiction ? : using play logs to infer song selection motivation", "abstract": "online music services are increasing in popularity . they enable us to analyze people 's music listening behavior based on play logs . although it is known that people listen to music based on topic ( e.g. , rock or jazz ) , we assume that when a user is addicted to an artist , s/he chooses the artist 's songs regardless of topic . based on this assumption , in this paper , we propose a probabilistic model to analyze people 's music listening behavior . our main contributions are three-fold . first , to the best of our knowledge , this is the first study modeling music listening behavior by taking into account the influence of addiction to artists . second , by using real-world datasets of play logs , we showed the effectiveness of our proposed model . third , we carried out qualitative experiments and showed that taking addiction into account enables us to analyze music listening behavior from a new viewpoint in terms of how people listen to music according to the time of day , how an artist 's songs are listened to by people , etc . we also discuss the possibility of applying the analysis results to applications such as artist similarity computation and song recommendation ."}
{"title": "strong asymptotic assertions for discrete mdl in regression and classification", "abstract": "we study the properties of the mdl ( or maximum penalized complexity ) estimator for regression and classification , where the underlying model class is countable . we show in particular a finite bound on the hellinger losses under the only assumption that there is a `` true '' model contained in the class . this implies almost sure convergence of the predictive distribution to the true one at a fast rate . it corresponds to solomonoff 's central theorem of universal induction , however with a bound that is exponentially larger ."}
{"title": "deep knowledge tracing", "abstract": "knowledge tracing -- -where a machine models the knowledge of a student as they interact with coursework -- -is a well established problem in computer supported education . though effectively modeling student knowledge would have high educational impact , the task has many inherent challenges . in this paper we explore the utility of using recurrent neural networks ( rnns ) to model student learning . the rnn family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge , and can capture more complex representations of student knowledge . using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets . moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks . these results suggest a promising new line of research for knowledge tracing and an exemplary application task for rnns ."}
{"title": "learning diagnostic policies from examples by systematic search", "abstract": "a diagnostic policy specifies what test to perform next , based on the results of previous tests , and when to stop and make a diagnosis . cost-sensitive diagnostic policies perform tradeoffs between ( a ) the cost of tests and ( b ) the cost of misdiagnoses . an optimal diagnostic policy minimizes the expected total cost . we formalize this diagnosis process as a markov decision process ( mdp ) . we investigate two types of algorithms for solving this mdp : systematic search based on ao* algorithm and greedy search ( particularly the value of information method ) . we investigate the issue of learning the mdp probabilities from examples , but only as they are relevant to the search for good policies . we do not learn nor assume a bayesian network for the diagnosis process . regularizers are developed to control overfitting and speed up the search . this research is the first that integrates overfitting prevention into systematic search . the paper has two contributions : it discusses the factors that make systematic search feasible for diagnosis , and it shows experimentally , on benchmark data sets , that systematic search methods produce better diagnostic policies than greedy methods ."}
{"title": "simulated annealing for weighted polygon packing", "abstract": "in this paper we present a new algorithm for a layout optimization problem : this concerns the placement of weighted polygons inside a circular container , the two objectives being to minimize imbalance of mass and to minimize the radius of the container . this problem carries real practical significance in industrial applications ( such as the design of satellites ) , as well as being of significant theoretical interest . previous work has dealt with circular or rectangular objects , but here we deal with the more realistic case where objects may be represented as polygons and the polygons are allowed to rotate . we present a solution based on simulated annealing and first test it on instances with known optima . our results show that the algorithm obtains container radii that are close to optimal . we also compare our method with existing algorithms for the ( special ) rectangular case . experimental results show that our approach out-performs these methods in terms of solution quality ."}
{"title": "non-sentential utterances in dialogue : experiments in classification and interpretation", "abstract": "non-sentential utterances ( nsus ) are utterances that lack a complete sentential form but whose meaning can be inferred from the dialogue context , such as `` ok '' , `` where ? `` , `` probably at his apartment '' . the interpretation of non-sentential utterances is an important problem in computational linguistics since they constitute a frequent phenomena in dialogue and they are intrinsically context-dependent . the interpretation of nsus is the task of retrieving their full semantic content from their form and the dialogue context . the first half of this thesis is devoted to the nsu classification task . our work builds upon fern\\'andez et al . ( 2007 ) which present a series of machine-learning experiments on the classification of nsus . we extended their approach with a combination of new features and semi-supervised learning techniques . the empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance . the consecutive , yet independent , problem is how to infer an appropriate semantic representation of such nsus on the basis of the dialogue context . fern\\'andez ( 2006 ) formalizes this task in terms of `` resolution rules '' built on top of the type theory with records ( ttr ) . our work is focused on the reimplementation of the resolution rules from fern\\'andez ( 2006 ) with a probabilistic account of the dialogue state . the probabilistic rules formalism lison ( 2014 ) is particularly suited for this task because , similarly to the framework developed by ginzburg ( 2012 ) and fern\\'andez ( 2006 ) , it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation . however , the probabilistic rules can also encode probabilistic knowledge , thereby providing a principled account of ambiguities in the nsu resolution process ."}
{"title": "arriving on time : estimating travel time distributions on large-scale road networks", "abstract": "most optimal routing problems focus on minimizing travel time or distance traveled . oftentimes , a more useful objective is to maximize the probability of on-time arrival , which requires statistical distributions of travel times , rather than just mean values . we propose a method to estimate travel time distributions on large-scale road networks , using probe vehicle data collected from gps . we present a framework that works with large input of data , and scales linearly with the size of the network . leveraging the planar topology of the graph , the method computes efficiently the time correlations between neighboring streets . first , raw probe vehicle traces are compressed into pairs of travel times and number of stops for each traversed road segment using a ` stop-and-go ' algorithm developed for this work . the compressed data is then used as input for training a path travel time model , which couples a markov model along with a gaussian markov random field . finally , scalable inference algorithms are developed for obtaining path travel time distributions from the composite mm-gmrf model . we illustrate the accuracy and scalability of our model on a 505,000 road link network spanning the san francisco bay area ."}
{"title": "a review of network traffic analysis and prediction techniques", "abstract": "analysis and prediction of network traffic has applications in wide comprehensive set of areas and has newly attracted significant number of studies . different kinds of experiments are conducted and summarized to identify various problems in existing computer network applications . network traffic analysis and prediction is a proactive approach to ensure secure , reliable and qualitative network communication . various techniques are proposed and experimented for analyzing network traffic including neural network based techniques to data mining techniques . similarly , various linear and non-linear models are proposed for network traffic prediction . several interesting combinations of network analysis and prediction techniques are implemented to attain efficient and effective results . this paper presents a survey on various such network analysis and traffic prediction techniques . the uniqueness and rules of previous studies are investigated . moreover , various accomplished areas of analysis and prediction of network traffic have been summed ."}
{"title": "dependency networks for collaborative filtering and data visualization", "abstract": "we describe a graphical model for probabilistic relationships -- -an alternative to the bayesian network -- -called a dependency network . the graph of a dependency network , unlike a bayesian network , is potentially cyclic . the probability component of a dependency network , like a bayesian network , is a set of conditional distributions , one for each node given its parents . we identify several basic properties of this representation and describe a computationally efficient procedure for learning the graph and probability components from data . we describe the application of this representation to probabilistic inference , collaborative filtering ( the task of predicting preferences ) , and the visualization of acausal predictive relationships ."}
{"title": "the formal-logical characterisation of lies , deception , and associated notions", "abstract": "defining various dishonest notions in a formal way is a key step to enable intelligent agents to act in untrustworthy environments . this review evaluates the literature for this topic by looking at formal definitions based on modal logic as well as other formal approaches . criteria from philosophical groundwork is used to assess the definitions for correctness and completeness . the key contribution of this review is to show that only a few definitions fully comply with this gold standard and to point out the missing steps towards a successful application of these definitions in an actual agent environment ."}
{"title": "how to complete an interactive configuration process ?", "abstract": "when configuring customizable software , it is useful to provide interactive tool-support that ensures that the configuration does not breach given constraints . but , when is a configuration complete and how can the tool help the user to complete it ? we formalize this problem and relate it to concepts from non-monotonic reasoning well researched in artificial intelligence . the results are interesting for both practitioners and theoreticians . practitioners will find a technique facilitating an interactive configuration process and experiments supporting feasibility of the approach . theoreticians will find links between well-known formal concepts and a concrete practical application ."}
{"title": "policy-contingent abstraction for robust robot control", "abstract": "this paper presents a scalable control algorithm that enables a deployed mobile robot system to make high-level decisions under full consideration of its probabilistic belief . our approach is based on insights from the rich literature of hierarchical controllers and hierarchical mdps . the resulting controller has been successfully deployed in a nursing facility near pittsburgh , pa. to the best of our knowledge , this work is a unique instance of applying pomdps to high-level robotic control problems ."}
{"title": "performance analysis of anfis in short term wind speed prediction", "abstract": "results are presented on the performance of adaptive neuro-fuzzy inference system ( anfis ) for wind velocity forecasts in the isthmus of tehuantepec region in the state of oaxaca , mexico . the data bank was provided by the meteorological station located at the university of isthmus , tehuantepec campus , and this data bank covers the period from 2008 to 2011. three data models were constructed to carry out 16 , 24 and 48 hours forecasts using the following variables : wind velocity , temperature , barometric pressure , and date . the performance measure for the three models is the mean standard error ( mse ) . in this work , performance analysis in short-term prediction is presented , because it is essential in order to define an adequate wind speed model for eolian parks , where a right planning provide economic benefits ."}
{"title": "an application of answer set programming to the field of second language acquisition", "abstract": "this paper explores the contributions of answer set programming ( asp ) to the study of an established theory from the field of second language acquisition : input processing . the theory describes default strategies that learners of a second language use in extracting meaning out of a text , based on their knowledge of the second language and their background knowledge about the world . we formalized this theory in asp , and as a result we were able to determine opportunities for refining its natural language description , as well as directions for future theory development . we applied our model to automating the prediction of how learners of english would interpret sentences containing the passive voice . we present a system , pias , that uses these predictions to assist language instructors in designing teaching materials . to appear in theory and practice of logic programming ( tplp ) ."}
{"title": "scene-centric joint parsing of cross-view videos", "abstract": "cross-view video understanding is an important yet under-explored area in computer vision . in this paper , we introduce a joint parsing framework that integrates view-centric proposals into scene-centric parse graphs that represent a coherent scene-centric understanding of cross-view scenes . our key observations are that overlapping fields of views embed rich appearance and geometry correlations and that knowledge fragments corresponding to individual vision tasks are governed by consistency constraints available in commonsense knowledge . the proposed joint parsing framework represents such correlations and constraints explicitly and generates semantic scene-centric parse graphs . quantitative experiments show that scene-centric predictions in the parse graph outperform view-centric predictions ."}
{"title": "user modeling combining access logs , page content and semantics", "abstract": "the paper proposes an approach to modeling users of large web sites based on combining different data sources : access logs and content of the accessed pages are combined with semantic information about the web pages , the users and the accesses of the users to the web site . the assumption is that we are dealing with a large web site providing content to a large number of users accessing the site . the proposed approach represents each user by a set of features derived from the different data sources , where some feature values may be missing for some users . it further enables user modeling based on the provided characteristics of the targeted user subset . the approach is evaluated on real-world data where we compare performance of the automatic assignment of a user to a predefined user segment when different data sources are used to represent the users ."}
{"title": "a linear approximation method for probabilistic inference", "abstract": "an approximation method is presented for probabilistic inference with continuous random variables . these problems can arise in many practical problems , in particular where there are `` second order '' probabilities . the approximation , based on the gaussian influence diagram , iterates over linear approximations to the inference problem ."}
{"title": "alpha-expansion is exact on stable instances", "abstract": "approximate algorithms for structured prediction problems -- -such as the popular alpha-expansion algorithm ( boykov et al . 2001 ) in computer vision -- -typically far exceed their theoretical performance guarantees on real-world instances . these algorithms often find solutions that are very close to optimal . the goal of this paper is to partially explain the performance of alpha-expansion on map inference in ferromagnetic potts models ( fpms ) . our main results use the connection between energy minimization in fpms and the uniform metric labeling problem to give a stability condition under which the alpha-expansion algorithm provably recovers the optimal map solution . this theoretical result complements the numerous empirical observations of alpha-expansion 's performance . additionally , we give a different stability condition under which an lp-based algorithm recovers the optimal solution ."}
{"title": "conflict-directed backjumping revisited", "abstract": "in recent years , many improvements to backtracking algorithms for solving constraint satisfaction problems have been proposed . the techniques for improving backtracking algorithms can be conveniently classified as look-ahead schemes and look-back schemes . unfortunately , look-ahead and look-back schemes are not entirely orthogonal as it has been observed empirically that the enhancement of look-ahead techniques is sometimes counterproductive to the effects of look-back techniques . in this paper , we focus on the relationship between the two most important look-ahead techniques -- -using a variable ordering heuristic and maintaining a level of local consistency during the backtracking search -- -and the look-back technique of conflict-directed backjumping ( cbj ) . we show that there exists a `` perfect '' dynamic variable ordering such that cbj becomes redundant . we also show theoretically that as the level of local consistency that is maintained in the backtracking search is increased , the less that backjumping will be an improvement . our theoretical results partially explain why a backtracking algorithm doing more in the look-ahead phase can not benefit more from the backjumping look-back scheme . finally , we show empirically that adding cbj to a backtracking algorithm that maintains generalized arc consistency ( gac ) , an algorithm that we refer to as gac-cbj , can still provide orders of magnitude speedups . our empirical results contrast with bessiere and regin 's conclusion ( 1996 ) that cbj is useless to an algorithm that maintains arc consistency ."}
{"title": "a branch-and-bound algorithm for mdl learning bayesian networks", "abstract": "this paper extends the work in [ suzuki , 1996 ] and presents an efficient depth-first branch-and-bound algorithm for learning bayesian network structures , based on the minimum description length ( mdl ) principle , for a given ( consistent ) variable ordering . the algorithm exhaustively searches through all network structures and guarantees to find the network with the best mdl score . preliminary experiments show that the algorithm is efficient , and that the time complexity grows slowly with the sample size . the algorithm is useful for empirically studying both the performance of suboptimal heuristic search algorithms and the adequacy of the mdl principle in learning bayesian networks ."}
{"title": "contradiction measures and specificity degrees of basic belief assignments", "abstract": "in the theory of belief functions , many measures of uncertainty have been introduced . however , it is not always easy to understand what these measures really try to represent . in this paper , we re-interpret some measures of uncertainty in the theory of belief functions . we present some interests and drawbacks of the existing measures . on these observations , we introduce a measure of contradiction . therefore , we present some degrees of non-specificity and bayesianity of a mass . we propose a degree of specificity based on the distance between a mass and its most specific associated mass . we also show how to use the degree of specificity to measure the specificity of a fusion rule . illustrations on simple examples are given ."}
{"title": "reducing uncertainty in navigation and exploration", "abstract": "a significant problem in designing mobile robot control systems involves coping with the uncertainty that arises in moving about in an unknown or partially unknown environment and relying on noisy or ambiguous sensor data to acquire knowledge about that environment . we describe a control system that chooses what activity to engage in next on the basis of expectations about how the information re- turned as a result of a given activity will improve 2 its knowledge about the spatial layout of its environment . certain of the higher-level components of the control system are specified in terms of probabilistic decision models whose output is used to mediate the behavior of lower-level control components responsible for movement and sensing ."}
{"title": "studying a set of properties of inconsistency indices for pairwise comparisons", "abstract": "pairwise comparisons between alternatives are a well-established tool to decompose decision problems into smaller and more easily tractable sub-problems . however , due to our limited rationality , the subjective preferences expressed by decision makers over pairs of alternatives can hardly ever be consistent . therefore , several inconsistency indices have been proposed in the literature to quantify the extent of the deviation from complete consistency . only recently , a set of properties has been proposed to define a family of functions representing inconsistency indices . the scope of this paper is twofold . firstly , it expands the set of properties by adding and justifying a new one . secondly , it continues the study of inconsistency indices to check whether or not they satisfy the above mentioned properties . out of the four indices considered in this paper , in its present form , two fail to satisfy some properties . an adjusted version of one index is proposed so that it fulfills them ."}
{"title": "classification of message spreading in a heterogeneous social network", "abstract": "nowadays , social networks such as twitter , facebook and linkedin become increasingly popular . in fact , they introduced new habits , new ways of communication and they collect every day several information that have different sources . most existing research works fo-cus on the analysis of homogeneous social networks , i.e . we have a single type of node and link in the network . however , in the real world , social networks offer several types of nodes and links . hence , with a view to preserve as much information as possible , it is important to consider so-cial networks as heterogeneous and uncertain . the goal of our paper is to classify the social message based on its spreading in the network and the theory of belief functions . the proposed classifier interprets the spread of messages on the network , crossed paths and types of links . we tested our classifier on a real word network that we collected from twitter , and our experiments show the performance of our belief classifier ."}
{"title": "optimal route planning with prioritized task scheduling for auv missions", "abstract": "this paper presents a solution to autonomous underwater vehicles ( auvs ) large scale route planning and task assignment joint problem . given a set of constraints ( e.g. , time ) and a set of task priority values , the goal is to find the optimal route for underwater mission that maximizes the sum of the priorities and minimizes the total risk percentage while meeting the given constraints . making use of the heuristic nature of genetic and swarm intelligence algorithms in solving np-hard graph problems , particle swarm optimization ( pso ) and genetic algorithm ( ga ) are employed to find the optimum solution , where each individual in the population is a candidate solution ( route ) . to evaluate the robustness of the proposed methods , the performance of the all ps and ga algorithms are examined and compared for a number of monte carlo runs . simulation results suggest that the routes generated by both algorithms are feasible and reliable enough , and applicable for underwater motion planning . however , the ga-based route planner produces superior results comparing to the results obtained from the pso based route planner ."}
{"title": "web usage mining using artificial ant colony clustering and genetic programming", "abstract": "the rapid e-commerce growth has made both business community and customers face a new situation . due to intense competition on one hand and the customer 's option to choose from several alternatives business community has realized the necessity of intelligent marketing strategies and relationship management . web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the web . web usage mining has become very critical for effective web site management , creating adaptive web sites , business and support services , personalization , network traffic flow analysis and so on . the study of ant colonies behavior and their self-organizing capabilities is of interest to knowledge retrieval/management and decision support systems sciences , because it provides models of distributed adaptive organization , which are useful to solve difficult optimization , classification , and distributed control problems , among others . in this paper , we propose an ant clustering algorithm to discover web usage patterns ( data clusters ) and a linear genetic programming approach to analyze the visitor trends . empirical results clearly shows that ant colony clustering performs well when compared to a self-organizing map ( for clustering web usage patterns ) even though the performance accuracy is not that efficient when comparared to evolutionary-fuzzy clustering ( i-miner ) approach . keywords : web usage mining , swarm intelligence , ant systems , stigmergy , data-mining , linear genetic programming ."}
{"title": "learning to learn from weak supervision by full supervision", "abstract": "in this paper , we propose a method for training neural networks when we have a large set of data with weak labels and a small amount of data with true labels . in our proposed model , we train two neural networks : a target network , the learner and a confidence network , the meta-learner . the target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated . we propose to control the magnitude of the gradient updates to the target network using the scores provided by the second confidence network , which is trained on a small amount of supervised data . thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model ."}
{"title": "autonomous extracting a hierarchical structure of tasks in reinforcement learning and multi-task reinforcement learning", "abstract": "reinforcement learning ( rl ) , while often powerful , can suffer from slow learning speeds , particularly in high dimensional spaces . the autonomous decomposition of tasks and use of hierarchical methods hold the potential to significantly speed up learning in such domains . this paper proposes a novel practical method that can autonomously decompose tasks , by leveraging association rule mining , which discovers hidden relationship among entities in data mining . we introduce a novel method called arm-hstrl ( association rule mining to extract hierarchical structure of tasks in reinforcement learning ) . it extracts temporal and structural relationships of sub-goals in rl , and multi-task rl . in particular , it finds sub-goals and relationship among them . it is shown the significant efficiency and performance of the proposed method in two main topics of rl ."}
{"title": "exploiting the rule structure for decision making within the independent choice logic", "abstract": "this paper introduces the independent choice logic , and in particular the `` single agent with nature '' instance of the independent choice logic , namely icldt . this is a logical framework for decision making uncertainty that extends both logic programming and stochastic models such as influence diagrams . this paper shows how the representation of a decision problem within the independent choice logic can be exploited to cut down the combinatorics of dynamic programming . one of the main problems with influence diagram evaluation techniques is the need to optimise a decision for all values of the 'parents ' of a decision variable . in this paper we show how the rule based nature of the icldt can be exploited so that we only make distinctions in the values of the information available for a decision that will make a difference to utility ."}
{"title": "marginal sequential monte carlo for doubly intractable models", "abstract": "bayesian inference for models that have an intractable partition function is known as a doubly intractable problem , where standard monte carlo methods are not applicable . the past decade has seen the development of auxiliary variable monte carlo techniques ( m { \\o } ller et al. , 2006 ; murray et al. , 2006 ) for tackling this problem ; these approaches being members of the more general class of pseudo-marginal , or exact-approximate , monte carlo algorithms ( andrieu and roberts , 2009 ) , which make use of unbiased estimates of intractable posteriors . everitt et al . ( 2017 ) investigated the use of exact-approximate importance sampling ( is ) and sequential monte carlo ( smc ) in doubly intractable problems , but focussed only on smc algorithms that used data-point tempering . this paper describes smc samplers that may use alternative sequences of distributions , and describes ways in which likelihood estimates may be improved adaptively as the algorithm progresses , building on ideas from moores et al . ( 2015 ) . this approach is compared with a number of alternative algorithms for doubly intractable problems , including approximate bayesian computation ( abc ) , which we show is closely related to the method of m { \\o } ller et al . ( 2006 ) ."}
{"title": "a denotational semantics for first-order logic", "abstract": "in apt and bezem [ ab99 ] ( see cs.lo/9811017 ) we provided a computational interpretation of first-order formulas over arbitrary interpretations . here we complement this work by introducing a denotational semantics for first-order logic . additionally , by allowing an assignment of a non-ground term to a variable we introduce in this framework logical variables . the semantics combines a number of well-known ideas from the areas of semantics of imperative programming languages and logic programming . in the resulting computational view conjunction corresponds to sequential composition , disjunction to `` do n't know '' nondeterminism , existential quantification to declaration of a local variable , and negation to the `` negation as finite failure '' rule . the soundness result shows correctness of the semantics with respect to the notion of truth . the proof resembles in some aspects the proof of the soundness of the sldnf-resolution ."}
{"title": "a conservation law method in optimization", "abstract": "we propose some algorithms to find local minima in nonconvex optimization and to obtain global minima in some degree from the newton second law without friction . with the key observation of the velocity observable and controllable in the motion , the algorithms simulate the newton second law without friction based on symplectic euler scheme . from the intuitive analysis of analytical solution , we give a theoretical analysis for the high-speed convergence in the algorithm proposed . finally , we propose the experiments for strongly convex function , non-strongly convex function and nonconvex function in high-dimension ."}
{"title": "partially observed , multi-objective markov games", "abstract": "the intent of this research is to generate a set of non-dominated policies from which one of two agents ( the leader ) can select a most preferred policy to control a dynamic system that is also affected by the control decisions of the other agent ( the follower ) . the problem is described by an infinite horizon , partially observed markov game ( pomg ) . at each decision epoch , each agent knows : its past and present states , its past actions , and noise corrupted observations of the other agent 's past and present states . the actions of each agent are determined at each decision epoch based on these data . the leader considers multiple objectives in selecting its policy . the follower considers a single objective in selecting its policy with complete knowledge of and in response to the policy selected by the leader . this leader-follower assumption allows the pomg to be transformed into a specially structured , partially observed markov decision process ( pomdp ) . this pomdp is used to determine the follower 's best response policy . a multi-objective genetic algorithm ( moga ) is used to create the next generation of leader policies based on the fitness measures of each leader policy in the current generation . computing a fitness measure for a leader policy requires a value determination calculation , given the leader policy and the follower 's best response policy . the policies from which the leader can select a most preferred policy are the non-dominated policies of the final generation of leader policies created by the moga . an example is presented that illustrates how these results can be used to support a manager of a liquid egg production process ( the leader ) in selecting a sequence of actions to best control this process over time , given that there is an attacker ( the follower ) who seeks to contaminate the liquid egg production process with a chemical or biological toxin ."}
{"title": "hedera : scalable indexing and exploring entities in wikipedia revision history", "abstract": "much of work in semantic web relying on wikipedia as the main source of knowledge often work on static snapshots of the dataset . the full history of wikipedia revisions , while contains much more useful information , is still difficult to access due to its exceptional volume . to enable further research on this collection , we developed a tool , named hedera , that efficiently extracts semantic information from wikipedia revision history datasets . hedera exploits map-reduce paradigm to achieve rapid extraction , it is able to handle one entire wikipedia articles revision history within a day in a medium-scale cluster , and supports flexible data structures for various kinds of semantic web study ."}
{"title": "objective variables for probabilistic revenue maximization in second-price auctions with reserve", "abstract": "many online companies sell advertisement space in second-price auctions with reserve . in this paper , we develop a probabilistic method to learn a profitable strategy to set the reserve price . we use historical auction data with features to fit a predictor of the best reserve price . this problem is delicate - the structure of the auction is such that a reserve price set too high is much worse than a reserve price set too low . to address this we develop objective variables , a new framework for combining probabilistic modeling with optimal decision-making . objective variables are `` hallucinated observations '' that transform the revenue maximization task into a regularized maximum likelihood estimation problem , which we solve with an em algorithm . this framework enables a variety of prediction mechanisms to set the reserve price . as examples , we study objective variable methods with regression , kernelized regression , and neural networks on simulated and real data . our methods outperform previous approaches both in terms of scalability and profit ."}
{"title": "bayesian prediction for artificial intelligence", "abstract": "this paper shows that the common method used for making predictions under uncertainty in a1 and science is in error . this method is to use currently available data to select the best model from a given class of models-this process is called abduction-and then to use this model to make predictions about future data . the correct method requires averaging over all the models to make a prediction-we call this method transduction . using transduction , an ai system will not give misleading results when basing predictions on small amounts of data , when no model is clearly best . for common classes of models we show that the optimal solution can be given in closed form ."}
{"title": "tongue image constitution recognition based on complexity perception method", "abstract": "background and object : in china , body constitution is highly related to physiological and pathological functions of human body and determines the tendency of the disease , which is of great importance for treatment in clinical medicine . tongue diagnosis , as a key part of traditional chinese medicine inspection , is an important way to recognize the type of constitution.in order to deploy tongue image constitution recognition system on non-invasive mobile device to achieve fast , efficient and accurate constitution recognition , an efficient method is required to deal with the challenge of this kind of complex environment . methods : in this work , we perform the tongue area detection , tongue area calibration and constitution classification using methods which are based on deep convolutional neural network . subject to the variation of inconstant environmental condition , the distribution of the picture is uneven , which has a bad effect on classification performance . to solve this problem , we propose a method based on the complexity of individual instances to divide dataset into two subsets and classify them separately , which is capable of improving classification accuracy . to evaluate the performance of our proposed method , we conduct experiments on three sizes of tongue datasets , in which deep convolutional neural network method and traditional digital image analysis method are respectively applied to extract features for tongue images . the proposed method is combined with the base classifier softmax , svm , and decisiontree respectively . results : as the experiments results shown , our proposed method improves the classification accuracy by 1.135 % on average and achieves 59.99 % constitution classification accuracy . conclusions : experimental results on three datasets show that our proposed method can effectively improve the classification accuracy of tongue constitution recognition ."}
{"title": "application of fuzzy mathematics to speech-to-text conversion by elimination of paralinguistic content", "abstract": "for the past few decades , man has been trying to create an intelligent computer which can talk and respond like he can . the task of creating a system that can talk like a human being is the primary objective of automatic speech recognition . various speech recognition techniques have been developed in theory and have been applied in practice . this paper discusses the problems that have been encountered in developing speech recognition , the techniques that have been applied to automate the task , and a representation of the core problems of present day speech recognition by using fuzzy mathematics ."}
{"title": "on the expressive power of unit resolution", "abstract": "this preliminary report addresses the expressive power of unit resolution regarding input data encoded with partial truth assignments of propositional variables . a characterization of the functions that are computable in this way , which we propose to call propagatable functions , is given . by establishing that propagatable functions can also be computed using monotone circuits , we show that there exist polynomial time complexity propagable functions requiring an exponential amount of clauses to be computed using unit resolution . these results shed new light on studying cnf encodings of np-complete problems in order to solve them using propositional satisfiability algorithms ."}
{"title": "semi-supervised bayesian deep multi-modal emotion recognition", "abstract": "in emotion recognition , it is difficult to recognize human 's emotional states using just a single modality . besides , the annotation of physiological emotional data is particularly expensive . these two aspects make the building of effective emotion recognition model challenging . in this paper , we first build a multi-view deep generative model to simulate the generative process of multi-modality emotional data . by imposing a mixture of gaussians assumption on the posterior approximation of the latent variables , our model can learn the shared deep representation from multiple modalities . to solve the labeled-data-scarcity problem , we further extend our multi-view model to semi-supervised learning scenario by casting the semi-supervised classification problem as a specialized missing data imputation task . our semi-supervised multi-view deep generative framework can leverage both labeled and unlabeled data from multiple modalities , where the weight factor for each modality can be learned automatically . compared with previous emotion recognition methods , our method is more robust and flexible . the experiments conducted on two real multi-modal emotion datasets have demonstrated the superiority of our framework over a number of competitors ."}
{"title": "mojitalk : generating emotional responses at scale", "abstract": "generating emotional language is a key step towards building empathetic natural language processing agents . however , a major challenge for this line of research is the lack of large-scale labeled training data , and previous studies are limited to only small sets of human annotated sentiment labels . additionally , explicitly controlling the emotion and sentiment of generated text is also difficult . in this paper , we take a more radical approach : we exploit the idea of leveraging twitter data that are naturally labeled with emojis . more specifically , we collect a large corpus of twitter conversations that include emojis in the response , and assume the emojis convey the underlying emotions of the sentence . we then introduce a reinforced conditional variational encoder approach to train a deep generative model on these conversations , which allows us to use emojis to control the emotion of the generated text . experimentally , we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions ."}
{"title": "building upon fast multipole methods to detect and model organizations", "abstract": "many models in natural and social sciences are comprised of sets of inter-acting entities whose intensity of interaction decreases with distance . this often leads to structures of interest in these models composed of dense packs of entities . fast multipole methods are a family of methods developed to help with the calculation of a number of computable models such as described above . we propose a method that builds upon fmm to detect and model the dense structures of these systems ."}
{"title": "a new understanding of prediction markets via no-regret learning", "abstract": "we explore the striking mathematical connections that exist between market scoring rules , cost function based prediction markets , and no-regret learning . we show that any cost function based prediction market can be interpreted as an algorithm for the commonly studied problem of learning from expert advice by equating trades made in the market with losses observed by the learning algorithm . if the loss of the market organizer is bounded , this bound can be used to derive an o ( sqrt ( t ) ) regret bound for the corresponding learning algorithm . we then show that the class of markets with convex cost functions exactly corresponds to the class of follow the regularized leader learning algorithms , with the choice of a cost function in the market corresponding to the choice of a regularizer in the learning problem . finally , we show an equivalence between market scoring rules and prediction markets with convex cost functions . this implies that market scoring rules can also be interpreted naturally as follow the regularized leader algorithms , and may be of independent interest . these connections provide new insight into how it is that commonly studied markets , such as the logarithmic market scoring rule , can aggregate opinions into accurate estimates of the likelihood of future events ."}
{"title": "sparse linear dynamical system with its application in multivariate clinical time series", "abstract": "linear dynamical system ( lds ) is an elegant mathematical framework for modeling and learning multivariate time series . however , in general , it is difficult to set the dimension of its hidden state space . a small number of hidden states may not be able to model the complexities of a time series , while a large number of hidden states can lead to overfitting . in this paper , we study methods that impose an $ \\ell_1 $ regularization on the transition matrix of an lds model to alleviate the problem of choosing the optimal number of hidden states . we incorporate a generalized gradient descent method into the maximum a posteriori ( map ) framework and use expectation maximization ( em ) to iteratively achieve sparsity on the transition matrix of an lds model . we show that our sparse linear dynamical system ( slds ) improves the predictive performance when compared to ordinary lds on a multivariate clinical time series dataset ."}
{"title": "the assumptions behind dempster 's rule", "abstract": "this paper examines the concept of a combination rule for belief functions . it is shown that two fairly simple and apparently reasonable assumptions determine dempster 's rule , giving a new justification for it ."}
{"title": "point neurons with conductance-based synapses in the neural engineering framework", "abstract": "the mathematical model underlying the neural engineering framework ( nef ) expresses neuronal input as a linear combination of synaptic currents . however , in biology , synapses are not perfect current sources and are thus nonlinear . detailed synapse models are based on channel conductances instead of currents , which require independent handling of excitatory and inhibitory synapses . this , in particular , significantly affects the influence of inhibitory signals on the neuronal dynamics . in this technical report we first summarize the relevant portions of the nef and conductance-based synapse models . we then discuss a na\\ '' ive translation between populations of lif neurons with current- and conductance-based synapses based on an estimation of an average membrane potential . experiments show that this simple approach works relatively well for feed-forward communication channels , yet performance degrades for nef networks describing more complex dynamics , such as integration ."}
{"title": "a model of the mechanisms underlying exploratory behaviour", "abstract": "a model of the mechanisms underlying exploratory behaviour , based on empirical research and refined using a computer simulation , is presented . the behaviour of killifish from two lakes , one with killifish predators and one without , was compared in the laboratory . plotting average activity in a novel environment versus time resulted in an inverted-u-shaped curve for both groups ; however , the curve for killifish from the lake without predators was ( 1 ) steeper , ( 2 ) reached a peak value earlier , ( s ) reached a higher peak value , and ( 4 ) subsumed less area than the curve for killifish from the lake with predators . we hypothesize that the shape of the exploration curve reflects a competition between motivational subsystems that excite and inhibit exploratory behaviour in a way that is tuned to match the affordance probabilities of the animal 's environment . a computer implementation of this model produced curves which differed along the same four dimensions as differentiate the two killifish curves . all four differences were reproduced in the model by tuning a single parameter : the time-dependent component of the decay-rate of the exploration-inhibiting subsystem ."}
{"title": "enhancing qa systems with complex temporal question processing capabilities", "abstract": "this paper presents a multilayered architecture that enhances the capabilities of current qa systems and allows different types of complex questions or queries to be processed . the answers to these questions need to be gathered from factual information scattered throughout different documents . specifically , we designed a specialized layer to process the different types of temporal questions . complex temporal questions are first decomposed into simple questions , according to the temporal relations expressed in the original question . in the same way , the answers to the resulting simple questions are recomposed , fulfilling the temporal restrictions of the original complex question . a novel aspect of this approach resides in the decomposition which uses a minimal quantity of resources , with the final aim of obtaining a portable platform that is easily extensible to other languages . in this paper we also present a methodology for evaluation of the decomposition of the questions as well as the ability of the implemented temporal layer to perform at a multilingual level . the temporal layer was first performed for english , then evaluated and compared with : a ) a general purpose qa system ( f-measure 65.47 % for qa plus english temporal layer vs. 38.01 % for the general qa system ) , and b ) a well-known qa system . much better results were obtained for temporal questions with the multilayered system . this system was therefore extended to spanish and very good results were again obtained in the evaluation ( f-measure 40.36 % for qa plus spanish temporal layer vs. 22.94 % for the general qa system ) ."}
{"title": "multi-task neural network for non-discrete attribute prediction in knowledge graphs", "abstract": "many popular knowledge graphs such as freebase , yago or dbpedia maintain a list of non-discrete attributes for each entity . intuitively , these attributes such as height , price or population count are able to richly characterize entities in knowledge graphs . this additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs . unfortunately , many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs . in this paper , we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete attribute information in a relational setting . specifically , we train a neural network for triplet prediction along with a separate network for attribute value regression . via multi-task learning , we are able to learn representations of entities , relations and attributes that encode information about both tasks . moreover , such attributes are not only central to many predictive tasks as an information source but also as a prediction target . therefore , models that are able to encode , incorporate and predict such information in a relational learning context are highly attractive as well . we show that our approach outperforms many state-of-the-art methods for the tasks of relational triplet classification and attribute value prediction ."}
{"title": "hybrid gps-gsm localization of automobile tracking system", "abstract": "an integrated gps-gsm system is proposed to track vehicles using google earth application . the remote module has a gps mounted on the moving vehicle to identify its current position , and to be transferred by gsm with other parameters acquired by the automobile 's data port as an sms to a recipient station . the received gps coordinates are filtered using a kalman filter to enhance the accuracy of measured position . after data processing , google earth application is used to view the current location and status of each vehicle . this goal of this system is to manage fleet , police automobiles distribution and car theft cautions ."}
{"title": "narrative science systems : a review", "abstract": "automatic narration of events and entities is the need of the hour , especially when live reporting is critical and volume of information to be narrated is huge . this paper discusses the challenges in this context , along with the algorithms used to build such systems . from a systematic study , we can infer that most of the work done in this area is related to statistical data . it was also found that subjective evaluation or contribution of experts is also limited for narration context ."}
{"title": "learning robust options", "abstract": "robust reinforcement learning aims to produce policies that have strong guarantees even in the face of environments/transition models whose parameters have strong uncertainty . existing work uses value-based methods and the usual primitive action setting . in this paper , we propose robust methods for learning temporally abstract actions , in the framework of options . we present a robust options policy iteration ( ropi ) algorithm with convergence guarantees , which learns options that are robust to model uncertainty . we utilize ropi to learn robust options with the robust options deep q network ( ro-dqn ) that solves multiple tasks and mitigates model misspecification due to model uncertainty . we present experimental results which suggest that policy iteration with linear features may have an inherent form of robustness when using coarse feature representations . in addition , we present experimental results which demonstrate that robustness helps policy iteration implemented on top of deep neural networks to generalize over a much broader range of dynamics than non-robust policy iteration ."}
{"title": "shiva : a framework for graph based ontology matching", "abstract": "since long , corporations are looking for knowledge sources which can provide structured description of data and can focus on meaning and shared understanding . structures which can facilitate open world assumptions and can be flexible enough to incorporate and recognize more than one name for an entity . a source whose major purpose is to facilitate human communication and interoperability . clearly , databases fail to provide these features and ontologies have emerged as an alternative choice , but corporations working on same domain tend to make different ontologies . the problem occurs when they want to share their data/knowledge . thus we need tools to merge ontologies into one . this task is termed as ontology matching . this is an emerging area and still we have to go a long way in having an ideal matcher which can produce good results . in this paper we have shown a framework to matching ontologies using graphs ."}
{"title": "scalable co-optimization of morphology and control in embodied machines", "abstract": "evolution sculpts both the body plans and nervous systems of agents together over time . in contrast , in ai and robotics , a robot 's body plan is usually designed by hand , and control policies are then optimized for that fixed design . the task of simultaneously co-optimizing the morphology and controller of an embodied robot has remained a challenge . in psychology , the theory of embodied cognition posits that behavior arises from a close coupling between body plan and sensorimotor control , which suggests why co-optimizing these two subsystems is so difficult : most evolutionary changes to morphology tend to adversely impact sensorimotor control , leading to an overall decrease in behavioral performance . here , we further examine this hypothesis and demonstrate a technique for `` morphological innovation protection '' , which temporarily reduces selection pressure on recently morphologically-changed individuals , thus enabling evolution some time to `` readapt '' to the new morphology with subsequent control policy mutations . we show the potential for this method to avoid local optima and converge to similar highly fit morphologies across widely varying initial conditions , while sustaining fitness improvements further into optimization . while this technique is admittedly only the first of many steps that must be taken to achieve scalable optimization of embodied machines , we hope that theoretical insight into the cause of evolutionary stagnation in current methods will help to enable the automation of robot design and behavioral training -- while simultaneously providing a testbed to investigate the theory of embodied cognition ."}
{"title": "optimizing supply chain management using gravitational search algorithm and multi agent system", "abstract": "supply chain management is a very dynamic operation research problem where one has to quickly adapt according to the changes perceived in environment in order to maximize the benefit or minimize the loss . therefore we require a system which changes as per the changing requirements . multi agent system technology in recent times has emerged as a possible way of efficient solution implementation for many such complex problems . our research here focuses on building a multi agent system ( mas ) , which implements a modified version of gravitational search swarm intelligence algorithm ( gsa ) to find out an optimal strategy in managing the demand supply chain . we target the grains distribution system among various centers of food corporation of india ( fci ) as application domain . we assume centers with larger stocks as objects of greater mass and vice versa . applying newtonian law of gravity as suggested in gsa , larger objects attract objects of smaller mass towards itself , creating a virtual grain supply source . as heavier object sheds its mass by supplying some to the one in demand , it loses its gravitational pull and thus keeps the whole system of supply chain per-fectly in balance . the multi agent system helps in continuous updation of the whole system with the help of autonomous agents which react to the change in environment and act accordingly . this model also reduces the communication bottleneck to greater extents ."}
{"title": "neutrosophic soft sets with applications in decision making", "abstract": "we firstly present definitions and properties in study of maji \\cite { maji-2013 } on neutrosophic soft sets . we then give a few notes on his study . next , based on \\c { c } a\\u { g } man \\cite { cagman-2014 } , we redefine the notion of neutrosophic soft set and neutrosophic soft set operations to make more functional . by using these new definitions we construct a decision making method and a group decision making method which selects a set of optimum elements from the alternatives . we finally present examples which shows that the methods can be successfully applied to many problems that contain uncertainties ."}
{"title": "spontaneous vs. posed smiles - can we tell the difference ?", "abstract": "smile is an irrefutable expression that shows the physical state of the mind in both true and deceptive ways . generally , it shows happy state of the mind , however , ` smiles ' can be deceptive , for example people can give a smile when they feel happy and sometimes they might also give a smile ( in a different way ) when they feel pity for others . this work aims to distinguish spontaneous ( felt ) smile expressions from posed ( deliberate ) smiles by extracting and analyzing both global ( macro ) motion of the face and subtle ( micro ) changes in the facial expression features through both tracking a series of facial fiducial markers as well as using dense optical flow . specifically the eyes and lips features are captured and used for analysis . it aims to automatically classify all smiles into either ` spontaneous ' or ` posed ' categories , by using support vector machines ( svm ) . experimental results on large database show promising results as compared to other relevant methods ."}
{"title": "cdf-intervals : a reliable framework to reason about data with uncertainty", "abstract": "this research introduces a new constraint domain for reasoning about data with uncertainty . it extends convex modeling with the notion of p-box to gain additional quantifiable information on the data whereabouts . unlike existing approaches , the p-box envelops an unknown probability instead of approximating its representation . the p-box bounds are uniform cumulative distribution functions ( cdf ) in order to employ linear computations in the probabilistic domain . the reasoning by means of p-box cdf-intervals is an interval computation which is exerted on the real domain then it is projected onto the cdf domain . this operation conveys additional knowledge represented by the obtained probabilistic bounds . empirical evaluation shows that , with minimal overhead , the output solution set realizes a full enclosure of the data along with tighter bounds on its probabilistic distributions ."}
{"title": "comparison-based choices", "abstract": "a broad range of on-line behaviors are mediated by interfaces in which people make choices among sets of options . a rich and growing line of work in the behavioral sciences indicate that human choices follow not only from the utility of alternatives , but also from the choice set in which alternatives are presented . in this work we study comparison-based choice functions , a simple but surprisingly rich class of functions capable of exhibiting so-called choice-set effects . motivated by the challenge of predicting complex choices , we study the query complexity of these functions in a variety of settings . we consider settings that allow for active queries or passive observation of a stream of queries , and give analyses both at the granularity of individuals or populations that might exhibit heterogeneous choice behavior . our main result is that any comparison-based choice function in one dimension can be inferred as efficiently as a basic maximum or minimum choice function across many query contexts , suggesting that choice-set effects need not entail any fundamental algorithmic barriers to inference . we also introduce a class of choice functions we call distance-comparison-based functions , and briefly discuss the analysis of such functions . the framework we outline provides intriguing connections between human choice behavior and a range of questions in the theory of sorting ."}
{"title": "traffic optimization for a mixture of self-interested and compliant agents", "abstract": "this paper focuses on two commonly used path assignment policies for agents traversing a congested network : self-interested routing , and system-optimum routing . in the self-interested routing policy each agent selects a path that optimizes its own utility , while the system-optimum routing agents are assigned paths with the goal of maximizing system performance . this paper considers a scenario where a centralized network manager wishes to optimize utilities over all agents , i.e. , implement a system-optimum routing policy . in many real-life scenarios , however , the system manager is unable to influence the route assignment of all agents due to limited influence on route choice decisions . motivated by such scenarios , a computationally tractable method is presented that computes the minimal amount of agents that the system manager needs to influence ( compliant agents ) in order to achieve system optimal performance . moreover , this methodology can also determine whether a given set of compliant agents is sufficient to achieve system optimum and compute the optimal route assignment for the compliant agents to do so . experimental results are presented showing that in several large-scale , realistic traffic networks optimal flow can be achieved with as low as 13 % of the agent being compliant and up to 54 % ."}
{"title": "sub-committee approval voting and generalised justified representation axioms", "abstract": "social choice is replete with various settings including single-winner voting , multi-winner voting , probabilistic voting , multiple referenda , and public decision making . we study a general model of social choice called sub-committee voting ( scv ) that simultaneously generalizes these settings . we then focus on sub-committee voting with approvals and propose extensions of the justified representation axioms that have been considered for proportional representation in approval-based committee voting . we study the properties and relations of these axioms . for each of the axioms , we analyse whether a representative committee exists and also examine the complexity of computing and verifying such a committee ."}
{"title": "systems of natural-language-facilitated human-robot cooperation : a review", "abstract": "natural-language-facilitated human-robot cooperation ( nlc ) , in which natural language ( nl ) is used to share knowledge between a human and a robot for conducting intuitive human-robot cooperation ( hrc ) , is continuously developing in the recent decade . currently , nlc is used in several robotic domains such as manufacturing , daily assistance and health caregiving . it is necessary to summarize current nlc-based robotic systems and discuss the future developing trends , providing helpful information for future nlc research . in this review , we first analyzed the driving forces behind the nlc research . regarding to a robot s cognition level during the cooperation , the nlc implementations then were categorized into four types { nl-based control , nl-based robot training , nl-based task execution , nl-based social companion } for comparison and discussion . last based on our perspective and comprehensive paper review , the future research trends were discussed ."}
{"title": "quality and diversity optimization : a unifying modular framework", "abstract": "the optimization of functions to find the best solution according to one or several objectives has a central role in many engineering and research fields . recently , a new family of optimization algorithms , named quality-diversity optimization , has been introduced , and contrasts with classic algorithms . instead of searching for a single solution , quality-diversity algorithms are searching for a large collection of both diverse and high-performing solutions . the role of this collection is to cover the range of possible solution types as much as possible , and to contain the best solution for each type . the contribution of this paper is threefold . firstly , we present a unifying framework of quality-diversity optimization algorithms that covers the two main algorithms of this family ( multi-dimensional archive of phenotypic elites and the novelty search with local competition ) , and that highlights the large variety of variants that can be investigated within this family . secondly , we propose algorithms with a new selection mechanism for quality-diversity algorithms that outperforms all the algorithms tested in this paper . lastly , we present a new collection management that overcomes the erosion issues observed when using unstructured collections . these three contributions are supported by extensive experimental comparisons of quality-diversity algorithms on three different experimental scenarios ."}
{"title": "entropic causality and greedy minimum entropy coupling", "abstract": "we study the problem of identifying the causal relationship between two discrete random variables from observational data . we recently proposed a novel framework called entropic causality that works in a very general functional model but makes the assumption that the unobserved exogenous variable has small entropy in the true causal direction . this framework requires the solution of a minimum entropy coupling problem : given marginal distributions of m discrete random variables , each on n states , find the joint distribution with minimum entropy , that respects the given marginals . this corresponds to minimizing a concave function of nm variables over a convex polytope defined by nm linear constraints , called a transportation polytope . unfortunately , it was recently shown that this minimum entropy coupling problem is np-hard , even for 2 variables with n states . even representing points ( joint distributions ) over this space can require exponential complexity ( in n , m ) if done naively . in our recent work we introduced an efficient greedy algorithm to find an approximate solution for this problem . in this paper we analyze this algorithm and establish two results : that our algorithm always finds a local minimum and also is within an additive approximation error from the unknown global optimum ."}
{"title": "physics-guided neural networks ( pgnn ) : an application in lake temperature modeling", "abstract": "this paper introduces a novel framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery . this framework , termed as physics-guided neural network ( pgnn ) , leverages the output of physics-based model simulations along with observational features to generate predictions using a neural network architecture . further , this paper presents a novel framework for using physics-based loss functions in the learning objective of neural networks , to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set . we illustrate the effectiveness of pgnn for the problem of lake temperature modeling , where physical relationships between the temperature , density , and depth of water are used to design a physics-based loss function . by using scientific knowledge to guide the construction and learning of neural networks , we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results ."}
{"title": "focus of attention for linear predictors", "abstract": "we present a method to stop the evaluation of a prediction process when the result of the full evaluation is obvious . this trait is highly desirable in prediction tasks where a predictor evaluates all its features for every example in large datasets . we observe that some examples are easier to classify than others , a phenomenon which is characterized by the event when most of the features agree on the class of an example . by stopping the feature evaluation when encountering an easy- to-classify example , the predictor can achieve substantial gains in computation . our method provides a natural attention mechanism for linear predictors where the predictor concentrates most of its computation on hard-to-classify examples and quickly discards easy-to-classify ones . by modifying a linear prediction algorithm such as an svm or adaboost to include our attentive method we prove that the average number of features computed is o ( sqrt ( n log 1/sqrt ( delta ) ) ) where n is the original number of features , and delta is the error rate incurred due to early stopping . we demonstrate the effectiveness of attentive prediction on mnist , real-sim , gisette , and synthetic datasets ."}
{"title": "automated reasoning in deontic logic", "abstract": "deontic logic is a very well researched branch of mathematical logic and philosophy . various kinds of deontic logics are discussed for different application domains like argumentation theory , legal reasoning , and acts in multi-agent systems . in this paper , we show how standard deontic logic can be stepwise transformed into description logic and dl- clauses , such that it can be processed by hyper , a high performance theorem prover which uses a hypertableau calculus . two use cases , one from multi-agent research and one from the development of normative system are investigated ."}
{"title": "liquid democracy : an analysis in binary aggregation and diffusion", "abstract": "the paper proposes an analysis of liquid democracy ( or , delegable proxy voting ) from the perspective of binary aggregation and of binary diffusion models . we show how liquid democracy on binary issues can be embedded into the framework of binary aggregation with abstentions , enabling the transfer of known results about the latter -- -such as impossibility theorems -- -to the former . this embedding also sheds light on the relation between delegation cycles in liquid democracy and the probability of collective abstentions , as well as the issue of individual rationality in a delegable proxy voting setting . we then show how liquid democracy on binary issues can be modeled and analyzed also as a specific process of dynamics of binary opinions on networks . these processes -- -called boolean degroot processes -- -are a special case of the degroot stochastic model of opinion diffusion . we establish the convergence conditions of such processes and show they provide some novel insights on how the effects of delegation cycles and individual rationality could be mitigated within liquid democracy . the study is a first attempt to provide theoretical foundations to the delgable proxy features of the liquid democracy voting system . our analysis suggests recommendations on how the system may be modified to make it more resilient with respect to the handling of delegation cycles and of inconsistent majorities ."}
{"title": "when should a decision maker ignore the advice of a decision aid ?", "abstract": "this paper argues that the principal difference between decision aids and most other types of information systems is the greater reliance of decision aids on fallible algorithms -- algorithms that sometimes generate incorrect advice . it is shown that interactive problem solving with a decision aid that is based on a fallible algorithm can easily result in aided performance which is poorer than unaided performance , even if the algorithm , by itself , performs significantly better than the unaided decision maker . this suggests that unless certain conditions are satisfied , using a decision aid as an aid is counterproductive . some conditions under which a decision aid is best used as an aid are derived ."}
{"title": "approximation complexity of maximum a posteriori inference in sum-product networks", "abstract": "we discuss the computational complexity of approximating maximum a posteriori inference in sum-product networks . we first show np-hardness in trees of height two by a reduction from maximum independent set ; this implies non-approximability within a sublinear factor . we show that this is a tight bound , as we can find an approximation within a linear factor in networks of height two . we then show that , in trees of height three , it is np-hard to approximate the problem within a factor $ 2^ { f ( n ) } $ for any sublinear function $ f $ of the size of the input $ n $ . again , this bound is tight , as we prove that the usual max-product algorithm finds ( in any network ) approximations within factor $ 2^ { c \\cdot n } $ for some constant $ c < 1 $ . last , we present a simple algorithm , and show that it provably produces solutions at least as good as , and potentially much better than , the max-product algorithm . we empirically analyze the proposed algorithm against max-product using synthetic and realistic networks ."}
{"title": "bad universal priors and notions of optimality", "abstract": "a big open question of algorithmic information theory is the choice of the universal turing machine ( utm ) . for kolmogorov complexity and solomonoff induction we have invariance theorems : the choice of the utm changes bounds only by a constant . for the universally intelligent agent aixi ( hutter , 2005 ) no invariance theorem is known . our results are entirely negative : we discuss cases in which unlucky or adversarial choices of the utm cause aixi to misbehave drastically . we show that legg-hutter intelligence and thus balanced pareto optimality is entirely subjective , and that every policy is pareto optimal in the class of all computable environments . this undermines all existing optimality properties for aixi . while it may still serve as a gold standard for ai , our results imply that aixi is a relative theory , dependent on the choice of the utm ."}
{"title": "a tool for implementation of a domain model based on fuzzy relationships", "abstract": "the domain model is one of the important components used by adaptive learning systems to automatically generate customized courses for the learners . in this paper our contribution is to propose a new tool for implementation of a domain model based on fuzzy relationships among concepts . this tool allows the experts and teachers to find the best parameters in order to adapt the learners 's differences ."}
{"title": "new ideas for brain modelling 2", "abstract": "this paper describes a relatively simple way of allowing a brain model to self-organise its concept patterns through nested structures . for a simulation , time reduction is helpful and it would be able to show how patterns may form and then fire in sequence , as part of a search or thought process . it uses a very simple equation to show how the inhibitors in particular , can switch off certain areas , to allow other areas to become the prominent ones and thereby define the current brain state . this allows for a small amount of control over what appears to be a chaotic structure inside of the brain . it is attractive because it is still mostly mechanical and therefore can be added as an automatic process , or the modelling of that . the paper also describes how the nested pattern structure can be used as a basic counting mechanism . another mathematical conclusion provides a basis for maintaining memory or concept patterns . the self-organisation can space itself through automatic processes . this might allow new neurons to be added in a more even manner and could help to maintain the concept integrity . the process might also help with finding memory structures afterwards . this extended version integrates further with the existing cognitive model and provides some new conclusions ."}
{"title": "linearized and single-pass belief propagation", "abstract": "how can we tell when accounts are fake or real in a social network ? and how can we tell which accounts belong to liberal , conservative or centrist users ? often , we can answer such questions and label nodes in a network based on the labels of their neighbors and appropriate assumptions of homophily ( `` birds of a feather flock together '' ) or heterophily ( `` opposites attract '' ) . one of the most widely used methods for this kind of inference is belief propagation ( bp ) which iteratively propagates the information from a few nodes with explicit labels throughout a network until convergence . one main problem with bp , however , is that there are no known exact guarantees of convergence in graphs with loops . this paper introduces linearized belief propagation ( linbp ) , a linearization of bp that allows a closed-form solution via intuitive matrix equations and , thus , comes with convergence guarantees . it handles homophily , heterophily , and more general cases that arise in multi-class settings . plus , it allows a compact implementation in sql . the paper also introduces single-pass belief propagation ( sbp ) , a `` localized '' version of linbp that propagates information across every edge at most once and for which the final class assignments depend only on the nearest labeled neighbors . in addition , sbp allows fast incremental updates in dynamic networks . our runtime experiments show that linbp and sbp are orders of magnitude faster than standard"}
{"title": "a tableau methodology for deontic conditional logics", "abstract": "in this paper we present a theorem proving methodology for a restricted but significant fragment of the conditional language made up of ( boolean combinations of ) conditional statements with unnested antecedents . the method is based on the possible world semantics for conditional logics . the kem label formalism , designed to account for the semantics of normal modal logics , is easily adapted to the semantics of conditional logics by simply indexing labels with formulas . the inference rules are provided by the propositional system ke+ - a tableau-like analytic proof system devised to be used both as a refutation and a direct method of proof - enlarged with suitable elimination rules for the conditional connective . the theorem proving methodology we are going to present can be viewed as a first step towards developing an appropriate algorithmic framework for several conditional logics for ( defeasible ) conditional obligation ."}
{"title": "anisotropic selection in cellular genetic algorithms", "abstract": "in this paper we introduce a new selection scheme in cellular genetic algorithms ( cgas ) . anisotropic selection ( as ) promotes diversity and allows accurate control of the selective pressure . first we compare this new scheme with the classical rectangular grid shapes solution according to the selective pressure : we can obtain the same takeover time with the two techniques although the spreading of the best individual is different . we then give experimental results that show to what extent as promotes the emergence of niches that support low coupling and high cohesion . finally , using a cga with anisotropic selection on a quadratic assignment problem we show the existence of an anisotropic optimal value for which the best average performance is observed . further work will focus on the selective pressure self-adjustment ability provided by this new selection scheme ."}
{"title": "towards a universal theory of artificial intelligence based on algorithmic probability and sequential decision theory", "abstract": "decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental probability distribution is known . solomonoff 's theory of universal induction formally solves the problem of sequence prediction for unknown distribution . we unify both theories and give strong arguments that the resulting universal aixi model behaves optimal in any computable environment . the major drawback of the aixi model is that it is uncomputable . to overcome this problem , we construct a modified algorithm aixi^tl , which is still superior to any other time t and space l bounded agent . the computation time of aixi^tl is of the order t x 2^l ."}
{"title": "automatic pattern classification by unsupervised learning using dimensionality reduction of data with mirroring neural networks", "abstract": "this paper proposes an unsupervised learning technique by using multi-layer mirroring neural network and forgy 's clustering algorithm . multi-layer mirroring neural network is a neural network that can be trained with generalized data inputs ( different categories of image patterns ) to perform non-linear dimensionality reduction and the resultant low-dimensional code is used for unsupervised pattern classification using forgy 's algorithm . by adapting the non-linear activation function ( modified sigmoidal function ) and initializing the weights and bias terms to small random values , mirroring of the input pattern is initiated . in training , the weights and bias terms are changed in such a way that the input presented is reproduced at the output by back propagating the error . the mirroring neural network is capable of reducing the input vector to a great degree ( approximately 1/30th the original size ) and also able to reconstruct the input pattern at the output layer from this reduced code units . the feature set ( output of central hidden layer ) extracted from this network is fed to forgy 's algorithm , which classify input data patterns into distinguishable classes . in the implementation of forgy 's algorithm , initial seed points are selected in such a way that they are distant enough to be perfectly grouped into different categories . thus a new method of unsupervised learning is formulated and demonstrated in this paper . this method gave impressive results when applied to classification of different image patterns ."}
{"title": "mixing metaphors", "abstract": "mixed metaphors have been neglected in recent metaphor research . this paper suggests that such neglect is short-sighted . though mixing is a more complex phenomenon than straight metaphors , the same kinds of reasoning and knowledge structures are required . this paper provides an analysis of both parallel and serial mixed metaphors within the framework of an ai system which is already capable of reasoning about straight metaphorical manifestations and argues that the processes underlying mixing are central to metaphorical meaning . therefore , any theory of metaphors must be able to account for mixing ."}
{"title": "sequential operators in computability logic", "abstract": "computability logic ( cl ) ( see http : //www.cis.upenn.edu/~giorgi/cl.html ) is a semantical platform and research program for redeveloping logic as a formal theory of computability , as opposed to the formal theory of truth which it has more traditionally been . formulas in cl stand for ( interactive ) computational problems , understood as games between a machine and its environment ; logical operators represent operations on such entities ; and `` truth '' is understood as existence of an effective solution , i.e. , of an algorithmic winning strategy . the formalism of cl is open-ended , and may undergo series of extensions as the study of the subject advances . the main groups of operators on which cl has been focused so far are the parallel , choice , branching , and blind operators . the present paper introduces a new important group of operators , called sequential . the latter come in the form of sequential conjunction and disjunction , sequential quantifiers , and sequential recurrences . as the name may suggest , the algorithmic intuitions associated with this group are those of sequential computations , as opposed to the intuitions of parallel computations associated with the parallel group of operations : playing a sequential combination of games means playing its components in a sequential fashion , one after one . the main technical result of the present paper is a sound and complete axiomatization of the propositional fragment of computability logic whose vocabulary , together with negation , includes all three -- parallel , choice and sequential -- sorts of conjunction and disjunction . an extension of this result to the first-order level is also outlined ."}
{"title": "feature weight tuning for recursive neural networks", "abstract": "this paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence , in other words , to perform `` weight tuning '' for higher-level representation acquisition . we propose two models , weighted neural network ( wnn ) and binary-expectation neural network ( benn ) , which automatically control how much one specific unit contributes to the higher-level representation . the proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks . experimental results demonstrate the significant improvement over standard neural models ."}
{"title": "slt-resolution for the well-founded semantics", "abstract": "global sls-resolution and slg-resolution are two representative mechanisms for top-down evaluation of the well-founded semantics of general logic programs . global sls-resolution is linear for query evaluation but suffers from infinite loops and redundant computations . in contrast , slg-resolution resolves infinite loops and redundant computations by means of tabling , but it is not linear . the principal disadvantage of a non-linear approach is that it can not be implemented using a simple , efficient stack-based memory structure nor can it be easily extended to handle some strictly sequential operators such as cuts in prolog . in this paper , we present a linear tabling method , called slt-resolution , for top-down evaluation of the well-founded semantics . slt-resolution is a substantial extension of sldnf-resolution with tabling . its main features include : ( 1 ) it resolves infinite loops and redundant computations while preserving the linearity . ( 2 ) it is terminating , and sound and complete w.r.t . the well-founded semantics for programs with the bounded-term-size property with non-floundering queries . its time complexity is comparable with slg-resolution and polynomial for function-free logic programs . ( 3 ) because of its linearity for query evaluation , slt-resolution bridges the gap between the well-founded semantics and standard prolog implementation techniques . it can be implemented by an extension to any existing prolog abstract machines such as wam or atoam ."}
{"title": "the symbolic interior point method", "abstract": "a recent trend in probabilistic inference emphasizes the codification of models in a formal syntax , with suitable high-level features such as individuals , relations , and connectives , enabling descriptive clarity , succinctness and circumventing the need for the modeler to engineer a custom solver . unfortunately , bringing these linguistic and pragmatic benefits to numerical optimization has proven surprisingly challenging . in this paper , we turn to these challenges : we introduce a rich modeling language , for which an interior-point method computes approximate solutions in a generic way . while logical features easily complicates the underlying model , often yielding intricate dependencies , we exploit and cache local structure using algebraic decision diagrams ( adds ) . indeed , standard matrix-vector algebra is efficiently realizable in adds , but we argue and show that well-known optimization methods are not ideal for adds . our engine , therefore , invokes a sophisticated matrix-free approach . we demonstrate the flexibility of the resulting symbolic-numeric optimizer on decision making and compressed sensing tasks with millions of non-zero entries ."}
{"title": "multivariate time series classification using dynamic time warping template selection for human activity recognition", "abstract": "accurate and computationally efficient means for classifying human activities have been the subject of extensive research efforts . most current research focuses on extracting complex features to achieve high classification accuracy . we propose a template selection approach based on dynamic time warping , such that complex feature extraction and domain knowledge is avoided . we demonstrate the predictive capability of the algorithm on both simulated and real smartphone data ."}
{"title": "when majority voting fails : comparing quality assurance methods for noisy human computation environment", "abstract": "quality assurance remains a key topic in human computation research . prior work indicates that majority voting is effective for low difficulty tasks , but has limitations for harder tasks . this paper explores two methods of addressing this problem : tournament selection and elimination selection , which exploit 2- , 3- and 4-way comparisons between different answers to human computation tasks . our experimental results and statistical analyses show that both methods produce the correct answer in noisy human computation environment more often than majority voting . furthermore , we find that the use of 4-way comparisons can significantly reduce the cost of quality assurance relative to the use of 2-way comparisons ."}
{"title": "analysis of hybrid soft and hard computing techniques for forex monitoring systems", "abstract": "in a universe with a single currency , there would be no foreign exchange market , no foreign exchange rates , and no foreign exchange . over the past twenty-five years , the way the market has performed those tasks has changed enormously . the need for intelligent monitoring systems has become a necessity to keep track of the complex forex market . the vast currency market is a foreign concept to the average individual . however , once it is broken down into simple terms , the average individual can begin to understand the foreign exchange market and use it as a financial instrument for future investing . in this paper , we attempt to compare the performance of hybrid soft computing and hard computing techniques to predict the average monthly forex rates one month ahead . the soft computing models considered are a neural network trained by the scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a takagi-sugeno fuzzy inference system . we also considered multivariate adaptive regression splines ( mars ) , classification and regression trees ( cart ) and a hybrid cart-mars technique . we considered the exchange rates of australian dollar with respect to us dollar , singapore dollar , new zealand dollar , japanese yen and united kingdom pounds . the models were trained using 70 % of the data and remaining was used for testing and validation purposes . it is observed that the proposed hybrid models could predict the forex rates more accurately than all the techniques when applied individually . empirical results also reveal that the hybrid hard computing approach also improved some of our previous work using a neuro-fuzzy approach ."}
{"title": "deciding consistency of databases containing defeasible and strict information", "abstract": "we propose a norm of consistency for a mixed set of defeasible and strict sentences , based on a probabilistic semantics . this norm establishes a clear distinction between knowledge bases depicting exceptions and those containing outright contradictions . we then define a notion of entailment based also on probabilistic considerations and provide a characterization of the relation between consistency and entailment . we derive necessary and sufficient conditions for consistency , and provide a simple decision procedure for testing consistency and deciding whether a sentence is entailed by a database . finally , it is shown that if al1 sentences are horn clauses , consistency and entailment can be tested in polynomial time ."}
{"title": "arabic call system based on pedagogically indexed text", "abstract": "this article introduces the benefits of using computer as a tool for foreign language teaching and learning . it describes the effect of using natural language processing ( nlp ) tools for learning arabic . the technique explored in this particular case is the employment of pedagogically indexed corpora . this text-based method provides the teacher the advantage of building activities based on texts adapted to a particular pedagogical situation . this paper also presents arac : a platform dedicated to language educators allowing them to create activities within their own pedagogical area of interest ."}
{"title": "a selective macro-learning algorithm and its application to the nxn sliding-tile puzzle", "abstract": "one of the most common mechanisms used for speeding up problem solvers is macro-learning . macros are sequences of basic operators acquired during problem solving . macros are used by the problem solver as if they were basic operators . the major problem that macro-learning presents is the vast number of macros that are available for acquisition . macros increase the branching factor of the search space and can severely degrade problem-solving efficiency . to make macro learning useful , a program must be selective in acquiring and utilizing macros . this paper describes a general method for selective acquisition of macros . solvable training problems are generated in increasing order of difficulty . the only macros acquired are those that take the problem solver out of a local minimum to a better state . the utility of the method is demonstrated in several domains , including the domain of nxn sliding-tile puzzles . after learning on small puzzles , the system is able to efficiently solve puzzles of any size ."}
{"title": "pixel-level domain transfer", "abstract": "we present an image-conditional image generation model . the model transfers an input domain to a target domain in semantic level , and generates the target image in pixel level . to generate realistic target images , we employ the real/fake-discriminator as in generative adversarial nets , but also introduce a novel domain-discriminator to make the generated image relevant to the input image . we verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person . we present a high quality clothing dataset containing the two domains , and succeed in demonstrating decent results ."}
{"title": "deriving probability density functions from probabilistic functional programs", "abstract": "the probability density function of a probability distribution is a fundamental concept in probability theory and a key ingredient in various widely used machine learning methods . however , the necessary framework for compiling probabilistic functional programs to density functions has only recently been developed . in this work , we present a density compiler for a probabilistic language with failure and both discrete and continuous distributions , and provide a proof of its soundness . the compiler greatly reduces the development effort of domain experts , which we demonstrate by solving inference problems from various scientific applications , such as modelling the global carbon cycle , using a standard markov chain monte carlo framework ."}
{"title": "an introduction to the dsm theory for the combination of paradoxical , uncertain , and imprecise sources of information", "abstract": "the management and combination of uncertain , imprecise , fuzzy and even paradoxical or high conflicting sources of information has always been , and still remains today , of primal importance for the development of reliable modern information systems involving artificial reasoning . in this introduction , we present a survey of our recent theory of plausible and paradoxical reasoning , known as dezert-smarandache theory ( dsmt ) in the literature , developed for dealing with imprecise , uncertain and paradoxical sources of information . we focus our presentation here rather on the foundations of dsmt , and on the two important new rules of combination , than on browsing specific applications of dsmt available in literature . several simple examples are given throughout the presentation to show the efficiency and the generality of this new approach ."}
{"title": "convergence , continuity and recurrence in dynamic epistemic logic", "abstract": "the paper analyzes dynamic epistemic logic from a topological perspective . the main contribution consists of a framework in which dynamic epistemic logic satisfies the requirements for being a topological dynamical system thus interfacing discrete dynamic logics with continuous mappings of dynamical systems . the setting is based on a notion of logical convergence , demonstratively equivalent with convergence in stone topology . presented is a flexible , parametrized family of metrics inducing the latter , used as an analytical aid . we show maps induced by action model transformations continuous with respect to the stone topology and present results on the recurrent behavior of said maps ."}
{"title": "incremental probabilistic inference", "abstract": "propositional representation services such as truth maintenance systems offer powerful support for incremental , interleaved , problem-model construction and evaluation . probabilistic inference systems , in contrast , have lagged behind in supporting this incrementality typically demanded by problem solvers . the problem , we argue , is that the basic task of probabilistic inference is typically formulated at too large a grain-size . we show how a system built around a smaller grain-size inference task can have the desired incrementality and serve as the basis for a low-level ( propositional ) probabilistic representation service ."}
{"title": "a dikw paradigm to cognitive engineering", "abstract": "though the word cognitive has a wide range of meanings we define cognitive engineering as learning from brain to bolster engineering solutions . however , giving an achievable framework to the process towards this has been a difficult task . in this work we take the classic data information knowledge wisdom ( dikw ) framework to set some achievable goals and sub-goals towards cognitive engineering . a layered framework like dikw aligns nicely with the layered structure of pre-frontal cortex . and breaking the task into sub-tasks based on the layers also makes it easier to start developmental endeavours towards achieving the final goal of a brain-inspired system ."}
{"title": "randomized distributed configuration management of wireless networks : multi-layer markov random fields and near-optimality", "abstract": "distributed configuration management is imperative for wireless infrastructureless networks where each node adjusts locally its physical and logical configuration through information exchange with neighbors . two issues remain open . the first is the optimality . the second is the complexity . we study these issues through modeling , analysis , and randomized distributed algorithms . modeling defines the optimality . we first derive a global probabilistic model for a network configuration which characterizes jointly the statistical spatial dependence of a physical- and a logical-configuration . we then show that a local model which approximates the global model is a two-layer markov random field or a random bond model . the complexity of the local model is the communication range among nodes . the local model is near-optimal when the approximation error to the global model is within a given error bound . we analyze the trade-off between an approximation error and complexity , and derive sufficient conditions on the near-optimality of the local model . we validate the model , the analysis and the randomized distributed algorithms also through simulation ."}
{"title": "principal manifolds and nonlinear dimension reduction via local tangent space alignment", "abstract": "nonlinear manifold learning from unorganized data points is a very challenging unsupervised learning and data visualization problem with a great variety of applications . in this paper we present a new algorithm for manifold learning and nonlinear dimension reduction . based on a set of unorganized data points sampled with noise from the manifold , we represent the local geometry of the manifold using tangent spaces learned by fitting an affine subspace in a neighborhood of each data point . those tangent spaces are aligned to give the internal global coordinates of the data points with respect to the underlying manifold by way of a partial eigendecomposition of the neighborhood connection matrix . we present a careful error analysis of our algorithm and show that the reconstruction errors are of second-order accuracy . we illustrate our algorithm using curves and surfaces both in 2d/3d and higher dimensional euclidean spaces , and 64-by-64 pixel face images with various pose and lighting conditions . we also address several theoretical and algorithmic issues for further research and improvements ."}
{"title": "dynamic move chains -- a forward pruning approach to tree search in computer chess", "abstract": "this paper proposes a new mechanism for pruning a search game-tree in computer chess . the algorithm stores and then reuses chains or sequences of moves , built up from previous searches . these move sequences have a built-in forward-pruning mechanism that can radically reduce the search space . a typical search process might retrieve a move from a transposition table , where the decision of what move to retrieve would be based on the position itself . this algorithm stores move sequences based on what previous sequences were better , or caused cutoffs . this is therefore position independent and so it could also be useful in games with imperfect information or uncertainty , where the whole situation is not known at any one time . over a small set of tests , the algorithm was shown to clearly out-perform transposition tables , both in terms of search reduction and game-play results ."}
{"title": "look-ahead before you leap : end-to-end active recognition by forecasting the effect of motion", "abstract": "visual recognition systems mounted on autonomous moving agents face the challenge of unconstrained data , but simultaneously have the opportunity to improve their performance by moving to acquire new views of test data . in this work , we first show how a recurrent neural network-based system may be trained to perform end-to-end learning of motion policies suited for this `` active recognition '' setting . further , we hypothesize that active vision requires an agent to have the capacity to reason about the effects of its motions on its view of the world . to verify this hypothesis , we attempt to induce this capacity in our active recognition pipeline , by simultaneously learning to forecast the effects of the agent 's motions on its internal representation of the environment conditional on all past views . results across two challenging datasets confirm both that our end-to-end system successfully learns meaningful policies for active category recognition , and that `` learning to look ahead '' further boosts recognition performance ."}
{"title": "a bayesian multiresolution independence test for continuous variables", "abstract": "in this paper we present a method ofcomputing the posterior probability ofconditional independence of two or morecontinuous variables from data , examined at several resolutions . ourapproach is motivated by theobservation that the appearance ofcontinuous data varies widely atvarious resolutions , producing verydifferent independence estimatesbetween the variablesinvolved . therefore , it is difficultto ascertain independence withoutexamining data at several carefullyselected resolutions . in our paper , weaccomplish this using the exactcomputation of the posteriorprobability of independence , calculatedanalytically given a resolution . ateach examined resolution , we assume amultinomial distribution with dirichletpriors for the discretized tableparameters , and compute the posteriorusing bayesian integration . acrossresolutions , we use a search procedureto approximate the bayesian integral ofprobability over an exponential numberof possible histograms . our methodgeneralizes to an arbitrary numbervariables in a straightforward manner.the test is suitable for bayesiannetwork learning algorithms that useindependence tests to infer the networkstructure , in domains that contain anymix of continuous , ordinal andcategorical variables ."}
{"title": "a framework for searching and/or graphs with cycles", "abstract": "search in cyclic and/or graphs was traditionally known to be an unsolved problem . in the recent past several important studies have been reported in this domain . in this paper , we have taken a fresh look at the problem . first , a new and comprehensive theoretical framework for cyclic and/or graphs has been presented , which was found missing in the recent literature . based on this framework , two best-first search algorithms , s1 and s2 , have been developed . s1 does uninformed search and is a simple modification of the bottom-up algorithm by martelli and montanari . s2 performs a heuristically guided search and replicates the modification in bottom-up 's successors , namely hs and ao* . both s1 and s2 solve the problem of searching and/or graphs in presence of cycles . we then present a detailed analysis for the correctness and complexity results of s1 and s2 , using the proposed framework . we have observed through experiments that s1 and s2 output correct results in all cases ."}
{"title": "generalized qualitative probability : savage revisited", "abstract": "preferences among acts are analyzed in the style of l. savage , but as partially ordered . the rationality postulates considered are weaker than savage 's on three counts . the sure thing principle is derived in this setting . the postulates are shown to lead to a characterization of generalized qualitative probability that includes and blends both traditional qualitative probability and the ranked structures used in logical approaches ."}
{"title": "contextualizing concepts using a mathematical generalization of the quantum formalism", "abstract": "we outline the rationale and preliminary results of using the state context property ( scop ) formalism , originally developed as a generalization of quantum mechanics , to describe the contextual manner in which concepts are evoked , used and combined to generate meaning . the quantum formalism was developed to cope with problems arising in the description of ( i ) the measurement process , and ( ii ) the generation of new states with new properties when particles become entangled . similar problems arising with concepts motivated the formal treatment introduced here . concepts are viewed not as fixed representations , but entities existing in states of potentiality that require interaction with a context-a stimulus or another concept-to 'collapse ' to an instantiated form ( e.g . exemplar , prototype , or other possibly imaginary instance ) . the stimulus situation plays the role of the measurement in physics , acting as context that induces a change of the cognitive state from superposition state to collapsed state . the collapsed state is more likely to consist of a conjunction of concepts for associative than analytic thought because more stimulus or concept properties take part in the collapse . we provide two contextual measures of conceptual distance-one using collapse probabilities and the other weighted properties-and show how they can be applied to conjunctions using the pet fish problem ."}
{"title": "learning to generalize to new compositions in image understanding", "abstract": "recurrent neural networks have recently been used for learning to describe images using natural language . however , it has been observed that these models generalize poorly to scenes that were not observed during training , possibly depending too strongly on the statistics of the text in the training data . here we propose to describe images using short structured representations , aiming to capture the crux of a description . these structured representations allow us to tease-out and evaluate separately two types of generalization : standard generalization to new images with similar scenes , and generalization to new combinations of known entities . we compare two learning approaches on the ms-coco dataset : a state-of-the-art recurrent network based on an lstm ( show , attend and tell ) , and a simple structured prediction model on top of a deep network . we find that the structured model generalizes to new compositions substantially better than the lstm , ~7 times the accuracy of predicting structured representations . by providing a concrete method to quantify generalization for unseen combinations , we argue that structured representations and compositional splits are a useful benchmark for image captioning , and advocate compositional models that capture linguistic and visual structure ."}
{"title": "commonsense scene semantics for cognitive robotics : towards grounding embodied visuo-locomotive interactions", "abstract": "we present a commonsense , qualitative model for the semantic grounding of embodied visuo-spatial and locomotive interactions . the key contribution is an integrative methodology combining low-level visual processing with high-level , human-centred representations of space and motion rooted in artificial intelligence . we demonstrate practical applicability with examples involving object interactions , and indoor movement ."}
{"title": "exploiting sparsity to build efficient kernel based collaborative filtering for top-n item recommendation", "abstract": "the increasing availability of implicit feedback datasets has raised the interest in developing effective collaborative filtering techniques able to deal asymmetrically with unambiguous positive feedback and ambiguous negative feedback . in this paper , we propose a principled kernel-based collaborative filtering method for top-n item recommendation with implicit feedback . we present an efficient implementation using the linear kernel , and we show how to generalize it to kernels of the dot product family preserving the efficiency . we also investigate on the elements which influence the sparsity of a standard cosine kernel . this analysis shows that the sparsity of the kernel strongly depends on the properties of the dataset , in particular on the long tail distribution . we compare our method with state-of-the-art algorithms achieving good results both in terms of efficiency and effectiveness ."}
{"title": "randomised variable neighbourhood search for multi objective optimisation", "abstract": "various local search approaches have recently been applied to machine scheduling problems under multiple objectives . their foremost consideration is the identification of the set of pareto optimal alternatives . an important aspect of successfully solving these problems lies in the definition of an appropriate neighbourhood structure . unclear in this context remains , how interdependencies within the fitness landscape affect the resolution of the problem . the paper presents a study of neighbourhood search operators for multiple objective flow shop scheduling . experiments have been carried out with twelve different combinations of criteria . to derive exact conclusions , small problem instances , for which the optimal solutions are known , have been chosen . statistical tests show that no single neighbourhood operator is able to equally identify all pareto optimal alternatives . significant improvements however have been obtained by hybridising the solution algorithm using a randomised variable neighbourhood search technique ."}
{"title": "network unfolding map by edge dynamics modeling", "abstract": "the emergence of collective dynamics in neural networks is a mechanism of the animal and human brain for information processing . in this paper , we develop a computational technique using distributed processing elements in a complex network , which are called particles , to solve semi-supervised learning problems . three actions govern the particles ' dynamics : generation , walking , and absorption . labeled vertices generate new particles that compete against rival particles for edge domination . active particles randomly walk in the network until they are absorbed by either a rival vertex or an edge currently dominated by rival particles . the result from the model evolution consists of sets of edges arranged by the label dominance . each set tends to form a connected subnetwork to represent a data class . although the intrinsic dynamics of the model is a stochastic one , we prove there exists a deterministic version with largely reduced computational complexity ; specifically , with linear growth . furthermore , the edge domination process corresponds to an unfolding map in such way that edges `` stretch '' and `` shrink '' according to the vertex-edge dynamics . consequently , the unfolding effect summarizes the relevant relationships between vertices and the uncovered data classes . the proposed model captures important details of connectivity patterns over the vertex-edge dynamics evolution , in contrast to previous approaches which focused on only vertex or only edge dynamics . computer simulations reveal that the new model can identify nonlinear features in both real and artificial data , including boundaries between distinct classes and overlapping structures of data ."}
{"title": "evolving a stigmergic self-organized data-mining", "abstract": "self-organizing complex systems typically are comprised of a large number of frequently similar components or events . through their process , a pattern at the global-level of a system emerges solely from numerous interactions among the lower-level components of the system . moreover , the rules specifying interactions among the system 's components are executed using only local information , without reference to the global pattern , which , as in many real-world problems is not easily accessible or possible to be found . stigmergy , a kind of indirect communication and learning by the environment found in social insects is a well know example of self-organization , providing not only vital clues in order to understand how the components can interact to produce a complex pattern , as can pinpoint simple biological non-linear rules and methods to achieve improved artificial intelligent adaptive categorization systems , critical for data-mining . on the present work it is our intention to show that a new type of data-mining can be designed based on stigmergic paradigms , taking profit of several natural features of this phenomenon . by hybridizing bio-inspired swarm intelligence with evolutionary computation we seek for an entire distributed , adaptive , collective and cooperative self-organized data-mining . as a real-world , real-time test bed for our proposal , world-wide-web mining will be used . having that purpose in mind , web usage data was collected from the monash university 's web site ( australia ) , with over 7 million hits every week . results are compared to other recent systems , showing that the system presented is by far promising ."}
{"title": "a history of metaheuristics", "abstract": "this chapter describes the history of metaheuristics in five distinct periods , starting long before the first use of the term and ending a long time in the future ."}
{"title": "relational data mining through extraction of representative exemplars", "abstract": "with the growing interest on network analysis , relational data mining is becoming an emphasized domain of data mining . this paper addresses the problem of extracting representative elements from a relational dataset . after defining the notion of degree of representativeness , computed using the borda aggregation procedure , we present the extraction of exemplars which are the representative elements of the dataset . we use these concepts to build a network on the dataset . we expose the main properties of these notions and we propose two typical applications of our framework . the first application consists in resuming and structuring a set of binary images and the second in mining co-authoring relation in a research team ."}
{"title": "pmlb : a large benchmark suite for machine learning evaluation and comparison", "abstract": "the selection , development , or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study . numerous publicly available real-world and simulated benchmark datasets have emerged from different sources , but their organization and adoption as standards have been inconsistent . as such , selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists . the present study introduces an accessible , curated , and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies . we compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data . finally , we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance . this work is an important first step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future ."}
{"title": "model-powered conditional independence test", "abstract": "we consider the problem of non-parametric conditional independence testing ( ci testing ) for continuous random variables . given i.i.d samples from the joint distribution $ f ( x , y , z ) $ of continuous random vectors $ x , y $ and $ z , $ we determine whether $ x \\perp y | z $ . we approach this by converting the conditional independence test into a classification problem . this allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks . these models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art , for high-dimensional ci testing . the main technical challenge in the classification problem is the need for samples from the conditional product distribution $ f^ { ci } ( x , y , z ) = f ( x|z ) f ( y|z ) f ( z ) $ -- the joint distribution if and only if $ x \\perp y | z. $ -- when given access only to i.i.d . samples from the true joint distribution $ f ( x , y , z ) $ . to tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to $ f^ { ci } $ in terms of total variational distance . we then develop theoretical results regarding the generalization bounds for classification for our problem , which translate into error bounds for ci testing . we provide a novel analysis of rademacher type classification bounds in the presence of non-i.i.d near-independent samples . we empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods ."}
{"title": "r-extreme signalling for congestion control", "abstract": "in many `` smart city '' applications , congestion arises in part due to the nature of signals received by individuals from a central authority . in the model of marecek et al . [ arxiv:1406.7639 , int . j. control 88 ( 10 ) , 2015 ] , each agent uses one out of multiple resources at each time instant . the per-use cost of a resource depends on the number of concurrent users . a central authority has up-to-date knowledge of the congestion across all resources and uses randomisation to provide a scalar or an interval for each resource at each time . in this paper , the interval to broadcast per resource is obtained by taking the minima and maxima of costs observed within a time window of length r , rather than by randomisation . we show that the resulting distribution of agents across resources also converges in distribution , under plausible assumptions about the evolution of the population over time ."}
{"title": "linear combination of one-step predictive information with an external reward in an episodic policy gradient setting : a critical analysis", "abstract": "one of the main challenges in the field of embodied artificial intelligence is the open-ended autonomous learning of complex behaviours . our approach is to use task-independent , information-driven intrinsic motivation ( s ) to support task-dependent learning . the work presented here is a preliminary step in which we investigate the predictive information ( the mutual information of the past and future of the sensor stream ) as an intrinsic drive , ideally supporting any kind of task acquisition . previous experiments have shown that the predictive information ( pi ) is a good candidate to support autonomous , open-ended learning of complex behaviours , because a maximisation of the pi corresponds to an exploration of morphology- and environment-dependent behavioural regularities . the idea is that these regularities can then be exploited in order to solve any given task . three different experiments are presented and their results lead to the conclusion that the linear combination of the one-step pi with an external reward function is not generally recommended in an episodic policy gradient setting . only for hard tasks a great speed-up can be achieved at the cost of an asymptotic performance lost ."}
{"title": "annotation order matters : recurrent image annotator for arbitrary length image tagging", "abstract": "automatic image annotation has been an important research topic in facilitating large scale image management and retrieval . existing methods focus on learning image-tag correlation or correlation between tags to improve annotation accuracy . however , most of these methods evaluate their performance using top-k retrieval performance , where k is fixed . although such setting gives convenience for comparing different methods , it is not the natural way that humans annotate images . the number of annotated tags should depend on image contents . inspired by the recent progress in machine translation and image captioning , we propose a novel recurrent image annotator ( ria ) model that forms image annotation task as a sequence generation problem so that ria can natively predict the proper length of tags according to image contents . we evaluate the proposed model on various image annotation datasets . in addition to comparing our model with existing methods using the conventional top-k evaluation measures , we also provide our model as a high quality baseline for the arbitrary length image tagging task . moreover , the results of our experiments show that the order of tags in training phase has a great impact on the final annotation performance ."}
{"title": "residual component analysis", "abstract": "probabilistic principal component analysis ( ppca ) seeks a low dimensional representation of a data set in the presence of independent spherical gaussian noise , sigma = ( sigma^2 ) *i. the maximum likelihood solution for the model is an eigenvalue problem on the sample covariance matrix . in this paper we consider the situation where the data variance is already partially explained by other factors , e.g . covariates of interest , or temporal correlations leaving some residual variance . we decompose the residual variance into its components through a generalized eigenvalue problem , which we call residual component analysis ( rca ) . we show that canonical covariates analysis ( cca ) is a special case of our algorithm and explore a range of new algorithms that arise from the framework . we illustrate the ideas on a gene expression time series data set and the recovery of human pose from silhouette ."}
{"title": "polyethism in a colony of artificial ants", "abstract": "we explore self-organizing strategies for role assignment in a foraging task carried out by a colony of artificial agents . our strategies are inspired by various mechanisms of division of labor ( polyethism ) observed in eusocial insects like ants , termites , or bees . specifically we instantiate models of caste polyethism and age or temporal polyethism to evaluated the benefits to foraging in a dynamic environment . our experiment is directly related to the exploration/exploitation trade of in machine learning ."}
{"title": "the morphospace of consciousness", "abstract": "given recent proposals to synthesize consciousness , how many forms of conscious machines can one distinguish and on what grounds ? based on current clinical scales of consciousness , that measure cognitive awareness and wakefulness , we take a perspective on how contemporary artificially intelligent machines and synthetically engineered life forms would measure on these scales . to do so , we argue that awareness and wakefulness can be associated to computational and autonomous complexity respectively . then , building on insights from cognitive robotics , we ask what function consciousness serves , and interpret it as an evolutionary game-theoretic strategy . we make the case for a third type of complexity necessary for describing consciousness , namely , social complexity . having identified these complexity types , allows us to represent both , biological and synthetic systems in a common morphospace . this suggests an embodiment-based taxonomy of consciousness . in particular , we distinguish four forms of consciousness , based on embodiment : biological , synthetic , group ( resulting from group interactions ) and simulated consciousness ( embodied by virtual agents within a simulated reality ) . such a taxonomy is useful for studying comparative signatures of consciousness across domains , in order to highlight design principles necessary to engineer conscious machines . this is particularly relevant in the light of recent developments at the crossroads of neuroscience , biomedical engineering , artificial intelligence and biomimetics ."}
{"title": "the mind as a computational system", "abstract": "the present document is an excerpt of an essay that i wrote as part of my application material to graduate school in computer science ( with a focus on artificial intelligence ) , in 1986. i was not invited by any of the schools that received it , so i became a theoretical physicist instead . the essay 's full title was `` some topics in philosophy and computer science '' . i am making this text ( unchanged from 1985 , preserving the typesetting as much as possible ) available now in memory of jerry fodor , whose writings had influenced me significantly at the time ( even though i did not always agree ) ."}
{"title": "running time analysis of the ( 1+1 ) -ea for onemax and leadingones under bit-wise noise", "abstract": "in many real-world optimization problems , the objective function evaluation is subject to noise , and we can not obtain the exact objective value . evolutionary algorithms ( eas ) , a type of general-purpose randomized optimization algorithm , have shown able to solve noisy optimization problems well . however , previous theoretical analyses of eas mainly focused on noise-free optimization , which makes the theoretical understanding largely insufficient . meanwhile , the few existing theoretical studies under noise often considered the one-bit noise model , which flips a randomly chosen bit of a solution before evaluation ; while in many realistic applications , several bits of a solution can be changed simultaneously . in this paper , we study a natural extension of one-bit noise , the bit-wise noise model , which independently flips each bit of a solution with some probability . we analyze the running time of the ( 1+1 ) -ea solving onemax and leadingones under bit-wise noise for the first time , and derive the ranges of the noise level for polynomial and super-polynomial running time bounds . the analysis on leadingones under bit-wise noise can be easily transferred to one-bit noise , and improves the previously known results . since our analysis discloses that the ( 1+1 ) -ea can be efficient only under low noise levels , we also study whether the sampling strategy can bring robustness to noise . we prove that using sampling can significantly increase the largest noise level allowing a polynomial running time , that is , sampling is robust to noise ."}
{"title": "on the complexity of connection games", "abstract": "in this paper , we study three connection games among the most widely played : havannah , twixt , and slither . we show that determining the outcome of an arbitrary input position is pspace-complete in all three cases . our reductions are based on the popular graph problem generalized geography and on hex itself . we also consider the complexity of generalizations of hex parameterized by the length of the solution and establish that while short generalized hex is w [ 1 ] -hard , short hex is fpt . finally , we prove that the ultra-weak solution to the empty starting position in hex can not be fully adapted to any of these three games ."}
{"title": "operations for learning with graphical models", "abstract": "this paper is a multidisciplinary review of empirical , statistical learning from a graphical model perspective . well-known examples of graphical models include bayesian networks , directed graphs representing a markov chain , and undirected networks representing a markov field . these graphical models are extended to model data analysis and empirical learning using the notation of plates . graphical operations for simplifying and manipulating a problem are provided including decomposition , differentiation , and the manipulation of probability models from the exponential family . two standard algorithm schemas for learning are reviewed in a graphical framework : gibbs sampling and the expectation maximization algorithm . using these operations and schemas , some popular algorithms can be synthesized from their graphical specification . this includes versions of linear regression , techniques for feed-forward networks , and learning gaussian and discrete bayesian networks from data . the paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented . the main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms ."}
{"title": "dynamic teaching in sequential decision making environments", "abstract": "we describe theoretical bounds and a practical algorithm for teaching a model by demonstration in a sequential decision making environment . unlike previous efforts that have optimized learners that watch a teacher demonstrate a static policy , we focus on the teacher as a decision maker who can dynamically choose different policies to teach different parts of the environment . we develop several teaching frameworks based on previously defined supervised protocols , such as teaching dimension , extending them to handle noise and sequences of inputs encountered in an mdp.we provide theoretical bounds on the learnability of several important model classes in this setting and suggest a practical algorithm for dynamic teaching ."}
{"title": "towards rational deployment of multiple heuristics in a*", "abstract": "the obvious way to use several admissible heuristics in a* is to take their maximum . in this paper we aim to reduce the time spent on computing heuristics . we discuss lazy a* , a variant of a* where heuristics are evaluated lazily : only when they are essential to a decision to be made in the a* search process . we present a new rational meta-reasoning based scheme , rational lazy a* , which decides whether to compute the more expensive heuristics at all , based on a myopic value of information estimate . both methods are examined theoretically . empirical evaluation on several domains supports the theoretical results , and shows that lazy a* and rational lazy a* are state-of-the-art heuristic combination methods ."}
{"title": "an algorithm for the construction of bayesian network structures from data", "abstract": "previous algorithms for the construction of bayesian belief network structures from data have been either highly dependent on conditional independence ( ci ) tests , or have required an ordering on the nodes to be supplied by the user . we present an algorithm that integrates these two approaches - ci tests are used to generate an ordering on the nodes from the database which is then used to recover the underlying bayesian network structure using a non ci based method . results of preliminary evaluation of the algorithm on two networks ( alarm and led ) are presented . we also discuss some algorithm performance issues and open problems ."}
{"title": "fast frequent querying with lazy control flow compilation", "abstract": "control flow compilation is a hybrid between classical wam compilation and meta-call , limited to the compilation of non-recursive clause bodies . this approach is used successfully for the execution of dynamically generated queries in an inductive logic programming setting ( ilp ) . control flow compilation reduces compilation times up to an order of magnitude , without slowing down execution . a lazy variant of control flow compilation is also presented . by compiling code by need , it removes the overhead of compiling unreached code ( a frequent phenomenon in practical ilp settings ) , and thus reduces the size of the compiled code . both dynamic compilation approaches have been implemented and were combined with query packs , an efficient ilp execution mechanism . it turns out that locality of data and code is important for performance . the experiments reported in the paper show that lazy control flow compilation is superior in both artificial and real life settings ."}
{"title": "an evolving cascade neural network technique for cleaning sleep electroencephalograms", "abstract": "evolving cascade neural networks ( ecnns ) and a new training algorithm capable of selecting informative features are described . the ecnn initially learns with one input node and then evolves by adding new inputs as well as new hidden neurons . the resultant ecnn has a near minimal number of hidden neurons and inputs . the algorithm is successfully used for training ecnn to recognise artefacts in sleep electroencephalograms ( eegs ) which were visually labelled by eeg-viewers . in our experiments , the ecnn outperforms the standard neural-network as well as evolutionary techniques ."}
{"title": "high-dimensional feature selection by feature-wise non-linear lasso", "abstract": "the goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values . the least absolute shrinkage and selection operator ( lasso ) allows computationally efficient feature selection based on linear dependency between input features and output values . in this paper , we consider a feature-wise kernelized lasso for capturing non-linear input-output dependency . we first show that , with particular choices of kernel functions , non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures . we then show that the globally optimal solution can be efficiently computed ; this makes the approach scalable to high-dimensional problems . the effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features ."}
{"title": "tractable answer-set programming with weight constraints : bounded treewidth is not enough", "abstract": "cardinality constraints or , more generally , weight constraints are well recognized as an important extension of answer-set programming . clearly , all common algorithmic tasks related to programs with cardinality or weight constraints - like checking the consistency of a program - are intractable . many intractable problems in the area of knowledge representation and reasoning have been shown to become linear time tractable if the treewidth of the programs or formulas under consideration is bounded by some constant . the goal of this paper is to apply the notion of treewidth to programs with cardinality or weight constraints and to identify tractable fragments . it will turn out that the straightforward application of treewidth to such class of programs does not suffice to obtain tractability . however , by imposing further restrictions , tractability can be achieved ."}
{"title": "an enhanced method to compute the similarity between concepts of ontology", "abstract": "with the use of ontologies in several domains such as semantic web , information retrieval , artificial intelligence , the concept of similarity measuring has become a very important domain of research . therefore , in the current paper , we propose our method of similarity measuring which uses the dijkstra algorithm to define and compute the shortest path . then , we use this one to compute the semantic distance between two concepts defined in the same hierarchy of ontology . afterward , we base on this result to compute the semantic similarity . finally , we present an experimental comparison between our method and other methods of similarity measuring ."}
{"title": "on first-order model-based reasoning", "abstract": "reasoning semantically in first-order logic is notoriously a challenge . this paper surveys a selection of semantically-guided or model-based methods that aim at meeting aspects of this challenge . for first-order logic we touch upon resolution-based methods , tableaux-based methods , dpll-inspired methods , and we give a preview of a new method called sggs , for semantically-guided goal-sensitive reasoning . for first-order theories we highlight hierarchical and locality-based methods , concluding with the recent model-constructing satisfiability calculus ."}
{"title": "human pose estimation in space and time using 3d cnn", "abstract": "this paper explores the capabilities of convolutional neural networks to deal with a task that is easily manageable for humans : perceiving 3d pose of a human body from varying angles . however , in our approach , we are restricted to using a monocular vision system . for this purpose , we apply a convolutional neural network approach on rgb videos and extend it to three dimensional convolutions . this is done via encoding the time dimension in videos as the 3\\ts { rd } dimension in convolutional space , and directly regressing to human body joint positions in 3d coordinate space . this research shows the ability of such a network to achieve state-of-the-art performance on the selected human3.6m dataset , thus demonstrating the possibility of successfully representing temporal data with an additional dimension in the convolutional operation ."}
{"title": "inverse reward design", "abstract": "autonomous agents optimize the reward function we give them . what they do n't know is how hard it is for us to design a reward function that actually captures what we want . when designing the reward , we might think of some specific training scenarios , and make sure that the reward will lead to the right behavior in those scenarios . inevitably , agents encounter new scenarios ( e.g. , new types of terrain ) where optimizing that same reward may lead to undesired behavior . our insight is that reward functions are merely observations about what the designer actually wants , and that they should be interpreted in the context in which they were designed . we introduce inverse reward design ( ird ) as the problem of inferring the true objective based on the designed reward and the training mdp . we introduce approximate methods for solving ird problems , and use their solution to plan risk-averse behavior in test mdps . empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking ."}
{"title": "log-linear rnns : towards recurrent neural networks with flexible prior knowledge", "abstract": "we introduce ll-rnns ( log-linear rnns ) , an extension of recurrent neural networks that replaces the softmax output layer by a log-linear output layer , of which the softmax is a special case . this conceptually simple move has two main advantages . first , it allows the learner to combat training data sparsity by allowing it to model words ( or more generally , output symbols ) as complex combinations of attributes without requiring that each combination is directly observed in the training data ( as the softmax does ) . second , it permits the inclusion of flexible prior knowledge in the form of a priori specified modular features , where the neural network component learns to dynamically control the weights of a log-linear distribution exploiting these features . we conduct experiments in the domain of language modelling of french , that exploit morphological prior knowledge and show an important decrease in perplexity relative to a baseline rnn . we provide other motivating iillustrations , and finally argue that the log-linear and the neural-network components contribute complementary strengths to the ll-rnn : the ll aspect allows the model to incorporate rich prior knowledge , while the nn aspect , according to the `` representation learning '' paradigm , allows the model to discover novel combination of characteristics ."}
{"title": "combining monte-carlo and hyper-heuristic methods for the multi-mode resource-constrained multi-project scheduling problem", "abstract": "multi-mode resource and precedence-constrained project scheduling is a well-known challenging real-world optimisation problem . an important variant of the problem requires scheduling of activities for multiple projects considering availability of local and global resources while respecting a range of constraints . a critical aspect of the benchmarks addressed in this paper is that the primary objective is to minimise the sum of the project completion times , with the usual makespan minimisation as a secondary objective . we observe that this leads to an expected different overall structure of good solutions and discuss the effects this has on the algorithm design . this paper presents a carefully designed hybrid of monte-carlo tree search , novel neighbourhood moves , memetic algorithms , and hyper-heuristic methods . the implementation is also engineered to increase the speed with which iterations are performed , and to exploit the computing power of multicore machines . empirical evaluation shows that the resulting information-sharing multi-component algorithm significantly outperforms other solvers on a set of `` hidden '' instances , i.e . instances not available at the algorithm design phase ."}
{"title": "efficient simulation of financial stress testing scenarios with suppes-bayes causal networks", "abstract": "the most recent financial upheavals have cast doubt on the adequacy of some of the conventional quantitative risk management strategies , such as var ( value at risk ) , in many common situations . consequently , there has been an increasing need for verisimilar financial stress testings , namely simulating and analyzing financial portfolios in extreme , albeit rare scenarios . unlike conventional risk management which exploits statistical correlations among financial instruments , here we focus our analysis on the notion of probabilistic causation , which is embodied by suppes-bayes causal networks ( sbcns ) , sbcns are probabilistic graphical models that have many attractive features in terms of more accurate causal analysis for generating financial stress scenarios . in this paper , we present a novel approach for conducting stress testing of financial portfolios based on sbcns in combination with classical machine learning classification tools . the resulting method is shown to be capable of correctly discovering the causal relationships among financial factors that affect the portfolios and thus , simulating stress testing scenarios with a higher accuracy and lower computational complexity than conventional monte carlo simulations ."}
{"title": "an automated compatibility prediction engine using disc theory based classification and neural networks", "abstract": "traditionally psychometric tests were used for profiling incoming workers . these methods use disc profiling method to classify people into distinct personality types , which are further used to predict if a person may be a possible fit to the organizational culture . this concept is taken further by introducing a novel technique to predict if a particular pair of an incoming worker and the manager being assigned are compatible at a psychological scale . this is done using multilayer perceptron neural network which can be adaptively trained to showcase the true nature of the compatibility index . the proposed prototype model is used to quantify the relevant attributes , use them to train the prediction engine , and to define the data pipeline required for it ."}
{"title": "prioritised default logic as argumentation with partial order default priorities", "abstract": "we express brewka 's prioritised default logic ( pdl ) as argumentation using aspic+ . by representing pdl as argumentation and designing an argument preference relation that takes the argument structure into account , we prove that the conclusions of the justified arguments correspond to the pdl extensions . we will first assume that the default priority is total , and then generalise to the case where it is a partial order . this provides a characterisation of non-monotonic inference in pdl as an exchange of argument and counter-argument , providing a basis for distributed non-monotonic reasoning in the form of dialogue ."}
{"title": "populous : a tool for populating ontology templates", "abstract": "we present populous , a tool for gathering content with which to populate an ontology . domain experts need to add content , that is often repetitive in its form , but without having to tackle the underlying ontological representation . populous presents users with a table based form in which columns are constrained to take values from particular ontologies ; the user can select a concept from an ontology via its meaningful label to give a value for a given entity attribute . populated tables are mapped to patterns that can then be used to automatically generate the ontology 's content . populous 's contribution is in the knowledge gathering stage of ontology development . it separates knowledge gathering from the conceptualisation and also separates the user from the standard ontology authoring environments . as a result , populous can allow knowledge to be gathered in a straight-forward manner that can then be used to do mass production of ontology content ."}
{"title": "representing , simulating and analysing ho chi minh city tsunami plan by means of process models", "abstract": "this paper considers the textual plan ( guidelines ) proposed by people 's committee of ho chi minh city ( vietnam ) to manage earthquake and tsunami , and try to represent it in a more formal way , in order to provide means to simulate , analyse and adapt it . we first present a state of the art about coordination models for disaster management with a focus on process oriented approaches . we give an overview of the different dimensions of the textual tsunami plan of ho chi minh city and then the graphical representation of its process with bpmn ( business process model and notation ) . we finally show how to exploit this process with workflow tools to simulate ( yawl tool ) and analyse it ( prom tool ) ."}
{"title": "bank distress in the news : describing events through deep learning", "abstract": "while many models are purposed for detecting the occurrence of significant events in financial systems , the task of providing qualitative detail on the developments is not usually as well automated . we present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events . supervised by only a small set of event information , comprising entity names and dates , the model is leveraged by unsupervised learning of semantic vector representations on extensive text data . we demonstrate applicability to the study of financial risk based on news ( 6.6m articles ) , particularly bank distress and government interventions ( 243 events ) , where indices can signal the level of bank-stress-related reporting at the entity level , or aggregated at national or european level , while being coupled with explanations . thus , we exemplify how text , as timely , widely available and descriptive data , can serve as a useful complementary source of information for financial and systemic risk analytics ."}
{"title": "twist your logic with touist", "abstract": "sat provers are powerful tools for solving real-sized logic problems , but using them requires solid programming knowledge and may be seen w.r.t.\\ logic like assembly language w.r.t.\\ programming . something like a high level language was missing to ease various users to take benefit of these tools . { \\sc \\texttt { touist } } \\ aims at filling this gap . it is devoted to propositional logic and its main features are 1 ) to offer a high-level logic langage for expressing succintly complex formulas ( e.g.\\ formulas describing sudoku rules , planification problems , \\ldots ) and 2 ) to find models to these formulas by using the adequate powerful prover , which the user has no need to know about . it consists in a friendly interface that offers several syntactic facilities and which is connected with some sufficiently powerful provers allowing to automatically solve big instances of difficult problems ( such as time-tables or sudokus ) . it can interact with various provers : pure sat solver but also smt provers ( sat modulo theories - like linear theory of reals , etc ) and thus may also be used by beginners for experiencing with pure propositional problems up to graduate students or even researchers for solving planification problems involving big sets of fluents and numerical constraints on them ."}
{"title": "hi\u00e9rarchisation des r\u00e8gles d'association en fouille de textes", "abstract": "extraction of association rules is widely used as a data mining method . however , one of the limit of this approach comes from the large number of extracted rules and the difficulty for a human expert to deal with the totality of these rules . we propose to solve this problem by structuring the set of rules into hierarchy . the expert can then therefore explore the rules , access from one rule to another one more general when we raise up in the hierarchy , and in other hand , or a more specific rules . rules are structured at two levels . the global level aims at building a hierarchy from the set of rules extracted . thus we define a first type of rule-subsomption relying on galois lattices . the second level consists in a local and more detailed analysis of each rule . it generate for a given rule a set of generalization rules structured into a local hierarchy . this leads to the definition of a second type of subsomption . this subsomption comes from inductive logic programming and integrates a terminological model ."}
{"title": "rewriting recursive aggregates in answer set programming : back to monotonicity", "abstract": "aggregation functions are widely used in answer set programming for representing and reasoning on knowledge involving sets of objects collectively . current implementations simplify the structure of programs in order to optimize the overall performance . in particular , aggregates are rewritten into simpler forms known as monotone aggregates . since the evaluation of normal programs with monotone aggregates is in general on a lower complexity level than the evaluation of normal programs with arbitrary aggregates , any faithful translation function must introduce disjunction in rule heads in some cases . however , no function of this kind is known . the paper closes this gap by introducing a polynomial , faithful , and modular translation for rewriting common aggregation functions into the simpler form accepted by current solvers . a prototype system allows for experimenting with arbitrary recursive aggregates , which are also supported in the recent version 4.5 of the grounder \\textsc { gringo } , using the methods presented in this paper . to appear in theory and practice of logic programming ( tplp ) , proceedings of iclp 2015 ."}
{"title": "fast preprocessing for robust face sketch synthesis", "abstract": "exemplar-based face sketch synthesis methods usually meet the challenging problem that input photos are captured in different lighting conditions from training photos . the critical step causing the failure is the search of similar patch candidates for an input photo patch . conventional illumination invariant patch distances are adopted rather than directly relying on pixel intensity difference , but they will fail when local contrast within a patch changes . in this paper , we propose a fast preprocessing method named bidirectional luminance remapping ( blr ) , which interactively adjust the lighting of training and input photos . our method can be directly integrated into state-of-the-art exemplar-based methods to improve their robustness with ignorable computational cost ."}
{"title": "the continuous hint factory - providing hints in vast and sparsely populated edit distance spaces", "abstract": "intelligent tutoring systems can support students in solving multi-step tasks by providing a hint regarding what to do next . however , engineering such next-step hints manually or using an expert model becomes infeasible if the space of possible states is too large . therefore , several approaches have emerged to infer next-step hints automatically , relying on past student 's data . such hints typically have the form of an edit which could have been performed by capable students in the given situation , based on what past capable students have done . in this contribution we provide a mathematical framework to analyze edit-based hint policies and , based on this theory , propose a novel hint policy to provide edit hints for learning tasks with a vast state space and sparse student data . we call this technique the continuous hint factory because it embeds student data in a continuous space , in which the most likely edit can be inferred in a probabilistic sense , similar to the hint factory . in our experimental evaluation we demonstrate that the continuous hint factory can predict what capable students would do in solving a multi-step programming task and that hints provided by the continuous hint factory match to some extent the edit hints that human tutors would have given in the same situation ."}
{"title": "ultrametric component analysis with application to analysis of text and of emotion", "abstract": "we review the theory and practice of determining what parts of a data set are ultrametric . it is assumed that the data set , to begin with , is endowed with a metric , and we include discussion of how this can be brought about if a dissimilarity , only , holds . the basis for part of the metric-endowed data set being ultrametric is to consider triplets of the observables ( vectors ) . we develop a novel consensus of hierarchical clusterings . we do this in order to have a framework ( including visualization and supporting interpretation ) for the parts of the data that are determined to be ultrametric . furthermore a major objective is to determine locally ultrametric relationships as opposed to non-local ultrametric relationships . as part of this work , we also study a particular property of our ultrametricity coefficient , namely , it being a function of the difference of angles of the base angles of the isosceles triangle . this work is completed by a review of related work , on consensus hierarchies , and of a major new application , namely quantifying and interpreting the emotional content of narrative ."}
{"title": "integrative windowing", "abstract": "in this paper we re-investigate windowing for rule learning algorithms . we show that , contrary to previous results for decision tree learning , windowing can in fact achieve significant run-time gains in noise-free domains and explain the different behavior of rule learning algorithms by the fact that they learn each rule independently . the main contribution of this paper is integrative windowing , a new type of algorithm that further exploits this property by integrating good rules into the final theory right after they have been discovered . thus it avoids re-learning these rules in subsequent iterations of the windowing process . experimental evidence in a variety of noise-free domains shows that integrative windowing can in fact achieve substantial run-time gains . furthermore , we discuss the problem of noise in windowing and present an algorithm that is able to achieve run-time gains in a set of experiments in a simple domain with artificial noise ."}
{"title": "deep big simple neural nets excel on handwritten digit recognition", "abstract": "good old on-line back-propagation for plain multi-layer perceptrons yields a very low 0.35 % error rate on the famous mnist handwritten digits benchmark . all we need to achieve this best result so far are many hidden layers , many neurons per layer , numerous deformed training images , and graphics cards to greatly speed up learning ."}
{"title": "spontaneous analogy by piggybacking on a perceptual system", "abstract": "most computational models of analogy assume they are given a delineated source domain and often a specified target domain . these systems do not address how analogs can be isolated from large domains and spontaneously retrieved from long-term memory , a process we call spontaneous analogy . we present a system that represents relational structures as feature bags . using this representation , our system leverages perceptual algorithms to automatically create an ontology of relational structures and to efficiently retrieve analogs for new relational structures from long-term memory . we provide a demonstration of our approach that takes a set of unsegmented stories , constructs an ontology of analogical schemas ( corresponding to plot devices ) , and uses this ontology to efficiently find analogs within new stories , yielding significant time-savings over linear analog retrieval at a small accuracy cost ."}
{"title": "active learning machine learns to create new quantum experiments", "abstract": "how useful can machine learning be in a quantum laboratory ? here we raise the question of the potential of intelligent machines in the context of scientific research . a major motivation for the present work is the unknown reachability of various entanglement classes in quantum experiments . we investigate this question by using the projective simulation model , a physics-oriented approach to artificial intelligence . in our approach , the projective simulation system is challenged to design complex photonic quantum experiments that produce high-dimensional entangled multiphoton states , which are of high interest in modern quantum experiments . the artificial intelligence system learns to create a variety of entangled states , and improves the efficiency of their realization . in the process , the system autonomously ( re ) discovers experimental techniques which are only now becoming standard in modern quantum optical experiments - a trait which was not explicitly demanded from the system but emerged through the process of learning . such features highlight the possibility that machines could have a significantly more creative role in future research ."}
{"title": "soft constraint logic programming for electric vehicle travel optimization", "abstract": "soft constraint logic programming is a natural and flexible declarative programming formalism , which allows to model and solve real-life problems involving constraints of different types . in this paper , after providing a slightly more general and elegant presentation of the framework , we show how we can apply it to the e-mobility problem of coordinating electric vehicles in order to overcome both energetic and temporal constraints and so to reduce their running cost . in particular , we focus on the journey optimization sub-problem , considering sequences of trips from a user 's appointment to another one . solutions provide the best alternatives in terms of time and energy consumption , including route sequences and possible charging events ."}
{"title": "bayesian unification of gradient and bandit-based learning for accelerated global optimisation", "abstract": "bandit based optimisation has a remarkable advantage over gradient based approaches due to their global perspective , which eliminates the danger of getting stuck at local optima . however , for continuous optimisation problems or problems with a large number of actions , bandit based approaches can be hindered by slow learning . gradient based approaches , on the other hand , navigate quickly in high-dimensional continuous spaces through local optimisation , following the gradient in fine grained steps . yet , apart from being susceptible to local optima , these schemes are less suited for online learning due to their reliance on extensive trial-and-error before the optimum can be identified . in this paper , we propose a bayesian approach that unifies the above two paradigms in one single framework , with the aim of combining their advantages . at the heart of our approach we find a stochastic linear approximation of the function to be optimised , where both the gradient and values of the function are explicitly captured . this allows us to learn from both noisy function and gradient observations , and predict these properties across the action space to support optimisation . we further propose an accompanying bandit driven exploration scheme that uses bayesian credible bounds to trade off exploration against exploitation . our empirical results demonstrate that by unifying bandit and gradient based learning , one obtains consistently improved performance across a wide spectrum of problem environments . furthermore , even when gradient feedback is unavailable , the flexibility of our model , including gradient prediction , still allows us outperform competing approaches , although with a smaller margin . due to the pervasiveness of bandit based optimisation , our scheme opens up for improved performance both in meta-optimisation and in applications where gradient related information is readily available ."}
{"title": "analysing sensitivity data from probabilistic networks", "abstract": "with the advance of efficient analytical methods for sensitivity analysis ofprobabilistic networks , the interest in the sensitivities revealed by real-life networks is rekindled . as the amount of data resulting from a sensitivity analysis of even a moderately-sized network is alreadyoverwhelming , methods for extracting relevant information are called for . one such methodis to study the derivative of the sensitivity functions yielded for a network 's parameters . we further propose to build upon the concept of admissible deviation , that is , the extent to which a parameter can deviate from the true value without inducing a change in the most likely outcome . we illustrate these concepts by means of a sensitivity analysis of a real-life probabilistic network in oncology ."}
{"title": "finding undetected protein associations in cell signaling by belief propagation", "abstract": "external information propagates in the cell mainly through signaling cascades and transcriptional activation , allowing it to react to a wide spectrum of environmental changes . high throughput experiments identify numerous molecular components of such cascades that may , however , interact through unknown partners . some of them may be detected using data coming from the integration of a protein-protein interaction network and mrna expression profiles . this inference problem can be mapped onto the problem of finding appropriate optimal connected subgraphs of a network defined by these datasets . the optimization procedure turns out to be computationally intractable in general . here we present a new distributed algorithm for this task , inspired from statistical physics , and apply this scheme to alpha factor and drug perturbations data in yeast . we identify the role of the cos8 protein , a member of a gene family of previously unknown function , and validate the results by genetic experiments . the algorithm we present is specially suited for very large datasets , can run in parallel , and can be adapted to other problems in systems biology . on renowned benchmarks it outperforms other algorithms in the field ."}
{"title": "evolutionary computing", "abstract": "evolutionary computing ( ec ) is an exciting development in computer science . it amounts to building , applying and studying algorithms based on the darwinian principles of natural selection . in this paper we briefly introduce the main concepts behind evolutionary computing . we present the main components all evolutionary algorithms ( ea ) , sketch the differences between different types of eas and survey application areas ranging from optimization , modeling and simulation to entertainment ."}
{"title": "mix-nets : factored mixtures of gaussians in bayesian networks with mixed continuous and discrete variables", "abstract": "recently developed techniques have made it possible to quickly learn accurate probability density functions from data in low-dimensional continuous space . in particular , mixtures of gaussians can be fitted to data very quickly using an accelerated em algorithm that employs multiresolution kd-trees ( moore , 1999 ) . in this paper , we propose a kind of bayesian networks in which low-dimensional mixtures of gaussians over different subsets of the domain 's variables are combined into a coherent joint probability model over the entire domain . the network is also capable of modeling complex dependencies between discrete variables and continuous variables without requiring discretization of the continuous variables . we present efficient heuristic algorithms for automatically learning these networks from data , and perform comparative experiments illustrated how well these networks model real scientific data and synthetic data . we also briefly discuss some possible improvements to the networks , as well as possible applications ."}
{"title": "kronecker determinantal point processes", "abstract": "determinantal point processes ( dpps ) are probabilistic models over all subsets a ground set of $ n $ items . they have recently gained prominence in several applications that rely on `` diverse '' subsets . however , their applicability to large problems is still limited due to the $ \\mathcal o ( n^3 ) $ complexity of core tasks such as sampling and learning . we enable efficient sampling and learning for dpps by introducing krondpp , a dpp model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices . this decomposition immediately enables fast exact sampling . but contrary to what one may expect , leveraging the kronecker product structure for speeding up dpp learning turns out to be more difficult . we overcome this challenge , and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a krondpp ."}
{"title": "algorithm runtime prediction : methods & evaluation", "abstract": "perhaps surprisingly , it is possible to predict how long an algorithm will take to run on a previously unseen input , using machine learning techniques to build a model of the algorithm 's runtime as a function of problem-specific instance features . such models have important applications to algorithm analysis , portfolio-based algorithm selection , and the automatic configuration of parameterized algorithms . over the past decade , a wide variety of techniques have been studied for building such models . here , we describe extensions and improvements of existing models , new families of models , and -- perhaps most importantly -- a much more thorough treatment of algorithm parameters as model inputs . we also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability ( sat ) , travelling salesperson ( tsp ) and mixed integer programming ( mip ) problems . we evaluate these innovations through the largest empirical analysis of its kind , comparing to a wide range of runtime modelling techniques from the literature . our experiments consider 11 algorithms and 35 instance distributions ; they also span a very wide range of sat , mip , and tsp instances , with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications . overall , we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances , to new algorithms from a parameterized space , and to both simultaneously ."}
{"title": "learning with abandonment", "abstract": "consider a platform that wants to learn a personalized policy for each user , but the platform faces the risk of a user abandoning the platform if she is dissatisfied with the actions of the platform . for example , a platform is interested in personalizing the number of newsletters it sends , but faces the risk that the user unsubscribes forever . we propose a general thresholded learning model for scenarios like this , and discuss the structure of optimal policies . we describe salient features of optimal personalization algorithms and how feedback the platform receives impacts the results . furthermore , we investigate how the platform can efficiently learn the heterogeneity across users by interacting with a population and provide performance guarantees ."}
{"title": "learning high-level representations from demonstrations", "abstract": "hierarchical learning ( hl ) is key to solving complex sequential decision problems with long horizons and sparse rewards . it allows learning agents to break-up large problems into smaller , more manageable subtasks . a common approach to hl , is to provide the agent with a number of high-level skills that solve small parts of the overall problem . a major open question , however , is how to identify a suitable set of reusable skills . we propose a principled approach that uses human demonstrations to infer a set of subgoals based on changes in the demonstration dynamics . using these subgoals , we decompose the learning problem into an abstract high-level representation and a set of low-level subtasks . the abstract description captures the overall problem structure , while subtasks capture desired skills . we demonstrate that we can jointly optimize over both levels of learning . we show that the resulting method significantly outperforms previous baselines on two challenging problems : the atari 2600 game montezuma 's revenge , and a simulated robotics problem moving the ant robot through a maze ."}
{"title": "defensive player classification in the national basketball association", "abstract": "the national basketball association ( nba ) has expanded their data gathering and have heavily invested in new technologies to gather advanced performance metrics on players . this expanded data set allows analysts to use unique performance metrics in models to estimate and classify player performance . instead of grouping players together based on physical attributes and positions played , analysts can group together players that play similar to each other based on these tracked metrics . existing methods for player classification have typically used offensive metrics for clustering [ 1 ] . there have been attempts to classify players using past defensive metrics , but the lack of quality metrics has not produced promising results . the classifications presented in the paper use newly introduced defensive metrics to find different defensive positions for each player . without knowing the number of categories that players can be cast into , gaussian mixture models ( gmm ) can be applied to find the optimal number of clusters . in the model presented , five different defensive player types can be identified ."}
{"title": "optimal binary autoencoding with pairwise correlations", "abstract": "we formulate learning of a binary autoencoder as a biconvex optimization problem which learns from the pairwise correlations between encoded and decoded bits . among all possible algorithms that use this information , ours finds the autoencoder that reconstructs its inputs with worst-case optimal loss . the optimal decoder is a single layer of artificial neurons , emerging entirely from the minimax loss minimization , and with weights learned by convex optimization . all this is reflected in competitive experimental results , demonstrating that binary autoencoding can be done efficiently by conveying information in pairwise correlations in an optimal fashion ."}
{"title": "two steps feature selection and neural network classification for the trec-8 routing", "abstract": "for the trec-8 routing , one specific filter is built for each topic . each filter is a classifier trained to recognize the documents that are relevant to the topic . when presented with a document , each classifier estimates the probability for the document to be relevant to the topic for which it has been trained . since the procedure for building a filter is topic-independent , the system is fully automatic . by making use of a sample of documents that have previously been evaluated as relevant or not relevant to a particular topic , a term selection is performed , and a neural network is trained . each document is represented by a vector of frequencies of a list of selected terms . this list depends on the topic to be filtered ; it is constructed in two steps . the first step defines the characteristic words used in the relevant documents of the corpus ; the second one chooses , among the previous list , the most discriminant ones . the length of the vector is optimized automatically for each topic . at the end of the term selection , a vector of typically 25 words is defined for the topic , so that each document which has to be processed is represented by a vector of term frequencies . this vector is subsequently input to a classifier that is trained from the same sample . after training , the classifier estimates for each document of a test set its probability of being relevant ; for submission to trec , the top 1000 documents are ranked in order of decreasing relevance ."}
{"title": "video highlight prediction using audience chat reactions", "abstract": "sports channel video portals offer an exciting domain for research on multimodal , multilingual analysis . we present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang , in both english and traditional chinese . we present a novel dataset based on league of legends championships recorded from north american and taiwanese twitch.tv channels ( will be released for further research ) , and demonstrate strong results on these using multimodal , character-level cnn-rnn model architectures ."}
{"title": "hardness results for approximate pure horn cnf formulae minimization", "abstract": "we study the hardness of approximation of clause minimum and literal minimum representations of pure horn functions in $ n $ boolean variables . we show that unless p=np , it is not possible to approximate in polynomial time the minimum number of clauses and the minimum number of literals of pure horn cnf representations to within a factor of $ 2^ { \\log^ { 1-o ( 1 ) } n } $ . this is the case even when the inputs are restricted to pure horn 3-cnfs with $ o ( n^ { 1+\\varepsilon } ) $ clauses , for some small positive constant $ \\varepsilon $ . furthermore , we show that even allowing sub-exponential time computation , it is still not possible to obtain constant factor approximations for such problems unless the exponential time hypothesis turns out to be false ."}
{"title": "semantic optimization techniques for preference queries", "abstract": "preference queries are relational algebra or sql queries that contain occurrences of the winnow operator ( `` find the most preferred tuples in a given relation '' ) . such queries are parameterized by specific preference relations . semantic optimization techniques make use of integrity constraints holding in the database . in the context of semantic optimization of preference queries , we identify two fundamental properties : containment of preference relations relative to integrity constraints and satisfaction of order axioms relative to integrity constraints . we show numerous applications of those notions to preference query evaluation and optimization . as integrity constraints , we consider constraint-generating dependencies , a class generalizing functional dependencies . we demonstrate that the problems of containment and satisfaction of order axioms can be captured as specific instances of constraint-generating dependency entailment . this makes it possible to formulate necessary and sufficient conditions for the applicability of our techniques as constraint validity problems . we characterize the computational complexity of such problems ."}
{"title": "on some equivalence relations between incidence calculus and dempster-shafer theory of evidence", "abstract": "incidence calculus and dempster-shafer theory of evidence are both theories to describe agents ' degrees of belief in propositions , thus being appropriate to represent uncertainty in reasoning systems . this paper presents a straightforward equivalence proof between some special cases of these theories ."}
{"title": "an optimization algorithm inspired by the states of matter that improves the balance between exploration and exploitation", "abstract": "the ability of an evolutionary algorithm ( ea ) to find a global optimal solution depends on its capacity to find a good rate between exploitation of found so far elements and exploration of the search space . inspired by natural phenomena , researchers have developed many successful evolutionary algorithms which , at original versions , define operators that mimic the way nature solves complex problems , with no actual consideration of the exploration/exploitation balance . in this paper , a novel nature-inspired algorithm called the states of matter search ( sms ) is introduced . the sms algorithm is based on the simulation of the states of matter phenomenon . in sms , individuals emulate molecules which interact to each other by using evolutionary operations which are based on the physical principles of the thermal-energy motion mechanism . the algorithm is devised by considering each state of matter at one different exploration/exploitation ratio . the evolutionary process is divided into three phases which emulate the three states of matter : gas , liquid and solid . in each state , molecules ( individuals ) exhibit different movement capacities . beginning from the gas state ( pure exploration ) , the algorithm modifies the intensities of exploration and exploitation until the solid state ( pure exploitation ) is reached . as a result , the approach can substantially improve the balance between exploration/exploitation , yet preserving the good search capabilities of an evolutionary approach ."}
{"title": "flow-based propagators for the sequence and related global constraints", "abstract": "we propose new filtering algorithms for the sequence constraint and some extensions of the sequence constraint based on network flows . we enforce domain consistency on the sequence constraint in $ o ( n^2 ) $ time down a branch of the search tree . this improves upon the best existing domain consistency algorithm by a factor of $ o ( \\log n ) $ . the flows used in these algorithms are derived from a linear program . some of them differ from the flows used to propagate global constraints like gcc since the domains of the variables are encoded as costs on the edges rather than capacities . such flows are efficient for maintaining bounds consistency over large domains and may be useful for other global constraints ."}
{"title": "feature base fusion for splicing forgery detection based on neuro fuzzy", "abstract": "most of researches on image forensics have been mainly focused on detection of artifacts introduced by a single processing tool . they lead in the development of many specialized algorithms looking for one or more particular footprints under specific settings . naturally , the performance of such algorithms are not perfect , and accordingly the provided output might be noisy , inaccurate and only partially correct . furthermore , a forged image in practical scenarios is often the result of utilizing several tools available by image-processing software systems . therefore , reliable tamper detection requires developing more poweful tools to deal with various tempering scenarios . fusion of forgery detection tools based on fuzzy inference system has been used before for addressing this problem . adjusting the membership functions and defining proper fuzzy rules for attaining to better results are time-consuming processes . this can be accounted as main disadvantage of fuzzy inference systems . in this paper , a neuro-fuzzy inference system for fusion of forgery detection tools is developed . the neural network characteristic of these systems provides appropriate tool for automatically adjusting the membership functions . moreover , initial fuzzy inference system is generated based on fuzzy clustering techniques . the proposed framework is implemented and validated on a benchmark image splicing data set in which three forgery detection tools are fused based on adaptive neuro-fuzzy inference system . the outcome of the proposed method reveals that applying neuro fuzzy inference systems could be a better approach for fusion of forgery detection tools ."}
{"title": "sample efficient policy search for optimal stopping domains", "abstract": "optimal stopping problems consider the question of deciding when to stop an observation-generating process in order to maximize a return . we examine the problem of simultaneously learning and planning in such domains , when data is collected directly from the environment . we propose gfse , a simple and flexible model-free policy search method that reuses data for sample efficiency by leveraging problem structure . we bound the sample complexity of our approach to guarantee uniform convergence of policy value estimates , tightening existing pac bounds to achieve logarithmic dependence on horizon length for our setting . we also examine the benefit of our method against prevalent model-based and model-free approaches on 3 domains taken from diverse fields ."}
{"title": "map estimation , linear programming and belief propagation with convex free energies", "abstract": "finding the most probable assignment ( map ) in a general graphical model is known to be np hard but good approximations have been attained with max-product belief propagation ( bp ) and its variants . in particular , it is known that using bp on a single-cycle graph or tree reweighted bp on an arbitrary graph will give the map solution if the beliefs have no ties . in this paper we extend the setting under which bp can be used to provably extract the map . we define convex bp as bp algorithms based on a convex free energy approximation and show that this class includes ordinary bp with single-cycle , tree reweighted bp and many other bp variants . we show that when there are no ties , fixed-points of convex max-product bp will provably give the map solution . we also show that convex sum-product bp at sufficiently small temperatures can be used to solve linear programs that arise from relaxing the map problem . finally , we derive a novel condition that allows us to derive the map solution even if some of the convex bp beliefs have ties . in experiments , we show that our theorems allow us to find the map in many real-world instances of graphical models where exact inference using junction-tree is impossible ."}
{"title": "a constrained sequence-to-sequence neural model for sentence simplification", "abstract": "sentence simplification reduces semantic complexity to benefit people with language impairments . previous simplification studies on the sentence level and word level have achieved promising results but also meet great challenges . for sentence-level studies , sentences after simplification are fluent but sometimes are not really simplified . for word-level studies , words are simplified but also have potential grammar errors due to different usages of words before and after simplification . in this paper , we propose a two-step simplification framework by combining both the word-level and the sentence-level simplifications , making use of their corresponding advantages . based on the two-step framework , we implement a novel constrained neural generation model to simplify sentences given simplified words . the final results on wikipedia and simple wikipedia aligned datasets indicate that our method yields better performance than various baselines ."}
{"title": "assessing human error against a benchmark of perfection", "abstract": "an increasing number of domains are providing us with detailed trace data on human decisions in settings where we can evaluate the quality of these decisions via an algorithm . motivated by this development , an emerging line of work has begun to consider whether we can characterize and predict the kinds of decisions where people are likely to make errors . to investigate what a general framework for human error prediction might look like , we focus on a model system with a rich history in the behavioral sciences : the decisions made by chess players as they select moves in a game . we carry out our analysis at a large scale , employing datasets with several million recorded games , and using chess tablebases to acquire a form of ground truth for a subset of chess positions that have been completely solved by computers but remain challenging even for the best players in the world . we organize our analysis around three categories of features that we argue are present in most settings where the analysis of human error is applicable : the skill of the decision-maker , the time available to make the decision , and the inherent difficulty of the decision . we identify rich structure in all three of these categories of features , and find strong evidence that in our domain , features describing the inherent difficulty of an instance are significantly more powerful than features based on skill or time ."}
{"title": "reasoning with uncertain knowledge", "abstract": "a model of knowledge representation is described in which propositional facts and the relationships among them can be supported by other facts . the set of knowledge which can be supported is called the set of cognitive units , each having associated descriptions of their explicit and implicit support structures , summarizing belief and reliability of belief . this summary is precise enough to be useful in a computational model while remaining descriptive of the underlying symbolic support structure . when a fact supports another supportive relationship between facts we call this meta-support . this facilitates reasoning about both the propositional knowledge . and the support structures underlying it ."}
{"title": "multi-pointer co-attention networks for recommendation", "abstract": "many recent state-of-the-art recommender systems such as d-att , transnet and deepconn exploit reviews for representation learning . this paper proposes a new neural architecture for recommendation with reviews . our model operates on a multi-hierarchical paradigm and is based on the intuition that not all reviews are created equal , i.e. , only a select few are important . the importance , however , should be dynamically inferred depending on the current target . to this end , we propose a review-by-review pointer-based learning scheme that extracts important reviews , subsequently matching them in a word-by-word fashion . this enables not only the most informative reviews to be utilized for prediction but also a deeper word-level interaction . our pointer-based method operates with a novel gumbel-softmax based pointer mechanism that enables the incorporation of discrete vectors within differentiable neural architectures . our pointer mechanism is co-attentive in nature , learning pointers which are co-dependent on user-item relationships . finally , we propose a multi-pointer learning scheme that learns to combine multiple views of interactions between user and item . overall , we demonstrate the effectiveness of our proposed model via extensive experiments on \\textbf { 24 } benchmark datasets from amazon and yelp . empirical results show that our approach significantly outperforms existing state-of-the-art , with up to 19 % and 71 % relative improvement when compared to transnet and deepconn respectively . we study the behavior of our multi-pointer learning mechanism , shedding light on evidence aggregation patterns in review-based recommender systems ."}
{"title": "inverse reinforcement learning with conditional choice probabilities", "abstract": "we make an important connection to existing results in econometrics to describe an alternative formulation of inverse reinforcement learning ( irl ) . in particular , we describe an algorithm using conditional choice probabilities ( ccp ) , which are maximum likelihood estimates of the policy estimated from expert demonstrations , to solve the irl problem . using the language of structural econometrics , we re-frame the optimal decision problem and introduce an alternative representation of value functions due to ( hotz and miller 1993 ) . in addition to presenting the theoretical connections that bridge the irl literature between economics and robotics , the use of ccps also has the practical benefit of reducing the computational cost of solving the irl problem . specifically , under the ccp representation , we show how one can avoid repeated calls to the dynamic programming subroutine typically used in irl . we show via extensive experimentation on standard irl benchmarks that ccp-irl is able to outperform maxent-irl , with as much as a 5x speedup and without compromising on the quality of the recovered reward function ."}
{"title": "better computer go player with neural network and long-term prediction", "abstract": "competing with top human players in the ancient game of go has been a long-term goal of artificial intelligence . go 's high branching factor makes traditional search techniques ineffective , even on leading-edge hardware , and go 's evaluation function could change drastically with one stone change . recent works [ maddison et al . ( 2015 ) ; clark & storkey ( 2015 ) ] show that search is not strictly necessary for machine go players . a pure pattern-matching approach , based on a deep convolutional neural network ( dcnn ) that predicts the next move , can perform as well as monte carlo tree search ( mcts ) -based open source go engines such as pachi [ baudis & gailly ( 2012 ) ] if its search budget is limited . we extend this idea in our bot named darkforest , which relies on a dcnn designed for long-term predictions . darkforest substantially improves the win rate for pattern-matching approaches against mcts-based approaches , even with looser search budgets . against human players , the newest versions , darkfores2 , achieve a stable 3d level on kgs go server as a ranked bot , a substantial improvement upon the estimated 4k-5k ranks for dcnn reported in clark & storkey ( 2015 ) based on games against other machine players . adding mcts to darkfores2 creates a much stronger player named darkfmcts3 : with 5000 rollouts , it beats pachi with 10k rollouts in all 250 games ; with 75k rollouts it achieves a stable 5d level in kgs server , on par with state-of-the-art go ais ( e.g. , zen , dolbaram , crazystone ) except for alphago [ silver et al . ( 2016 ) ] ; with 110k rollouts , it won the 3rd place in january kgs go tournament ."}
{"title": "graph clustering bandits for recommendation", "abstract": "we investigate an efficient context-dependent clustering technique for recommender systems based on exploration-exploitation strategies through multi-armed bandits over multiple users . our algorithm dynamically groups users based on their observed behavioral similarity during a sequence of logged activities . in doing so , the algorithm reacts to the currently served user by shaping clusters around him/her but , at the same time , it explores the generation of clusters over users which are not currently engaged . we motivate the effectiveness of this clustering policy , and provide an extensive empirical analysis on real-world datasets , showing scalability and improved prediction performance over state-of-the-art methods for sequential clustering of users in multi-armed bandit scenarios ."}
{"title": "boolean equi-propagation for concise and efficient sat encodings of combinatorial problems", "abstract": "we present an approach to propagation-based sat encoding of combinatorial problems , boolean equi-propagation , where constraints are modeled as boolean functions which propagate information about equalities between boolean literals . this information is then applied to simplify the cnf encoding of the constraints . a key factor is that considering only a small fragment of a constraint model at one time enables us to apply stronger , and even complete , reasoning to detect equivalent literals in that fragment . once detected , equivalences apply to simplify the entire constraint model and facilitate further reasoning on other fragments . equi-propagation in combination with partial evaluation and constraint simplification provide the foundation for a powerful approach to sat-based finite domain constraint solving . we introduce a tool called bee ( ben-gurion equi-propagation encoder ) based on these ideas and demonstrate for a variety of benchmarks that our approach leads to a considerable reduction in the size of cnf encodings and subsequent speed-ups in sat solving times ."}
{"title": "student modeling using case-based reasoning in conventional learning system", "abstract": "conventional face-to-face classrooms are still the main learning system applied in indonesia . in assisting such conventional learning towards an optimal learning , formative evaluations are needed to monitor the progress of the class . this task can be very hard when the size of the class is large . hence , this research attempted to create a classroom monitoring system based on student data of department of electrical engineering and information technology . in order to achieve the goal , a student modeling using case-based reasoning was proposed . a generic student model based on a framework was developed . the model represented student knowledge of a subject . the result showed that the system was able to store and retrieve student data for suggestion of the current situation and formative evaluation for one of the subject in the department ."}
{"title": "concurrent cube-and-conquer", "abstract": "recent work introduced the cube-and-conquer technique to solve hard sat instances . it partitions the search space into cubes using a lookahead solver . each cube is tackled by a conflict-driven clause learning ( cdcl ) solver . crucial for strong performance is the cutoff heuristic that decides when to switch from lookahead to cdcl . yet , this offline heuristic is far from ideal . in this paper , we present a novel hybrid solver that applies the cube and conquer steps simultaneously . a lookahead and a cdcl solver work together on each cube , while communication is restricted to synchronization . our concurrent cube-and-conquer solver can solve many instances faster than pure lookahead , pure cdcl and offline cube-and-conquer , and can abort early in favor of a pure cdcl search if an instance is not suitable for cube-and-conquer techniques ."}
{"title": "design of a framework to facilitate decisions using information fusion", "abstract": "information fusion is an advanced research area which can assist decision makers in enhancing their decisions . this paper aims at designing a new multi-layer framework that can support the process of performing decisions from the obtained beliefs using information fusion . since it is not an easy task to cross the gap between computed beliefs of certain hypothesis and decisions , the proposed framework consists of the following layers in order to provide a suitable architecture ( ordered bottom up ) : 1. a layer for combination of basic belief assignments using an information fusion approach . such approach exploits dezert-smarandache theory , dsmt , and proportional conflict redistribution to provide more realistic final beliefs . 2. a layer for computation of pignistic probability of the underlying propositions from the corresponding final beliefs . 3. a layer for performing probabilistic reasoning using a bayesian network that can obtain the probable reason of a proposition from its pignistic probability . 4. ranking the system decisions is ultimately used to support decision making . a case study has been accomplished at various operational conditions in order to prove the concept , in addition it pointed out that : 1. the use of dsmt for information fusion yields not only more realistic beliefs but also reliable pignistic probabilities for the underlying propositions . 2. exploiting the pignistic probability for the integration of the information fusion with the bayesian network provides probabilistic inference and enable decision making on the basis of both belief based probabilities for the underlying propositions and bayesian based probabilities for the corresponding reasons . a comparative study of the proposed framework with respect to other information fusion systems confirms its superiority to support decision making ."}
{"title": "robots as powerful allies for the study of embodied cognition from the bottom up", "abstract": "a large body of compelling evidence has been accumulated demonstrating that embodiment - the agent 's physical setup , including its shape , materials , sensors and actuators - is constitutive for any form of cognition and as a consequence , models of cognition need to be embodied . in contrast to methods from empirical sciences to study cognition , robots can be freely manipulated and virtually all key variables of their embodiment and control programs can be systematically varied . as such , they provide an extremely powerful tool of investigation . we present a robotic bottom-up or developmental approach , focusing on three stages : ( a ) low-level behaviors like walking and reflexes , ( b ) learning regularities in sensorimotor spaces , and ( c ) human-like cognition . we also show that robotic based research is not only a productive path to deepening our understanding of cognition , but that robots can strongly benefit from human-like cognition in order to become more autonomous , robust , resilient , and safe ."}
{"title": "worldtree : a corpus of explanation graphs for elementary science questions supporting multi-hop inference", "abstract": "developing methods of automated inference that are able to provide users with compelling human-readable justifications for why the answer to a question is correct is critical for domains such as science and medicine , where user trust and detecting costly errors are limiting factors to adoption . one of the central barriers to training question answering models on explainable inference tasks is the lack of gold explanations to serve as training data . in this paper we present a corpus of explanations for standardized science exams , a recent challenge task for question answering . we manually construct a corpus of detailed explanations for nearly all publicly available standardized elementary science question ( approximately 1,680 3rd through 5th grade questions ) and represent these as `` explanation graphs '' -- sets of lexically overlapping sentences that describe how to arrive at the correct answer to a question through a combination of domain and world knowledge . we also provide an explanation-centered tablestore , a collection of semi-structured tables that contain the knowledge to construct these elementary science explanations . together , these two knowledge resources map out a substantial portion of the knowledge required for answering and explaining elementary science exams , and provide both structured and free-text training data for the explainable inference task ."}
{"title": "visualizing natural language descriptions : a survey", "abstract": "a natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines . one of the promising applications of such interfaces is generating visual interpretations of semantic content of a given natural language that can be then visualized either as a static scene or a dynamic animation . this survey discusses requirements and challenges of developing such systems and reports 26 graphical systems that exploit natural language interfaces and addresses both artificial intelligence and visualization aspects . this work serves as a frame of reference to researchers and to enable further advances in the field ."}
{"title": "further experimental evidence against the utility of occam 's razor", "abstract": "this paper presents new experimental evidence against the utility of occam 's razor . a~systematic procedure is presented for post-processing decision trees produced by c4.5 . this procedure was derived by rejecting occam 's razor and instead attending to the assumption that similar objects are likely to belong to the same class . it increases a decision tree 's complexity without altering the performance of that tree on the training data from which it is inferred . the resulting more complex decision trees are demonstrated to have , on average , for a variety of common learning tasks , higher predictive accuracy than the less complex original decision trees . this result raises considerable doubt about the utility of occam 's razor as it is commonly applied in modern machine learning ."}
{"title": "generalization without systematicity : on the compositional skills of sequence-to-sequence recurrent networks", "abstract": "humans can understand and produce new utterances effortlessly , thanks to their compositional skills . once a person learns the meaning of a new verb `` dax , '' he or she can immediately understand the meaning of `` dax twice '' or `` sing and dax . '' in this paper , we introduce the scan domain , consisting of a set of simple compositional navigation commands paired with the corresponding action sequences . we then test the zero-shot generalization capabilities of a variety of recurrent neural networks ( rnns ) trained on scan with sequence-to-sequence methods . we find that rnns can make successful zero-shot generalizations when the differences between training and test commands are small , so that they can apply `` mix-and-match '' strategies to solve the task . however , when generalization requires systematic compositional skills ( as in the `` dax '' example above ) , rnns fail spectacularly . we conclude with a proof-of-concept experiment in neural machine translation , suggesting that lack of systematicity might be partially responsible for neural networks ' notorious training data thirst ."}
{"title": "cogscik : clustering for cognitive science motivated decision making", "abstract": "computational models of decisionmaking must contend with the variance of context and any number of possible decisions that a defined strategic actor can make at a given time . relying on cognitive science theory , the authors have created an algorithm that captures the orientation of the actor towards an object and arrays the possible decisions available to that actor based on their given intersubjective orientation . this algorithm , like a traditional k-means clustering algorithm , relies on a core-periphery structure that gives the likelihood of moves as those closest to the cluster 's centroid . the result is an algorithm that enables unsupervised classification of an array of decision points belonging to an actor 's present state and deeply rooted in cognitive science theory ."}
{"title": "on the usability of probably approximately correct implication bases", "abstract": "we revisit the notion of probably approximately correct implication bases from the literature and present a first formulation in the language of formal concept analysis , with the goal to investigate whether such bases represent a suitable substitute for exact implication bases in practical use-cases . to this end , we quantitatively examine the behavior of probably approximately correct implication bases on artificial and real-world data sets and compare their precision and recall with respect to their corresponding exact implication bases . using a small example , we also provide qualitative insight that implications from probably approximately correct bases can still represent meaningful knowledge from a given data set ."}
{"title": "effective dimensions of hierarchical latent class models", "abstract": "hierarchical latent class ( hlc ) models are tree-structured bayesian networks where leaf nodes are observed while internal nodes are latent . there are no theoretically well justified model selection criteria for hlc models in particular and bayesian networks with latent nodes in general . nonetheless , empirical studies suggest that the bic score is a reasonable criterion to use in practice for learning hlc models . empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced with effective model dimension in the penalty term of the bic score . effective dimensions are difficult to compute . in this paper , we prove a theorem that relates the effective dimension of an hlc model to the effective dimensions of a number of latent class models . the theorem makes it computationally feasible to compute the effective dimensions of large hlc models . the theorem can also be used to compute the effective dimensions of general tree models ."}
{"title": "embedding and automating conditional logics in classical higher-order logic", "abstract": "a sound and complete embedding of conditional logics into classical higher-order logic is presented . this embedding enables the application of off-the-shelf higher-order automated theorem provers and model finders for reasoning within and about conditional logics ."}
{"title": "decision-theoretic planning with non-markovian rewards", "abstract": "a decision process in which rewards depend on history rather than merely on the current state is called a decision process with non-markovian rewards ( nmrdp ) . in decision-theoretic planning , where many desirable behaviours are more naturally expressed as properties of execution sequences rather than as properties of states , nmrdps form a more natural model than the commonly adopted fully markovian decision process ( mdp ) model . while the more tractable solution methods developed for mdps do not directly apply in the presence of non-markovian rewards , a number of solution methods for nmrdps have been proposed in the literature . these all exploit a compact specification of the non-markovian reward function in temporal logic , to automatically translate the nmrdp into an equivalent mdp which is solved using efficient mdp solution methods . this paper presents nmrdpp ( non-markovian reward decision process planner ) , a software platform for the development and experimentation of methods for decision-theoretic planning with non-markovian rewards . the current version of nmrdpp implements , under a single interface , a family of methods based on existing as well as new approaches which we describe in detail . these include dynamic programming , heuristic search , and structured methods . using nmrdpp , we compare the methods and identify certain problem features that affect their performance . nmrdpps treatment of non-markovian rewards is inspired by the treatment of domain-specific search control knowledge in the tlplan planner , which it incorporates as a special case . in the first international probabilistic planning competition , nmrdpp was able to compete and perform well in both the domain-independent and hand-coded tracks , using search control knowledge in the latter ."}
{"title": "domain adaptation : learning bounds and algorithms", "abstract": "this paper addresses the general problem of domain adaptation which arises in a variety of applications where the distribution of the labeled sample available somewhat differs from that of the test data . building on previous work by ben-david et al . ( 2007 ) , we introduce a novel distance between distributions , discrepancy distance , that is tailored to adaptation problems with arbitrary loss functions . we give rademacher complexity bounds for estimating the discrepancy distance from finite samples for different loss functions . using this distance , we derive novel generalization bounds for domain adaptation for a wide family of loss functions . we also present a series of novel adaptation bounds for large classes of regularization-based algorithms , including support vector machines and kernel ridge regression based on the empirical discrepancy . this motivates our analysis of the problem of minimizing the empirical discrepancy for various loss functions for which we also give novel algorithms . we report the results of preliminary experiments that demonstrate the benefits of our discrepancy minimization algorithms for domain adaptation ."}
{"title": "combination of upper and lower probabilities", "abstract": "in this paper , we consider several types of information and methods of combination associated with incomplete probabilistic systems . we discriminate between 'a priori ' and evidential information . the former one is a description of the whole population , the latest is a restriction based on observations for a particular case . then , we propose different combination methods for each one of them . we also consider conditioning as the heterogeneous combination of 'a priori ' and evidential information . the evidential information is represented as a convex set of likelihood functions . these will have an associated possibility distribution with behavior according to classical possibility theory ."}
{"title": "virtual pet images from ct data using deep convolutional networks : initial results", "abstract": "in this work we present a novel system for pet estimation using ct scans . we explore the use of fully convolutional networks ( fcn ) and conditional generative adversarial networks ( gan ) to export pet data from ct data . our dataset includes 25 pairs of pet and ct scans where 17 were used for training and 8 for testing . the system was tested for detection of malignant tumors in the liver region . initial results look promising showing high detection performance with a tpr of 92.3 % and fpr of 0.25 per case . future work entails expansion of the current system to the entire body using a much larger dataset . such a system can be used for tumor detection and drug treatment evaluation in a ct-only environment instead of the expansive and radioactive pet-ct scan ."}
{"title": "superintelligence can not be contained : lessons from computability theory", "abstract": "superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds . in light of recent advances in machine intelligence , a number of scientists , philosophers and technologists have revived the discussion about the potential catastrophic risks entailed by such an entity . in this article , we trace the origins and development of the neo-fear of superintelligence , and some of the major proposals for its containment . we argue that such containment is , in principle , impossible , due to fundamental limits inherent to computing itself . assuming that a superintelligence will contain a program that includes all the programs that can be executed by a universal turing machine on input potentially as complex as the state of the world , strict containment requires simulations of such a program , something theoretically ( and practically ) infeasible ."}
{"title": "self-critical sequence training for image captioning", "abstract": "recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand . in this paper we consider the problem of optimizing image captioning systems using reinforcement learning , and show that by carefully optimizing our systems using the test metrics of the mscoco task , significant gains in performance can be realized . our systems are built using a new optimization approach that we call self-critical sequence training ( scst ) . scst is a form of the popular reinforce algorithm that , rather than estimating a `` baseline '' to normalize the rewards and reduce variance , utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences . using this approach , estimating the reward signal ( as actor-critic methods must do ) and estimating normalization ( as reinforce algorithms typically do ) is avoided , while at the same time harmonizing the model with respect to its test-time inference procedure . empirically we find that directly optimizing the cider metric with scst and greedy decoding at test-time is highly effective . our results on the mscoco evaluation sever establish a new state-of-the-art on the task , improving the best result in terms of cider from 104.9 to 114.7 ."}
{"title": "a personalized system for conversational recommendations", "abstract": "searching for and making decisions about information is becoming increasingly difficult as the amount of information and number of choices increases . recommendation systems help users find items of interest of a particular type , such as movies or restaurants , but are still somewhat awkward to use . our solution is to take advantage of the complementary strengths of personalized recommendation systems and dialogue systems , creating personalized aides . we present a system -- the adaptive place advisor -- that treats item selection as an interactive , conversational process , with the program inquiring about item attributes and the user responding . individual , long-term user preferences are unobtrusively obtained in the course of normal recommendation dialogues and used to direct future conversations with the same user . we present a novel user model that influences both item search and the questions asked during a conversation . we demonstrate the effectiveness of our system in significantly reducing the time and number of interactions required to find a satisfactory item , as compared to a control group of users interacting with a non-adaptive version of the system ."}
{"title": "event-object reasoning with curated knowledge bases : deriving missing information", "abstract": "the broader goal of our research is to formulate answers to why and how questions with respect to knowledge bases , such as aura . one issue we face when reasoning with many available knowledge bases is that at times needed information is missing . examples of this include partially missing information about next sub-event , first sub-event , last sub-event , result of an event , input to an event , destination of an event , and raw material involved in an event . in many cases one can recover part of the missing knowledge through reasoning . in this paper we give a formal definition about how such missing information can be recovered and then give an asp implementation of it . we then discuss the implication of this with respect to answering why and how questions ."}
{"title": "fixpoint approximation of strategic abilities under imperfect information", "abstract": "model checking of strategic ability under imperfect information is known to be hard . the complexity results range from np-completeness to undecidability , depending on the precise setup of the problem . no less importantly , fixpoint equivalences do not generally hold for imperfect information strategies , which seriously hampers incremental synthesis of winning strategies . in this paper , we propose translations of atlir formulae that provide lower and upper bounds for their truth values , and are cheaper to verify than the original specifications . that is , if the expression is verified as true then the corresponding formula of atlir should also hold in the given model . we begin by showing where the straightforward approach does not work . then , we propose how it can be modified to obtain guaranteed lower bounds . to this end , we alter the next-step operator in such a way that traversing one 's indistinguishability relation is seen as atomic activity . most interestingly , the lower approximation is provided by a fixpoint expression that uses a nonstandard variant of the next-step ability operator . we show the correctness of the translations , establish their computational complexity , and validate the approach by experiments with a scalable scenario of bridge play ."}
{"title": "breaking value symmetry", "abstract": "symmetry is an important factor in solving many constraint satisfaction problems . one common type of symmetry is when we have symmetric values . in a recent series of papers , we have studied methods to break value symmetries . our results identify computational limits on eliminating value symmetry . for instance , we prove that pruning all symmetric values is np-hard in general . nevertheless , experiments show that much value symmetry can be broken in practice . these results may be useful to researchers in planning , scheduling and other areas as value symmetry occurs in many different domains ."}
{"title": "optimal control for a robotic exploration , pick-up and delivery problem", "abstract": "this paper addresses an optimal control problem for a robot that has to find and collect a finite number of objects and move them to a depot in minimum time . the robot has fourth-order dynamics that change instantaneously at any pick-up or drop-off of an object . the objects are modeled by point masses with a-priori unknown locations in a bounded two-dimensional space that may contain unknown obstacles . for this hybrid system , an optimal control problem ( ocp ) is approximately solved by a receding horizon scheme , where the derived lower bound for the cost-to-go is evaluated for the worst and for a probabilistic case , assuming a uniform distribution of the objects . first , a time-driven approximate solution based on time and position space discretization and mixed integer programming is presented . due to the high computational cost of this solution , an alternative event-driven approximate approach based on a suitable motion parameterization and gradient-based optimization is proposed . the solutions are compared in a numerical example , suggesting that the latter approach offers a significant computational advantage while yielding similar qualitative results compared to the former . the methods are particularly relevant for various robotic applications like automated cleaning , search and rescue , harvesting or manufacturing ."}
{"title": "belief hierarchical clustering", "abstract": "in the data mining field many clustering methods have been proposed , yet standard versions do not take into account uncertain databases . this paper deals with a new approach to cluster uncertain data by using a hierarchical clustering defined within the belief function framework . the main objective of the belief hierarchical clustering is to allow an object to belong to one or several clusters . to each belonging , a degree of belief is associated , and clusters are combined based on the pignistic properties . experiments with real uncertain data show that our proposed method can be considered as a propitious tool ."}
{"title": "hidden physics models : machine learning of nonlinear partial differential equations", "abstract": "while there is currently a lot of enthusiasm about `` big data '' , useful data is usually `` small '' and expensive to acquire . in this paper , we present a new paradigm of learning partial differential equations from { \\em small } data . in particular , we introduce \\emph { hidden physics models } , which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics , expressed by time dependent and nonlinear partial differential equations , to extract patterns from high-dimensional data generated from experiments . the proposed methodology may be applied to the problem of learning , system identification , or data-driven discovery of partial differential equations . our framework relies on gaussian processes , a powerful tool for probabilistic inference over functions , that enables us to strike a balance between model complexity and data fitting . the effectiveness of the proposed approach is demonstrated through a variety of canonical problems , spanning a number of scientific domains , including the navier-stokes , schr\\ '' odinger , kuramoto-sivashinsky , and time dependent linear fractional equations . the methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data ."}
{"title": "cross-lingual pseudo-projected expectation regularization for weakly supervised learning", "abstract": "we consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages . past approaches project labels across bitext and use them as features or gold labels for training . we propose a new method that projects model expectations rather than labels , which facilities transfer of model uncertainty across language boundaries . we encode expectations as constraints and train a discriminative crf model using generalized expectation criteria ( mann and mccallum , 2010 ) . evaluated on standard chinese-english and german-english ner datasets , our method demonstrates f1 scores of 64 % and 60 % when no labeled data is used . attaining the same accuracy with supervised crfs requires 12k and 1.5k labeled sentences . furthermore , when combined with labeled examples , our method yields significant improvements over state-of-the-art supervised methods , achieving best reported numbers to date on chinese ontonotes and german conll-03 datasets ."}
{"title": "decision support systems ( dss ) in construction tendering processes", "abstract": "the successful execution of a construction project is heavily impacted by making the right decision during tendering processes . managing tender procedures is very complex and uncertain involving coordination of many tasks and individuals with different priorities and objectives . bias and inconsistent decision are inevitable if the decision-making process is totally depends on intuition , subjective judgement or emotion . in making transparent decision and healthy competition tendering , there exists a need for flexible guidance tool for decision support . aim of this paper is to give a review on current practices of decision support systems ( dss ) technology in construction tendering processes . current practices of general tendering processes as applied to the most countries in different regions such as united states , europe , middle east and asia are comprehensively discussed . applications of web-based tendering processes is also summarised in terms of its properties . besides that , a summary of decision support system ( dss ) components is included in the next section . furthermore , prior researches on implementation of dss approaches in tendering processes are discussed in details . current issues arise from both of paper-based and web-based tendering processes are outlined . finally , conclusion is included at the end of this paper ."}
{"title": "memory augmented control networks", "abstract": "planning problems in partially observable environments can not be solved directly with convolutional networks and require some form of memory . but , even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan . to mitigate these challenges we introduce the memory augmented control network ( macn ) . the proposed network architecture consists of three main parts . the first part uses convolutions to extract features and the second part uses a neural network-based planning module to pre-plan in the environment . the third part uses a network controller that learns to store those specific instances of past information that are necessary for planning . the performance of the network is evaluated in discrete grid world environments for path planning in the presence of simple and complex obstacles . we show that our network learns to plan and can generalize to new environments ."}
{"title": "on the min-cost traveling salesman problem with drone", "abstract": "once known to be used exclusively in military domain , unmanned aerial vehicles ( drones ) have stepped up to become a part of new logistic method in commercial sector called `` last-mile delivery '' . in this novel approach , small unmanned aerial vehicles ( uav ) , also known as drones , are deployed alongside with trucks to deliver goods to customers in order to improve the service quality or reduce the transportation cost . it gives rise to a new variant of the traveling salesman problem ( tsp ) , of which we call tsp with drone ( tsp-d ) . in this article , we consider a variant of tsp-d where the main objective is to minimize the total transportation cost . we also propose two heuristics : `` drone first , truck second '' ( dfts ) and `` truck first , drone second '' ( tfds ) , to effectively solve the problem . the former constructs route for drone first while the latter constructs route for truck first . we solve a tsp to generate route for truck and propose a mixed integer programming ( mip ) formulation with different profit functions to build route for drone . numerical results obtained on many instances with different sizes and characteristics are presented . recommendations on promising algorithm choices are also provided ."}
{"title": "squishednets : squishing squeezenet further for edge device scenarios via deep evolutionary synthesis", "abstract": "while deep neural networks have been shown in recent years to outperform other machine learning methods in a wide range of applications , one of the biggest challenges with enabling deep neural networks for widespread deployment on edge devices such as mobile and other consumer devices is high computational and memory requirements . recently , there has been greater exploration into small deep neural network architectures that are more suitable for edge devices , with one of the most popular architectures being squeezenet , with an incredibly small model size of 4.8mb . taking further advantage of the notion that many applications of machine learning on edge devices are often characterized by a low number of target classes , this study explores the utility of combining architectural modifications and an evolutionary synthesis strategy for synthesizing even smaller deep neural architectures based on the more recent squeezenet v1.1 macroarchitecture for applications with fewer target classes . in particular , architectural modifications are first made to squeezenet v1.1 to accommodate for a 10-class imagenet-10 dataset , and then an evolutionary synthesis strategy is leveraged to synthesize more efficient deep neural networks based on this modified macroarchitecture . the resulting squishednets possess model sizes ranging from 2.4mb to 0.95mb ( ~5.17x smaller than squeezenet v1.1 , or 253x smaller than alexnet ) . furthermore , the squishednets are still able to achieve accuracies ranging from 81.2 % to 77 % , and able to process at speeds of 156 images/sec to as much as 256 images/sec on a nvidia jetson tx1 embedded chip . these preliminary results show that a combination of architectural modifications and an evolutionary synthesis strategy can be a useful tool for producing very small deep neural network architectures that are well-suited for edge device scenarios ."}
{"title": "a double competitive strategy based learning automata algorithm", "abstract": "learning automata ( la ) are considered as one of the most powerful tools in the field of reinforcement learning . the family of estimator algorithms is proposed to improve the convergence rate of la and has made great achievements . however , the estimators perform poorly on estimating the reward probabilities of actions in the initial stage of the learning process of la . in this situation , a lot of rewards would be added to the probabilities of non-optimal actions . thus , a large number of extra iterations are needed to compensate for these wrong rewards . in order to improve the speed of convergence , we propose a new p-model absorbing learning automaton by utilizing a double competitive strategy which is designed for updating the action probability vector . in this way , the wrong rewards can be corrected instantly . hence , the proposed double competitive algorithm overcomes the drawbacks of existing estimator algorithms . a refined analysis is presented to show the $ \\epsilon-optimality $ of the proposed scheme . the extensive experimental results in benchmark environments demonstrate that our proposed learning automata perform more efficiently than the most classic la $ se_ { ri } $ and the current fastest la $ dgcpa^ { * } $ ."}
{"title": "memorize or generalize ? searching for a compositional rnn in a haystack", "abstract": "neural networks are very powerful learning systems , but they do not readily generalize from one task to the other . this is partly due to the fact that they do not learn in a compositional way , that is , by discovering skills that are shared by different tasks , and recombining them to solve new problems . in this paper , we explore the compositional generalization capabilities of recurrent neural networks ( rnns ) . we first propose the lookup table composition domain as a simple setup to test compositional behaviour and show that it is theoretically possible for a standard rnn to learn to behave compositionally in this domain when trained with standard gradient descent and provided with additional supervision . we then remove this additional supervision and perform a search over a large number of model initializations to investigate the proportion of rnns that can still converge to a compositional solution . we discover that a small but non-negligible proportion of rnns do reach partial compositional solutions even without special architectural constraints . this suggests that a combination of gradient descent and evolutionary strategies directly favouring the minority models that developed more compositional approaches might suffice to lead standard rnns towards compositional solutions ."}
{"title": "a high speed multi-label classifier based on extreme learning machines", "abstract": "in this paper a high speed neural network classifier based on extreme learning machines for multi-label classification problem is proposed and dis-cussed . multi-label classification is a superset of traditional binary and multi-class classification problems . the proposed work extends the extreme learning machine technique to adapt to the multi-label problems . as opposed to the single-label problem , both the number of labels the sample belongs to , and each of those target labels are to be identified for multi-label classification resulting in in-creased complexity . the proposed high speed multi-label classifier is applied to six benchmark datasets comprising of different application areas such as multi-media , text and biology . the training time and testing time of the classifier are compared with those of the state-of-the-arts methods . experimental studies show that for all the six datasets , our proposed technique have faster execution speed and better performance , thereby outperforming all the existing multi-label clas-sification methods ."}
{"title": "mdps with unawareness", "abstract": "markov decision processes ( mdps ) are widely used for modeling decision-making problems in robotics , automated control , and economics . traditional mdps assume that the decision maker ( dm ) knows all states and actions . however , this may not be true in many situations of interest . we define a new framework , mdps with unawareness ( mdpus ) to deal with the possibilities that a dm may not be aware of all possible actions . we provide a complete characterization of when a dm can learn to play near-optimally in an mdpu , and give an algorithm that learns to play near-optimally when it is possible to do so , as efficiently as possible . in particular , we characterize when a near-optimal solution can be found in polynomial time ."}
{"title": "tuned and gpu-accelerated parallel data mining from comparable corpora", "abstract": "the multilingual nature of the world makes translation a crucial requirement today . parallel dictionaries constructed by humans are a widely-available resource , but they are limited and do not provide enough coverage for good quality translation purposes , due to out-of-vocabulary words and neologisms . this motivates the use of statistical translation systems , which are unfortunately dependent on the quantity and quality of training data . such has a very limited availability especially for some languages and very narrow text domains . is this research we present our improvements to yalign mining methodology by reimplementing the comparison algorithm , introducing a tuning scripts and by improving performance using gpu computing acceleration . the experiments are conducted on various text domains and bi-data is extracted from the wikipedia dumps ."}
{"title": "how good is the shapley value-based approach to the influence maximization problem ?", "abstract": "the shapley value has been recently advocated as a method to choose the seed nodes for the process of information diffusion . intuitively , since the shapley value evaluates the average marginal contribution of a player to the coalitional game , it can be used in the network context to evaluate the marginal contribution of a node in the process of information diffusion given various groups of already 'infected ' nodes . although the above direction of research seems promising , the current liter- ature is missing a throughout assessment of its performance . the aim of this work is to provide such an assessment of the existing shapley value-based approaches to information diffusion ."}
{"title": "influence-directed explanations for deep convolutional networks", "abstract": "we study the problem of explaining a rich class of behavioral properties of deep neural networks . distinctively , our influence-directed explanations approach this problem by peering inside the net- work to identify neurons with high influence on the property and distribution of interest using an axiomatically justified influence measure , and then providing an interpretation for the concepts these neurons represent . we evaluate our approach by training convolutional neural net- works on mnist , imagenet , pubfig , and diabetic retinopathy datasets . our evaluation demonstrates that influence-directed explanations ( 1 ) identify influential concepts that generalize across instances , ( 2 ) help extract the essence of what the network learned about a class , ( 3 ) isolate individual features the network uses to make decisions and distinguish related instances , and ( 4 ) assist in understanding misclassifications ."}
{"title": "on the complexity and completeness of static constraints for breaking row and column symmetry", "abstract": "we consider a common type of symmetry where we have a matrix of decision variables with interchangeable rows and columns . a simple and efficient method to deal with such row and column symmetry is to post symmetry breaking constraints like doublelex and snakelex . we provide a number of positive and negative results on posting such symmetry breaking constraints . on the positive side , we prove that we can compute in polynomial time a unique representative of an equivalence class in a matrix model with row and column symmetry if the number of rows ( or of columns ) is bounded and in a number of other special cases . on the negative side , we show that whilst doublelex and snakelex are often effective in practice , they can leave a large number of symmetric solutions in the worst case . in addition , we prove that propagating doublelex completely is np-hard . finally we consider how to break row , column and value symmetry , correcting a result in the literature about the safeness of combining different symmetry breaking constraints . we end with the first experimental study on how much symmetry is left by doublelex and snakelex on some benchmark problems ."}
{"title": "deductive and analogical reasoning on a semantically embedded knowledge graph", "abstract": "representing knowledge as high-dimensional vectors in a continuous semantic vector space can help overcome the brittleness and incompleteness of traditional knowledge bases . we present a method for performing deductive reasoning directly in such a vector space , combining analogy , association , and deduction in a straightforward way at each step in a chain of reasoning , drawing on knowledge from diverse sources and ontologies ."}
{"title": "a scalable conditional independence test for nonlinear , non-gaussian data", "abstract": "many relations of scientific interest are nonlinear , and even in linear systems distributions are often non-gaussian , for example in fmri bold data . a class of search procedures for causal relations in high dimensional data relies on sample derived conditional independence decisions . the most common applications rely on gaussian tests that can be systematically erroneous in nonlinear non-gaussian cases . recent work ( gretton et al . ( 2009 ) , tillman et al . ( 2009 ) , zhang et al . ( 2011 ) ) has proposed conditional independence tests using reproducing kernel hilbert spaces ( rkhs ) . among these , perhaps the most efficient has been kci ( kernel conditional independence , zhang et al . ( 2011 ) ) , with computational requirements that grow effectively at least as o ( n3 ) , placing it out of range of large sample size analysis , and restricting its applicability to high dimensional data sets . we propose a class of o ( n2 ) tests using conditional correlation independence ( cci ) that require a few seconds on a standard workstation for tests that require tens of minutes to hours for the kci method , depending on degree of parallelization , with similar accuracy . for accuracy on difficult nonlinear , non-gaussian data sets , we also compare a recent test due to harris & drton ( 2012 ) , applicable to nonlinear , non-gaussian distributions in the gaussian copula , as well as to partial correlation , a linear gaussian test ."}
{"title": "reinforcement learning in partially observable markov decision processes using hybrid probabilistic logic programs", "abstract": "we present a probabilistic logic programming framework to reinforcement learning , by integrating reinforce-ment learning , in pomdp environments , with normal hybrid probabilistic logic programs with probabilistic answer set seman-tics , that is capable of representing domain-specific knowledge . we formally prove the correctness of our approach . we show that the complexity of finding a policy for a reinforcement learning problem in our approach is np-complete . in addition , we show that any reinforcement learning problem can be encoded as a classical logic program with answer set semantics . we also show that a reinforcement learning problem can be encoded as a sat problem . we present a new high level action description language that allows the factored representation of pomdp . moreover , we modify the original model of pomdp so that it be able to distinguish between knowledge producing actions and actions that change the environment ."}
{"title": "computational pathology : challenges and promises for tissue analysis", "abstract": "the histological assessment of human tissue has emerged as the key challenge for detection and treatment of cancer . a plethora of different data sources ranging from tissue microarray data to gene expression , proteomics or metabolomics data provide a detailed overview of the health status of a patient . medical doctors need to assess these information sources and they rely on data driven automatic analysis tools . methods for classification , grouping and segmentation of heterogeneous data sources as well as regression of noisy dependencies and estimation of survival probabilities enter the processing workflow of a pathology diagnosis system at various stages . this paper reports on state-of-the-art of the design and effectiveness of computational pathology workflows and it discusses future research directions in this emergent field of medical informatics and diagnostic machine learning ."}
{"title": "universal intelligence : a definition of machine intelligence", "abstract": "a fundamental problem in artificial intelligence is that nobody really knows what intelligence is . the problem is especially acute when we need to consider artificial systems which are significantly different to humans . in this paper we approach this problem in the following way : we take a number of well known informal definitions of human intelligence that have been given by experts , and extract their essential features . these are then mathematically formalised to produce a general measure of intelligence for arbitrary machines . we believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense . we then show how this formal definition is related to the theory of universal optimal learning agents . finally , we survey the many other tests and definitions of intelligence that have been proposed for machines ."}
{"title": "consecutive support : better be close !", "abstract": "we propose a new measure of support ( the number of occur- rences of a pattern ) , in which instances are more important if they occur with a certain frequency and close after each other in the stream of trans- actions . we will explain this new consecutive support and discuss how patterns can be found faster by pruning the search space , for instance using so-called parent support recalculation . both consecutiveness and the notion of hypercliques are incorporated into the eclat algorithm . synthetic examples show how interesting phenomena can now be discov- ered in the datasets . the new measure can be applied in many areas , ranging from bio-informatics to trade , supermarkets , and even law en- forcement . e.g. , in bio-informatics it is important to find patterns con- tained in many individuals , where patterns close together in one chro- mosome are more significant ."}
{"title": "using recurrent neural network for learning expressive ontologies", "abstract": "recently , neural networks have been proven extremely effective in many natural language processing tasks such as sentiment analysis , question answering , or machine translation . aiming to exploit such advantages in the ontology learning process , in this technical report we present a detailed description of a recurrent neural network based system to be used to pursue such goal ."}
{"title": "integrating logical and probabilistic reasoning for decision making", "abstract": "we describe a representation and a set of inference methods that combine logic programming techniques with probabilistic network representations for uncertainty ( influence diagrams ) . the techniques emphasize the dynamic construction and solution of probabilistic and decision-theoretic models for complex and uncertain domains . given a query , a logical proof is produced if possible ; if not , an influence diagram based on the query and the knowledge of the decision domain is produced and subsequently solved . a uniform declarative , first-order , knowledge representation is combined with a set of integrated inference procedures for logical , probabilistic , and decision-theoretic reasoning ."}
{"title": "optimization in differentiable manifolds in order to determine the method of construction of prehistoric wall-paintings", "abstract": "in this paper a general methodology is introduced for the determination of potential prototype curves used for the drawing of prehistoric wall-paintings . the approach includes a ) preprocessing of the wall-paintings contours to properly partition them , according to their curvature , b ) choice of prototype curves families , c ) analysis and optimization in 4-manifold for a first estimation of the form of these prototypes , d ) clustering of the contour parts and the prototypes , to determine a minimal number of potential guides , e ) further optimization in 4-manifold , applied to each cluster separately , in order to determine the exact functional form of the potential guides , together with the corresponding drawn contour parts . the introduced methodology simultaneously deals with two problems : a ) the arbitrariness in data-points orientation and b ) the determination of one proper form for a prototype curve that optimally fits the corresponding contour data . arbitrariness in orientation has been dealt with a novel curvature based error , while the proper forms of curve prototypes have been exhaustively determined by embedding curvature deformations of the prototypes into 4-manifolds . application of this methodology to celebrated wall-paintings excavated at tyrins , greece and the greek island of thera , manifests it is highly probable that these wall-paintings had been drawn by means of geometric guides that correspond to linear spirals and hyperbolae . these geometric forms fit the drawings ' lines with an exceptionally low average error , less than 0.39mm . hence , the approach suggests the existence of accurate realizations of complicated geometric entities , more than 1000 years before their axiomatic formulation in classical ages ."}
{"title": "exploring parallelism in learning belief networks", "abstract": "it has been shown that a class of probabilistic domain models can not be learned correctly by several existing algorithms which employ a single-link look ahead search . when a multi-link look ahead search is used , the computational complexity of the learning algorithm increases . we study how to use parallelism to tackle the increased complexity in learning such models and to speed up learning in large domains . an algorithm is proposed to decompose the learning task for parallel processing . a further task decomposition is used to balance load among processors and to increase the speed-up and efficiency . for learning from very large datasets , we present a regrouping of the available processors such that slow data access through file can be replaced by fast memory access . our implementation in a parallel computer demonstrates the effectiveness of the algorithm ."}
{"title": "nonuniform dynamic discretization in hybrid networks", "abstract": "we consider probabilistic inference in general hybrid networks , which include continuous and discrete variables in an arbitrary topology . we reexamine the question of variable discretization in a hybrid network aiming at minimizing the information loss induced by the discretization . we show that a nonuniform partition across all variables as opposed to uniform partition of each variable separately reduces the size of the data structures needed to represent a continuous function . we also provide a simple but efficient procedure for nonuniform partition . to represent a nonuniform discretization in the computer memory , we introduce a new data structure , which we call a binary split partition ( bsp ) tree . we show that bsp trees can be an exponential factor smaller than the data structures in the standard uniform discretization in multiple dimensions and show how the bsp trees can be used in the standard join tree algorithm . we show that the accuracy of the inference process can be significantly improved by adjusting discretization with evidence . we construct an iterative anytime algorithm that gradually improves the quality of the discretization and the accuracy of the answer on a query . we provide empirical evidence that the algorithm converges ."}
{"title": "a computer program for simulating time travel and a possible 'solution ' for the grandfather paradox", "abstract": "while the possibility of time travel in physics is still debated , the explosive growth of virtual-reality simulations opens up new possibilities to rigorously explore such time travel and its consequences in the digital domain . here we provide a computational model of time travel and a computer program that allows exploring digital time travel . in order to explain our method we formalize a simplified version of the famous grandfather paradox , show how the system can allow the participant to go back in time , try to kill their ancestors before they were born , and experience the consequences . the system has even come up with scenarios that can be considered consistent `` solutions '' of the grandfather paradox . we discuss the conditions for digital time travel , which indicate that it has a large number of practical applications ."}
{"title": "a robust linguistic platform for efficient and domain specific web content analysis", "abstract": "web semantic access in specific domains calls for specialized search engines with enhanced semantic querying and indexing capacities , which pertain both to information retrieval ( ir ) and to information extraction ( ie ) . a rich linguistic analysis is required either to identify the relevant semantic units to index and weight them according to linguistic specific statistical distribution , or as the basis of an information extraction process . recent developments make natural language processing ( nlp ) techniques reliable enough to process large collections of documents and to enrich them with semantic annotations . this paper focuses on the design and the development of a text processing platform , ogmios , which has been developed in the alvis project . the ogmios platform exploits existing nlp modules and resources , which may be tuned to specific domains and produces linguistically annotated documents . we show how the three constraints of genericity , domain semantic awareness and performance can be handled all together ."}
{"title": "conscious intelligent systems - part ii - mind , thought , language and understanding", "abstract": "this is the second part of a paper on conscious intelligent systems . we use the understanding gained in the first part ( conscious intelligent systems part 1 : ixi ( arxiv id cs.ai/0612056 ) ) to look at understanding . we see how the presence of mind affects understanding and intelligent systems ; we see that the presence of mind necessitates language . the rise of language in turn has important effects on understanding . we discuss the humanoid question and how the question of self-consciousness ( and by association mind/thought/language ) would affect humanoids too ."}
{"title": "morphological error detection in 3d segmentations", "abstract": "deep learning algorithms for connectomics rely upon localized classification , rather than overall morphology . this leads to a high incidence of erroneously merged objects . humans , by contrast , can easily detect such errors by acquiring intuition for the correct morphology of objects . biological neurons have complicated and variable shapes , which are challenging to learn , and merge errors take a multitude of different forms . we present an algorithm , mergenet , that shows 3d convnets can , in fact , detect merge errors from high-level neuronal morphology . mergenet follows unsupervised training and operates across datasets . we demonstrate the performance of mergenet both on a variety of connectomics data and on a dataset created from merged mnist images ."}
{"title": "3d-prnn : generating shape primitives with recurrent neural networks", "abstract": "the success of various applications including robotics , digital content creation , and visualization demand a structured and abstract representation of the 3d world from limited sensor data . inspired by the nature of human perception of 3d shapes as a collection of simple parts , we explore such an abstract shape representation based on primitives . given a single depth image of an object , we present 3d-prnn , a generative recurrent neural network that synthesizes multiple plausible shapes composed of a set of primitives . our generative model encodes symmetry characteristics of common man-made objects , preserves long-range structural coherence , and describes objects of varying complexity with a compact representation . we also propose a method based on gaussian fields to generate a large scale dataset of primitive-based shape representations to train our network . we evaluate our approach on a wide range of examples and show that it outperforms nearest-neighbor based shape retrieval methods and is on-par with voxel-based generative models while using a significantly reduced parameter space ."}
{"title": "latent belief theory and belief dependencies : a solution to the recovery problem in the belief set theories", "abstract": "the agm recovery postulate says : assume a set of propositions x ; assume that it is consistent and that it is closed under logical consequences ; remove a belief p from the set minimally , but make sure that the resultant set is again some set of propositions x ' which is closed under the logical consequences ; now add p again and close the set under the logical consequences ; and we should get a set of propositions that contains all the propositions that were in x. this postulate has since met objections ; many have observed that it could bear counter-intuitive results . nevertheless , the attempts that have been made so far to amend it either recovered the postulate in full , had to relinquish the assumption of the logical closure altogether , or else had to introduce fresh controversies of their own . we provide a solution to the recovery paradox in this work . our theoretical basis is the recently proposed belief theory with latent beliefs ( simply the latent belief theory for short ) . firstly , through examples , we will illustrate that the vanilla latent belief theory can be made more expressive . we will identify that a latent belief , when it becomes visible , may remain visible only while the beliefs that triggered it into the agent 's consciousness are in the agent 's belief set . in order that such situations can be also handled , we will enrich the latent belief theory with belief dependencies among attributive beliefs , recording the information as to which belief is supported of its existence by which beliefs . we will show that the enriched latent belief theory does not possess the recovery property . the closure by logical consequences is maintained in the theory , however . hence it serves as a solution to the open problem in the belief set theories ."}
{"title": "towards data-driven autonomics in data centers", "abstract": "continued reliance on human operators for managing data centers is a major impediment for them from ever reaching extreme dimensions . large computer systems in general , and data centers in particular , will ultimately be managed using predictive computational and executable models obtained through data-science tools , and at that point , the intervention of humans will be limited to setting high-level goals and policies rather than performing low-level operations . data-driven autonomics , where management and control are based on holistic predictive models that are built and updated using generated data , opens one possible path towards limiting the role of operators in data centers . in this paper , we present a data-science study of a public google dataset collected in a 12k-node cluster with the goal of building and evaluating a predictive model for node failures . we use bigquery , the big data sql platform from the google cloud suite , to process massive amounts of data and generate a rich feature set characterizing machine state over time . we describe how an ensemble classifier can be built out of many random forest classifiers each trained on these features , to predict if machines will fail in a future 24-hour window . our evaluation reveals that if we limit false positive rates to 5 % , we can achieve true positive rates between 27 % and 88 % with precision varying between 50 % and 72 % . we discuss the practicality of including our predictive model as the central component of a data-driven autonomic manager and operating it on-line with live data streams ( rather than off-line on data logs ) . all of the scripts used for bigquery and classification analyses are publicly available from the authors ' website ."}
{"title": "cost based satisficing search considered harmful", "abstract": "recently , several researchers have found that cost-based satisficing search with a* often runs into problems . although some `` work arounds '' have been proposed to ameliorate the problem , there has not been any concerted effort to pinpoint its origin . in this paper , we argue that the origins can be traced back to the wide variance in action costs that is observed in most planning domains . we show that such cost variance misleads a* search , and that this is no trifling detail or accidental phenomenon , but a systemic weakness of the very concept of `` cost-based evaluation functions + systematic search + combinatorial graphs '' . we show that satisficing search with sized-based evaluation functions is largely immune to this problem ."}
{"title": "an empirical evaluation of possible variations of lazy propagation", "abstract": "as real-world bayesian networks continue to grow larger and more complex , it is important to investigate the possibilities for improving the performance of existing algorithms of probabilistic inference . motivated by examples , we investigate the dependency of the performance of lazy propagation on the message computation algorithm . we show how symbolic probabilistic inference ( spi ) and arc-reversal ( ar ) can be used for computation of clique to clique messages in the addition to the traditional use of variable elimination ( ve ) . in addition , the paper resents the results of an empirical evaluation of the performance of lazy propagation using ve , spi , and ar as the message computation algorithm . the results of the empirical evaluation show that for most networks , the performance of inference did not depend on the choice of message computation algorithm , but for some randomly generated networks the choice had an impact on both space and time performance . in the cases where the choice had an impact , ar produced the best results ."}
{"title": "neural networks in mobile robot motion", "abstract": "this paper deals with a path planning and intelligent control of an autonomous robot which should move safely in partially structured environment . this environment may involve any number of obstacles of arbitrary shape and size ; some of them are allowed to move . we describe our approach to solving the motion-planning problem in mobile robot control using neural networks-based technique . our method of the construction of a collision-free path for moving robot among obstacles is based on two neural networks . the first neural network is used to determine the `` free '' space using ultrasound range finder data . the second neural network `` finds '' a safe direction for the next robot section of the path in the workspace while avoiding the nearest obstacles . simulation examples of generated path with proposed techniques will be presented ."}
{"title": "formal approaches to a definition of agents", "abstract": "this thesis contributes to the formalisation of the notion of an agent within the class of finite multivariate markov chains . agents are seen as entities that act , perceive , and are goal-directed . we present a new measure that can be used to identify entities ( called $ \\iota $ -entities ) , some general requirements for entities in multivariate markov chains , as well as formal definitions of actions and perceptions suitable for such entities . the intuition behind $ \\iota $ -entities is that entities are spatiotemporal patterns for which every part makes every other part more probable . the measure , complete local integration ( cli ) , is formally investigated in general bayesian networks . it is based on the specific local integration ( sli ) which is measured with respect to a partition . cli is the minimum value of sli over all partitions . we prove that $ \\iota $ -entities are blocks in specific partitions of the global trajectory . these partitions are the finest partitions that achieve a given sli value . we also establish the transformation behaviour of sli under permutations of nodes in the network . we go on to present three conditions on general definitions of entities . these are not fulfilled by sets of random variables i.e.\\ the perception-action loop , which is often used to model agents , is too restrictive . we propose that any general entity definition should in effect specify a subset ( called an an entity-set ) of the set of all spatiotemporal patterns of a given multivariate markov chain . the set of $ \\iota $ -entities is such a set . importantly the perception-action loop also induces an entity-set . we then propose formal definitions of actions and perceptions for arbitrary entity-sets . these specialise to standard notions in case of the perception-action loop entity-set . finally we look at some very simple examples ."}
{"title": "entropy and belief networks", "abstract": "the product expansion of conditional probabilities for belief nets is not maximum entropy . this appears to deny a desirable kind of assurance for the model . however , a kind of guarantee that is almost as strong as maximum entropy can be derived . surprisingly , a variant model also exhibits the guarantee , and for many cases obtains a higher performance score than the product expansion ."}
{"title": "adversarial examples that fool detectors", "abstract": "an adversarial example is an example that has been adjusted to produce a wrong label when presented to a system at test time . to date , adversarial example constructions have been demonstrated for classifiers , but not for detectors . if adversarial examples that could fool a detector exist , they could be used to ( for example ) maliciously create security hazards on roads populated with smart vehicles . in this paper , we demonstrate a construction that successfully fools two standard detectors , faster rcnn and yolo . the existence of such examples is surprising , as attacking a classifier is very different from attacking a detector , and that the structure of detectors - which must search for their own bounding box , and which can not estimate that box very accurately - makes it quite likely that adversarial patterns are strongly disrupted . we show that our construction produces adversarial examples that generalize well across sequences digitally , even though large perturbations are needed . we also show that our construction yields physical objects that are adversarial ."}
{"title": "chemnet : a transferable and generalizable deep neural network for small-molecule property prediction", "abstract": "with access to large datasets , deep neural networks ( dnn ) have achieved human-level accuracy in image and speech recognition tasks . however , in chemistry , availability of large standardized and labelled datasets is scarce , and many chemical properties of research interest , chemical data is inherently small and fragmented . in this work , we explore transfer learning techniques in conjunction with the existing chemception cnn model , to create a transferable and generalizable deep neural network for small-molecule property prediction . our latest model , chemnet learns in a semi-supervised manner from inexpensive labels computed from the chembl database . when fine-tuned to the tox21 , hiv and freesolv dataset , which are 3 separate chemical properties that chemnet was not originally trained on , we demonstrate that chemnet exceeds the performance of existing chemception models and other contemporary dnn models . furthermore , as chemnet has been pre-trained on a large diverse chemical database , it can be used as a general-purpose plug-and-play deep neural network for the prediction of novel small-molecule chemical properties ."}
{"title": "when waiting is not an option : learning options with a deliberation cost", "abstract": "recent work has shown that temporally extended actions ( options ) can be learned fully end-to-end as opposed to being specified in advance . while the problem of `` how '' to learn options is increasingly well understood , the question of `` what '' good options should be has remained elusive . we formulate our answer to what `` good '' options should be in the bounded rationality framework ( simon , 1957 ) through the notion of deliberation cost . we then derive practical gradient-based learning algorithms to implement this objective . our results in the arcade learning environment ( ale ) show increased performance and interpretability ."}
{"title": "score and information for recursive exponential models with incomplete data", "abstract": "recursive graphical models usually underlie the statistical modelling concerning probabilistic expert systems based on bayesian networks . this paper defines a version of these models , denoted as recursive exponential models , which have evolved by the desire to impose sophisticated domain knowledge onto local fragments of a model . besides the structural knowledge , as specified by a given model , the statistical modelling may also include expert opinion about the values of parameters in the model . it is shown how to translate imprecise expert knowledge into approximately conjugate prior distributions . based on possibly incomplete data , the score and the observed information are derived for these models . this accounts for both the traditional score and observed information , derived as derivatives of the log-likelihood , and the posterior score and observed information , derived as derivatives of the log-posterior distribution . throughout the paper the specialization into recursive graphical models is accounted for by a simple example ."}
{"title": "a neural network approach for mixing language models", "abstract": "the performance of neural network ( nn ) -based language models is steadily improving due to the emergence of new architectures , which are able to learn different natural language characteristics . this paper presents a novel framework , which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture . this is done through 1 ) a feature layer , which separately learns different nn-based models and 2 ) a mixture layer , which merges the resulting model features . in doing so , this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time . extensive experiments conducted on the penn treebank ( ptb ) and the large text compression benchmark ( ltcb ) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures ."}
{"title": "technical note : exploring \u03c3^p_2 / \u03c0^p_2-hardness for argumentation problems with fixed distance to tractable classes", "abstract": "we study the complexity of reasoning in abstracts argumentation frameworks close to graph classes that allow for efficient reasoning methods , i.e.\\ to one of the classes of acyclic , noeven , biparite and symmetric afs . in this work we show that certain reasoning problems on the second level of the polynomial hierarchy still maintain their full complexity when restricted to instances of fixed distance to one of the above graph classes ."}
{"title": "fdconfig : a constraint-based interactive product configurator", "abstract": "we present a constraint-based approach to interactive product configuration . our configurator tool fdconfig is based on feature models for the representation of the product domain . such models can be directly mapped into constraint satisfaction problems and dealt with by appropriate constraint solvers . during the interactive configuration process the user generates new constraints as a result of his configuration decisions and even may retract constraints posted earlier . we discuss the configuration process , explain the underlying techniques and show optimizations ."}
{"title": "design of fuzzy logic traffic controller for isolated intersections with emergency vehicle priority system using matlab simulation", "abstract": "traffic is the chief puzzle problem which every country faces because of the enhancement in number of vehicles throughout the world , especially in large urban towns . hence the need arises for simulating and optimizing traffic control algorithms to better accommodate this increasing demand . fuzzy optimization deals with finding the values of input parameters of a complex simulated system which result in desired output . this paper presents a matlab simulation of fuzzy logic traffic controller for controlling flow of traffic in isolated intersections . this controller is based on the waiting time and queue length of vehicles at present green phase and vehicles queue lengths at the other phases . the controller controls the traffic light timings and phase difference to ascertain sebaceous flow of traffic with least waiting time and queue length . in this paper , the isolated intersection model used consists of two alleyways in each approach . every outlook has different value of queue length and waiting time , systematically , at the intersection . the maximum value of waiting time and vehicle queue length has to be selected by using proximity sensors as inputs to controller for the ameliorate control traffic flow at the intersection . an intelligent traffic model and fuzzy logic traffic controller are developed to evaluate the performance of traffic controller under different pre-defined conditions for oleaginous flow of traffic . additionally , this fuzzy logic traffic controller has emergency vehicle siren sensors which detect emergency vehicle movement like ambulance , fire brigade , police van etc . and gives maximum priority to him and pass preferred signal to it ."}
{"title": "plan explanations as model reconciliation : moving beyond explanation as soliloquy", "abstract": "when ai systems interact with humans in the loop , they are often called on to provide explanations for their plans and behavior . past work on plan explanations primarily involved the ai system explaining the correctness of its plan and the rationale for its decision in terms of its own model . such soliloquy is wholly inadequate in most realistic scenarios where the humans have domain and task models that differ significantly from that used by the ai system . we posit that the explanations are best studied in light of these differing models . in particular , we show how explanation can be seen as a `` model reconciliation problem '' ( mrp ) , where the ai system in effect suggests changes to the human 's model , so as to make its plan be optimal with respect to that changed human model . we will study the properties of such explanations , present algorithms for automatically computing them , and evaluate the performance of the algorithms ."}
{"title": "preliminary results on ontology-based open data publishing", "abstract": "despite the current interest in open data publishing , a formal and comprehensive methodology supporting an organization in deciding which data to publish and carrying out precise procedures for publishing high-quality data , is still missing . in this paper we argue that the ontology-based data management paradigm can provide a formal basis for a principled approach to publish high quality , semantically annotated open data . we describe two main approaches to using an ontology for this endeavor , and then we present some technical results on one of the approaches , called bottom-up , where the specification of the data to be published is given in terms of the sources , and specific techniques allow deriving suitable annotations for interpreting the published data under the light of the ontology ."}
{"title": "finding a posterior domain probability distribution by specifying nonspecific evidence", "abstract": "this article is an extension of the results of two earlier articles . in [ j. schubert , on nonspecific evidence , int . j. intell . syst . 8 ( 1993 ) 711-725 ] we established within dempster-shafer theory a criterion function called the metaconflict function . with this criterion we can partition into subsets a set of several pieces of evidence with propositions that are weakly specified in the sense that it may be uncertain to which event a proposition is referring . in a second article [ j. schubert , specifying nonspecific evidence , in cluster-based specification techniques in dempster-shafer theory for an evidential intelligence analysis of multiple target tracks , ph.d. thesis , trita-na-9410 , royal institute of technology , stockholm , 1994 , isbn 91-7170-801-4 ] we not only found the most plausible subset for each piece of evidence , we also found the plausibility for every subset that this piece of evidence belongs to the subset . in this article we aim to find a posterior probability distribution regarding the number of subsets . we use the idea that each piece of evidence in a subset supports the existence of that subset to the degree that this piece of evidence supports anything at all . from this we can derive a bpa that is concerned with the question of how many subsets we have . that bpa can then be combined with a given prior domain probability distribution in order to obtain the sought-after posterior domain distribution ."}
{"title": "online structured prediction via coactive learning", "abstract": "we propose coactive learning as a model of interaction between a learning system and a human user , where both have the common goal of providing results of maximum utility to the user . at each step , the system ( e.g . search engine ) receives a context ( e.g . query ) and predicts an object ( e.g . ranking ) . the user responds by correcting the system if necessary , providing a slightly improved -- but not necessarily optimal -- object as feedback . we argue that such feedback can often be inferred from observable user behavior , for example , from clicks in web-search . evaluating predictions by their cardinal utility to the user , we propose efficient learning algorithms that have $ { \\cal o } ( \\frac { 1 } { \\sqrt { t } } ) $ average regret , even though the learning algorithm never observes cardinal utility values as in conventional online learning . we demonstrate the applicability of our model and learning algorithms on a movie recommendation task , as well as ranking for web-search ."}
{"title": "paraconsistency and word puzzles", "abstract": "word puzzles and the problem of their representations in logic languages have received considerable attention in the last decade ( ponnuru et al . 2004 ; shapiro 2011 ; baral and dzifcak 2012 ; schwitter 2013 ) . of special interest is the problem of generating such representations directly from natural language ( nl ) or controlled natural language ( cnl ) . an interesting variation of this problem , and to the best of our knowledge , scarcely explored variation in this context , is when the input information is inconsistent . in such situations , the existing encodings of word puzzles produce inconsistent representations and break down . in this paper , we bring the well-known type of paraconsistent logics , called annotated predicate calculus ( apc ) ( kifer and lozinskii 1992 ) , to bear on the problem . we introduce a new kind of non-monotonic semantics for apc , called consistency preferred stable models and argue that it makes apc into a suitable platform for dealing with inconsistency in word puzzles and , more generally , in nl sentences . we also devise a number of general principles to help the user choose among the different representations of nl sentences , which might seem equivalent but , in fact , behave differently when inconsistent information is taken into account . these principles can be incorporated into existing cnl translators , such as attempto controlled english ( ace ) ( fuchs et al . 2008 ) and peng light ( white and schwitter 2009 ) . finally , we show that apc with the consistency preferred stable model semantics can be equivalently embedded in asp with preferences over stable models , and we use this embedding to implement this version of apc in clingo ( gebser et al . 2011 ) and its asprin add-on ( brewka et al . 2015 ) ."}
{"title": "inferring 3d articulated models for box packaging robot", "abstract": "given a point cloud , we consider inferring kinematic models of 3d articulated objects such as boxes for the purpose of manipulating them . while previous work has shown how to extract a planar kinematic model ( often represented as a linear chain ) , such planar models do not apply to 3d objects that are composed of segments often linked to the other segments in cyclic configurations . we present an approach for building a model that captures the relation between the input point cloud features and the object segment as well as the relation between the neighboring object segments . we use a conditional random field that allows us to model the dependencies between different segments of the object . we test our approach on inferring the kinematic structure from partial and noisy point cloud data for a wide variety of boxes including cake boxes , pizza boxes , and cardboard cartons of several sizes . the inferred structure enables our robot to successfully close these boxes by manipulating the flaps ."}
{"title": "approximation and heuristic algorithms for probabilistic physical search on general graphs", "abstract": "we consider an agent seeking to obtain an item , potentially available at different locations in a physical environment . the traveling costs between locations are known in advance , but there is only probabilistic knowledge regarding the possible prices of the item at any given location . given such a setting , the problem is to find a plan that maximizes the probability of acquiring the good while minimizing both travel and purchase costs . sample applications include agents in search-and-rescue or exploration missions , e.g. , a rover on mars seeking to mine a specific mineral . these probabilistic physical search problems have been previously studied , but we present the first approximation and heuristic algorithms for solving such problems on general graphs . we establish an interesting connection between these problems and classical graph-search problems , which led us to provide the approximation algorithms and hardness of approximation results for our settings . we further suggest several heuristics for practical use , and demonstrate their effectiveness with simulation on real graph structure and synthetic graphs ."}
{"title": "testing hypotheses by regularized maximum mean discrepancy", "abstract": "do two data samples come from different distributions ? recent studies of this fundamental problem focused on embedding probability distributions into sufficiently rich characteristic reproducing kernel hilbert spaces ( rkhss ) , to compare distributions by the distance between their embeddings . we show that regularized maximum mean discrepancy ( rmmd ) , our novel measure for kernel-based hypothesis testing , yields substantial improvements even when sample sizes are small , and excels at hypothesis tests involving multiple comparisons with power control . we derive asymptotic distributions under the null and alternative hypotheses , and assess power control . outstanding results are obtained on : challenging eeg data , mnist , the berkley covertype , and the flare-solar dataset ."}
{"title": "bank card usage prediction exploiting geolocation information", "abstract": "we describe the solution of team ismll for the ecml-pkdd 2016 discovery challenge on bank card usage for both tasks . our solution is based on three pillars . gradient boosted decision trees as a strong regression and classification model , an intensive search for good hyperparameter configurations and strong features that exploit geolocation information . this approach achieved the best performance on the public leaderboard for the first task and a decent fourth position for the second task ."}
{"title": "deep learning for ontology reasoning", "abstract": "in this work , we present a novel approach to ontology reasoning that is based on deep learning rather than logic-based formal reasoning . to this end , we introduce a new model for statistical relational learning that is built upon deep recursive neural networks , and give experimental evidence that it can easily compete with , or even outperform , existing logic-based reasoners on the task of ontology reasoning . more precisely , we compared our implemented system with one of the best logic-based ontology reasoners at present , rdfox , on a number of large standard benchmark datasets , and found that our system attained high reasoning quality , while being up to two orders of magnitude faster ."}
{"title": "finding remo ( related memory object ) : a simple neural architecture for text based reasoning", "abstract": "to solve the text-based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory . most approaches were based on external memory and four components proposed by memory network . the distinctive component among them was the way of finding the necessary information and it contributes to the performance . recently , a simple but powerful neural network module for reasoning called relation network ( rn ) has been introduced . we analyzed rn from the view of memory network , and realized that its mlp component is able to reveal the complicate relation between question and object pair . motivated from it , we introduce which uses mlp to find out relevant information on memory network architecture . it shows new state-of-the-art results in jointly trained babi-10k story-based question answering tasks and babi dialog-based question answering tasks ."}
{"title": "personalizing a dialogue system with transfer reinforcement learning", "abstract": "it is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient . personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs . one way to solve this problem is to consider a collection of multiple users ' data as a source domain and an individual user 's data as a target domain , and to perform a transfer learning from the source to the target domain . by following this idea , we propose `` petal '' ( personalized task-oriented dialogue ) , a transfer-learning framework based on pomdp to learn a personalized dialogue system . the system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user . this framework can avoid the negative transfer problem by considering differences between source and target users . the policy in the personalized pomdp can learn to choose different actions appropriately for different users . experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users , and thus effectively improve the dialogue quality under the personalized setting ."}
{"title": "conservative , proportional and optimistic contextual discounting in the belief functions theory", "abstract": "information discounting plays an important role in the theory of belief functions and , generally , in information fusion . nevertheless , neither classical uniform discounting nor contextual can not model certain use cases , notably temporal discounting . in this article , new contextual discounting schemes , conservative , proportional and optimistic , are proposed . some properties of these discounting operations are examined . classical discounting is shown to be a special case of these schemes . two motivating cases are discussed : modelling of source reliability and application to temporal discounting ."}
{"title": "a streaming accelerator for deep convolutional neural networks with image and feature decomposition for resource-limited system applications", "abstract": "deep convolutional neural networks ( cnn ) are widely used in modern artificial intelligence ( ai ) and smart vision systems but also limited by computation latency , throughput , and energy efficiency on a resource-limited scenario , such as mobile devices , internet of things ( iot ) , unmanned aerial vehicles ( uav ) , and so on . a hardware streaming architecture is proposed to accelerate convolution and pooling computations for state-of-the-art deep cnns . it is optimized for energy efficiency by maximizing local data reuse to reduce off-chip dram data access . in addition , image and feature decomposition techniques are introduced to optimize memory access pattern for an arbitrary size of image and number of features within limited on-chip sram capacity . a prototype accelerator was implemented in tsmc 65 nm cmos technology with 2.3 mm x 0.8 mm core area , which achieves 144 gops peak throughput and 0.8 tops/w peak energy efficiency ."}
{"title": "bayesian optimisation for safe navigation under localisation uncertainty", "abstract": "in outdoor environments , mobile robots are required to navigate through terrain with varying characteristics , some of which might significantly affect the integrity of the platform . ideally , the robot should be able to identify areas that are safe for navigation based on its own percepts about the environment while avoiding damage to itself . bayesian optimisation ( bo ) has been successfully applied to the task of learning a model of terrain traversability while guiding the robot through more traversable areas . an issue , however , is that localisation uncertainty can end up guiding the robot to unsafe areas and distort the model being learnt . in this paper , we address this problem and present a novel method that allows bo to consider localisation uncertainty by applying a gaussian process model for uncertain inputs as a prior . we evaluate the proposed method in simulation and in experiments with a real robot navigating over rough terrain and compare it against standard bo methods ."}
{"title": "towards ontology driven learning of visual concept detectors", "abstract": "the maturity of deep learning techniques has led in recent years to a breakthrough in object recognition in visual media . while for some specific benchmarks , neural techniques seem to match if not outperform human judgement , challenges are still open for detecting arbitrary concepts in arbitrary videos . in this paper , we propose a system that combines neural techniques , a large scale visual concepts ontology , and an active learning loop , to provide on the fly model learning of arbitrary concepts . we give an overview of the system as a whole , and focus on the central role of the ontology for guiding and bootstrapping the learning of new concepts , improving the recall of concept detection , and , on the user end , providing semantic search on a library of annotated videos ."}
{"title": "a characterization of the dirichlet distribution with application to learning bayesian networks", "abstract": "we provide a new characterization of the dirichlet distribution . this characterization implies that under assumptions made by several previous authors for learning belief networks , a dirichlet prior on the parameters is inevitable ."}
{"title": "nearly optimal exploration-exploitation decision thresholds", "abstract": "while in general trading off exploration and exploitation in reinforcement learning is hard , under some formulations relatively simple solutions exist . optimal decision thresholds for the multi-armed bandit problem , one for the infinite horizon discounted reward case and one for the finite horizon undiscounted reward case are derived , which make the link between the reward horizon , uncertainty and the need for exploration explicit . from this result follow two practical approximate algorithms , which are illustrated experimentally ."}
{"title": "one-shot reinforcement learning for robot navigation with interactive replay", "abstract": "recently , model-free reinforcement learning algorithms have been shown to solve challenging problems by learning from extensive interaction with the environment . a significant issue with transferring this success to the robotics domain is that interaction with the real world is costly , but training on limited experience is prone to overfitting . we present a method for learning to navigate , to a fixed goal and in a known environment , on a mobile robot . the robot leverages an interactive world model built from a single traversal of the environment , a pre-trained visual feature encoder , and stochastic environmental augmentation , to demonstrate successful zero-shot transfer under real-world environmental variations without fine-tuning ."}
{"title": "word sense disambiguation using english-spanish aligned phrases over comparable corpora", "abstract": "in this paper we describe a wsd experiment based on bilingual english-spanish comparable corpora in which individual noun phrases have been identified and aligned with their respective counterparts in the other language . the evaluation of the experiment has been carried out against semcor . we show that , with the alignment algorithm employed , potential precision is high ( 74.3 % ) , however the coverage of the method is low ( 2.7 % ) , due to alignments being far less frequent than we expected . contrary to our intuition , precision does not rise consistently with the number of alignments . the coverage is low due to several factors ; there are important domain differences , and english and spanish are too close languages for this approach to be able to discriminate efficiently between senses , rendering it unsuitable for wsd , although the method may prove more productive in machine translation ."}
{"title": "artificial intelligence and legal liability", "abstract": "a recent issue of a popular computing journal asked which laws would apply if a self-driving car killed a pedestrian . this paper considers the question of legal liability for artificially intelligent computer systems . it discusses whether criminal liability could ever apply ; to whom it might apply ; and , under civil law , whether an ai program is a product that is subject to product design legislation or a service to which the tort of negligence applies . the issue of sales warranties is also considered . a discussion of some of the practical limitations that ai systems are subject to is also included ."}
{"title": "epistemic analysis of strategic games with arbitrary strategy sets", "abstract": "we provide here an epistemic analysis of arbitrary strategic games based on the possibility correspondences . such an analysis calls for the use of transfinite iterations of the corresponding operators . our approach is based on tarski 's fixpoint theorem and applies both to the notions of rationalizability and the iterated elimination of strictly dominated strategies ."}
{"title": "block-diagonal hessian-free optimization for training neural networks", "abstract": "second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent , including better scaling to large mini-batch sizes and fewer updates needed for convergence . but they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations . we introduce a variant of the hessian-free method that leverages a block-diagonal approximation of the generalized gauss-newton matrix . our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block . experiments on deep autoencoders , deep convolutional networks , and multilayer lstms demonstrate better convergence and generalization compared to the original hessian-free approach and the adam method ."}
{"title": "prediction of platinum prices using dynamically weighted mixture of experts", "abstract": "neural networks are powerful tools for classification and regression in static environments . this paper describes a technique for creating an ensemble of neural networks that adapts dynamically to changing conditions . the model separates the input space into four regions and each network is given a weight in each region based on its performance on samples from that region . the ensemble adapts dynamically by constantly adjusting these weights based on the current performance of the networks . the data set used is a collection of financial indicators with the goal of predicting the future platinum price . an ensemble with no weightings does not improve on the naive estimate of no weekly change ; our weighting algorithm gives an average percentage error of 63 % for twenty weeks of prediction ."}
{"title": "uta-poly and uta-splines : additive value functions with polynomial marginals", "abstract": "additive utility function models are widely used in multiple criteria decision analysis . in such models , a numerical value is associated to each alternative involved in the decision problem . it is computed by aggregating the scores of the alternative on the different criteria of the decision problem . the score of an alternative is determined by a marginal value function that evolves monotonically as a function of the performance of the alternative on this criterion . determining the shape of the marginals is not easy for a decision maker . it is easier for him/her to make statements such as `` alternative $ a $ is preferred to $ b $ '' . in order to help the decision maker , uta disaggregation procedures use linear programming to approximate the marginals by piecewise linear functions based only on such statements . in this paper , we propose to infer polynomials and splines instead of piecewise linear functions for the marginals . in this aim , we use semidefinite programming instead of linear programming . we illustrate this new elicitation method and present some experimental results ."}
{"title": "an agent based approach towards metadata extraction , modelling and information retrieval over the web", "abstract": "web development is a challenging research area for its creativity and complexity . the existing raised key challenge in web technology technologic development is the presentation of data in machine read and process able format to take advantage in knowledge based information extraction and maintenance . currently it is not possible to search and extract optimized results using full text queries because there is no such mechanism exists which can fully extract the semantic from full text queries and then look for particular knowledge based information ."}
{"title": "an analysis of state-relevance weights and sampling distributions on l1-regularized approximate linear programming approximation accuracy", "abstract": "recent interest in the use of $ l_1 $ regularization in the use of value function approximation includes petrik et al . 's introduction of $ l_1 $ -regularized approximate linear programming ( ralp ) . ralp is unique among $ l_1 $ -regularized approaches in that it approximates the optimal value function using off-policy samples . additionally , it produces policies which outperform those of previous methods , such as lspi . ralp 's value function approximation quality is affected heavily by the choice of state-relevance weights in the objective function of the linear program , and by the distribution from which samples are drawn ; however , there has been no discussion of these considerations in the previous literature . in this paper , we discuss and explain the effects of choices in the state-relevance weights and sampling distribution on approximation quality , using both theoretical and experimental illustrations . the results provide insight not only onto these effects , but also provide intuition into the types of mdps which are especially well suited for approximation with ralp ."}
{"title": "optimal estimates for short horizon travel time prediction in urban areas", "abstract": "increasing popularity of mobile route planning applications based on gps technology provides opportunities for collecting traffic data in urban environments . one of the main challenges for travel time estimation and prediction in such a setting is how to aggregate data from vehicles that have followed different routes , and predict travel time for other routes of interest . one approach is to predict travel times for route segments , and sum those estimates to obtain a prediction for the whole route . we study how to obtain optimal predictions in this scenario . it appears that the optimal estimate , minimizing the expected mean absolute error , is a combination of the mean and the median travel times on each segment , where the combination function depends on the number of segments in the route of interest . we present a methodology for obtaining such predictions , and demonstrate its effectiveness with a case study using travel time data from a district of st. petersburg collected over one year . the proposed methodology can be applied for real-time prediction of expected travel times in an urban road network ."}
{"title": "clamping improves trw and mean field approximations", "abstract": "we examine the effect of clamping variables for approximate inference in undirected graphical models with pairwise relationships and discrete variables . for any number of variable labels , we demonstrate that clamping and summing approximate sub-partition functions can lead only to a decrease in the partition function estimate for trw , and an increase for the naive mean field method , in each case guaranteeing an improvement in the approximation and bound . we next focus on binary variables , add the bethe approximation to consideration and examine ways to choose good variables to clamp , introducing new methods . we show the importance of identifying highly frustrated cycles , and of checking the singleton entropy of a variable . we explore the value of our methods by empirical analysis and draw lessons to guide practitioners ."}
{"title": "um sistema multiagente no combate ao braqueamento de capitais", "abstract": "money laundering is a crime that makes it possible to finance other crimes , for this reason , it is important for criminal organizations and their combat is prioritized by nations around the world . the anti-money laundering process has not evolved as expected because it has prioritized only the signaling of suspicious transactions . the constant increasing in the volume of transactions has overloaded the indispensable human work of final evaluation of the suspicions . this article presents a multiagent system that aims to go beyond the capture of suspicious transactions , seeking to assist the human expert in the analysis of suspicions . the agents created use data mining techniques to create transactional behavioral profiles ; apply rules generated in learning process in conjunction with specific rules based on legal aspects and profiles created to capture suspicious transactions ; and analyze these suspicious transactions indicating to the human expert those that require more detailed analysis ."}
{"title": "retraction and generalized extension of computing with words", "abstract": "fuzzy automata , whose input alphabet is a set of numbers or symbols , are a formal model of computing with values . motivated by zadeh 's paradigm of computing with words rather than numbers , ying proposed a kind of fuzzy automata , whose input alphabet consists of all fuzzy subsets of a set of symbols , as a formal model of computing with all words . in this paper , we introduce a somewhat general formal model of computing with ( some special ) words . the new features of the model are that the input alphabet only comprises some ( not necessarily all ) fuzzy subsets of a set of symbols and the fuzzy transition function can be specified arbitrarily . by employing the methodology of fuzzy control , we establish a retraction principle from computing with words to computing with values for handling crisp inputs and a generalized extension principle from computing with words to computing with all words for handling fuzzy inputs . these principles show that computing with values and computing with all words can be respectively implemented by computing with words . some algebraic properties of retractions and generalized extensions are addressed as well ."}
{"title": "the generalized pignistic transformation", "abstract": "this paper presents in detail the generalized pignistic transformation ( gpt ) succinctly developed in the dezert-smarandache theory ( dsmt ) framework as a tool for decision process . the gpt allows to provide a subjective probability measure from any generalized basic belief assignment given by any corpus of evidence . we mainly focus our presentation on the 3d case and provide the complete result obtained by the gpt and its validation drawn from the probability theory ."}
{"title": "infinite-horizon policy-gradient estimation", "abstract": "gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods . in this paper we introduce gpomdp , a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in partially observable markov decision processes pomdps controlled by parameterized stochastic policies . a similar algorithm was proposed by ( kimura et al . 1995 ) . the algorithm 's chief advantages are that it requires storage of only twice the number of policy parameters , uses one free beta ( which has a natural interpretation in terms of bias-variance trade-off ) , and requires no knowledge of the underlying state . we prove convergence of gpomdp , and show how the correct choice of the parameter beta is related to the mixing time of the controlled pomdp . we briefly describe extensions of gpomdp to controlled markov chains , continuous state , observation and control spaces , multiple-agents , higher-order derivatives , and a version for training stochastic policies with internal states . in a companion paper ( baxter et al. , this volume ) we show how the gradient estimates generated by gpomdp can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward ."}
{"title": "planning , scheduling , and uncertainty in the sequence of future events", "abstract": "scheduling in the factory setting is compounded by computational complexity and temporal uncertainty . together , these two factors guarantee that the process of constructing an optimal schedule will be costly and the chances of executing that schedule will be slight . temporal uncertainty in the task execution time can be offset by several methods : eliminate uncertainty by careful engineering , restore certainty whenever it is lost , reduce the uncertainty by using more accurate sensors , and quantify and circumscribe the remaining uncertainty . unfortunately , these methods focus exclusively on the sources of uncertainty and fail to apply knowledge of the tasks which are to be scheduled . a complete solution must adapt the schedule of activities to be performed according to the evolving state of the production world . the example of vision-directed assembly is presented to illustrate that the principle of least commitment , in the creation of a plan , in the representation of a schedule , and in the execution of a schedule , enables a robot to operate intelligently and efficiently , even in the presence of considerable uncertainty in the sequence of future events ."}
{"title": "connectives in quantum and other cumulative logics", "abstract": "cumulative logics are studied in an abstract setting , i.e. , without connectives , very much in the spirit of makinson 's early work . a powerful representation theorem characterizes those logics by choice functions that satisfy a weakening of sen 's property alpha , in the spirit of the author 's `` nonmonotonic logics and semantics '' ( jlc ) . the representation results obtained are surprisingly smooth : in the completeness part the choice function may be defined on any set of worlds , not only definable sets and no definability-preservation property is required in the soundness part . for abstract cumulative logics , proper conjunction and negation may be defined . contrary to the situation studied in `` nonmonotonic logics and semantics '' no proper disjunction seems to be definable in general . the cumulative relations of klm that satisfy some weakening of the consistency preservation property all define cumulative logics with a proper negation . quantum logics , as defined by engesser and gabbay are such cumulative logics but the negation defined by orthogonal complement does not provide a proper negation ."}
{"title": "dynamic programming for structured continuous markov decision problems", "abstract": "we describe an approach for exploiting structure in markov decision processes with continuous state variables . at each step of the dynamic programming , the state space is dynamically partitioned into regions where the value function is the same throughout the region . we first describe the algorithm for piecewise constant representations . we then extend it to piecewise linear representations , using techniques from pomdps to represent and reason about linear surfaces efficiently . we show that for complex , structured problems , our approach exploits the natural structure so that optimal solutions can be computed efficiently ."}
{"title": "perseus : randomized point-based value iteration for pomdps", "abstract": "partially observable markov decision processes ( pomdps ) form an attractive and principled framework for agent planning under uncertainty . point-based approximate techniques for pomdps compute a policy based on a finite set of points collected in advance from the agents belief space . we present a randomized point-based value iteration algorithm called perseus . the algorithm performs approximate value backup stages , ensuring that in each backup stage the value of each point in the belief set is improved ; the key observation is that a single backup may improve the value of many belief points . contrary to other point-based methods , perseus backs up only a ( randomly selected ) subset of points in the belief set , sufficient for improving the value of each belief point in the set . we show how the same idea can be extended to dealing with continuous action spaces . experimental results show the potential of perseus in large scale pomdp problems ."}
{"title": "concept generation in language evolution", "abstract": "this thesis investigates the generation of new concepts from combinations of existing concepts as a language evolves . we give a method for combining concepts , and will be investigating the utility of composite concepts in language evolution and thence the utility of concept generation ."}
{"title": "spike and tyke , the quantized neuron model", "abstract": "modeling spike firing assumes that spiking statistics are poisson , but real data violates this assumption . to capture non-poissonian features , in order to fix the inevitable inherent irregularity , researchers rescale the time axis with tedious computational overhead instead of searching for another distribution . spikes or action potentials are precisely-timed changes in the ionic transport through synapses adjusting the synaptic weight , successfully modeled and developed as a memristor . memristance value is multiples of initial resistance . this reminds us with the foundations of quantum mechanics . we try to quantize potential and resistance , as done with energy . after reviewing planck curve for blackbody radiation , we propose the quantization equations . we introduce and prove a theorem that quantizes the resistance . then we define the tyke showing its basic characteristics . finally we give the basic transformations to model spiking and link an energy quantum to a tyke . investigation shows how this perfectly models the neuron spiking , with over 97 % match ."}
{"title": "truthful mechanisms for matching and clustering in an ordinal world", "abstract": "we study truthful mechanisms for matching and related problems in a partial information setting , where the agents ' true utilities are hidden , and the algorithm only has access to ordinal preference information . our model is motivated by the fact that in many settings , agents can not express the numerical values of their utility for different outcomes , but are still able to rank the outcomes in their order of preference . specifically , we study problems where the ground truth exists in the form of a weighted graph of agent utilities , but the algorithm can only elicit the agents ' private information in the form of a preference ordering for each agent induced by the underlying weights . against this backdrop , we design truthful algorithms to approximate the true optimum solution with respect to the hidden weights . our techniques yield universally truthful algorithms for a number of graph problems : a 1.76-approximation algorithm for max-weight matching , 2-approximation algorithm for max k-matching , a 6-approximation algorithm for densest k-subgraph , and a 2-approximation algorithm for max traveling salesman as long as the hidden weights constitute a metric . we also provide improved approximation algorithms for such problems when the agents are not able to lie about their preferences . our results are the first non-trivial truthful approximation algorithms for these problems , and indicate that in many situations , we can design robust algorithms even when the agents may lie and only provide ordinal information instead of precise utilities ."}
{"title": "continuous inverse optimal control with locally optimal examples", "abstract": "inverse optimal control , also known as inverse reinforcement learning , is the problem of recovering an unknown reward function in a markov decision process from expert demonstrations of the optimal policy . we introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality , and is suitable for large , continuous domains where even computing a full policy is impractical . by using a local approximation of the reward function , our method can also drop the assumption that the demonstrations are globally optimal , requiring only local optimality . this allows it to learn from examples that are unsuitable for prior methods ."}
{"title": "sensor synthesis for pomdps with reachability objectives", "abstract": "partially observable markov decision processes ( pomdps ) are widely used in probabilistic planning problems in which an agent interacts with an environment using noisy and imprecise sensors . we study a setting in which the sensors are only partially defined and the goal is to synthesize `` weakest '' additional sensors , such that in the resulting pomdp , there is a small-memory policy for the agent that almost-surely ( with probability~1 ) satisfies a reachability objective . we show that the problem is np-complete , and present a symbolic algorithm by encoding the problem into sat instances . we illustrate trade-offs between the amount of memory of the policy and the number of additional sensors on a simple example . we have implemented our approach and consider three classical pomdp examples from the literature , and show that in all the examples the number of sensors can be significantly decreased ( as compared to the existing solutions in the literature ) without increasing the complexity of the policies ."}
{"title": "towards moral autonomous systems", "abstract": "both the ethics of autonomous systems and the problems of their technical implementation have by now been studied in some detail . less attention has been given to the areas in which these two separate concerns meet . this paper , written by both philosophers and engineers of autonomous systems , addresses a number of issues in machine ethics that are located at precisely the intersection between ethics and engineering . we first discuss the main challenges which , in our view , machine ethics posses to moral philosophy . we them consider different approaches towards the conceptual design of autonomous systems and their implications on the ethics implementation in such systems . then we examine problematic areas regarding the specification and verification of ethical behavior in autonomous systems , particularly with a view towards the requirements of future legislation . we discuss transparency and accountability issues that will be crucial for any future wide deployment of autonomous systems in society . finally we consider the , often overlooked , possibility of intentional misuse of ai systems and the possible dangers arising out of deliberately unethical design , implementation , and use of autonomous robots ."}
{"title": "cost-optimal learning of causal graphs", "abstract": "we consider the problem of learning a causal graph over a set of variables with interventions . we study the cost-optimal causal graph learning problem : for a given skeleton ( undirected version of the causal graph ) , design the set of interventions with minimum total cost , that can uniquely identify any causal graph with the given skeleton . we show that this problem is solvable in polynomial time . later , we consider the case when the number of interventions is limited . for this case , we provide polynomial time algorithms when the skeleton is a tree or a clique tree . for a general chordal skeleton , we develop an efficient greedy algorithm , which can be improved when the causal graph skeleton is an interval graph ."}
{"title": "statistical modeling in continuous speech recognition ( csr ) ( invited talk )", "abstract": "automatic continuous speech recognition ( csr ) is sufficiently mature that a variety of real world applications are now possible including large vocabulary transcription and interactive spoken dialogues . this paper reviews the evolution of the statistical modelling techniques which underlie current-day systems , specifically hidden markov models ( hmms ) and n-grams . starting from a description of the speech signal and its parameterisation , the various modelling assumptions and their consequences are discussed . it then describes various techniques by which the effects of these assumptions can be mitigated . despite the progress that has been made , the limitations of current modelling techniques are still evident . the paper therefore concludes with a brief review of some of the more fundamental modelling work now in progress ."}
{"title": "unbounded human learning : optimal scheduling for spaced repetition", "abstract": "in the study of human learning , there is broad evidence that our ability to retain information improves with repeated exposure and decays with delay since last exposure . this plays a crucial role in the design of educational software , leading to a trade-off between teaching new material and reviewing what has already been taught . a common way to balance this trade-off is spaced repetition , which uses periodic review of content to improve long-term retention . though spaced repetition is widely used in practice , e.g. , in electronic flashcard software , there is little formal understanding of the design of these systems . our paper addresses this gap in three ways . first , we mine log data from spaced repetition software to establish the functional dependence of retention on reinforcement and delay . second , we use this memory model to develop a stochastic model for spaced repetition systems . we propose a queueing network model of the leitner system for reviewing flashcards , along with a heuristic approximation that admits a tractable optimization problem for review scheduling . finally , we empirically evaluate our queueing model through a mechanical turk experiment , verifying a key qualitative prediction of our model : the existence of a sharp phase transition in learning outcomes upon increasing the rate of new item introductions ."}
{"title": "networks utilization improvements for service discovery performance", "abstract": "service discovery requests ' messages have a vital role in sharing and locating resources in many of service discovery protocols . sending more messages than a link can handle may cause congestion and loss of messages which dramatically influences the performance of these protocols . re-send the lost messages result in latency and inefficiency in performing the tasks which user ( s ) require from the connected nodes . this issue become a serious problem in two cases : first , when the number of clients which performs a service discovery request is increasing , as this result in increasing in the number of sent discovery messages ; second , when the network resources such as bandwidth capacity are consumed by other applications . these two cases lead to network congestion and loss of messages . this paper propose an algorithm to improve the services discovery protocols performance by separating each consecutive burst of messages with a specific period of time which calculated regarding the available network resources . it was tested when the routers were connected in two configurations ; decentralised and centralised .in addition , this paper explains the impact of increasing the number of clients and the consumed network resources on the proposed algorithm ."}
{"title": "cognitive interpretation of everyday activities : toward perceptual narrative based visuo-spatial scene interpretation", "abstract": "we position a narrative-centred computational model for high-level knowledge representation and reasoning in the context of a range of assistive technologies concerned with `` visuo-spatial perception and cognition '' tasks . our proposed narrative model encompasses aspects such as \\emph { space , events , actions , change , and interaction } from the viewpoint of commonsense reasoning and learning in large-scale cognitive systems . the broad focus of this paper is on the domain of `` human-activity interpretation '' in smart environments , ambient intelligence etc . in the backdrop of a `` smart meeting cinematography '' domain , we position the proposed narrative model , preliminary work on perceptual narrativisation , and the immediate outlook on constructing general-purpose open-source tools for perceptual narrativisation . acm classification : i.2 artificial intelligence : i.2.0 general -- cognitive simulation , i.2.4 knowledge representation formalisms and methods , i.2.10 vision and scene understanding : architecture and control structures , motion , perceptual reasoning , shape , video analysis general keywords : cognitive systems ; human-computer interaction ; spatial cognition and computation ; commonsense reasoning ; spatial and temporal reasoning ; assistive technologies"}
{"title": "landmark-based plan recognition", "abstract": "recognition of goals and plans using incomplete evidence from action execution can be done efficiently by using planning techniques . in many applications it is important to recognize goals and plans not only accurately , but also quickly . in this paper , we develop a heuristic approach for recognizing plans based on planning techniques that rely on ordering constraints to filter candidate goals from observations . these ordering constraints are called landmarks in the planning literature , which are facts or actions that can not be avoided to achieve a goal . we show the applicability of planning landmarks in two settings : first , we use it directly to develop a heuristic-based plan recognition approach ; second , we refine an existing planning-based plan recognition approach by pre-filtering its candidate goals . our empirical evaluation shows that our approach is not only substantially more accurate than the state-of-the-art in all available datasets , it is also an order of magnitude faster ."}
{"title": "learning cost-effective treatment regimes using markov decision processes", "abstract": "decision makers , such as doctors and judges , make crucial decisions such as recommending treatments to patients , and granting bails to defendants on a daily basis . such decisions typically involve weighting the potential benefits of taking an action against the costs involved . in this work , we aim to automate this task of learning \\emph { cost-effective , interpretable and actionable treatment regimes } . we formulate this as a problem of learning a decision list -- a sequence of if-then-else rules -- which maps characteristics of subjects ( eg. , diagnostic test results of patients ) to treatments . we propose a novel objective to construct a decision list which maximizes outcomes for the population , and minimizes overall costs . we model the problem of learning such a list as a markov decision process ( mdp ) and employ a variant of the upper confidence bound for trees ( uct ) strategy which leverages customized checks for pruning the search space effectively . experimental results on real world observational data capturing judicial bail decisions and treatment recommendations for asthma patients demonstrate the effectiveness of our approach ."}
{"title": "redundant sudoku rules", "abstract": "the rules of sudoku are often specified using twenty seven \\texttt { all\\_different } constraints , referred to as the { \\em big } \\mrules . using graphical proofs and exploratory logic programming , the following main and new result is obtained : many subsets of six of these big \\mrules are redundant ( i.e. , they are entailed by the remaining twenty one \\mrules ) , and six is maximal ( i.e. , removing more than six \\mrules is not possible while maintaining equivalence ) . the corresponding result for binary inequality constraints , referred to as the { \\em small } \\mrules , is stated as a conjecture ."}
{"title": "quadripolar relational model : a framework for the description of borderline and narcissistic personality disorders", "abstract": "borderline personality disorder and narcissistic personality disorder are important nosographic entities and have been subject of intensive investigations . the currently prevailing psychodynamic theory for mental disorders is based on the repertoire of defense mechanisms employed . another line of research is concerned with the study of psychological traumas and dissociation as a defensive response . both theories can be used to shed light on some aspects of pathological mental functioning , and have many points of contact . this work merges these two psychological theories , and builds a model of mental function in a relational context called quadripolar relational model . the model , which is enriched with ideas borrowed from the field of computer science , leads to a new therapeutic proposal for psychological traumas and personality disorders ."}
{"title": "automated feedback generation for introductory programming assignments", "abstract": "we present a new method for automatically providing feedback for introductory programming problems . in order to use this method , we need a reference implementation of the assignment , and an error model consisting of potential corrections to errors that students might make . using this information , the system automatically derives minimal corrections to student 's incorrect solutions , providing them with a quantifiable measure of exactly how incorrect a given solution was , as well as feedback about what they did wrong . we introduce a simple language for describing error models in terms of correction rules , and formally define a rule-directed translation strategy that reduces the problem of finding minimal corrections in an incorrect program to the problem of synthesizing a correct program from a sketch . we have evaluated our system on thousands of real student attempts obtained from 6.00 and 6.00x . our results show that relatively simple error models can correct on average 65 % of all incorrect submissions ."}
{"title": "metric learning for generalizing spatial relations to new objects", "abstract": "human-centered environments are rich with a wide variety of spatial relations between everyday objects . for autonomous robots to operate effectively in such environments , they should be able to reason about these relations and generalize them to objects with different shapes and sizes . for example , having learned to place a toy inside a basket , a robot should be able to generalize this concept using a spoon and a cup . this requires a robot to have the flexibility to learn arbitrary relations in a lifelong manner , making it challenging for an expert to pre-program it with sufficient knowledge to do so beforehand . in this paper , we address the problem of learning spatial relations by introducing a novel method from the perspective of distance metric learning . our approach enables a robot to reason about the similarity between pairwise spatial relations , thereby enabling it to use its previous knowledge when presented with a new relation to imitate . we show how this makes it possible to learn arbitrary spatial relations from non-expert users using a small number of examples and in an interactive manner . our extensive evaluation with real-world data demonstrates the effectiveness of our method in reasoning about a continuous spectrum of spatial relations and generalizing them to new objects ."}
{"title": "on a correlational clustering of integers", "abstract": "correlation clustering is a concept of machine learning . the ultimate goal of such a clustering is to find a partition with minimal conflicts . in this paper we investigate a correlation clustering of integers , based upon the greatest common divisor ."}
{"title": "algorithms for batch hierarchical reinforcement learning", "abstract": "hierarchical reinforcement learning ( hrl ) exploits temporal abstraction to solve large markov decision processes ( mdp ) and provide transferable subtask policies . in this paper , we introduce an off-policy hrl algorithm : hierarchical q-value iteration ( hqi ) . we show that it is possible to effectively learn recursive optimal policies for any valid hierarchical decomposition of the original mdp , given a fixed dataset collected from a flat stochastic behavioral policy . we first formally prove the convergence of the algorithm for tabular mdp . then our experiments on the taxi domain show that hqi converges faster than a flat q-value iteration and enjoys easy state abstraction . also , we demonstrate that our algorithm is able to learn optimal policies for different hierarchical structures from the same fixed dataset , which enables model comparison without recollecting data ."}
{"title": "particle gibbs with ancestor sampling for probabilistic programs", "abstract": "particle markov chain monte carlo techniques rank among current state-of-the-art methods for probabilistic program inference . a drawback of these techniques is that they rely on importance resampling , which results in degenerate particle trajectories and a low effective sample size for variables sampled early in a program . we here develop a formalism to adapt ancestor resampling , a technique that mitigates particle degeneracy , to the probabilistic programming setting . we present empirical results that demonstrate nontrivial performance gains ."}
{"title": "internal guidance for satallax", "abstract": "we propose a new internal guidance method for automated theorem provers based on the given-clause algorithm . our method influences the choice of unprocessed clauses using positive and negative examples from previous proofs . to this end , we present an efficient scheme for naive bayesian classification by generalising label occurrences to types with monoid structure . this makes it possible to extend existing fast classifiers , which consider only positive examples , with negative ones . we implement the method in the higher-order logic prover satallax , where we modify the delay with which propositions are processed . we evaluated our method on a simply-typed higher-order logic version of the flyspeck project , where it solves 26 % more problems than satallax without internal guidance ."}
{"title": "quantum robot : structure , algorithms and applications", "abstract": "this paper has been withdrawn ."}
{"title": "diverse landmark sampling from determinantal point processes for scalable manifold learning", "abstract": "high computational costs of manifold learning prohibit its application for large point sets . a common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the nystr\\ '' om method . the two main challenges that arise are : ( i ) the landmarks selected in non-euclidean geometries must result in a low reconstruction error , ( ii ) the graph constructed from sparsely sampled landmarks must approximate the manifold well . we propose the sampling of landmarks from determinantal distributions on non-euclidean spaces . since current determinantal sampling algorithms have the same complexity as those for manifold learning , we present an efficient approximation running in linear time . further , we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix , estimated from the original point set . the resulting neighborhood selection based on the bhattacharyya distance improves the embedding of sparsely sampled manifolds . our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques ."}
{"title": "towards a common implementation of reinforcement learning for multiple robotic tasks", "abstract": "mobile robots are increasingly being employed for performing complex tasks in dynamic environments . reinforcement learning ( rl ) methods are recognized to be promising for specifying such tasks in a relatively simple manner . however , the strong dependency between the learning method and the task to learn is a well-known problem that restricts practical implementations of rl in robotics , often requiring major modifications of parameters and adding other techniques for each particular task . in this paper we present a practical core implementation of rl which enables the learning process for multiple robotic tasks with minimal per-task tuning or none . based on value iteration methods , this implementation includes a novel approach for action selection , called q-biased softmax regression ( qbiassr ) , which avoids poor performance of the learning process when the robot reaches new unexplored states . our approach takes advantage of the structure of the state space by attending the physical variables involved ( e.g. , distances to obstacles , x , y , { \\theta } pose , etc . ) , thus experienced sets of states may favor the decision-making process of unexplored or rarely-explored states . this improvement has a relevant role in reducing the tuning of the algorithm for particular tasks . experiments with real and simulated robots , performed with the software framework also introduced here , show that our implementation is effectively able to learn different robotic tasks without tuning the learning method . results also suggest that the combination of true online sarsa ( { \\lambda } ) with qbiassr can outperform the existing rl core algorithms in low-dimensional robotic tasks ."}
{"title": "a unified decision making framework for supply and demand management in microgrid networks", "abstract": "this paper considers two important problems - on the supply-side and demand-side respectively and studies both in a unified framework . on the supply side , we study the problem of energy sharing among microgrids with the goal of maximizing profit obtained from selling power while meeting customer demand . on the other hand , under shortage of power , this problem becomes one of deciding the amount of power to be bought with dynamically varying prices . on the demand side , we consider the problem of optimally scheduling the time-adjustable demand - i.e. , of loads with flexible time windows in which they can be scheduled . while previous works have treated these two problems in isolation , we combine these problems together and provide for the first time in the literature , a unified markov decision process ( mdp ) framework for these problems . we then apply the q-learning algorithm , a popular model-free reinforcement learning technique , to obtain the optimal policy . through simulations , we show that our model outperforms the traditional power sharing models ."}
{"title": "building end-to-end dialogue systems using generative hierarchical neural network models", "abstract": "we investigate the task of building open domain , conversational dialogue systems based on large dialogue corpora using generative models . generative models produce system responses that are autonomously generated word-by-word , opening up the possibility for realistic , flexible interactions . in support of this goal , we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain , and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models . we investigate the limitations of this and similar approaches , and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings ."}
{"title": "coefficients of relations for probabilistic reasoning", "abstract": "definitions and notations with historical references are given for some numerical coefficients commonly used to quantify relations among collections of objects for the purpose of expressing approximate knowledge and probabilistic reasoning ."}
{"title": "fuzzy approaches to abductive inference", "abstract": "this paper proposes two kinds of fuzzy abductive inference in the framework of fuzzy rule base . the abductive inference processes described here depend on the semantic of the rule . we distinguish two classes of interpretation of a fuzzy rule , certainty generation rules and possible generation rules . in this paper we present the architecture of abductive inference in the first class of interpretation . we give two kinds of problem that we can resolve by using the proposed models of inference ."}
{"title": "multilayered model of speech", "abstract": "human speech is the most important part of general artificial intelligence and subject of much research . the hypothesis proposed in this article provides explanation of difficulties that modern science tackles in the field of human brain simulation . the hypothesis is based on the author 's conviction that the brain of any given person has different ability to process and store information . therefore , the approaches that are currently used to create general artificial intelligence have to be altered ."}
{"title": "space-time graph modeling of ride requests based on real-world data", "abstract": "this paper focuses on modeling ride requests and their variations over location and time , based on analyzing extensive real-world data from a ride-sharing service . we introduce a graph model that captures the spatial and temporal variability of ride requests and the potentials for ride pooling . we discover these ride request graphs exhibit a well known property called densification power law often found in real graphs modelling human behaviors . we show the pattern of ride requests and the potential of ride pooling for a city can be characterized by the densification factor of the ride request graphs . previous works have shown that it is possible to automatically generate synthetic versions of these graphs that exhibit a given densification factor . we present an algorithm for automatic generation of synthetic ride request graphs that match quite well the densification factor of ride request graphs from actual ride request data ."}
{"title": "probabilistic conflict resolution in hierarchical hypothesis spaces", "abstract": "artificial intelligence applications such as industrial robotics , military surveillance , and hazardous environment clean-up , require situation understanding based on partial , uncertain , and ambiguous or erroneous evidence . it is necessary to evaluate the relative likelihood of multiple possible hypotheses of the ( current ) situation faced by the decision making program . often , the evidence and hypotheses are hierarchical in nature . in image understanding tasks , for example , evidence begins with raw imagery , from which ambiguous features are extracted which have multiple possible aggregations providing evidential support for the presence of multiple hypothesis of objects and terrain , which in turn aggregate in multiple ways to provide partial evidence for different interpretations of the ambient scene . information fusion for military situation understanding has a similar evidence/hypothesis hierarchy from multiple sensor through message level interpretations , and also provides evidence at multiple levels of the doctrinal hierarchy of military forces ."}
{"title": "tackling dynamic vehicle routing problem with time windows by means of ant colony system", "abstract": "the dynamic vehicle routing problem with time windows ( dvrptw ) is an extension of the well-known vehicle routing problem ( vrp ) , which takes into account the dynamic nature of the problem . this aspect requires the vehicle routes to be updated in an ongoing manner as new customer requests arrive in the system and must be incorporated into an evolving schedule during the working day . besides the vehicle capacity constraint involved in the classical vrp , dvrptw considers in addition time windows , which are able to better capture real-world situations . despite this , so far , few studies have focused on tackling this problem of greater practical importance . to this end , this study devises for the resolution of dvrptw , an ant colony optimization based algorithm , which resorts to a joint solution construction mechanism , able to construct in parallel the vehicle routes . this method is coupled with a local search procedure , aimed to further improve the solutions built by ants , and with an insertion heuristics , which tries to reduce the number of vehicles used to service the available customers . the experiments indicate that the proposed algorithm is competitive and effective , and on dvrptw instances with a higher dynamicity level , it is able to yield better results compared to existing ant-based approaches ."}
{"title": "understanding deep neural networks with rectified linear units", "abstract": "in this paper we investigate the family of functions representable by deep neural networks ( dnn ) with rectified linear units ( relu ) . we give an algorithm to train a relu dnn with one hidden layer to *global optimality* with runtime polynomial in the data size albeit exponential in the input dimension . further , we improve on the known lower bounds on size ( from exponential to super exponential ) for approximating a relu deep net function by a shallower relu net . our gap theorems hold for smoothly parametrized families of `` hard '' functions , contrary to countable , discrete families known in the literature . an example consequence of our gap theorems is the following : for every natural number $ k $ there exists a function representable by a relu dnn with $ k^2 $ hidden layers and total size $ k^3 $ , such that any relu dnn with at most $ k $ hidden layers will require at least $ \\frac { 1 } { 2 } k^ { k+1 } -1 $ total nodes . finally , for the family of $ \\mathbb { r } ^n\\to \\mathbb { r } $ dnns with relu activations , we show a new lowerbound on the number of affine pieces , which is larger than previous constructions in certain regimes of the network architecture and most distinctively our lowerbound is demonstrated by an explicit construction of a *smoothly parameterized* family of functions attaining this scaling . our construction utilizes the theory of zonotopes from polyhedral theory ."}
{"title": "parallel large-scale attribute reduction on cloud systems", "abstract": "the rapid growth of emerging information technologies and application patterns in modern society , e.g. , internet , internet of things , cloud computing and tri-network convergence , has caused the advent of the era of big data . big data contains huge values , however , mining knowledge from big data is a tremendously challenging task because of data uncertainty and inconsistency . attribute reduction ( also known as feature selection ) can not only be used as an effective preprocessing step , but also exploits the data redundancy to reduce the uncertainty . however , existing solutions are designed 1 ) either for a single machine that means the entire data must fit in the main memory and the parallelism is limited ; 2 ) or for the hadoop platform which means that the data have to be loaded into the distributed memory frequently and therefore become inefficient . in this paper , we overcome these shortcomings for maximum efficiency possible , and propose a unified framework for parallel large-scale attribute reduction , termed plar , for big data analysis . plar consists of three components : 1 ) granular computing ( grc ) -based initialization : it converts a decision table ( i.e. , original data representation ) into a granularity representation which reduces the amount of space and hence can be easily cached in the distributed memory : 2 ) model-parallelism : it simultaneously evaluates all feature candidates and makes attribute reduction highly parallelizable ; 3 ) data-parallelism : it computes the significance of an attribute in parallel using a mapreduce-style manner . we implement plar with four representative heuristic feature selection algorithms on spark , and evaluate them on various huge datasets , including uci and astronomical datasets , finding our method 's advantages beyond existing solutions ."}
{"title": "overcoming misleads in logic programs by redefining negation", "abstract": "negation as failure and incomplete information in logic programs have been studied by many researchers in order to explains how a negated conclusion was reached , we introduce and proof a different way for negating facts to overcoming misleads in logic programs . negating facts can be achieved by asking the user for constants that do not appear elsewhere in the knowledge base ."}
{"title": "a cross entropy based optimization algorithm with global convergence guarantees", "abstract": "the cross entropy ( ce ) method is a model based search method to solve optimization problems where the objective function has minimal structure . the monte-carlo version of the ce method employs the naive sample averaging technique which is inefficient , both computationally and space wise . we provide a novel stochastic approximation version of the ce method , where the sample averaging is replaced with incremental geometric averaging . this approach can save considerable computational and storage costs . our algorithm is incremental in nature and possesses additional attractive features such as accuracy , stability , robustness and convergence to the global optimum for a particular class of objective functions . we evaluate the algorithm on a variety of global optimization benchmark problems and the results obtained corroborate our theoretical findings ."}
{"title": "reinforcement learning on web interfaces using workflow-guided exploration", "abstract": "reinforcement learning ( rl ) agents improve through trial-and-error , but when reward is sparse and the agent can not discover successful action sequences , learning stagnates . this has been a notable problem in training deep rl agents to perform web-based tasks , such as booking flights or replying to emails , where a single mistake can ruin the entire sequence of actions . a common remedy is to `` warm-start '' the agent by pre-training it to mimic expert demonstrations , but this is prone to overfitting . instead , we propose to constrain exploration using demonstrations . from each demonstration , we induce high-level `` workflows '' which constrain the allowable actions at each time step to be similar to those in the demonstration ( e.g. , `` step 1 : click on a textbox ; step 2 : enter some text '' ) . our exploration policy then learns to identify successful workflows and samples actions that satisfy these workflows . workflows prune out bad exploration directions and accelerate the agent 's ability to discover rewards . we use our approach to train a novel neural policy designed to handle the semi-structured nature of websites , and evaluate on a suite of web tasks , including the recent world of bits benchmark . we achieve new state-of-the-art results , and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 100x ."}
{"title": "mental sampling in multimodal representations", "abstract": "both resources in the natural environment and concepts in a semantic space are distributed `` patchily '' , with large gaps in between the patches . to describe people 's internal and external foraging behavior , various random walk models have been proposed . in particular , internal foraging has been modeled as sampling : in order to gather relevant information for making a decision , people draw samples from a mental representation using random-walk algorithms such as markov chain monte carlo ( mcmc ) . however , two common empirical observations argue against simple sampling algorithms such as mcmc . first , the spatial structure is often best described by a l\\'evy flight distribution : the probability of the distance between two successive locations follows a power-law on the distances . second , the temporal structure of the sampling that humans and other animals produce have long-range , slowly decaying serial correlations characterized as $ 1/f $ -like fluctuations . we propose that mental sampling is not done by simple mcmc , but is instead adapted to multimodal representations and is implemented by metropolis-coupled markov chain monte carlo ( mc $ ^3 $ ) , one of the first algorithms developed for sampling from multimodal distributions . mc $ ^3 $ involves running multiple markov chains in parallel but with target distributions of different temperatures , and it swaps the states of the chains whenever a better location is found . heated chains more readily traverse valleys in the probability landscape to propose moves to far-away peaks , while the colder chains make the local steps that explore the current peak or patch . we show that mc $ ^3 $ generates distances between successive samples that follow a l\\'evy flight distribution and $ 1/f $ -like serial correlations , providing a single mechanistic account of these two puzzling empirical phenomena ."}
{"title": "risk-averse approximate dynamic programming with quantile-based risk measures", "abstract": "in this paper , we consider a finite-horizon markov decision process ( mdp ) for which the objective at each stage is to minimize a quantile-based risk measure ( qbrm ) of the sequence of future costs ; we call the overall objective a dynamic quantile-based risk measure ( dqbrm ) . in particular , we consider optimizing dynamic risk measures where the one-step risk measures are qbrms , a class of risk measures that includes the popular value at risk ( var ) and the conditional value at risk ( cvar ) . although there is considerable theoretical development of risk-averse mdps in the literature , the computational challenges have not been explored as thoroughly . we propose data-driven and simulation-based approximate dynamic programming ( adp ) algorithms to solve the risk-averse sequential decision problem . we address the issue of inefficient sampling for risk applications in simulated settings and present a procedure , based on importance sampling , to direct samples toward the `` risky region '' as the adp algorithm progresses . finally , we show numerical results of our algorithms in the context of an application involving risk-averse bidding for energy storage ."}
{"title": "learning solving procedure for artificial neural network", "abstract": "it is expected that progress toward true artificial intelligence will be achieved through the emergence of a system that integrates representation learning and complex reasoning ( lecun et al . 2015 ) . in response to this prediction , research has been conducted on implementing the symbolic reasoning of a von neumann computer in an artificial neural network ( graves et al . 2016 ; graves et al . 2014 ; reed et al . 2015 ) . however , these studies have many limitations in realizing neural-symbolic integration ( jaeger . 2016 ) . here , we present a new learning paradigm : a learning solving procedure ( lsp ) that learns the procedure for solving complex problems . this is not accomplished merely by learning input-output data , but by learning algorithms through a solving procedure that obtains the output as a sequence of tasks for a given input problem . the lsp neural network system not only learns simple problems of addition and multiplication , but also the algorithms of complicated problems , such as complex arithmetic expression , sorting , and hanoi tower . to realize this , the lsp neural network structure consists of a deep neural network and long short-term memory , which are recursively combined . through experimentation , we demonstrate the efficiency and scalability of lsp and its validity as a mechanism of complex reasoning ."}
{"title": "decompositions of grammar constraints", "abstract": "a wide range of constraints can be compactly specified using automata or formal languages . in a sequence of recent papers , we have shown that an effective means to reason with such specifications is to decompose them into primitive constraints . we can then , for instance , use state of the art sat solvers and profit from their advanced features like fast unit propagation , clause learning , and conflict-based search heuristics . this approach holds promise for solving combinatorial problems in scheduling , rostering , and configuration , as well as problems in more diverse areas like bioinformatics , software testing and natural language processing . in addition , decomposition may be an effective method to propagate other global constraints ."}
{"title": "gradient descent gan optimization is locally stable", "abstract": "despite the growing prominence of generative adversarial networks ( gans ) , optimization in gans is still a poorly understood topic . in this paper , we analyze the `` gradient descent '' form of gan optimization i.e. , the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters . we show that even though gan optimization does not correspond to a convex-concave game ( even for simple parameterizations ) , under proper conditions , equilibrium points of this optimization procedure are still \\emph { locally asymptotically stable } for the traditional gan formulation . on the other hand , we show that the recently proposed wasserstein gan can have non-convergent limit cycles near equilibrium . motivated by this stability analysis , we propose an additional regularization term for gradient descent gan updates , which \\emph { is } able to guarantee local stability for both the wgan and the traditional gan , and also shows practical promise in speeding up convergence and addressing mode collapse ."}
{"title": "dempster 's rule for evidence ordered in a complete directed acyclic graph", "abstract": "for the case of evidence ordered in a complete directed acyclic graph this paper presents a new algorithm with lower computational complexity for dempster 's rule than that of step-by-step application of dempster 's rule . in this problem , every original pair of evidences , has a corresponding evidence against the simultaneous belief in both propositions . in this case , it is uncertain whether the propositions of any two evidences are in logical conflict . the original evidences are associated with the vertices and the additional evidences are associated with the edges . the original evidences are ordered , i.e. , for every pair of evidences it is determinable which of the two evidences is the earlier one . we are interested in finding the most probable completely specified path through the graph , where transitions are possible only from lower- to higher-ranked vertices . the path is here a representation for a sequence of states , for instance a sequence of snapshots of a physical object 's track . a completely specified path means that the path includes no other vertices than those stated in the path representation , as opposed to an incompletely specified path that may also include other vertices than those stated . in a hierarchical network of all subsets of the frame , i.e. , of all incompletely specified paths , the original and additional evidences support subsets that are not disjoint , thus it is not possible to prune the network to a tree . instead of propagating belief , the new algorithm reasons about the logical conditions of a completely specified path through the graph . the new algorithm is o ( |theta| log |theta| ) , compared to o ( |theta| ** log |theta| ) of the classic brute force algorithm ."}
{"title": "search using n-gram technique based statistical analysis for knowledge extraction in case based reasoning systems", "abstract": "searching techniques for case based reasoning systems involve extensive methods of elimination . in this paper , we look at a new method of arriving at the right solution by performing a series of transformations upon the data . these involve n-gram based comparison and deduction of the input data with the case data , using morphemes and phonemes as the deciding parameters . a similar technique for eliminating possible errors using a noise removal function is performed . the error tracking and elimination is performed through a statistical analysis of obtained data , where the entire data set is analyzed as sub-categories of various etymological derivatives . a probability analysis for the closest match is then performed , which yields the final expression . this final expression is referred to the case base . the output is redirected through an expert system based on best possible match . the threshold for the match is customizable , and could be set by the knowledge-architect ."}
{"title": "balancing explicability and explanation in human-aware planning", "abstract": "human aware planning requires an agent to be aware of the intentions , capabilities and mental model of the human in the loop during its decision process . this can involve generating plans that are explicable to a human observer as well as the ability to provide explanations when such plans can not be generated . this has led to the notion `` multi-model planning '' which aim to incorporate effects of human expectation in the deliberative process of a planner - either in the form of explicable task planning or explanations produced thereof . in this paper , we bring these two concepts together and show how a planner can account for both these needs and achieve a trade-off during the plan generation process itself by means of a model-space search method mega . this in effect provides a comprehensive perspective of what it means for a decision making agent to be `` human-aware '' by bringing together existing principles of planning under the umbrella of a single plan generation process . we situate our discussion specifically keeping in mind the recent work on explicable planning and explanation generation , and illustrate these concepts in modified versions of two well known planning domains , as well as a demonstration on a robot involved in a typical search and reconnaissance task with an external supervisor ."}
{"title": "robust natural language processing - combining reasoning , cognitive semantics and construction grammar for spatial language", "abstract": "we present a system for generating and understanding of dynamic and static spatial relations in robotic interaction setups . robots describe an environment of moving blocks using english phrases that include spatial relations such as `` across '' and `` in front of '' . we evaluate the system in robot-robot interactions and show that the system can robustly deal with visual perception errors , language omissions and ungrammatical utterances ."}
{"title": "cultural algorithm toolkit for multi-objective rule mining", "abstract": "cultural algorithm is a kind of evolutionary algorithm inspired from societal evolution and is composed of a belief space , a population space and a protocol that enables exchange of knowledge between these sources . knowledge created in the population space is accepted into the belief space while this collective knowledge from these sources is combined to influence the decisions of the individual agents in solving problems . classification rules comes under descriptive knowledge discovery in data mining and are the most sought out by users since they represent highly comprehensible form of knowledge . the rules have certain properties which make them useful forms of actionable knowledge to users . the rules are evaluated using these properties namely the rule metrics . in the current study a cultural algorithm toolkit for classification rule mining ( cat-crm ) is proposed which allows the user to control three different set of parameters namely the evolutionary parameters , the rule parameters as well as agent parameters and hence can be used for experimenting with an evolutionary system , a rule mining system or an agent based social system . results of experiments conducted to observe the effect of different number and type of metrics on the performance of the algorithm on bench mark data sets is reported ."}
{"title": "stochastic local search for pattern set mining", "abstract": "local search methods can quickly find good quality solutions in cases where systematic search methods might take a large amount of time . moreover , in the context of pattern set mining , exhaustive search methods are not applicable due to the large search space they have to explore . in this paper , we propose the application of stochastic local search to solve the pattern set mining . specifically , to the task of concept learning . we applied a number of local search algorithms on a standard benchmark instances for pattern set mining and the results show the potentials for further exploration ."}
{"title": "competitive coevolution through evolutionary complexification", "abstract": "two major goals in machine learning are the discovery and improvement of solutions to complex problems . in this paper , we argue that complexification , i.e . the incremental elaboration of solutions through adding new structure , achieves both these goals . we demonstrate the power of complexification through the neuroevolution of augmenting topologies ( neat ) method , which evolves increasingly complex neural network architectures . neat is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head . because the robot duel domain supports a wide range of strategies , and because coevolution benefits from an escalating arms race , it serves as a suitable testbed for studying complexification . when compared to the evolution of networks with fixed structure , complexifying evolution discovers significantly more sophisticated strategies . the results suggest that in order to discover and improve complex solutions , evolution , and search in general , should be allowed to complexify as well as optimize ."}
{"title": "on the equivalence of hopfield networks and boltzmann machines", "abstract": "a specific type of neural network , the restricted boltzmann machine ( rbm ) , is implemented for classification and feature detection in machine learning . rbm is characterized by separate layers of visible and hidden units , which are able to learn efficiently a generative model of the observed data . we study a `` hybrid '' version of rbm 's , in which hidden units are analog and visible units are binary , and we show that thermodynamics of visible units are equivalent to those of a hopfield network , in which the n visible units are the neurons and the p hidden units are the learned patterns . we apply the method of stochastic stability to derive the thermodynamics of the model , by considering a formal extension of this technique to the case of multiple sets of stored patterns , which may act as a benchmark for the study of correlated sets . our results imply that simulating the dynamics of a hopfield network , requiring the update of n neurons and the storage of n ( n-1 ) /2 synapses , can be accomplished by a hybrid boltzmann machine , requiring the update of n+p neurons but the storage of only np synapses . in addition , the well known glass transition of the hopfield network has a counterpart in the boltzmann machine : it corresponds to an optimum criterion for selecting the relative sizes of the hidden and visible layers , resolving the trade-off between flexibility and generality of the model . the low storage phase of the hopfield model corresponds to few hidden units and hence a overly constrained rbm , while the spin-glass phase ( too many hidden units ) corresponds to unconstrained rbm prone to overfitting of the observed data ."}
{"title": "foundations of the pareto iterated local search metaheuristic", "abstract": "the paper describes the proposition and application of a local search metaheuristic for multi-objective optimization problems . it is based on two main principles of heuristic search , intensification through variable neighborhoods , and diversification through perturbations and successive iterations in favorable regions of the search space . the concept is successfully tested on permutation flow shop scheduling problems under multiple objectives . while the obtained results are encouraging in terms of their quality , another positive attribute of the approach is its ' simplicity as it does require the setting of only very few parameters . the implementation of the pareto iterated local search metaheuristic is based on the moopps computer system of local search heuristics for multi-objective scheduling which has been awarded the european academic software award 2002 in ronneby , sweden ( http : //www.easa-award.net/ , http : //www.bth.se/llab/easa_2002.nsf )"}
{"title": "nonmonotonic logics and semantics", "abstract": "tarski gave a general semantics for deductive reasoning : a formula a may be deduced from a set a of formulas iff a holds in all models in which each of the elements of a holds . a more liberal semantics has been considered : a formula a may be deduced from a set a of formulas iff a holds in all of the `` preferred '' models in which all the elements of a hold . shoham proposed that the notion of `` preferred '' models be defined by a partial ordering on the models of the underlying language . a more general semantics is described in this paper , based on a set of natural properties of choice functions . this semantics is here shown to be equivalent to a semantics based on comparing the relative `` importance '' of sets of models , by what amounts to a qualitative probability measure . the consequence operations defined by the equivalent semantics are then characterized by a weakening of tarski 's properties in which the monotonicity requirement is replaced by three weaker conditions . classical propositional connectives are characterized by natural introduction-elimination rules in a nonmonotonic setting . even in the nonmonotonic setting , one obtains classical propositional logic , thus showing that monotonicity is not required to justify classical propositional connectives ."}
{"title": "google 's multilingual neural machine translation system : enabling zero-shot translation", "abstract": "we propose a simple solution to use a single neural machine translation ( nmt ) model to translate between multiple languages . our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language . the rest of the model , which includes encoder , decoder and attention , remains unchanged and is shared across all languages . using a shared wordpiece vocabulary , our approach enables multilingual nmt using a single model without any increase in parameters , which is significantly simpler than previous proposals for multilingual nmt . our method often improves the translation quality of all involved language pairs , even while keeping the total number of model parameters constant . on the wmt'14 benchmarks , a single multilingual model achieves comparable performance for english $ \\rightarrow $ french and surpasses state-of-the-art results for english $ \\rightarrow $ german . similarly , a single multilingual model surpasses state-of-the-art results for french $ \\rightarrow $ english and german $ \\rightarrow $ english on wmt'14 and wmt'15 benchmarks respectively . on production corpora , multilingual models of up to twelve language pairs allow for better translation of many individual pairs . in addition to improving the translation quality of language pairs that the model was trained with , our models can also learn to perform implicit bridging between language pairs never seen explicitly during training , showing that transfer learning and zero-shot translation is possible for neural translation . finally , we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages ."}
{"title": "complexity results and practical algorithms for logics in knowledge representation", "abstract": "description logics ( dls ) are used in knowledge-based systems to represent and reason about terminological knowledge of the application domain in a semantically well-defined manner . in this thesis , we establish a number of novel complexity results and give practical algorithms for expressive dls that provide different forms of counting quantifiers . we show that , in many cases , adding local counting in the form of qualifying number restrictions to dls does not increase the complexity of the inference problems , even if binary coding of numbers in the input is assumed . on the other hand , we show that adding different forms of global counting restrictions to a logic may increase the complexity of the inference problems dramatically . we provide exact complexity results and a practical , tableau based algorithm for the dl shiq , which forms the basis of the highly optimized dl system ifact . finally , we describe a tableau algorithm for the clique guarded fragment ( cgf ) , which we hope will serve as the basis for an efficient implementation of a cgf reasoner ."}
{"title": "indonesian earthquake decision support system", "abstract": "earthquake dss is an information technology environment which can be used by government to sharpen , make faster and better the earthquake mitigation decision . earthquake dss can be delivered as e-government which is not only for government itself but in order to guarantee each citizen 's rights for education , training and information about earthquake and how to overcome the earthquake . knowledge can be managed for future use and would become mining by saving and maintain all the data and information about earthquake and earthquake mitigation in indonesia . using web technology will enhance global access and easy to use . datawarehouse as unnormalized database for multidimensional analysis will speed the query process and increase reports variation . link with other disaster dss in one national disaster dss , link with other government information system and international will enhance the knowledge and sharpen the reports ."}
{"title": "building a conversational agent overnight with dialogue self-play", "abstract": "we propose machines talking to machines ( m2m ) , a framework combining automation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents for goal-oriented dialogues in arbitrary domains . m2m scales to new tasks with just a task schema and an api client from the dialogue system developer , but it is also customizable to cater to task-specific interactions . compared to the wizard-of-oz approach for data collection , m2m achieves greater diversity and coverage of salient dialogue flows while maintaining the naturalness of individual utterances . in the first phase , a simulated user bot and a domain-agnostic system bot converse to exhaustively generate dialogue `` outlines '' , i.e . sequences of template utterances and their semantic parses . in the second phase , crowd workers provide contextual rewrites of the dialogues to make the utterances more natural while preserving their meaning . the entire process can finish within a few hours . we propose a new corpus of 3,000 dialogues spanning 2 domains collected with m2m , and present comparisons with popular dialogue datasets on the quality and diversity of the surface forms and dialogue flows ."}
{"title": "generating retinal flow maps from structural optical coherence tomography with artificial intelligence", "abstract": "despite significant advances in artificial intelligence ( ai ) for computer vision , its application in medical imaging has been limited by the burden and limits of expert-generated labels . we used images from optical coherence tomography angiography ( octa ) , a relatively new imaging modality that measures perfusion of the retinal vasculature , to train an ai algorithm to generate vasculature maps from standard structural optical coherence tomography ( oct ) images of the same retinae , both exceeding the ability and bypassing the need for expert labeling . deep learning was able to infer perfusion of microvasculature from structural oct images with similar fidelity to octa and significantly better than expert clinicians ( p < 0.00001 ) . octa suffers from need of specialized hardware , laborious acquisition protocols , and motion artifacts ; whereas our model works directly from standard oct which are ubiquitous and quick to obtain , and allows unlocking of large volumes of previously collected standard oct data both in existing clinical trials and clinical practice . this finding demonstrates a novel application of ai to medical imaging , whereby subtle regularities between different modalities are used to image the same body part and ai is used to generate detailed and accurate inferences of tissue function from structure imaging ."}
{"title": "the effect of social learning on individual learning and evolution", "abstract": "we consider the effects of social learning on the individual learning and genetic evolution of a colony of artificial agents capable of genetic , individual and social modes of adaptation . we confirm that there is strong selection pressure to acquire traits of individual learning and social learning when these are adaptive traits . we show that selection pressure for learning of either kind can supress selection pressure for reproduction or greater fitness . we show that social learning differs from individual learning in that it can support a second evolutionary system that is decoupled from the biological evolutionary system . this decoupling leads to an emergent interaction where immature agents are more likely to engage in learning activities than mature agents ."}
{"title": "a methodology for empirical analysis of lod datasets", "abstract": "cocoe stands for complexity , coherence and entropy , and presents an extensible methodology for empirical analysis of linked open data ( i.e. , rdf graphs ) . cocoe can offer answers to questions like : is dataset a better than b for knowledge discovery since it is more complex and informative ? , is dataset x better than y for simple value lookups due its flatter structure ? , etc . in order to address such questions , we introduce a set of well-founded measures based on complementary notions from distributional semantics , network analysis and information theory . these measures are part of a specific implementation of the cocoe methodology that is available for download . last but not least , we illustrate cocoe by its application to selected biomedical rdf datasets ."}
{"title": "applp : a dialogue on applications of logic programming", "abstract": "this document describes the contributions of the 2016 applications of logic programming workshop ( applp ) , which was held on october 17 and associated with the international conference on logic programming ( iclp ) in flushing , new york city ."}
{"title": "proceedings of the thirteenth conference on uncertainty in artificial intelligence ( 1997 )", "abstract": "this is the proceedings of the thirteenth conference on uncertainty in artificial intelligence , which was held in providence , ri , august 1-3 , 1997"}
{"title": "neighborhood mixture model for knowledge base completion", "abstract": "knowledge bases are useful resources for many natural language processing tasks , however , they are far from complete . in this paper , we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on transe-a well-known embedding model for knowledge base completion . experimental results show that the neighborhood information significantly helps to improve the results of the transe model , leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification , entity prediction and relation prediction tasks ."}
{"title": "mapping heritability of large-scale brain networks with a billion connections { \\em via } persistent homology", "abstract": "in many human brain network studies , we do not have sufficient number ( n ) of images relative to the number ( p ) of voxels due to the prohibitively expensive cost of scanning enough subjects . thus , brain network models usually suffer the small-n large-p problem . such a problem is often remedied by sparse network models , which are usually solved numerically by optimizing l1-penalties . unfortunately , due to the computational bottleneck associated with optimizing l1-penalties , it is not practical to apply such methods to construct large-scale brain networks at the voxel-level . in this paper , we propose a new scalable sparse network model using cross-correlations that bypass the computational bottleneck . our model can build sparse brain networks at the voxel level with p > 25000. instead of using a single sparse parameter that may not be optimal in other studies and datasets , the computational speed gain enables us to analyze the collection of networks at every possible sparse parameter in a coherent mathematical framework via persistent homology . the method is subsequently applied in determining the extent of heritability on a functional brain network at the voxel-level for the first time using twin fmri ."}
{"title": "a cure for pathological behavior in games that use minimax", "abstract": "the traditional approach to choosing moves in game-playing programs is the minimax procedure . the general belief underlying its use is that increasing search depth improves play . recent research has shown that given certain simplifying assumptions about a game tree 's structure , this belief is erroneous : searching deeper decreases the probability of making a correct move . this phenomenon is called game tree pathology . among these simplifying assumptions is uniform depth of win/loss ( terminal ) nodes , a condition which is not true for most real games . analytic studies in [ 10 ] have shown that if every node in a pathological game tree is made terminal with probability exceeding a certain threshold , the resulting tree is nonpathological . this paper considers a new evaluation function which recognizes increasing densities of forced wins at deeper levels in the tree . this property raises two points that strengthen the hypothesis that uniform win depth causes pathology . first , it proves mathematically that as search deepens , an evaluation function that does not explicitly check for certain forced win patterns becomes decreasingly likely to force wins . this failing predicts the pathological behavior of the original evaluation function . second , it shows empirically that despite recognizing fewer mid-game wins than the theoretically predicted minimum , the new function is nonpathological ."}
{"title": "approximated structured prediction for learning large scale graphical models", "abstract": "this manuscripts contains the proofs for `` a primal-dual message-passing algorithm for approximated large scale structured prediction '' ."}
{"title": "time-series classification through histograms of symbolic polynomials", "abstract": "time-series classification has attracted considerable research attention due to the various domains where time-series data are observed , ranging from medicine to econometrics . traditionally , the focus of time-series classification has been on short time-series data composed of a unique pattern with intraclass pattern distortions and variations , while recently there have been attempts to focus on longer series composed of various local patterns . this study presents a novel method which can detect local patterns in long time-series via fitting local polynomial functions of arbitrary degrees . the coefficients of the polynomial functions are converted to symbolic words via equivolume discretizations of the coefficients ' distributions . the symbolic polynomial words enable the detection of similar local patterns by assigning the same words to similar polynomials . moreover , a histogram of the frequencies of the words is constructed from each time-series ' bag of words . each row of the histogram enables a new representation for the series and symbolize the existence of local patterns and their frequencies . experimental evidence demonstrates outstanding results of our method compared to the state-of-art baselines , by exhibiting the best classification accuracies in all the datasets and having statistically significant improvements in the absolute majority of experiments ."}
{"title": "knapsack based optimal policies for budget-limited multi-armed bandits", "abstract": "in budget-limited multi-armed bandit ( mab ) problems , the learner 's actions are costly and constrained by a fixed budget . consequently , an optimal exploitation policy may not be to pull the optimal arm repeatedly , as is the case in other variants of mab , but rather to pull the sequence of different arms that maximises the agent 's total reward within the budget . this difference from existing mabs means that new approaches to maximising the total reward are required . given this , we develop two pulling policies , namely : ( i ) kube ; and ( ii ) fractional kube . whereas the former provides better performance up to 40 % in our experimental settings , the latter is computationally less expensive . we also prove logarithmic upper bounds for the regret of both policies , and show that these bounds are asymptotically optimal ( i.e . they only differ from the best possible regret by a constant factor ) ."}
{"title": "a human - machine interface for teleoperation of arm manipulators in a complex environment", "abstract": "this paper discusses the feasibility of using configuration space ( c-space ) as a means of visualization and control in operator-guided real-time motion of a robot arm manipulator . the motivation is to improve performance of the human operator in tasks involving the manipulator motion in an environment with obstacles . unlike some other motion planning tasks , operators are known to make expensive mistakes in such tasks , even in a simpler two-dimensional case . they have difficulty learning better procedures and their performance improves very little with practice . using an example of a two-dimensional arm manipulator , we show that translating the problem into c-space improves the operator performance rather remarkably , on the order of magnitude compared to the usual work space control . an interface that makes the transfer possible is described , and an example of its use in a virtual environment is shown ."}
{"title": "an efficient metric of automatic weight generation for properties in instance matching technique", "abstract": "the proliferation of heterogeneous data sources of semantic knowledge base intensifies the need of an automatic instance matching technique . however , the efficiency of instance matching is often influenced by the weight of a property associated to instances . automatic weight generation is a non-trivial , however an important task in instance matching technique . therefore , identifying an appropriate metric for generating weight for a property automatically is nevertheless a formidable task . in this paper , we investigate an approach of generating weights automatically by considering hypotheses : ( 1 ) the weight of a property is directly proportional to the ratio of the number of its distinct values to the number of instances contain the property , and ( 2 ) the weight is also proportional to the ratio of the number of distinct values of a property to the number of instances in a training dataset . the basic intuition behind the use of our approach is the classical theory of information content that infrequent words are more informative than frequent ones . our mathematical model derives a metric for generating property weights automatically , which is applied in instance matching system to produce re-conciliated instances efficiently . our experiments and evaluations show the effectiveness of our proposed metric of automatic weight generation for properties in an instance matching technique ."}
{"title": "domain generalization for object recognition with multi-task autoencoders", "abstract": "the problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available , and to then successfully apply it to previously unseen domains . we propose a new feature learning algorithm , multi-task autoencoder ( mtae ) , that provides good generalization performance for cross-domain object recognition . our algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects . instead of reconstructing images from noisy versions , mtae learns to transform the original image into analogs in multiple related domains . it thereby learns features that are robust to variations across domains . the learnt features are then used as inputs to a classifier . we evaluated the performance of the algorithm on benchmark image recognition datasets , where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets . we found that ( denoising ) mtae outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization ."}
{"title": "extension of three-variable counterfactual casual graphic model : from two-value to three-value random variable", "abstract": "the extension of counterfactual causal graphic model with three variables of vertex set in directed acyclic graph ( dag ) is discussed in this paper by extending two- value distribution to three-value distribution of the variables involved in dag . using the conditional independence as ancillary information , 6 kinds of extension counterfactual causal graphic models with some variables are extended from two-value distribution to three-value distribution and the sufficient conditions of identifiability are derived ."}
{"title": "data acquisition and database management system for samsung superconductor test facility", "abstract": "in order to fulfill the test requirement of kstar ( korea superconducting tokamak advanced research ) superconducting magnet system , a large scale superconducting magnet and conductor test facility , sstf ( samsung superconductor test facility ) , has been constructed at samsung advanced institute of technology . the computer system for sstf dac ( data acquisition and control ) is based on unix system and vxworks is used for the real-time os of the vme system . epics ( experimental physics and industrial control system ) is used for the communication between ioc server and client . a database program has been developed for the efficient management of measured data and a linux workstation with pentium-4 cpu is used for the database server . in this paper , the current status of sstf dac system , the database management system and recent test results are presented ."}
{"title": "obtaining accurate probabilistic causal inference by post-processing calibration", "abstract": "discovery of an accurate causal bayesian network structure from observational data can be useful in many areas of science . often the discoveries are made under uncertainty , which can be expressed as probabilities . to guide the use of such discoveries , including directing further investigation , it is important that those probabilities be well-calibrated . in this paper , we introduce a novel framework to derive calibrated probabilities of causal relationships from observational data . the framework consists of three components : ( 1 ) an approximate method for generating initial probability estimates of the edge types for each pair of variables , ( 2 ) the availability of a relatively small number of the causal relationships in the network for which the truth status is known , which we call a calibration training set , and ( 3 ) a calibration method for using the approximate probability estimates and the calibration training set to generate calibrated probabilities for the many remaining pairs of variables . we also introduce a new calibration method based on a shallow neural network . our experiments on simulated data support that the proposed approach improves the calibration of causal edge predictions . the results also support that the approach often improves the precision and recall of predictions ."}
{"title": "clustering and feature selection using sparse principal component analysis", "abstract": "in this paper , we study the application of sparse principal component analysis ( pca ) to clustering and feature selection problems . sparse pca seeks sparse factors , or linear combinations of the data variables , explaining a maximum amount of variance in the data while having only a limited number of nonzero coefficients . pca is often used as a simple clustering technique and sparse factors allow us here to interpret the clusters in terms of a reduced set of variables . we begin with a brief introduction and motivation on sparse pca and detail our implementation of the algorithm in d'aspremont et al . ( 2005 ) . we then apply these results to some classic clustering and feature selection problems arising in biology ."}
{"title": "mining patterns with a balanced interval", "abstract": "in many applications it will be useful to know those patterns that occur with a balanced interval , e.g. , a certain combination of phone numbers are called almost every friday or a group of products are sold a lot on tuesday and thursday . in previous work we proposed a new measure of support ( the number of occurrences of a pattern in a dataset ) , where we count the number of times a pattern occurs ( nearly ) in the middle between two other occurrences . if the number of non-occurrences between two occurrences of a pattern stays almost the same then we call the pattern balanced . it was noticed that some very frequent patterns obviously also occur with a balanced interval , meaning in every transaction . however more interesting patterns might occur , e.g. , every three transactions . here we discuss a solution using standard deviation and average . furthermore we propose a simpler approach for pruning patterns with a balanced interval , making estimating the pruning threshold more intuitive ."}
{"title": "encoding a taxonomy of web attacks with different-length vectors", "abstract": "web attacks , i.e . attacks exclusively using the http protocol , are rapidly becoming one of the fundamental threats for information systems connected to the internet . when the attacks suffered by web servers through the years are analyzed , it is observed that most of them are very similar , using a reduced number of attacking techniques . it is generally agreed that classification can help designers and programmers to better understand attacks and build more secure applications . as an effort in this direction , a new taxonomy of web attacks is proposed in this paper , with the objective of obtaining a practically useful reference framework for security applications . the use of the taxonomy is illustrated by means of multiplatform real world web attack examples . along with this taxonomy , important features of each attack category are discussed . a suitable semantic-dependent web attack encoding scheme is defined that uses different-length vectors . possible applications are described , which might benefit from this taxonomy and encoding scheme , such as intrusion detection systems and application firewalls ."}
{"title": "network of recurrent neural networks", "abstract": "we describe a class of systems theory based neural networks called `` network of recurrent neural networks '' ( nor ) , which introduces a new structure level to rnn related models . in nor , rnns are viewed as the high-level neurons and are used to build the high-level layers . more specifically , we propose several methodologies to design different nor topologies according to the theory of system evolution . then we carry experiments on three different tasks to evaluate our implementations . experimental results show our models outperform simple rnn remarkably under the same number of parameters , and sometimes achieve even better results than gru and lstm ."}
{"title": "robust contextual outlier detection : where context meets sparsity", "abstract": "outlier detection is a fundamental data science task with applications ranging from data cleaning to network security . given the fundamental nature of the task , this has been the subject of much research . recently , a new class of outlier detection algorithms has emerged , called { \\it contextual outlier detection } , and has shown improved performance when studying anomalous behavior in a specific context . however , as we point out in this article , such approaches have limited applicability in situations where the context is sparse ( i.e . lacking a suitable frame of reference ) . moreover , approaches developed to date do not scale to large datasets . to address these problems , here we propose a novel and robust approach alternative to the state-of-the-art called robust contextual outlier detection ( rocod ) . we utilize a local and global behavioral model based on the relevant contexts , which is then integrated in a natural and robust fashion . we also present several optimizations to improve the scalability of the approach . we run rocod on both synthetic and real-world datasets and demonstrate that it outperforms other competitive baselines on the axes of efficacy and efficiency ( 40x speedup compared to modern contextual outlier detection methods ) . we also drill down and perform a fine-grained analysis to shed light on the rationale for the performance gains of rocod and reveal its effectiveness when handling objects with sparse contexts ."}
{"title": "scalability of genetic programming and probabilistic incremental program evolution", "abstract": "this paper discusses scalability of standard genetic programming ( gp ) and the probabilistic incremental program evolution ( pipe ) . to investigate the need for both effective mixing and linkage learning , two test problems are considered : order problem , which is rather easy for any recombination-based gp , and trap or the deceptive trap problem , which requires the algorithm to learn interactions among subsets of terminals . the scalability results show that both gp and pipe scale up polynomially with problem size on the simple order problem , but they both scale up exponentially on the deceptive problem . this indicates that while standard recombination is sufficient when no interactions need to be considered , for some problems linkage learning is necessary . these results are in agreement with the lessons learned in the domain of binary-string genetic algorithms ( gas ) . furthermore , the paper investigates the effects of introducing utnnecessary and irrelevant primitives on the performance of gp and pipe ."}
{"title": "from 3d point clouds to semantic objects an ontology-based detection approach", "abstract": "this paper presents a knowledge-based detection of objects approach using the owl ontology language , the semantic web rule language , and 3d processing built-ins aiming at combining geometrical analysis of 3d point clouds and specialist 's knowledge . this combination allows the detection and the annotation of objects contained in point clouds . the context of the study is the detection of railway objects such as signals , technical cupboards , electric poles , etc . thus , the resulting enriched and populated ontology , that contains the annotations of objects in the point clouds , is used to feed a gis systems or an ifc file for architecture purposes ."}
{"title": "representing and solving asymmetric bayesian decision problems", "abstract": "this paper deals with the representation and solution of asymmetric bayesian decision problems . we present a formal framework , termed asymmetric influence diagrams , that is based on the influence diagram and allows an efficient representation of asymmetric decision problems . as opposed to existing frameworks , the asymmetric influece diagram primarily encodes asymmetry at the qualitative level and it can therefore be read directly from the model . we give an algorithm for solving asymmetric influence diagrams . the algorithm initially decomposes the asymmetric decision problem into a structure of symmetric subproblems organized as a tree . a solution to the decision problem can then be found by propagating from the leaves toward the root using existing evaluation methods to solve the sub-problems ."}
{"title": "using mathml to represent units of measurement for improved ontology alignment", "abstract": "ontologies provide a formal description of concepts and their relationships in a knowledge domain . the goal of ontology alignment is to identify semantically matching concepts and relationships across independently developed ontologies that purport to describe the same knowledge . in order to handle the widest possible class of ontologies , many alignment algorithms rely on terminological and structural meth- ods , but the often fuzzy nature of concepts complicates the matching process . however , one area that should provide clear matching solutions due to its mathematical nature , is units of measurement . several on- tologies for units of measurement are available , but there has been no attempt to align them , notwithstanding the obvious importance for tech- nical interoperability . we propose a general strategy to map these ( and similar ) ontologies by introducing mathml to accurately capture the semantic description of concepts specified therein . we provide mapping results for three ontologies , and show that our approach improves on lexical comparisons ."}
{"title": "reformulating global grammar constraints", "abstract": "an attractive mechanism to specify global constraints in rostering and other domains is via formal languages . for instance , the regular and grammar constraints specify constraints in terms of the languages accepted by an automaton and a context-free grammar respectively . taking advantage of the fixed length of the constraint , we give an algorithm to transform a context-free grammar into an automaton . we then study the use of minimization techniques to reduce the size of such automata and speed up propagation . we show that minimizing such automata after they have been unfolded and domains initially reduced can give automata that are more compact than minimizing before unfolding and reducing . experimental results show that such transformations can improve the size of rostering problems that we can 'model and run ' ."}
{"title": "ai in arbitrary world", "abstract": "in order to build ai we have to create a program which copes well in an arbitrary world . in this paper we will restrict our attention on one concrete world , which represents the game tick-tack-toe . this world is a very simple one but it is sufficiently complicated for our task because most people can not manage with it . the main difficulty in this world is that the player can not see the entire internal state of the world so he has to build a model in order to understand the world . the model which we will offer will consist of final automata and first order formulas ."}
{"title": "societal implicit memory and his speed on tracking extrema over dynamic environments using self-regulatory swarms", "abstract": "in order to overcome difficult dynamic optimization and environment extrema tracking problems , we propose a self-regulated swarm ( srs ) algorithm which hybridizes the advantageous characteristics of swarm intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape ( properly balancing the exploration/exploitation nature of our dynamic search strategy ) , with a simple evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population , speeding it up globally . in order to test his adaptive response and robustness , we have recurred to different dynamic multimodal complex functions as well as to dynamic optimization control problems , measuring reaction speeds and performance . final comparisons were made with standard genetic algorithms ( gas ) , bacterial foraging strategies ( bfoa ) , as well as with recent co-evolutionary approaches . srs 's were able to demonstrate quick adaptive responses , while outperforming the results obtained by the other approaches . additionally , some successful behaviors were found . one of the most interesting illustrate that the present srs collective swarm of bio-inspired ant-like agents is able to track about 65 % of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system ."}
{"title": "beautiful and damned . combined effect of content quality and social ties on user engagement", "abstract": "user participation in online communities is driven by the intertwinement of the social network structure with the crowd-generated content that flows along its links . these aspects are rarely explored jointly and at scale . by looking at how users generate and access pictures of varying beauty on flickr , we investigate how the production of quality impacts the dynamics of online social systems . we develop a deep learning computer vision model to score images according to their aesthetic value and we validate its output through crowdsourcing . by applying it to over 15b flickr photos , we study for the first time how image beauty is distributed over a large-scale social system . beautiful images are evenly distributed in the network , although only a small core of people get social recognition for them . to study the impact of exposure to quality on user engagement , we set up matching experiments aimed at detecting causality from observational data . exposure to beauty is double-edged : following people who produce high-quality content increases one 's probability of uploading better photos ; however , an excessive imbalance between the quality generated by a user and the user 's neighbors leads to a decline in engagement . our analysis has practical implications for improving link recommender systems ."}
{"title": "binary join trees", "abstract": "the main goal of this paper is to describe a data structure called binary join trees that are useful in computing multiple marginals efficiently using the shenoy-shafer architecture . we define binary join trees , describe their utility , and sketch a procedure for constructing them ."}
{"title": "punny captions : witty wordplay in image descriptions", "abstract": "wit is a quintessential form of rich inter-human interaction , and is often grounded in a specific situation ( e.g. , a comment in response to an event ) . in this work , we attempt to build computational models that can produce witty descriptions for a given image . inspired by a cognitive account of humor appreciation , we employ linguistic wordplay , specifically puns . we compare our approach against meaningful baseline approaches via human studies . in a turing test style evaluation , people find our model 's description for an image to be wittier than a human 's witty description 55 % of the time !"}
{"title": "`` is there anything else i can help you with ? `` : challenges in deploying an on-demand crowd-powered conversational agent", "abstract": "intelligent conversational assistants , such as apple 's siri , microsoft 's cortana , and amazon 's echo , have quickly become a part of our digital life . however , these assistants have major limitations , which prevents users from conversing with them as they would with human dialog partners . this limits our ability to observe how users really want to interact with the underlying system . to address this problem , we developed a crowd-powered conversational assistant , chorus , and deployed it to see how users and workers would interact together when mediated by the system . chorus sophisticatedly converses with end users over time by recruiting workers on demand , which in turn decide what might be the best response for each user sentence . up to the first month of our deployment , 59 users have held conversations with chorus during 320 conversational sessions . in this paper , we present an account of chorus ' deployment , with a focus on four challenges : ( i ) identifying when conversations are over , ( ii ) malicious users and workers , ( iii ) on-demand recruiting , and ( iv ) settings in which consensus is not enough . our observations could assist the deployment of crowd-powered conversation systems and crowd-powered systems in general ."}
{"title": "fast restricted causal inference", "abstract": "hidden variables are well known sources of disturbance when recovering belief networks from data based only on measurable variables . hence models assuming existence of hidden variables are under development . this paper presents a new algorithm `` accelerating '' the known ci algorithm of spirtes , glymour and scheines { spirtes:93 } . we prove that this algorithm does not produces ( conditional ) independencies not present in the data if statistical independence test is reliable . this result is to be considered as non-trivial since e.g . the same claim fails to be true for fci algorithm , another `` accelerator '' of ci , developed in { spirtes:93 } ."}
{"title": "minimal cost feature selection of data with normal distribution measurement errors", "abstract": "minimal cost feature selection is devoted to obtain a trade-off between test costs and misclassification costs . this issue has been addressed recently on nominal data . in this paper , we consider numerical data with measurement errors and study minimal cost feature selection in this model . first , we build a data model with normal distribution measurement errors . second , the neighborhood of each data item is constructed through the confidence interval . comparing with discretized intervals , neighborhoods are more reasonable to maintain the information of data . third , we define a new minimal total cost feature selection problem through considering the trade-off between test costs and misclassification costs . fourth , we proposed a backtracking algorithm with three effective pruning techniques to deal with this problem . the algorithm is tested on four uci data sets . experimental results indicate that the pruning techniques are effective , and the algorithm is efficient for data sets with nearly one thousand objects ."}
{"title": "teacher-student curriculum learning", "abstract": "we propose teacher-student curriculum learning ( tscl ) , a framework for automatic curriculum learning , where the student tries to learn a complex task and the teacher automatically chooses subtasks from a given set for the student to train on . we describe a family of teacher algorithms that rely on the intuition that the student should practice more those tasks on which it makes the fastest progress , i.e . where the slope of the learning curve is highest . in addition , the teacher algorithms address the problem of forgetting by also choosing tasks where the student 's performance is getting worse . we demonstrate that tscl matches or surpasses the results of carefully hand-crafted curricula in two tasks : addition of decimal numbers with lstm and navigation in minecraft . using our automatically generated curriculum enabled to solve a minecraft maze that could not be solved at all when training directly on solving the maze , and the learning was an order of magnitude faster than uniform sampling of subtasks ."}
{"title": "minimum model semantics for logic programs with negation-as-failure", "abstract": "we give a purely model-theoretic characterization of the semantics of logic programs with negation-as-failure allowed in clause bodies . in our semantics the meaning of a program is , as in the classical case , the unique minimum model in a program-independent ordering . we use an expanded truth domain that has an uncountable linearly ordered set of truth values between false ( the minimum element ) and true ( the maximum ) , with a zero element in the middle . the truth values below zero are ordered like the countable ordinals . the values above zero have exactly the reverse order . negation is interpreted as reflection about zero followed by a step towards zero ; the only truth value that remains unaffected by negation is zero . we show that every program has a unique minimum model m_p , and that this model can be constructed with a t_p iteration which proceeds through the countable ordinals . furthermore , we demonstrate that m_p can also be obtained through a model intersection construction which generalizes the well-known model intersection theorem for classical logic programming . finally , we show that by collapsing the true and false values of the infinite-valued model m_p to ( the classical ) true and false , we obtain a three-valued model identical to the well-founded one ."}
{"title": "tractable triangles and cross-free convexity in discrete optimisation", "abstract": "the minimisation problem of a sum of unary and pairwise functions of discrete variables is a general np-hard problem with wide applications such as computing map configurations in markov random fields ( mrf ) , minimising gibbs energy , or solving binary valued constraint satisfaction problems ( vcsps ) . we study the computational complexity of classes of discrete optimisation problems given by allowing only certain types of costs in every triangle of variable-value assignments to three distinct variables . we show that for several computational problems , the only non- trivial tractable classes are the well known maximum matching problem and the recently discovered joint-winner property . our results , apart from giving complete classifications in the studied cases , provide guidance in the search for hybrid tractable classes ; that is , classes of problems that are not captured by restrictions on the functions ( such as submodularity ) or the structure of the problem graph ( such as bounded treewidth ) . furthermore , we introduce a class of problems with convex cardinality functions on cross-free sets of assignments . we prove that while imposing only one of the two conditions renders the problem np-hard , the conjunction of the two gives rise to a novel tractable class satisfying the cross-free convexity property , which generalises the joint-winner property to problems of unbounded arity ."}
{"title": "cauchy annealing schedule : an annealing schedule for boltzmann selection scheme in evolutionary algorithms", "abstract": "boltzmann selection is an important selection mechanism in evolutionary algorithms as it has theoretical properties which help in theoretical analysis . however , boltzmann selection is not used in practice because a good annealing schedule for the ` inverse temperature ' parameter is lacking . in this paper we propose a cauchy annealing schedule for boltzmann selection scheme based on a hypothesis that selection-strength should increase as evolutionary process goes on and distance between two selection strengths should decrease for the process to converge . to formalize these aspects , we develop formalism for selection mechanisms using fitness distributions and give an appropriate measure for selection-strength . in this paper , we prove an important result , by which we derive an annealing schedule called cauchy annealing schedule . we demonstrate the novelty of proposed annealing schedule using simulations in the framework of genetic algorithms ."}
{"title": "guidelines for artificial intelligence containment", "abstract": "with almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the ai research community . building on our previous work on ai containment problem we propose a number of guidelines which should help ai safety researchers to develop reliable sandboxing software for intelligent programs of all levels . such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage , social engineering attacks and cyberattacks from within the container ."}
{"title": "a deep q-learning agent for the l-game with variable batch training", "abstract": "we employ the deep q-learning algorithm with experience replay to train an agent capable of achieving a high-level of play in the l-game while self-learning from low-dimensional states . we also employ variable batch size for training in order to mitigate the loss of the rare reward signal and significantly accelerate training . despite the large action space due to the number of possible moves , the low-dimensional state space and the rarity of rewards , which only come at the end of a game , dql is successful in training an agent capable of strong play without the use of any search methods or domain knowledge ."}
{"title": "probabilistic acceptance", "abstract": "the idea of fully accepting statements when the evidence has rendered them probable enough faces a number of difficulties . we leave the interpretation of probability largely open , but attempt to suggest a contextual approach to full belief . we show that the difficulties of probabilistic acceptance are not as severe as they are sometimes painted , and that though there are oddities associated with probabilistic acceptance they are in some instances less awkward than the difficulties associated with other nonmonotonic formalisms . we show that the structure at which we arrive provides a natural home for statistical inference ."}
{"title": "decentralized control of cooperative systems : categorization and complexity analysis", "abstract": "decentralized control of cooperative systems captures the operation of a group of decision makers that share a single global objective . the difficulty in solving optimally such problems arises when the agents lack full observability of the global state of the system when they operate . the general problem has been shown to be nexp-complete . in this paper , we identify classes of decentralized control problems whose complexity ranges between nexp and p. in particular , we study problems characterized by independent transitions , independent observations , and goal-oriented objective functions . two algorithms are shown to solve optimally useful classes of goal-oriented decentralized processes in polynomial time . this paper also studies information sharing among the decision-makers , which can improve their performance . we distinguish between three ways in which agents can exchange information : indirect communication , direct communication and sharing state features that are not controlled by the agents . our analysis shows that for every class of problems we consider , introducing direct or indirect communication does not change the worst-case complexity . the results provide a better understanding of the complexity of decentralized control problems that arise in practice and facilitate the development of planning algorithms for these problems ."}
{"title": "a general framework for equivalences in answer-set programming by countermodels in the logic of here-and-there", "abstract": "different notions of equivalence , such as the prominent notions of strong and uniform equivalence , have been studied in answer-set programming , mainly for the purpose of identifying programs that can serve as substitutes without altering the semantics , for instance in program optimization . such semantic comparisons are usually characterized by various selections of models in the logic of here-and-there ( ht ) . for uniform equivalence however , correct characterizations in terms of ht-models can only be obtained for finite theories , respectively programs . in this article , we show that a selection of countermodels in ht captures uniform equivalence also for infinite theories . this result is turned into coherent characterizations of the different notions of equivalence by countermodels , as well as by a mixture of ht-models and countermodels ( so-called equivalence interpretations ) . moreover , we generalize the so-called notion of relativized hyperequivalence for programs to propositional theories , and apply the same methodology in order to obtain a semantic characterization which is amenable to infinite settings . this allows for a lifting of the results to first-order theories under a very general semantics given in terms of a quantified version of ht . we thus obtain a general framework for the study of various notions of equivalence for theories under answer-set semantics . moreover , we prove an expedient property that allows for a simplified treatment of extended signatures , and provide further results for non-ground logic programs . in particular , uniform equivalence coincides under open and ordinary answer-set semantics , and for finite non-ground programs under these semantics , also the usual characterization of uniform equivalence in terms of maximal and total ht-models of the grounding is correct , even for infinite domains , when corresponding ground programs are infinite ."}
{"title": "grammar-based random walkers in semantic networks", "abstract": "semantic networks qualify the meaning of an edge relating any two vertices . determining which vertices are most `` central '' in a semantic network is difficult because one relationship type may be deemed subjectively more important than another . for this reason , research into semantic network metrics has focused primarily on context-based rankings ( i.e . user prescribed contexts ) . moreover , many of the current semantic network metrics rank semantic associations ( i.e . directed paths between two vertices ) and not the vertices themselves . this article presents a framework for calculating semantically meaningful primary eigenvector-based metrics such as eigenvector centrality and pagerank in semantic networks using a modified version of the random walker model of markov chain analysis . random walkers , in the context of this article , are constrained by a grammar , where the grammar is a user defined data structure that determines the meaning of the final vertex ranking . the ideas in this article are presented within the context of the resource description framework ( rdf ) of the semantic web initiative ."}
{"title": "a transformational characterization of equivalent bayesian network structures", "abstract": "we present a simple characterization of equivalent bayesian network structures based on local transformations . the significance of the characterization is twofold . first , we are able to easily prove several new invariant properties of theoretical interest for equivalent structures . second , we use the characterization to derive an efficient algorithm that identifies all of the compelled edges in a structure . compelled edge identification is of particular importance for learning bayesian network structures from data because these edges indicate causal relationships when certain assumptions hold ."}
{"title": "solar radiation forecasting using ad-hoc time series preprocessing and neural networks", "abstract": "in this paper , we present an application of neural networks in the renewable energy domain . we have developed a methodology for the daily prediction of global solar radiation on a horizontal surface . we use an ad-hoc time series preprocessing and a multi-layer perceptron ( mlp ) in order to predict solar radiation at daily horizon . first results are promising with nrmse < 21 % and rmse < 998 wh/m2 . our optimized mlp presents prediction similar to or even better than conventional methods such as arima techniques , bayesian inference , markov chains and k-nearest-neighbors approximators . moreover we found that our data preprocessing approach can reduce significantly forecasting errors ."}
{"title": "monte carlo localization in hand-drawn maps", "abstract": "robot localization is a one of the most important problems in robotics . most of the existing approaches assume that the map of the environment is available beforehand and focus on accurate metrical localization . in this paper , we address the localization problem when the map of the environment is not present beforehand , and the robot relies on a hand-drawn map from a non-expert user . we addressed this problem by expressing the robot pose in the pixel coordinate and simultaneously estimate a local deformation of the hand-drawn map . experiments show that we are able to localize the robot in the correct room with a robustness up to 80 %"}
{"title": "optimizing causal orderings for generating dags from data", "abstract": "an algorithm for generating the structure of a directed acyclic graph from data using the notion of causal input lists is presented . the algorithm manipulates the ordering of the variables with operations which very much resemble arc reversal . operations are only applied if the dag after the operation represents at least the independencies represented by the dag before the operation until no more arcs can be removed from the dag . the resulting dag is a minimal l-map ."}
{"title": "sentiment identification in code-mixed social media text", "abstract": "sentiment analysis is the natural language processing ( nlp ) task dealing with the detection and classification of sentiments in texts . while some tasks deal with identifying the presence of sentiment in the text ( subjectivity analysis ) , other tasks aim at determining the polarity of the text categorizing them as positive , negative and neutral . whenever there is a presence of sentiment in the text , it has a source ( people , group of people or any entity ) and the sentiment is directed towards some entity , object , event or person . sentiment analysis tasks aim to determine the subject , the target and the polarity or valence of the sentiment . in our work , we try to automatically extract sentiment ( positive or negative ) from facebook posts using a machine learning approach.while some works have been done in code-mixed social media data and in sentiment analysis separately , our work is the first attempt ( as of now ) which aims at performing sentiment analysis of code-mixed social media text . we have used extensive pre-processing to remove noise from raw text . multilayer perceptron model has been used to determine the polarity of the sentiment . we have also developed the corpus for this task by manually labeling facebook posts with their associated sentiments ."}
{"title": "a multivalued knowledge-base model", "abstract": "the basic aim of our study is to give a possible model for handling uncertain information . this model is worked out in the framework of datalog . at first the concept of fuzzy datalog will be summarized , then its extensions for intuitionistic- and interval-valued fuzzy logic is given and the concept of bipolar fuzzy datalog is introduced . based on these ideas the concept of multivalued knowledge-base will be defined as a quadruple of any background knowledge ; a deduction mechanism ; a connecting algorithm , and a function set of the program , which help us to determine the uncertainty levels of the results . at last a possible evaluation strategy is given ."}
{"title": "the value of inferring the internal state of traffic participants for autonomous freeway driving", "abstract": "safe interaction with human drivers is one of the primary challenges for autonomous vehicles . in order to plan driving maneuvers effectively , the vehicle 's control system must infer and predict how humans will behave based on their latent internal state ( e.g. , intentions and aggressiveness ) . this research uses a simple model for human behavior with unknown parameters that make up the internal states of the traffic participants and presents a method for quantifying the value of estimating these states and planning with their uncertainty explicitly modeled . an upper performance bound is established by an omniscient monte carlo tree search ( mcts ) planner that has perfect knowledge of the internal states . a baseline lower bound is established by planning with mcts assuming that all drivers have the same internal state . mcts variants are then used to solve a partially observable markov decision process ( pomdp ) that models the internal state uncertainty to determine whether inferring the internal state offers an advantage over the baseline . applying this method to a freeway lane changing scenario reveals that there is a significant performance gap between the upper bound and baseline . pomdp planning techniques come close to closing this gap , especially when important hidden model parameters are correlated with measurable parameters ."}
{"title": "automatically discovering hidden transformation chaining constraints", "abstract": "model transformations operate on models conforming to precisely defined metamodels . consequently , it often seems relatively easy to chain them : the output of a transformation may be given as input to a second one if metamodels match . however , this simple rule has some obvious limitations . for instance , a transformation may only use a subset of a metamodel . therefore , chaining transformations appropriately requires more information . we present here an approach that automatically discovers more detailed information about actual chaining constraints by statically analyzing transformations . the objective is to provide developers who decide to chain transformations with more data on which to base their choices . this approach has been successfully applied to the case of a library of endogenous transformations . they all have the same source and target metamodel but have some hidden chaining constraints . in such a case , the simple metamodel matching rule given above does not provide any useful information ."}
{"title": "projectionnet : learning efficient on-device deep networks using neural projections", "abstract": "deep neural networks have become ubiquitous for applications related to visual recognition and language understanding tasks . however , it is often prohibitive to use typical neural networks on devices like mobile phones or smart watches since the model sizes are huge and can not fit in the limited memory available on such devices . while these devices could make use of machine learning models running on high-performance data centers with cpus or gpus , this is not feasible for many applications because data can be privacy sensitive and inference needs to be performed directly `` on '' device . we introduce a new architecture for training compact neural networks using a joint optimization framework . at its core lies a novel objective that jointly trains using two different types of networks -- a full trainer neural network ( using existing architectures like feed-forward nns or lstm rnns ) combined with a simpler `` projection '' network that leverages random projections to transform inputs or intermediate representations into bits . the simpler network encodes lightweight and efficient-to-compute operations in bit space with a low memory footprint . the two networks are trained jointly using backpropagation , where the projection network learns from the full network similar to apprenticeship learning . once trained , the smaller network can be used directly for inference at low memory and computation cost . we demonstrate the effectiveness of the new approach at significantly shrinking the memory requirements of different types of neural networks while preserving good accuracy on visual recognition and text classification tasks . we also study the question `` how many neural bits are required to solve a given task ? '' using the new framework and show empirical results contrasting model predictive capacity ( in bits ) versus accuracy on several datasets ."}
{"title": "on self-regulated swarms , societal memory , speed and dynamics", "abstract": "we propose a self-regulated swarm ( srs ) algorithm which hybridizes the advantageous characteristics of swarm intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape ( properly balancing the exploration/exploitation nature of our dynamic search strategy ) , with a simple evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population , speeding it up globally . in order to test his adaptive response and robustness , we have recurred to different dynamic multimodal complex functions as well as to dynamic optimization control problems , measuring reaction speeds and performance . final comparisons were made with standard genetic algorithms ( gas ) , bacterial foraging strategies ( bfoa ) , as well as with recent co-evolutionary approaches . srs 's were able to demonstrate quick adaptive responses , while outperforming the results obtained by the other approaches . additionally , some successful behaviors were found . one of the most interesting illustrate that the present srs collective swarm of bio-inspired ant-like agents is able to track about 65 % of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system ."}
{"title": "from virtual to real world visual perception using domain adaptation -- the dpm as example", "abstract": "supervised learning tends to produce more accurate classifiers than unsupervised learning in general . this implies that training data is preferred with annotations . when addressing visual perception challenges , such as localizing certain object classes within an image , the learning of the involved classifiers turns out to be a practical bottleneck . the reason is that , at least , we have to frame object examples with bounding boxes in thousands of images . a priori , the more complex the model is regarding its number of parameters , the more annotated examples are required . this annotation task is performed by human oracles , which ends up in inaccuracies and errors in the annotations ( aka ground truth ) since the task is inherently very cumbersome and sometimes ambiguous . as an alternative we have pioneered the use of virtual worlds for collecting such annotations automatically and with high precision . however , since the models learned with virtual data must operate in the real world , we still need to perform domain adaptation ( da ) . in this chapter we revisit the da of a deformable part-based model ( dpm ) as an exemplifying case of virtual- to-real-world da . as a use case , we address the challenge of vehicle detection for driver assistance , using different publicly available virtual-world data . while doing so , we investigate questions such as : how does the domain gap behave due to virtual-vs-real data with respect to dominant object appearance per domain , as well as the role of photo-realism in the virtual world ."}
{"title": "symbolic probabilitistic inference in large bn2o networks", "abstract": "a bn2o network is a two level belief net in which the parent interactions are modeled using the noisy-or interaction model . in this paper we discuss application of the spi local expression language to efficient inference in large bn2o networks . in particular , we show that there is significant structure , which can be exploited to improve over the quickscore result . we further describe how symbolic techniques can provide information which can significantly reduce the computation required for computing all cause posterior marginals . finally , we present a novel approximation technique with preliminary experimental results ."}
{"title": "taming primary key violations to query large inconsistent data", "abstract": "consistent query answering over a database that violates primary key constraints is a classical hard problem in database research that has been traditionally dealt with logic programming . however , the applicability of existing logic-based solutions is restricted to data sets of moderate size . this paper presents a novel decomposition and pruning strategy that reduces , in polynomial time , the problem of computing the consistent answer to a conjunctive query over a database subject to primary key constraints to a collection of smaller problems of the same sort that can be solved independently . the new strategy is naturally modeled and implemented using answer set programming ( asp ) . an experiment run on benchmarks from the database world prove the effectiveness and efficiency of our asp-based approach also on large data sets . to appear in theory and practice of logic programming ( tplp ) , proceedings of iclp 2015 ."}
{"title": "optimal limited contingency planning", "abstract": "for a given problem , the optimal markov policy can be considerred as a conditional or contingent plan containing a ( potentially large ) number of branches . unfortunately , there are applications where it is desirable to strictly limit the number of decision points and branches in a plan . for example , it may be that plans must later undergo more detailed simulation to verify correctness and safety , or that they must be simple enough to be understood and analyzed by humans . as a result , it may be necessary to limit consideration to plans with only a small number of branches . this raises the question of how one goes about finding optimal plans containing only a limited number of branches . in this paper , we present an any-time algorithm for optimal k-contingency planning ( okp ) . it is the first optimal algorithm for limited contingency planning that is not an explicit enumeration of possible contingent plans . by modelling the problem as a partially observable markov decision process , it implements the bellman optimality principle and prunes the solution space . we present experimental results of applying this algorithm to some simple test cases ."}
{"title": "repeated inverse reinforcement learning", "abstract": "we introduce a novel repeated inverse reinforcement learning problem : the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted . each time the human is surprised , the agent is provided a demonstration of the desired behavior by the human . we formalize this problem , including how the sequence of tasks is chosen , in a few different ways and provide some foundational results ."}
{"title": "variable and value elimination in binary constraint satisfaction via forbidden patterns", "abstract": "variable or value elimination in a constraint satisfaction problem ( csp ) can be used in preprocessing or during search to reduce search space size . a variable elimination rule ( value elimination rule ) allows the polynomial-time identification of certain variables ( domain elements ) whose elimination , without the introduction of extra compensatory constraints , does not affect the satisfiability of an instance . we show that there are essentially just four variable elimination rules and three value elimination rules defined by forbidding generic sub-instances , known as irreducible existential patterns , in arc-consistent csp instances . one of the variable elimination rules is the already-known broken triangle property , whereas the other three are novel . the three value elimination rules can all be seen as strict generalisations of neighbourhood substitution ."}
{"title": "a comparative study of arithmetic constraints on integer intervals", "abstract": "we propose here a number of approaches to implement constraint propagation for arithmetic constraints on integer intervals . to this end we introduce integer interval arithmetic . each approach is explained using appropriate proof rules that reduce the variable domains . we compare these approaches using a set of benchmarks ."}
{"title": "vector-space analysis of belief-state approximation for pomdps", "abstract": "we propose a new approach to value-directed belief state approximation for pomdps . the value-directed model allows one to choose approximation methods for belief state monitoring that have a small impact on decision quality . using a vector space analysis of the problem , we devise two new search procedures for selecting an approximation scheme that have much better computational properties than existing methods . though these provide looser error bounds , we show empirically that they have a similar impact on decision quality in practice , and run up to two orders of magnitude more quickly ."}
{"title": "symbol emergence in robotics : a survey", "abstract": "humans can learn the use of language through physical interaction with their environment and semiotic communication with other people . it is very important to obtain a computational understanding of how humans can form a symbol system and obtain semiotic skills through their autonomous mental development . recently , many studies have been conducted on the construction of robotic systems and machine-learning methods that can learn the use of language through embodied multimodal interaction with their environment and other systems . understanding human social interactions and developing a robot that can smoothly communicate with human users in the long term , requires an understanding of the dynamics of symbol systems and is crucially important . the embodied cognition and social interaction of participants gradually change a symbol system in a constructive manner . in this paper , we introduce a field of research called symbol emergence in robotics ( ser ) . ser is a constructive approach towards an emergent symbol system . the emergent symbol system is socially self-organized through both semiotic communications and physical interactions with autonomous cognitive developmental agents , i.e. , humans and developmental robots . specifically , we describe some state-of-art research topics concerning ser , e.g. , multimodal categorization , word discovery , and a double articulation analysis , that enable a robot to obtain words and their embodied meanings from raw sensory -- motor information , including visual information , haptic information , auditory information , and acoustic speech signals , in a totally unsupervised manner . finally , we suggest future directions of research in ser ."}
{"title": "deep echo state network ( deepesn ) : a brief survey", "abstract": "the study of deep recurrent neural networks ( rnns ) and , in particular , of deep reservoir computing ( rc ) is gaining an increasing research attention in the neural networks community . the recently introduced deep echo state network ( deepesn ) model opened the way to an extremely efficient approach for designing deep neural networks for temporal data . at the same time , the study of deepesns allowed to shed light on the intrinsic properties of state dynamics developed by hierarchical compositions of recurrent layers , i.e . on the bias of depth in rnns architectural design . in this paper , we summarize the advancements in the development , analysis and applications of deepesns ."}
{"title": "a decidable subclass of finitary programs", "abstract": "answer set programming - the most popular problem solving paradigm based on logic programs - has been recently extended to support uninterpreted function symbols . all of these approaches have some limitation . in this paper we propose a class of programs called fp2 that enjoys a different trade-off between expressiveness and complexity . fp2 programs enjoy the following unique combination of properties : ( i ) the ability of expressing predicates with infinite extensions ; ( ii ) full support for predicates with arbitrary arity ; ( iii ) decidability of fp2 membership checking ; ( iv ) decidability of skeptical and credulous stable model reasoning for call-safe queries . odd cycles are supported by composing fp2 programs with argument restricted programs ."}
{"title": "classification of ordinal data", "abstract": "classification of ordinal data is one of the most important tasks of relation learning . in this thesis a novel framework for ordered classes is proposed . the technique reduces the problem of classifying ordered classes to the standard two-class problem . the introduced method is then mapped into support vector machines and neural networks . compared with a well-known approach using pairwise objects as training samples , the new algorithm has a reduced complexity and training time . a second novel model , the unimodal model , is also introduced and a parametric version is mapped into neural networks . several case studies are presented to assert the validity of the proposed models ."}
{"title": "relaxing exclusive control in boolean games", "abstract": "in the typical framework for boolean games ( bg ) each player can change the truth value of some propositional atoms , while attempting to make her goal true . in standard bg goals are propositional formulas , whereas in iterated bg goals are formulas of linear temporal logic . both notions of bg are characterised by the fact that agents have exclusive control over their set of atoms , meaning that no two agents can control the same atom . in the present contribution we drop the exclusivity assumption and explore structures where an atom can be controlled by multiple agents . we introduce concurrent game structures with shared propositional control ( cgs-spc ) and show that they ac- count for several classes of repeated games , including iterated boolean games , influence games , and aggregation games . our main result shows that , as far as verification is concerned , cgs-spc can be reduced to concurrent game structures with exclusive control . this result provides a polynomial reduction for the model checking problem of specifications in alternating-time temporal logic on cgs-spc ."}
{"title": "map-aided fusion using evidential grids for mobile perception in urban environment", "abstract": "evidential grids have been recently used for mobile object perception . the novelty of this article is to propose a perception scheme using prior map knowledge . a geographic map is considered an additional source of information fused with a grid representing sensor data . yager 's rule is adapted to exploit the dempster-shafer conflict information at large . in order to distinguish stationary and mobile objects , a counter is introduced and used as a factor for mass function specialisation . contextual discounting is used , since we assume that different pieces of information become obsolete at different rates . tests on real-world data are also presented ."}
{"title": "david : influence diagram processing system for the macintosh", "abstract": "influence diagrams are a directed graph representation for uncertainties as probabilities . the graph distinguishes between those variables which are under the control of a decision maker ( decisions , shown as rectangles ) and those which are not ( chances , shown as ovals ) , as well as explicitly denoting a goal for solution ( value , shown as a rounded rectangle ."}
{"title": "semantic entity retrieval toolkit", "abstract": "unsupervised learning of low-dimensional , semantic representations of words and entities has recently gained attention . in this paper we describe the semantic entity retrieval toolkit ( sert ) that provides implementations of our previously published entity representation models . the toolkit provides a unified interface to different representation learning algorithms , fine-grained parsing configuration and can be used transparently with gpus . in addition , users can easily modify existing models or implement their own models in the framework . after model training , sert can be used to rank entities according to a textual query and extract the learned entity/word representation for use in downstream algorithms , such as clustering or recommendation ."}
{"title": "learning symbolic models of stochastic domains", "abstract": "in this article , we work towards the goal of developing agents that can learn to act in complex worlds . we develop a probabilistic , relational planning rule representation that compactly models noisy , nondeterministic action effects , and show how such rules can be effectively learned . through experiments in simple planning domains and a 3d simulated blocks world with realistic physics , we demonstrate that this learning algorithm allows agents to effectively model world dynamics ."}
{"title": "revisiting unreasonable effectiveness of data in deep learning era", "abstract": "the success of deep learning in vision can be attributed to : ( a ) models with high capacity ; ( b ) increased computational power ; and ( c ) availability of large-scale labeled data . since 2012 , there have been significant advances in representation capabilities of the models and computational capabilities of gpus . but the size of the biggest dataset has surprisingly remained constant . what will happen if we increase the dataset size by 10x or 100x ? this paper takes a step towards clearing the clouds of mystery surrounding the relationship between ` enormous data ' and visual deep learning . by exploiting the jft-300m dataset which has more than 375m noisy labels for 300m images , we investigate how the performance of current vision tasks would change if this data was used for representation learning . our paper delivers some surprising ( and some expected ) findings . first , we find that the performance on vision tasks increases logarithmically based on volume of training data size . second , we show that representation learning ( or pre-training ) still holds a lot of promise . one can improve performance on many vision tasks by just training a better base model . finally , as expected , we present new state-of-the-art results for different vision tasks including image classification , object detection , semantic segmentation and human pose estimation . our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets ."}
{"title": "a literature based approach to define the scope of biomedical ontologies : a case study on a rehabilitation therapy ontology", "abstract": "in this article , we investigate our early attempts at building an ontology describing rehabilitation therapies following brain injury . these therapies are wide-ranging , involving interventions of many different kinds . as a result , these therapies are hard to describe . as well as restricting actual practice , this is also a major impediment to evidence-based medicine as it is hard to meaningfully compare two treatment plans . ontology development requires significant effort from both ontologists and domain experts . knowledge elicited from domain experts forms the scope of the ontology . the process of knowledge elicitation is expensive , consumes experts ' time and might have biases depending on the selection of the experts . various methodologies and techniques exist for enabling this knowledge elicitation , including community groups and open development practices . a related problem is that of defining scope . by defining the scope , we can decide whether a concept ( i.e . term ) should be represented in the ontology . this is the opposite of knowledge elicitation , in the sense that it defines what should not be in the ontology . this can be addressed by pre-defining a set of competency questions . these approaches are , however , expensive and time-consuming . here , we describe our work toward an alternative approach , bootstrapping the ontology from an initially small corpus of literature that will define the scope of the ontology , expanding this to a set covering the domain , then using information extraction to define an initial terminology to provide the basis and the competencies for the ontology . here , we discuss four approaches to building a suitable corpus that is both sufficiently covering and precise ."}
{"title": "android malware characterization using metadata and machine learning techniques", "abstract": "android malware has emerged as a consequence of the increasing popularity of smartphones and tablets . while most previous work focuses on inherent characteristics of android apps to detect malware , this study analyses indirect features and meta-data to identify patterns in malware applications . our experiments show that : ( 1 ) the permissions used by an application offer only moderate performance results ; ( 2 ) other features publicly available at android markets are more relevant in detecting malware , such as the application developer and certificate issuer , and ( 3 ) compact and efficient classifiers can be constructed for the early detection of malware applications prior to code inspection or sandboxing ."}
{"title": "analyzing games with ambiguous player types using the $ { \\rm minthenmax } $ decision model", "abstract": "in many common interactive scenarios , participants lack information about other participants , and specifically about the preferences of other participants . in this work , we model an extreme case of incomplete information , which we term games with type ambiguity , where a participant lacks even information enabling him to form a belief on the preferences of others . under type ambiguity , one can not analyze the scenario using the commonly used bayesian framework , and therefore he needs to model the participants using a different decision model . in this work , we present the $ { \\rm minthenmax } $ decision model under ambiguity . this model is a refinement of wald 's minimax principle , which we show to be too coarse for games with type ambiguity . we characterize $ { \\rm minthenmax } $ as the finest refinement of the minimax principle that satisfies three properties we claim are necessary for games with type ambiguity . this prior-less approach we present her also follows the common practice in computer science of worst-case analysis . finally , we define and analyze the corresponding equilibrium concept assuming all players follow $ { \\rm minthenmax } $ . we demonstrate this equilibrium by applying it to two common economic scenarios : coordination games and bilateral trade . we show that in both scenarios , an equilibrium in pure strategies always exists and we analyze the equilibria ."}
{"title": "penalty logic and its link with dempster-shafer theory", "abstract": "penalty logic , introduced by pinkas , associates to each formula of a knowledge base the price to pay if this formula is violated . penalties may be used as a criterion for selecting preferred consistent subsets in an inconsistent knowledge base , thus inducing a non-monotonic inference relation . a precise formalization and the main properties of penalty logic and of its associated non-monotonic inference relation are given in the first part . we also show that penalty logic and dempster-shafer theory are related , especially in the infinitesimal case ."}
{"title": "from imprecise probability assessments to conditional probabilities with quasi additive classes of conditioning events", "abstract": "in this paper , starting from a generalized coherent ( i.e . avoiding uniform loss ) intervalvalued probability assessment on a finite family of conditional events , we construct conditional probabilities with quasi additive classes of conditioning events which are consistent with the given initial assessment . quasi additivity assures coherence for the obtained conditional probabilities . in order to reach our goal we define a finite sequence of conditional probabilities by exploiting some theoretical results on g-coherence . in particular , we use solutions of a finite sequence of linear systems ."}
{"title": "mining smart card data for travelers ' mini activities", "abstract": "in the context of public transport modeling and simulation , we address the problem of mismatch between simulated transit trips and observed ones . we point to the weakness of the current travel demand modeling process ; the trips it generates are over-optimistic and do not reflect the real passenger choices . we introduce the notion of mini activities the travelers do during the trips ; they can explain the deviation of simulated trips from the observed trips . we propose to mine the smart card data to extract the mini activities . we develop a technique to integrate them in the generated trips and learn such an integration from two available sources , the trip history and trip planner recommendations . for an input travel demand , we build a markov chain over the trip collection and apply the monte carlo markov chain algorithm to integrate mini activities in such a way that the selected characteristics converge to the desired distributions . we test our method in different settings on the passenger trip collection of nancy , france . we report experimental results demonstrating a very important mismatch reduction ."}
{"title": "making life better one large system at a time : challenges for uai research", "abstract": "the rapid growth and diversity in service offerings and the ensuing complexity of information technology ecosystems present numerous management challenges ( both operational and strategic ) . instrumentation and measurement technology is , by and large , keeping pace with this development and growth . however , the algorithms , tools , and technology required to transform the data into relevant information for decision making are not . the claim in this paper ( and the invited talk ) is that the line of research conducted in uncertainty in artificial intelligence is very well suited to address the challenges and close this gap . i will support this claim and discuss open problems using recent examples in diagnosis , model discovery , and policy optimization on three real life distributed systems ."}
{"title": "developing embodied multisensory dialogue agents", "abstract": "a few decades of work in the ai field have focused efforts on developing a new generation of systems which can acquire knowledge via interaction with the world . yet , until very recently , most such attempts were underpinned by research which predominantly regarded linguistic phenomena as separated from the brain and body . this could lead one into believing that to emulate linguistic behaviour , it suffices to develop 'software ' operating on abstract representations that will work on any computational machine . this picture is inaccurate for several reasons , which are elucidated in this paper and extend beyond sensorimotor and semantic resonance . beginning with a review of research , i list several heterogeneous arguments against disembodied language , in an attempt to draw conclusions for developing embodied multisensory agents which communicate verbally and non-verbally with their environment . without taking into account both the architecture of the human brain , and embodiment , it is unrealistic to replicate accurately the processes which take place during language acquisition , comprehension , production , or during non-linguistic actions . while robots are far from isomorphic with humans , they could benefit from strengthened associative connections in the optimization of their processes and their reactivity and sensitivity to environmental stimuli , and in situated human-machine interaction . the concept of multisensory integration should be extended to cover linguistic input and the complementary information combined from temporally coincident sensory impressions ."}
{"title": "a consumer bci for automated music evaluation within a popular on-demand music streaming service - taking listener 's brainwaves to extremes", "abstract": "we investigated the possibility of using a machine-learning scheme in conjunction with commercial wearable eeg-devices for translating listener 's subjective experience of music into scores that can be used for the automated annotation of music in popular on-demand streaming services . based on the established -neuroscientifically sound- concepts of brainwave frequency bands , activation asymmetry index and cross-frequency-coupling ( cfc ) , we introduce a brain computer interface ( bci ) system that automatically assigns a rating score to the listened song . our research operated in two distinct stages : i ) a generic feature engineering stage , in which features from signal-analytics were ranked and selected based on their ability to associate music induced perturbations in brainwaves with listener 's appraisal of music . ii ) a personalization stage , during which the efficiency of ex- treme learning machines ( elms ) is exploited so as to translate the derived pat- terns into a listener 's score . encouraging experimental results , from a pragmatic use of the system , are presented ."}
{"title": "self corrective perturbations for semantic segmentation and classification", "abstract": "convolutional neural networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems . however , the behavior of deep networks is yet to be fully understood and is still an active area of research . in this work , we present an intriguing behavior : pre-trained cnns can be made to improve their predictions by structurally perturbing the input . we observe that these perturbations - referred as guided perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights . we perform various ablative experiments to understand how these perturbations affect the local context and feature representations . furthermore , we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the pascal voc dataset and supervised classification tasks on mnist and cifar10 datasets ."}
{"title": "fuzzy inference system for integrated vvc in isolated power systems", "abstract": "this paper presents a fuzzy inference system for integrated volt/var control ( vvc ) in distribution substations . the purpose is go forward to automation distribution applying conservation voltage reduction ( cvr ) in isolated power systems where control capabilities are limited . a fuzzy controller has been designed . working as an on-line tool , it has been tested under real conditions and it has managed the operation during a whole day in a distribution substation . within the limits of control capabilities of the system , the controller maintained successfully an acceptable voltage profile , power factor values over 0,98 and it has ostensibly improved the performance given by an optimal power flow based automation system . cvr savings during the test are evaluated and the aim to integrate it in the vvc is presented ."}
{"title": "planning with incomplete information", "abstract": "planning is a natural domain of application for frameworks of reasoning about actions and change . in this paper we study how one such framework , the language e , can form the basis for planning under ( possibly ) incomplete information . we define two types of plans : weak and safe plans , and propose a planner , called the e-planner , which is often able to extend an initial weak plan into a safe plan even though the ( explicit ) information available is incomplete , e.g . for cases where the initial state is not completely known . the e-planner is based upon a reformulation of the language e in argumentation terms and a natural proof theory resulting from the reformulation . it uses an extension of this proof theory by means of abduction for the generation of plans and adopts argumentation-based techniques for extending weak plans into safe plans . we provide representative examples illustrating the behaviour of the e-planner , in particular for cases where the status of fluents is incompletely known ."}
{"title": "formal analysis of htm spatial pooler performance under predefined operation conditions", "abstract": "this paper introduces mathematical formalism for spatial ( sp ) of hierarchical temporal memory ( htm ) with a spacial consideration for its hardware implementation . performance of htm network and its ability to learn and adjust to a problem at hand is governed by a large set of parameters . most of parameters are codependent which makes creating efficient htm-based solutions challenging . it requires profound knowledge of the settings and their impact on the performance of system . consequently , this paper introduced a set of formulas which are to facilitate the design process by enhancing tedious trial-and-error method with a tool for choosing initial parameters which enable quick learning convergence . this is especially important in hardware implementations which are constrained by the limited resources of a platform . the authors focused especially on a formalism of spatial pooler and derive at the formulas for quality and convergence of the model . this may be considered as recipes for designing efficient htm models for given input patterns ."}
{"title": "quip - a tool for computing nonmonotonic reasoning tasks", "abstract": "in this paper , we outline the prototype of an automated inference tool , called quip , which provides a uniform implementation for several nonmonotonic reasoning formalisms . the theoretical basis of quip is derived from well-known results about the computational complexity of nonmonotonic logics and exploits a representation of the different reasoning tasks in terms of quantified boolean formulae ."}
{"title": "a labelling framework for probabilistic argumentation", "abstract": "the combination of argumentation and probability paves the way to new accounts of qualitative and quantitative uncertainty , thereby offering new theoretical and applicative opportunities . due to a variety of interests , probabilistic argumentation is approached in the literature with different frameworks , pertaining to structured and abstract argumentation , and with respect to diverse types of uncertainty , in particular the uncertainty on the credibility of the premises , the uncertainty about which arguments to consider , and the uncertainty on the acceptance status of arguments or statements . towards a general framework for probabilistic argumentation , we investigate a labelling-oriented framework encompassing a basic setting for rule-based argumentation and its ( semi- ) abstract account , along with diverse types of uncertainty . our framework provides a systematic treatment of various kinds of uncertainty and of their relationships and allows us to retrieve ( by derivation ) multiple statements ( sometimes assumed ) or results from the literature ."}
{"title": "a minimum description length approach to multitask feature selection", "abstract": "many regression problems involve not one but several response variables ( y 's ) . often the responses are suspected to share a common underlying structure , in which case it may be advantageous to share information across them ; this is known as multitask learning . as a special case , we can use multiple responses to better identify shared predictive features -- a project we might call multitask feature selection . this thesis is organized as follows . section 1 introduces feature selection for regression , focusing on ell_0 regularization methods and their interpretation within a minimum description length ( mdl ) framework . section 2 proposes a novel extension of mdl feature selection to the multitask setting . the approach , called the `` multiple inclusion criterion '' ( mic ) , is designed to borrow information across regression tasks by more easily selecting features that are associated with multiple responses . we show in experiments on synthetic and real biological data sets that mic can reduce prediction error in settings where features are at least partially shared across responses . section 3 surveys hypothesis testing by regression with a single response , focusing on the parallel between the standard bonferroni correction and an mdl approach . mirroring the ideas in section 2 , section 4 proposes a novel mic approach to hypothesis testing with multiple responses and shows that on synthetic data with significant sharing of features across responses , mic sometimes outperforms standard fdr-controlling methods in terms of finding true positives for a given level of false positives . section 5 concludes ."}
{"title": "pix2code : generating code from a graphical user interface screenshot", "abstract": "transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software , websites , and mobile applications . in this paper , we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77 % of accuracy for three different platforms ( i.e . ios , android and web-based technologies ) ."}
{"title": "the consciousness prior", "abstract": "a new prior is proposed for representation learning , which can be combined with other priors in order to help disentangling abstract factors from each other . it is inspired by the phenomenon of consciousness seen as the formation of a low-dimensional combination of a few concepts constituting a conscious thought , i.e. , consciousness as awareness at a particular time instant . this provides a powerful constraint on the representation in that such low-dimensional thought vectors can correspond to statements about reality which are true , highly probable , or very useful for taking decisions . the fact that a few elements of the current state can be combined into such a predictive or useful statement is a strong constraint and deviates considerably from the maximum likelihood approaches to modelling data and how states unfold in the future based on an agent 's actions . instead of making predictions in the sensory ( e.g . pixel ) space , the consciousness prior allows the agent to make predictions in the abstract space , with only a few dimensions of that space being involved in each of these predictions . the consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical ai knowledge in the form of facts and rules , although the conscious states may be richer than what can be expressed easily in the form of a sentence , a fact or a rule ."}
{"title": "managing sparsity , time , and quality of inference in topic models", "abstract": "inference is an integral part of probabilistic topic models , but is often non-trivial to derive an efficient algorithm for a specific model . it is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents . in this article , we introduce a simple framework for inference in probabilistic topic models , denoted by fw . this framework is general and flexible enough to be easily adapted to mixture models . it has a linear convergence rate , offers an easy way to incorporate prior knowledge , and provides us an easy way to directly trade off sparsity against quality and time . we demonstrate the goodness and flexibility of fw over existing inference methods by a number of tasks . finally , we show how inference in topic models with nonconjugate priors can be done efficiently ."}
{"title": "rule writing or annotation : cost-efficient resource usage for base noun phrase chunking", "abstract": "this paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker : human rule writing and active learning using interactive real-time human annotation . several novel variations on active learning are investigated , and underlying cost models for cross-modal machine learning comparison are presented and explored . results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment ."}
{"title": "eclectic extraction of propositional rules from neural networks", "abstract": "artificial neural network is among the most popular algorithm for supervised learning . however , neural networks have a well-known drawback of being a `` black box '' learner that is not comprehensible to the users . this lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision . rule extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained network . many such extraction algorithms have been developed over the years with their respective strengths and weaknesses . they have been broadly categorized into three types based on their approach to use internal model of the network . eclectic methods are hybrid algorithms that combine the other approaches to attain more performance . in this paper , we present an eclectic method called heretic . our algorithm uses inductive decision tree learning combined with information of the neural network structure for extracting logical rules . experiments and theoretical analysis show heretic to be better in terms of speed and performance ."}
{"title": "deep optimization for spectrum repacking", "abstract": "over 13 months in 2016-17 the fcc conducted an `` incentive auction '' to repurpose radio spectrum from broadcast television to wireless internet . in the end , the auction yielded $ 19.8 billion , $ 10.05 billion of which was paid to 175 broadcasters for voluntarily relinquishing their licenses across 14 uhf channels . stations that continued broadcasting were assigned potentially new channels to fit as densely as possible into the channels that remained . the government netted more than $ 7 billion ( used to pay down the national debt ) after covering costs . a crucial element of the auction design was the construction of a solver , dubbed satfc , that determined whether sets of stations could be `` repacked '' in this way ; it needed to run every time a station was given a price quote . this paper describes the process by which we built satfc . we adopted an approach we dub `` deep optimization '' , taking a data-driven , highly parametric , and computationally intensive approach to solver design . more specifically , to build satfc we designed software that could pair both complete and local-search sat-encoded feasibility checking with a wide range of domain-specific techniques . we then used automatic algorithm configuration techniques to construct a portfolio of eight complementary algorithms to be run in parallel , aiming to achieve good performance on instances that arose in proprietary auction simulations . to evaluate the impact of our solver in this paper , we built an open-source reverse auction simulator . we found that within the short time budget required in practice , satfc solved more than 95 % of the problems it encountered . furthermore , the incentive auction paired with satfc produced nearly optimal allocations in a restricted setting and substantially outperformed other alternatives at national scale ."}
{"title": "entropy , neutro-entropy and anti-entropy for neutrosophic information", "abstract": "this approach presents a multi-valued representation of the neutrosophic information . it highlights the link between the bifuzzy information and neutrosophic one . the constructed deca-valued structure shows the neutrosophic information complexity . this deca-valued structure led to construction of two new concepts for the neutrosophic information : neutro-entropy and anti-entropy . these two concepts are added to the two existing : entropy and non-entropy . thus , we obtained the following triad : entropy , neutro-entropy and anti-entropy ."}
{"title": "multi-objective influence diagrams", "abstract": "we describe multi-objective influence diagrams , based on a set of p objectives , where utility values are vectors in rp , and are typically only partially ordered . these can still be solved by a variable elimination algorithm , leading to a set of maximal values of expected utility . if the pareto ordering is used this set can often be prohibitively large . we consider approximate representations of the pareto set based on e-coverings , allowing much larger problems to be solved . in addition , we define a method for incorporating user tradeoffs , which also greatly improves the efficiency ."}
{"title": "comparative concept similarity over minspaces : axiomatisation and tableaux calculus", "abstract": "we study the logic of comparative concept similarity $ \\csl $ introduced by sheremet , tishkovsky , wolter and zakharyaschev to capture a form of qualitative similarity comparison . in this logic we can formulate assertions of the form `` objects a are more similar to b than to c '' . the semantics of this logic is defined by structures equipped by distance functions evaluating the similarity degree of objects . we consider here the particular case of the semantics induced by \\emph { minspaces } , the latter being distance spaces where the minimum of a set of distances always exists . it turns out that the semantics over arbitrary minspaces can be equivalently specified in terms of preferential structures , typical of conditional logics . we first give a direct axiomatisation of this logic over minspaces . we next define a decision procedure in the form of a tableaux calculus . both the calculus and the axiomatisation take advantage of the reformulation of the semantics in terms of preferential structures ."}
{"title": "intelligent biohybrid neurotechnologies : are they really what they claim ?", "abstract": "in the era of intelligent biohybrid neurotechnologies for brain repair , new fanciful terms are appearing in the scientific dictionary to define what has so far been unimaginable . as the emerging neurotechnologies are becoming increasingly polyhedral and sophisticated , should we talk about evolution and rank the intelligence of these devices ?"}
{"title": "classification of artificial intelligence ids for smurf attack", "abstract": "many methods have been developed to secure the network infrastructure and communication over the internet . intrusion detection is a relatively new addition to such techniques . intrusion detection systems ( ids ) are used to find out if someone has intrusion into or is trying to get it the network . one big problem is amount of intrusion which is increasing day by day . we need to know about network attack information using ids , then analysing the effect . due to the nature of idss which are solely signature based , every new intrusion can not be detected ; so it is important to introduce artificial intelligence ( ai ) methods / techniques in ids . introduction of ai necessitates the importance of normalization in intrusions . this work is focused on classification of ai based ids techniques which will help better design intrusion detection systems in the future . we have also proposed a support vector machine for ids to detect smurf attack with much reliable accuracy ."}
{"title": "distributed bayesian piecewise sparse linear models", "abstract": "the importance of interpretability of machine learning models has been increasing due to emerging enterprise predictive analytics , threat of data privacy , accountability of artificial intelligence in society , and so on . piecewise linear models have been actively studied to achieve both accuracy and interpretability . they often produce competitive accuracy against state-of-the-art non-linear methods . in addition , their representations ( i.e. , rule-based segmentation plus sparse linear formula ) are often preferred by domain experts . a disadvantage of such models , however , is high computational cost for simultaneous determinations of the number of `` pieces '' and cardinality of each linear predictor , which has restricted their applicability to middle-scale data sets . this paper proposes a distributed factorized asymptotic bayesian ( fab ) inference of learning piece-wise sparse linear models on distributed memory architectures . the distributed fab inference solves the simultaneous model selection issue without communicating $ o ( n ) $ data where n is the number of training samples and achieves linear scale-out against the number of cpu cores . experimental results demonstrate that the distributed fab inference achieves high prediction accuracy and performance scalability with both synthetic and benchmark data ."}
{"title": "a structured self-attentive sentence embedding", "abstract": "this paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention . instead of using a vector , we use a 2-d matrix to represent the embedding , with each row of the matrix attending on a different part of the sentence . we also propose a self-attention mechanism and a special regularization term for the model . as a side effect , the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding . we evaluate our model on 3 different tasks : author profiling , sentiment classification , and textual entailment . results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks ."}
{"title": "tractable inference for complex stochastic processes", "abstract": "the monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory . in the case of a stochastic system , these tasks typically involve the use of a belief state- a probability distribution over the state of the process at a given point in time . unfortunately , the state spaces of complex processes are very large , making an explicit representation of a belief state intractable . even in dynamic bayesian networks ( dbns ) , where the process itself can be represented compactly , the representation of the belief state is intractable . we investigate the idea of maintaining a compact approximation to the true belief state , and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant . we show that the error in a belief state contracts exponentially as the process evolves . thus , even with multiple approximations , the error in our process remains bounded indefinitely . we show how the additional structure of a dbn can be used to design our approximation scheme , improving its performance significantly . we demonstrate the applicability of our ideas in the context of a monitoring task , showing that orders of magnitude faster inference can be achieved with only a small degradation in accuracy ."}
{"title": "hi , how can i help you ? : automating enterprise it support help desks", "abstract": "question answering is one of the primary challenges of natural language understanding . in realizing such a system , providing complex long answers to questions is a challenging task as opposed to factoid answering as the former needs context disambiguation . the different methods explored in the literature can be broadly classified into three categories namely : 1 ) classification based , 2 ) knowledge graph based and 3 ) retrieval based . individually , none of them address the need of an enterprise wide assistance system for an it support and maintenance domain . in this domain the variance of answers is large ranging from factoid to structured operating procedures ; the knowledge is present across heterogeneous data sources like application specific documentation , ticket management systems and any single technique for a general purpose assistance is unable to scale for such a landscape . to address this , we have built a cognitive platform with capabilities adopted for this domain . further , we have built a general purpose question answering system leveraging the platform that can be instantiated for multiple products , technologies in the support domain . the system uses a novel hybrid answering model that orchestrates across a deep learning classifier , a knowledge graph based context disambiguation module and a sophisticated bag-of-words search system . this orchestration performs context switching for a provided question and also does a smooth hand-off of the question to a human expert if none of the automated techniques can provide a confident answer . this system has been deployed across 675 internal enterprise it support and maintenance projects ."}
{"title": "learning convolutional neural networks for graphs", "abstract": "numerous important problems can be framed as learning from graph data . we propose a framework for learning convolutional neural networks for arbitrary graphs . these graphs may be undirected , directed , and with both discrete and continuous node and edge attributes . analogous to image-based convolutional networks that operate on locally connected regions of the input , we present a general approach to extracting locally connected regions from graphs . using established benchmark data sets , we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient ."}
{"title": "system-generated requests for rewriting proposals", "abstract": "we present an online deliberation system using mutual evaluation in order to collaboratively develop solutions . participants submit their proposals and evaluate each other 's proposals ; some of them may then be invited by the system to rewrite 'problematic ' proposals . two cases are discussed : a proposal supported by many , but not by a given person , who is then invited to rewrite it for making yet more acceptable ; and a poorly presented but presumably interesting proposal . the first of these cases has been successfully implemented . proposals are evaluated along two axes-understandability ( or clarity , or , more generally , quality ) , and agreement . the latter is used by the system to cluster proposals according to their ideas , while the former is used both to present the best proposals on top of their clusters , and to find poorly written proposals candidates for rewriting . these functionalities may be considered as important components of a large scale online deliberation system ."}
{"title": "on the problem of computing the well-founded semantics", "abstract": "the well-founded semantics is one of the most widely studied and used semantics of logic programs with negation . in the case of finite propositional programs , it can be computed in polynomial time , more specifically , in o ( |at ( p ) |size ( p ) ) steps , where size ( p ) denotes the total number of occurrences of atoms in a logic program p. this bound is achieved by an algorithm introduced by van gelder and known as the alternating-fixpoint algorithm . improving on the alternating-fixpoint algorithm turned out to be difficult . in this paper we study extensions and modifications of the alternating-fixpoint approach . we then restrict our attention to the class of programs whose rules have no more than one positive occurrence of an atom in their bodies . for programs in that class we propose a new implementation of the alternating-fixpoint method in which false atoms are computed in a top-down fashion . we show that our algorithm is faster than other known algorithms and that for a wide class of programs it is linear and so , asymptotically optimal ."}
{"title": "planning by prioritized sweeping with small backups", "abstract": "efficient planning plays a crucial role in model-based reinforcement learning . traditionally , the main planning operation is a full backup based on the current estimates of the successor states . consequently , its computation time is proportional to the number of successor states . in this paper , we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states . this new backup , which we call a small backup , opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods . we empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations ."}
{"title": "space-contained conflict revision , for geographic information", "abstract": "using qualitative reasoning with geographic information , contrarily , for instance , with robotics , looks not only fastidious ( i.e . : encoding knowledge propositional logics pl ) , but appears to be computational complex , and not tractable at all , most of the time . however , knowledge fusion or revision , is a common operation performed when users merge several different data sets in a unique decision making process , without much support . introducing logics would be a great improvement , and we propose in this paper , means for deciding -a priori- if one application can benefit from a complete revision , under only the assumption of a conjecture that we name the `` containment conjecture '' , which limits the size of the minimal conflicts to revise . we demonstrate that this conjecture brings us the interesting computational property of performing a not-provable but global , revision , made of many local revisions , at a tractable size . we illustrate this approach on an application ."}
{"title": "diversity is all you need : learning skills without a reward function", "abstract": "intelligent creatures can explore their environments and learn useful skills without supervision . in this paper , we propose diayn ( `` diversity is all you need '' ) , a method for learning useful skills without a reward function . our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy . on a variety of simulated robotic tasks , we show that this simple objective results in the unsupervised emergence of diverse skills , such as walking and jumping . in a number of reinforcement learning benchmark environments , our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward . in these environments , some of the learned skills correspond to solving the task , and each skill that solves the task does so in a distinct manner . our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning"}
{"title": "a note on darwiche and pearl", "abstract": "it is shown that darwiche and pearl 's postulates imply an interesting property , not noticed by the authors ."}
{"title": "a fuzzy topsis multiple-attribute decision making for scholarship selection", "abstract": "as the education fees are becoming more expensive , more students apply for scholarships . consequently , hundreds and even thousands of applications need to be handled by the sponsor . to solve the problems , some alternatives based on several attributes ( criteria ) need to be selected . in order to make a decision on such fuzzy problems , fuzzy multiple attribute decision making ( fmdam ) can be applied . in this study , unified modeling language ( uml ) in fmadm with topsis and weighted product ( wp ) methods is applied to select the candidates for academic and non-academic scholarships at universitas islam negeri sunan kalijaga . data used were a crisp and fuzzy data . the results show that topsis and weighted product fmadm methods can be used to select the most suitable candidates to receive the scholarships since the preference values applied in this method can show applicants with the highest eligibility"}
{"title": "opentripplanner , openstreetmap , general transit feed specification : tools for disaster relief and recovery", "abstract": "open trip planner was identified as the most promising open source multi-modal trip planning software . open street map , which provides mapping data to open trip planner , is one of the most well-known open source international repository of geographic data . general transit feed specification , which provides transportation data to open trip planner , has been the standard for describing transit systems and platform for numerous applications . together , when used to implement an instance of open trip planner , these software has been helping in traffic decongestion all over the world by assisting commuters to shift from using private transportation modes to public ones . their potential however goes beyond providing multi-modal public transportation routes . this paper aims to first discuss the researchers ' experience in implementing a public transportation route planner for the purpose of traffic decongestion.the researchers would examine the prospective of using the system for disaster preparedness and recovery and concrete ways on how to realize them ."}
{"title": "run , skeleton , run : skeletal model in a physics-based simulation", "abstract": "in this paper , we present our approach to solve a physics-based reinforcement learning challenge `` learning to run '' with objective to train physiologically-based human model to navigate a complex obstacle course as quickly as possible . the environment is computationally expensive , has a high-dimensional continuous action space and is stochastic . we benchmark state of the art policy-gradient methods and test several improvements , such as layer normalization , parameter noise , action and state reflecting , to stabilize training and improve its sample-efficiency . we found that the deep deterministic policy gradient method is the most efficient method for this environment and the improvements we have introduced help to stabilize training . learned models are able to generalize to new physical scenarios , e.g . different obstacle courses ."}
{"title": "genetic algorithms and the art of zen", "abstract": "in this paper we present a novel genetic algorithm ( ga ) solution to a simple yet challenging commercial puzzle game known as the zen puzzle garden ( zpg ) . we describe the game in detail , before presenting a suitable encoding scheme and fitness function for candidate solutions . we then compare the performance of the genetic algorithm with that of the a* algorithm . our results show that the ga is competitive with informed search in terms of solution quality , and significantly out-performs it in terms of computational resource requirements . we conclude with a brief discussion of the implications of our findings for game solving and other `` real world '' problems ."}
{"title": "bayesian inference in model-based machine vision", "abstract": "this is a preliminary version of visual interpretation integrating multiple sensors in successor , an intelligent , model-based vision system . we pursue a thorough integration of hierarchical bayesian inference with comprehensive physical representation of objects and their relations in a system for reasoning with geometry , surface materials and sensor models in machine vision . bayesian inference provides a framework for accruing_ probabilities to rank order hypotheses ."}
{"title": "robust local search for solving rcpsp/max with durational uncertainty", "abstract": "scheduling problems in manufacturing , logistics and project management have frequently been modeled using the framework of resource constrained project scheduling problems with minimum and maximum time lags ( rcpsp/max ) . due to the importance of these problems , providing scalable solution schedules for rcpsp/max problems is a topic of extensive research . however , all existing methods for solving rcpsp/max assume that durations of activities are known with certainty , an assumption that does not hold in real world scheduling problems where unexpected external events such as manpower availability , weather changes , etc . lead to delays or advances in completion of activities . thus , in this paper , our focus is on providing a scalable method for solving rcpsp/max problems with durational uncertainty . to that end , we introduce the robust local search method consisting of three key ideas : ( a ) introducing and studying the properties of two decision rule approximations used to compute start times of activities with respect to dynamic realizations of the durational uncertainty ; ( b ) deriving the expression for robust makespan of an execution strategy based on decision rule approximations ; and ( c ) a robust local search mechanism to efficiently compute activity execution strategies that are robust against durational uncertainty . furthermore , we also provide enhancements to local search that exploit temporal dependencies between activities . our experimental results illustrate that robust local search is able to provide robust execution strategies efficiently ."}
{"title": "laying down the yellow brick road : development of a wizard-of-oz interface for collecting human-robot dialogue", "abstract": "we describe the adaptation and refinement of a graphical user interface designed to facilitate a wizard-of-oz ( woz ) approach to collecting human-robot dialogue data . the data collected will be used to develop a dialogue system for robot navigation . building on an interface previously used in the development of dialogue systems for virtual agents and video playback , we add templates with open parameters which allow the wizard to quickly produce a wide variety of utterances . our research demonstrates that this approach to data collection is viable as an intermediate step in developing a dialogue system for physical robots in remote locations from their users - a domain in which the human and robot need to regularly verify and update a shared understanding of the physical environment . we show that our woz interface and the fixed set of utterances and templates therein provide for a natural pace of dialogue with good coverage of the navigation domain ."}
{"title": "searching for spaceships", "abstract": "we describe software that searches for spaceships in conway 's game of life and related two-dimensional cellular automata . our program searches through a state space related to the de bruijn graph of the automaton , using a method that combines features of breadth first and iterative deepening search , and includes fast bit-parallel graph reachability and path enumeration algorithms for finding the successors of each state . successful results include a new 2c/7 spaceship in life , found by searching a space with 2^126 states ."}
{"title": "exchangeability and sets of desirable gambles", "abstract": "sets of desirable gambles constitute a quite general type of uncertainty model with an interesting geometrical interpretation . we give a general discussion of such models and their rationality criteria . we study exchangeability assessments for them , and prove counterparts of de finetti 's finite and infinite representation theorems . we show that the finite representation in terms of count vectors has a very nice geometrical interpretation , and that the representation in terms of frequency vectors is tied up with multivariate bernstein ( basis ) polynomials . we also lay bare the relationships between the representations of updated exchangeable models , and discuss conservative inference ( natural extension ) under exchangeability and the extension of exchangeable sequences ."}
{"title": "transforming graph representations for statistical relational learning", "abstract": "relational data representations have become an increasingly important topic due to the recent proliferation of network datasets ( e.g. , social , biological , information networks ) and a corresponding increase in the application of statistical relational learning ( srl ) algorithms to these domains . in this article , we examine a range of representation issues for graph-based relational data . since the choice of relational data representation for the nodes , links , and features can dramatically affect the capabilities of srl algorithms , we survey approaches and opportunities for relational representation transformation designed to improve the performance of these algorithms . this leads us to introduce an intuitive taxonomy for data representation transformations in relational domains that incorporates link transformation and node transformation as symmetric representation tasks . in particular , the transformation tasks for both nodes and links include ( i ) predicting their existence , ( ii ) predicting their label or type , ( iii ) estimating their weight or importance , and ( iv ) systematically constructing their relevant features . we motivate our taxonomy through detailed examples and use it to survey and compare competing approaches for each of these tasks . we also discuss general conditions for transforming links , nodes , and features . finally , we highlight challenges that remain to be addressed ."}
{"title": "a differential semantics of lazy ar propagation", "abstract": "in this paper we present a differential semantics of lazy ar propagation ( larp ) in discrete bayesian networks . we describe how both single and multi dimensional partial derivatives of the evidence may easily be calculated from a junction tree in larp equilibrium . we show that the simplicity of the calculations stems from the nature of larp . based on the differential semantics we describe how variable propagation in the larp architecture may give access to additional partial derivatives . the cautious larp ( clarp ) scheme is derived to produce a flexible clarp equilibrium that offers additional opportunities for calculating single and multidimensional partial derivatives of the evidence and subsets of the evidence from a single propagation . the results of an empirical evaluation illustrates how the access to a largely increased number of partial derivatives comes at a low computational cost ."}
{"title": "application of evidential reasoning to helicopter flight path control", "abstract": "this paper presents a methodology for research and development of the inferencing and knowledge representation aspects of an expert system approach for performing reasoning under uncertainty in support of a real time vehicle guidance and navigation system . such a system could be of major benefit for non-terrain following low altitude flight systems operating in foreign hostile environments such as might be experienced by noe helicopter or similar mission craft . an innovative extension of the evidential reasoning methodology , termed the sum-and-lattice-points method , has been developed . the research and development effort presented in this paper consists of a formal mathematical development of the sum-and-lattice-points method , its formulation and representation in a parallel environment , prototype software development of the method within an expert system , and initial testing of the system within the confines of the vehicle guidance system ."}
{"title": "markov chains on orbits of permutation groups", "abstract": "we present a novel approach to detecting and utilizing symmetries in probabilistic graphical models with two main contributions . first , we present a scalable approach to computing generating sets of permutation groups representing the symmetries of graphical models . second , we introduce orbital markov chains , a novel family of markov chains leveraging model symmetries to reduce mixing times . we establish an insightful connection between model symmetries and rapid mixing of orbital markov chains . thus , we present the first lifted mcmc algorithm for probabilistic graphical models . both analytical and empirical results demonstrate the effectiveness and efficiency of the approach ."}
{"title": "demonstration of topological data analysis on a quantum processor", "abstract": "topological data analysis offers a robust way to extract useful information from noisy , unstructured data by identifying its underlying structure . recently , an efficient quantum algorithm was proposed [ lloyd , garnerone , zanardi , nat . commun . 7 , 10138 ( 2016 ) ] for calculating betti numbers of data points -- topological features that count the number of topological holes of various dimensions in a scatterplot . here , we implement a proof-of-principle demonstration of this quantum algorithm by employing a six-photon quantum processor to successfully analyze the topological features of betti numbers of a network including three data points , providing new insights into data analysis in the era of quantum computing ."}
{"title": "heuristics in conflict resolution", "abstract": "modern solvers for boolean satisfiability ( sat ) and answer set programming ( asp ) are based on sophisticated boolean constraint solving techniques . in both areas , conflict-driven learning and related techniques constitute key features whose application is enabled by conflict analysis . although various conflict analysis schemes have been proposed , implemented , and studied both theoretically and practically in the sat area , the heuristic aspects involved in conflict analysis have not yet received much attention . assuming a fixed conflict analysis scheme , we address the open question of how to identify `` good '' reasons for conflicts , and we investigate several heuristics for conflict analysis in asp solving . to our knowledge , a systematic study like ours has not yet been performed in the sat area , thus , it might be beneficial for both the field of asp as well as the one of sat solving ."}
{"title": "design for a darwinian brain : part 1. philosophy and neuroscience", "abstract": "physical symbol systems are needed for open-ended cognition . a good way to understand physical symbol systems is by comparison of thought to chemistry . both have systematicity , productivity and compositionality . the state of the art in cognitive architectures for open-ended cognition is critically assessed . i conclude that a cognitive architecture that evolves symbol structures in the brain is a promising candidate to explain open-ended cognition . part 2 of the paper presents such a cognitive architecture ."}
{"title": "online article ranking as a constrained , dynamic , multi-objective optimization problem", "abstract": "the content ranking problem in a social news website , is typically a function that maximizes a scalar metric of interest like dwell-time . however , like in most real-world applications we are interested in more than one metric -- -for instance simultaneously maximizing click-through rate , monetization metrics , dwell-time -- -and also satisfy the traffic requirements promised to different publishers . all this needs to be done on online data and under the settings where the objective function and the constraints can dynamically change ; this could happen if for instance new publishers are added , some contracts are adjusted , or if some contracts are over . in this paper , we formulate this problem as a constrained , dynamic , multi-objective optimization problem . we propose a novel framework that extends a successful genetic optimization algorithm , nsga-ii , to solve this online , data-driven problem . we design the modules of nsga-ii to suit our problem . we evaluate optimization performance using hypervolume and introduce a confidence interval metric for assessing the practicality of a solution . we demonstrate the application of this framework on a real-world article ranking problem . we observe that we make considerable improvements in both time and performance over a brute-force baseline technique that is currently in production ."}
{"title": "an order of magnitude calculus", "abstract": "this paper develops a simple calculus for order of magnitude reasoning . a semantics is given with soundness and completeness results . order of magnitude probability functions are easily defined and turn out to be equivalent to kappa functions , which are slight generalizations of spohn 's natural conditional functions . the calculus also gives rise to an order of magnitude decision theory , which can be used to justify an amended version of pearl 's decision theory for kappa functions , although the latter is weaker and less expressive ."}
{"title": "an example illustrating the imprecision of the efficient approach for diagnosis of petri nets via integer linear programming", "abstract": "this document demonstrates that the efficient approach for diagnosis of petri nets via integer linear programming may be unable to detect a fault even if the system is diagnosable ."}
{"title": "the logical meaning of expansion", "abstract": "the expansion property considered by researchers in social choice is shown to correspond to a logical property of nonmonotonic consequence relations that is the { \\em pure } , i.e. , not involving connectives , version of a previously known weak rationality condition . the assumption that the union of two definable sets of models is definable is needed for the soundness part of the result ."}
{"title": "nils : a neutrality-based iterated local search and its application to flowshop scheduling", "abstract": "this paper presents a new methodology that exploits specific characteristics from the fitness landscape . in particular , we are interested in the property of neutrality , that deals with the fact that the same fitness value is assigned to numerous solutions from the search space . many combinatorial optimization problems share this property , that is generally very inhibiting for local search algorithms . a neutrality-based iterated local search , that allows neutral walks to move on the plateaus , is proposed and experimented on a permutation flowshop scheduling problem with the aim of minimizing the makespan . our experiments show that the proposed approach is able to find improving solutions compared with a classical iterated local search . moreover , the tradeoff between the exploitation of neutrality and the exploration of new parts of the search space is deeply analyzed ."}
{"title": "introducing partial matching approach in association rules for better treatment of missing values", "abstract": "handling missing values in training datasets for constructing learning models or extracting useful information is considered to be an important research task in data mining and knowledge discovery in databases . in recent years , lot of techniques are proposed for imputing missing values by considering attribute relationships with missing value observation and other observations of training dataset . the main deficiency of such techniques is that , they depend upon single approach and do not combine multiple approaches , that why they are less accurate . to improve the accuracy of missing values imputation , in this paper we introduce a novel partial matching concept in association rules mining , which shows better results as compared to full matching concept that we described in our previous work . our imputation technique combines the partial matching concept in association rules with k-nearest neighbor approach . since this is a hybrid technique , therefore its accuracy is much better than as compared to those techniques which depend upon single approach . to check the efficiency of our technique , we also provide detail experimental results on number of benchmark datasets which show better results as compared to previous approaches ."}
{"title": "epistemological relevance and statistical knowledge", "abstract": "for many years , at least since mccarthy and hayes ( 1969 ) , writers have lamented , and attempted to compensate for , the alleged fact that we often do not have adequate statistical knowledge for governing the uncertainty of belief , for making uncertain inferences , and the like . it is hardly ever spelled out what `` adequate statistical knowledge '' would be , if we had it , and how adequate statistical knowledge could be used to control and regulate epistemic uncertainty ."}
{"title": "the fractal dimension of sat formulas", "abstract": "modern sat solvers have experienced a remarkable progress on solving industrial instances . most of the techniques have been developed after an intensive experimental testing process . recently , there have been some attempts to analyze the structure of these formulas in terms of complex networks , with the long-term aim of explaining the success of these sat solving techniques , and possibly improving them . we study the fractal dimension of sat formulas , and show that most industrial families of formulas are self-similar , with a small fractal dimension . we also show that this dimension is not affected by the addition of learnt clauses . we explore how the dimension of a formula , together with other graph properties can be used to characterize sat instances . finally , we give empirical evidence that these graph properties can be used in state-of-the-art portfolios ."}
{"title": "a behavioural foundation for natural computing and a programmability test", "abstract": "what does it mean to claim that a physical or natural system computes ? one answer , endorsed here , is that computing is about programming a system to behave in different ways . this paper offers an account of what it means for a physical system to compute based on this notion . it proposes a behavioural characterisation of computing in terms of a measure of programmability , which reflects a system 's ability to react to external stimuli . the proposed measure of programmability is useful for classifying computers in terms of the apparent algorithmic complexity of their evolution in time . i make some specific proposals in this connection and discuss this approach in the context of other behavioural approaches , notably turing 's test of machine intelligence . i also anticipate possible objections and consider the applicability of these proposals to the task of relating abstract computation to nature-like computation ."}
{"title": "faith in the algorithm , part 1 : beyond the turing test", "abstract": "since the turing test was first proposed by alan turing in 1950 , the primary goal of artificial intelligence has been predicated on the ability for computers to imitate human behavior . however , the majority of uses for the computer can be said to fall outside the domain of human abilities and it is exactly outside of this domain where computers have demonstrated their greatest contribution to intelligence . another goal for artificial intelligence is one that is not predicated on human mimicry , but instead , on human amplification . this article surveys various systems that contribute to the advancement of human and social intelligence ."}
{"title": "a new penta-valued logic based knowledge representation", "abstract": "in this paper a knowledge representation model are proposed , fp5 , which combine the ideas from fuzzy sets and penta-valued logic . fp5 represents imprecise properties whose accomplished degree is undefined , contradictory or indeterminate for some objects . basic operations of conjunction , disjunction and negation are introduced . relations to other representation models like fuzzy sets , intuitionistic , paraconsistent and bipolar fuzzy sets are discussed ."}
{"title": "janus : automatic ontology builder from xsd files", "abstract": "the construction of a reference ontology for a large domain still remains an hard human task . the process is sometimes assisted by software tools that facilitate the information extraction from a textual corpus . despite of the great use of xml schema files on the internet and especially in the b2b domain , tools that offer a complete semantic analysis of xml schemas are really rare . in this paper we introduce janus , a tool for automatically building a reference knowledge base starting from xml schema files . janus also provides different useful views to simplify b2b application integration ."}
{"title": "dominating manipulations in voting with partial information", "abstract": "we consider manipulation problems when the manipulator only has partial information about the votes of the nonmanipulators . such partial information is described by an information set , which is the set of profiles of the nonmanipulators that are indistinguishable to the manipulator . given such an information set , a dominating manipulation is a non-truthful vote that the manipulator can cast which makes the winner at least as preferable ( and sometimes more preferable ) as the winner when the manipulator votes truthfully . when the manipulator has full information , computing whether or not there exists a dominating manipulation is in p for many common voting rules ( by known results ) . we show that when the manipulator has no information , there is no dominating manipulation for many common voting rules . when the manipulator 's information is represented by partial orders and only a small portion of the preferences are unknown , computing a dominating manipulation is np-hard for many common voting rules . our results thus throw light on whether we can prevent strategic behavior by limiting information about the votes of other voters ."}
{"title": "fullie and wiselie : a dual-stream recurrent convolutional attention model for activity recognition", "abstract": "multimodal features play a key role in wearable sensor based human activity recognition ( har ) . selecting the most salient features adaptively is a promising way to maximize the effectiveness of multimodal sensor data . in this regard , we propose a `` collect fully and select wisely ( fullie and wiselie ) '' principle as well as a dual-stream recurrent convolutional attention model , recurrent attention and activity frame ( raaf ) , to improve the recognition performance . we first collect modality features and the relations between each pair of features to generate activity frames , and then introduce an attention mechanism to select the most prominent regions from activity frames precisely . the selected frames not only maximize the utilization of valid features but also reduce the number of features to be computed effectively . we further analyze the hyper-parameters , accuracy , interpretability , and annotation dependency of the proposed model based on extensive experiments . the results show that raaf achieves competitive performance on two benchmarked datasets and works well in real life scenarios ."}
{"title": "fages ' theorem and answer set programming", "abstract": "we generalize a theorem by francois fages that describes the relationship between the completion semantics and the answer set semantics for logic programs with negation as failure . the study of this relationship is important in connection with the emergence of answer set programming . whenever the two semantics are equivalent , answer sets can be computed by a satisfiability solver , and the use of answer set solvers such as smodels and dlv is unnecessary . a logic programming representation of the blocks world due to ilkka niemelae is discussed as an example ."}
{"title": "harmonization of conflicting medical opinions using argumentation protocols and textual entailment - a case study on parkinson disease", "abstract": "parkinson 's disease is the second most common neurodegenerative disease , affecting more than 1.2 million people in europe . medications are available for the management of its symptoms , but the exact cause of the disease is unknown and there is currently no cure on the market . to better understand the relations between new findings and current medical knowledge , we need tools able to analyse published medical papers based on natural language processing and tools capable to identify various relationships of new findings with the current medical knowledge . our work aims to fill the above technological gap . to identify conflicting information in medical documents , we enact textual entailment technology . to encapsulate existing medical knowledge , we rely on ontologies . to connect the formal axioms in ontologies with natural text in medical articles , we exploit ontology verbalisation techniques . to assess the level of disagreement between human agents with respect to a medical issue , we rely on fuzzy aggregation . to harmonize this disagreement , we design mediation protocols within a multi-agent framework ."}
{"title": "combining linear non-gaussian acyclic model with logistic regression model for estimating causal structure from mixed continuous and discrete data", "abstract": "estimating causal models from observational data is a crucial task in data analysis . for continuous-valued data , shimizu et al . have proposed a linear acyclic non-gaussian model to understand the data generating process , and have shown that their model is identifiable when the number of data is sufficiently large . however , situations in which continuous and discrete variables coexist in the same problem are common in practice . most existing causal discovery methods either ignore the discrete data and apply a continuous-valued algorithm or discretize all the continuous data and then apply a discrete bayesian network approach . these methods possibly loss important information when we ignore discrete data or introduce the approximation error due to discretization . in this paper , we define a novel hybrid causal model which consists of both continuous and discrete variables . the model assumes : ( 1 ) the value of a continuous variable is a linear function of its parent variables plus a non-gaussian noise , and ( 2 ) each discrete variable is a logistic variable whose distribution parameters depend on the values of its parent variables . in addition , we derive the bic scoring function for model selection . the new discovery algorithm can learn causal structures from mixed continuous and discrete data without discretization . we empirically demonstrate the power of our method through thorough simulations ."}
{"title": "protein folding optimization using differential evolution extended with local search and component reinitialization", "abstract": "this paper presents a novel differential evolution algorithm for protein folding optimization that is applied to a three-dimensional ab off-lattice model . the proposed algorithm includes two new mechanisms . a local search is used to improve convergence speed and to reduce the runtime complexity of the energy calculation . for this purpose , a local movement is introduced within the local search . the designed evolutionary algorithm has fast convergence and , therefore , when it is trapped into local optimum or a relatively good solution is located , it is hard to locate a better similar solution . the similar solution is different from the good solution in only a few components . a component reinitialization method is designed to mitigate this problem . both the new mechanisms and the proposed algorithm were analyzed on well-known amino-acid sequences that are used frequently in the literature . experimental results show that the employed new mechanisms improve the efficiency of our algorithm and the proposed algorithm is superior to other state-of-the-art algorithms . it obtained a hit ratio of 100 % for sequences up to 18 monomers within a budget of $ 10^ { 11 } $ solution evaluations . new best-known solutions were obtained for most of the sequences . the existence of the symmetric best-known solutions is also demonstrated in the paper ."}
{"title": "cross-modal recurrent models for weight objective prediction from multimodal time-series data", "abstract": "we analyse multimodal time-series data corresponding to weight , sleep and steps measurements . we focus on predicting whether a user will successfully achieve his/her weight objective . for this , we design several deep long short-term memory ( lstm ) architectures , including a novel cross-modal lstm ( x-lstm ) , and demonstrate their superiority over baseline approaches . the x-lstm improves parameter efficiency by processing each modality separately and allowing for information flow between them by way of recurrent cross-connections . we present a general hyperparameter optimisation technique for x-lstms , which allows us to significantly improve on the lstm and a prior state-of-the-art cross-modal approach , using a comparable number of parameters . finally , we visualise the model 's predictions , revealing implications about latent variables in this task ."}
{"title": "probabilistic planning by probabilistic programming", "abstract": "automated planning is a major topic of research in artificial intelligence , and enjoys a long and distinguished history . the classical paradigm assumes a distinguished initial state , comprised of a set of facts , and is defined over a set of actions which change that state in one way or another . planning in many real-world settings , however , is much more involved : an agent 's knowledge is almost never simply a set of facts that are true , and actions that the agent intends to execute never operate the way they are supposed to . thus , probabilistic planning attempts to incorporate stochastic models directly into the planning process . in this article , we briefly report on probabilistic planning through the lens of probabilistic programming : a programming paradigm that aims to ease the specification of structured probability distributions . in particular , we provide an overview of the features of two systems , hype and allegro , which emphasise different strengths of probabilistic programming that are particularly useful for complex modelling issues raised in probabilistic planning . among other things , with these systems , one can instantiate planning problems with growing and shrinking state spaces , discrete and continuous probability distributions , and non-unique prior distributions in a first-order setting ."}
{"title": "network of bandits insure privacy of end-users", "abstract": "in order to distribute the best arm identification task as close as possible to the user 's devices , on the edge of the radio access network , we propose a new problem setting , where distributed players collaborate to find the best arm . this architecture guarantees privacy to end-users since no events are stored . the only thing that can be observed by an adversary through the core network is aggregated information across users . we provide a first algorithm , distributed median elimination , which is optimal in term of number of transmitted bits and near optimal in term of speed-up factor with respect to an optimal algorithm run independently on each player . in practice , this first algorithm can not handle the trade-off between the communication cost and the speed-up factor , and requires some knowledge about the distribution of players . extended distributed median elimination overcomes these limitations , by playing in parallel different instances of distributed median elimination and selecting the best one . experiments illustrate and complete the analysis . according to the analysis , in comparison to median elimination performed on each player , the proposed algorithm shows significant practical improvements ."}
{"title": "ontology verbalization using semantic-refinement", "abstract": "we propose a rule-based technique to generate redundancy-free nl descriptions of owl entities.the existing approaches which address the problem of verbalizing owl ontologies generate nl text segments which are close to their counterpart owl statements.some of these approaches also perform grouping and aggregating of these nl text segments to generate a more fluent and comprehensive form of the content.restricting our attention to description of individuals and concepts , we find that the approach currently followed in the available tools is that of determining the set of all logical conditions that are satisfied by the given individual/concept name and translate these conditions verbatim into corresponding nl descriptions.human-understandability of such descriptions is affected by the presence of repetitions and redundancies , as they have high fidelity to their owl representation.in the literature , no efforts had been taken to remove redundancies and repetitions at the logical-level before generating the nl descriptions of entities and we find this to be the main reason for lack of readability of the generated text.herein , we propose a technique called semantic-refinement ( sr ) to generate meaningful and easily-understandable descriptions of individuals and concepts of a given owlontology.we identify the combinations of owl/dl constructs that lead to repetitive/redundant descriptions and propose a series of refinement rules to rewrite the conditions that are satisfied by an individual/concept in a meaning-preserving manner.the reduced set of conditions are then employed for generating nl descriptions.our experiments show that , sr leads to significantly improved descriptions of ontology entities.we also test the effectiveness and usefulness of the the generated descriptions for the purpose of validating the ontologies and find that the proposed technique is indeed helpful in the context ."}
{"title": "performing hybrid recommendation in intermodal transportation-the ftmarket system 's recommendation module", "abstract": "diverse recommendation techniques have been already proposed and encapsulated into several e-business applications , aiming to perform a more accurate evaluation of the existing information and accordingly augment the assistance provided to the users involved . this paper reports on the development and integration of a recommendation module in an agent-based transportation transactions management system . the module is built according to a novel hybrid recommendation technique , which combines the advantages of collaborative filtering and knowledge-based approaches . the proposed technique and supporting module assist customers in considering in detail alternative transportation transactions that satisfy their requests , as well as in evaluating completed transactions . the related services are invoked through a software agent that constructs the appropriate knowledge rules and performs a synthesis of the recommendation policy ."}
{"title": "stochastic constraint programming", "abstract": "to model combinatorial decision problems involving uncertainty and probability , we introduce stochastic constraint programming . stochastic constraint programs contain both decision variables ( which we can set ) and stochastic variables ( which follow a probability distribution ) . they combine together the best features of traditional constraint satisfaction , stochastic integer programming , and stochastic satisfiability . we give a semantics for stochastic constraint programs , and propose a number of complete algorithms and approximation procedures . finally , we discuss a number of extensions of stochastic constraint programming to relax various assumptions like the independence between stochastic variables , and compare with other approaches for decision making under uncertainty ."}
{"title": "q-cp : learning action values for cooperative planning", "abstract": "research on multi-robot systems has demonstrated promising results in manifold applications and domains . still , efficiently learning an effective robot behaviors is very difficult , due to unstructured scenarios , high uncertainties , and large state dimensionality ( e.g . hyper-redundant and groups of robot ) . to alleviate this problem , we present q-cp a cooperative model-based reinforcement learning algorithm , which exploits action values to both ( 1 ) guide the exploration of the state space and ( 2 ) generate effective policies . specifically , we exploit q-learning to attack the curse-of-dimensionality in the iterations of a monte-carlo tree search . we implement and evaluate q-cp on different stochastic cooperative ( general-sum ) games : ( 1 ) a simple cooperative navigation problem among 3 robots , ( 2 ) a cooperation scenario between a pair of kuka youbots performing hand-overs , and ( 3 ) a coordination task between two mobile robots entering a door . the obtained results show the effectiveness of q-cp in the chosen applications , where action values drive the exploration and reduce the computational demand of the planning process while achieving good performance ."}
{"title": "raising a hardness result", "abstract": "this article presents a technique for proving problems hard for classes of the polynomial hierarchy or for pspace . the rationale of this technique is that some problem restrictions are able to simulate existential or universal quantifiers . if this is the case , reductions from quantified boolean formulae ( qbf ) to these restrictions can be transformed into reductions from qbfs having one more quantifier in the front . this means that a proof of hardness of a problem at level n in the polynomial hierarchy can be split into n separate proofs , which may be simpler than a proof directly showing a reduction from a class of qbfs to the considered problem ."}
{"title": "direct computation of diagnoses for ontology debugging", "abstract": "modern ontology debugging methods allow efficient identification and localization of faulty axioms defined by a user while developing an ontology . the ontology development process in this case is characterized by rather frequent and regular calls to a reasoner resulting in an early user awareness of modeling errors . in such a scenario an ontology usually includes only a small number of conflict sets , i.e . sets of axioms preserving the faults . this property allows efficient use of standard model-based diagnosis techniques based on the application of hitting set algorithms to a number of given conflict sets . however , in many use cases such as ontology alignment the ontologies might include many more conflict sets than in usual ontology development settings , thus making precomputation of conflict sets and consequently ontology diagnosis infeasible . in this paper we suggest a debugging approach based on a direct computation of diagnoses that omits calculation of conflict sets . embedded in an ontology debugger , the proposed algorithm is able to identify diagnoses for an ontology which includes a large number of faults and for which application of standard diagnosis methods fails . the evaluation results show that the approach is practicable and is able to identify a fault in adequate time ."}
{"title": "marginalization in composed probabilistic models", "abstract": "composition of low-dimensional distributions , whose foundations were laid in the papaer published in the proceeding of uai'97 ( jirousek 1997 ) , appeared to be an alternative apparatus to describe multidimensional probabilistic models . in contrast to graphical markov models , which define multidomensinoal distributions in a declarative way , this approach is rather procedural . ordering of low-dimensional distributions into a proper sequence fully defines the resepctive computational procedure ; therefore , a stury of different type of generating sequences is one fo the central problems in this field . thus , it appears that an important role is played by special sequences that are called perfect . their main characterization theorems are presetned in this paper . however , the main result of this paper is a solution to the problem of margnialization for general sequences . the main theorem describes a way to obtain a generating sequence that defines the model corresponding to the marginal of the distribution defined by an arbitrary genearting sequence . from this theorem the reader can see to what extent these comutations are local ; i.e. , the sequence consists of marginal distributions whose computation must be made by summing up over the values of the variable eliminated ( the paper deals with finite model ) ."}
{"title": "importance sampling in bayesian networks : an influence-based approximation strategy for importance functions", "abstract": "one of the main problems of importance sampling in bayesian networks is representation of the importance function , which should ideally be as close as possible to the posterior joint distribution . typically , we represent an importance function as a factorization , i.e. , product of conditional probability tables ( cpts ) . given diagnostic evidence , we do not have explicit forms for the cpts in the networks . we first derive the exact form for the cpts of the optimal importance function . since the calculation is hard , we usually only use their approximations . we review several popular strategies and point out their limitations . based on an analysis of the influence of evidence , we propose a method for approximating the exact form of importance function by explicitly modeling the most important additional dependence relations introduced by evidence . our experimental results show that the new approximation strategy offers an immediate improvement in the quality of the importance function ."}
{"title": "answering fuzzy conjunctive queries over finitely valued fuzzy ontologies", "abstract": "fuzzy description logics ( dls ) provide a means for representing vague knowledge about an application domain . in this paper , we study fuzzy extensions of conjunctive queries ( cqs ) over the dl $ \\mathcal { sroiq } $ based on finite chains of degrees of truth . to answer such queries , we extend a well-known technique that reduces the fuzzy ontology to a classical one , and use classical dl reasoners as a black box . we improve the complexity of previous reduction techniques for finitely valued fuzzy dls , which allows us to prove tight complexity results for answering certain kinds of fuzzy cqs . we conclude with an experimental evaluation of a prototype implementation , showing the feasibility of our approach ."}
{"title": "quantitative cba : small and comprehensible association rule classification models", "abstract": "quantitative cba is a postprocessing algorithm for association rule classification algorithm cba ( liu et al , 1998 ) . qcba uses original , undiscretized numerical attributes to optimize the discovered association rules , refining the boundaries of literals in the antecedent of the rules produced by cba . some rules as well as literals from the rules can consequently be removed , which makes the resulting classifier smaller . one-rule classification and crisp rules make cba classification models possibly most comprehensible among all association rule classification algorithms . these viable properties are retained by qcba . the postprocessing is conceptually fast , because it is performed on a relatively small number of rules that passed data coverage pruning in cba . benchmark of our qcba approach on 22 uci datasets shows average 53 % decrease in the total size of the model as measured by the total number of conditions in all rules . model accuracy remains on the same level as for cba ."}
{"title": "an empirical study of adequate vision span for attention-based neural machine translation", "abstract": "recently , the attention mechanism plays a key role to achieve high performance for neural machine translation models . however , as it computes a score function for the encoder states in all positions at each decoding step , the attention model greatly increases the computational complexity . in this paper , we investigate the adequate vision span of attention models in the context of machine translation , by proposing a novel attention framework that is capable of reducing redundant score computation dynamically . the term `` vision span '' means a window of the encoder states considered by the attention model in one step . in our experiments , we found that the average window size of vision span can be reduced by over 50 % with modest loss in accuracy on english-japanese and german-english translation tasks. % this results indicate that the conventional attention mechanism performs a significant amount of redundant computation ."}
{"title": "nk landscapes difficulty and negative slope coefficient : how sampling influences the results", "abstract": "negative slope coefficient is an indicator of problem hardness that has been introduced in 2004 and that has returned promising results on a large set of problems . it is based on the concept of fitness cloud and works by partitioning the cloud into a number of bins representing as many different regions of the fitness landscape . the measure is calculated by joining the bins centroids by segments and summing all their negative slopes . in this paper , for the first time , we point out a potential problem of the negative slope coefficient : we study its value for different instances of the well known nk-landscapes and we show how this indicator is dramatically influenced by the minimum number of points contained into a bin . successively , we formally justify this behavior of the negative slope coefficient and we discuss pros and cons of this measure ."}
{"title": "converting cascade-correlation neural nets into probabilistic generative models", "abstract": "humans are not only adept in recognizing what class an input instance belongs to ( i.e. , classification task ) , but perhaps more remarkably , they can imagine ( i.e. , generate ) plausible instances of a desired class with ease , when prompted . inspired by this , we propose a framework which allows transforming cascade-correlation neural networks ( ccnns ) into probabilistic generative models , thereby enabling ccnns to generate samples from a category of interest . ccnns are a well-known class of deterministic , discriminative nns , which autonomously construct their topology , and have been successful in giving accounts for a variety of psychological phenomena . our proposed framework is based on a markov chain monte carlo ( mcmc ) method , called the metropolis-adjusted langevin algorithm , which capitalizes on the gradient information of the target distribution to direct its explorations towards regions of high probability , thereby achieving good mixing properties . through extensive simulations , we demonstrate the efficacy of our proposed framework ."}
{"title": "efficiency analysis of asp encodings for sequential pattern mining tasks", "abstract": "this article presents the use of answer set programming ( asp ) to mine sequential patterns . asp is a high-level declarative logic programming paradigm for high level encoding combinatorial and optimization problem solving as well as knowledge representation and reasoning . thus , asp is a good candidate for implementing pattern mining with background knowledge , which has been a data mining issue for a long time . we propose encodings of the classical sequential pattern mining tasks within two representations of embeddings ( fill-gaps vs skip-gaps ) and for various kinds of patterns : frequent , constrained and condensed . we compare the computational performance of these encodings with each other to get a good insight into the efficiency of asp encodings . the results show that the fill-gaps strategy is better on real problems due to lower memory consumption . finally , compared to a constraint programming approach ( cpsm ) , another declarative programming paradigm , our proposal showed comparable performance ."}
{"title": "analysis of vanilla rolling horizon evolution parameters in general video game playing", "abstract": "monte carlo tree search techniques have generally dominated general video game playing , but recent research has started looking at evolutionary algorithms and their potential at matching tree search level of play or even outperforming these methods . online or rolling horizon evolution is one of the options available to evolve sequences of actions for planning in general video game playing , but no research has been done up to date that explores the capabilities of the vanilla version of this algorithm in multiple games . this study aims to critically analyse the different configurations regarding population size and individual length in a set of 20 games from the general video game ai corpus . distinctions are made between deterministic and stochastic games , and the implications of using superior time budgets are studied . results show that there is scope for the use of these techniques , which in some configurations outperform monte carlo tree search , and also suggest that further research in these methods could boost their performance ."}
{"title": "asymmetric actor critic for image-based robot learning", "abstract": "deep reinforcement learning ( rl ) has proven a powerful technique in many sequential decision making domains . however , robotics poses many challenges for rl , most notably training on a physical system can be expensive and dangerous , which has sparked significant interest in learning control policies using a physics simulator . while several recent works have shown promising results in transferring policies trained in simulation to the real world , they often do not fully utilize the advantage of working with a simulator . in this work , we exploit the full state observability in the simulator to train better policies which take as input only partial observations ( rgbd images ) . we do this by employing an actor-critic training algorithm in which the critic is trained on full states while the actor ( or policy ) gets rendered images as input . we show experimentally on a range of simulated tasks that using these asymmetric inputs significantly improves performance . finally , we combine this method with domain randomization and show real robot experiments for several tasks like picking , pushing , and moving a block . we achieve this simulation to real world transfer without training on any real world data ."}
{"title": "recurrent reinforcement learning : a hybrid approach", "abstract": "successful applications of reinforcement learning in real-world problems often require dealing with partially observable states . it is in general very challenging to construct and infer hidden states as they often depend on the agent 's entire interaction history and may require substantial domain knowledge . in this work , we investigate a deep-learning approach to learning the representation of states in partially observable tasks , with minimal prior knowledge of the domain . in particular , we propose a new family of hybrid models that combines the strength of both supervised learning ( sl ) and reinforcement learning ( rl ) , trained in a joint fashion : the sl component can be a recurrent neural networks ( rnn ) or its long short-term memory ( lstm ) version , which is equipped with the desired property of being able to capture long-term dependency on history , thus providing an effective way of learning the representation of hidden states . the rl component is a deep q-network ( dqn ) that learns to optimize the control for maximizing long-term rewards . extensive experiments in a direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach , which performs the best among a set of previous state-of-the-art methods ."}
{"title": "coping with the limitations of rational inference in the framework of possibility theory", "abstract": "possibility theory offers a framework where both lehmann 's `` preferential inference '' and the more productive ( but less cautious ) `` rational closure inference '' can be represented . however , there are situations where the second inference does not provide expected results either because it can not produce them , or even provide counter-intuitive conclusions . this state of facts is not due to the principle of selecting a unique ordering of interpretations ( which can be encoded by one possibility distribution ) , but rather to the absence of constraints expressing pieces of knowledge we have implicitly in mind . it is advocated in this paper that constraints induced by independence information can help finding the right ordering of interpretations . in particular , independence constraints can be systematically assumed with respect to formulas composed of literals which do not appear in the conditional knowledge base , or for default rules with respect to situations which are `` normal '' according to the other default rules in the base . the notion of independence which is used can be easily expressed in the qualitative setting of possibility theory . moreover , when a counter-intuitive plausible conclusion of a set of defaults , is in its rational closure , but not in its preferential closure , it is always possible to repair the set of defaults so as to produce the desired conclusion ."}
{"title": "k-histograms : an efficient clustering algorithm for categorical dataset", "abstract": "clustering categorical data is an integral part of data mining and has attracted much attention recently . in this paper , we present k-histogram , a new efficient algorithm for clustering categorical data . the k-histogram algorithm extends the k-means algorithm to categorical domain by replacing the means of clusters with histograms , and dynamically updates histograms in the clustering process . experimental results on real datasets show that k-histogram algorithm can produce better clustering results than k-modes algorithm , the one related with our work most closely ."}
{"title": "loide : a web-based ide for logic programming - preliminary technical report", "abstract": "logic-based paradigms are nowadays widely used in many different fields , also thank to the availability of robust tools and systems that allow the development of real-world and industrial applications . in this work we present loide , an advanced and modular web-editor for logic-based languages that also integrates with state-of-the-art solvers ."}
{"title": "maximizing non-monotone/non-submodular functions by multi-objective evolutionary algorithms", "abstract": "evolutionary algorithms ( eas ) are a kind of nature-inspired general-purpose optimization algorithm , and have shown empirically good performance in solving various real-word optimization problems . however , due to the highly randomized and complex behavior , the theoretical analysis of eas is difficult and is an ongoing challenge , which has attracted a lot of research attentions . during the last two decades , promising results on the running time analysis ( one essential theoretical aspect ) of eas have been obtained , while most of them focused on isolated combinatorial optimization problems , which do not reflect the general-purpose nature of eas . to provide a general theoretical explanation of the behavior of eas , it is desirable to study the performance of eas on a general class of combinatorial optimization problems . to the best of our knowledge , this direction has been rarely touched and the only known result is the provably good approximation guarantees of eas for the problem class of maximizing monotone submodular set functions with matroid constraints , which includes many np-hard combinatorial optimization problems . the aim of this work is to contribute to this line of research . as many combinatorial optimization problems also involve non-monotone or non-submodular objective functions , we consider these two general problem classes , maximizing non-monotone submodular functions without constraints and maximizing monotone non-submodular functions with a size constraint . we prove that a simple multi-objective ea called gsemo can generally achieve good approximation guarantees in polynomial expected running time ."}
{"title": "ai-powered social bots", "abstract": "this paper gives an overview of impersonation bots that generate output in one , or possibly , multiple modalities . we also discuss rapidly advancing areas of machine learning and artificial intelligence that could lead to frighteningly powerful new multi-modal social bots . our main conclusion is that most commonly known bots are one dimensional ( i.e. , chatterbot ) , and far from deceiving serious interrogators . however , using recent advances in machine learning , it is possible to unleash incredibly powerful , human-like armies of social bots , in potentially well coordinated campaigns of deception and influence ."}
{"title": "linearity properties of bayes nets with binary variables", "abstract": "it is `` well known '' that in linear models : ( 1 ) testable constraints on the marginal distribution of observed variables distinguish certain cases in which an unobserved cause jointly influences several observed variables ; ( 2 ) the technique of `` instrumental variables '' sometimes permits an estimation of the influence of one variable on another even when the association between the variables may be confounded by unobserved common causes ; ( 3 ) the association ( or conditional probability distribution of one variable given another ) of two variables connected by a path or trek can be computed directly from the parameter values associated with each edge in the path or trek ; ( 4 ) the association of two variables produced by multiple treks can be computed from the parameters associated with each trek ; and ( 5 ) the independence of two variables conditional on a third implies the corresponding independence of the sums of the variables over all units conditional on the sums over all units of each of the original conditioning variables.these properties are exploited in search procedures . it is also known that properties ( 2 ) - ( 5 ) do not hold for all bayes nets with binary variables . we show that ( 1 ) holds for all bayes nets with binary variables and ( 5 ) holds for all singly trek-connected bayes nets of that kind . we further show that all five properties hold for bayes nets with any dag and binary variables parameterized with noisy-or and noisy-and gates ."}
{"title": "large-scale validation of counterfactual learning methods : a test-bed", "abstract": "the ability to perform effective off-policy learning would revolutionize the process of building better interactive systems , such as search engines and recommendation systems for e-commerce , computational advertising and news . recent approaches for off-policy evaluation and learning in these settings appear promising . with this paper , we provide real-world data and a standardized test-bed to systematically investigate these algorithms using data from display advertising . in particular , we consider the problem of filling a banner ad with an aggregate of multiple products the user may want to purchase . this paper presents our test-bed , the sanity checks we ran to ensure its validity , and shows results comparing state-of-the-art off-policy learning methods like doubly robust optimization , poem , and reductions to supervised learning using regression baselines . our results show experimental evidence that recent off-policy learning methods can improve upon state-of-the-art supervised learning techniques on a large-scale real-world data set ."}
{"title": "towards an automated method based on iterated local search optimization for tuning the parameters of support vector machines", "abstract": "we provide preliminary details and formulation of an optimization strategy under current development that is able to automatically tune the parameters of a support vector machine over new datasets . the optimization strategy is a heuristic based on iterated local search , a modification of classic hill climbing which iterates calls to a local search routine ."}
{"title": "modeling epidemic spread in synthetic populations - virtual plagues in massively multiplayer online games", "abstract": "a virtual plague is a process in which a behavior-affecting property spreads among characters in a massively multiplayer online game ( mmog ) . the mmog individuals constitute a synthetic population , and the game can be seen as a form of interactive executable model for studying disease spread , albeit of a very special kind . to a game developer maintaining an mmog , recognizing , monitoring , and ultimately controlling a virtual plague is important , regardless of how it was initiated . the prospect of using tools , methods and theory from the field of epidemiology to do this seems natural and appealing . we will address the feasibility of such a prospect , first by considering some basic measures used in epidemiology , then by pointing out the differences between real world epidemics and virtual plagues . we also suggest directions for mmog developer control through epidemiological modeling . our aim is understanding the properties of virtual plagues , rather than trying to eliminate them or mitigate their effects , as would be in the case of real infectious disease ."}
{"title": "pddl+ planning via constraint answer set programming", "abstract": "pddl+ is an extension of pddl that enables modelling planning domains with mixed discrete-continuous dynamics . in this paper we present a new approach to pddl+ planning based on constraint answer set programming ( casp ) , i.e . asp rules plus numerical constraints . to the best of our knowledge , ours is the first attempt to link pddl+ planning and logic programming . we provide an encoding of pddl+ models into casp problems . the encoding can handle non-linear hybrid domains , and represents a solid basis for applying logic programming to pddl+ planning . as a case study , we consider the ezcsp casp solver and obtain promising results on a set of pddl+ benchmark problems ."}
{"title": "deepalgebra - an outline of a program", "abstract": "we outline a program in the area of formalization of mathematics to automate theorem proving in algebra and algebraic geometry . we propose a construction of a dictionary between automated theorem provers and ( la ) tex exploiting syntactic parsers . we describe its application to a repository of human-written facts and definitions in algebraic geometry ( the stacks project ) . we use deep learning techniques ."}
{"title": "audio adversarial examples : targeted attacks on speech-to-text", "abstract": "we construct targeted audio adversarial examples on automatic speech recognition . given any audio waveform , we can produce another that is over 99.9 % similar , but transcribes as any phrase we choose ( at a rate of up to 50 characters per second ) . we apply our iterative optimization-based attack to mozilla 's implementation deepspeech end-to-end , and show it has a 100 % success rate . the feasibility of this attack introduce a new domain to study adversarial examples ."}
{"title": "towards automated proof strategy generalisation", "abstract": "the ability to automatically generalise ( interactive ) proofs and use such generalisations to discharge related conjectures is a very hard problem which remains unsolved . here , we develop a notion of goal types to capture key properties of goals , which enables abstractions over the specific order and number of sub-goals arising when composing tactics . we show that the goal types form a lattice , and utilise this property in the techniques we develop to automatically generalise proof strategies in order to reuse it for proofs of related conjectures . we illustrate our approach with an example ."}
{"title": "team-maxmin equilibrium : efficiency bounds and algorithms", "abstract": "the team-maxmin equilibrium prescribes the optimal strategies for a team of rational players sharing the same goal and without the capability of correlating their strategies in strategic games against an adversary . this solution concept can capture situations in which an agent controls multiple resources-corresponding to the team members-that can not communicate . it is known that such equilibrium always exists and it is unique ( unless degeneracy ) and these properties make it a credible solution concept to be used in real-world applications , especially in security scenarios . nevertheless , to the best of our knowledge , the team-maxmin equilibrium is almost completely unexplored in the literature . in this paper , we investigate bounds of ( in ) efficiency of the team-maxmin equilibrium w.r.t . the nash equilibria and w.r.t . the maxmin equilibrium when the team members can play correlated strategies . furthermore , we study a number of algorithms to find and/or approximate an equilibrium , discussing their theoretical guarantees and evaluating their performance by using a standard testbed of game instances ."}
{"title": "zaliql : a sql-based framework for drawing causal inference from big data", "abstract": "causal inference from observational data is a subject of active research and development in statistics and computer science . many toolkits have been developed for this purpose that depends on statistical software . however , these toolkits do not scale to large datasets . in this paper we describe a suite of techniques for expressing causal inference tasks from observational data in sql . this suite supports the state-of-the-art methods for causal inference and run at scale within a database engine . in addition , we introduce several optimization techniques that significantly speedup causal inference , both in the online and offline setting . we evaluate the quality and performance of our techniques by experiments of real datasets ."}
{"title": "design of the artificial : lessons from the biological roots of general intelligence", "abstract": "our desire and fascination with intelligent machines dates back to the antiquity 's mythical automaton talos , aristotle 's mode of mechanical thought ( syllogism ) and heron of alexandria 's mechanical machines and automata . however , the quest for artificial general intelligence ( agi ) is troubled with repeated failures of strategies and approaches throughout the history . this decade has seen a shift in interest towards bio-inspired software and hardware , with the assumption that such mimicry entails intelligence . though these steps are fruitful in certain directions and have advanced automation , their singular design focus renders them highly inefficient in achieving agi . which set of requirements have to be met in the design of agi ? what are the limits in the design of the artificial ? here , a careful examination of computation in biological systems hints that evolutionary tinkering of contextual processing of information enabled by a hierarchical architecture is the key to build agi ."}
{"title": "idl-expressions : a formalism for representing and parsing finite languages in natural language processing", "abstract": "we propose a formalism for representation of finite languages , referred to as the class of idl-expressions , which combines concepts that were only considered in isolation in existing formalisms . the suggested applications are in natural language processing , more specifically in surface natural language generation and in machine translation , where a sentence is obtained by first generating a large set of candidate sentences , represented in a compact way , and then by filtering such a set through a parser . we study several formal properties of idl-expressions and compare this new formalism with more standard ones . we also present a novel parsing algorithm for idl-expressions and prove a non-trivial upper bound on its time complexity ."}
{"title": "graphical reasoning in compact closed categories for quantum computation", "abstract": "compact closed categories provide a foundational formalism for a variety of important domains , including quantum computation . these categories have a natural visualisation as a form of graphs . we present a formalism for equational reasoning about such graphs and develop this into a generic proof system with a fixed logical kernel for equational reasoning about compact closed categories . automating this reasoning process is motivated by the slow and error prone nature of manual graph manipulation . a salient feature of our system is that it provides a formal and declarative account of derived results that can include ` ellipses'-style notation . we illustrate the framework by instantiating it for a graphical language of quantum computation and show how this can be used to perform symbolic computation ."}
{"title": "extracting hierarchies of search tasks & subtasks via a bayesian nonparametric approach", "abstract": "a significant amount of search queries originate from some real world information need or tasks . in order to improve the search experience of the end users , it is important to have accurate representations of tasks . as a result , significant amount of research has been devoted to extracting proper representations of tasks in order to enable search systems to help users complete their tasks , as well as providing the end user with better query suggestions , for better recommendations , for satisfaction prediction , and for improved personalization in terms of tasks . most existing task extraction methodologies focus on representing tasks as flat structures . however , tasks often tend to have multiple subtasks associated with them and a more naturalistic representation of tasks would be in terms of a hierarchy , where each task can be composed of multiple ( sub ) tasks . to this end , we propose an efficient bayesian nonparametric model for extracting hierarchies of such tasks \\ & subtasks . we evaluate our method based on real world query log data both through quantitative and crowdsourced experiments and highlight the importance of considering task/subtask hierarchies ."}
{"title": "fast top-k area topics extraction with knowledge base", "abstract": "what are the most popular research topics in artificial intelligence ( ai ) ? we formulate the problem as extracting top- $ k $ topics that can best represent a given area with the help of knowledge base . we theoretically prove that the problem is np-hard and propose an optimization model , fastkate , to address this problem by combining both explicit and latent representations for each topic . we leverage a large-scale knowledge base ( wikipedia ) to generate topic embeddings using neural networks and use this kind of representations to help capture the representativeness of topics for given areas . we develop a fast heuristic algorithm to efficiently solve the problem with a provable error bound . we evaluate the proposed model on three real-world datasets . experimental results demonstrate our model 's effectiveness , robustness , real-timeness ( return results in $ < 1 $ s ) , and its superiority over several alternative methods ."}
{"title": "practical issues in constructing a bayes ' belief network", "abstract": "bayes belief networks and influence diagrams are tools for constructing coherent probabilistic representations of uncertain knowledge . the process of constructing such a network to represent an expert 's knowledge is used to illustrate a variety of techniques which can facilitate the process of structuring and quantifying uncertain relationships . these include some generalizations of the `` noisy or gate '' concept . sensitivity analysis of generic elements of bayes ' networks provides insight into when rough probability assessments are sufficient and when greater precision may be important ."}
{"title": "resolving distributed knowledge", "abstract": "distributed knowledge is the sum of the knowledge in a group ; what someone who is able to discern between two possible worlds whenever any member of the group can discern between them , would know . sometimes distributed knowledge is referred to as the potential knowledge of a group , or the joint knowledge they could obtain if they had unlimited means of communication . in epistemic logic , the formula d_g { \\phi } is intended to express the fact that group g has distributed knowledge of { \\phi } , that there is enough information in the group to infer { \\phi } . but this is not the same as reasoning about what happens if the members of the group share their information . in this paper we introduce an operator r_g , such that r_g { \\phi } means that { \\phi } is true after g have shared all their information with each other - after g 's distributed knowledge has been resolved . the r_g operators are called resolution operators . semantically , we say that an expression r_g { \\phi } is true iff { \\phi } is true in what van benthem [ 11 , p. 249 ] calls ( g 's ) communication core ; the model update obtained by removing links to states for members of g that are not linked by all members of g. we study logics with different combinations of resolution operators and operators for common and distributed knowledge . of particular interest is the relationship between distributed and common knowledge . the main results are sound and complete axiomatizations ."}
{"title": "combining evaluation metrics via the unanimous improvement ratio and its application to clustering tasks", "abstract": "many artificial intelligence tasks can not be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings . a problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings . this paper introduces the unanimous improvement ratio ( uir ) , a measure that complements standard metric combination criteria ( such as van rijsbergen 's f-measure ) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics . uir is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted . besides discussing the theoretical foundations of uir , this paper presents empirical results that confirm the validity and usefulness of the metric for the text clustering problem , where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them . remarkably , our experiments show that uir can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed ."}
{"title": "a comparison of learning algorithms on the arcade learning environment", "abstract": "reinforcement learning agents have traditionally been evaluated on small toy problems . with advances in computing power and the advent of the arcade learning environment , it is now possible to evaluate algorithms on diverse and difficult problems within a consistent framework . we discuss some challenges posed by the arcade learning environment which do not manifest in simpler environments . we then provide a comparison of model-free , linear learning algorithms on this challenging problem set ."}
{"title": "e-commerce in your inbox : product recommendations at scale", "abstract": "in recent years online advertising has become increasingly ubiquitous and effective . advertisements shown to visitors fund sites and apps that publish digital content , manage social networks , and operate e-mail services . given such large variety of internet resources , determining an appropriate type of advertising for a given platform has become critical to financial success . native advertisements , namely ads that are similar in look and feel to content , have had great success in news and social feeds . however , to date there has not been a winning formula for ads in e-mail clients . in this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to yahoo mail users . we propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations , which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence . we conducted rigorous offline testing using a large-scale product purchase data set , covering purchases of more than 29 million users from 172 e-commerce websites . ads in the form of product recommendations were successfully tested on online traffic , where we observed a steady 9 % lift in click-through rates over other ad formats in mail , as well as comparable lift in conversion rates . following successful tests , the system was launched into production during the holiday season of 2014 ."}
{"title": "well-founded semantics for extended logic programs with dynamic preferences", "abstract": "the paper describes an extension of well-founded semantics for logic programs with two types of negation . in this extension information about preferences between rules can be expressed in the logical language and derived dynamically . this is achieved by using a reserved predicate symbol and a naming technique . conflicts among rules are resolved whenever possible on the basis of derived preference information . the well-founded conclusions of prioritized logic programs can be computed in polynomial time . a legal reasoning example illustrates the usefulness of the approach ."}
{"title": "spectral ranking using seriation", "abstract": "we describe a seriation algorithm for ranking a set of items given pairwise comparisons between these items . intuitively , the algorithm assigns similar rankings to items that compare similarly with all others . it does so by constructing a similarity matrix from pairwise comparisons , using seriation methods to reorder this matrix and construct a ranking . we first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order . we then show that ranking reconstruction is still exact when some pairwise comparisons are corrupted or missing , and that seriation based spectral ranking is more robust to noise than classical scoring methods . finally , we bound the ranking error when only a random subset of the comparions are observed . an additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems . experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods ."}
{"title": "decision making for inconsistent expert judgments using negative probabilities", "abstract": "in this paper we provide a simple random-variable example of inconsistent information , and analyze it using three different approaches : bayesian , quantum-like , and negative probabilities . we then show that , at least for this particular example , both the bayesian and the quantum-like approaches have less normative power than the negative probabilities one ."}
{"title": "constraint solvers : an empirical evaluation of design decisions", "abstract": "this paper presents an evaluation of the design decisions made in four state-of-the-art constraint solvers ; choco , eclipse , gecode , and minion . to assess the impact of design decisions , instances of the five problem classes n-queens , golomb ruler , magic square , social golfers , and balanced incomplete block design are modelled and solved with each solver . the results of the experiments are not meant to give an indication of the performance of a solver , but rather investigate what influence the choice of algorithms and data structures has . the analysis of the impact of the design decisions focuses on the different ways of memory management , behaviour with increasing problem size , and specialised algorithms for specific types of variables . it also briefly considers other , less significant decisions ."}
{"title": "reasoning about knowledge and strategies : epistemic strategy logic", "abstract": "in this paper we introduce epistemic strategy logic ( esl ) , an extension of strategy logic with modal operators for individual knowledge . this enhanced framework allows us to represent explicitly and to reason about the knowledge agents have of their own and other agents ' strategies . we provide a semantics to esl in terms of epistemic concurrent game models , and consider the corresponding model checking problem . we show that the complexity of model checking esl is not worse than ( non-epistemic ) strategy logic"}
{"title": "toward large-scale agent guidance in an urban taxi service", "abstract": "empty taxi cruising represents a wastage of resources in the context of urban taxi services . in this work , we seek to minimize such wastage . an analysis of a large trace of taxi operations reveals that the services ' inefficiency is caused by drivers ' greedy cruising behavior . we model the existing system as a continuous time markov chain . to address the problem , we propose that each taxi be equipped with an intelligent agent that will guide the driver when cruising for passengers . then , drawing from ai literature on multiagent planning , we explore two possible ways to compute such guidance . the first formulation assumes fully cooperative drivers . this allows us , in principle , to compute systemwide optimal cruising policy . this is modeled as a markov decision process . the second formulation assumes rational drivers , seeking to maximize their own profit . this is modeled as a stochastic congestion game , a specialization of stochastic games . nash equilibrium policy is proposed as the solution to the game , where no driver has the incentive to singly deviate from it . empirical result shows that both formulations improve the efficiency of the service significantly ."}
{"title": "liftago on-demand transport dataset and market formation algorithm based on machine learning", "abstract": "this document serves as a technical report for the analysis of on-demand transport dataset . moreover we show how the dataset can be used to develop a market formation algorithm based on machine learning . data used in this work comes from liftago , a prague based company which connects taxi drivers and customers through a smartphone app . the dataset is analysed from the machine-learning perspective : we give an overview of features available as well as results of feature ranking . later we propose the simple data-driven market formation ( sidmaf ) algorithm which aims to improve a relevance while connecting customers with relevant drivers . we compare the heuristics currently used by liftago with sidmaf using two key performance indicators ."}
{"title": "kbgan : adversarial learning for knowledge graph embeddings", "abstract": "we introduce kbgan , an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models . because knowledge graphs typically only contain positive facts , sampling useful negative training examples is a non-trivial task . replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts , but the majority of the generated negative facts can be easily discriminated from positive facts , and will contribute little towards the training . inspired by generative adversarial networks ( gans ) , we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model , which acts as the discriminator in gans . this framework is independent of the concrete form of generator and discriminator , and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks . in experiments , we adversarially train two translation-based models , transe and transd , each with assistance from one of the two probability-based models , distmult and complex . we evaluate the performances of kbgan on the link prediction task , using three knowledge base completion datasets : fb15k-237 , wn18 and wn18rr . experimental results show that adversarial training substantially improves the performances of target embedding models under various settings ."}
{"title": "qualitative possibilistic mixed-observable mdps", "abstract": "possibilistic and qualitative pomdps ( pi-pomdps ) are counterparts of pomdps used to model situations where the agent 's initial belief or observation probabilities are imprecise due to lack of past experiences or insufficient data collection . however , like probabilistic pomdps , optimally solving pi-pomdps is intractable : the finite belief state space exponentially grows with the number of system 's states . in this paper , a possibilistic version of mixed-observable mdps is presented to get around this issue : the complexity of solving pi-pomdps , some state variables of which are fully observable , can be then dramatically reduced . a value iteration algorithm for this new formulation under infinite horizon is next proposed and the optimality of the returned policy ( for a specified criterion ) is shown assuming the existence of a `` stay '' action in some goal states . experimental work finally shows that this possibilistic model outperforms probabilistic pomdps commonly used in robotics , for a target recognition problem where the agent 's observations are imprecise ."}
{"title": "decision aids for adversarial planning in military operations : algorithms , tools , and turing-test-like experimental validation", "abstract": "use of intelligent decision aids can help alleviate the challenges of planning complex operations . we describe integrated algorithms , and a tool capable of translating a high-level concept for a tactical military operation into a fully detailed , actionable plan , producing automatically ( or with human guidance ) plans with realistic degree of detail and of human-like quality . tight interleaving of several algorithms -- planning , adversary estimates , scheduling , routing , attrition and consumption estimates -- comprise the computational approach of this tool . although originally developed for army large-unit operations , the technology is generic and also applies to a number of other domains , particularly in critical situations requiring detailed planning within a constrained period of time . in this paper , we focus particularly on the engineering tradeoffs in the design of the tool . in an experimental evaluation , reminiscent of the turing test , the tool 's performance compared favorably with human planners ."}
{"title": "of starships and klingons : bayesian logic for the 23rd century", "abstract": "intelligent systems in an open world must reason about many interacting entities related to each other in diverse ways and having uncertain features and relationships . traditional probabilistic languages lack the expressive power to handle relational domains . classical first-order logic is sufficiently expressive , but lacks a coherent plausible reasoning capability . recent years have seen the emergence of a variety of approaches to integrating first-order logic , probability , and machine learning . this paper presents multi-entity bayesian networks ( mebn ) , a formal system that integrates first order logic ( fol ) with bayesian probability theory . mebn extends ordinary bayesian networks to allow representation of graphical models with repeated sub-structures , and can express a probability distribution over models of any consistent , finitely axiomatizable first-order theory . we present the logic using an example inspired by the paramount series startrek ."}
{"title": "learning to poke by poking : experiential learning of intuitive physics", "abstract": "we investigate an experiential learning paradigm for acquiring an internal model of intuitive physics . our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking . the robot gathered over 400 hours of experience by executing more than 100k pokes on different objects . we propose a novel approach based on deep neural networks for modeling the dynamics of robot 's interactions directly from images , by jointly estimating forward and inverse models of dynamics . the inverse model objective provides supervision to construct informative visual features , which the forward model can then predict and in turn regularize the feature space for the inverse model . the interplay between these two objectives creates useful , accurate models that can then be used for multi-step decision making . this formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels . our experiments show that this joint modeling approach outperforms alternative methods ."}
{"title": "p=np", "abstract": "we claim to resolve the p= ? np problem via a formal argument for p=np ."}
{"title": "integration of knowledge to support automatic object reconstruction from images and 3d data", "abstract": "object reconstruction is an important task in many fields of application as it allows to generate digital representations of our physical world used as base for analysis , planning , construction , visualization or other aims . a reconstruction itself normally is based on reliable data ( images , 3d point clouds for example ) expressing the object in his complete extent . this data then has to be compiled and analyzed in order to extract all necessary geometrical elements , which represent the object and form a digital copy of it . traditional strategies are largely based on manual interaction and interpretation , because with increasing complexity of objects human understanding is inevitable to achieve acceptable and reliable results . but human interaction is time consuming and expensive , why many researches has already been invested to use algorithmic support , what allows to speed up the process and to reduce manual work load . presently most of such supporting algorithms are data-driven and concentate on specific features of the objects , being accessible to numerical models . by means of these models , which normally will represent geometrical ( flatness , roughness , for example ) or physical features ( color , texture ) , the data is classified and analyzed . this is successful for objects with low complexity , but gets to its limits with increasing complexness of objects . then purely numerical strategies are not able to sufficiently model the reality . therefore , the intention of our approach is to take human cognitive strategy as an example , and to simulate extraction processes based on available human defined knowledge for the objects of interest . such processes will introduce a semantic structure for the objects and guide the algorithms used to detect and recognize objects , which will yield a higher effectiveness . hence , our research proposes an approach using knowledge to guide the algorithms in 3d point cloud and image processing ."}
{"title": "syntax , parsing and production of natural language in a framework of information compression by multiple alignment , unification and search", "abstract": "this article introduces the idea that `` information compression by multiple alignment , unification and search '' ( icmaus ) provides a framework within which natural language syntax may be represented in a simple format and the parsing and production of natural language may be performed in a transparent manner . the icmaus concepts are embodied in a software model , sp61 . the organisation and operation of the model are described and a simple example is presented showing how the model can achieve parsing of natural language . notwithstanding the apparent paradox of 'decompression by compression ' , the icmaus framework , without any modification , can produce a sentence by decoding a compressed code for the sentence . this is illustrated with output from the sp61 model . the article includes four other examples - one of the parsing of a sentence in french and three from the domain of english auxiliary verbs . these examples show how the icmaus framework and the sp61 model can accommodate 'context sensitive ' features of syntax in a relatively simple and direct manner ."}
{"title": "a differential approach to inference in bayesian networks", "abstract": "we present a new approach for inference in bayesian networks , which is mainly based on partial differentiation . according to this approach , one compiles a bayesian network into a multivariate polynomial and then computes the partial derivatives of this polynomial with respect to each variable . we show that once such derivatives are made available , one can compute in constant-time answers to a large class of probabilistic queries , which are central to classical inference , parameter estimation , model validation and sensitivity analysis . we present a number of complexity results relating to the compilation of such polynomials and to the computation of their partial derivatives . we argue that the combined simplicity , comprehensiveness and computational complexity of the presented framework is unique among existing frameworks for inference in bayesian networks ."}
{"title": "a logical characterization of constraint-based causal discovery", "abstract": "we present a novel approach to constraint-based causal discovery , that takes the form of straightforward logical inference , applied to a list of simple , logical statements about causal relations that are derived directly from observed ( in ) dependencies . it is both sound and complete , in the sense that all invariant features of the corresponding partial ancestral graph ( pag ) are identified , even in the presence of latent variables and selection bias . the approach shows that every identifiable causal relation corresponds to one of just two fundamental forms . more importantly , as the basic building blocks of the method do not rely on the detailed ( graphical ) structure of the corresponding pag , it opens up a range of new opportunities , including more robust inference , detailed accountability , and application to large models ."}
{"title": "perceptual context in cognitive hierarchies", "abstract": "cognition does not only depend on bottom-up sensor feature abstraction , but also relies on contextual information being passed top-down . context is higher level information that helps to predict belief states at lower levels . the main contribution of this paper is to provide a formalisation of perceptual context and its integration into a new process model for cognitive hierarchies . several simple instantiations of a cognitive hierarchy are used to illustrate the role of context . notably , we demonstrate the use context in a novel approach to visually track the pose of rigid objects with just a 2d camera ."}
{"title": "recommendations for marketing campaigns in telecommunication business based on the footprint analysis", "abstract": "a major investment made by a telecom operator goes into the infrastructure and its maintenance , while business revenues are proportional to how big and good the customer base is . we present a data-driven analytic strategy based on combinatorial optimization and analysis of historical data . the data cover historical mobility of the users in one region of sweden during a week . applying the proposed method to the case study , we have identified the optimal proportion of geo-demographic segments in the customer base , developed a functionality to assess the potential of a planned marketing campaign , and explored the problem of an optimal number and types of the geo-demographic segments to target through marketing campaigns . with the help of fuzzy logic , the conclusions of data analysis are automatically translated into comprehensible recommendations in a natural language ."}
{"title": "extreme extraction : only one hour per relation", "abstract": "information extraction ( ie ) aims to automatically generate a large knowledge base from natural language text , but progress remains slow . supervised learning requires copious human annotation , while unsupervised and weakly supervised approaches do not deliver competitive accuracy . as a result , most fielded applications of ie , as well as the leading tac-kbp systems , rely on significant amounts of manual engineering . even `` extreme '' methods , such as those reported in freedman et al . 2011 , require about 10 hours of expert labor per relation . this paper shows how to reduce that effort by an order of magnitude . we present a novel system , instaread , that streamlines authoring with an ensemble of methods : 1 ) encoding extraction rules in an expressive and compositional representation , 2 ) guiding the user to promising rules based on corpus statistics and mined resources , and 3 ) introducing a new interactive development cycle that provides immediate feedback -- - even on large datasets . experiments show that experts can create quality extractors in under an hour and even nlp novices can author good extractors . these extractors equal or outperform ones obtained by comparably supervised and state-of-the-art distantly supervised approaches ."}
{"title": "rdfviews : a storage tuning wizard for rdf applications", "abstract": "in recent years , the significant growth of rdf data used in numerous applications has made its efficient and scalable manipulation an important issue . in this paper , we present rdfviews , a system capable of choosing the most suitable views to materialize , in order to minimize the query response time for a specific sparql query workload , while taking into account the view maintenance cost and storage space constraints . our system employs practical algorithms and heuristics to navigate through the search space of potential view configurations , and exploits the possibly available semantic information - expressed via an rdf schema - to ensure the completeness of the query evaluation ."}
{"title": "highway and residual networks learn unrolled iterative estimation", "abstract": "the past year saw the introduction of new architectures such as highway networks and residual networks which , for the first time , enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent . while depth of representation has been posited as a primary reason for their success , there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer . in this report , we argue that this view is incomplete and does not adequately explain several recent findings . we propose an alternative viewpoint based on unrolled iterative estimation -- a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation . we demonstrate that this viewpoint directly leads to the construction of highway and residual networks . finally we provide preliminary experiments to discuss the similarities and differences between the two architectures ."}
{"title": "ethical artificial intelligence - an open question", "abstract": "artificial intelligence ( ai ) is an effective science which employs strong enough approaches , methods , and techniques to solve unsolvable real world based problems . because of its unstoppable rise towards the future , there are also some discussions about its ethics and safety . shaping an ai friendly environment for people and a people friendly environment for ai can be a possible answer for finding a shared context of values for both humans and robots . in this context , objective of this paper is to address the ethical issues of ai and explore the moral dilemmas that arise from ethical algorithms , from pre set or acquired values . in addition , the paper will also focus on the subject of ai safety . as general , the paper will briefly analyze the concerns and potential solutions to solving the ethical issues presented and increase readers awareness on ai safety as another related research interest ."}
{"title": "glass-box program synthesis : a machine learning approach", "abstract": "recently proposed models which learn to write computer programs from data use either input/output examples or rich execution traces . instead , we argue that a novel alternative is to use a glass-box loss function , given as a program itself that can be directly inspected . glass-box optimization covers a wide range of problems , from computing the greatest common divisor of two integers , to learning-to-learn problems . in this paper , we present an intelligent search system which learns , given the partial program and the glass-box problem , the probabilities over the space of programs . we empirically demonstrate that our informed search procedure leads to significant improvements compared to brute-force program search , both in terms of accuracy and time . for our experiments we use rich context free grammars inspired by number theory , text processing , and algebra . our results show that ( i ) performing 4 rounds of our framework typically solves about 70 % of the target problems , ( ii ) our framework can improve itself even in domain agnostic scenarios , and ( iii ) it can solve problems that would be otherwise too slow to solve with brute-force search ."}
{"title": "introduction to ross : a new representational scheme", "abstract": "ross ( `` representation , ontology , structure , star '' ) is introduced as a new method for knowledge representation that emphasizes representational constructs for physical structure . the ross representational scheme includes a language called `` star '' for the specification of ontology classes . the ross method also includes a formal scheme called the `` instance model '' . instance models are used in the area of natural language meaning representation to represent situations . this paper provides both the rationale and the philosophical background for the ross method ."}
{"title": "summary - terpret : a probabilistic programming language for program induction", "abstract": "we study machine learning formulations of inductive program synthesis ; that is , given input-output examples , synthesize source code that maps inputs to corresponding outputs . our key contribution is terpret , a domain-specific language for expressing program synthesis problems . a terpret model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs . the inference task is to observe a set of input-output examples and infer the underlying program . from a terpret model we automatically perform inference using four different back-ends : gradient descent ( thus each terpret model can be seen as defining a differentiable interpreter ) , linear program ( lp ) relaxations for graphical models , discrete satisfiability solving , and the sketch program synthesis system . terpret has two main benefits . first , it enables rapid exploration of a range of domains , program representations , and interpreter models . second , it separates the model specification from the inference algorithm , allowing proper comparisons between different approaches to inference . we illustrate the value of terpret by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models . to our knowledge , this is the first work to compare gradient-based search over program space to traditional search-based alternatives . our key empirical finding is that constraint solvers dominate the gradient descent and lp-based formulations . this is a workshop summary of a longer report at arxiv:1608.04428"}
{"title": "domain adaptation with soft-margin multiple feature-kernel learning beats deep learning for surveillance face recognition", "abstract": "face recognition ( fr ) is the most preferred mode for biometric-based surveillance , due to its passive nature of detecting subjects , amongst all different types of biometric traits . fr under surveillance scenario does not give satisfactory performance due to low contrast , noise and poor illumination conditions on probes , as compared to the training samples . a state-of-the-art technology , deep learning , even fails to perform well in these scenarios . we propose a novel soft-margin based learning method for multiple feature-kernel combinations , followed by feature transformed using domain adaptation , which outperforms many recent state-of-the-art techniques , when tested using three real-world surveillance face datasets ."}
{"title": "generalized quantum reinforcement learning with quantum technologies", "abstract": "we propose a protocol to perform generalized quantum reinforcement learning with quantum technologies . at variance with recent results on quantum reinforcement learning with superconducting circuits [ l. lamata , sci . rep. 7 , 1609 ( 2017 ) ] , in our current protocol coherent feedback during the learning process is not required , enabling its implementation in a wide variety of quantum systems . we consider diverse possible scenarios for an agent , an environment , and a register that connects them , involving multiqubit and multilevel systems , as well as open-system dynamics . we finally propose possible implementations of this protocol in trapped ions and superconducting circuits . the field of quantum reinforcement learning with quantum technologies will enable enhanced quantum control , as well as more efficient machine learning calculations ."}
{"title": "learning in the model space for fault diagnosis", "abstract": "the emergence of large scaled sensor networks facilitates the collection of large amounts of real-time data to monitor and control complex engineering systems . however , in many cases the collected data may be incomplete or inconsistent , while the underlying environment may be time-varying or un-formulated . in this paper , we have developed an innovative cognitive fault diagnosis framework that tackles the above challenges . this framework investigates fault diagnosis in the model space instead of in the signal space . learning in the model space is implemented by fitting a series of models using a series of signal segments selected with a rolling window . by investigating the learning techniques in the fitted model space , faulty models can be discriminated from healthy models using one-class learning algorithm . the framework enables us to construct fault library when unknown faults occur , which can be regarded as cognitive fault isolation . this paper also theoretically investigates how to measure the pairwise distance between two models in the model space and incorporates the model distance into the learning algorithm in the model space . the results on three benchmark applications and one simulated model for the barcelona water distribution network have confirmed the effectiveness of the proposed framework ."}
{"title": "solution-guided multi-point constructive search for job shop scheduling", "abstract": "solution-guided multi-point constructive search ( sgmpcs ) is a novel constructive search technique that performs a series of resource-limited tree searches where each search begins either from an empty solution ( as in randomized restart ) or from a solution that has been encountered during the search . a small number of these `` elite solutions is maintained during the search . we introduce the technique and perform three sets of experiments on the job shop scheduling problem . first , a systematic , fully crossed study of sgmpcs is carried out to evaluate the performance impact of various parameter settings . second , we inquire into the diversity of the elite solution set , showing , contrary to expectations , that a less diverse set leads to stronger performance . finally , we compare the best parameter setting of sgmpcs from the first two experiments to chronological backtracking , limited discrepancy search , randomized restart , and a sophisticated tabu search algorithm on a set of well-known benchmark problems . results demonstrate that sgmpcs is significantly better than the other constructive techniques tested , though lags behind the tabu search ."}
{"title": "counterfactual multi-agent policy gradients", "abstract": "cooperative multi-agent systems can be naturally used to model many real world problems , such as network packet routing and the coordination of autonomous vehicles . there is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems . to this end , we propose a new multi-agent actor-critic method called counterfactual multi-agent ( coma ) policy gradients . coma uses a centralised critic to estimate the q-function and decentralised actors to optimise the agents ' policies . in addition , to address the challenges of multi-agent credit assignment , it uses a counterfactual baseline that marginalises out a single agent 's action , while keeping the other agents ' actions fixed . coma also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass . we evaluate coma in the testbed of starcraft unit micromanagement , using a decentralised variant with significant partial observability . coma significantly improves average performance over other multi-agent actor-critic methods in this setting , and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state ."}
{"title": "dependence space of matroids and its application to attribute reduction", "abstract": "attribute reduction is a basic issue in knowledge representation and data mining . rough sets provide a theoretical foundation for the issue . matroids generalized from matrices have been widely used in many fields , particularly greedy algorithm design , which plays an important role in attribute reduction . therefore , it is meaningful to combine matroids with rough sets to solve the optimization problems . in this paper , we introduce an existing algebraic structure called dependence space to study the reduction problem in terms of matroids . first , a dependence space of matroids is constructed . second , the characterizations for the space such as consistent sets and reducts are studied through matroids . finally , we investigate matroids by the means of the space and present two expressions for their bases . in a word , this paper provides new approaches to study attribute reduction ."}
{"title": "quality of geographic information : ontological approach and artificial intelligence tools", "abstract": "the objective is to present one important aspect of the european ist-fet project `` rev ! gis '' 1 : the methodology which has been developed for the translation ( interpretation ) of the quality of the data into a `` fitness for use '' information , that we can confront to the user needs in its application . this methodology is based upon the notion of `` ontologies '' as a conceptual framework able to capture the explicit and implicit knowledge involved in the application . we do not address the general problem of formalizing such ontologies , instead , we rather try to illustrate this with three applications which are particular cases of the more general `` data fusion '' problem . in each application , we show how to deploy our methodology , by comparing several possible solutions , and we try to enlighten where are the quality issues , and what kind of solution to privilege , even at the expense of a highly complex computational approach . the expectation of the rev ! gis project is that computationally tractable solutions will be available among the next generation ai tools ."}
{"title": "probabilistic programming with gaussian process memoization", "abstract": "gaussian processes ( gps ) are widely used tools in statistics , machine learning , robotics , computer vision , and scientific computation . however , despite their popularity , they can be difficult to apply ; all but the simplest classification or regression applications require specification and inference over complex covariance functions that do not admit simple analytical posteriors . this paper shows how to embed gaussian processes in any higher-order probabilistic programming language , using an idiom based on memoization , and demonstrates its utility by implementing and extending classic and state-of-the-art gp applications . the interface to gaussian processes , called gpmem , takes an arbitrary real-valued computational process as input and returns a statistical emulator that automatically improve as the original process is invoked and its input-output behavior is recorded . the flexibility of gpmem is illustrated via three applications : ( i ) robust gp regression with hierarchical hyper-parameter learning , ( ii ) discovering symbolic expressions from time-series data by fully bayesian structure learning over kernels generated by a stochastic grammar , and ( iii ) a bandit formulation of bayesian optimization with automatic inference and action selection . all applications share a single 50-line python library and require fewer than 20 lines of probabilistic code each ."}
{"title": "ai researchers , video games are your friends !", "abstract": "if you are an artificial intelligence researcher , you should look to video games as ideal testbeds for the work you do . if you are a video game developer , you should look to ai for the technology that makes completely new types of games possible . this chapter lays out the case for both of these propositions . it asks the question `` what can video games do for ai '' , and discusses how in particular general video game playing is the ideal testbed for artificial general intelligence research . it then asks the question `` what can ai do for video games '' , and lays out a vision for what video games might look like if we had significantly more advanced ai at our disposal . the chapter is based on my keynote at ijcci 2015 , and is written in an attempt to be accessible to a broad audience ."}
{"title": "empirically grounded agent-based models of innovation diffusion : a critical review", "abstract": "innovation diffusion has been studied extensively in a variety of disciplines , including sociology , economics , marketing , ecology , and computer science . traditional literature on innovation diffusion has been dominated by models of aggregate behavior and trends . however , the agent-based modeling ( abm ) paradigm is gaining popularity as it captures agent heterogeneity and enables fine-grained modeling of interactions mediated by social and geographic networks . while most abm work on innovation diffusion is theoretical , empirically grounded models are increasingly important , particularly in guiding policy decisions . we present a critical review of empirically grounded agent-based models of innovation diffusion , developing a categorization of this research based on types of agent models as well as applications . by connecting the modeling methodologies in the fields of information and innovation diffusion , we suggest that the maximum likelihood estimation framework widely used in the former is a promising paradigm for calibration of agent-based models for innovation diffusion . although many advances have been made to standardize abm methodology , we identify four major issues in model calibration and validation , and suggest potential solutions ."}
{"title": "towards learning object affordance priors from technical texts", "abstract": "everyday activities performed by artificial assistants can potentially be executed naively and dangerously given their lack of common sense knowledge . this paper presents conceptual work towards obtaining prior knowledge on the usual modality ( passive or active ) of any given entity , and their affordance estimates , by extracting high-confidence ability modality semantic relations ( x can y relationship ) from non-figurative texts , by analyzing co-occurrence of grammatical instances of subjects and verbs , and verbs and objects . the discussion includes an outline of the concept , potential and limitations , and possible feature and learning framework adoption ."}
{"title": "geogebra tools with proof capabilities", "abstract": "we report about significant enhancements of the complex algebraic geometry theorem proving subsystem in geogebra for automated proofs in euclidean geometry , concerning the extension of numerous geogebra tools with proof capabilities . as a result , a number of elementary theorems can be proven by using geogebra 's intuitive user interface on various computer architectures including native java and web based systems with javascript . we also provide a test suite for benchmarking our results with 200 test cases ."}
{"title": "i , quantum robot : quantum mind control on a quantum computer", "abstract": "the logic which describes quantum robots is not orthodox quantum logic , but a deductive calculus which reproduces the quantum tasks ( computational processes , and actions ) taking into account quantum superposition and quantum entanglement . a way toward the realization of intelligent quantum robots is to adopt a quantum metalanguage to control quantum robots . a physical implementation of a quantum metalanguage might be the use of coherent states in brain signals ."}
{"title": "multimodal content analysis for effective advertisements on youtube", "abstract": "the rapid advances in e-commerce and web 2.0 technologies have greatly increased the impact of commercial advertisements on the general public . as a key enabling technology , a multitude of recommender systems exists which analyzes user features and browsing patterns to recommend appealing advertisements to users . in this work , we seek to study the characteristics or attributes that characterize an effective advertisement and recommend a useful set of features to aid the designing and production processes of commercial advertisements . we analyze the temporal patterns from multimedia content of advertisement videos including auditory , visual and textual components , and study their individual roles and synergies in the success of an advertisement . the objective of this work is then to measure the effectiveness of an advertisement , and to recommend a useful set of features to advertisement designers to make it more successful and approachable to users . our proposed framework employs the signal processing technique of cross modality feature learning where data streams from different components are employed to train separate neural network models and are then fused together to learn a shared representation . subsequently , a neural network model trained on this joint feature embedding representation is utilized as a classifier to predict advertisement effectiveness . we validate our approach using subjective ratings from a dedicated user study , the sentiment strength of online viewer comments , and a viewer opinion metric of the ratio of the likes and views received by each advertisement from an online platform ."}
{"title": "improve sat-solving with machine learning", "abstract": "in this project , we aimed to improve the runtime of minisat , a conflict-driven clause learning ( cdcl ) solver that solves the propositional boolean satisfiability ( sat ) problem . we first used a logistic regression model to predict the satisfiability of propositional boolean formulae after fixing the values of a certain fraction of the variables in each formula . we then applied the logistic model and added a preprocessing period to minisat to determine the preferable initial value ( either true or false ) of each boolean variable using a monte-carlo approach . concretely , for each monte-carlo trial , we fixed the values of a certain ratio of randomly selected variables , and calculated the confidence that the resulting sub-formula is satisfiable with our logistic regression model . the initial value of each variable was set based on the mean confidence scores of the trials that started from the literals of that variable . we were particularly interested in setting the initial values of the backbone variables correctly , which are variables that have the same value in all solutions of a sat formula . our monte-carlo method was able to set 78 % of the backbones correctly . excluding the preprocessing time , compared with the default setting of minisat , the runtime of minisat for satisfiable formulae decreased by 23 % . however , our method did not outperform vanilla minisat in runtime , as the decrease in the conflicts was outweighed by the long runtime of the preprocessing period ."}
{"title": "hatp : an htn planner for robotics", "abstract": "hierarchical task network ( htn ) planning is a popular approach that cuts down on the classical planning search space by relying on a given hierarchical library of domain control knowledge . this provides an intuitive methodology for specifying high-level instructions on how robots and agents should perform tasks , while also giving the planner enough flexibility to choose the lower-level steps and their ordering . in this paper we present the hatp ( hierarchical agent-based task planner ) planning framework which extends the traditional htn planning domain representation and semantics by making them more suitable for roboticists , and treating agents as `` first class '' entities in the language . the former is achieved by allowing `` social rules '' to be defined which specify what behaviour is acceptable/unacceptable by the agents/robots in the domain , and interleaving planning with geometric reasoning in order to validate online -with respect to a detailed geometric 3d world- the human/robot actions currently being pursued by hatp ."}
{"title": "an alc ( d ) -based combination of temporal constraints and spatial constraints suitable for continuous ( spatial ) change", "abstract": "we present a family of spatio-temporal theories suitable for continuous spatial change in general , and for continuous motion of spatial scenes in particular . the family is obtained by spatio-temporalising the well-known alc ( d ) family of description logics ( dls ) with a concrete domain d , as follows , where tcsps denotes `` temporal constraint satisfaction problems '' , a well-known constraint-based framework : ( 1 ) temporalisation of the roles , so that they consist of tcsp constraints ( specifically , of an adaptation of tcsp constraints to interval variables ) ; and ( 2 ) spatialisation of the concrete domain d : the concrete domain is now $ d_x $ , and is generated by a spatial relation algebra ( ra ) $ x $ , in the style of the region-connection calculus rcc8 . we assume durative truth ( i.e. , holding during a durative interval ) . we also assume the homogeneity property ( if a truth holds during a given interval , it holds during all of its subintervals ) . among other things , these assumptions raise the `` conflicting '' problem of overlapping truths , which the work solves with the use of a specific partition of the 13 atomic relations of allen 's interval algebra ."}
{"title": "knowledge base completion : baselines strike back", "abstract": "many papers have been published on the knowledge base completion task in the past few years . most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as fb15k and wn18 . this paper shows that the accuracy of almost all models published on the fb15k can be outperformed by an appropriately tuned baseline - our reimplementation of the distmult model . our findings cast doubt on the claim that the performance improvements of recent models are due to architectural changes as opposed to hyper-parameter tuning or different training objectives . this should prompt future research to re-consider how the performance of models is evaluated and reported ."}
{"title": "non-depth-first search against independent distributions on an and-or tree", "abstract": "suzuki and niida ( ann . pure . appl . logic , 2015 ) showed the following results on independent distributions ( ids ) on an and-or tree , where they took only depth-first algorithms into consideration . ( 1 ) among ids such that probability of the root having value 0 is fixed as a given r such that 0 < r < 1 , if d is a maximizer of cost of the best algorithm then d is an independent and identical distribution ( iid ) . ( 2 ) among all ids , if d is a maximizer of cost of the best algorithm then d is an iid . in the case where non-depth-first algorithms are taken into consideration , the counter parts of ( 1 ) and ( 2 ) are left open in the above work . peng et al . ( inform . process . lett. , 2017 ) extended ( 1 ) and ( 2 ) to multi-branching trees , where in ( 2 ) they put an additional hypothesis on ids that probability of the root having value 0 is neither 0 nor 1. we give positive answers for the two questions of suzuki-niida . a key to the proof is that if id d achieves the equilibrium among ids then we can chose an algorithm of the best cost against d from depth-first algorithms . in addition , we extend the result of peng et al . to the case where non-depth-first algorithms are taken into consideration ."}
{"title": "powerai ddl", "abstract": "as deep neural networks become more complex and input datasets grow larger , it can take days or even weeks to train a deep neural network to the desired accuracy . therefore , distributed deep learning at a massive scale is a critical capability , since it offers the potential to reduce the training time from weeks to hours . in this paper , we present a software-hardware co-optimized distributed deep learning system that can achieve near-linear scaling up to hundreds of gpus . the core algorithm is a multi-ring communication pattern that provides a good tradeoff between latency and bandwidth and adapts to a variety of system configurations . the communication algorithm is implemented as a library for easy use . this library has been integrated into tensorflow , caffe , and torch . we train resnet-101 on imagenet 22k with 64 ibm power8 s822lc servers ( 256 gpus ) in about 7 hours to an accuracy of 33.8 % validation accuracy . microsoft 's adam and google 's distbelief results did not reach 30 % validation accuracy for imagenet 22k . compared to facebook ai research 's recent paper on 256 gpu training , we use a different communication algorithm , and our combined software and hardware system offers better communication overhead for resnet-50 . a powerai ddl enabled version of torch completed 90 epochs of training on resnet 50 for 1k classes in 50 minutes using 64 ibm power8 s822lc servers ( 256 gpus ) ."}
{"title": "efficient hierarchical robot motion planning under uncertainty and hybrid dynamics", "abstract": "noisy observations coupled with nonlinear dynamics pose one of the biggest challenges in robot motion planning . by decomposing the nonlinear dynamics into a discrete set of local dynamics models , hybrid dynamics provide a natural way to model nonlinear dynamics , especially in systems with sudden `` jumps '' in the dynamics , due to factors such as contacts . we propose a hierarchical pomdp planner that develops locally optimal motion plans for hybrid dynamics models . the hierarchical planner first develops a high-level motion plan to sequence the local dynamics models to be visited . the high-level plan is then converted into a detailed cost-optimized continuous state plan . this hierarchical planning approach results in a decomposition of the pomdp planning problem into smaller sub-parts that can be solved with significantly lower computational costs . the ability to sequence the visitation of local dynamics models also provides a powerful way to leverage the hybrid dynamics to reduce state uncertainty . we evaluate the proposed planner for two navigation and localization tasks in simulated domains , as well as an assembly task with a real robotic manipulator ."}
{"title": "principled hybrids of generative and discriminative domain adaptation", "abstract": "we propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way . under this framework , generative and discriminative models correspond to specific choices of the prior over parameters . this provides us a very general way to interpolate between generative and discriminative extremes through different choices of priors . by maximizing both the marginal and the conditional log-likelihoods , models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from both source and target domains . under this framework , we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances , where marginal distributions are given by proper kernel density estimations . this provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning . we instantiate our framework using neural networks , and build a concrete model , dauto . empirically , we demonstrate the effectiveness of dauto on text , image and speech datasets , showing that it outperforms related competitors when domain adaptation is possible ."}
{"title": "asynchronous dynamic bayesian networks", "abstract": "systems such as sensor networks and teams of autonomous robots consist of multiple autonomous entities that interact with each other in a distributed , asynchronous manner . these entities need to keep track of the state of the system as it evolves . asynchronous systems lead to special challenges for monitoring , as nodes must update their beliefs independently of each other and no central coordination is possible . furthermore , the state of the system continues to change as beliefs are being updated . previous approaches to developing distributed asynchronous probabilistic reasoning systems have used static models . we present an approach using dynamic models , that take into account the way the system changes state over time . our approach , which is based on belief propagation , is fully distributed and asynchronous , and allows the world to keep on changing as messages are being sent around . experimental results show that our approach compares favorably to the factored frontier algorithm ."}
{"title": "a novel energy aware node clustering algorithm for wireless sensor networks using a modified artificial fish swarm algorithm", "abstract": "clustering problems are considered amongst the prominent challenges in statistics and computational science . clustering of nodes in wireless sensor networks which is used to prolong the life-time of networks is one of the difficult tasks of clustering procedure . in order to perform nodes clustering , a number of nodes are determined as cluster heads and other ones are joined to one of these heads , based on different criteria e.g . euclidean distance . so far , different approaches have been proposed for this process , where swarm and evolutionary algorithms contribute in this regard . in this study , a novel algorithm is proposed based on artificial fish swarm algorithm ( afsa ) for clustering procedure . in the proposed method , the performance of the standard afsa is improved by increasing balance between local and global searches . furthermore , a new mechanism has been added to the base algorithm for improving convergence speed in clustering problems . performance of the proposed technique is compared to a number of state-of-the-art techniques in this field and the outcomes indicate the supremacy of the proposed technique ."}
{"title": "game-theoretic network centrality : a review", "abstract": "game-theoretic centrality is a flexible and sophisticated approach to identify the most important nodes in a network . it builds upon the methods from cooperative game theory and network theory . the key idea is to treat nodes as players in a cooperative game , where the value of each coalition is determined by certain graph-theoretic properties . using solution concepts from cooperative game theory , it is then possible to measure how responsible each node is for the worth of the network . the literature on the topic is already quite large , and is scattered among game-theoretic and computer science venues . we review the main game-theoretic network centrality measures from both bodies of literature and organize them into two categories : those that are more focused on the connectivity of nodes , and those that are more focused on the synergies achieved by nodes in groups . we present and explain each centrality , with a focus on algorithms and complexity ."}
{"title": "monotone conditional complexity bounds on future prediction errors", "abstract": "we bound the future loss when predicting any ( computably ) stochastic sequence online . solomonoff finitely bounded the total deviation of his universal predictor m from the true distribution m by the algorithmic complexity of m. here we assume we are at a time t > 1 and already observed x=x_1 ... x_t . we bound the future prediction performance on x_ { t+1 } x_ { t+2 } ... by a new variant of algorithmic complexity of m given x , plus the complexity of the randomness deficiency of x. the new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged . we also briefly discuss potential generalizations to bayesian model classes and to classification problems ."}
{"title": "release zero.0.1 of package refereetoolbox", "abstract": "refereetoolbox is a java package implementing combination operators for fusing evidences . it is downloadable from : http : //refereefunction.fredericdambreville.com/releases refereetoolbox is based on an interpretation of the fusion rules by means of referee functions . this approach implies a dissociation between the definition of the combination and its actual implementation , which is common to all referee-based combinations . as a result , refereetoolbox is designed with the aim to be generic and evolutive ."}
{"title": "plan development using local probabilistic models", "abstract": "approximate models of world state transitions are necessary when building plans for complex systems operating in dynamic environments . external event probabilities can depend on state feature values as well as time spent in that particular state . we assign temporally -dependent probability functions to state transitions . these functions are used to locally compute state probabilities , which are then used to select highly probable goal paths and eliminate improbable states . this probabilistic model has been implemented in the cooperative intelligent real-time control architecture ( circa ) , which combines an ai planner with a separate real-time system such that plans are developed , scheduled , and executed with real-time guarantees . we present flight simulation tests that demonstrate how our probabilistic model may improve circa performance ."}
{"title": "a machine learning based analytical framework for semantic annotation requirements", "abstract": "the semantic web is an extension of the current web in which information is given well-defined meaning . the perspective of semantic web is to promote the quality and intelligence of the current web by changing its contents into machine understandable form . therefore , semantic level information is one of the cornerstones of the semantic web . the process of adding semantic metadata to web resources is called semantic annotation . there are many obstacles against the semantic annotation , such as multilinguality , scalability , and issues which are related to diversity and inconsistency in content of different web pages . due to the wide range of domains and the dynamic environments that the semantic annotation systems must be performed on , the problem of automating annotation process is one of the significant challenges in this domain . to overcome this problem , different machine learning approaches such as supervised learning , unsupervised learning and more recent ones like , semi-supervised learning and active learning have been utilized . in this paper we present an inclusive layered classification of semantic annotation challenges and discuss the most important issues in this field . also , we review and analyze machine learning applications for solving semantic annotation problems . for this goal , the article tries to closely study and categorize related researches for better understanding and to reach a framework that can map machine learning techniques into the semantic annotation challenges and requirements ."}
{"title": "an evaluation of an algorithm for inductive learning of bayesian belief networks usin", "abstract": "bayesian learning of belief networks ( bln ) is a method for automatically constructing belief networks ( bns ) from data using search and bayesian scoring techniques . k2 is a particular instantiation of the method that implements a greedy search strategy . to evaluate the accuracy of k2 , we randomly generated a number of bns and for each of those we simulated data sets . k2 was then used to induce the generating bns from the simulated data . we examine the performance of the program , and the factors that influence it . we also present a simple bn model , developed from our results , which predicts the accuracy of k2 , when given various characteristics of the data set ."}
{"title": "building a stochastic dynamic model of application use", "abstract": "many intelligent user interfaces employ application and user models to determine the user 's preferences , goals and likely future actions . such models require application analysis , adaptation and expansion . building and maintaining such models adds a substantial amount of time and labour to the application development cycle . we present a system that observes the interface of an unmodified application and records users ' interactions with the application . from a history of such observations we build a coarse state space of observed interface states and actions between them . to refine the space , we hypothesize sub-states based upon the histories that led users to a given state . we evaluate the information gain of possible state splits , varying the length of the histories considered in such splits . in this way , we automatically produce a stochastic dynamic model of the application and of how it is used . to evaluate our approach , we present models derived from real-world application usage data ."}
{"title": "interpretable policies for reinforcement learning by genetic programming", "abstract": "the search for interpretable reinforcement learning policies is of high academic and industrial interest . especially for industrial systems , domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate . basic algebraic equations are supposed to meet these requirements , as long as they are restricted to an adequate complexity . here we introduce the genetic programming for reinforcement learning ( gprl ) approach based on model-based batch reinforcement learning and genetic programming , which autonomously learns policy equations from pre-existing default state-action trajectory samples . gprl is compared to a straight-forward method which utilizes genetic programming for symbolic regression , yielding policies imitating an existing well-performing , but non-interpretable policy . experiments on three reinforcement learning benchmarks , i.e. , mountain car , cart-pole balancing , and industrial benchmark , demonstrate the superiority of our gprl approach compared to the symbolic regression method . gprl is capable of producing well-performing interpretable reinforcement learning policies from pre-existing default trajectory data ."}
{"title": "a-nice-mc : adversarial training for mcmc", "abstract": "existing markov chain monte carlo ( mcmc ) methods are either based on general-purpose and domain-agnostic schemes which can lead to slow convergence , or hand-crafting of problem-specific proposals by an expert . we propose a-nice-mc , a novel method to train flexible parametric markov chain kernels to produce samples with desired properties . first , we propose an efficient likelihood-free adversarial training method to train a markov chain and mimic a given data distribution . then , we leverage flexible volume preserving flows to obtain parametric kernels for mcmc . using a bootstrap approach , we show how to train efficient markov chains to sample from a prescribed posterior distribution by iteratively improving the quality of both the model and the samples . a-nice-mc provides the first framework to automatically design efficient domain-specific mcmc proposals . empirical results demonstrate that a-nice-mc combines the strong guarantees of mcmc with the expressiveness of deep neural networks , and is able to significantly outperform competing methods such as hamiltonian monte carlo ."}
{"title": "combination strategies for semantic role labeling", "abstract": "this paper introduces and analyzes a battery of inference models for the problem of semantic role labeling : one based on constraint satisfaction , and several strategies that model the inference as a meta-learning problem using discriminative classifiers . these classifiers are developed with a rich set of novel features that encode proposition and sentence-level information . to our knowledge , this is the first work that : ( a ) performs a thorough analysis of learning-based inference models for semantic role labeling , and ( b ) compares several inference strategies in this context . we evaluate the proposed inference strategies in the framework of the conll-2005 shared task using only automatically-generated syntactic information . the extensive experimental evaluation and analysis indicates that all the proposed inference strategies are successful -they all outperform the current best results reported in the conll-2005 evaluation exercise- but each of the proposed approaches has its advantages and disadvantages . several important traits of a state-of-the-art srl combination strategy emerge from this analysis : ( i ) individual models should be combined at the granularity of candidate arguments rather than at the granularity of complete solutions ; ( ii ) the best combination strategy uses an inference model based in learning ; and ( iii ) the learning-based inference benefits from max-margin classifiers and global feedback ."}
{"title": "pddl2.1 - the art of the possible ? commentary on fox and long", "abstract": "pddl2.1 was designed to push the envelope of what planning algorithms can do , and it has succeeded . it adds two important features : durative actions , which take time ( and may have continuous effects ) ; and objective functions for measuring the quality of plans . the concept of durative actions is flawed ; and the treatment of their semantics reveals too strong an attachment to the way many contemporary planners work . future pddl innovators should focus on producing a clean semantics for additions to the language , and let planner implementers worry about coupling their algorithms to problems expressed in the latest version of the language ."}
{"title": "space and camera path reconstruction for omni-directional vision", "abstract": "in this paper , we address the inverse problem of reconstructing a scene as well as the camera motion from the image sequence taken by an omni-directional camera . our structure from motion results give sharp conditions under which the reconstruction is unique . for example , if there are three points in general position and three omni-directional cameras in general position , a unique reconstruction is possible up to a similarity . we then look at the reconstruction problem with m cameras and n points , where n and m can be large and the over-determined system is solved by least square methods . the reconstruction is robust and generalizes to the case of a dynamic environment where landmarks can move during the movie capture . possible applications of the result are computer assisted scene reconstruction , 3d scanning , autonomous robot navigation , medical tomography and city reconstructions ."}
{"title": "improved mean and variance approximations for belief net responses via network doubling", "abstract": "a bayesian belief network models a joint distribution with an directed acyclic graph representing dependencies among variables and network parameters characterizing conditional distributions . the parameters are viewed as random variables to quantify uncertainty about their values . belief nets are used to compute responses to queries ; i.e. , conditional probabilities of interest . a query is a function of the parameters , hence a random variable . van allen et al . ( 2001 , 2008 ) showed how to quantify uncertainty about a query via a delta method approximation of its variance . we develop more accurate approximations for both query mean and variance . the key idea is to extend the query mean approximation to a `` doubled network '' involving two independent replicates . our method assumes complete data and can be applied to discrete , continuous , and hybrid networks ( provided discrete variables have only discrete parents ) . we analyze several improvements , and provide empirical studies to demonstrate their effectiveness ."}
{"title": "a cookbook of translating english to xapi", "abstract": "the xapagy cognitive architecture had been designed to perform narrative reasoning : to model and mimic the activities performed by humans when witnessing , reading , recalling , narrating and talking about stories . xapagy communicates with the outside world using xapi , a simplified , `` pidgin '' language which is strongly tied to the internal representation model ( instances , scenes and verb instances ) and reasoning techniques ( shadows and headless shadows ) . while not fully a semantic equivalent of natural language , xapi can represent a wide range of complex stories . we illustrate the representation technique used in xapi through examples taken from folk physics , folk psychology as well as some more unusual literary examples . we argue that while the xapi model represents a conceptual shift from the english representation , the mapping is logical and consistent , and a trained knowledge engineer can translate between english and xapi at near-native speed ."}
{"title": "where are the hard manipulation problems ?", "abstract": "one possible escape from the gibbard-satterthwaite theorem is computational complexity . for example , it is np-hard to compute if the stv rule can be manipulated . however , there is increasing concern that such results may not re ect the difficulty of manipulation in practice . in this tutorial , i survey recent results in this area ."}
{"title": "option discovery in hierarchical reinforcement learning using spatio-temporal clustering", "abstract": "this paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states . identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms . these structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch . we use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states . the spectral clustering algorithm pcca+ is used to identify suitable abstractions aligned to the underlying structure . skills are defined in terms of the sequence of actions that lead to transitions between such abstract states . the connectivity information from pcca+ is used to generate these skills or options . these skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model . this approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate . we also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space ."}
{"title": "beyond the technical challenges for deploying machine learning solutions in a software company", "abstract": "recently software development companies started to embrace machine learning ( ml ) techniques for introducing a series of advanced functionality in their products such as personalisation of the user experience , improved search , content recommendation and automation . the technical challenges for tackling these problems are heavily researched in literature . a less studied area is a pragmatic approach to the role of humans in a complex modern industrial environment where ml based systems are developed . key stakeholders affect the system from inception and up to operation and maintenance . product managers want to embed `` smart '' experiences for their users and drive the decisions on what should be built next ; software engineers are challenged to build or utilise ml software tools that require skills that are well outside of their comfort zone ; legal and risk departments may influence design choices and data access ; operations teams are requested to maintain ml systems which are non-stationary in their nature and change behaviour over time ; and finally ml practitioners should communicate with all these stakeholders to successfully build a reliable system . this paper discusses some of the challenges we faced in atlassian as we started investing more in the ml space ."}
{"title": "information pursuit : a bayesian framework for sequential scene parsing", "abstract": "despite enormous progress in object detection and classification , the problem of incorporating expected contextual relationships among object instances into modern recognition systems remains a key challenge . in this work we propose information pursuit , a bayesian framework for scene parsing that combines prior models for the geometry of the scene and the spatial arrangement of objects instances with a data model for the output of high-level image classifiers trained to answer specific questions about the scene . in the proposed framework , the scene interpretation is progressively refined as evidence accumulates from the answers to a sequence of questions . at each step , we choose the question to maximize the mutual information between the new answer and the full interpretation given the current evidence obtained from previous inquiries . we also propose a method for learning the parameters of the model from synthesized , annotated scenes obtained by top-down sampling from an easy-to-learn generative scene model . finally , we introduce a database of annotated indoor scenes of dining room tables , which we use to evaluate the proposed approach ."}
{"title": "universal algorithmic intelligence : a mathematical top- > down approach", "abstract": "sequential decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known . solomonoff 's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution . we combine both ideas and get a parameter-free theory of universal artificial intelligence . we give strong arguments that the resulting aixi model is the most intelligent unbiased agent possible . we outline how the aixi model can formally solve a number of problem classes , including sequence prediction , strategic games , function minimization , reinforcement and supervised learning . the major drawback of the aixi model is that it is uncomputable . to overcome this problem , we construct a modified algorithm aixitl that is still effectively more intelligent than any other time t and length l bounded agent . the computation time of aixitl is of the order t x 2^l . the discussion includes formal definitions of intelligence order relations , the horizon problem and relations of the aixi theory to other ai approaches ."}
{"title": "probabilistic graphical models for credibility analysis in evolving online communities", "abstract": "one of the major hurdles preventing the full exploitation of information from online communities is the widespread concern regarding the quality and credibility of user-contributed content . prior works in this domain operate on a static snapshot of the community , making strong assumptions about the structure of the data ( e.g. , relational tables ) , or consider only shallow features for text classification . to address the above limitations , we propose probabilistic graphical models that can leverage the joint interplay between multiple factors in online communities -- - like user interactions , community dynamics , and textual content -- - to automatically assess the credibility of user-contributed online content , and the expertise of users and their evolution with user-interpretable explanation . to this end , we devise new models based on conditional random fields for different settings like incorporating partial expert knowledge for semi-supervised learning , and handling discrete labels as well as numeric ratings for fine-grained analysis . this enables applications such as extracting reliable side-effects of drugs from user-contributed posts in healthforums , and identifying credible content in news communities . online communities are dynamic , as users join and leave , adapt to evolving trends , and mature over time . to capture this dynamics , we propose generative models based on hidden markov model , latent dirichlet allocation , and brownian motion to trace the continuous evolution of user expertise and their language model over time . this allows us to identify expert users and credible content jointly over time , improving state-of-the-art recommender systems by explicitly considering the maturity of users . this also enables applications such as identifying helpful product reviews , and detecting fake and anomalous reviews with limited information ."}
{"title": "matrust : an effective multi-aspect trust inference model", "abstract": "trust is a fundamental concept in many real-world applications such as e-commerce and peer-to-peer networks . in these applications , users can generate local opinions about the counterparts based on direct experiences , and these opinions can then be aggregated to build trust among unknown users . the mechanism to build new trust relationships based on existing ones is referred to as trust inference . state-of-the-art trust inference approaches employ the transitivity property of trust by propagating trust along connected users . in this paper , we propose a novel trust inference model ( matrust ) by exploring an equally important property of trust , i.e. , the multi-aspect property . matrust directly characterizes multiple latent factors for each trustor and trustee from the locally-generated trust relationships . furthermore , it can naturally incorporate prior knowledge as specified factors . these factors in turn serve as the basis to infer the unseen trustworthiness scores . experimental evaluations on real data sets show that the proposed matrust significantly outperforms several benchmark trust inference models in both effectiveness and efficiency ."}
{"title": "pg-causality : identifying spatiotemporal causal pathways for air pollutants with urban big data", "abstract": "many countries are suffering from severe air pollution . understanding how different air pollutants accumulate and propagate is critical to making relevant public policies . in this paper , we use urban big data ( air quality data and meteorological data ) to identify the \\emph { spatiotemporal ( st ) causal pathways } for air pollutants . this problem is challenging because : ( 1 ) there are numerous noisy and low-pollution periods in the raw air quality data , which may lead to unreliable causality analysis , ( 2 ) for large-scale data in the st space , the computational complexity of constructing a causal structure is very high , and ( 3 ) the \\emph { st causal pathways } are complex due to the interactions of multiple pollutants and the influence of environmental factors . therefore , we present \\emph { p-causality } , a novel pattern-aided causality analysis approach that combines the strengths of \\emph { pattern mining } and \\emph { bayesian learning } to efficiently and faithfully identify the \\emph { st causal pathways } . first , \\emph { pattern mining } helps suppress the noise by capturing frequent evolving patterns ( feps ) of each monitoring sensor , and greatly reduce the complexity by selecting the pattern-matched sensors as `` causers '' . then , \\emph { bayesian learning } carefully encodes the local and st causal relations with a gaussian bayesian network ( gbn ) -based graphical model , which also integrates environmental influences to minimize biases in the final results . we evaluate our approach with three real-world data sets containing 982 air quality sensors , in three regions of china from 01-jun-2013 to 19-dec-2015 . results show that our approach outperforms the traditional causal structure learning methods in time efficiency , inference accuracy and interpretability ."}
{"title": "changing the environment based on intrinsic motivation", "abstract": "one of the remarkable feats of intelligent life is that it restructures the world it lives in for its own benefit . this extended abstract outlines how the information-theoretic principle of empowerment , as an intrinsic motivation , can be used to restructure the environment an agent lives in . we present a first qualitative evaluation of how an agent in a 3d-gridworld builds a staircase-like structure , which reflects the agent 's embodiment ."}
{"title": "on finding optimal polytrees", "abstract": "inferring probabilistic networks from data is a notoriously difficult task . under various goodness-of-fit measures , finding an optimal network is np-hard , even if restricted to polytrees of bounded in-degree . polynomial-time algorithms are known only for rare special cases , perhaps most notably for branchings , that is , polytrees in which the in-degree of every node is at most one . here , we study the complexity of finding an optimal polytree that can be turned into a branching by deleting some number of arcs or nodes , treated as a parameter . we show that the problem can be solved via a matroid intersection formulation in polynomial time if the number of deleted arcs is bounded by a constant . the order of the polynomial time bound depends on this constant , hence the algorithm does not establish fixed-parameter tractability when parameterized by the number of deleted arcs . we show that a restricted version of the problem allows fixed-parameter tractability and hence scales well with the parameter . we contrast this positive result by showing that if we parameterize by the number of deleted nodes , a somewhat more powerful parameter , the problem is not fixed-parameter tractable , subject to a complexity-theoretic assumption ."}
{"title": "a maximum likelihood approach for selecting sets of alternatives", "abstract": "we consider the problem of selecting a subset of alternatives given noisy evaluations of the relative strength of different alternatives . we wish to select a k-subset ( for a given k ) that provides a maximum likelihood estimate for one of several objectives , e.g. , containing the strongest alternative . although this problem is np-hard , we show that when the noise level is sufficiently high , intuitive methods provide the optimal solution . we thus generalize classical results about singling out one alternative and identifying the hidden ranking of alternatives by strength . extensive experiments show that our methods perform well in practical settings ."}
{"title": "a latent-class model for estimating product-choice probabilities from clickstream data", "abstract": "this paper analyzes customer product-choice behavior based on the recency and frequency of each customer 's page views on e-commerce sites . recently , we devised an optimization model for estimating product-choice probabilities that satisfy monotonicity , convexity , and concavity constraints with respect to recency and frequency . this shape-restricted model delivered high predictive performance even when there were few training samples . however , typical e-commerce sites deal in many different varieties of products , so the predictive performance of the model can be further improved by integration of such product heterogeneity . for this purpose , we develop a novel latent-class shape-restricted model for estimating product-choice probabilities for each latent class of products . we also give a tailored expectation-maximization algorithm for parameter estimation . computational results demonstrate that higher predictive performance is achieved with our latent-class model than with the previous shape-restricted model and common latent-class logistic regression ."}
{"title": "predictive state representations : a new theory for modeling dynamical systems", "abstract": "modeling dynamical systems , both for control purposes and to make predictions about their behavior , is ubiquitous in science and engineering . predictive state representations ( psrs ) are a recently introduced class of models for discrete-time dynamical systems . the key idea behind psrs and the closely related ooms ( jaeger 's observable operator models ) is to represent the state of the system as a set of predictions of observable outcomes of experiments one can do in the system . this makes psrs rather different from history-based models such as nth-order markov models and hidden-state-based models such as hmms and pomdps . we introduce an interesting construct , the systemdynamics matrix , and show how psrs can be derived simply from it . we also use this construct to show formally that psrs are more general than both nth-order markov models and hmms/pomdps . finally , we discuss the main difference between psrs and ooms and conclude with directions for future work ."}
{"title": "on the existence and multiplicity of extensions in dialectical argumentation", "abstract": "in the present paper , the existence and multiplicity problems of extensions are addressed . the focus is on extension of the stable type . the main result of the paper is an elegant characterization of the existence and multiplicity of extensions in terms of the notion of dialectical justification , a close cousin of the notion of admissibility . the characterization is given in the context of the particular logic for dialectical argumentation deflog . the results are of direct relevance for several well-established models of defeasible reasoning ( like default logic , logic programming and argumentation frameworks ) , since elsewhere dialectical argumentation has been shown to have close formal connections with these models ."}
{"title": "n-valued refined neutrosophic logic and its applications to physics", "abstract": "in this paper we present a short history of logics : from particular cases of 2-symbol or numerical valued logic to the general case of n-symbol or numerical valued logic . we show generalizations of 2-valued boolean logic to fuzzy logic , also from the kleene and lukasiewicz 3-symbol valued logics or belnap 4-symbol valued logic to the most general n-symbol or numerical valued refined neutrosophic logic . two classes of neutrosophic norm ( n-norm ) and neutrosophic conorm ( n-conorm ) are defined . examples of applications of neutrosophic logic to physics are listed in the last section . similar generalizations can be done for n-valued refined neutrosophic set , and respectively n- valued refined neutrosopjhic probability ."}
{"title": "events in property patterns", "abstract": "a pattern-based approach to the presentation , codification and reuse of property specifications for finite-state verification was proposed by dwyer and his collegues . the patterns enable non-experts to read and write formal specifications for realistic systems and facilitate easy conversion of specifications between formalisms , such as ltl , ctl , qre . in this paper , we extend the pattern system with events - changes of values of variables in the context of ltl ."}
{"title": "cortical prediction markets", "abstract": "we investigate cortical learning from the perspective of mechanism design . first , we show that discretizing standard models of neurons and synaptic plasticity leads to rational agents maximizing simple scoring rules . second , our main result is that the scoring rules are proper , implying that neurons faithfully encode expected utilities in their synaptic weights and encode high-scoring outcomes in their spikes . third , with this foundation in hand , we propose a biologically plausible mechanism whereby neurons backpropagate incentives which allows them to optimize their usefulness to the rest of cortex . finally , experiments show that networks that backpropagate incentives can learn simple tasks ."}
{"title": "the mean and median criterion for automatic kernel bandwidth selection for support vector data description", "abstract": "support vector data description ( svdd ) is a popular technique for detecting anomalies . the svdd classifier partitions the whole space into an inlier region , which consists of the region near the training data , and an outlier region , which consists of points away from the training data . the computation of the svdd classifier requires a kernel function , and the gaussian kernel is a common choice for the kernel function . the gaussian kernel has a bandwidth parameter , whose value is important for good results . a small bandwidth leads to overfitting , and the resulting svdd classifier overestimates the number of anomalies . a large bandwidth leads to underfitting , and the classifier fails to detect many anomalies . in this paper we present a new automatic , unsupervised method for selecting the gaussian kernel bandwidth . the selected value can be computed quickly , and it is competitive with existing bandwidth selection methods ."}
{"title": "cut-simulation and impredicativity", "abstract": "we investigate cut-elimination and cut-simulation in impredicative ( higher-order ) logics . we illustrate that adding simple axioms such as leibniz equations to a calculus for an impredicative logic -- in our case a sequent calculus for classical type theory -- is like adding cut . the phenomenon equally applies to prominent axioms like boolean- and functional extensionality , induction , choice , and description . this calls for the development of calculi where these principles are built-in instead of being treated axiomatically ."}
{"title": "finding risk-averse shortest path with time-dependent stochastic costs", "abstract": "in this paper , we tackle the problem of risk-averse route planning in a transportation network with time-dependent and stochastic costs . to solve this problem , we propose an adaptation of the a* algorithm that accommodates any risk measure or decision criterion that is monotonic with first-order stochastic dominance . we also present a case study of our algorithm on the manhattan , nyc , transportation network ."}
{"title": "vicious circle principle and formation of sets in asp based languages", "abstract": "the paper continues the investigation of poincare and russel 's vicious circle principle ( vcp ) in the context of the design of logic programming languages with sets . we expand previously introduced language alog with aggregates by allowing infinite sets and several additional set related constructs useful for knowledge representation and teaching . in addition , we propose an alternative formalization of the original vcp and incorporate it into the semantics of new language , slog+ , which allows more liberal construction of sets and their use in programming rules . we show that , for programs without disjunction and infinite sets , the formal semantics of aggregates in slog+ coincides with that of several other known languages . their intuitive and formal semantics , however , are based on quite different ideas and seem to be more involved than that of slog+ ."}
{"title": "mining compressed repetitive gapped sequential patterns efficiently", "abstract": "mining frequent sequential patterns from sequence databases has been a central research topic in data mining and various efficient mining sequential patterns algorithms have been proposed and studied . recently , in many problem domains ( e.g , program execution traces ) , a novel sequential pattern mining research , called mining repetitive gapped sequential patterns , has attracted the attention of many researchers , considering not only the repetition of sequential pattern in different sequences but also the repetition within a sequence is more meaningful than the general sequential pattern mining which only captures occurrences in different sequences . however , the number of repetitive gapped sequential patterns generated by even these closed mining algorithms may be too large to understand for users , especially when support threshold is low . in this paper , we propose and study the problem of compressing repetitive gapped sequential patterns . inspired by the ideas of summarizing frequent itemsets , rpglobal , we develop an algorithm , crgsgrow ( compressing repetitive gapped sequential pattern grow ) , including an efficient pruning strategy , syncscan , and an efficient representative pattern checking scheme , -dominate sequential pattern checking . the crgsgrow is a two-step approach : in the first step , we obtain all closed repetitive sequential patterns as the candidate set of representative repetitive sequential patterns , and at the same time get the most of representative repetitive sequential patterns ; in the second step , we only spend a little time in finding the remaining the representative patterns from the candidate set . an empirical study with both real and synthetic data sets clearly shows that the crgsgrow has good performance ."}
{"title": "dac-h3 : a proactive robot cognitive architecture to acquire and express knowledge about the world and the self", "abstract": "this paper introduces a cognitive architecture for a humanoid robot to engage in a proactive , mixed-initiative exploration and manipulation of its environment , where the initiative can originate from both the human and the robot . the framework , based on a biologically-grounded theory of the brain and mind , integrates a reactive interaction engine , a number of state-of-the-art perceptual and motor learning algorithms , as well as planning abilities and an autobiographical memory . the architecture as a whole drives the robot behavior to solve the symbol grounding problem , acquire language capabilities , execute goal-oriented behavior , and express a verbal narrative of its own experience in the world . we validate our approach in human-robot interaction experiments with the icub humanoid robot , showing that the proposed cognitive architecture can be applied in real time within a realistic scenario and that it can be used with naive users ."}
{"title": "les pomdp font de meilleurs hackers : tenir compte de l'incertitude dans les tests de penetration", "abstract": "penetration testing is a methodology for assessing network security , by generating and executing possible hacking attacks . doing so automatically allows for regular and systematic testing . a key question is how to generate the attacks . this is naturally formulated as planning under uncertainty , i.e. , under incomplete knowledge about the network configuration . previous work uses classical planning , and requires costly pre-processes reducing this uncertainty by extensive application of scanning methods . by contrast , we herein model the attack planning problem in terms of partially observable markov decision processes ( pomdp ) . this allows to reason about the knowledge available , and to intelligently employ scanning actions as part of the attack . as one would expect , this accurate solution does not scale . we devise a method that relies on pomdps to find good attacks on individual machines , which are then composed into an attack on the network as a whole . this decomposition exploits network structure to the extent possible , making targeted approximations ( only ) where needed . evaluating this method on a suitably adapted industrial test suite , we demonstrate its effectiveness in both runtime and solution quality ."}
{"title": "learning to order things", "abstract": "there are many applications in which it is desirable to order rather than classify instances . here we consider the problem of learning how to order instances given feedback in the form of preference judgments , i.e. , statements to the effect that one instance should be ranked ahead of another . we outline a two-stage approach in which one first learns by conventional means a binary preference function indicating whether it is advisable to rank one instance before another . here we consider an on-line algorithm for learning preference functions that is based on freund and schapire 's 'hedge ' algorithm . in the second stage , new instances are ordered so as to maximize agreement with the learned preference function . we show that the problem of finding the ordering that agrees best with a learned preference function is np-complete . nevertheless , we describe simple greedy algorithms that are guaranteed to find a good approximation . finally , we show how metasearch can be formulated as an ordering problem , and present experimental results on learning a combination of 'search experts ' , each of which is a domain-specific query expansion strategy for a web search engine ."}
{"title": "learning to schedule deadline- and operator-sensitive tasks", "abstract": "the use of semi-autonomous and autonomous robotic assistants to aid in care of the elderly is expected to ease the burden on human caretakers , with small-stage testing already occurring in a variety of countries . yet , it is likely that these robots will need to request human assistance via teleoperation when domain expertise is needed for a specific task . as deployment of robotic assistants moves to scale , mapping these requests for human aid to the teleoperators themselves will be a difficult online optimization problem . in this paper , we design a system that allocates requests to a limited number of teleoperators , each with different specialities , in an online fashion . we generalize a recent model of online job scheduling with a worst-case competitive-ratio bound to our setting . next , we design a scalable machine-learning-based teleoperator-aware task scheduling algorithm and show , experimentally , that it performs well when compared to an omniscient optimal scheduling algorithm ."}
{"title": "redefinition of the concept of fuzzy set based on vague partition from the perspective of axiomatization", "abstract": "based on the in-depth analysis of the essence and features of vague phenomena , this paper focuses on establishing the axiomatical foundation of membership degree theory for vague phenomena , presents an axiomatic system to govern membership degrees and their interconnections . on this basis , the concept of vague partition is introduced , further , the concept of fuzzy set introduced by zadeh in 1965 is redefined based on vague partition from the perspective of axiomatization . the thesis defended in this paper is that the relationship among vague attribute values should be the starting point to recognize and model vague phenomena from a quantitative view ."}
{"title": "context-specific approximation in probabilistic inference", "abstract": "there is evidence that the numbers in probabilistic inference do n't really matter . this paper considers the idea that we can make a probabilistic model simpler by making fewer distinctions . unfortunately , the level of a bayesian network seems too coarse ; it is unlikely that a parent will make little difference for all values of the other parents . in this paper we consider an approximation scheme where distinctions can be ignored in some contexts , but not in other contexts . we elaborate on a notion of a parent context that allows a structured context-specific decomposition of a probability distribution and the associated probabilistic inference scheme called probabilistic partial evaluation ( poole 1997 ) . this paper shows a way to simplify a probabilistic model by ignoring distinctions which have similar probabilities , a method to exploit the simpler model , a bound on the resulting errors , and some preliminary empirical results on simple networks ."}
{"title": "practical reasoning for very expressive description logics", "abstract": "description logics ( dls ) are a family of knowledge representation formalisms mainly characterised by constructors to build complex concepts and roles from atomic ones . expressive role constructors are important in many applications , but can be computationally problematical . we present an algorithm that decides satisfiability of the dl alc extended with transitive and inverse roles and functional restrictions with respect to general concept inclusion axioms and role hierarchies ; early experiments indicate that this algorithm is well-suited for implementation . additionally , we show that alc extended with just transitive and inverse roles is still in pspace . we investigate the limits of decidability for this family of dls , showing that relaxing the constraints placed on the kinds of roles used in number restrictions leads to the undecidability of all inference problems . finally , we describe a number of optimisation techniques that are crucial in obtaining implementations of the decision procedures , which , despite the worst-case complexity of the problem , exhibit good performance with real-life problems ."}
{"title": "the opacity of backbones and backdoors under a weak assumption", "abstract": "backdoors and backbones of boolean formulas are hidden structural properties . a natural goal , already in part realized , is that solver algorithms seek to obtain substantially better performance by exploiting these structures . however , the present paper is not intended to improve the performance of sat solvers , but rather is a cautionary paper . in particular , the theme of this paper is that there is a potential chasm between the existence of such structures in the boolean formula and being able to effectively exploit them . this does not mean that these structures are not useful to solvers . it does mean that one must be very careful not to assume that it is computationally easy to go from the existence of a structure to being able to get one 's hands on it and/or being able to exploit the structure . for example , in this paper we show that , under the assumption that p $ \\neq $ np , there are easily recognizable sets of boolean formulas for which it is hard to determine whether they have a large backbone . we also show that , also under the assumption p $ \\neq $ np , there are easily recognizable families of boolean formulas with strong backdoors that are easy to find , yet for which it is hard to determine whether they are satisfiable ."}
{"title": "kernel truncated regression representation for robust subspace clustering", "abstract": "subspace clustering aims to group data points into multiple clusters of which each corresponds to one subspace . most existing subspace clustering methods assume that the data could be linearly represented with each other in the input space . in practice , however , this assumption is hard to be satisfied . to achieve nonlinear subspace clustering , we propose a novel method which consists of the following three steps : 1 ) projecting the data into a hidden space in which the data can be linearly reconstructed from each other ; 2 ) calculating the globally linear reconstruction coefficients in the kernel space ; 3 ) truncating the trivial coefficients to achieve robustness and block-diagonality , and then achieving clustering by solving a graph laplacian problem . our method has the advantages of a closed-form solution and capacity of clustering data points that lie in nonlinear subspaces . the first advantage makes our method efficient in handling large-scale data sets , and the second one enables the proposed method to address the nonlinear subspace clustering challenge . extensive experiments on five real-world datasets demonstrate the effectiveness and the efficiency of the proposed method in comparison with ten state-of-the-art approaches regarding four evaluation metrics ."}
{"title": "a linear algebraic approach to datalog evaluation", "abstract": "in this paper , we propose a fundamentally new approach to datalog evaluation . given a linear datalog program db written using n constants and binary predicates , we first translate if-and-only-if completions of clauses in db into a set eq ( db ) of matrix equations with a non-linear operation where relations in m_db , the least herbrand model of db , are encoded as adjacency matrices . we then translate eq ( db ) into another , but purely linear matrix equations tilde_eq ( db ) . it is proved that the least solution of tilde_eq ( db ) in the sense of matrix ordering is converted to the least solution of eq ( db ) and the latter gives m_db as a set of adjacency matrices . hence computing the least solution of tilde_eq ( db ) is equivalent to computing m_db specified by db . for a class of tail recursive programs and for some other types of programs , our approach achieves o ( n^3 ) time complexity irrespective of the number of variables in a clause since only matrix operations costing o ( n^3 ) or less are used . we conducted two experiments that compute the least herbrand models of linear datalog programs . the first experiment computes transitive closure of artificial data and real network data taken from the koblenz network collection . the second one compared the proposed approach with the state-of-the-art symbolic systems including two prolog systems and two asp systems , in terms of computation time for a transitive closure program and the same generation program . in the experiment , it is observed that our linear algebraic approach runs 10^1 ~ 10^4 times faster than the symbolic systems when data is not sparse . to appear in theory and practice of logic programming ( tplp ) ."}
{"title": "neuro-fuzzy algorithmic ( nfa ) models and tools for estimation", "abstract": "accurate estimation such as cost estimation , quality estimation and risk analysis is a major issue in management . we propose a patent pending soft computing framework to tackle this challenging problem . our generic framework is independent of the nature and type of estimation . it consists of neural network , fuzzy logic , and an algorithmic estimation model . we made use of the constructive cost model ( cocomo ) , analysis of variance ( anova ) , and function point analysis as the algorithmic models and validated the accuracy of the neuro-fuzzy algorithmic ( nfa ) model in software cost estimation using industrial project data . our model produces more accurate estimation than using an algorithmic model alone . we also discuss the prototypes of our tools that implement the nfa model . we conclude with our roadmap and direction to enrich the model in tackling different estimation challenges ."}
{"title": "universal higher order grammar", "abstract": "we examine the class of languages that can be defined entirely in terms of provability in an extension of the sorted type theory ( ty_n ) by embedding the logic of phonologies , without introduction of special types for syntactic entities . this class is proven to precisely coincide with the class of logically closed languages that may be thought of as functions from expressions to sets of logically equivalent ty_n terms . for a specific sub-class of logically closed languages that are described by finite sets of rules or rule schemata , we find effective procedures for building a compact ty_n representation , involving a finite number of axioms or axiom schemata . the proposed formalism is characterized by some useful features unavailable in a two-component architecture of a language model . a further specialization and extension of the formalism with a context type enable effective account of intensional and dynamic semantics ."}
{"title": "cognitive database : a step towards endowing relational databases with artificial intelligence capabilities", "abstract": "we propose cognitive databases , an approach for transparently enabling artificial intelligence ( ai ) capabilities in relational databases . a novel aspect of our design is to first view the structured data source as meaningful unstructured text , and then use the text to build an unsupervised neural network model using a natural language processing ( nlp ) technique called word embedding . this model captures the hidden inter-/intra-column relationships between database tokens of different types . for each database token , the model includes a vector that encodes contextual semantic relationships . we seamlessly integrate the word embedding model into existing sql query infrastructure and use it to enable a new class of sql-based analytics queries called cognitive intelligence ( ci ) queries . ci queries use the model vectors to enable complex queries such as semantic matching , inductive reasoning queries such as analogies , predictive queries using entities not present in a database , and , more generally , using knowledge from external sources . we demonstrate unique capabilities of cognitive databases using an apache spark based prototype to execute inductive reasoning ci queries over a multi-modal database containing text and images . we believe our first-of-a-kind system exemplifies using ai functionality to endow relational databases with capabilities that were previously very hard to realize in practice ."}
{"title": "the tomaco hybrid matching framework for sawsdl semantic web services", "abstract": "this work aims to resolve issues related to web service retrieval , also known as service selection , discovery or essentially matching , in two directions . firstly , a novel matching algorithm for sawsdl is introduced . the algorithm is hybrid in nature , combining novel and known concepts , such as a logic-based strategy and syntactic text-similarity measures on semantic annotations and textual descriptions . a plugin for the s3 contest environment was developed , in order to position tomaco amongst state-of-the-art in an objective , reproducible manner . evaluation showed that tomaco ranks high amongst state of the art , especially for early recall levels . secondly , this work introduces the tomaco web application , which aims to accelerate the wide-spread adoption of semantic web service technologies and algorithms while targeting the lack of user-friendly applications in this field . tomaco integrates a variety of configurable matching algorithms proposed in this paper . it , finally , allows discovery of both existing and user-contributed service collections and ontologies , serving also as a service registry ."}
{"title": "an adaptive simulated annealing-based satellite observation scheduling method combined with a dynamic task clustering strategy", "abstract": "efficient scheduling is of great significance to rationally make use of scarce satellite resources . task clustering has been demonstrated to realize an effective strategy to improve the efficiency of satellite scheduling . however , the previous task clustering strategy is static . that is , it is integrated into the scheduling in a two-phase manner rather than in a dynamic fashion , without expressing its full potential in improving the satellite scheduling performance . in this study , we present an adaptive simulated annealing based scheduling algorithm aggregated with a dynamic task clustering strategy ( or asa-dtc for short ) for satellite observation scheduling problems ( sosps ) . first , we develop a formal model for the scheduling of earth observing satellites . second , we analyze the related constraints involved in the observation task clustering process . thirdly , we detail an implementation of the dynamic task clustering strategy and the adaptive simulated annealing algorithm . the adaptive simulated annealing algorithm is efficient , with the endowment of some sophisticated mechanisms , i.e . adaptive temperature control , tabu-list based revisiting avoidance mechanism , and intelligent combination of neighborhood structures . finally , we report on experimental simulation studies to demonstrate the competitive performance of asa-dtc . moreover , we show that asa-dtc is especially effective when sosps contain a large number of targets or these targets are densely distributed in a certain area ."}
{"title": "induction of interpretable possibilistic logic theories from relational data", "abstract": "the field of statistical relational learning ( srl ) is concerned with learning probabilistic models from relational data . learned srl models are typically represented using some kind of weighted logical formulas , which make them considerably more interpretable than those obtained by e.g . neural networks . in practice , however , these models are often still difficult to interpret correctly , as they can contain many formulas that interact in non-trivial ways and weights do not always have an intuitive meaning . to address this , we propose a new srl method which uses possibilistic logic to encode relational models . learned models are then essentially stratified classical theories , which explicitly encode what can be derived with a given level of certainty . compared to markov logic networks ( mlns ) , our method is faster and produces considerably more interpretable models ."}
{"title": "verification & validation of agent based simulations using the vomas ( virtual overlay multi-agent system ) approach", "abstract": "agent based models are very popular in a number of different areas . for example , they have been used in a range of domains ranging from modeling of tumor growth , immune systems , molecules to models of social networks , crowds and computer and mobile self-organizing networks . one reason for their success is their intuitiveness and similarity to human cognition . however , with this power of abstraction , in spite of being easily applicable to such a wide number of domains , it is hard to validate agent-based models . in addition , building valid and credible simulations is not just a challenging task but also a crucial exercise to ensure that what we are modeling is , at some level of abstraction , a model of our conceptual system ; the system that we have in mind . in this paper , we address this important area of validation of agent based models by presenting a novel technique which has broad applicability and can be applied to all kinds of agent-based models . we present a framework , where a virtual overlay multi-agent system can be used to validate simulation models . in addition , since agent-based models have been typically growing , in parallel , in multiple domains , to cater for all of these , we present a new single validation technique applicable to all agent based models . our technique , which allows for the validation of agent based simulations uses vomas : a virtual overlay multi-agent system . this overlay multi-agent system can comprise various types of agents , which form an overlay on top of the agent based simulation model that needs to be validated . other than being able to watch and log , each of these agents contains clearly defined constraints , which , if violated , can be logged in real time . to demonstrate its effectiveness , we show its broad applicability in a wide variety of simulation models ranging from social sciences to computer networks in spatial and non-spatial conceptual models ."}
{"title": "ptarithmetic", "abstract": "the present article introduces ptarithmetic ( short for `` polynomial time arithmetic '' ) -- a formal number theory similar to the well known peano arithmetic , but based on the recently born computability logic ( see http : //www.cis.upenn.edu/~giorgi/cl.html ) instead of classical logic . the formulas of ptarithmetic represent interactive computational problems rather than just true/false statements , and their `` truth '' is understood as existence of a polynomial time solution . the system of ptarithmetic elaborated in this article is shown to be sound and complete . sound in the sense that every theorem t of the system represents an interactive number-theoretic computational problem with a polynomial time solution and , furthermore , such a solution can be effectively extracted from a proof of t. and complete in the sense that every interactive number-theoretic problem with a polynomial time solution is represented by some theorem t of the system . the paper is self-contained , and can be read without any previous familiarity with computability logic ."}
{"title": "switching portfolios", "abstract": "a constant rebalanced portfolio is an asset allocation algorithm which keeps the same distribution of wealth among a set of assets along a period of time . recently , there has been work on on-line portfolio selection algorithms which are competitive with the best constant rebalanced portfolio determined in hindsight . by their nature , these algorithms employ the assumption that high returns can be achieved using a fixed asset allocation strategy . however , stock markets are far from being stationary and in many cases the wealth achieved by a constant rebalanced portfolio is much smaller than the wealth achieved by an ad-hoc investment strategy that adapts to changes in the market . in this paper we present an efficient bayesian portfolio selection algorithm that is able to track a changing market . we also describe a simple extension of the algorithm for the case of a general transaction cost , including the transactions cost models recently investigated by blum and kalai . we provide a simple analysis of the competitiveness of the algorithm and check its performance on real stock data from the new york stock exchange accumulated during a 22-year period ."}
{"title": "np-hardness of sortedness constraints", "abstract": "in constraint programming , global constraints allow to model and solve many combinatorial problems . among these constraints , several sortedness constraints have been defined , for which propagation algorithms are available , but for which the tractability is not settled . we show that the sort ( u , v ) constraint ( older et . al , 1995 ) is intractable for integer variables whose domains are not limited to intervals . as a consequence , the similar result holds for the sort ( u , v , p ) constraint ( zhou , 1996 ) . moreover , the intractability holds even under the stability condition present in the recently introduced keysorting ( u , v , keys , p ) constraint ( carlsson et al. , 2014 ) , and requiring that the order of the variables with the same value in the list u be preserved in the list v. therefore , keysorting ( u , v , keys , p ) is intractable as well ."}
{"title": "iterative deepening branch and bound", "abstract": "in tree search problem the best-first search algorithm needs too much of space . to remove such drawbacks of these algorithms the ida* was developed which is both space and time cost efficient . but again ida* can give an optimal solution for real valued problems like flow shop scheduling , travelling salesman and 0/1 knapsack due to their real valued cost estimates . thus further modifications are done on it and the iterative deepening branch and bound search algorithms is developed which meets the requirements . we have tried using this algorithm for the flow shop scheduling problem and have found that it is quite effective ."}
{"title": "the `` psychological map of the brain '' , as a personal information card ( file ) , - a project for the student of the 21st century", "abstract": "we suggest a procedure that is relevant both to electronic performance and human psychology , so that the creative logic and the respect for human nature appear in a good agreement . the idea is to create an electronic card containing basic information about a person 's psychological behavior in order to make it possible to quickly decide about the suitability of one for another . this `` psychological electronics '' approach could be tested via student projects ."}
{"title": "the dynamic of belief in the transferable belief model and specialization-generalization matrices", "abstract": "the fundamental updating process in the transferable belief model is related to the concept of specialization and can be described by a specialization matrix . the degree of belief in the truth of a proposition is a degree of justified support . the principle of minimal commitment implies that one should never give more support to the truth of a proposition than justified . we show that dempster 's rule of conditioning corresponds essentially to the least committed specialization , and that dempster 's rule of combination results essentially from commutativity requirements . the concept of generalization , dual to thc concept of specialization , is described ."}
{"title": "random forests for industrial device functioning diagnostics using wireless sensor networks", "abstract": "in this paper , random forests are proposed for operating devices diagnostics in the presence of a variable number of features . in various contexts , like large or difficult-to-access monitored areas , wired sensor networks providing features to achieve diagnostics are either very costly to use or totally impossible to spread out . using a wireless sensor network can solve this problem , but this latter is more subjected to flaws . furthermore , the networks ' topology often changes , leading to a variability in quality of coverage in the targeted area . diagnostics at the sink level must take into consideration that both the number and the quality of the provided features are not constant , and that some politics like scheduling or data aggregation may be developed across the network . the aim of this article is ( $ 1 $ ) to show that random forests are relevant in this context , due to their flexibility and robustness , and ( $ 2 $ ) to provide first examples of use of this method for diagnostics based on data provided by a wireless sensor network ."}
{"title": "latent collaborative retrieval", "abstract": "retrieval tasks typically require a ranking of items given a query . collaborative filtering tasks , on the other hand , learn to model user 's preferences over items . in this paper we study the joint problem of recommending items to a user with respect to a given query , which is a surprisingly common task . this setup differs from the standard collaborative filtering one in that we are given a query x user x item tensor for training instead of the more traditional user x item matrix . compared to document retrieval we do have a query , but we may or may not have content features ( we will consider both cases ) and we can also take account of the user 's profile . we introduce a factorized model for this new task that optimizes the top-ranked items returned for the given query and user . we report empirical results where it outperforms several baselines ."}
{"title": "integrating probabilistic rules into neural networks : a stochastic em learning algorithm", "abstract": "the em-algorithm is a general procedure to get maximum likelihood estimates if part of the observations on the variables of a network are missing . in this paper a stochastic version of the algorithm is adapted to probabilistic neural networks describing the associative dependency of variables . these networks have a probability distribution , which is a special case of the distribution generated by probabilistic inference networks . hence both types of networks can be combined allowing to integrate probabilistic rules as well as unspecified associations in a sound way . the resulting network may have a number of interesting features including cycles of probabilistic rules , hidden 'unobservable ' variables , and uncertain and contradictory evidence ."}
{"title": "instantaneously trained neural networks", "abstract": "this paper presents a review of instantaneously trained neural networks ( itnns ) . these networks trade learning time for size and , in the basic model , a new hidden node is created for each training sample . various versions of the corner-classification family of itnns , which have found applications in artificial intelligence ( ai ) , are described . implementation issues are also considered ."}
{"title": "parallelized kendall 's tau coefficient computation via simd vectorized sorting on many-integrated-core processors", "abstract": "pairwise association measure is an important operation in data analytics . kendall 's tau coefficient is one widely used correlation coefficient identifying non-linear relationships between ordinal variables . in this paper , we investigated a parallel algorithm accelerating all-pairs kendall 's tau coefficient computation via single instruction multiple data ( simd ) vectorized sorting on intel xeon phis by taking advantage of many processing cores and 512-bit simd vector instructions . to facilitate workload balancing and overcome on-chip memory limitation , we proposed a generic framework for symmetric all-pairs computation by building provable bijective functions between job identifier and coordinate space . performance evaluation demonstrated that our algorithm on one 5110p phi achieves two orders-of-magnitude speedups over 16-threaded matlab and three orders-of-magnitude speedups over sequential r , both running on high-end cpus . besides , our algorithm exhibited rather good distributed computing scalability with respect to number of phis . source code and datasets are publicly available at http : //lightpcc.sourceforge.net ."}
{"title": "extending universal intelligence models with formal notion of representation", "abstract": "solomonoff induction is known to be universal , but incomputable . its approximations , namely , the minimum description ( or message ) length ( mdl ) principles , are adopted in practice in the efficient , but non-universal form . recent attempts to bridge this gap leaded to development of the representational mdl principle that originates from formal decomposition of the task of induction . in this paper , possible extension of the rmdl principle in the context of universal intelligence agents is considered , for which introduction of representations is shown to be an unavoidable meta-heuristic and a step toward efficient general intelligence . hierarchical representations and model optimization with the use of information-theoretic interpretation of the adaptive resonance are also discussed ."}
{"title": "dempster-shafer clustering using potts spin mean field theory", "abstract": "in this article we investigate a problem within dempster-shafer theory where 2**q - 1 pieces of evidence are clustered into q clusters by minimizing a metaconflict function , or equivalently , by minimizing the sum of weight of conflict over all clusters . previously one of us developed a method based on a hopfield and tank model . however , for very large problems we need a method with lower computational complexity . we demonstrate that the weight of conflict of evidence can , as an approximation , be linearized and mapped to an antiferromagnetic potts spin model . this facilitates efficient numerical solution , even for large problem sizes . optimal or nearly optimal solutions are found for dempster-shafer clustering benchmark tests with a time complexity of approximately o ( n**2 log**2 n ) . furthermore , an isomorphism between the antiferromagnetic potts spin model and a graph optimization problem is shown . the graph model has dynamic variables living on the links , which have a priori probabilities that are directly related to the pairwise conflict between pieces of evidence . hence , the relations between three different models are shown ."}
{"title": "causal transportability of experiments on controllable subsets of variables : z-transportability", "abstract": "we introduce z-transportability , the problem of estimating the causal effect of a set of variables x on another set of variables y in a target domain from experiments on any subset of controllable variables z where z is an arbitrary subset of observable variables v in a source domain . z-transportability generalizes z-identifiability , the problem of estimating in a given domain the causal effect of x on y from surrogate experiments on a set of variables z such that z is disjoint from x ; . z-transportability also generalizes transportability which requires that the causal effect of x on y in the target domain be estimable from experiments on any subset of all observable variables in the source domain . we first generalize z-identifiability to allow cases where z is not necessarily disjoint from x. then , we establish a necessary and sufficient condition for z-transportability in terms of generalized z-identifiability and transportability . we provide a correct and complete algorithm that determines whether a causal effect is z-transportable ; and if it is , produces a transport formula , that is , a recipe for estimating the causal effect of x on y in the target domain using information elicited from the results of experimental manipulations of z in the source domain and observational data from the target domain . our results also show that do-calculus is complete for z-transportability ."}
{"title": "reading dependencies from polytree-like bayesian networks", "abstract": "we present a graphical criterion for reading dependencies from the minimal directed independence map g of a graphoid p when g is a polytree and p satisfies composition and weak transitivity . we prove that the criterion is sound and complete . we argue that assuming composition and weak transitivity is not too restrictive ."}
{"title": "a suffix tree approach to email filtering", "abstract": "we present an approach to email filtering based on the suffix tree data structure . a method for the scoring of emails using the suffix tree is developed and a number of scoring and score normalisation functions are tested . our results show that the character level representation of emails and classes facilitated by the suffix tree can significantly improve classification accuracy when compared with the currently popular methods , such as naive bayes . we believe the method can be extended to the classification of documents in other domains ."}
{"title": "smart augmentation - learning an optimal data augmentation strategy", "abstract": "a recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks ( dnn ) . there are many techniques to address this , including data augmentation , dropout , and transfer learning . in this paper , we introduce an additional method which we call smart augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network . smart augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss . this allows us to learn augmentations that minimize the error of that network . smart augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested . in addition , it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases ."}
{"title": "tracking object 's type changes with fuzzy based fusion rule", "abstract": "in this paper the behavior of three combinational rules for temporal/sequential attribute data fusion for target type estimation are analyzed . the comparative analysis is based on : dempster 's fusion rule proposed in dempster-shafer theory ; proportional conflict redistribution rule no . 5 ( pcr5 ) , proposed in dezert-smarandache theory and one alternative class fusion rule , connecting the combination rules for information fusion with particular fuzzy operators , focusing on the t-norm based conjunctive rule as an analog of the ordinary conjunctive rule and t-conorm based disjunctive rule as an analog of the ordinary disjunctive rule . the way how different t-conorms and t-norms functions within tcn fusion rule influence over target type estimation performance is studied and estimated ."}
{"title": "a semantic approach for the requirement-driven discovery of web services in the life sciences", "abstract": "research in the life sciences depends on the integration of large , distributed and heterogeneous data sources and web services . the discovery of which of these resources are the most appropriate to solve a given task is a complex research question , since there is a large amount of plausible candidates and there is little , mostly unstructured , metadata to be able to decide among them.we contribute a semi-automatic approach , based on semantic techniques , to assist researchers in the discovery of the most appropriate web services to full a set of given requirements ."}
{"title": "a computational model to disentangle semantic information embedded in word association norms", "abstract": "two well-known databases of semantic relationships between pairs of words used in psycholinguistics , feature-based and association-based , are studied as complex networks . we propose an algorithm to disentangle feature based relationships from free association semantic networks . the algorithm uses the rich topology of the free association semantic network to produce a new set of relationships between words similar to those observed in feature production norms ."}
{"title": "shapenet : an information-rich 3d model repository", "abstract": "we present shapenet : a richly-annotated , large-scale repository of shapes represented by 3d cad models of objects . shapenet contains 3d models from a multitude of semantic categories and organizes them under the wordnet taxonomy . it is a collection of datasets providing many semantic annotations for each 3d model such as consistent rigid alignments , parts and bilateral symmetry planes , physical sizes , keywords , as well as other planned annotations . annotations are made available through a public web-based interface to enable data visualization of object attributes , promote data-driven geometric analysis , and provide a large-scale quantitative benchmark for research in computer graphics and vision . at the time of this technical report , shapenet has indexed more than 3,000,000 models , 220,000 models out of which are classified into 3,135 categories ( wordnet synsets ) . in this report we describe the shapenet effort as a whole , provide details for all currently available datasets , and summarize future plans ."}
{"title": "helping ai to play hearthstone : aaia'17 data mining challenge", "abstract": "this paper summarizes the aaia'17 data mining challenge : helping ai to play hearthstone which was held between march 23 , and may 15 , 2017 at the knowledge pit platform . we briefly describe the scope and background of this competition in the context of a more general project related to the development of an ai engine for video games , called grail . we also discuss the outcomes of this challenge and demonstrate how predictive models for the assessment of player 's winning chances can be utilized in a construction of an intelligent agent for playing hearthstone . finally , we show a few selected machine learning approaches for modeling state and action values in hearthstone . we provide evaluation for a few promising solutions that may be used to create more advanced types of agents , especially in conjunction with monte carlo tree search algorithms ."}
{"title": "lattice structures of fixed points of the lower approximations of two types of covering-based rough sets", "abstract": "covering is a common type of data structure and covering-based rough set theory is an efficient tool to process this data . lattice is an important algebraic structure and used extensively in investigating some types of generalized rough sets . in this paper , we propose two family of sets and study the conditions that these two sets become some lattice structures . these two sets are consisted by the fixed point of the lower approximations of the first type and the sixth type of covering-based rough sets , respectively . these two sets are called the fixed point set of neighborhoods and the fixed point set of covering , respectively . first , for any covering , the fixed point set of neighborhoods is a complete and distributive lattice , at the same time , it is also a double p-algebra . especially , when the neighborhood forms a partition of the universe , the fixed point set of neighborhoods is both a boolean lattice and a double stone algebra . second , for any covering , the fixed point set of covering is a complete lattice.when the covering is unary , the fixed point set of covering becomes a distributive lattice and a double p-algebra . a distributive lattice and a double p-algebra when the covering is unary . especially , when the reduction of the covering forms a partition of the universe , the fixed point set of covering is both a boolean lattice and a double stone algebra ."}
{"title": "learning what data to learn", "abstract": "machine learning is essentially the sciences of playing with data . an adaptive data selection strategy , enabling to dynamically choose different data at various training stages , can reach a more effective model in a more efficient way . in this paper , we propose a deep reinforcement learning framework , which we call \\emph { \\textbf { n } eural \\textbf { d } ata \\textbf { f } ilter } ( \\textbf { ndf } ) , to explore automatic and adaptive data selection in the training process . in particular , ndf takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data , such that the future accumulative reward ( e.g. , the convergence speed ) is maximized . in contrast to previous studies in data selection that is mainly based on heuristic strategies , ndf is quite generic and thus can be widely suitable for many machine learning tasks . taking neural network training with stochastic gradient descent ( sgd ) as an example , comprehensive experiments with respect to various neural network modeling ( e.g. , multi-layer perceptron networks , convolutional neural networks and recurrent neural networks ) and several applications ( e.g. , image classification and text understanding ) demonstrate that ndf powered sgd can achieve comparable accuracy with standard sgd process by using less data and fewer iterations ."}
{"title": "functional answer set programming", "abstract": "in this paper we propose an extension of answer set programming ( asp ) , and in particular , of its most general logical counterpart , quantified equilibrium logic ( qel ) , to deal with partial functions . although the treatment of equality in qel can be established in different ways , we first analyse the choice of decidable equality with complete functions and herbrand models , recently proposed in the literature . we argue that this choice yields some counterintuitive effects from a logic programming and knowledge representation point of view . we then propose a variant called qelf where the set of functions is partitioned into partial and herbrand functions ( we also call constructors ) . in the rest of the paper , we show a direct connection to scott 's logic of existence and present a practical application , proposing an extension of normal logic programs to deal with partial functions and equality , so that they can be translated into function-free normal programs , being possible in this way to compute their answer sets with any standard asp solver ."}
{"title": "convergent message passing algorithms - a unifying view", "abstract": "message-passing algorithms have emerged as powerful techniques for approximate inference in graphical models . when these algorithms converge , they can be shown to find local ( or sometimes even global ) optima of variational formulations to the inference problem . but many of the most popular algorithms are not guaranteed to converge . this has lead to recent interest in convergent message-passing algorithms . in this paper , we present a unified view of convergent message-passing algorithms . we present a simple derivation of an abstract algorithm , tree-consistency bound optimization ( tcbo ) that is provably convergent in both its sum and max product forms . we then show that many of the existing convergent algorithms are instances of our tcbo algorithm , and obtain novel convergent algorithms `` for free '' by exchanging maximizations and summations in existing algorithms . in particular , we show that wainwright 's non-convergent sum-product algorithm for tree based variational bounds , is actually convergent with the right update order for the case where trees are monotonic chains ."}
{"title": "d numbers theory : a generalization of dempster-shafer theory", "abstract": "dempster-shafer theory is widely applied to uncertainty modelling and knowledge reasoning due to its ability of expressing uncertain information . however , some conditions , such as exclusiveness hypothesis and completeness constraint , limit its development and application to a large extend . to overcome these shortcomings in dempster-shafer theory and enhance its capability of representing uncertain information , a novel theory called d numbers theory is systematically proposed in this paper . within the proposed theory , uncertain information is expressed by d numbers , reasoning and synthesization of information are implemented by d numbers combination rule . the proposed d numbers theory is an generalization of dempster-shafer theory , which inherits the advantage of dempster-shafer theory and strengthens its capability of uncertainty modelling ."}
