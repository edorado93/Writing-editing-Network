{"title": "loopy belief propagation as a basis for communication in sensor networks", "abstract": "sensor networks are an exciting new kind of computer system . consisting of a large number of tiny , cheap computational devices physically distributed in an environment , they gather and process data about the environment in real time . one of the central questions in sensor networks is what to do with the data , i.e. , how to reason with it and how to communicate it . this paper argues that the lessons of the uai community , in particular that one should produce and communicate beliefs rather than raw sensor values , are highly relevant to sensor networks . we contend that loopy belief propagation is particularly well suited to communicating beliefs in sensor networks , due to its compact implementation and distributed nature . we investigate the ability of loopy belief propagation to function under the stressful conditions likely to prevail in sensor networks . our experiments show that it performs well and degrades gracefully . it converges to appropriate beliefs even in highly asynchronous settings where some nodes communicate far less frequently than others ; it continues to function if some nodes fail to participate in the propagation process ; and it can track changes in the environment that occur while beliefs are propagating . as a result , we believe that sensor networks present an important application opportunity for uai ."}
{"title": "learning features by watching objects move", "abstract": "this paper presents a novel yet intuitive approach to unsupervised feature learning . inspired by the human visual system , we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation . specifically , we use unsupervised motion-based segmentation on videos to obtain segments , which we use as 'pseudo ground truth ' to train a convolutional network to segment objects from a single frame . given the extensive evidence that motion plays a key role in the development of the human visual system , we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext ' tasks studied in the literature . indeed , our extensive experiments show that this is the case . when used for transfer learning on object detection , our representation significantly outperforms previous unsupervised approaches across multiple settings , especially when training data for the target task is scarce ."}
{"title": "cognitive memory network", "abstract": "a resistive memory network that has no crossover wiring is proposed to overcome the hardware limitations to size and functional complexity that is associated with conventional analogue neural networks . the proposed memory network is based on simple network cells that are arranged in a hierarchical modular architecture . cognitive functionality of this network is demonstrated by an example of character recognition . the network is trained by an evolutionary process to completely recognise characters deformed by random noise , rotation , scaling and shifting"}
{"title": "belief updating by enumerating high-probability independence-based assignments", "abstract": "independence-based ( ib ) assignments to bayesian belief networks were originally proposed as abductive explanations . ib assignments assign fewer variables in abductive explanations than do schemes assigning values to all evidentially supported variables . we use ib assignments to approximate marginal probabilities in bayesian belief networks . recent work in belief updating for bayes networks attempts to approximate posterior probabilities by finding a small number of the highest probability complete ( or perhaps evidentially supported ) assignments . under certain assumptions , the probability mass in the union of these assignments is sufficient to obtain a good approximation . such methods are especially useful for highly-connected networks , where the maximum clique size or the cutset size make the standard algorithms intractable . since ib assignments contain fewer assigned variables , the probability mass in each assignment is greater than in the respective complete assignment . thus , fewer ib assignments are sufficient , and a good approximation can be obtained more efficiently . ib assignments can be used for efficiently approximating posterior node probabilities even in cases which do not obey the rather strict skewness assumptions used in previous research . two algorithms for finding the high probability ib assignments are suggested : one by doing a best-first heuristic search , and another by special-purpose integer linear programming . experimental results show that this approach is feasible for highly connected belief networks ."}
{"title": "on the complexity of existential positive queries", "abstract": "we systematically investigate the complexity of model checking the existential positive fragment of first-order logic . in particular , for a set of existential positive sentences , we consider model checking where the sentence is restricted to fall into the set ; a natural question is then to classify which sentence sets are tractable and which are intractable . with respect to fixed-parameter tractability , we give a general theorem that reduces this classification question to the corresponding question for primitive positive logic , for a variety of representations of structures . this general theorem allows us to deduce that an existential positive sentence set having bounded arity is fixed-parameter tractable if and only if each sentence is equivalent to one in bounded-variable logic . we then use the lens of classical complexity to study these fixed-parameter tractable sentence sets . we show that such a set can be np-complete , and consider the length needed by a translation from sentences in such a set to bounded-variable logic ; we prove superpolynomial lower bounds on this length using the theory of compilability , obtaining an interesting type of formula size lower bound . overall , the tools , concepts , and results of this article set the stage for the future consideration of the complexity of model checking on more expressive logics ."}
{"title": "unsupervised ranking of multi-attribute objects based on principal curves", "abstract": "unsupervised ranking faces one critical challenge in evaluation applications , that is , no ground truth is available . when pagerank and its variants show a good solution in related subjects , they are applicable only for ranking from link-structure data . in this work , we focus on unsupervised ranking from multi-attribute data which is also common in evaluation tasks . to overcome the challenge , we propose five essential meta-rules for the design and assessment of unsupervised ranking approaches : scale and translation invariance , strict monotonicity , linear/nonlinear capacities , smoothness , and explicitness of parameter size . these meta-rules are regarded as high level knowledge for unsupervised ranking tasks . inspired by the works in [ 8 ] and [ 14 ] , we propose a ranking principal curve ( rpc ) model , which learns a one-dimensional manifold function to perform unsupervised ranking tasks on multi-attribute observations . furthermore , the rpc is modeled to be a cubic b\\'ezier curve with control points restricted in the interior of a hypercube , thereby complying with all the five meta-rules to infer a reasonable ranking list . with control points as the model parameters , one is able to understand the learned manifold and to interpret the ranking list semantically . numerical experiments of the presented rpc model are conducted on two open datasets of different ranking applications . in comparison with the state-of-the-art approaches , the new model is able to show more reasonable ranking lists ."}
{"title": "listen , attend , and walk : neural mapping of navigational instructions to action sequences", "abstract": "we propose a neural sequence-to-sequence model for direction following , a task that is essential to realizing effective autonomous agents . our alignment-based encoder-decoder model with long short-term memory recurrent neural networks ( lstm-rnn ) translates natural language instructions to action sequences based upon a representation of the observable world state . we introduce a multi-level aligner that empowers our model to focus on sentence `` regions '' salient to the current world state by using multiple abstractions of the input sentence . in contrast to existing methods , our model uses no specialized linguistic resources ( e.g. , parsers ) or task-specific annotations ( e.g. , seed lexicons ) . it is therefore generalizable , yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting . we analyze our model through a series of ablations that elucidate the contributions of the primary components of our model ."}
{"title": "algorithm selection for combinatorial search problems : a survey", "abstract": "the algorithm selection problem is concerned with selecting the best algorithm to solve a given problem on a case-by-case basis . it has become especially relevant in the last decade , as researchers are increasingly investigating how to identify the most suitable existing algorithm for solving a problem instead of developing new algorithms . this survey presents an overview of this work focusing on the contributions made in the area of combinatorial search problems , where algorithm selection techniques have achieved significant performance improvements . we unify and organise the vast literature according to criteria that determine algorithm selection systems in practice . the comprehensive classification of approaches identifies and analyses the different directions from which algorithm selection has been approached . this paper contrasts and compares different methods for solving the problem as well as ways of using these solutions . it closes by identifying directions of current and future research ."}
{"title": "a semantic similarity measure for expressive description logics", "abstract": "a totally semantic measure is presented which is able to calculate a similarity value between concept descriptions and also between concept description and individual or between individuals expressed in an expressive description logic . it is applicable on symbolic descriptions although it uses a numeric approach for the calculus . considering that description logics stand as the theoretic framework for the ontological knowledge representation and reasoning , the proposed measure can be effectively used for agglomerative and divisional clustering task applied to the semantic web domain ."}
{"title": "hierarchical compound poisson factorization", "abstract": "non-negative matrix factorization models based on a hierarchical gamma-poisson structure capture user and item behavior effectively in extremely sparse data sets , making them the ideal choice for collaborative filtering applications . hierarchical poisson factorization ( hpf ) in particular has proved successful for scalable recommendation systems with extreme sparsity . hpf , however , suffers from a tight coupling of sparsity model ( absence of a rating ) and response model ( the value of the rating ) , which limits the expressiveness of the latter . here , we introduce hierarchical compound poisson factorization ( hcpf ) that has the favorable gamma-poisson structure and scalability of hpf to high-dimensional extremely sparse matrices . more importantly , hcpf decouples the sparsity model from the response model , allowing us to choose the most suitable distribution for the response . hcpf can capture binary , non-negative discrete , non-negative continuous , and zero-inflated continuous responses . we compare hcpf with hpf on nine discrete and three continuous data sets and conclude that hcpf captures the relationship between sparsity and response better than hpf ."}
{"title": "generalized proportional conflict redistribution rule applied to sonar imagery and radar targets classification", "abstract": "in this chapter , we present two applications in information fusion in order to evaluate the generalized proportional conflict redistribution rule presented in the chapter \\cite { martin06a } . most of the time the combination rules are evaluated only on simple examples . we study here different combination rules and compare them in terms of decision on real data . indeed , in real applications , we need a reliable decision and it is the final results that matter . two applications are presented here : a fusion of human experts opinions on the kind of underwater sediments depict on sonar image and a classifier fusion for radar targets recognition ."}
{"title": "efficient dodgson-score calculation using heuristics and parallel computing", "abstract": "conflict of interest is the permanent companion of any population of agents ( computational or biological ) . for that reason , the ability to compromise is of paramount importance , making voting a key element of societal mechanisms . one of the voting procedures most often discussed in the literature and , due to its intuitiveness , also conceptually quite appealing is charles dodgson 's scoring rule , basically using the respective closeness to being a condorcet winner for evaluating competing alternatives . in this paper , we offer insights on the practical limits of algorithms computing the exact dodgson scores from a number of votes . while the problem itself is theoretically intractable , this work proposes and analyses five different solutions which try distinct approaches to practically solve the issue in an effective manner . additionally , three of the discussed procedures can be run in parallel which has the potential of drastically reducing the problem size ."}
{"title": "local consistency and sat-solvers", "abstract": "local consistency techniques such as k-consistency are a key component of specialised solvers for constraint satisfaction problems . in this paper we show that the power of using k-consistency techniques on a constraint satisfaction problem is precisely captured by using a particular inference rule , which we call negative-hyper-resolution , on the standard direct encoding of the problem into boolean clauses . we also show that current clause-learning sat-solvers will discover in expected polynomial time any inconsistency that can be deduced from a given set of clauses using negative-hyper-resolvents of a fixed size . we combine these two results to show that , without being explicitly designed to do so , current clause-learning sat-solvers efficiently simulate k-consistency techniques , for all fixed values of k. we then give some experimental results to show that this feature allows clause-learning sat-solvers to efficiently solve certain families of constraint problems which are challenging for conventional constraint-programming solvers ."}
{"title": "logical foundations of rdf ( s ) with datatypes", "abstract": "the resource description framework ( rdf ) is a semantic web standard that provides a data language , simply called rdf , as well as a lightweight ontology language , called rdf schema . we investigate embeddings of rdf in logic and show how standard logic programming and description logic technology can be used for reasoning with rdf . we subsequently consider extensions of rdf with datatype support , considering d entailment , defined in the rdf semantics specification , and d* entailment , a semantic weakening of d entailment , introduced by ter horst . we use the embeddings and properties of the logics to establish novel upper bounds for the complexity of deciding entailment . we subsequently establish two novel lower bounds , establishing that rdfs entailment is ptime-complete and that simple-d entailment is conp-hard , when considering arbitrary datatypes , both in the size of the entailing graph . the results indicate that rdfs may not be as lightweight as one may expect ."}
{"title": "slice sampling particle belief propagation", "abstract": "inference in continuous label markov random fields is a challenging task . we use particle belief propagation ( pbp ) for solving the inference problem in continuous label space . sampling particles from the belief distribution is typically done by using metropolis-hastings markov chain monte carlo methods which involves sampling from a proposal distribution . this proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence . we propose to avoid dependence on a proposal distribution by introducing a slice sampling based pbp algorithm . the proposed approach shows superior convergence performance on an image denoising toy example . our findings are validated on a challenging relational 2d feature tracking application ."}
{"title": "un r\u00e9sultat intrigant en commande sans mod\u00e8le", "abstract": "an elementary mathematical example proves , thanks to the routh-hurwitz criterion , a result that is intriguing with respect to today 's practical understanding of model-free control , i.e. , an `` intelligent '' proportional controller ( ip ) may turn to be more difficult to tune than an intelligent proportional-derivative one ( ipd ) . the vast superiority of ipds when compared to classic pids is shown via computer simulations . the introduction as well as the conclusion analyse model-free control in the light of recent advances ."}
{"title": "supervised random walks : predicting and recommending links in social networks", "abstract": "predicting the occurrence of links is a fundamental problem in networks . in the link prediction problem we are given a snapshot of a network and would like to infer which interactions among existing members are likely to occur in the near future or which existing interactions are we missing . although this problem has been extensively studied , the challenge of how to effectively combine the information from the network structure with rich node and edge attribute data remains largely open . we develop an algorithm based on supervised random walks that naturally combines the information from the network structure with node and edge level attributes . we achieve this by using these attributes to guide a random walk on the graph . we formulate a supervised learning task where the goal is to learn a function that assigns strengths to edges in the network such that a random walker is more likely to visit the nodes to which new links will be created in the future . we develop an efficient training algorithm to directly learn the edge strength estimation function . our experiments on the facebook social graph and large collaboration networks show that our approach outperforms state-of-the-art unsupervised approaches as well as approaches that are based on feature extraction ."}
{"title": "monotone retargeting for unsupervised rank aggregation with object features", "abstract": "learning the true ordering between objects by aggregating a set of expert opinion rank order lists is an important and ubiquitous problem in many applications ranging from social choice theory to natural language processing and search aggregation . we study the problem of unsupervised rank aggregation where no ground truth ordering information in available , neither about the true preference ordering between any set of objects nor about the quality of individual rank lists . aggregating the often inconsistent and poor quality rank lists in such an unsupervised manner is a highly challenging problem , and standard consensus-based methods are often ill-defined , and difficult to solve . in this manuscript we propose a novel framework to bypass these issues by using object attributes to augment the standard rank aggregation framework . we design algorithms that learn joint models on both rank lists and object features to obtain an aggregated rank ordering that is more accurate and robust , and also helps weed out rank lists of dubious validity . we validate our techniques on synthetic datasets where our algorithm is able to estimate the true rank ordering even when the rank lists are corrupted . experiments on three real datasets , mq2008 , mq2008 and ohsumed , show that using object features can result in significant improvement in performance over existing rank aggregation methods that do not use object information . furthermore , when at least some of the rank lists are of high quality , our methods are able to effectively exploit their high expertise to output an aggregated rank ordering of great accuracy ."}
{"title": "aba+ : assumption-based argumentation with preferences", "abstract": "we present aba+ , a new approach to handling preferences in a well known structured argumentation formalism , assumption-based argumentation ( aba ) . in aba+ , preference information given over assumptions is incorporated directly into the attack relation , thus resulting in attack reversal . aba+ conservatively extends aba and exhibits various desirable features regarding relationship among argumentation semantics as well as preference handling . we also introduce weak contraposition , a principle concerning reasoning with rules and preferences that relaxes the standard principle of contraposition , while guaranteeing additional desirable features for aba+ ."}
{"title": "an effective training method for deep convolutional neural network", "abstract": "in this paper , we propose the nonlinearity generation method to speed up and stabilize the training of deep convolutional neural networks . the proposed method modifies a family of activation functions as nonlinearity generators ( ngs ) . ngs make the activation functions linear symmetric for their inputs to lower model capacity , and automatically introduce nonlinearity to enhance the capacity of the model during training . the proposed method can be considered an unusual form of regularization : the model parameters are obtained by training a relatively low-capacity model , that is relatively easy to optimize at the beginning , with only a few iterations , and these parameters are reused for the initialization of a higher-capacity model . we derive the upper and lower bounds of variance of the weight variation , and show that the initial symmetric structure of ngs helps stabilize training . we evaluate the proposed method on different frameworks of convolutional neural networks over two object recognition benchmark tasks ( cifar-10 and cifar-100 ) . experimental results showed that the proposed method allows us to ( 1 ) speed up the convergence of training , ( 2 ) allow for less careful weight initialization , ( 3 ) improve or at least maintain the performance of the model at negligible extra computational cost , and ( 4 ) easily train a very deep model ."}
{"title": "between sense and sensibility : declarative narrativisation of mental models as a basis and benchmark for visuo-spatial cognition and computation focussed collaborative cognitive systems", "abstract": "what lies between ` \\emph { sensing } ' and ` \\emph { sensibility } ' ? in other words , what kind of cognitive processes mediate sensing capability , and the formation of sensible impressions -- -e.g. , abstractions , analogies , hypotheses and theory formation , beliefs and their revision , argument formation -- - in domain-specific problem solving , or in regular activities of everyday living , working and simply going around in the environment ? how can knowledge and reasoning about such capabilities , as exhibited by humans in particular problem contexts , be used as a model and benchmark for the development of collaborative cognitive ( interaction ) systems concerned with human assistance , assurance , and empowerment ? we pose these questions in the context of a range of assistive technologies concerned with \\emph { visuo-spatial perception and cognition } tasks encompassing aspects such as commonsense , creativity , and the application of specialist domain knowledge and problem-solving thought processes . assistive technologies being considered include : ( a ) human activity interpretation ; ( b ) high-level cognitive rovotics ; ( c ) people-centred creative design in domains such as architecture & digital media creation , and ( d ) qualitative analyses geographic information systems . computational narratives not only provide a rich cognitive basis , but they also serve as a benchmark of functional performance in our development of computational cognitive assistance systems . we posit that computational narrativisation pertaining to space , actions , and change provides a useful model of \\emph { visual } and \\emph { spatio-temporal thinking } within a wide-range of problem-solving tasks and application areas where collaborative cognitive systems could serve an assistive and empowering function ."}
{"title": "hybrid processing of beliefs and constraints", "abstract": "this paper explores algorithms for processing probabilistic and deterministic information when the former is represented as a belief network and the latter as a set of boolean clauses . the motivating tasks are 1. evaluating beliefs networks having a large number of deterministic relationships and2 . evaluating probabilities of complex boolean querie over a belief network . we propose a parameterized family of variable elimination algorithms that exploit both types of information , and that allows varying levels of constraint propagation inferences . the complexity of the scheme is controlled by the induced-width of the graph { em augmented } by the dependencies introduced by the boolean constraints . preliminary empirical evaluation demonstrate the effect of constraint propagation on probabilistic computation ."}
{"title": "possibility neutrosophic soft sets with applications in decision making and similarity measure", "abstract": "in this paper , concept of possibility neutrosophic soft set and its operations are defined , and their properties are studied . an application of this theory in decision making is investigated . also a similarity measure of two possibility neutrosophic soft sets is introduced and discussed . finally an application of this similarity measure is given to select suitable person for position in a firm ."}
{"title": "deep q-learning from demonstrations", "abstract": "deep reinforcement learning ( rl ) has achieved several high profile successes in difficult decision-making problems . however , these algorithms typically require a huge amount of data before they reach reasonable performance . in fact , their performance during learning can be extremely poor . this may be acceptable for a simulator , but it severely limits the applicability of deep rl to many real-world tasks , where the agent must learn in the real environment . in this paper we study a setting where the agent may access data from previous control of the system . we present an algorithm , deep q-learning from demonstrations ( dqfd ) , that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism . dqfd works by combining temporal difference updates with supervised classification of the demonstrator 's actions . we show that dqfd has better initial performance than prioritized dueling double deep q-networks ( pdd dqn ) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes pdd dqn 83 million steps to catch up to dqfd 's performance . dqfd learns to out-perform the best demonstration given in 14 of 42 games . in addition , dqfd leverages human demonstrations to achieve state-of-the-art results for 11 games . finally , we show that dqfd performs better than three related algorithms for incorporating demonstration data into dqn ."}
{"title": "chalet : cornell house agent learning environment", "abstract": "we present chalet , a 3d house simulator with support for navigation and manipulation . chalet includes 58 rooms and 10 house configuration , and allows to easily create new house and room layouts . chalet supports a range of common household activities , including moving objects , toggling appliances , and placing objects inside closeable containers . the environment and actions available are designed to create a challenging domain to train and evaluate autonomous agents , including for tasks that combine language , vision , and planning in a dynamic environment ."}
{"title": "blockchain and artificial intelligence", "abstract": "it is undeniable that artificial intelligence ( ai ) and blockchain concepts are spreading at a phenomenal rate . both technologies have distinct degree of technological complexity and multi-dimensional business implications . however , a common misunderstanding about blockchain concept , in particular , is that blockchain is decentralized and is not controlled by anyone . but the underlying development of a blockchain system is still attributed to a cluster of core developers . take smart contract as an example , it is essentially a collection of codes ( or functions ) and data ( or states ) that are programmed and deployed on a blockchain ( say , ethereum ) by different human programmers . it is thus , unfortunately , less likely to be free of loopholes and flaws . in this article , through a brief overview about how artificial intelligence could be used to deliver bug-free smart contract so as to achieve the goal of blockchain 2.0 , we to emphasize that the blockchain implementation can be assisted or enhanced via various ai techniques . the alliance of ai and blockchain is expected to create numerous possibilities ."}
{"title": "the computational power of dynamic bayesian networks", "abstract": "this paper considers the computational power of constant size , dynamic bayesian networks . although discrete dynamic bayesian networks are no more powerful than hidden markov models , dynamic bayesian networks with continuous random variables and discrete children of continuous parents are capable of performing turing-complete computation . with modified versions of existing algorithms for belief propagation , such a simulation can be carried out in real time . this result suggests that dynamic bayesian networks may be more powerful than previously considered . relationships to causal models and recurrent neural networks are also discussed ."}
{"title": "exploration of the scalability of locfaults approach for error localization with while-loops programs", "abstract": "a model checker can produce a trace of counterexample , for an erroneous program , which is often long and difficult to understand . in general , the part about the loops is the largest among the instructions in this trace . this makes the location of errors in loops critical , to analyze errors in the overall program . in this paper , we explore the scala-bility capabilities of locfaults , our error localization approach exploiting paths of cfg ( control flow graph ) from a counterexample to calculate the mcds ( minimal correction deviations ) , and mcss ( minimal correction subsets ) from each found mcd . we present the times of our approach on programs with while-loops unfolded b times , and a number of deviated conditions ranging from 0 to n. our preliminary results show that the times of our approach , constraint-based and flow-driven , are better compared to bugassist which is based on sat and transforms the entire program to a boolean formula , and further the information provided by locfaults is more expressive for the user ."}
{"title": "fast low-rank shared dictionary learning for image classification", "abstract": "despite the fact that different objects possess distinct class-specific features , they also usually share common patterns . this observation has been exploited partially in a recently proposed dictionary learning framework by separating the particularity and the commonality ( copar ) . inspired by this , we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification with more intuitive constraints . our dictionary learning framework is hence characterized by both a shared dictionary and particular ( class-specific ) dictionaries . for the shared dictionary , we enforce a low-rank constraint , i.e . claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar . for the particular dictionaries , we impose on them the well-known constraints stated in the fisher discrimination dictionary learning ( fddl ) . further , we develop new fast and accurate algorithms to solve the subproblems in the learning step , accelerating its convergence . the said algorithms could also be applied to fddl and its extensions . the efficiencies of these algorithms are theoretically and experimentally verified by comparing their complexities and running time with those of other well-known dictionary learning methods . experimental results on widely used image datasets establish the advantages of our method over state-of-the-art dictionary learning methods ."}
{"title": "representation of uncertainty for limit processes", "abstract": "many mathematical models utilize limit processes . continuous functions and the calculus , differential equations and topology , all are based on limits and continuity . however , when we perform measurements and computations , we can achieve only approximate results . in some cases , this discrepancy between theoretical schemes and practical actions changes drastically outcomes of a research and decision-making resulting in uncertainty of knowledge . in the paper , a mathematical approach to such kind of uncertainty , which emerges in computation and measurement , is suggested on the base of the concept of a fuzzy limit . a mathematical technique is developed for differential models with uncertainty . to take into account the intrinsic uncertainty of a model , it is suggested to use fuzzy derivatives instead of conventional derivatives of functions in this model ."}
{"title": "dependence and relevance : a probabilistic view", "abstract": "we examine three probabilistic concepts related to the sentence `` two variables have no bearing on each other '' . we explore the relationships between these three concepts and establish their relevance to the process of constructing similarity networks -- -a tool for acquiring probabilistic knowledge from human experts . we also establish a precise relationship between connectedness in bayesian networks and relevance in probability ."}
{"title": "enrichment of qualitative beliefs for reasoning under uncertainty", "abstract": "this paper deals with enriched qualitative belief functions for reasoning under uncertainty and for combining information expressed in natural language through linguistic labels . in this work , two possible enrichments ( quantitative and/or qualitative ) of linguistic labels are considered and operators ( addition , multiplication , division , etc ) for dealing with them are proposed and explained . we denote them $ qe $ -operators , $ qe $ standing for `` qualitative-enriched '' operators . these operators can be seen as a direct extension of the classical qualitative operators ( $ q $ -operators ) proposed recently in the dezert-smarandache theory of plausible and paradoxist reasoning ( dsmt ) . $ q $ -operators are also justified in details in this paper . the quantitative enrichment of linguistic label is a numerical supporting degree in $ [ 0 , \\infty ) $ , while the qualitative enrichment takes its values in a finite ordered set of linguistic values . quantitative enrichment is less precise than qualitative enrichment , but it is expected more close with what human experts can easily provide when expressing linguistic labels with supporting degrees . two simple examples are given to show how the fusion of qualitative-enriched belief assignments can be done ."}
{"title": "learning nested sparse structures in deep neural networks", "abstract": "recently , there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve the inference speed . while many recent works focus on reducing the redundancy by eliminating unneeded weight parameters , it is not possible to apply a single deep architecture for multiple devices with different resources . when a new device or circumstantial condition requires a new deep architecture , it is necessary to construct and train a new network from scratch . in this work , we propose a novel deep learning framework , called a nested sparse network , which exploits an n-in-1-type nested structure in a neural network . a nested sparse network consists of multiple levels of networks with a different sparsity ratio associated with each level , and higher level networks share parameters with lower level networks to enable stable nested learning . the proposed framework realizes a resource-aware versatile architecture as the same network can meet diverse resource requirements . moreover , the proposed nested network can learn different forms of knowledge in its internal networks at different levels , enabling multiple tasks using a single network , such as coarse-to-fine hierarchical classification . in order to train the proposed nested sparse network , we propose efficient weight connection learning and channel and layer scheduling strategies . we evaluate our network in multiple tasks , including adaptive deep compression , knowledge distillation , and learning class hierarchy , and demonstrate that nested sparse networks perform competitively , but more efficiently , than existing methods ."}
{"title": "fractional order fuzzy control of hybrid power system with renewable generation using chaotic pso", "abstract": "this paper investigates the operation of a hybrid power system through a novel fuzzy control scheme . the hybrid power system employs various autonomous generation systems like wind turbine , solar photovoltaic , diesel engine , fuel-cell , aqua electrolyzer etc . other energy storage devices like the battery , flywheel and ultra-capacitor are also present in the network . a novel fractional order ( fo ) fuzzy control scheme is employed and its parameters are tuned with a particle swarm optimization ( pso ) algorithm augmented with two chaotic maps for achieving an improved performance . this fo fuzzy controller shows better performance over the classical pid , and the integer order fuzzy pid controller in both linear and nonlinear operating regimes . the fo fuzzy controller also shows stronger robustness properties against system parameter variation and rate constraint nonlinearity , than that with the other controller structures . the robustness is a highly desirable property in such a scenario since many components of the hybrid power system may be switched on/off or may run at lower/higher power output , at different time instants ."}
{"title": "towards reducing the multidimensionality of olap cubes using the evolutionary algorithms and factor analysis methods", "abstract": "data warehouses are structures with large amount of data collected from heterogeneous sources to be used in a decision support system . data warehouses analysis identifies hidden patterns initially unexpected which analysis requires great memory and computation cost . data reduction methods were proposed to make this analysis easier . in this paper , we present a hybrid approach based on genetic algorithms ( ga ) as evolutionary algorithms and the multiple correspondence analysis ( mca ) as analysis factor methods to conduct this reduction . our approach identifies reduced subset of dimensions from the initial subset p where p ' < p where it is proposed to find the profile fact that is the closest to reference . gas identify the possible subsets and the khi formula of the acm evaluates the quality of each subset . the study is based on a distance measurement between the reference and n facts profile extracted from the warehouses ."}
{"title": "augmenting ordered binary decision diagrams with conjunctive decomposition", "abstract": "this paper augments obdd with conjunctive decomposition to propose a generalization called obdd [ $ \\wedge $ ] . by imposing reducedness and the finest $ \\wedge $ -decomposition bounded by integer $ i $ ( $ \\wedge_ { \\widehat { i } } $ -decomposition ) on obdd [ $ \\wedge $ ] , we identify a family of canonical languages called robdd [ $ \\wedge_ { \\widehat { i } } $ ] , where robdd [ $ \\wedge_ { \\widehat { 0 } } $ ] is equivalent to robdd . we show that the succinctness of robdd [ $ \\wedge_ { \\widehat { i } } $ ] is strictly increasing when $ i $ increases . we introduce a new time-efficiency criterion called rapidity which reflects that exponential operations may be preferable if the language can be exponentially more succinct , and show that : the rapidity of each operation on robdd [ $ \\wedge_ { \\widehat { i } } $ ] is increasing when $ i $ increases ; particularly , the rapidity of some operations ( e.g. , conjoining ) is strictly increasing . finally , our empirical results show that : a ) the size of robdd [ $ \\wedge_ { \\widehat { i } } $ ] is normally not larger than that of the equivalent \\robddc { \\widehat { i+1 } } ; b ) conjoining two robdd [ $ \\wedge_ { \\widehat { 1 } } $ ] s is more efficient than conjoining two robdd [ $ \\wedge_ { \\widehat { 0 } } $ ] s in most cases , where the former is np-hard but the latter is in p ; and c ) the space-efficiency of robdd [ $ \\wedge_ { \\widehat { \\infty } } $ ] is comparable with that of d-dnnf and that of another canonical generalization of \\robdd { } called sdd ."}
{"title": "sample-efficient actor-critic reinforcement learning with supervised data for dialogue management", "abstract": "deep reinforcement learning ( rl ) methods have significant potential for dialogue policy optimisation . however , they suffer from a poor performance in the early stages of learning . this is especially problematic for on-line learning with real users . two approaches are introduced to tackle this problem . firstly , to speed up the learning process , two sample-efficient neural networks algorithms : trust region actor-critic with experience replay ( tracer ) and episodic natural actor-critic with experience replay ( enacer ) are presented . for tracer , the trust region helps to control the learning step size and avoid catastrophic model changes . for enacer , the natural gradient identifies the steepest ascent direction in policy space to speed up the convergence . both models employ off-policy learning with experience replay to improve sample-efficiency . secondly , to mitigate the cold start issue , a corpus of demonstration data is utilised to pre-train the models prior to on-line reinforcement learning . combining these two approaches , we demonstrate a practical approach to learn deep rl-based dialogue policies and demonstrate their effectiveness in a task-oriented information seeking domain ."}
{"title": "achieving compositionality of the stable model semantics for smodels programs", "abstract": "in this paper , a gaifman-shapiro-style module architecture is tailored to the case of smodels programs under the stable model semantics . the composition of smodels program modules is suitably limited by module conditions which ensure the compatibility of the module system with stable models . hence the semantics of an entire smodels program depends directly on stable models assigned to its modules . this result is formalized as a module theorem which truly strengthens lifschitz and turner 's splitting-set theorem for the class of smodels programs . to streamline generalizations in the future , the module theorem is first proved for normal programs and then extended to cover smodels programs using a translation from the latter class of programs to the former class . moreover , the respective notion of module-level equivalence , namely modular equivalence , is shown to be a proper congruence relation : it is preserved under substitutions of modules that are modularly equivalent . principles for program decomposition are also addressed . the strongly connected components of the respective dependency graph can be exploited in order to extract a module structure when there is no explicit a priori knowledge about the modules of a program . the paper includes a practical demonstration of tools that have been developed for automated ( de ) composition of smodels programs . to appear in theory and practice of logic programming ."}
{"title": "some improved results on communication between information systems", "abstract": "to study the communication between information systems , wang et al . [ c. wang , c. wu , d. chen , q. hu , and c. wu , communicating between information systems , information sciences 178 ( 2008 ) 3228-3239 ] proposed two concepts of type-1 and type-2 consistent functions . some properties of such functions and induced relation mappings have been investigated there . in this paper , we provide an improvement of the aforementioned work by disclosing the symmetric relationship between type-1 and type-2 consistent functions . we present more properties of consistent functions and induced relation mappings and improve upon several deficient assertions in the original work . in particular , we unify and extend type-1 and type-2 consistent functions into the so-called neighborhood-consistent functions . this provides a convenient means for studying the communication between information systems based on various neighborhoods ."}
{"title": "improving muc extraction thanks to local search", "abstract": "extractingmucs ( minimalunsatisfiablecores ) fromanunsatisfiable constraint network is a useful process when causes of unsatisfiability must be understood so that the network can be re-engineered and relaxed to become sat- isfiable . despite bad worst-case computational complexity results , various muc- finding approaches that appear tractable for many real-life instances have been proposed . many of them are based on the successive identification of so-called transition constraints . in this respect , we show how local search can be used to possibly extract additional transition constraints at each main iteration step . the approach is shown to outperform a technique based on a form of model rotation imported from the sat-related technology and that also exhibits additional transi- tion constraints . our extensive computational experimentations show that this en- hancement also boosts the performance of state-of-the-art dc ( wcore ) -like muc extractors ."}
{"title": "existence and finiteness conditions for risk-sensitive planning : results and conjectures", "abstract": "decision-theoretic planning with risk-sensitive planning objectives is important for building autonomous agents or decision-support systems for real-world applications . however , this line of research has been largely ignored in the artificial intelligence and operations research communities since planning with risk-sensitive planning objectives is more complicated than planning with risk-neutral planning objectives . to remedy this situation , we derive conditions that guarantee that the optimal expected utilities of the total plan-execution reward exist and are finite for fully observable markov decision process models with non-linear utility functions . in case of markov decision process models with both positive and negative rewards , most of our results hold for stationary policies only , but we conjecture that they can be generalized to non stationary policies ."}
{"title": "outcome-oriented predictive process monitoring : review and benchmark", "abstract": "predictive business process monitoring refers to the act of making predictions about the future state of ongoing cases of a business process , based on their incomplete execution traces and logs of historical ( completed ) traces . motivated by the increasingly pervasive availability of fine-grained event data about business process executions , the problem of predictive process monitoring has received substantial attention in the past years . in particular , a considerable number of methods have been put forward to address the problem of outcome-oriented predictive process monitoring , which refers to classifying each ongoing case of a process according to a given set of possible outcomes - e.g . will the customer complain or not ? will an order be delivered , cancelled or withdrawn ? unfortunately , different authors have used different datasets , experimental settings , evaluation measures and baselines to assess their proposals , resulting in poor comparability and an unclear picture of the relative merits and applicability of different methods . to address this gap , this article presents a systematic review and taxonomy of outcome-oriented predictive process monitoring methods , and a comparative experimental evaluation of eleven representative methods using a benchmark covering twelve predictive process monitoring tasks based on four real-life event logs ."}
{"title": "a constraint satisfaction approach to decision under uncertainty", "abstract": "the constraint satisfaction problem ( csp ) framework offers a simple and sound basis for representing and solving simple decision problems , without uncertainty . this paper is devoted to an extension of the csp framework enabling us to deal with some decisions problems under uncertainty . this extension relies on a differentiation between the agent-controllable decision variables and the uncontrollable parameters whose values depend on the occurrence of uncertain events . the uncertainty on the values of the parameters is assumed to be given under the form of a probability distribution . two algorithms are given , for computing respectively decisions solving the problem with a maximal probability , and conditional decisions mapping the largest possible amount of possible cases to actual decisions ."}
{"title": "the application of differential privacy for rank aggregation : privacy and accuracy", "abstract": "the potential risk of privacy leakage prevents users from sharing their honest opinions on social platforms . this paper addresses the problem of privacy preservation if the query returns the histogram of rankings . the framework of differential privacy is applied to rank aggregation . the error probability of the aggregated ranking is analyzed as a result of noise added in order to achieve differential privacy . upper bounds on the error rates for any positional ranking rule are derived under the assumption that profiles are uniformly distributed . simulation results are provided to validate the probabilistic analysis ."}
{"title": "multi-generator generative adversarial nets", "abstract": "we propose a new approach to train the generative adversarial nets ( gans ) with a mixture of generators to overcome the mode collapsing problem . the main intuition is to employ multiple generators , instead of using a single one as in the original gan . the idea is simple , yet proven to be extremely effective at covering diverse data modes , easily overcoming the mode collapse and delivering state-of-the-art results . a minimax formulation is able to establish among a classifier , a discriminator , and a set of generators in a similar spirit with gan . generators create samples that are intended to come from the same distribution as the training data , whilst the discriminator determines whether samples are true data or generated by generators , and the classifier specifies which generator a sample comes from . the distinguishing feature is that internal samples are created from multiple generators , and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model . we term our method mixture gan ( mgan ) . we develop theoretical analysis to prove that , at the equilibrium , the jensen-shannon divergence ( jsd ) between the mixture of generators ' distributions and the empirical data distribution is minimal , whilst the jsd among generators ' distributions is maximal , hence effectively avoiding the mode collapse . by utilizing parameter sharing , our proposed model adds minimal computational cost to the standard gan , and thus can also efficiently scale to large-scale datasets . we conduct extensive experiments on synthetic 2d data and natural image databases ( cifar-10 , stl-10 and imagenet ) to demonstrate the superior performance of our mgan in achieving state-of-the-art inception scores over latest baselines , generating diverse and appealing recognizable objects at different resolutions , and specializing in capturing different types of objects by generators ."}
{"title": "multi exit configuration of mesoscopic pedestrian simulation", "abstract": "a mesoscopic approach to modeling pedestrian simulation with multiple exits is proposed in this paper . a floor field based on qlearning algorithm is used . attractiveness of exits to pedestrian typically is based on shortest path . however , several factors may influence pedestrian choice of exits . scenarios with multiple exits are presented and effect of q-learning rewards system on navigation is investigated"}
{"title": "improving the asymmetric tsp by considering graph structure", "abstract": "recent works on cost based relaxations have improved constraint programming ( cp ) models for the traveling salesman problem ( tsp ) . we provide a short survey over solving asymmetric tsp with cp . then , we suggest new implied propagators based on general graph properties . we experimentally show that such implied propagators bring robustness to pathological instances and highlight the fact that graph structure can significantly improve search heuristics behavior . finally , we show that our approach outperforms current state of the art results ."}
{"title": "relations between assumption-based approaches in nonmonotonic logic and formal argumentation", "abstract": "in this paper we make a contribution to the unification of formal models of defeasible reasoning . we present several translations between formal argumentation frameworks and nonmonotonic logics for reasoning with plausible assumptions . more specifically , we translate adaptive logics into assumption-based argumentation and aspic+ , aspic+ into assumption-based argumentation and a fragment of assumption-based argumentation into adaptive logics . adaptive logics are closely related to makinson 's default assumptions and to a significant class of systems within the tradition of preferential semantics in the vein of klm and shoham . thus , our results also provide close links between formal argumentation and the latter approaches ."}
{"title": "semantic parsing based on verbal subcategorization", "abstract": "the aim of this work is to explore new methodologies on semantic parsing for unrestricted texts . our approach follows the current trends in information extraction ( ie ) and is based on the application of a verbal subcategorization lexicon ( lexpir ) by means of complex pattern recognition techniques . lexpir is framed on the theoretical model of the verbal subcategorization developed in the pirapides project ."}
{"title": "can turing machine be curious about its turing test results ? three informal lectures on physics of intelligence", "abstract": "what is the nature of curiosity ? is there any scientific way to understand the origin of this mysterious force that drives the behavior of even the stupidest naturally intelligent systems and is completely absent in their smartest artificial analogs ? can we build ai systems that could be curious about something , systems that would have an intrinsic motivation to learn ? is such a motivation quantifiable ? is it implementable ? i will discuss this problem from the standpoint of physics . the relationship between physics and intelligence is a consequence of the fact that correctly predicted information is nothing but an energy resource , and the process of thinking can be viewed as a process of accumulating and spending this resource through the acts of perception and , respectively , decision making . the natural motivation of any autonomous system to keep this accumulation/spending balance as high as possible allows one to treat the problem of describing the dynamics of thinking processes as a resource optimization problem . here i will propose and discuss a simple theoretical model of such an autonomous system which i call the autonomous turing machine ( atm ) . the potential attractiveness of atm lies in the fact that it is the model of a self-propelled ai for which the only available energy resource is the information itself . for atm , the problem of optimal thinking , learning , and decision-making becomes conceptually simple and mathematically well tractable . this circumstance makes the atm an ideal playground for studying the dynamics of intelligent behavior and allows one to quantify many seemingly unquantifiable features of genuine intelligence ."}
{"title": "no-regret learning in extensive-form games with imperfect recall", "abstract": "counterfactual regret minimization ( cfr ) is an efficient no-regret learning algorithm for decision problems modeled as extensive games . cfr 's regret bounds depend on the requirement of perfect recall : players always remember information that was revealed to them and the order in which it was revealed . in games without perfect recall , however , cfr 's guarantees do not apply . in this paper , we present the first regret bound for cfr when applied to a general class of games with imperfect recall . in addition , we show that cfr applied to any abstraction belonging to our general class results in a regret bound not just for the abstract game , but for the full game as well . we verify our theory and show how imperfect recall can be used to trade a small increase in regret for a significant reduction in memory in three domains : die-roll poker , phantom tic-tac-toe , and bluff ."}
{"title": "jointly learning to align and convert graphemes to phonemes with neural attention models", "abstract": "we propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion . most previous work has tackled the problem via joint sequence models that require explicit alignments for training . in contrast , the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes . we explore different types of attention models , including global and local attention , and our best models achieve state-of-the-art results on three standard data sets ( cmudict , pronlex , and nettalk ) ."}
{"title": "learning from the memory of atari 2600", "abstract": "we train a number of neural networks to play games bowling , breakout and seaquest using information stored in the memory of a video game console atari 2600. we consider four models of neural networks which differ in size and architecture : two networks which use only information contained in the ram and two mixed networks which use both information in the ram and information from the screen . as the benchmark we used the convolutional model proposed in nips and received comparable results in all considered games . quite surprisingly , in the case of seaquest we were able to train ram-only agents which behave better than the benchmark screen-only agent . mixing screen and ram did not lead to an improved performance comparing to screen-only and ram-only agents ."}
{"title": "intelligent device discovery in the internet of things - enabling the robot society", "abstract": "the internet of things ( iot ) is continuously growing to connect billions of smart devices anywhere and anytime in an internet-like structure , which enables a variety of applications , services and interactions between human and objects . in the future , the smart devices are supposed to be able to autonomously discover a target device with desired features and generate a set of entirely new services and applications that are not supervised or even imagined by human beings . the pervasiveness of smart devices , as well as the heterogeneity of their design and functionalities , raise a major concern : how can a smart device efficiently discover a desired target device ? in this paper , we propose a social-aware and distributed ( sand ) scheme that achieves a fast , scalable and efficient device discovery in the iot . the proposed sand scheme adopts a novel device ranking criteria that measures the device 's degree , social relationship diversity , clustering coefficient and betweenness . based on the device ranking criteria , the discovery request can be guided to travel through critical devices that stand at the major intersections of the network , and thus quickly reach the desired target device by contacting only a limited number of intermediate devices . with the help of such an intelligent device discovery as sand , the iot devices , as well as other computing facilities , software and data on the internet , can autonomously establish new social connections with each other as human being do . they can formulate self-organized computing groups to perform required computing tasks , facilitate a fusion of a variety of computing service , network service and data to generate novel applications and services , evolve from the individual aritificial intelligence to the collaborative intelligence , and eventually enable the birth of a robot society ."}
{"title": "how to correctly prune tropical trees", "abstract": "we present tropical games , a generalization of combinatorial min-max games based on tropical algebras . our model breaks the traditional symmetry of rational zero-sum games where players have exactly opposed goals ( min vs. max ) , is more widely applicable than min-max and also supports a form of pruning , despite it being less effective than alpha-beta . actually , min-max games may be seen as particular cases where both the game and its dual are tropical : when the dual of a tropical game is also tropical , the power of alpha-beta is completely recovered . we formally develop the model and prove that the tropical pruning strategy is correct , then conclude by showing how the problem of approximated parsing can be modeled as a tropical game , profiting from pruning ."}
{"title": "plug and play ! a simple , universal model for energy disaggregation", "abstract": "energy disaggregation is to discover the energy consumption of individual appliances from their aggregated energy values . to solve the problem , most existing approaches rely on either appliances ' signatures or their state transition patterns , both hard to obtain in practice . aiming at developing a simple , universal model that works without depending on sophisticated machine learning techniques or auxiliary equipments , we make use of easily accessible knowledge of appliances and the sparsity of the switching events to design a sparse switching event recovering ( sser ) method . by minimizing the total variation ( tv ) of the ( sparse ) event matrix , sser can effectively recover the individual energy consumption values from the aggregated ones . to speed up the process , a parallel local optimization algorithm ( ploa ) is proposed to solve the problem in active epochs of appliance activities in parallel . using real-world trace data , we compare the performance of our method with that of the state-of-the-art solutions , including least square estimation ( lse ) and iterative hidden markov model ( hmm ) . the results show that our approach has an overall higher detection accuracy and a smaller overhead ."}
{"title": "a novel method for stock forecasting based on fuzzy time series combined with the longest common/repeated sub-sequence", "abstract": "stock price forecasting is an important issue for investors since extreme accuracy in forecasting can bring about high profits . fuzzy time series ( fts ) and longest common/repeated sub-sequence ( lcs/lrs ) are two important issues for forecasting prices . however , to the best of our knowledge , there are no significant studies using lcs/lrs to predict stock prices . it is impossible that prices stay exactly the same as historic prices . therefore , this paper proposes a state-of-the-art method which combines fts and lcs/lrs to predict stock prices . this method is based on the principle that history will repeat itself . it uses different interval lengths in fts to fuzzify the prices , and lcs/lrs to look for the same pattern in the historical prices to predict future stock prices . in the experiment , we examine various intervals of fuzzy time sets in order to achieve high prediction accuracy . the proposed method outperforms traditional methods in terms of prediction accuracy and , furthermore , it is easy to implement ."}
{"title": "efficient inference in fully connected crfs with gaussian edge potentials", "abstract": "most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions . while region-level models often feature dense pairwise connectivity , pixel-level models are considerably larger and have only permitted sparse graph structures . in this paper , we consider fully connected crf models defined on the complete set of pixels in an image . the resulting graphs have billions of edges , making traditional inference algorithms impractical . our main contribution is a highly efficient approximate inference algorithm for fully connected crf models in which the pairwise edge potentials are defined by a linear combination of gaussian kernels . our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy ."}
{"title": "accurately and efficiently interpreting human-robot instructions of varying granularities", "abstract": "humans can ground natural language commands to tasks at both abstract and fine-grained levels of specificity . for instance , a human forklift operator can be instructed to perform a high-level action , like `` grab a pallet '' or a lowlevel action like `` tilt back a little bit . '' while robots are also capable of grounding language commands to tasks , previous methods implicitly assume that all commands and tasks reside at a single , fixed level of abstraction . additionally , those approaches that do not use abstraction experience inefficient planning and execution times due to the large , intractable state-action spaces , which closely resemble real world complexity . in this work , by grounding commands to all the tasks or subtasks available in a hierarchical planning framework , we arrive at a model capable of interpreting language at multiple levels of specificity ranging from coarse to more granular . we show that the accuracy of the grounding procedure is improved when simultaneously inferring the degree of abstraction in language used to communicate the task . leveraging hierarchy also improves efficiency : our proposed approach enables a robot to respond to a command within one second on 90 % of our tasks , while baselines take over twenty seconds on half the tasks . finally , we demonstrate that a real , physical robot can ground commands at multiple levels of abstraction allowing it to efficiently plan different subtasks within the same planning hierarchy ."}
{"title": "order to disorder transitions in hybrid intelligent systems : a hatch to the interactions of nations -governments", "abstract": "in this study , under general frame of many connected intelligent particles systems ( macips ) , we reproduce two new simple subsets of such intelligent complex network , namely hybrid intelligent systems , involved a few prominent intelligent computing and approximate reasoning methods : self organizing feature map ( som ) , neuro-fuzzy inference system and rough set theory ( rst ) . over this , we show how our algorithms can be construed as a linkage of government-society interaction , where government catches various fashions of behavior : solid ( absolute ) or flexible . so , transition of such society , by changing of connectivity parameters ( noise ) from order to disorder is inferred . add to this , one may find an indirect mapping among financial systems and eventual market fluctuations with macips ."}
{"title": "neural networks for information retrieval", "abstract": "machine learning plays a role in many aspects of modern ir systems , and deep learning is applied in all of them . the fast pace of modern-day research has given rise to many different approaches for many different ir problems . the amount of information available can be overwhelming both for junior students and for experienced researchers looking for new research topics and directions . additionally , it is interesting to see what key insights into ir problems the new technologies are able to give us . the aim of this full-day tutorial is to give a clear overview of current tried-and-trusted neural methods in ir and how they benefit ir research . it covers key architectures , as well as the most promising future directions ."}
{"title": "identifying diabetic patients with high risk of readmission", "abstract": "hospital readmissions are expensive and reflect the inadequacies in healthcare system . in the united states alone , treatment of readmitted diabetic patients exceeds 250 million dollars per year . early identification of patients facing a high risk of readmission can enable healthcare providers to to conduct additional investigations and possibly prevent future readmissions . this not only improves the quality of care but also reduces the medical expenses on readmission . machine learning methods have been leveraged on public health data to build a system for identifying diabetic patients facing a high risk of future readmission . number of inpatient visits , discharge disposition and admission type were identified as strong predictors of readmission . further , it was found that the number of laboratory tests and discharge disposition together predict whether the patient will be readmitted shortly after being discharged from the hospital ( i.e . < 30 days ) or after a longer period of time ( i.e . > 30 days ) . these insights can help healthcare providers to improve inpatient diabetic care . finally , the cost analysis suggests that \\ $ 252.76 million can be saved across 98,053 diabetic patient encounters by incorporating the proposed cost sensitive analysis model ."}
{"title": "x.ent : r package for entities and relations extraction based on unsupervised learning and document structure", "abstract": "relation extraction with accurate precision is still a challenge when processing full text databases . we propose an approach based on cooccurrence analysis in each document for which we used document organization to improve accuracy of relation extraction . this approach is implemented in a r package called \\emph { x.ent } . another facet of extraction relies on use of extracted relation into a querying system for expert end-users . two datasets had been used . one of them gets interest from specialists of epidemiology in plant health . for this dataset usage is dedicated to plant-disease exploration through agricultural information news . an open-data platform exploits exports from \\emph { x.ent } and is publicly available ."}
{"title": "modelling creativity : identifying key components through a corpus-based approach", "abstract": "creativity is a complex , multi-faceted concept encompassing a variety of related aspects , abilities , properties and behaviours . if we wish to study creativity scientifically , then a tractable and well-articulated model of creativity is required . such a model would be of great value to researchers investigating the nature of creativity and in particular , those concerned with the evaluation of creative practice . this paper describes a unique approach to developing a suitable model of how creative behaviour emerges that is based on the words people use to describe the concept . using techniques from the field of statistical natural language processing , we identify a collection of fourteen key components of creativity through an analysis of a corpus of academic papers on the topic . words are identified which appear significantly often in connection with discussions of the concept . using a measure of lexical similarity to help cluster these words , a number of distinct themes emerge , which collectively contribute to a comprehensive and multi-perspective model of creativity . the components provide an ontology of creativity : a set of building blocks which can be used to model creative practice in a variety of domains . the components have been employed in two case studies to evaluate the creativity of computational systems and have proven useful in articulating achievements of this work and directions for further research ."}
{"title": "hierarchical learning algorithm for the beta basis function neural network", "abstract": "the paper presents a two-level learning method for the design of the beta basis function neural network bbfnn . a genetic algorithm is employed at the upper level to construct bbfnn , while the key learning parameters : the width , the centers and the beta form are optimised using the gradient algorithm at the lower level . in order to demonstrate the effectiveness of this hierarchical learning algorithm hlabbfnn , we need to validate our algorithm for the approximation of non-linear function ."}
{"title": "random forest models of the retention constants in the thin layer chromatography", "abstract": "in the current study we examine an application of the machine learning methods to model the retention constants in the thin layer chromatography ( tlc ) . this problem can be described with hundreds or even thousands of descriptors relevant to various molecular properties , most of them redundant and not relevant for the retention constant prediction . hence we employed feature selection to significantly reduce the number of attributes . additionally we have tested application of the bagging procedure to the feature selection . the random forest regression models were built using selected variables . the resulting models have better correlation with the experimental data than the reference models obtained with linear regression . the cross-validation confirms robustness of the models ."}
{"title": "use of ensembles of fourier spectra in capturing recurrent concepts in data streams", "abstract": "in this research , we apply ensembles of fourier encoded spectra to capture and mine recurring concepts in a data stream environment . previous research showed that compact versions of decision trees can be obtained by applying the discrete fourier transform to accurately capture recurrent concepts in a data stream . however , in highly volatile environments where new concepts emerge often , the approach of encoding each concept in a separate spectrum is no longer viable due to memory overload and thus in this research we present an ensemble approach that addresses this problem . our empirical results on real world data and synthetic data exhibiting varying degrees of recurrence reveal that the ensemble approach outperforms the single spectrum approach in terms of classification accuracy , memory and execution time ."}
{"title": "extend commitment protocols with temporal regulations : why and how", "abstract": "the proposal of elisa marengo 's thesis is to extend commitment protocols to explicitly account for temporal regulations . this extension will satisfy two needs : ( 1 ) it will allow representing , in a flexible and modular way , temporal regulations with a normative force , posed on the interaction , so as to represent conventions , laws and suchlike ; ( 2 ) it will allow committing to complex conditions , which describe not only what will be achieved but to some extent also how . these two aspects will be deeply investigated in the proposal of a unified framework , which is part of the ongoing work and will be included in the thesis ."}
{"title": "autonomous grounding of visual field experience through sensorimotor prediction", "abstract": "in a developmental framework , autonomous robots need to explore the world and learn how to interact with it . without an a priori model of the system , this opens the challenging problem of having robots master their interface with the world : how to perceive their environment using their sensors , and how to act in it using their motors . the sensorimotor approach of perception claims that a naive agent can learn to master this interface by capturing regularities in the way its actions transform its sensory inputs . in this paper , we apply such an approach to the discovery and mastery of the visual field associated with a visual sensor . a computational model is formalized and applied to a simulated system to illustrate the approach ."}
{"title": "temporal description logic for ontology-based data access ( extended version )", "abstract": "our aim is to investigate ontology-based data access over temporal data with validity time and ontologies capable of temporal conceptual modelling . to this end , we design a temporal description logic , tql , that extends the standard ontology language owl 2 ql , provides basic means for temporal conceptual modelling and ensures first-order rewritability of conjunctive queries for suitably defined data instances with validity time ."}
{"title": "dynamic adjustment of the motivation degree in an action selection mechanism", "abstract": "this paper presents a model for dynamic adjustment of the motivation degree , using a reinforcement learning approach , in an action selection mechanism previously developed by the authors . the learning takes place in the modification of a parameter of the model of combination of internal and external stimuli . experiments that show the claimed properties are presented , using a vr simulation developed for such purposes . the importance of adaptation by learning in action selection is also discussed ."}
{"title": "search-based methods to bound diagnostic probabilities in very large belief nets", "abstract": "since exact probabilistic inference is intractable in general for large multiply connected belief nets , approximate methods are required . a promising approach is to use heuristic search among hypotheses ( instantiations of the network ) to find the most probable ones , as in the topn algorithm . search is based on the relative probabilities of hypotheses which are efficient to compute . given upper and lower bounds on the relative probability of partial hypotheses , it is possible to obtain bounds on the absolute probabilities of hypotheses . best-first search aimed at reducing the maximum error progressively narrows the bounds as more hypotheses are examined . here , qualitative probabilistic analysis is employed to obtain bounds on the relative probability of partial hypotheses for the bn20 class of networks networks and a generalization replacing the noisy or assumption by negative synergy . the approach is illustrated by application to a very large belief network , qmr-bn , which is a reformulation of the internist-1 system for diagnosis in internal medicine ."}
{"title": "co-attending free-form regions and detections with multi-modal multiplicative feature embedding for visual question answering", "abstract": "recently , the visual question answering ( vqa ) task has gained increasing attention in artificial intelligence . existing vqa methods mainly adopt the visual attention mechanism to associate the input question with corresponding image regions for effective question answering . the free-form region based and the detection-based visual attention mechanisms are mostly investigated , with the former ones attending free-form image regions and the latter ones attending pre-specified detection-box regions . we argue that the two attention mechanisms are able to provide complementary information and should be effectively integrated to better solve the vqa problem . in this paper , we propose a novel deep neural network for vqa that integrates both attention mechanisms . our proposed framework effectively fuses features from free-form image regions , detection boxes , and question representations via a multi-modal multiplicative feature embedding scheme to jointly attend question-related free-form image regions and detection boxes for more accurate question answering . the proposed method is extensively evaluated on two publicly available datasets , coco-qa and vqa , and outperforms state-of-the-art approaches . source code is available at https : //github.com/lupantech/dual-mfa-vqa ."}
{"title": "unsupervised learning human 's activities by overexpressed recognized non-speech sounds", "abstract": "human activity and environment produces sounds such as , at home , the noise produced by water , cough , or television . these sounds can be used to determine the activity in the environment . the objective is to monitor a person 's activity or determine his environment using a single low cost microphone by sound analysis . the purpose is to adapt programs to the activity or environment or detect abnormal situations . some patterns of over expressed repeatedly in the sequences of recognized sounds inter and intra environment allow to characterize activities such as the entrance of a person in the house , or a tv program watched . we first manually annotated 1500 sounds of daily life activity of old persons living at home recognized sounds . then we inferred an ontology and enriched the database of annotation with a crowed sourced manual annotation of 7500 sounds to help with the annotation of the most frequent sounds . using learning sound algorithms , we defined 50 types of the most frequent sounds . we used this set of recognizable sounds as a base to tag sounds and put tags on them . by using over expressed number of motifs of sequences of the tags , we were able to categorize using only a single low-cost microphone , complex activities of daily life of a persona at home as watching tv , entrance in the apartment of a person , or phone conversation including detecting unknown activities as repeated tasks performed by users ."}
{"title": "parametric constructive kripke-semantics for standard multi-agent belief and knowledge ( knowledge as unbiased belief )", "abstract": "we propose parametric constructive kripke-semantics for multi-agent kd45-belief and s5-knowledge in terms of elementary set-theoretic constructions of two basic functional building blocks , namely bias ( or viewpoint ) and visibility , functioning also as the parameters of the doxastic and epistemic accessibility relation . the doxastic accessibility relates two possible worlds whenever the application of the composition of bias with visibility to the first world is equal to the application of visibility to the second world . the epistemic accessibility is the transitive closure of the union of our doxastic accessibility and its converse . therefrom , accessibility relations for common and distributed belief and knowledge can be constructed in a standard way . as a result , we obtain a general definition of knowledge in terms of belief that enables us to view s5-knowledge as accurate ( unbiased and thus true ) kd45-belief , negation-complete belief and knowledge as exact kd45-belief and s5-knowledge , respectively , and perfect s5-knowledge as precise ( exact and accurate ) kd45-belief , and all this generically for arbitrary functions of bias and visibility . our results can be seen as a semantic complement to previous foundational results by halpern et al . about the ( un ) definability and ( non- ) reducibility of knowledge in terms of and to belief , respectively ."}
{"title": "logical hidden markov models", "abstract": "logical hidden markov models ( lohmms ) upgrade traditional hidden markov models to deal with sequences of structured symbols in the form of logical atoms , rather than flat characters . this note formally introduces lohmms and presents solutions to the three central inference problems for lohmms : evaluation , most likely hidden state sequence and parameter estimation . the resulting representation and algorithms are experimentally evaluated on problems from the domain of bioinformatics ."}
{"title": "representation and coding of signal geometry", "abstract": "approaches to signal representation and coding theory have traditionally focused on how to best represent signals using parsimonious representations that incur the lowest possible distortion . classical examples include linear and non-linear approximations , sparse representations , and rate-distortion theory . very often , however , the goal of processing is to extract specific information from the signal , and the distortion should be measured on the extracted information . the corresponding representation should , therefore , represent that information as parsimoniously as possible , without necessarily accurately representing the signal itself . in this paper , we examine the problem of encoding signals such that sufficient information is preserved about their pairwise distances and their inner products . for that goal , we consider randomized embeddings as an encoding mechanism and provide a framework to analyze their performance . we also demonstrate that it is possible to design the embedding such that it represents different ranges of distances with different precision . these embeddings also allow the computation of kernel inner products with control on their inner product-preserving properties . our results provide a broad framework to design and analyze embeddins , and generalize existing results in this area , such as random fourier kernels and universal embeddings ."}
{"title": "knowledge graph completion via complex tensor factorization", "abstract": "in statistical relational learning , knowledge graph completion deals with automatically understanding the structure of large knowledge graphs -- -labeled directed graphs -- -and predicting missing relationships -- -labeled edges . state-of-the-art embedding models propose different trade-offs between modeling expressiveness , and time and space complexity . we reconcile both expressiveness and complexity through the use of complex-valued embeddings and explore the link between such complex-valued embeddings and unitary diagonalization . we corroborate our approach theoretically and show that all real square matrices -- -thus all possible relation/adjacency matrices -- -are the real part of some unitarily diagonalizable matrix . this results opens the door to a lot of other applications of square matrices factorization . our approach based on complex embeddings is arguably simple , as it only involves a hermitian dot product , the complex counterpart of the standard dot product between real vectors , whereas other methods resort to more and more complicated composition functions to increase their expressiveness . the proposed complex embeddings are scalable to large data sets as it remains linear in both space and time , while consistently outperforming alternative approaches on standard link prediction benchmarks ."}
{"title": "learning multi-modal word representation grounded in visual context", "abstract": "representing the semantics of words is a long-standing problem for the natural language processing community . most methods compute word semantics given their textual context in large corpora . more recently , researchers attempted to integrate perceptual and visual features . most of these works consider the visual appearance of objects to enhance word representations but they ignore the visual environment and context in which objects appear . we propose to unify text-based techniques with vision-based techniques by simultaneously leveraging textual and visual context to learn multimodal word embeddings . we explore various choices for what can serve as a visual context and present an end-to-end method to integrate visual context elements in a multimodal skip-gram model . we provide experiments and extensive analysis of the obtained results ."}
{"title": "semi-supervised hierarchical semantic object parsing", "abstract": "models based on convolutional neural networks ( cnns ) have been proven very successful for semantic segmentation and object parsing that yield hierarchies of features . our key insight is to build convolutional networks that take input of arbitrary size and produce object parsing output with efficient inference and learning . in this work , we focus on the task of instance segmentation and parsing which recognizes and localizes objects down to a pixel level base on deep cnn . therefore , unlike some related work , a pixel can not belong to multiple instances and parsing . our model is based on a deep neural network trained for object masking that supervised with input image and follow incorporates a conditional random field ( crf ) with end-to-end trainable piecewise order potentials based on object parsing outputs . in each crf unit we designed terms to capture the short range and long range dependencies from various neighbors . the accurate instance-level segmentation that our network produce is reflected by the considerable improvements obtained over previous work at high apr thresholds . we demonstrate the effectiveness of our model with extensive experiments on challenging dataset subset of pascal voc2012 ."}
{"title": "encoding monotonic multi-set preferences using ci-nets : preliminary report", "abstract": "cp-nets and their variants constitute one of the main ai approaches for specifying and reasoning about preferences . ci-nets , in particular , are a cp-inspired formalism for representing ordinal preferences over sets of goods , which are typically required to be monotonic . considering also that goods often come in multi-sets rather than sets , a natural question is whether ci-nets can be used more or less directly to encode preferences over multi-sets . we here provide some initial ideas on how to achieve this , in the sense that at least a restricted form of reasoning on our framework , which we call `` confined reasoning '' , can be efficiently reduced to reasoning on ci-nets . our framework nevertheless allows for encoding preferences over multi-sets with unbounded multiplicities . we also show the extent to which it can be used to represent preferences where multiplicites of the goods are not stated explicitly ( `` purely qualitative preferences '' ) as well as a potential use of our generalization of ci-nets as a component of a recent system for evidence aggregation ."}
{"title": "comparing rule-based and deep learning models for patient phenotyping", "abstract": "objective : we investigate whether deep learning techniques for natural language processing ( nlp ) can be used efficiently for patient phenotyping . patient phenotyping is a classification task for determining whether a patient has a medical condition , and is a crucial part of secondary analysis of healthcare data . we assess the performance of deep learning algorithms and compare them with classical nlp approaches . materials and methods : we compare convolutional neural networks ( cnns ) , n-gram models , and approaches based on ctakes that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes . the performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the mimic-iii database . results : cnns outperform other phenotyping algorithms in all 10 tasks . the average f1-score of our model is 76 ( ppv of 83 , and sensitivity of 71 ) with our model having an f1-score up to 37 points higher than alternative approaches . we additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction . conclusion : we show that nlp methods based on deep learning improve the performance of patient phenotyping . our cnn-based algorithm automatically learns the phrases associated with each patient phenotype . as such , it reduces the annotation complexity for clinical domain experts , who are normally required to develop task-specific annotation rules and identify relevant phrases . our method performs well in terms of both performance and interpretability , which indicates that deep learning is an effective approach to patient phenotyping based on clinicians ' notes ."}
{"title": "full first-order sequent and tableau calculi with preservation of solutions and the liberalized delta-rule but without skolemization", "abstract": "we present a combination of raising , explicit variable dependency representation , the liberalized delta-rule , and preservation of solutions for first-order deductive theorem proving . our main motivation is to provide the foundation for our work on inductive theorem proving , where the preservation of solutions is indispensable ."}
{"title": "lazily adapted constant kinky inference for nonparametric regression and model-reference adaptive control", "abstract": "techniques known as nonlinear set membership prediction , lipschitz interpolation or kinky inference are approaches to machine learning that utilise presupposed lipschitz properties to compute inferences over unobserved function values . provided a bound on the true best lipschitz constant of the target function is known a priori they offer convergence guarantees as well as bounds around the predictions . considering a more general setting that builds on hoelder continuity relative to pseudo-metrics , we propose an online method for estimating the hoelder constant online from function value observations that possibly are corrupted by bounded observational errors . utilising this to compute adaptive parameters within a kinky inference rule gives rise to a nonparametric machine learning method , for which we establish strong universal approximation guarantees . that is , we show that our prediction rule can learn any continuous function in the limit of increasingly dense data to within a worst-case error bound that depends on the level of observational uncertainty . we apply our method in the context of nonparametric model-reference adaptive control ( mrac ) . across a range of simulated aircraft roll-dynamics and performance metrics our approach outperforms recently proposed alternatives that were based on gaussian processes and rbf-neural networks . for discrete-time systems , we provide stability guarantees for our learning-based controllers both for the batch and the online learning setting ."}
{"title": "complexity and compilation of gz-aggregates in answer set programming", "abstract": "gelfond and zhang recently proposed a new stable model semantics based on vicious circle principle in order to improve the interpretation of logic programs with aggregates . the paper focuses on this proposal , and analyzes the complexity of both coherence testing and cautious reasoning under the new semantics . some surprising results highlight similarities and differences versus mainstream stable model semantics for aggregates . moreover , the paper reports on the design of compilation techniques for implementing the new semantics on top of existing asp solvers , which eventually lead to realize a prototype system that allows for experimenting with gelfond-zhang 's aggregates . to appear in theory and practice of logic programming ( tplp ) , proceedings of iclp 2015 ."}
{"title": "fighting sample degeneracy and impoverishment in particle filters : a review of intelligent approaches", "abstract": "during the last two decades there has been a growing interest in particle filtering ( pf ) . however , pf suffers from two long-standing problems that are referred to as sample degeneracy and impoverishment . we are investigating methods that are particularly efficient at particle distribution optimization ( pdo ) to fight sample degeneracy and impoverishment , with an emphasis on intelligence choices . these methods benefit from such methods as markov chain monte carlo methods , mean-shift algorithms , artificial intelligence algorithms ( e.g. , particle swarm optimization , genetic algorithm and ant colony optimization ) , machine learning approaches ( e.g. , clustering , splitting and merging ) and their hybrids , forming a coherent standpoint to enhance the particle filter . the working mechanism , interrelationship , pros and cons of these approaches are provided . in addition , approaches that are effective for dealing with high-dimensionality are reviewed . while improving the filter performance in terms of accuracy , robustness and convergence , it is noted that advanced techniques employed in pf often causes additional computational requirement that will in turn sacrifice improvement obtained in real life filtering . this fact , hidden in pure simulations , deserves the attention of the users and designers of new filters ."}
{"title": "crowd behavior analysis : a review where physics meets biology", "abstract": "although the traits emerged in a mass gathering are often non-deliberative , the act of mass impulse may lead to irre- vocable crowd disasters . the two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision , towards effective and proactive crowd surveillance . computer vision stud- ies related to crowd are observed to resonate with the understanding of the emergent behavior in physics ( complex systems ) and biology ( animal swarm ) . these studies , which are inspired by biology and physics , share surprisingly common insights , and interesting contradictions . however , this aspect of discussion has not been fully explored . therefore , this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives . we provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision ."}
{"title": "options discovery with budgeted reinforcement learning", "abstract": "we consider the problem of learning hierarchical policies for reinforcement learning able to discover options , an option corresponding to a sub-policy over a set of primitive actions . different models have been proposed during the last decade that usually rely on a predefined set of options . we specifically address the problem of automatically discovering options in decision processes . we describe a new learning model called budgeted option neural network ( bonn ) able to discover options based on a budgeted learning objective . the bonn model is evaluated on different classical rl problems , demonstrating both quantitative and qualitative interesting results ."}
{"title": "limbo : a fast and flexible library for bayesian optimization", "abstract": "limbo is an open-source c++11 library for bayesian optimization which is designed to be both highly flexible and very fast . it can be used to optimize functions for which the gradient is unknown , evaluations are expensive , and runtime cost matters ( e.g. , on embedded systems or robots ) . benchmarks on standard functions show that limbo is about 2 times faster than bayesopt ( another c++ library ) for a similar accuracy ."}
{"title": "towards using unlabeled data in a sparse-coding framework for human activity recognition", "abstract": "we propose a sparse-coding framework for activity recognition in ubiquitous and mobile computing that alleviates two fundamental problems of current supervised learning approaches . ( i ) it automatically derives a compact , sparse and meaningful feature representation of sensor data that does not rely on prior expert knowledge and generalizes extremely well across domain boundaries . ( ii ) it exploits unlabeled sample data for bootstrapping effective activity recognizers , i.e. , substantially reduces the amount of ground truth annotation required for model estimation . such unlabeled data is trivial to obtain , e.g. , through contemporary smartphones carried by users as they go about their everyday activities . based on the self-taught learning paradigm we automatically derive an over-complete set of basis vectors from unlabeled data that captures inherent patterns present within activity data . through projecting raw sensor data onto the feature space defined by such over-complete sets of basis vectors effective feature extraction is pursued . given these learned feature representations , classification backends are then trained using small amounts of labeled training data . we study the new approach in detail using two datasets which differ in terms of the recognition tasks and sensor modalities . primarily we focus on transportation mode analysis task , a popular task in mobile-phone based sensing . the sparse-coding framework significantly outperforms the state-of-the-art in supervised learning approaches . furthermore , we demonstrate the great practical potential of the new approach by successfully evaluating its generalization capabilities across both domain and sensor modalities by considering the popular opportunity dataset . our feature learning approach outperforms state-of-the-art approaches to analyzing activities in daily living ."}
{"title": "neural responding machine for short-text conversation", "abstract": "we propose neural responding machine ( nrm ) , a neural network-based response generator for short-text conversation . nrm takes the general encoder-decoder framework : it formalizes the generation of response as a decoding process based on the latent representation of the input text , while both encoding and decoding are realized with recurrent neural networks ( rnn ) . the nrm is trained with a large amount of one-round conversation data collected from a microblogging service . empirical study shows that nrm can generate grammatically correct and content-wise appropriate responses to over 75 % of the input text , outperforming state-of-the-arts in the same setting , including retrieval-based and smt-based models ."}
{"title": "atrank : an attention-based user behavior modeling framework for recommendation", "abstract": "a user can be represented as what he/she does along the history . a common way to deal with the user modeling problem is to manually extract all kinds of aggregated features over the heterogeneous behaviors , which may fail to fully represent the data itself due to limited human instinct . recent works usually use rnn-based methods to give an overall embedding of a behavior sequence , which then could be exploited by the downstream applications . however , this can only preserve very limited information , or aggregated memories of a person . when a downstream application requires to facilitate the modeled user features , it may lose the integrity of the specific highly correlated behavior of the user , and introduce noises derived from unrelated behaviors . this paper proposes an attention based user behavior modeling framework called atrank , which we mainly use for recommendation tasks . heterogeneous user behaviors are considered in our model that we project all types of behaviors into multiple latent semantic spaces , where influence can be made among the behaviors via self-attention . downstream applications then can use the user behavior vectors via vanilla attention . experiments show that atrank can achieve better performance and faster training process . we further explore atrank to use one unified model to predict different types of user behaviors at the same time , showing a comparable performance with the highly optimized individual models ."}
{"title": "connectionist theory refinement : genetically searching the space of network topologies", "abstract": "an algorithm that learns from a set of examples should ideally be able to exploit the available resources of ( a ) abundant computing power and ( b ) domain-specific knowledge to improve its ability to generalize . connectionist theory-refinement systems , which use background knowledge to select a neural network 's topology and initial weights , have proven to be effective at exploiting domain-specific knowledge ; however , most do not exploit available computing power . this weakness occurs because they lack the ability to refine the topology of the neural networks they produce , thereby limiting generalization , especially when given impoverished domain theories . we present the regent algorithm which uses ( a ) domain-specific knowledge to help create an initial population of knowledge-based neural networks and ( b ) genetic operators of crossover and mutation ( specifically designed for knowledge-based networks ) to continually search for better network topologies . experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization compared to a standard connectionist theory-refinement system , as well as our previous algorithm for growing knowledge-based networks ."}
{"title": "iq of neural networks", "abstract": "iq tests are an accepted method for assessing human intelligence . the tests consist of several parts that must be solved under a time constraint . of all the tested abilities , pattern recognition has been found to have the highest correlation with general intelligence . this is primarily because pattern recognition is the ability to find order in a noisy environment , a necessary skill for intelligent agents . in this paper , we propose a convolutional neural network ( cnn ) model for solving geometric pattern recognition problems . the cnn receives as input multiple ordered input images and outputs the next image according to the pattern . our cnn is able to solve problems involving rotation , reflection , color , size and shape patterns and score within the top 5 % of human performance ."}
{"title": "a framework for easing the development of applications embedding answer set programming", "abstract": "answer set programming ( asp ) is a well-established declarative problem solving paradigm which became widely used in ai and recognized as a powerful tool for knowledge representation and reasoning ( krr ) , especially for its high expressiveness and the ability to deal also with incomplete knowledge . recently , thanks to the availability of a number of robust and efficient implementations , asp has been increasingly employed in a number of different domains , and used for the development of industrial-level and enterprise applications . this made clear the need for proper development tools and interoperability mechanisms for easing interaction and integration with external systems in the widest range of real-world scenarios , including mobile applications and educational contexts . in this work we present a framework for integrating the krr capabilities of asp into generic applications . we show the use of the framework by illustrating proper specializations for some relevant asp systems over different platforms , including the mobile setting ; furthermore , the potential of the framework for educational purposes is illustrated by means of the development of several asp-based applications ."}
{"title": "towards a logic-based unifying framework for computing", "abstract": "in this paper we propose a logic-based , framework inspired by artificial intelligence , but scaled down for practical database and programming applications . computation in the framework is viewed as the task of generating a sequence of state transitions , with the purpose of making an agent 's goals all true . states are represented by sets of atomic sentences ( or facts ) , representing the values of program variables , tuples in a coordination language , facts in relational databases , or herbrand models . in the model-theoretic semantics , the entire sequence of states and events are combined into a single model-theoretic structure , by associating timestamps with facts and events . but in the operational semantics , facts are updated destructively , without timestamps . we show that the model generated by destructive updates is identical to the model generated by reasoning with facts containing timestamps . we also extend the model with intentional predicates and composite event predicates defined by logic programs containing conditions in first-order logic , which query the current state ."}
{"title": "juxtaposition of system dynamics and agent-based simulation for a case study in immunosenescence", "abstract": "advances in healthcare and in the quality of life significantly increase human life expectancy . with the ageing of populations , new un-faced challenges are brought to science . the human body is naturally selected to be well-functioning until the age of reproduction to keep the species alive . however , as the lifespan extends , unseen problems due to the body deterioration emerge . there are several age-related diseases with no appropriate treatment ; therefore , the complex ageing phenomena needs further understanding . immunosenescence , the ageing of the immune system , is highly correlated to the negative effects of ageing , such as the increase of auto-inflammatory diseases and decrease in responsiveness to new diseases . besides clinical and mathematical tools , we believe there is opportunity to further exploit simulation tools to understand immunosenescence . compared to real-world experimentation , benefits include time and cost effectiveness due to the laborious , resource-intensiveness of the biological environment and the possibility of conducting experiments without ethic restrictions . contrasted with mathematical models , simulation modelling is more suitable for representing complex systems and emergence . in addition , there is the belief that simulation models are easier to communicate in interdisciplinary contexts . our work investigates the usefulness of simulations to understand immunosenescence by employing two different simulation methods , agent-based and system dynamics simulation , to a case study of immune cells depletion with age ."}
{"title": "intrinsically motivated goal exploration processes with automatic curriculum learning", "abstract": "intrinsically motivated spontaneous exploration is a key enabler of autonomous lifelong learning in human children . it allows them to discover and acquire large repertoires of skills through self-generation , self-selection , self-ordering and self-experimentation of learning goals . we present the unsupervised multi-goal reinforcement learning formal framework as well as an algorithmic approach called intrinsically motivated goal exploration processes ( imgep ) to enable similar properties of autonomous learning in machines . the imgep algorithmic architecture relies on several principles : 1 ) self-generation of goals as parameterized reinforcement learning problems ; 2 ) selection of goals based on intrinsic rewards ; 3 ) exploration with parameterized time-bounded policies and fast incremental goal-parameterized policy search ; 4 ) systematic reuse of information acquired when targeting a goal for improving other goals . we present a particularly efficient form of imgep that uses a modular representation of goal spaces as well as intrinsic rewards based on learning progress . we show how imgeps automatically generate a learning curriculum within an experimental setup where a real humanoid robot can explore multiple spaces of goals with several hundred continuous dimensions . while no particular target goal is provided to the system beforehand , this curriculum allows the discovery of skills of increasing complexity , that act as stepping stone for learning more complex skills ( like nested tool use ) . we show that learning several spaces of diverse problems can be more efficient for learning complex skills than only trying to directly learn these complex skills . we illustrate the computational efficiency of imgeps as these robotic experiments use a simple memory-based low-level policy representations and search algorithm , enabling the whole system to learn online and incrementally on a raspberry pi 3 ."}
{"title": "the role of pragmatics in legal norm representation", "abstract": "despite the 'apparent clarity ' of a given legal provision , its application may result in an outcome that does not exactly conform to the semantic level of a statute . the vagueness within a legal text is induced intentionally to accommodate all possible scenarios under which such norms should be applied , thus making the role of pragmatics an important aspect also in the representation of a legal norm and reasoning on top of it . the notion of pragmatics considered in this paper does not focus on the aspects associated with judicial decision making . the paper aims to shed light on the aspects of pragmatics in legal linguistics , mainly focusing on the domain of patent law , only from a knowledge representation perspective . the philosophical discussions presented in this paper are grounded based on the legal theories from grice and marmor ."}
{"title": "ai programmer : autonomously creating software programs using genetic algorithms", "abstract": "in this paper , we present the first-of-its-kind machine learning ( ml ) system , called ai programmer , that can automatically generate full software programs requiring only minimal human guidance . at its core , ai programmer uses genetic algorithms ( ga ) coupled with a tightly constrained programming language that minimizes the overhead of its ml search space . part of ai programmer 's novelty stems from ( i ) its unique system design , including an embedded , hand-crafted interpreter for efficiency and security and ( ii ) its augmentation of gas to include instruction-gene randomization bindings and programming language-specific genome construction and elimination techniques . we provide a detailed examination of ai programmer 's system design , several examples detailing how the system works , and experimental data demonstrating its software generation capabilities and performance using only mainstream cpus ."}
{"title": "thinking adaptive : towards a behaviours virtual laboratory", "abstract": "in this paper we name some of the advantages of virtual laboratories ; and propose that a behaviours virtual laboratory should be useful for both biologists and ai researchers , offering a new perspective for understanding adaptive behaviour . we present our development of a behaviours virtual laboratory , which at this stage is focused in action selection , and show some experiments to illustrate the properties of our proposal , which can be accessed via internet ."}
{"title": "the computational theory of intelligence : data aggregation", "abstract": "in this paper , we will expound upon the concepts proffered in [ 1 ] , where we proposed an information theoretic approach to intelligence in the computational sense . we will examine data and meme aggregation , and study the effect of limited resources on the resulting meme amplitudes ."}
{"title": "abstract syntax networks for code generation and semantic parsing", "abstract": "tasks like code generation and semantic parsing require mapping unstructured ( or partially structured ) inputs to well-formed , executable outputs . we introduce abstract syntax networks , a modeling framework for these problems . the outputs are represented as abstract syntax trees ( asts ) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree . on the benchmark hearthstone dataset for code generation , our model obtains 79.2 bleu and 22.7 % exact match accuracy , compared to previous state-of-the-art values of 67.1 and 6.1 % . furthermore , we perform competitively on the atis , jobs , and geo semantic parsing datasets with no task-specific engineering ."}
{"title": "predicting occupancy trends in barcelona 's bicycle service stations using open data", "abstract": "in 2008 , the ceo of the company that manages and maintains the public bicycle service in barcelona recognized that one may not expect to always find a place to leave the rented bike nearby their destination , similarly to the case when , driving a car , people may not find a parking lot . in this work , we make predictions about the statuses of the stations of the public bicycle service in barcelona . we show that it is feasible to correctly predict nearly half of the times when the stations are either completely full of bikes or completely empty , up to 2 days before they actually happen . that is , users might avoid stations at times when they could not return a bicycle that they have rented before , or when they would not find a bike to rent . to achieve that , we apply the random forest algorithm to classify the status of the stations and improve the lifetime of the models using publicly available data , such as information about the weather forecast . finally , we expect that the results of the predictions can be used to improve the quality of the service and make it more reliable for the users ."}
{"title": "representing hybrid automata by action language modulo theories", "abstract": "both hybrid automata and action languages are formalisms for describing the evolution of dynamic systems . this paper establishes a formal relationship between them . we show how to succinctly represent hybrid automata in an action language which in turn is defined as a high-level notation for answer set programming modulo theories ( aspmt ) -- - an extension of answer set programs to the first-order level similar to the way satisfiability modulo theories ( smt ) extends propositional satisfiability ( sat ) . we first show how to represent linear hybrid automata with convex invariants by an action language modulo theories . a further translation into smt allows for computing them using smt solvers that support arithmetic over reals . next , we extend the representation to the general class of non-linear hybrid automata allowing even non-convex invariants . we represent them by an action language modulo ode ( ordinary differential equations ) , which can be compiled into satisfiability modulo ode . we developed a prototype system cplus2aspmt based on these translations , which allows for a succinct representation of hybrid transition systems that can be computed effectively by the state-of-the-art smt solver dreal ."}
{"title": "more-or-less cp-networks", "abstract": "preferences play an important role in our everyday lives . cp-networks , or cp-nets in short , are graphical models for representing conditional qualitative preferences under ceteris paribus ( `` all else being equal '' ) assumptions . despite their intuitive nature and rich representation , dominance testing with cp-nets is computationally complex , even when the cp-nets are restricted to binary-valued preferences . tractable algorithms exist for binary cp-nets , but these algorithms are incomplete for multi-valued cpnets . in this paper , we identify a class of multivalued cp-nets , which we call more-or-less cpnets , that have the same computational complexity as binary cp-nets . more-or-less cp-nets exploit the monotonicity of the attribute values and use intervals to aggregate values that induce similar preferences . we then present a search control rule for dominance testing that effectively prunes the search space while preserving completeness ."}
{"title": "on the compressive power of deep rectifier networks for high resolution representation of class boundaries", "abstract": "this paper provides a theoretical justification of the superior classification performance of deep rectifier networks over shallow rectifier networks from the geometrical perspective of piecewise linear ( pwl ) classifier boundaries . we show that , for a given threshold on the approximation error , the required number of boundary facets to approximate a general smooth boundary grows exponentially with the dimension of the data , and thus the number of boundary facets , referred to as boundary resolution , of a pwl classifier is an important quality measure that can be used to estimate a lower bound on the classification errors . however , learning naively an exponentially large number of boundary facets requires the determination of an exponentially large number of parameters and also requires an exponentially large number of training patterns . to overcome this issue of `` curse of dimensionality '' , compressive representations of high resolution classifier boundaries are required . to show the superior compressive power of deep rectifier networks over shallow rectifier networks , we prove that the maximum boundary resolution of a single hidden layer rectifier network classifier grows exponentially with the number of units when this number is smaller than the dimension of the patterns . when the number of units is larger than the dimension of the patterns , the growth rate is reduced to a polynomial order . consequently , the capacity of generating a high resolution boundary will increase if the same large number of units are arranged in multiple layers instead of a single hidden layer . taking high dimensional spherical boundaries as examples , we show how deep rectifier networks can utilize geometric symmetries to approximate a boundary with the same accuracy but with a significantly fewer number of parameters than single hidden layer nets ."}
{"title": "practical inventory routing : a problem definition and an optimization method", "abstract": "the global objective of this work is to provide practical optimization methods to companies involved in inventory routing problems , taking into account this new type of data . also , companies are sometimes not able to deal with changing plans every period and would like to adopt regular structures for serving customers ."}
{"title": "towards statistical reasoning in description logics over finite domains ( full version )", "abstract": "we present a probabilistic extension of the description logic $ \\mathcal { alc } $ for reasoning about statistical knowledge . we consider conditional statements over proportions of the domain and are interested in the probabilistic-logical consequences of these proportions . after introducing some general reasoning problems and analyzing their properties , we present first algorithms and complexity results for reasoning in some fragments of statistical $ \\mathcal { alc } $ ."}
{"title": "first-order conditional logic revisited", "abstract": "conditional logics play an important role in recent attempts to formulate theories of default reasoning . this paper investigates first-order conditional logic . we show that , as for first-order probabilistic logic , it is important not to confound statistical conditionals over the domain ( such as `` most birds fly '' ) , and subjective conditionals over possible worlds ( such as `` i believe that tweety is unlikely to fly '' ) . we then address the issue of ascribing semantics to first-order conditional logic . as in the propositional case , there are many possible semantics . to study the problem in a coherent way , we use plausibility structures . these provide us with a general framework in which many of the standard approaches can be embedded . we show that while these standard approaches are all the same at the propositional level , they are significantly different in the context of a first-order language . furthermore , we show that plausibilities provide the most natural extension of conditional logic to the first-order case : we provide a sound and complete axiomatization that contains only the klm properties and standard axioms of first-order modal logic . we show that most of the other approaches have additional properties , which result in an inappropriate treatment of an infinitary version of the lottery paradox ."}
{"title": "formal model of uncertainty for possibilistic rules", "abstract": "given a universe of discourse x-a domain of possible outcomes-an experiment may consist of selecting one of its elements , subject to the operation of chance , or of observing the elements , subject to imprecision . a priori uncertainty about the actual result of the experiment may be quantified , representing either the likelihood of the choice of : r_x or the degree to which any such x would be suitable as a description of the outcome . the former case corresponds to a probability distribution , while the latter gives a possibility assignment on x. the study of such assignments and their properties falls within the purview of possibility theory [ dp88 , y80 , z783 . it , like probability theory , assigns values between 0 and 1 to express likelihoods of outcomes . here , however , the similarity ends . possibility theory uses the maximum and minimum functions to combine uncertainties , whereas probability theory uses the plus and times operations . this leads to very dissimilar theories in terms of analytical framework , even though they share several semantic concepts . one of the shared concepts consists of expressing quantitatively the uncertainty associated with a given distribution . in probability theory its value corresponds to the gain of information that would result from conducting an experiment and ascertaining an actual result . this gain of information can equally well be viewed as a decrease in uncertainty about the outcome of an experiment . in this case the standard measure of information , and thus uncertainty , is shannon entropy [ ad75 , g77 ] . it enjoys several advantages-it is characterized uniquely by a few , very natural properties , and it can be conveniently used in decision processes . this application is based on the principle of maximum entropy ; it has become a popular method of relating decisions to uncertainty . this paper demonstrates that an equally integrated theory can be built on the foundation of possibility theory . we first show how to define measures of in formation and uncertainty for possibility assignments . next we construct an information-based metric on the space of all possibility distributions defined on a given domain . it allows us to capture the notion of proximity in information content among the distributions . lastly , we show that all the above constructions can be carried out for continuous distributions-possibility assignments on arbitrary measurable domains . we consider this step very significant-finite domains of discourse are but approximations of the real-life infinite domains . if possibility theory is to represent real world situations , it must handle continuous distributions both directly and through finite approximations . in the last section we discuss a principle of maximum uncertainty for possibility distributions . we show how such a principle could be formalized as an inference rule . we also suggest it could be derived as a consequence of simple assumptions about combining information . we would like to mention that possibility assignments can be viewed as fuzzy sets and that every fuzzy set gives rise to an assignment of possibilities . this correspondence has far reaching consequences in logic and in control theory . our treatment here is independent of any special interpretation ; in particular we speak of possibility distributions and possibility measures , defining them as measurable mappings into the interval [ 0 , 1 ] . our presentation is intended as a self-contained , albeit terse summary . topics discussed were selected with care , to demonstrate both the completeness and a certain elegance of the theory . proofs are not included ; we only offer illustrative examples ."}
{"title": "mudos-ng : multi-document summaries using n-gram graphs ( tech report )", "abstract": "this report describes the mudos-ng summarization system , which applies a set of language-independent and generic methods for generating extractive summaries . the proposed methods are mostly combinations of simple operators on a generic character n-gram graph representation of texts . this work defines the set of used operators upon n-gram graphs and proposes using these operators within the multi-document summarization process in such subtasks as document analysis , salient sentence selection , query expansion and redundancy control . furthermore , a novel chunking methodology is used , together with a novel way to assign concepts to sentences for query expansion . the experimental results of the summarization system , performed upon widely used corpora from the document understanding and the text analysis conferences , are promising and provide evidence for the potential of the generic methods introduced . this work aims to designate core methods exploiting the n-gram graph representation , providing the basis for more advanced summarization systems ."}
{"title": "cross-entropic learning of a machine for the decision in a partially observable universe", "abstract": "revision of the paper previously entitled `` learning a machine for the decision in a partially observable markov universe '' in this paper , we are interested in optimal decisions in a partially observable universe . our approach is to directly approximate an optimal strategic tree depending on the observation . this approximation is made by means of a parameterized probabilistic law . a particular family of hidden markov models , with input \\emph { and } output , is considered as a model of policy . a method for optimizing the parameters of these hmms is proposed and applied . this optimization is based on the cross-entropic principle for rare events simulation developed by rubinstein ."}
{"title": "visualization optimization : application to the robocup rescue domain", "abstract": "in this paper we demonstrate the use of intelligent optimization methodologies on the visualization optimization of virtual / simulated environments . the problem of automatic selection of an optimized set of views , which better describes an on-going simulation over a virtual environment is addressed in the context of the robocup rescue simulation domain . a generic architecture for optimization is proposed and described . we outline the possible extensions of this architecture and argue on how several problems within the fields of interactive rendering and visualization can benefit from it ."}
{"title": "integration of declarative and constraint programming", "abstract": "combining a set of existing constraint solvers into an integrated system of cooperating solvers is a useful and economic principle to solve hybrid constraint problems . in this paper we show that this approach can also be used to integrate different language paradigms into a unified framework . furthermore , we study the syntactic , semantic and operational impacts of this idea for the amalgamation of declarative and constraint programming ."}
{"title": "signal representations on graphs : tools and applications", "abstract": "we present a framework for representing and modeling data on graphs . based on this framework , we study three typical classes of graph signals : smooth graph signals , piecewise-constant graph signals , and piecewise-smooth graph signals . for each class , we provide an explicit definition of the graph signals and construct a corresponding graph dictionary with desirable properties . we then study how such graph dictionary works in two standard tasks : approximation and sampling followed with recovery , both from theoretical as well as algorithmic perspectives . finally , for each class , we present a case study of a real-world problem by using the proposed methodology ."}
{"title": "estimating the maximum expected value : an analysis of ( nested ) cross validation and the maximum sample average", "abstract": "we investigate the accuracy of the two most common estimators for the maximum expected value of a general set of random variables : a generalization of the maximum sample average , and cross validation . no unbiased estimator exists and we show that it is non-trivial to select a good estimator without knowledge about the distributions of the random variables . we investigate and bound the bias and variance of the aforementioned estimators and prove consistency . the variance of cross validation can be significantly reduced , but not without risking a large bias . the bias and variance of different variants of cross validation are shown to be very problem-dependent , and a wrong choice can lead to very inaccurate estimates ."}
{"title": "problem solving and computational thinking in a learning environment", "abstract": "computational thinking is a new problem soling method named for its extensive use of computer science techniques . it synthesizes critical thinking and existing knowledge and applies them in solving complex technological problems . the term was coined by j. wing , but the relationship between computational and critical thinking , the two modes of thiking in solving problems , has not been yet learly established . this paper aims at shedding some light into this relationship . we also present two classroom experiments performed recently at the graduate technological educational institute of patras in greece . the results of these experiments give a strong indication that the use of computers as a tool for problem solving enchances the students ' abilities in solving real world problems involving mathematical modelling . this is also crossed by earlier findings of other researchers for the problem solving process in general ( not only for mathematical problems ) ."}
{"title": "necessary and sufficient conditions for surrogate functions of pareto frontiers and their synthesis using gaussian processes", "abstract": "this paper introduces the necessary and sufficient conditions that surrogate functions must satisfy to properly define frontiers of non-dominated solutions in multi-objective optimization problems . these new conditions work directly on the objective space , thus being agnostic about how the solutions are evaluated . therefore , real objectives or user-designed objectives ' surrogates are allowed , opening the possibility of linking independent objective surrogates . to illustrate the practical consequences of adopting the proposed conditions , we use gaussian processes as surrogates endowed with monotonicity soft constraints and with an adjustable degree of flexibility , and compare them to regular gaussian processes and to a frontier surrogate method in the literature that is the closest to the method proposed in this paper . results show that the necessary and sufficient conditions proposed here are finely managed by the constrained gaussian process , guiding to high-quality surrogates capable of suitably synthesizing an approximation to the pareto frontier in challenging instances of multi-objective optimization , while an existing approach that does not take the theory proposed in consideration defines surrogates which greatly violate the conditions to describe a valid frontier ."}
{"title": "$ \\mathbf { d^3 } $ : deep dual-domain based fast restoration of jpeg-compressed images", "abstract": "in this paper , we design a deep dual-domain ( $ \\mathbf { d^3 } $ ) based fast restoration model to remove artifacts of jpeg compressed images . it leverages the large learning capacity of deep networks , as well as the problem-specific expertise that was hardly incorporated in the past design of deep architectures . for the latter , we take into consideration both the prior knowledge of the jpeg compression scheme , and the successful practice of the sparsity-based dual-domain approach . we further design the one-step sparse inference ( 1-si ) module , as an efficient and light-weighted feed-forward approximation of sparse coding . extensive experiments verify the superiority of the proposed $ d^3 $ model over several state-of-the-art methods . specifically , our best model is capable of outperforming the latest deep model for around 1 db in psnr , and is 30 times faster ."}
{"title": "iterated belief change due to actions and observations", "abstract": "in action domains where agents may have erroneous beliefs , reasoning about the effects of actions involves reasoning about belief change . in this paper , we use a transition system approach to reason about the evolution of an agents beliefs as actions are executed . some actions cause an agent to perform belief revision while others cause an agent to perform belief update , but the interaction between revision and update can be non-elementary . we present a set of rationality properties describing the interaction between revision and update , and we introduce a new class of belief change operators for reasoning about alternating sequences of revisions and updates . our belief change operators can be characterized in terms of a natural shifting operation on total pre-orderings over interpretations . we compare our approach with related work on iterated belief change due to action , and we conclude with some directions for future research ."}
{"title": "fuzzy constraints linear discriminant analysis", "abstract": "in this paper we introduce a fuzzy constraint linear discriminant analysis ( fc-lda ) . the fc-lda tries to minimize misclassification error based on modified perceptron criterion that benefits handling the uncertainty near the decision boundary by means of a fuzzy linear programming approach with fuzzy resources . the method proposed has low computational complexity because of its linear characteristics and the ability to deal with noisy data with different degrees of tolerance . obtained results verify the success of the algorithm when dealing with different problems . comparing fc-lda and lda shows superiority in classification task ."}
{"title": "solving planning domains with polytree causal graphs is np-complete", "abstract": "we show that solving planning domains on binary variables with polytree causal graph is \\np-complete . this is in contrast to a polynomial-time algorithm of domshlak and brafman that solves these planning domains for polytree causal graphs of bounded indegree ."}
{"title": "deep semantic abstractions of everyday human activities : on commonsense representations of human interactions", "abstract": "we propose a deep semantic characterization of space and motion categorically from the viewpoint of grounding embodied human-object interactions . our key focus is on an ontological model that would be adept to formalisation from the viewpoint of commonsense knowledge representation , relational learning , and qualitative reasoning about space and motion in cognitive robotics settings . we demonstrate key aspects of the space & motion ontology and its formalization as a representational framework in the backdrop of select examples from a dataset of everyday activities . furthermore , focussing on human-object interaction data obtained from rgbd sensors , we also illustrate how declarative ( spatio-temporal ) reasoning in the ( constraint ) logic programming family may be performed with the developed deep semantic abstractions ."}
{"title": "outlier detection from network data with subnetwork interpretation", "abstract": "detecting a small number of outliers from a set of data observations is always challenging . this problem is more difficult in the setting of multiple network samples , where computing the anomalous degree of a network sample is generally not sufficient . in fact , explaining why the network is exceptional , expressed in the form of subnetwork , is also equally important . in this paper , we develop a novel algorithm to address these two key problems . we treat each network sample as a potential outlier and identify subnetworks that mostly discriminate it from nearby regular samples . the algorithm is developed in the framework of network regression combined with the constraints on both network topology and l1-norm shrinkage to perform subnetwork discovery . our method thus goes beyond subspace/subgraph discovery and we show that it converges to a global optimum . evaluation on various real-world network datasets demonstrates that our algorithm not only outperforms baselines in both network and high dimensional setting , but also discovers highly relevant and interpretable local subnetworks , further enhancing our understanding of anomalous networks ."}
{"title": "a new fundamental evidence of non-classical structure in the combination of natural concepts", "abstract": "we recently performed cognitive experiments on conjunctions and negations of two concepts with the aim of investigating the combination problem of concepts . our experiments confirmed the deviations ( conceptual vagueness , underextension , overextension , etc . ) from the rules of classical ( fuzzy ) logic and probability theory observed by several scholars in concept theory , while our data were successfully modeled in a quantum-theoretic framework developed by ourselves . in this paper , we isolate a new , very stable and systematic pattern of violation of classicality that occurs in concept combinations . in addition , the strength and regularity of this non-classical effect leads us to believe that it occurs at a more fundamental level than the deviations observed up to now . it is our opinion that we have identified a deep non-classical mechanism determining not only how concepts are combined but , rather , how they are formed . we show that this effect can be faithfully modeled in a two-sector fock space structure , and that it can be exactly explained by assuming that human thought is the supersposition of two processes , a 'logical reasoning ' , guided by 'logic ' , and a 'conceptual reasoning ' guided by 'emergence ' , and that the latter generally prevails over the former . all these findings provide a new fundamental support to our quantum-theoretic approach to human cognition ."}
{"title": "towards well-specified semi-supervised model-based classifiers via structural adaptation", "abstract": "semi-supervised learning plays an important role in large-scale machine learning . properly using additional unlabeled data ( largely available nowadays ) often can improve the machine learning accuracy . however , if the machine learning model is misspecified for the underlying true data distribution , the model performance could be seriously jeopardized . this issue is known as model misspecification . to address this issue , we focus on generative models and propose a criterion to detect the onset of model misspecification by measuring the performance difference between models obtained using supervised and semi-supervised learning . then , we propose to automatically modify the generative models during model training to achieve an unbiased generative model . rigorous experiments were carried out to evaluate the proposed method using two image classification data sets pascal voc'07 and mir flickr . our proposed method has been demonstrated to outperform a number of state-of-the-art semi-supervised learning approaches for the classification task ."}
{"title": "fast meta-learning for adaptive hierarchical classifier design", "abstract": "we propose a new splitting criterion for a meta-learning approach to multiclass classifier design that adaptively merges the classes into a tree-structured hierarchy of increasingly difficult binary classification problems . the classification tree is constructed from empirical estimates of the henze-penrose bounds on the pairwise bayes misclassification rates that rank the binary subproblems in terms of difficulty of classification . the proposed empirical estimates of the bayes error rate are computed from the minimal spanning tree ( mst ) of the samples from each pair of classes . moreover , a meta-learning technique is presented for quantifying the one-vs-rest bayes error rate for each individual class from a single mst on the entire dataset . extensive simulations on benchmark datasets show that the proposed hierarchical method can often be learned much faster than competing methods , while achieving competitive accuracy ."}
{"title": "proceedings of the fourteenth conference on uncertainty in artificial intelligence ( 1998 )", "abstract": "this is the proceedings of the fourteenth conference on uncertainty in artificial intelligence , which was held in madison , wi , july 24-26 , 1998"}
{"title": "tree projections and constraint optimization problems : fixed-parameter tractability and parallel algorithms", "abstract": "tree projections provide a unifying framework to deal with most structural decomposition methods of constraint satisfaction problems ( csps ) . within this framework , a csp instance is decomposed into a number of sub-problems , called views , whose solutions are either already available or can be computed efficiently . the goal is to arrange portions of these views in a tree-like structure , called tree projection , which determines an efficiently solvable csp instance equivalent to the original one . deciding whether a tree projection exists is np-hard . solution methods have therefore been proposed in the literature that do not require a tree projection to be given , and that either correctly decide whether the given csp instance is satisfiable , or return that a tree projection actually does not exist . these approaches had not been generalized so far on csp extensions for optimization problems , where the goal is to compute a solution of maximum value/minimum cost . the paper fills the gap , by exhibiting a fixed-parameter polynomial-time algorithm that either disproves the existence of tree projections or computes an optimal solution , with the parameter being the size of the expression of the objective function to be optimized over all possible solutions ( and not the size of the whole constraint formula , used in related works ) . tractability results are also established for the problem of returning the best k solutions . finally , parallel algorithms for such optimization problems are proposed and analyzed . given that the classes of acyclic hypergraphs , hypergraphs of bounded treewidth , and hypergraphs of bounded generalized hypertree width are all covered as special cases of the tree projection framework , the results in this paper directly apply to these classes . these classes are extensively considered in the csp setting , as well as in conjunctive database query evaluation and optimization ."}
{"title": "on understanding and machine understanding", "abstract": "in the present paper , we try to propose a self-similar network theory for the basic understanding . by extending the natural languages to a kind of so called idealy sufficient language , we can proceed a few steps to the investigation of the language searching and the language understanding of ai . image understanding , and the familiarity of the brain to the surrounding environment are also discussed . group effects are discussed by addressing the essense of the power of influences , and constructing the influence network of a society . we also give a discussion of inspirations ."}
{"title": "learning domain-invariant subspace using domain features and independence maximization", "abstract": "domain adaptation algorithms are useful when the distributions of the training and the test data are different . in this paper , we focus on the problem of instrumental variation and time-varying drift in the field of sensors and measurement , which can be viewed as discrete and continuous distributional change in the feature space . we propose maximum independence domain adaptation ( mida ) and semi-supervised mida ( smida ) to address this problem . domain features are first defined to describe the background information of a sample , such as the device label and acquisition time . then , mida learns a subspace which has maximum independence with the domain features , so as to reduce the inter-domain discrepancy in distributions . a feature augmentation strategy is also designed to project samples according to their backgrounds so as to improve the adaptation . the proposed algorithms are flexible and fast . their effectiveness is verified by experiments on synthetic datasets and four real-world ones on sensors , measurement , and computer vision . they can greatly enhance the practicability of sensor systems , as well as extend the application scope of existing domain adaptation algorithms by uniformly handling different kinds of distributional change ."}
{"title": "one model for the learning of language", "abstract": "a major target of linguistics and cognitive science has been to understand what class of learning systems can acquire the key structures of natural language . until recently , the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space . here , we describe a learning system that is maximally unconstrained , operating over the space of all computations , and is able to acquire several of the key structures present natural language from positive evidence alone . the model successfully acquires regular ( e.g . $ ( ab ) ^n $ ) , context-free ( e.g . $ a^n b^n $ , $ x x^r $ ) , and context-sensitive ( e.g . $ a^nb^nc^n $ , $ a^nb^mc^nd^m $ , $ xx $ ) formal languages . our approach develops the concept of factorized programs in bayesian program induction in order to help manage the complexity of representation . we show in learning , the model predicts several phenomena empirically observed in human grammar acquisition experiments ."}
{"title": "building a truly distributed constraint solver with jade", "abstract": "real life problems such as scheduling meeting between people at different locations can be modelled as distributed constraint satisfaction problems ( csps ) . suitable and satisfactory solutions can then be found using constraint satisfaction algorithms which can be exhaustive ( backtracking ) or otherwise ( local search ) . however , most research in this area tested their algorithms by simulation on a single pc with a single program entry point . the main contribution of our work is the design and implementation of a truly distributed constraint solver based on a local search algorithm using java agent development framework ( jade ) to enable communication between agents on different machines . particularly , we discuss design and implementation issues related to truly distributed constraint solver which might not be critical when simulated on a single machine . evaluation results indicate that our truly distributed constraint solver works well within the observed limitations when tested with various distributed csps . our application can also incorporate any constraint solving algorithm with little modifications ."}
{"title": "sample-and-accumulate algorithms for belief updating in bayes networks", "abstract": "belief updating in bayes nets , a well known computationally hard problem , has recently been approximated by several deterministic algorithms , and by various randomized approximation algorithms . deterministic algorithms usually provide probability bounds , but have an exponential runtime . some randomized schemes have a polynomial runtime , but provide only probability estimates . we present randomized algorithms that enumerate high-probability partial instantiations , resulting in probability bounds . some of these algorithms are also sampling algorithms . specifically , we introduce and evaluate a variant of backward sampling , both as a sampling algorithm and as a randomized enumeration algorithm . we also relax the implicit assumption used by both sampling and accumulation algorithms , that query nodes must be instantiated in all the samples ."}
{"title": "application of a fuzzy programming technique to production planning in the textile industry", "abstract": "many engineering optimization problems can be considered as linear programming problems where all or some of the parameters involved are linguistic in nature . these can only be quantified using fuzzy sets . the aim of this paper is to solve a fuzzy linear programming problem in which the parameters involved are fuzzy quantities with logistic membership functions . to explore the applicability of the method a numerical example is considered to determine the monthly production planning quotas and profit of a home textile group ."}
{"title": "bayesian approach to neuro-rough models", "abstract": "this paper proposes a neuro-rough model based on multi-layered perceptron and rough set . the neuro-rough model is then tested on modelling the risk of hiv from demographic data . the model is formulated using bayesian framework and trained using monte carlo method and metropolis criterion . when the model was tested to estimate the risk of hiv infection given the demographic data it was found to give the accuracy of 62 % . the proposed model is able to combine the accuracy of the bayesian mlp model and the transparency of bayesian rough set model ."}
{"title": "using descriptive video services to create a large data source for video annotation research", "abstract": "in this work , we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time . our dataset is based on the descriptive video service ( dvs ) that is now encoded on many digital media products such as dvds . dvs is an audio narration describing the visual elements and actions in a movie for the visually impaired . it is temporally aligned with the movie and mixed with the original movie soundtrack . we describe an automatic dvs segmentation and alignment method for movies , that enables us to scale up the collection of a dvs-derived dataset with minimal human intervention . using this method , we have collected the largest dvs-derived dataset for video description of which we are aware . our dataset currently includes over 84.6 hours of paired video/sentences from 92 dvds and is growing ."}
{"title": "a coordinated mdp approach to multi-agent planning for resource allocation , with applications to healthcare", "abstract": "this paper considers a novel approach to scalable multiagent resource allocation in dynamic settings . we propose an approximate solution in which each resource consumer is represented by an independent mdp-based agent that models expected utility using an average model of its expected access to resources given only limited information about all other agents . a global auction-based mechanism is proposed for allocations based on expected regret . we assume truthful bidding and a cooperative coordination mechanism , as we are considering healthcare scenarios . we illustrate the performance of our coordinated mdp approach against a monte-carlo based planning algorithm intended for large-scale applications , as well as other approaches suitable for allocating medical resources . the evaluations show that the global utility value across all consumer agents is closer to optimal when using our algorithms under certain time constraints , with low computational cost . as such , we offer a promising approach for addressing complex resource allocation problems that arise in healthcare settings ."}
{"title": "estimation of passenger route choice pattern using smart card data for complex metro systems", "abstract": "nowadays , metro systems play an important role in meeting the urban transportation demand in large cities . the understanding of passenger route choice is critical for public transit management . the wide deployment of automated fare collection ( afc ) systems opens up a new opportunity . however , only each trip 's tap-in and tap-out timestamp and stations can be directly obtained from afc system records ; the train and route chosen by a passenger are unknown , which are necessary to solve our problem . while existing methods work well in some specific situations , they do n't work for complicated situations . in this paper , we propose a solution that needs no additional equipment or human involvement than the afc systems . we develop a probabilistic model that can estimate from empirical analysis how the passenger flows are dispatched to different routes and trains . we validate our approach using a large scale data set collected from the shenzhen metro system . the measured results provide us with useful inputs when building the passenger path choice model ."}
{"title": "digital advertising traffic operation : machine learning for process discovery", "abstract": "in a web advertising traffic operation it 's necessary to manage the day-to-day trafficking , pacing and optimization of digital and paid social campaigns . the data analyst on traffic operation can not only quickly provide answers but also speaks the language of the process manager and visually displays the discovered process problems . in order to solve a growing number of complaints in the customer service process , the weaknesses in the process itself must be identified and communicated to the department . with the help of process mining for the crm data it is possible to identify unwanted loops and delays in the process . with this paper we propose a process discovery based on machine learning technique to automatically discover variations and detect at first glance what the problem is , and undertake corrective measures ."}
{"title": "variable elimination in the fourier domain", "abstract": "the ability to represent complex high dimensional probability distributions in a compact form is one of the key insights in the field of graphical models . factored representations are ubiquitous in machine learning and lead to major computational advantages . we explore a different type of compact representation based on discrete fourier representations , complementing the classical approach based on conditional independencies . we show that a large class of probabilistic graphical models have a compact fourier representation . this theoretical result opens up an entirely new way of approximating a probability distribution . we demonstrate the significance of this approach by applying it to the variable elimination algorithm . compared with the traditional bucket representation and other approximate inference algorithms , we obtain significant improvements ."}
{"title": "knowledge-based decision model construction for hierarchical diagnosis : a preliminary report", "abstract": "numerous methods for probabilistic reasoning in large , complex belief or decision networks are currently being developed . there has been little research on automating the dynamic , incremental construction of decision models . a uniform value-driven method of decision model construction is proposed for the hierarchical complete diagnosis . hierarchical complete diagnostic reasoning is formulated as a stochastic process and modeled using influence diagrams . given observations , this method creates decision models in order to obtain the best actions sequentially for locating and repairing a fault at minimum cost . this method construct decision models incrementally , interleaving probe actions with model construction and evaluation . the method treats meta-level and baselevel tasks uniformly . that is , the method takes a decision-theoretic look at the control of search in causal pathways and structural hierarchies ."}
{"title": "marginalizing out future passengers in group elevator control", "abstract": "group elevator scheduling is an np-hard sequential decision-making problem with unbounded state spaces and substantial uncertainty . decision-theoretic reasoning plays a surprisingly limited role in fielded systems . a new opportunity for probabilistic methods has opened with the recent discovery of a tractable solution for the expected waiting times of all passengers in the building , marginalized over all possible passenger itineraries . though commercially competitive , this solution does not contemplate future passengers . yet in up-peak traffic , the effects of future passengers arriving at the lobby and entering elevator cars can dominate all waiting times . we develop a probabilistic model of how these arrivals affect the behavior of elevator cars at the lobby , and demonstrate how this model can be used to very significantly reduce the average waiting time of all passengers ."}
{"title": "ontology-supported processing of clinical text using medical knowledge integration for multi-label classification of diagnosis coding", "abstract": "this paper discusses the knowledge integration of clinical information extracted from distributed medical ontology in order to ameliorate a machine learning-based multi-label coding assignment system . the proposed approach is implemented using a decision tree based cascade hierarchical technique on the university hospital data for patients with coronary heart disease ( chd ) . the preliminary results obtained show a satisfactory finding ."}
{"title": "smodels : a system for answer set programming", "abstract": "the smodels system implements the stable model semantics for normal logic programs . it handles a subclass of programs which contain no function symbols and are domain-restricted but supports extensions including built-in functions as well as cardinality and weight constraints . on top of this core engine more involved systems can be built . as an example , we have implemented total and partial stable model computation for disjunctive logic programs . an interesting application method is based on answer set programming , i.e. , encoding an application problem as a set of rules so that its solutions are captured by the stable models of the rules . smodels has been applied to a number of areas including planning , model checking , reachability analysis , product configuration , dynamic constraint satisfaction , and feature interaction ."}
{"title": "proceedings of the 8th international workshop on non-monotonic reasoning , nmr'2000", "abstract": "the papers gathered in this collection were presented at the 8th international workshop on nonmonotonic reasoning , nmr2000 . the series was started by john mccarthy in 1978. the first international nmr workshop was held at mohonk mountain house , new paltz , new york in june , 1984 , and was organized by ray reiter and bonnie webber . in the last 10 years the area of nonmonotonic reasoning has seen a number of important developments . significant theoretical advances were made in the understanding of general abstract principles underlying nonmonotonicity . key results on the expressibility and computational complexity of nonmonotonic logics were established . the role of nonmonotonic reasoning in belief revision , abduction , reasoning about action , planing and uncertainty was further clarified . several successful nmr systems were built and used in applications such as planning , scheduling , logic programming and constraint satisfaction . the papers in the proceedings reflect these recent advances in the field . they are grouped into sections corresponding to special sessions as they were held at the workshop : 1. general nmr track 2. abductive reasonig 3. belief revision : theory and practice 4. representing action and planning 5. systems descriptions and demonstrations 6. uncertainty frameworks in nmr"}
{"title": "memen : multi-layer embedding with memory networks for machine comprehension", "abstract": "machine comprehension ( mc ) style question answering is a representative problem in natural language processing . previous methods rarely spend time on the improvement of encoding layer , especially the embedding of syntactic information and name entity of the words , which are very crucial to the quality of encoding . moreover , existing attention methods represent each query word as a vector or use a single vector to represent the whole query sentence , neither of them can handle the proper weight of the key words in query sentence . in this paper , we introduce a novel neural network architecture called multi-layer embedding with memory network ( memen ) for machine reading task . in the encoding layer , we employ classic skip-gram model to the syntactic and semantic information of the words to train a new kind of embedding layer . we also propose a memory network of full-orientation matching of the query and passage to catch more pivotal information . experiments show that our model has competitive results both from the perspectives of precision and efficiency in stanford question answering dataset ( squad ) among all published results and achieves the state-of-the-art results on triviaqa dataset ."}
{"title": "maximum resilience of artificial neural networks", "abstract": "the deployment of artificial neural networks ( anns ) in safety-critical applications poses a number of new verification and certification challenges . in particular , for ann-enabled self-driving vehicles it is important to establish properties about the resilience of anns to noisy or even maliciously manipulated sensory input . we are addressing these challenges by defining resilience properties of ann-based classifiers as the maximal amount of input or sensor perturbation which is still tolerated . this problem of computing maximal perturbation bounds for anns is then reduced to solving mixed integer optimization problems ( mip ) . a number of mip encoding heuristics are developed for drastically reducing mip-solver runtimes , and using parallelization of mip-solvers results in an almost linear speed-up in the number ( up to a certain limit ) of computing cores in our experiments . we demonstrate the effectiveness and scalability of our approach by means of computing maximal resilience bounds for a number of ann benchmark sets ranging from typical image recognition scenarios to the autonomous maneuvering of robots ."}
{"title": "transferring autonomous driving knowledge on simulated and real intersections", "abstract": "we view intersection handling on autonomous vehicles as a reinforcement learning problem , and study its behavior in a transfer learning setting . we show that a network trained on one type of intersection generally is not able to generalize to other intersections . however , a network that is pre-trained on one intersection and fine-tuned on another performs better on the new task compared to training in isolation . this network also retains knowledge of the prior task , even though some forgetting occurs . finally , we show that the benefits of fine-tuning hold when transferring simulated intersection handling knowledge to a real autonomous vehicle ."}
{"title": "ignorability in statistical and probabilistic inference", "abstract": "when dealing with incomplete data in statistical learning , or incomplete observations in probabilistic inference , one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened . since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive , one asks for conditions under which it can be safely ignored . such conditions are given by the missing at random ( mar ) and coarsened at random ( car ) assumptions . in this paper we provide an in-depth analysis of several questions relating to mar/car assumptions . main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process . this question is complicated by the fact that several distinct versions of mar/car assumptions exist . we therefore first provide an overview over these different versions , in which we highlight the distinction between distributional and coarsening variable induced versions . we show that distributional versions are less restrictive and sufficient for most applications . we then address from two different perspectives the question of when the mar/car assumption is warranted . first we provide a static analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations . here we obtain an equivalence characterization that improves and extends a recent result by grunwald and halpern . we then turn to a procedural analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data ( or observation ) generating process . the main result of this analysis is that the stronger coarsened completely at random ( ccar ) condition is arguably the most reasonable assumption , as it alone corresponds to data coarsening procedures that satisfy a natural robustness property ."}
{"title": "verbal characterization of probabilistic clusters using minimal discriminative propositions", "abstract": "in a knowledge discovery process , interpretation and evaluation of the mined results are indispensable in practice . in the case of data clustering , however , it is often difficult to see in what aspect each cluster has been formed . this paper proposes a method for automatic and objective characterization or `` verbalization '' of the clusters obtained by mixture models , in which we collect conjunctions of propositions ( attribute-value pairs ) that help us interpret or evaluate the clusters . the proposed method provides us with a new , in-depth and consistent tool for cluster interpretation/evaluation , and works for various types of datasets including continuous attributes and missing values . experimental results with a couple of standard datasets exhibit the utility of the proposed method , and the importance of the feedbacks from the interpretation/evaluation step ."}
{"title": "multi source feedback based performance appraisal system using fuzzy logic decision support system", "abstract": "in multi-source feedback or 360 degree feedback , data on the performance of an individual are collected systematically from a number of stakeholders and are used for improving performance . the 360-degree feedback approach provides a consistent management philosophy meeting the criterion outlined previously . the 360-degree feedback appraisal process describes a human resource methodology that is frequently used for both employee appraisal and employee development . used in employee performance appraisals , the 360-degree feedback methodology is differentiated from traditional , top-down appraisal methods in which the supervisor responsible for the appraisal provides the majority of the data . instead it seeks to use information gained from other sources to provide a fuller picture of employees ' performances . similarly , when this technique used in employee development it augments employees ' perceptions of training needs with those of the people with whom they interact . the 360-degree feedback based appraisal is a comprehensive method where in the feedback about the employee comes from all the sources that come into contact with the employee on his/her job . the respondents for an employee can be her/his peers , managers , subordinates team members , customers , suppliers and vendors . hence anyone who comes into contact with the employee , the 360 degree appraisal has four components that include self-appraisal , superior 's appraisal , subordinate 's appraisal student 's appraisal and peer 's appraisal .the proposed system is an attempt to implement the 360 degree feedback based appraisal system in academics especially engineering colleges ."}
{"title": "consideration on example 2 of `` an algorithm of general fuzzy inferencewith the reductive property ''", "abstract": "in this paper , we will show that ( 1 ) the results about the fuzzy reasoning algoritm obtained in the paper `` computer sciences vol . 34 , no.4 , pp.145-148 , 2007 '' according to the paper `` ieee transactions on systems , man and cybernetics , 18 , pp.1049-1056 , 1988 '' are correct ; ( 2 ) example 2 in the paper `` an algorithm of general fuzzy inference with the reductive property '' presented by he ying-si , quan hai-jin and deng hui-wen according to the paper `` an approximate analogical reasoning approach based on similarity measures '' presented by tursken i.b . and zhong zhao is incorrect ; ( 3 ) the mistakes in their paper are modified and then a calculation example of fmt is supplemented ."}
{"title": "handwriting profiling using generative adversarial networks", "abstract": "handwriting is a skill learned by humans from a very early age . the ability to develop one 's own unique handwriting as well as mimic another person 's handwriting is a task learned by the brain with practice . this paper deals with this very problem where an intelligent system tries to learn the handwriting of an entity using generative adversarial networks ( gans ) . we propose a modified architecture of dcgan ( radford , metz , and chintala 2015 ) to achieve this . we also discuss about applying reinforcement learning techniques to achieve faster learning . our algorithm hopes to give new insights in this area and its uses include identification of forged documents , signature verification , computer generated art , digitization of documents among others . our early implementation of the algorithm illustrates a good performance with mnist datasets ."}
{"title": "a partial taxonomy of substitutability and interchangeability", "abstract": "substitutability , interchangeability and related concepts in constraint programming were introduced approximately twenty years ago and have given rise to considerable subsequent research . we survey this work , classify , and relate the different concepts , and indicate directions for future work , in particular with respect to making connections with research into symmetry breaking . this paper is a condensed version of a larger work in progress ."}
{"title": "conditional generative adversarial nets", "abstract": "generative adversarial nets [ 8 ] were recently introduced as a novel way to train generative models . in this work we introduce the conditional version of generative adversarial nets , which can be constructed by simply feeding the data , y , we wish to condition on to both the generator and discriminator . we show that this model can generate mnist digits conditioned on class labels . we also illustrate how this model could be used to learn a multi-modal model , and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels ."}
{"title": "a new approach to probabilistic programming inference", "abstract": "we introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle markov chain monte carlo . our approach is simple to implement and easy to parallelize . it applies to turing-complete probabilistic programming languages and supports accurate inference in models that make use of complex control flow , including stochastic recursion . it also includes primitives from bayesian nonparametric statistics . our experiments show that this approach can be more efficient than previously introduced single-site metropolis-hastings methods ."}
{"title": "unit contradiction versus unit propagation", "abstract": "some aspects of the result of applying unit resolution on a cnf formula can be formalized as functions with domain a set of partial truth assignments . we are interested in two ways for computing such functions , depending on whether the result is the production of the empty clause or the assignment of a variable with a given truth value . we show that these two models can compute the same functions with formulae of polynomially related sizes , and we explain how this result is related to the cnf encoding of boolean constraints ."}
{"title": "improved neural relation detection for knowledge base question answering", "abstract": "relation detection is a core component for many nlp applications including knowledge base question answering ( kbqa ) . in this paper , we propose a hierarchical recurrent neural network enhanced by residual learning that detects kb relations given an input question . our method uses deep residual bidirectional lstms to compare questions and relation names via different hierarchies of abstraction . additionally , we propose a simple kbqa system that integrates entity linking and our proposed relation detector to enable one enhance another . experimental results evidence that our approach achieves not only outstanding relation detection performance , but more importantly , it helps our kbqa system to achieve state-of-the-art accuracy for both single-relation ( simplequestions ) and multi-relation ( webqsp ) qa benchmarks ."}
{"title": "testing quantum models of conjunction fallacy on the world wide web", "abstract": "the 'conjunction fallacy ' has been extensively debated by scholars in cognitive science and , in recent times , the discussion has been enriched by the proposal of modeling the fallacy using the quantum formalism . two major quantum approaches have been put forward : the first assumes that respondents use a two-step sequential reasoning and that the fallacy results from the presence of 'question order effects ' ; the second assumes that respondents evaluate the cognitive situation as a whole and that the fallacy results from the 'emergence of new meanings ' , as an 'effect of overextension ' in the conceptual conjunction . thus , the question arises as to determine whether and to what extent conjunction fallacies would result from 'order effects ' or , instead , from 'emergence effects ' . to help clarify this situation , we propose to use the world wide web as an 'information space ' that can be interrogated both in a sequential and non-sequential way , to test these two quantum approaches . we find that 'emergence effects ' , and not 'order effects ' , should be considered the main cognitive mechanism producing the observed conjunction fallacies ."}
{"title": "a reactive tabu search algorithm for stimuli generation in psycholinguistics", "abstract": "the generation of meaningless `` words '' matching certain statistical and/or linguistic criteria is frequently needed for experimental purposes in psycholinguistics . such stimuli receive the name of pseudowords or nonwords in the cognitive neuroscience literatue . the process for building nonwords sometimes has to be based on linguistic units such as syllables or morphemes , resulting in a numerical explosion of combinations when the size of the nonwords is increased . in this paper , a reactive tabu search scheme is proposed to generate nonwords of variables size . the approach builds pseudowords by using a modified metaheuristic algorithm based on a local search procedure enhanced by a feedback-based scheme . experimental results show that the new algorithm is a practical and effective tool for nonword generation ."}
{"title": "mining complex hydrobiological data with galois lattices", "abstract": "we have used galois lattices for mining hydrobiological data . these data are about macrophytes , that are macroscopic plants living in water bodies . these plants are characterized by several biological traits , that own several modalities . our aim is to cluster the plants according to their common traits and modalities and to find out the relations between traits . galois lattices are efficient methods for such an aim , but apply on binary data . in this article , we detail a few approaches we used to transform complex hydrobiological data into binary data and compare the first results obtained thanks to galois lattices ."}
{"title": "a situation calculus-based approach to model ubiquitous information services", "abstract": "this paper presents an augmented situation calculus-based approach to model autonomous computing paradigm in ubiquitous information services . to make it practical for commercial development and easier to support autonomous paradigm imposed by ubiquitous information services , we made improvements based on reiter 's standard situation calculus . first we explore the inherent relationship between fluents and evolution : since not all fluents contribute to systems ' evolution and some fluents can be derived from some others , we define those fluents that are sufficient and necessary to determine evolutional potential as decisive fluents , and then we prove that their successor states wrt to deterministic complex actions satisfy markov property . then , within the calculus framework we build , we introduce validity theory to model the autonomous services with application-specific validity requirements , including : validity fluents to axiomatize validity requirements , heuristic multiple alternative service choices ranging from complete acceptance , partial acceptance , to complete rejection , and validity-ensured policy to comprise such alternative service choices into organic , autonomously-computable services . our approach is demonstrated by a ubiquitous calendaring service , acs , throughout the paper ."}
{"title": "an empirical study of w-cutset sampling for bayesian networks", "abstract": "the paper studies empirically the time-space trade-off between sampling and inference in a sl cutset sampling algorithm . the algorithm samples over a subset of nodes in a bayesian network and applies exact inference over the rest . consequently , while the size of the sampling space decreases , requiring less samples for convergence , the time for generating each single sample increases . the w-cutset sampling selects a sampling set such that the induced-width of the network when the sampling set is observed is bounded by w , thus requiring inference whose complexity is exponential in w. in this paper , we investigate performance of w-cutset sampling over a range of w values and measure the accuracy of w-cutset sampling as a function of w. our experiments demonstrate that the cutset sampling idea is quite powerful showing that an optimal balance between inference and sampling benefits substantially from restricting the cutset size , even at the cost of more complex inference ."}
{"title": "determining the unithood of word sequences using mutual information and independence measure", "abstract": "most works related to unithood were conducted as part of a larger effort for the determination of termhood . consequently , the number of independent research that study the notion of unithood and produce dedicated techniques for measuring unithood is extremely small . we propose a new approach , independent of any influences of termhood , that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from google search engine for the measurement of unithood . our evaluations revealed a precision and recall of 98.68 % and 91.82 % respectively with an accuracy at 95.42 % in measuring the unithood of 1005 test cases ."}
{"title": "learning scale-free networks by dynamic node-specific degree prior", "abstract": "learning the network structure underlying data is an important problem in machine learning . this paper introduces a novel prior to study the inference of scale-free networks , which are widely used to model social and biological networks . the prior not only favors a desirable global node degree distribution , but also takes into consideration the relative strength of all the possible edges adjacent to the same node and the estimated degree of each individual node . to fulfill this , ranking is incorporated into the prior , which makes the problem challenging to solve . we employ an admm ( alternating direction method of multipliers ) framework to solve the gaussian graphical model regularized by this prior . our experiments on both synthetic and real data show that our prior not only yields a scale-free network , but also produces many more correctly predicted edges than the others such as the scale-free inducing prior , the hub-inducing prior and the $ l_1 $ norm ."}
{"title": "optimally fuzzy temporal memory", "abstract": "any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past . if the signal has a characteristic timescale relevant to future prediction , the memory can be a simple shift register -- -a moving window extending into the past , requiring storage resources that linearly grows with the timescale to be represented . however , an independent general purpose learner can not a priori know the characteristic prediction-relevant timescale of the signal . moreover , many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded . hence the learner should maintain information from the longest possible timescale allowed by resource availability . here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales . using several illustrative examples , we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals . when the available storage resources are limited , we suggest that a general purpose learner would be better off committing to such a fuzzy memory system ."}
{"title": "intelligent search strategies based on adaptive constraint handling rules", "abstract": "the most advanced implementation of adaptive constraint processing with constraint handling rules ( chr ) allows the application of intelligent search strategies to solve constraint satisfaction problems ( csp ) . this presentation compares an improved version of conflict-directed backjumping and two variants of dynamic backtracking with respect to chronological backtracking on some of the aim instances which are a benchmark set of random 3-sat problems . a chr implementation of a boolean constraint solver combined with these different search strategies in java is thus being compared with a chr implementation of the same boolean constraint solver combined with chronological backtracking in sicstus prolog . this comparison shows that the addition of `` intelligence '' to the search process may reduce the number of search steps dramatically . furthermore , the runtime of their java implementations is in most cases faster than the implementations of chronological backtracking . more specifically , conflict-directed backjumping is even faster than the sicstus prolog implementation of chronological backtracking , although our java implementation of chr lacks the optimisations made in the sicstus prolog system . to appear in theory and practice of logic programming ( tplp ) ."}
{"title": "learning to attend , copy , and generate for session-based query suggestion", "abstract": "users try to articulate their complex information needs during search sessions by reformulating their queries . to make this process more effective , search engines provide related queries to help users in specifying the information need in their search process . in this paper , we propose a customized sequence-to-sequence model for session-based query suggestion . in our model , we employ a query-aware attention mechanism to capture the structure of the session context . is enables us to control the scope of the session from which we infer the suggested next query , which helps not only handle the noisy data but also automatically detect session boundaries . furthermore , we observe that , based on the user query reformulation behavior , within a single session a large portion of query terms is retained from the previously submitted queries and consists of mostly infrequent or unseen terms that are usually not included in the vocabulary . we therefore empower the decoder of our model to access the source words from the session context during decoding by incorporating a copy mechanism . moreover , we propose evaluation metrics to assess the quality of the generative models for query suggestion . we conduct an extensive set of experiments and analysis . e results suggest that our model outperforms the baselines both in terms of the generating queries and scoring candidate queries for the task of query suggestion ."}
{"title": "rio : minimizing user interaction in debugging of knowledge bases", "abstract": "the best currently known interactive debugging systems rely upon some meta-information in terms of fault probabilities in order to improve their efficiency . however , misleading meta information might result in a dramatic decrease of the performance and its assessment is only possible a-posteriori . consequently , as long as the actual fault is unknown , there is always some risk of suboptimal interactions . in this work we present a reinforcement learning strategy that continuously adapts its behavior depending on the performance achieved and minimizes the risk of using low-quality meta information . therefore , this method is suitable for application scenarios where reliable prior fault estimates are difficult to obtain . using diverse real-world knowledge bases , we show that the proposed interactive query strategy is scalable , features decent reaction time , and outperforms both entropy-based and no-risk strategies on average w.r.t . required amount of user interaction ."}
{"title": "an improved system for sentence-level novelty detection in textual streams", "abstract": "novelty detection in news events has long been a difficult problem . a number of models performed well on specific data streams but certain issues are far from being solved , particularly in large data streams from the www where unpredictability of new terms requires adaptation in the vector space model . we present a novel event detection system based on the incremental term frequency-inverse document frequency ( tf-idf ) weighting incorporated with locality sensitive hashing ( lsh ) . our system could efficiently and effectively adapt to the changes within the data streams of any new terms with continual updates to the vector space model . regarding miss probability , our proposed novelty detection framework outperforms a recognised baseline system by approximately 16 % when evaluating a benchmark dataset from google news ."}
{"title": "reasoning with very expressive fuzzy description logics", "abstract": "it is widely recognized today that the management of imprecision and vagueness will yield more intelligent and realistic knowledge-based applications . description logics ( dls ) are a family of knowledge representation languages that have gained considerable attention the last decade , mainly due to their decidability and the existence of empirically high performance of reasoning algorithms . in this paper , we extend the well known fuzzy alc dl to the fuzzy shin dl , which extends the fuzzy alc dl with transitive role axioms ( s ) , inverse roles ( i ) , role hierarchies ( h ) and number restrictions ( n ) . we illustrate why transitive role axioms are difficult to handle in the presence of fuzzy interpretations and how to handle them properly . then we extend these results by adding role hierarchies and finally number restrictions . the main contributions of the paper are the decidability proof of the fuzzy dl languages fuzzy-si and fuzzy-shin , as well as decision procedures for the knowledge base satisfiability problem of the fuzzy-si and fuzzy-shin ."}
{"title": "mining arguments from cancer documents using natural language processing and ontologies", "abstract": "in the medical domain , the continuous stream of scientific research contains contradictory results supported by arguments and counter-arguments . as medical expertise occurs at different levels , part of the human agents have difficulties to face the huge amount of studies , but also to understand the reasons and pieces of evidences claimed by the proponents and the opponents of the debated topic . to better understand the supporting arguments for new findings related to current state of the art in the medical domain we need tools able to identify arguments in scientific papers . our work here aims to fill the above technological gap . quite aware of the difficulty of this task , we embark to this road by relying on the well-known interleaving of domain knowledge with natural language processing . to formalise the existing medical knowledge , we rely on ontologies . to structure the argumentation model we use also the expressivity and reasoning capabilities of description logics . to perform argumentation mining we formalise various linguistic patterns in a rule-based language . we tested our solution against a corpus of scientific papers related to breast cancer . the run experiments show a f-measure between 0.71 and 0.86 for identifying conclusions of an argument and between 0.65 and 0.86 for identifying premises of an argument ."}
{"title": "fully decentralized multi-agent reinforcement learning with networked agents", "abstract": "we consider the problem of \\emph { fully decentralized } multi-agent reinforcement learning ( marl ) , where the agents are located at the nodes of a time-varying communication network . specifically , we assume that the reward functions of the agents might correspond to different tasks , and are only known to the corresponding agent . moreover , each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network . within this setting , the collective goal of the agents is to maximize the globally averaged return over the network through exchanging information with their neighbors . to this end , we propose two decentralized actor-critic algorithms with function approximation , which are applicable to large-scale marl problems where both the number of states and the number of agents are massively large . under the decentralized structure , the actor step is performed individually by each agent with no need to infer the policies of others . for the critic step , we propose a consensus update via communication over the network . our algorithms are fully incremental and can be implemented in an online fashion . convergence analyses of the algorithms are provided when the value functions are approximated within the class of linear functions . extensive simulation results with both linear and nonlinear function approximations are presented to validate the proposed algorithms . our work appears to be the first study of fully decentralized marl algorithms for networked agents with function approximation , with provable convergence guarantees ."}
{"title": "a game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems", "abstract": "the ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination . we conceptualise this problem formally using a game-theoretic model , called the stochastic bayesian game , in which the behaviour of a player is determined by its private information , or type . based on this model , we derive a solution , called harsanyi-bellman ad hoc coordination ( hba ) , which utilises the concept of bayesian nash equilibrium in a planning procedure to find optimal actions in the sense of bellman optimal control . we evaluate hba in a multiagent logistics domain called level-based foraging , showing that it achieves higher flexibility and efficiency than several alternative algorithms . we also report on a human-machine experiment at a public science exhibition in which the human participants played repeated prisoner 's dilemma and rock-paper-scissors against hba and alternative algorithms , showing that hba achieves equal efficiency and a significantly higher welfare and winning rate ."}
{"title": "exceptional subclasses in qualitative probability", "abstract": "system z+ [ goldszmidt and pearl , 1991 , goldszmidt , 1992 ] is a formalism for reasoning with normality defaults of the form `` typically if phi then + ( with strength cf ) '' where 6 is a positive integer . the system has a critical shortcoming in that it does not sanction inheritance across exceptional subclasses . in this paper we propose an extension to system z+ that rectifies this shortcoming by extracting additional conditions between worlds from the defaults database . we show that the additional constraints do not change the notion of the consistency of a database . we also make comparisons with competing default reasoning systems ."}
{"title": "learning belief networks in domains with recursively embedded pseudo independent submodels", "abstract": "a pseudo independent ( pi ) model is a probabilistic domain model ( pdm ) where proper subsets of a set of collectively dependent variables display marginal independence . pi models can not be learned correctly by many algorithms that rely on a single link search . earlier work on learning pi models has suggested a straightforward multi-link search algorithm . however , when a domain contains recursively embedded pi submodels , it may escape the detection of such an algorithm . in this paper , we propose an improved algorithm that ensures the learning of all embedded pi submodels whose sizes are upper bounded by a predetermined parameter . we show that this improved learning capability only increases the complexity slightly beyond that of the previous algorithm . the performance of the new algorithm is demonstrated through experiment ."}
{"title": "parallelpc : an r package for efficient constraint based causal exploration", "abstract": "discovering causal relationships from data is the ultimate goal of many research areas . constraint based causal exploration algorithms , such as pc , fci , rfci , pc-simple , ida and joint-ida have achieved significant progress and have many applications . a common problem with these methods is the high computational complexity , which hinders their applications in real world high dimensional datasets , e.g gene expression datasets . in this paper , we present an r package , parallelpc , that includes the parallelised versions of these causal exploration algorithms . the parallelised algorithms help speed up the procedure of experimenting big datasets and reduce the memory used when running the algorithms . the package is not only suitable for super-computers or clusters , but also convenient for researchers using personal computers with multi core cpus . our experiment results on real world datasets show that using the parallelised algorithms it is now practical to explore causal relationships in high dimensional datasets with thousands of variables in a single multicore computer . parallelpc is available in cran repository at https : //cran.rproject.org/web/packages/parallelpc/index.html ."}
{"title": "self-organized adaptation of a simple neural circuit enables complex robot behaviour", "abstract": "controlling sensori-motor systems in higher animals or complex robots is a challenging combinatorial problem , because many sensory signals need to be simultaneously coordinated into a broad behavioural spectrum . to rapidly interact with the environment , this control needs to be fast and adaptive . current robotic solutions operate with limited autonomy and are mostly restricted to few behavioural patterns . here we introduce chaos control as a new strategy to generate complex behaviour of an autonomous robot . in the presented system , 18 sensors drive 18 motors via a simple neural control circuit , thereby generating 11 basic behavioural patterns ( e.g. , orienting , taxis , self-protection , various gaits ) and their combinations . the control signal quickly and reversibly adapts to new situations and additionally enables learning and synaptic long-term storage of behaviourally useful motor responses . thus , such neural control provides a powerful yet simple way to self-organize versatile behaviours in autonomous agents with many degrees of freedom ."}
{"title": "stator flux optimization on direct torque control with fuzzy logic", "abstract": "the direct torque control ( dtc ) is well known as an effective control technique for high performance drives in a wide variety of industrial applications and conventional dtc technique uses two constant reference value : torque and stator flux . in this paper , fuzzy logic based stator flux optimization technique for dtc drives that has been proposed . the proposed fuzzy logic based stator flux optimizer self-regulates the stator flux reference using induction motor load situation without need of any motor parameters . simulation studies have been carried out with matlab/simulink to compare the proposed system behaviors at vary load conditions . simulation results show that the performance of the proposed dtc technique has been improved and especially at low-load conditions torque ripple are greatly reduced with respect to the conventional dtc ."}
{"title": "a study of entanglement in a categorical framework of natural language", "abstract": "in both quantum mechanics and corpus linguistics based on vector spaces , the notion of entanglement provides a means for the various subsystems to communicate with each other . in this paper we examine a number of implementations of the categorical framework of coecke , sadrzadeh and clark ( 2010 ) for natural language , from an entanglement perspective . specifically , our goal is to better understand in what way the level of entanglement of the relational tensors ( or the lack of it ) affects the compositional structures in practical situations . our findings reveal that a number of proposals for verb construction lead to almost separable tensors , a fact that considerably simplifies the interactions between the words . we examine the ramifications of this fact , and we show that the use of frobenius algebras mitigates the potential problems to a great extent . finally , we briefly examine a machine learning method that creates verb tensors exhibiting a sufficient level of entanglement ."}
{"title": "temporal difference updating without a learning rate", "abstract": "we derive an equation for temporal difference learning from statistical principles . specifically , we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates . the resulting equation is similar to the standard equation for temporal difference learning with eligibility traces , so called td ( lambda ) , however it lacks the parameter alpha that specifies the learning rate . in the place of this free parameter there is now an equation for the learning rate that is specific to each state transition . we experimentally test this new learning rule against td ( lambda ) and find that it offers superior performance in various settings . finally , we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning . to do this we combine our update equation with both watkins ' q ( lambda ) and sarsa ( lambda ) and find that it again offers superior performance without a learning rate parameter ."}
{"title": "one-shot session recommendation systems with combinatorial items", "abstract": "in recent years , content recommendation systems in large websites ( or \\emph { content providers } ) capture an increased focus . while the type of content varies , e.g.\\ movies , articles , music , advertisements , etc. , the high level problem remains the same . based on knowledge obtained so far on the user , recommend the most desired content . in this paper we present a method to handle the well known user-cold-start problem in recommendation systems . in this scenario , a recommendation system encounters a new user and the objective is to present items as relevant as possible with the hope of keeping the user 's session as long as possible . we formulate an optimization problem aimed to maximize the length of this initial session , as this is believed to be the key to have the user come back and perhaps register to the system . in particular , our model captures the fact that a single round with low quality recommendation is likely to terminate the session . in such a case , we do not proceed to the next round as the user leaves the system , possibly never to seen again . we denote this phenomenon a \\emph { one-shot session } . our optimization problem is formulated as an mdp where the action space is of a combinatorial nature as we recommend in each round , multiple items . this huge action space presents a computational challenge making the straightforward solution intractable . we analyze the structure of the mdp to prove monotone and submodular like properties that allow a computationally efficient solution via a method denoted by \\emph { greedy value iteration } ( g-vi ) ."}
{"title": "a model building framework for answer set programming with external computations", "abstract": "as software systems are getting increasingly connected , there is a need for equipping nonmonotonic logic programs with access to external sources that are possibly remote and may contain information in heterogeneous formats . to cater for this need , hex programs were designed as a generalization of answer set programs with an api style interface that allows to access arbitrary external sources , providing great flexibility . efficient evaluation of such programs however is challenging , and it requires to interleave external computation and model building ; to decide when to switch between these tasks is difficult , and existing approaches have limited scalability in many real-world application scenarios . we present a new approach for the evaluation of logic programs with external source access , which is based on a configurable framework for dividing the non-ground program into possibly overlapping smaller parts called evaluation units . the latter will be processed by interleaving external evaluation and model building using an evaluation graph and a model graph , respectively , and by combining intermediate results . experiments with our prototype implementation show a significant improvement compared to previous approaches . while designed for hex-programs , the new evaluation approach may be deployed to related rule-based formalisms as well ."}
{"title": "analysis and visualisation of rdf resources in ondex", "abstract": "ondex is a data integration and visualization platform developed to support systems biology research . at its core is a data model based on two main principles : first , all information can be represented as a graph and , second , all elements of the graph can be annotated with ontologies . this data model is conformant to the semantic web framework , in particular to rdf , and therefore ondex is ideally positioned as a platform that can exploit the semantic web ."}
{"title": "robust spectral inference for joint stochastic matrix factorization", "abstract": "spectral inference provides fast algorithms and provable optimality for latent topic analysis . but for real data these algorithms require additional ad-hoc heuristics , and even then often produce unusable results . we explain this poor performance by casting the problem of topic inference in the framework of joint stochastic matrix factorization ( jsmf ) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist . we then propose a novel rectification method that learns high quality topics and their interactions even on small , noisy data . this method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality ."}
{"title": "towards the development of a simulator for investigating the impact of people management practices on retail performance", "abstract": "often models for understanding the impact of management practices on retail performance are developed under the assumption of stability , equilibrium and linearity , whereas retail operations are considered in reality to be dynamic , non-linear and complex . alternatively , discrete event and agent-based modelling are approaches that allow the development of simulation models of heterogeneous non-equilibrium systems for testing out different scenarios . when developing simulation models one has to abstract and simplify from the real world , which means that one has to try and capture the 'essence ' of the system required for developing a representation of the mechanisms that drive the progression in the real system . simulation models can be developed at different levels of abstraction . to know the appropriate level of abstraction for a specific application is often more of an art than a science . we have developed a retail branch simulation model to investigate which level of model accuracy is required for such a model to obtain meaningful results for practitioners ."}
{"title": "information-theoretic interestingness measures for cross-ontology data mining", "abstract": "community annotation of biological entities with concepts from multiple bio-ontologies has created large and growing repositories of ontology-based annotation data with embedded implicit relationships among orthogonal ontologies . development of efficient data mining methods and metrics to mine and assess the quality of the mined relationships has not kept pace with the growth of annotation data . in this study , we present a data mining method that uses ontology-guided generalization to discover relationships across ontologies along with a new interestingness metric based on information theory . we apply our data mining algorithm and interestingness measures to datasets from the gene expression database at the mouse genome informatics as a preliminary proof of concept to mine relationships between developmental stages in the mouse anatomy ontology and gene ontology concepts ( biological process , molecular function and cellular component ) . in addition , we present a comparison of our interestingness metric to four existing metrics . ontology-based annotation datasets provide a valuable resource for discovery of relationships across ontologies . the use of efficient data mining methods and appropriate interestingness metrics enables the identification of high quality relationships ."}
{"title": "building competitive direct acoustics-to-word models for english conversational speech recognition", "abstract": "direct acoustics-to-word ( a2w ) models in the end-to-end paradigm have received increasing attention compared to conventional sub-word based automatic speech recognition models using phones , characters , or context-dependent hidden markov model states . this is because a2w models recognize words from speech without any decoder , pronunciation lexicon , or externally-trained language model , making training and decoding with such models simple . prior work has shown that a2w models require orders of magnitude more training data in order to perform comparably to conventional models . our work also showed this accuracy gap when using the english switchboard-fisher data set . this paper describes a recipe to train an a2w model that closes this gap and is at-par with state-of-the-art sub-word based models . we achieve a word error rate of 8.8 % /13.9 % on the hub5-2000 switchboard/callhome test sets without any decoder or language model . we find that model initialization , training data order , and regularization have the most impact on the a2w model performance . next , we present a joint word-character a2w model that learns to first spell the word and then recognize it . this model provides a rich output to the user instead of simple word hypotheses , making it especially useful in the case of words unseen or rarely-seen during training ."}
{"title": "detection of abnormal input-output associations", "abstract": "we study a novel outlier detection problem that aims to identify abnormal input-output associations in data , whose instances consist of multi-dimensional input ( context ) and output ( responses ) pairs . we present our approach that works by analyzing data in the conditional ( input -- output ) relation space , captured by a decomposable probabilistic model . experimental results demonstrate the ability of our approach in identifying multivariate conditional outliers ."}
{"title": "an influence diagram-based approach for estimating staff training in software industry", "abstract": "the successful completion of a software development process depends on the analytical capability and foresightedness of the project manager . for the project manager , the main intriguing task is to manage the risk factors as they adversely influence the completion deadline . one such key risk factor is staff training . the risk of this factor can be avoided by pre-judging the amount of training required by the staff . so , a procedure is required to help the project manager make this decision . this paper presents a system that uses influence diagrams to implement the risk model to aid decision making . the system also considers the cost of conducting the training , based on various risk factors such as , ( i ) lack of experience with project software ; ( ii ) newly appointed staff ; ( iii ) staff not well versed with the required quality standards ; and ( iv ) lack of experience with project environment . the system provides estimated requirement details for staff training at the beginning of a software development project ."}
{"title": "learning feature hierarchies with centered deep boltzmann machines", "abstract": "deep boltzmann machines are in principle powerful models for extracting the hierarchical structure of data . unfortunately , attempts to train layers jointly ( without greedy layer-wise pretraining ) have been largely unsuccessful . we propose a modification of the learning algorithm that initially recenters the output of the activation functions to zero . this modification leads to a better conditioned hessian and thus makes learning easier . we test the algorithm on real data and demonstrate that our suggestion , the centered deep boltzmann machine , learns a hierarchy of increasingly abstract representations and a better generative model of data ."}
{"title": "bat algorithm : literature review and applications", "abstract": "bat algorithm ( ba ) is a bio-inspired algorithm developed by yang in 2010 and ba has been found to be very efficient . as a result , the literature has expanded significantly in the last 3 years . this paper provides a timely review of the bat algorithm and its new variants . a wide range of diverse applications and case studies are also reviewed and summarized briefly here . further research topics are also discussed ."}
{"title": "structured sparsity via alternating direction methods", "abstract": "we consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm which incorporates prior knowledge of the group structure of the features . such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term . in this paper , we focus on two commonly adopted sparsity-inducing regularization terms , the overlapping group lasso penalty $ l_1/l_2 $ -norm and the $ l_1/l_\\infty $ -norm . we propose a unified framework based on the augmented lagrangian method , under which problems with both types of regularization and their variants can be efficiently solved . as the core building-block of this framework , we develop new algorithms using an alternating partial-linearization/splitting technique , and we prove that the accelerated versions of these algorithms require $ o ( \\frac { 1 } { \\sqrt { \\epsilon } } ) $ iterations to obtain an $ \\epsilon $ -optimal solution . to demonstrate the efficiency and relevance of our algorithms , we test them on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms ."}
{"title": "medical diagnosis from laboratory tests by combining generative and discriminative learning", "abstract": "a primary goal of computational phenotype research is to conduct medical diagnosis . in hospital , physicians rely on massive clinical data to make diagnosis decisions , among which laboratory tests are one of the most important resources . however , the longitudinal and incomplete nature of laboratory test data casts a significant challenge on its interpretation and usage , which may result in harmful decisions by both human physicians and automatic diagnosis systems . in this work , we take advantage of deep generative models to deal with the complex laboratory tests . specifically , we propose an end-to-end architecture that involves a deep generative variational recurrent neural networks ( vrnn ) to learn robust and generalizable features , and a discriminative neural network ( nn ) model to learn diagnosis decision making , and the two models are trained jointly . our experiments are conducted on a dataset involving 46,252 patients , and the 50 most frequent tests are used to predict the 50 most common diagnoses . the results show that our model , vrnn+nn , significantly ( p < 0.001 ) outperforms other baseline models . moreover , we demonstrate that the representations learned by the joint training are more informative than those learned by pure generative models . finally , we find that our model offers a surprisingly good imputation for missing values ."}
{"title": "the exact closest string problem as a constraint satisfaction problem", "abstract": "we report ( to our knowledge ) the first evaluation of constraint satisfaction as a computational framework for solving closest string problems . we show that careful consideration of symbol occurrences can provide search heuristics that provide several orders of magnitude speedup at and above the optimal distance . we also report ( to our knowledge ) the first analysis and evaluation -- using any technique -- of the computational difficulties involved in the identification of all closest strings for a given input set . we describe algorithms for web-scale distributed solution of closest string problems , both purely based on ai backtrack search and also hybrid numeric-ai methods ."}
{"title": "evidential reasoning with conditional belief functions", "abstract": "in the existing evidential networks with belief functions , the relations among the variables are always represented by joint belief functions on the product space of the involved variables . in this paper , we use conditional belief functions to represent such relations in the network and show some relations of these two kinds of representations . we also present a propagation algorithm for such networks . by analyzing the properties of some special evidential networks with conditional belief functions , we show that the reasoning process can be simplified in such kinds of networks ."}
{"title": "learning bayesian networks with local structure", "abstract": "in this paper we examine a novel addition to the known methods for learning bayesian networks from data that improves the quality of the learned networks . our approach explicitly represents and learns the local structure in the conditional probability tables ( cpts ) , that quantify these networks . this increases the space of possible models , enabling the representation of cpts with a variable number of parameters that depends on the learned local structures . the resulting learning procedure is capable of inducing models that better emulate the real complexity of the interactions present in the data . we describe the theoretical foundations and practical aspects of learning local structures , as well as an empirical evaluation of the proposed method . this evaluation indicates that learning curves characterizing the procedure that exploits the local structure converge faster than these of the standard procedure . our results also show that networks learned with local structure tend to be more complex ( in terms of arcs ) , yet require less parameters ."}
{"title": "an application of proof-theory in answer set programming", "abstract": "we apply proof-theoretic techniques in answer set programming . the main results include : 1. a characterization of continuity properties of gelfond-lifschitz operator for logic program . 2. a propositional characterization of stable models of logic programs ( without referring to loop formulas ."}
{"title": "the multi-engine asp solver me-asp : progress report", "abstract": "measp is a multi-engine solver for ground asp programs . it exploits algorithm selection techniques based on classification to select one among a set of out-of-the-box heterogeneous asp solvers used as black-box engines . in this paper we report on ( i ) a new optimized implementation of measp ; and ( ii ) an attempt of applying algorithm selection to non-ground programs . an experimental analysis reported in the paper shows that ( i ) the new implementation of \\measp is substantially faster than the previous version ; and ( ii ) the multi-engine recipe can be applied to the evaluation of non-ground programs with some benefits ."}
{"title": "a social welfare optimal sequential allocation procedure", "abstract": "we consider a simple sequential allocation procedure for sharing indivisible items between agents in which agents take turns to pick items . supposing additive utilities and independence between the agents , we show that the expected utility of each agent is computable in polynomial time . using this result , we prove that the expected utilitarian social welfare is maximized when agents take alternate turns . we also argue that this mechanism remains optimal when agents behave strategically"}
{"title": "proceedings of the pacific knowledge acquisition workshop 2004", "abstract": "artificial intelligence ( ai ) research has evolved over the last few decades and knowledge acquisition research is at the core of ai research . pkaw-04 is one of three international knowledge acquisition workshops held in the pacific-rim , canada and europe over the last two decades . pkaw-04 has a strong emphasis on incremental knowledge acquisition , machine learning , neural nets and active mining . the proceedings contain 19 papers that were selected by the program committee among 24 submitted papers . all papers were peer reviewed by at least two reviewers . the papers in these proceedings cover the methods and tools as well as the applications related to develop expert systems or knowledge based systems ."}
{"title": "the traits of the personable", "abstract": "information personalization is fertile ground for application of ai techniques . in this article i relate personalization to the ability to capture partial information in an information-seeking interaction . the specific focus is on personalizing interactions at web sites . using ideas from partial evaluation and explanation-based generalization , i present a modeling methodology for reasoning about personalization . this approach helps identify seven tiers of ` personable traits ' in web sites ."}
{"title": "interactive ontology debugging : two query strategies for efficient fault localization", "abstract": "effective debugging of ontologies is an important prerequisite for their broad application , especially in areas that rely on everyday users to create and maintain knowledge bases , such as the semantic web . in such systems ontologies capture formalized vocabularies of terms shared by its users . however in many cases users have different local views of the domain , i.e . of the context in which a given term is used . inappropriate usage of terms together with natural complications when formulating and understanding logical descriptions may result in faulty ontologies . recent ontology debugging approaches use diagnosis methods to identify causes of the faults . in most debugging scenarios these methods return many alternative diagnoses , thus placing the burden of fault localization on the user . this paper demonstrates how the target diagnosis can be identified by performing a sequence of observations , that is , by querying an oracle about entailments of the target ontology . to identify the best query we propose two query selection strategies : a simple `` split-in-half '' strategy and an entropy-based strategy . the latter allows knowledge about typical user errors to be exploited to minimize the number of queries . our evaluation showed that the entropy-based method significantly reduces the number of required queries compared to the `` split-in-half '' approach . we experimented with different probability distributions of user errors and different qualities of the a-priori probabilities . our measurements demonstrated the superiority of entropy-based query selection even in cases where all fault probabilities are equal , i.e . where no information about typical user errors is available ."}
{"title": "proceedings of the tenth conference on uncertainty in artificial intelligence ( 1994 )", "abstract": "this is the proceedings of the tenth conference on uncertainty in artificial intelligence , which was held in seattle , wa , july 29-31 , 1994"}
{"title": "fifth system for general-purpose connectionist computation", "abstract": "to date , work on formalizing connectionist computation in a way that is at least turing-complete has focused on recurrent architectures and developed equivalences to turing machines or similar super-turing models , which are of more theoretical than practical significance . we instead develop connectionist computation within the framework of information propagation networks extended with unbounded recursion , which is related to constraint logic programming and is more declarative than the semantics typically used in practical programming , but is still formally known to be turing-complete . this approach yields contributions to the theory and practice of both connectionist computation and programming languages . connectionist computations are carried out in a way that lets them communicate with , and be understood and interrogated directly in terms of the high-level semantics of a general-purpose programming language . meanwhile , difficult ( unbounded-dimension , np-hard ) search problems in programming that have previously been left to the programmer to solve in a heuristic , domain-specific way are solved uniformly a priori in a way that approximately achieves information-theoretic limits on performance ."}
{"title": "enhancing constraint propagation with composition operators", "abstract": "constraint propagation is a general algorithmic approach for pruning the search space of a csp . in a uniform way , k. r. apt has defined a computation as an iteration of reduction functions over a domain . he has also demonstrated the need for integrating static properties of reduction functions ( commutativity and semi-commutativity ) to design specialized algorithms such as ac3 and dac . we introduce here a set of operators for modeling compositions of reduction functions . two of the major goals are to tackle parallel computations , and dynamic behaviours ( such as slow convergence ) ."}
{"title": "representation dependence in probabilistic inference", "abstract": "non-deductive reasoning systems are often { \\em representation dependent } : representing the same situation in two different ways may cause such a system to return two different answers . some have viewed this as a significant problem . for example , the principle of maximum entropy has been subjected to much criticism due to its representation dependence . there has , however , been almost no work investigating representation dependence . in this paper , we formalize this notion and show that it is not a problem specific to maximum entropy . in fact , we show that any representation-independent probabilistic inference procedure that ignores irrelevant information is essentially entailment , in a precise sense . moreover , we show that representation independence is incompatible with even a weak default assumption of independence . we then show that invariance under a restricted class of representation changes can form a reasonable compromise between representation independence and other desiderata , and provide a construction of a family of inference procedures that provides such restricted representation independence , using relative entropy ."}
{"title": "pareto efficient multi objective optimization for local tuning of analogy based estimation", "abstract": "analogy based effort estimation ( abe ) is one of the prominent methods for software effort estimation . the fundamental concept of abe is closer to the mentality of expert estimation but with an automated procedure in which the final estimate is generated by reusing similar historical projects . the main key issue when using abe is how to adapt the effort of the retrieved nearest neighbors . the adaptation process is an essential part of abe to generate more successful accurate estimation based on tuning the selected raw solutions , using some adaptation strategy . in this study we show that there are three interrelated decision variables that have great impact on the success of adaptation method : ( 1 ) number of nearest analogies ( k ) , ( 2 ) optimum feature set needed for adaptation , and ( 3 ) adaptation weights . to find the right decision regarding these variables , one need to study all possible combinations and evaluate them individually to select the one that can improve all prediction evaluation measures . the existing evaluation measures usually behave differently , presenting sometimes opposite trends in evaluating prediction methods . this means that changing one decision variable could improve one evaluation measure while it is decreasing the others . therefore , the main theme of this research is how to come up with best decision variables that improve adaptation strategy and thus , the overall evaluation measures without degrading the others . the impact of these decisions together has not been investigated before , therefore we propose to view the building of adaptation procedure as a multi-objective optimization problem . the particle swarm optimization algorithm ( pso ) is utilized to find the optimum solutions for such decision variables based on optimizing multiple evaluation measures"}
{"title": "large-scale detection of non-technical losses in imbalanced data sets", "abstract": "non-technical losses ( ntl ) such as electricity theft cause significant harm to our economies , as in some countries they may range up to 40 % of the total electricity distributed . detecting ntls requires costly on-site inspections . accurate prediction of ntls for customers using machine learning is therefore crucial . to date , related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced , that ntl proportions may change and mostly consider small data sets , often not allowing to deploy the results in production . in this paper , we present a comprehensive approach to assess three ntl detection models for different ntl proportions in large real world data sets of 100ks of customers : boolean rules , fuzzy logic and support vector machine . this work has resulted in appreciable results that are about to be deployed in a leading industry solution . we believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets ."}
{"title": "listening to the world improves speech command recognition", "abstract": "we study transfer learning in convolutional network architectures applied to the task of recognizing audio , such as environmental sound events and speech commands . our key finding is that not only is it possible to transfer representations from an unrelated task like environmental sound classification to a voice-focused task like speech command recognition , but also that doing so improves accuracies significantly . we also investigate the effect of increased model capacity for transfer learning audio , by first validating known results from the field of computer vision of achieving better accuracies with increasingly deeper networks on two audio datasets : urbansound8k and the newly released google speech commands dataset . then we propose a simple multiscale input representation using dilated convolutions and show that it is able to aggregate larger contexts and increase classification performance . further , the models trained using a combination of transfer learning and multiscale input representations need only 40 % of the training data to achieve similar accuracies as a freshly trained model with 100 % of the training data . finally , we demonstrate a positive interaction effect for the multiscale input and transfer learning , making a case for the joint application of the two techniques ."}
{"title": "worst-case vs average-case design for estimation from fixed pairwise comparisons", "abstract": "pairwise comparison data arises in many domains , including tournament rankings , web search , and preference elicitation . given noisy comparisons of a fixed subset of pairs of items , we study the problem of estimating the underlying comparison probabilities under the assumption of strong stochastic transitivity ( sst ) . we also consider the noisy sorting subclass of the sst model . we show that when the assignment of items to the topology is arbitrary , these permutation-based models , unlike their parametric counterparts , do not admit consistent estimation for most comparison topologies used in practice . we then demonstrate that consistent estimation is possible when the assignment of items to the topology is randomized , thus establishing a dichotomy between worst-case and average-case designs . we propose two estimators in the average-case setting and analyze their risk , showing that it depends on the comparison topology only through the degree sequence of the topology . the rates achieved by these estimators are shown to be optimal for a large class of graphs . our results are corroborated by simulations on multiple comparison topologies ."}
{"title": "discrete and fuzzy dynamical genetic programming in the xcsf learning classifier system", "abstract": "a number of representation schemes have been presented for use within learning classifier systems , ranging from binary encodings to neural networks . this paper presents results from an investigation into using discrete and fuzzy dynamical system representations within the xcsf learning classifier system . in particular , asynchronous random boolean networks are used to represent the traditional condition-action production system rules in the discrete case and asynchronous fuzzy logic networks in the continuous-valued case . it is shown possible to use self-adaptive , open-ended evolution to design an ensemble of such dynamical systems within xcsf to solve a number of well-known test problems ."}
{"title": "a theoretical framework for context-sensitive temporal probability model construction with application to plan projection", "abstract": "we define a context-sensitive temporal probability logic for representing classes of discrete-time temporal bayesian networks . context constraints allow inference to be focused on only the relevant portions of the probabilistic knowledge . we provide a declarative semantics for our language . we present a bayesian network construction algorithm whose generated networks give sound and complete answers to queries . we use related concepts in logic programming to justify our approach . we have implemented a bayesian network construction algorithm for a subset of the theory and demonstrate it 's application to the problem of evaluating the effectiveness of treatments for acute cardiac conditions ."}
{"title": "formal definition of ai", "abstract": "a definition of artificial intelligence was proposed in [ 1 ] but this definition was not absolutely formal at least because the word `` human '' was used . in this paper we will formalize the definition from [ 1 ] . the biggest problem in this definition was that the level of intelligence of ai is compared to the intelligence of a human being . in order to change this we will introduce some parameters to which ai will depend . one of this parameters will be the level of intelligence and we will define one ai to each level of intelligence . we assume that for some level of intelligence the respective ai will be more intelligent than a human being . nevertheless , we can not say which is this level because we can not calculate its exact value ."}
{"title": "a comparison between supervised learning algorithms for word sense disambiguation", "abstract": "this paper describes a set of comparative experiments , including cross-corpus evaluation , between five alternative algorithms for supervised word sense disambiguation ( wsd ) , namely naive bayes , exemplar-based learning , snow , decision lists , and boosting . two main conclusions can be drawn : 1 ) the lazyboosting algorithm outperforms the other four state-of-the-art algorithms in terms of accuracy and ability to tune to new domains ; 2 ) the domain dependence of wsd systems seems very strong and suggests that some kind of adaptation or tuning is required for cross-corpus application ."}
{"title": "high-resolution multispectral dataset for semantic segmentation", "abstract": "unmanned aircraft have decreased the cost required to collect remote sensing imagery , which has enabled researchers to collect high-spatial resolution data from multiple sensor modalities more frequently and easily . the increase in data will push the need for semantic segmentation frameworks that are able to classify non-rgb imagery , but this type of algorithmic development requires an increase in publicly available benchmark datasets with class labels . in this paper , we introduce a high-resolution multispectral dataset with image labels . this new benchmark dataset has been pre-split into training/testing folds in order to standardize evaluation and continue to push state-of-the-art classification frameworks for non-rgb imagery ."}
{"title": "voi-aware mcts", "abstract": "uct , a state-of-the art algorithm for monte carlo tree search ( mcts ) in games and markov decision processes , is based on ucb1 , a sampling policy for the multi-armed bandit problem ( mab ) that minimizes the cumulative regret . however , search differs from mab in that in mcts it is usually only the final `` arm pull '' ( the actual move selection ) that collects a reward , rather than all `` arm pulls '' . in this paper , an mcts sampling policy based on value of information ( voi ) estimates of rollouts is suggested . empirical evaluation of the policy and comparison to ucb1 and uct is performed on random mab instances as well as on computer go ."}
{"title": "proceedings of the 12th workshop on user interfaces for theorem provers", "abstract": "the uitp workshop series brings together researchers interested in designing , developing and evaluating user interfaces for automated reasoning tools , such as interactive proof assistants , automated theorem provers , model finders , tools for formal methods , and tools for visualising and manipulating logical formulas and proofs . the twelth edition of uitp took place in coimbra , portugal , and was part of the international joint conference on automated reasoning ( ijcar'16 ) . the workshop consisted of an invited talk , six presentations of submitted papers and lively hands-on session for reasoning tools and their user-interface . these post-proceedings contain four contributed papers accepted for publication after a second round of reviewing after the workshop as well as the invited paper ."}
{"title": "pgmhd : a scalable probabilistic graphical model for massive hierarchical data problems", "abstract": "in the big data era , scalability has become a crucial requirement for any useful computational model . probabilistic graphical models are very useful for mining and discovering data insights , but they are not scalable enough to be suitable for big data problems . bayesian networks particularly demonstrate this limitation when their data is represented using few random variables while each random variable has a massive set of values . with hierarchical data - data that is arranged in a treelike structure with several levels - one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels . when modeling this kind of hierarchical data across large data sets , bayesian networks become infeasible for representing the probability distributions for the following reasons : i ) each level represents a single random variable with hundreds of thousands of values , ii ) the number of levels is usually small , so there are also few random variables , and iii ) the structure of the network is predefined since the dependency is modeled top-down from each parent to each of its child nodes , so the network would contain a single linear path for the random variables from each parent to each child node . in this paper we present a scalable probabilistic graphical model to overcome these limitations for massive hierarchical data . we believe the proposed model will lead to an easily-scalable , more readable , and expressive implementation for problems that require probabilistic-based solutions for massive amounts of hierarchical data . we successfully applied this model to solve two different challenging probabilistic-based problems on massive hierarchical data sets for different domains , namely , bioinformatics and latent semantic discovery over search logs ."}
{"title": "redundancy in logic iii : non-mononotonic reasoning", "abstract": "results about the redundancy of circumscriptive and default theories are presented . in particular , the complexity of establishing whether a given theory is redundant is establihsed ."}
{"title": "an analysis of tournament structure", "abstract": "this paper explores a novel way for analyzing the tournament structures to find a best suitable one for the tournament under consideration . it concerns about three aspects such as tournament conducting cost , competitiveness development and ranking precision . it then proposes a new method using progress tree to detect potential throwaway matches . the analysis performed using the proposed method reveals the strengths and weaknesses of tournament structures . as a conclusion , single elimination is best if we want to qualify one winner only , all matches conducted are exciting in term of competitiveness . double elimination with proper seeding system is a better choice if we want to qualify more winners . a reasonable number of extra matches need to be conducted in exchange of being able to qualify top four winners . round-robin gives reliable ranking precision for all participants . however , its conduction cost is very high , and it fails to maintain competitiveness development ."}
{"title": "on the generation of alternative explanations with implications for belief revision", "abstract": "in general , the best explanation for a given observation makes no promises on how good it is with respect to other alternative explanations . a major deficiency of message-passing schemes for belief revision in bayesian networks is their inability to generate alternatives beyond the second best . in this paper , we present a general approach based on linear constraint systems that naturally generates alternative explanations in an orderly and highly efficient manner . this approach is then applied to cost-based abduction problems as well as belief revision in bayesian net works ."}
{"title": "a random block-coordinate douglas-rachford splitting method with low computational complexity for binary logistic regression", "abstract": "in this paper , we propose a new optimization algorithm for sparse logistic regression based on a stochastic version of the douglas-rachford splitting method . our algorithm sweeps the training set by randomly selecting a mini-batch of data at each iteration , and it allows us to update the variables in a block coordinate manner . our approach leverages the proximity operator of the logistic loss , which is expressed with the generalized lambert w function . experiments carried out on standard datasets demonstrate the efficiency of our approach w.r.t . stochastic gradient-like methods ."}
{"title": "makeup like a superstar : deep localized makeup transfer network", "abstract": "in this paper , we propose a novel deep localized makeup transfer network to automatically recommend the most suitable makeup for a female and synthesis the makeup on her face . given a before-makeup face , her most suitable makeup is determined automatically . then , both the beforemakeup and the reference faces are fed into the proposed deep transfer network to generate the after-makeup face . our end-to-end makeup transfer network have several nice properties including : ( 1 ) with complete functions : including foundation , lip gloss , and eye shadow transfer ; ( 2 ) cosmetic specific : different cosmetics are transferred in different manners ; ( 3 ) localized : different cosmetics are applied on different facial regions ; ( 4 ) producing naturally looking results without obvious artifacts ; ( 5 ) controllable makeup lightness : various results from light makeup to heavy makeup can be generated . qualitative and quantitative experiments show that our network performs much better than the methods of [ guo and sim , 2009 ] and two variants of nerualstyle [ gatys et al. , 2015a ] ."}
{"title": "technical report : directed controller synthesis of discrete event systems", "abstract": "this paper presents a directed controller synthesis ( dcs ) technique for discrete event systems . the dcs method explores the solution space for reactive controllers guided by a domain-independent heuristic . the heuristic is derived from an efficient abstraction of the environment based on the componentized way in which complex environments are described . then by building the composition of the components on-the-fly dcs obtains a solution by exploring a reduced portion of the state space . this work focuses on untimed discrete event systems with safety and co-safety ( i.e . reachability ) goals . an evaluation for the technique is presented comparing it to other well-known approaches to controller synthesis ( based on symbolic representation and compositional analyses ) ."}
{"title": "xor-sampling for network design with correlated stochastic events", "abstract": "many network optimization problems can be formulated as stochastic network design problems in which edges are present or absent stochastically . furthermore , protective actions can guarantee that edges will remain present . we consider the problem of finding the optimal protection strategy under a budget limit in order to maximize some connectivity measurements of the network . previous approaches rely on the assumption that edges are independent . in this paper , we consider a more realistic setting where multiple edges are not independent due to natural disasters or regional events that make the states of multiple edges stochastically correlated . we use markov random fields to model the correlation and define a new stochastic network design framework . we provide a novel algorithm based on sample average approximation ( saa ) coupled with a gibbs or xor sampler . the experimental results on real road network data show that the policies produced by saa with the xor sampler have higher quality and lower variance compared to saa with gibbs sampler ."}
{"title": "visualizing the consequences of evidence in bayesian networks", "abstract": "this paper addresses the challenge of viewing and navigating bayesian networks as their structural size and complexity grow . starting with a review of the state of the art of visualizing bayesian networks , an area which has largely been passed over , we improve upon existing visualizations in three ways . first , we apply a disciplined approach to the graphic design of the basic elements of the bayesian network . second , we propose a technique for direct , visual comparison of posterior distributions resulting from alternative evidence sets . third , we leverage a central mathematical tool in information theory , to assist the user in finding variables of interest in the network , and to reduce visual complexity where unimportant . we present our methods applied to two modestly large bayesian networks constructed from real-world data sets . results suggest the new techniques can be a useful tool for discovering information flow phenomena , and also for qualitative comparisons of different evidence configurations , especially in large probabilistic networks ."}
{"title": "object-oriented bayesian networks", "abstract": "bayesian networks provide a modeling language and associated inference algorithm for stochastic domains . they have been successfully applied in a variety of medium-scale applications . however , when faced with a large complex domain , the task of modeling using bayesian networks begins to resemble the task of programming using logical circuits . in this paper , we describe an object-oriented bayesian network ( oobn ) language , which allows complex domains to be described in terms of inter-related objects . we use a bayesian network fragment to describe the probabilistic relations between the attributes of an object . these attributes can themselves be objects , providing a natural framework for encoding part-of hierarchies . classes are used to provide a reusable probabilistic model which can be applied to multiple similar objects . classes also support inheritance of model fragments from a class to a subclass , allowing the common aspects of related classes to be defined only once . our language has clear declarative semantics : an oobn can be interpreted as a stochastic functional program , so that it uniquely specifies a probabilistic model . we provide an inference algorithm for oobns , and show that much of the structural information encoded by an oobn -- particularly the encapsulation of variables within an object and the reuse of model fragments in different contexts -- can also be used to speed up the inference process ."}
{"title": "a scheme-driven approach to learning programs from input/output equations", "abstract": "we describe an approach to learn , in a term-rewriting setting , function definitions from input/output equations . by confining ourselves to structurally recursive definitions we obtain a fairly fast learning algorithm that often yields definitions close to intuitive expectations . we provide a prolog prototype implementation of our approach , and indicate open issues of further investigation ."}
{"title": "automated problem identification : regression vs classification via evolutionary deep networks", "abstract": "regression or classification ? this is perhaps the most basic question faced when tackling a new supervised learning problem . we present an evolutionary deep learning ( edl ) algorithm that automatically solves this by identifying the question type with high accuracy , along with a proposed deep architecture . typically , a significant amount of human insight and preparation is required prior to executing machine learning algorithms . for example , when creating deep neural networks , the number of parameters must be selected in advance and furthermore , a lot of these choices are made based upon pre-existing knowledge of the data such as the use of a categorical cross entropy loss function . humans are able to study a dataset and decide whether it represents a classification or a regression problem , and consequently make decisions which will be applied to the execution of the neural network . we propose the automated problem identification ( api ) algorithm , which uses an evolutionary algorithm interface to tensorflow to manipulate a deep neural network to decide if a dataset represents a classification or a regression problem . we test api on 16 different classification , regression and sentiment analysis datasets with up to 10,000 features and up to 17,000 unique target values . api achieves an average accuracy of $ 96.3\\ % $ in identifying the problem type without hardcoding any insights about the general characteristics of regression or classification problems . for example , api successfully identifies classification problems even with 1000 target values . furthermore , the algorithm recommends which loss function to use and also recommends a neural network architecture . our work is therefore a step towards fully automated machine learning ."}
{"title": "an information theoretic measure of judea pearl 's identifiability and causal influence", "abstract": "in this paper , we define a new information theoretic measure that we call the `` uprooted information '' . we show that a necessary and sufficient condition for a probability $ p ( s|do ( t ) ) $ to be `` identifiable '' ( in the sense of pearl ) in a graph $ g $ is that its uprooted information be non-negative for all models of the graph $ g $ . in this paper , we also give a new algorithm for deciding , for a bayesian net that is semi-markovian , whether a probability $ p ( s|do ( t ) ) $ is identifiable , and , if it is identifiable , for expressing it without allusions to confounding variables . our algorithm is closely based on a previous algorithm by tian and pearl , but seems to correct a small flaw in theirs . in this paper , we also find a { \\it necessary and sufficient graphical condition } for a probability $ p ( s|do ( t ) ) $ to be identifiable when $ t $ is a singleton set . so far , in the prior literature , it appears that only a { \\it sufficient graphical condition } has been given for this . by `` graphical '' we mean that it is directly based on judea pearl 's 3 rules of do-calculus ."}
{"title": "perception , attention , and resources : a decision-theoretic approach to graphics rendering", "abstract": "we describe work to control graphics rendering under limited computational resources by taking a decision-theoretic perspective on perceptual costs and computational savings of approximations . the work extends earlier work on the control of rendering by introducing methods and models for computing the expected cost associated with degradations of scene components . the expected cost is computed by considering the perceptual cost of degradations and a probability distribution over the attentional focus of viewers . we review the critical literature describing findings on visual search and attention , discuss the implications of the findings , and introduce models of expected perceptual cost . finally , we discuss policies that harness information about the expected cost of scene components ."}
{"title": "semi-bounded rationality : a model for decision making", "abstract": "in this paper the theory of semi-bounded rationality is proposed as an extension of the theory of bounded rationality . in particular , it is proposed that a decision making process involves two components and these are the correlation machine , which estimates missing values , and the causal machine , which relates the cause to the effect . rational decision making involves using information which is almost always imperfect and incomplete as well as some intelligent machine which if it is a human being is inconsistent to make decisions . in the theory of bounded rationality this decision is made irrespective of the fact that the information to be used is incomplete and imperfect and the human brain is inconsistent and thus this decision that is to be made is taken within the bounds of these limitations . in the theory of semi-bounded rationality , signal processing is used to filter noise and outliers in the information and the correlation machine is applied to complete the missing information and artificial intelligence is used to make more consistent decisions ."}
{"title": "integrating structured metadata with relational affinity propagation", "abstract": "structured and semi-structured data describing entities , taxonomies and ontologies appears in many domains . there is a huge interest in integrating structured information from multiple sources ; however integrating structured data to infer complex common structures is a difficult task because the integration must aggregate similar structures while avoiding structural inconsistencies that may appear when the data is combined . in this work , we study the integration of structured social metadata : shallow personal hierarchies specified by many individual users on the socialweb , and focus on inferring a collection of integrated , consistent taxonomies . we frame this task as an optimization problem with structural constraints . we propose a new inference algorithm , which we refer to as relational affinity propagation ( rap ) that extends affinity propagation ( frey and dueck 2007 ) by introducing structural constraints . we validate the approach on a real-world social media dataset , collected from the photosharing website flickr . our empirical results show that our proposed approach is able to construct deeper and denser structures compared to an approach using only the standard affinity propagation algorithm ."}
{"title": "modeling complex higher order patterns", "abstract": "the goal of this paper is to show that generalizing the notion of frequent patterns can be useful in extending association analysis to more complex higher order patterns . to that end , we describe a general framework for modeling a complex pattern based on evaluating the interestingness of its sub-patterns . a key goal of any framework is to allow people to more easily express , explore , and communicate ideas , and hence , we illustrate how our framework can be used to describe a variety of commonly used patterns , such as frequent patterns , frequent closed patterns , indirect association patterns , hub patterns and authority patterns . to further illustrate the usefulness of the framework , we also present two new kinds of patterns that derived from the framework : clique pattern and bi-clique pattern and illustrate their practical use ."}
{"title": "a first empirical study of emphatic temporal difference learning", "abstract": "in this paper we present the first empirical study of the emphatic temporal-difference learning algorithm ( etd ) , comparing it with conventional temporal-difference learning , in particular , with linear td ( 0 ) , on on-policy and off-policy variations of the mountain car problem . the initial motivation for developing etd was that it has good convergence properties under off-policy training ( sutton , mahmood and white 2016 ) , but it is also a new algorithm for the on-policy case . in both our on-policy and off-policy experiments , we found that each method converged to a characteristic asymptotic level of error , with etd better than td ( 0 ) . td ( 0 ) achieved a still lower error level temporarily before falling back to its higher asymptote , whereas etd never showed this kind of `` bounce '' . in the off-policy case ( in which td ( 0 ) is not guaranteed to converge ) , etd was significantly slower ."}
{"title": "ubuntuworld 1.0 lts - a platform for automated problem solving & troubleshooting in the ubuntu os", "abstract": "in this paper , we present ubuntuworld 1.0 lts - a platform for developing automated technical support agents in the ubuntu operating system . specifically , we propose to use the bash terminal as a simulator of the ubuntu environment for a learning-based agent and demonstrate the usefulness of adopting reinforcement learning ( rl ) techniques for basic problem solving and troubleshooting in this environment . we provide a plug-and-play interface to the simulator as a python package where different types of agents can be plugged in and evaluated , and provide pathways for integrating data from online support forums like askubuntu into an automated agent 's learning process . finally , we show that the use of this data significantly improves the agent 's learning efficiency . we believe that this platform can be adopted as a real-world test bed for research on automated technical support ."}
{"title": "defining explanation in probabilistic systems", "abstract": "as probabilistic systems gain popularity and are coming into wider use , the need for a mechanism that explains the system 's findings and recommendations becomes more critical . the system will also need a mechanism for ordering competing explanations . we examine two representative approaches to explanation in the literature - one due to g\\ '' ardenfors and one due to pearl - and show that both suffer from significant problems . we propose an approach to defining a notion of `` better explanation '' that combines some of the features of both together with more recent work by pearl and others on causality ."}
{"title": "recognizing plans by learning embeddings from observed action distributions", "abstract": "recent advances in visual activity recognition have raised the possibility of applications such as automated video surveillance . effective approaches for such problems however require the ability to recognize the plans of the agents from video information . although traditional plan recognition algorithms depend on access to sophisticated domain models , one recent promising direction involves learning shallow models directly from the observed activity sequences , and using them to recognize/predict plans . one limitation of such approaches is that they expect observed action sequences as training data . in many cases involving vision or sensing from raw data , there is considerably uncertainty about the specific action at any given time point . the most we can expect in such cases is probabilistic information about the action at that point . the training data will then be sequences of such observed action distributions . in this paper , we focus on doing effective plan recognition with such uncertain observations . our contribution is a novel extension of word vector embedding techniques to directly handle such observation distributions as input . this involves computing embeddings by minimizing the distance between distributions ( measured as kl-divergence ) . we will show that our approach has superior performance when the perception error rate ( per ) is higher , and competitive performance when the per is lower . we will also explore the possibility of using importance sampling techniques to handle observed action distributions with traditional word vector embeddings . we will show that although such approaches can give good recognition accuracy , they take significantly longer training time and their performance will degrade significantly at higher perception error rate ."}
{"title": "multi-agent path finding with delay probabilities", "abstract": "several recently developed multi-agent path finding ( mapf ) solvers scale to large mapf instances by searching for mapf plans on 2 levels : the high-level search resolves collisions between agents , and the low-level search plans paths for single agents under the constraints imposed by the high-level search . we make the following contributions to solve the mapf problem with imperfect plan execution with small average makespans : first , we formalize the mapf problem with delay probabilities ( mapf-dp ) , define valid mapf-dp plans and propose the use of robust plan-execution policies for valid mapf-dp plans to control how each agent proceeds along its path . second , we discuss 2 classes of decentralized robust plan-execution policies ( called fully synchronized policies and minimal communication policies ) that prevent collisions during plan execution for valid mapf-dp plans . third , we present a 2-level mapf-dp solver ( called approximate minimization in expectation ) that generates valid mapf-dp plans ."}
{"title": "automated planning in repeated adversarial games", "abstract": "game theory 's prescriptive power typically relies on full rationality and/or self-play interactions . in contrast , this work sets aside these fundamental premises and focuses instead on heterogeneous autonomous interactions between two or more agents . specifically , we introduce a new and concise representation for repeated adversarial ( constant-sum ) games that highlight the necessary features that enable an automated planing agent to reason about how to score above the game 's nash equilibrium , when facing heterogeneous adversaries . to this end , we present teamup , a model-based rl algorithm designed for learning and planning such an abstraction . in essence , it is somewhat similar to r-max with a cleverly engineered reward shaping that treats exploration as an adversarial optimization problem . in practice , it attempts to find an ally with which to tacitly collude ( in more than two-player games ) and then collaborates on a joint plan of actions that can consistently score a high utility in adversarial repeated games . we use the inaugural lemonade stand game tournament to demonstrate the effectiveness of our approach , and find that teamup is the best performing agent , demoting the tournament 's actual winning strategy into second place . in our experimental analysis , we show hat our strategy successfully and consistently builds collaborations with many different heterogeneous ( and sometimes very sophisticated ) adversaries ."}
{"title": "unicast and multicast qos routing with soft constraint logic programming", "abstract": "we present a formal model to represent and solve the unicast/multicast routing problem in networks with quality of service ( qos ) requirements . to attain this , first we translate the network adapting it to a weighted graph ( unicast ) or and-or graph ( multicast ) , where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link : each component of the weights vector represents a different qos metric value ( e.g . bandwidth , cost , delay , packet loss ) . the second step consists in writing this graph as a program in soft constraint logic programming ( sclp ) : the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them ( e.g . delay < 40msec ) , thus finding a solution to qos routing problems . moreover , c-semiring structures are a convenient tool to model qos metrics . at last , we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved ."}
{"title": "deepcloak : masking deep neural network models for robustness against adversarial samples", "abstract": "recent studies have shown that deep neural networks ( dnn ) are vulnerable to adversarial samples : maliciously-perturbed samples crafted to yield incorrect model outputs . such attacks can severely undermine dnn systems , particularly in security-sensitive settings . it was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task . to overcome this problem , we introduce a defensive mechanism called deepcloak . by identifying and removing unnecessary features in a dnn model , deepcloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs . comparing with other defensive approaches , deepcloak is easy to implement and computationally efficient . experimental results show that deepcloak can increase the performance of state-of-the-art dnn models against adversarial samples ."}
{"title": "reasoning about uncertainty in metric spaces", "abstract": "we set up a model for reasoning about metric spaces with belief theoretic measures . the uncertainty in these spaces stems from both probability and metric . to represent both aspect of uncertainty , we choose an expected distance function as a measure of uncertainty . a formal logical system is constructed for the reasoning about expected distance . soundness and completeness are shown for this logic . for reasoning on product metric space with uncertainty , a new metric is defined and shown to have good properties ."}
{"title": "partial-order planning with concurrent interacting actions", "abstract": "in order to generate plans for agents with multiple actuators , agent teams , or distributed controllers , we must be able to represent and plan using concurrent actions with interacting effects . this has historically been considered a challenging task requiring a temporal planner with the ability to reason explicitly about time . we show that with simple modifications , the strips action representation language can be used to represent interacting actions . moreover , algorithms for partial-order planning require only small modifications in order to be applied in such multiagent domains . we demonstrate this fact by developing a sound and complete partial-order planner for planning with concurrent interacting actions , pomp , that extends existing partial-order planners in a straightforward way . these results open the way to the use of partial-order planners for the centralized control of cooperative multiagent systems ."}
{"title": "generalization error of invariant classifiers", "abstract": "this paper studies the generalization error of invariant classifiers . in particular , we consider the common scenario where the classification task is invariant to certain transformations of the input , and that the classifier is constructed ( or learned ) to be invariant to these transformations . our approach relies on factoring the input space into a product of a base space and a set of transformations . we show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space , the generalization error of an invariant classifier is proportional to the complexity of the base space . we also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space . our analysis applies to general classifiers such as convolutional neural networks . we demonstrate the implications of the developed theory for such classifiers with experiments on the mnist and cifar-10 datasets ."}
{"title": "anomaly detection in a digital video broadcasting system using timed automata", "abstract": "this paper focuses on detecting anomalies in a digital video broadcasting ( dvb ) system from providers ' perspective . we learn a probabilistic deterministic real timed automaton profiling benign behavior of encryption control in the dvb control access system . this profile is used as a one-class classifier . anomalous items in a testing sequence are detected when the sequence is not accepted by the learned model ."}
{"title": "an effective algorithm for hyperparameter optimization of neural networks", "abstract": "a major challenge in designing neural network ( nn ) systems is to determine the best structure and parameters for the network given the data for the machine learning problem at hand . examples of parameters are the number of layers and nodes , the learning rates , and the dropout rates . typically , these parameters are chosen based on heuristic rules and manually fine-tuned , which may be very time-consuming , because evaluating the performance of a single parametrization of the nn may require several hours . this paper addresses the problem of choosing appropriate parameters for the nn by formulating it as a box-constrained mathematical optimization problem , and applying a derivative-free optimization tool that automatically and effectively searches the parameter space . the optimization tool employs a radial basis function model of the objective function ( the prediction accuracy of the nn ) to accelerate the discovery of configurations yielding high accuracy . candidate configurations explored by the algorithm are trained to a small number of epochs , and only the most promising candidates receive full training . the performance of the proposed methodology is assessed on benchmark sets and in the context of predicting drug-drug interactions , showing promising results . the optimization tool used in this paper is open-source ."}
{"title": "inferring unknown biological function by integration of go annotations and gene expression data", "abstract": "characterizing genes with semantic information is an important process regarding the description of gene products . in spite that complete genomes of many organisms have been already sequenced , the biological functions of all of their genes are still unknown . since experimentally studying the functions of those genes , one by one , would be unfeasible , new computational methods for gene functions inference are needed . we present here a novel computational approach for inferring biological function for a set of genes with previously unknown function , given a set of genes with well-known information . this approach is based on the premise that genes with similar behaviour should be grouped together . this is known as the guilt-by-association principle . thus , it is possible to take advantage of clustering techniques to obtain groups of unknown genes that are co-clustered with genes that have well-known semantic information ( go annotations ) . meaningful knowledge to infer unknown semantic information can therefore be provided by these well-known genes . we provide a method to explore the potential function of new genes according to those currently annotated . the results obtained indicate that the proposed approach could be a useful and effective tool when used by biologists to guide the inference of biological functions for recently discovered genes . our work sets an important landmark in the field of identifying unknown gene functions through clustering , using an external source of biological input . a simple web interface to this proposal can be found at http : //fich.unl.edu.ar/sinc/webdemo/gamma-am/ ."}
{"title": "motion planning of an autonomous mobile robot using artificial neural network", "abstract": "the paper presents the electronic design and motion planning of a robot based on decision making regarding its straight motion and precise turn using artificial neural network ( ann ) . the ann helps in learning of robot so that it performs motion autonomously . the weights calculated are implemented in microcontroller . the performance has been tested to be excellent ."}
{"title": "modeling uncertain and vague knowledge in possibility and evidence theories", "abstract": "this paper advocates the usefulness of new theories of uncertainty for the purpose of modeling some facets of uncertain knowledge , especially vagueness , in ai . it can be viewed as a partial reply to cheeseman 's ( among others ) defense of probability ."}
{"title": "distributed deep q-learning", "abstract": "we propose a distributed deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning . the model is based on the deep q-network , a convolutional neural network trained with a variant of q-learning . its input is raw pixels and its output is a value function estimating future rewards from taking an action given a system state . to distribute the deep q-network training , we adapt the distbelief software framework to the context of efficiently training reinforcement learning agents . as a result , the method is completely asynchronous and scales well with the number of machines . we demonstrate that the deep q-network agent , receiving only the pixels and the game score as inputs , was able to achieve reasonable success on a simple game with minimal parameter tuning ."}
{"title": "dealing with metonymic readings of named entities", "abstract": "the aim of this paper is to propose a method for tagging named entities ( ne ) , using natural language processing techniques . beyond their literal meaning , named entities are frequently subject to metonymy . we show the limits of current ne type hierarchies and detail a new proposal aiming at dynamically capturing the semantics of entities in context . this model can analyze complex linguistic phenomena like metonymy , which are known to be difficult for natural language processing but crucial for most applications . we present an implementation and some test using the french ester corpus and give significant results ."}
{"title": "a new general method to generate random modal formulae for testing decision procedures", "abstract": "the recent emergence of heavily-optimized modal decision procedures has highlighted the key role of empirical testing in this domain . unfortunately , the introduction of extensive empirical tests for modal logics is recent , and so far none of the proposed test generators is very satisfactory . to cope with this fact , we present a new random generation method that provides benefits over previous methods for generating empirical tests . it fixes and much generalizes one of the best-known methods , the random cnf_ [ ] m test , allowing for generating a much wider variety of problems , covering in principle the whole input space . our new method produces much more suitable test sets for the current generation of modal decision procedures . we analyze the features of the new method by means of an extensive collection of empirical tests ."}
{"title": "fairness as a program property", "abstract": "we explore the following question : is a decision-making program fair , for some useful definition of fairness ? first , we describe how several algorithmic fairness questions can be phrased as program verification problems . second , we discuss an automated verification technique for proving or disproving fairness of decision-making programs with respect to a probabilistic model of the population ."}
{"title": "on the ontological modeling of trees", "abstract": "trees -- i.e. , the type of data structure known under this name -- are central to many aspects of knowledge organization . we investigate some central design choices concerning the ontological modeling of such trees . in particular , we consider the limits of what is expressible in the web ontology language , and provide a reusable ontology design pattern for trees ."}
{"title": "kaggle lshtc4 winning solution", "abstract": "our winning submission to the 2014 kaggle competition for large scale hierarchical text classification ( lshtc ) consists mostly of an ensemble of sparse generative models extending multinomial naive bayes . the base-classifiers consist of hierarchically smoothed models combining document , label , and hierarchy level multinomials , with feature pre-processing using variants of tf-idf and bm25 . additional diversification is introduced by different types of folds and random search optimization for different measures . the ensemble algorithm optimizes macrofscore by predicting the documents for each label , instead of the usual prediction of labels per document . scores for documents are predicted by weighted voting of base-classifier outputs with a variant of feature-weighted linear stacking . the number of documents per label is chosen using label priors and thresholding of vote scores . this document describes the models and software used to build our solution . reproducing the results for our solution can be done by running the scripts included in the kaggle package . a package omitting precomputed result files is also distributed . all code is open source , released under gnu gpl 2.0 , and gpl 3.0 for weka and meka dependencies ."}
{"title": "recommender systems for the conference paper assignment problem", "abstract": "conference paper assignment , i.e. , the task of assigning paper submissions to reviewers , presents multi-faceted issues for recommender systems research . besides the traditional goal of predicting ` who likes what ? ' , a conference management system must take into account aspects such as : reviewer capacity constraints , adequate numbers of reviews for papers , expertise modeling , conflicts of interest , and an overall distribution of assignments that balances reviewer preferences with conference objectives . among these , issues of modeling preferences and tastes in reviewing have traditionally been studied separately from the optimization of paper-reviewer assignment . in this paper , we present an integrated study of both these aspects . first , due to the paucity of data per reviewer or per paper ( relative to other recommender systems applications ) we show how we can integrate multiple sources of information to learn paper-reviewer preference models . second , our models are evaluated not just in terms of prediction accuracy but in terms of the end-assignment quality . using a linear programming-based assignment optimization formulation , we show how our approach better explores the space of unsupplied assignments to maximize the overall affinities of papers assigned to reviewers . we demonstrate our results on real reviewer preference data from the ieee icdm 2007 conference ."}
{"title": "generic global constraints based on mdds", "abstract": "constraint programming ( cp ) has been successfully applied to both constraint satisfaction and constraint optimization problems . a wide variety of specialized global constraints provide critical assistance in achieving a good model that can take advantage of the structure of the problem in the search for a solution . however , a key outstanding issue is the representation of 'ad-hoc ' constraints that do not have an inherent combinatorial nature , and hence are not modeled well using narrowly specialized global constraints . we attempt to address this issue by considering a hybrid of search and compilation . specifically we suggest the use of reduced ordered multi-valued decision diagrams ( romdds ) as the supporting data structure for a generic global constraint . we give an algorithm for maintaining generalized arc consistency ( gac ) on this constraint that amortizes the cost of the gac computation over a root-to-leaf path in the search tree without requiring asymptotically more space than used for the mdd . furthermore we present an approach for incrementally maintaining the reduced property of the mdd during the search , and show how this can be used for providing domain entailment detection . finally we discuss how to apply our approach to other similar data structures such as aomdds and case dags . the technique used can be seen as an extension of the gac algorithm for the regular language constraint on finite length input ."}
{"title": "ksr : a semantic representation of knowledge graph within a novel unsupervised paradigm", "abstract": "knowledge representation is a long-history topic in ai , which is very important . a variety of models have been proposed for knowledge graph embedding , which projects symbolic entities and relations into continuous vector space . however , most related methods merely focus on the data-fitting of knowledge graph , and ignore the interpretable semantic expression . thus , traditional embedding methods are not friendly for applications that require semantic analysis , such as question answering and entity retrieval . to this end , this paper proposes a semantic representation method for knowledge graph \\textbf { ( ksr ) } , which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple . since both aspects and categories are semantics-relevant , the collection of categories in each aspect is treated as the semantic representation of this triple . extensive experiments show that our model outperforms other state-of-the-art baselines substantially ."}
{"title": "fishing out winners from vote streams", "abstract": "we investigate the problem of winner determination from computational social choice theory in the data stream model . specifically , we consider the task of summarizing an arbitrarily ordered stream of $ n $ votes on $ m $ candidates into a small space data structure so as to be able to obtain the winner determined by popular voting rules . as we show , finding the exact winner requires storing essentially all the votes . so , we focus on the problem of finding an { \\em $ \\eps $ -winner } , a candidate who could win by a change of at most $ \\eps $ fraction of the votes . we show non-trivial upper and lower bounds on the space complexity of $ \\eps $ -winner determination for several voting rules , including $ k $ -approval , $ k $ -veto , scoring rules , approval , maximin , bucklin , copeland , and plurality with run off ."}
{"title": "application of fuzzy assessing for reliability decision making", "abstract": "this paper proposes a new fuzzy assessing procedure with application in management decision making . the proposed fuzzy approach build the membership functions for system characteristics of a standby repairable system . this method is used to extract a family of conventional crisp intervals from the fuzzy repairable system for the desired system characteristics . this can be determined with a set of nonlinear parametric programing using the membership functions . when system characteristics are governed by the membership functions , more information is provided for use by management , and because the redundant system is extended to the fuzzy environment , general repairable systems are represented more accurately and the analytic results are more useful for designers and practitioners . also beside standby , active redundancy systems are used in many cases so this article has many practical instances . different from other studies , our model provides , a good estimated value based on uncertain environments , a comparison discussion of using fuzzy theory and conventional method and also a comparison between parallel ( active redundancy ) and series system in fuzzy world when we have standby redundancy . when the membership function intervals can not be inverted explicitly , system management or designers can specify the system characteristics of interest , perform numerical calculations , examine the corresponding { \\alpha } -cuts , and use this information to develop or improve system processes ."}
{"title": "scatter component analysis : a unified framework for domain adaptation and domain generalization", "abstract": "this paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from ( but related to ) the target . two closely related frameworks , domain adaptation and domain generalization , are concerned with such tasks , where the only difference between those frameworks is the availability of the unlabeled target data : domain adaptation can leverage unlabeled target information , while domain generalization can not . we propose scatter component analyis ( sca ) , a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization . sca is based on a simple geometrical measure , i.e. , scatter , which operates on reproducing kernel hilbert space . sca finds a representation that trades between maximizing the separability of classes , minimizing the mismatch between domains , and maximizing the separability of data ; each of which is quantified through scatter . the optimization problem of sca can be reduced to a generalized eigenvalue problem , which results in a fast and exact solution . comprehensive experiments on benchmark cross-domain object recognition datasets verify that sca performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization . we also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation ."}
{"title": "efficient , safe , and probably approximately complete learning of action models", "abstract": "in this paper we explore the theoretical boundaries of planning in a setting where no model of the agent 's actions is given . instead of an action model , a set of successfully executed plans are given and the task is to generate a plan that is safe , i.e. , guaranteed to achieve the goal without failing . to this end , we show how to learn a conservative model of the world in which actions are guaranteed to be applicable . this conservative model is then given to an off-the-shelf classical planner , resulting in a plan that is guaranteed to achieve the goal . however , this reduction from a model-free planning to a model-based planning is not complete : in some cases a plan will not be found even when such exists . we analyze the relation between the number of observed plans and the likelihood that our conservative approach will indeed fail to solve a solvable problem . our analysis show that the number of trajectories needed scales gracefully ."}
{"title": "no need to worry about adversarial examples in object detection in autonomous vehicles", "abstract": "it has been shown that most machine learning algorithms are susceptible to adversarial perturbations . slightly perturbing an image in a carefully chosen direction in the image space may cause a trained neural network model to misclassify it . recently , it was shown that physical adversarial examples exist : printing perturbed images then taking pictures of them would still result in misclassification . this raises security and safety concerns . however , these experiments ignore a crucial property of physical objects : the camera can view objects from different distances and at different angles . in this paper , we show experiments that suggest that current constructions of physical adversarial examples do not disrupt object detection from a moving platform . instead , a trained neural network classifies most of the pictures taken from different distances and angles of a perturbed image correctly . we believe this is because the adversarial property of the perturbation is sensitive to the scale at which the perturbed picture is viewed , so ( for example ) an autonomous car will misclassify a stop sign only from a small range of distances . our work raises an important question : can one construct examples that are adversarial for many or most viewing conditions ? if so , the construction should offer very significant insights into the internal representation of patterns by deep networks . if not , there is a good prospect that adversarial examples can be reduced to a curiosity with little practical impact ."}
{"title": "reasoning about beliefs and actions under computational resource constraints", "abstract": "although many investigators affirm a desire to build reasoning systems that behave consistently with the axiomatic basis defined by probability theory and utility theory , limited resources for engineering and computation can make a complete normative analysis impossible . we attempt to move discussion beyond the debate over the scope of problems that can be handled effectively to cases where it is clear that there are insufficient computational resources to perform an analysis deemed as complete . under these conditions , we stress the importance of considering the expected costs and benefits of applying alternative approximation procedures and heuristics for computation and knowledge acquisition . we discuss how knowledge about the structure of user utility can be used to control value tradeoffs for tailoring inference to alternative contexts . we address the notion of real-time rationality , focusing on the application of knowledge about the expected timewise-refinement abilities of reasoning strategies to balance the benefits of additional computation with the costs of acting with a partial result . we discuss the benefits of applying decision theory to control the solution of difficult problems given limitations and uncertainty in reasoning resources ."}
{"title": "filter based taxonomy modification for improving hierarchical classification", "abstract": "hierarchical classification ( hc ) is a supervised learning problem where unlabeled instances are classified into a taxonomy of classes . several methods that utilize the hierarchical structure have been developed to improve the hc performance . however , in most cases apriori defined hierarchical structure by domain experts is inconsistent ; as a consequence performance improvement is not noticeable in comparison to flat classification methods . we propose a scalable data-driven filter based rewiring approach to modify an expert-defined hierarchy . experimental comparisons of top-down hc with our modified hierarchy , on a wide range of datasets shows classification performance improvement over the baseline hierarchy ( i : e : , defined by expert ) , clustered hierarchy and flattening based hierarchy modification approaches . in comparison to existing rewiring approaches , our developed method ( rewhier ) is computationally efficient , enabling it to scale to datasets with large numbers of classes , instances and features . we also show that our modified hierarchy leads to improved classification performance for classes with few training samples in comparison to flat and state-of-the-art hc approaches ."}
{"title": "non-iterative label propagation on optimal leading forest", "abstract": "graph based semi-supervised learning ( gssl ) has intuitive representation and can be improved by exploiting the matrix calculation . however , it has to perform iterative optimization to achieve a preset objective , which usually leads to low efficiency . another inconvenience lying in gssl is that when new data come , the graph construction and the optimization have to be conducted all over again . we propose a sound assumption , arguing that : the neighboring data points are not in peer-to-peer relation , but in a partial-ordered relation induced by the local density and distance between the data ; and the label of a center can be regarded as the contribution of its followers . starting from the assumption , we develop a highly efficient non-iterative label propagation algorithm based on a novel data structure named as optimal leading forest ( lapoleaf ) . the major weaknesses of the traditional gssl are addressed by this study . we further scale lapoleaf to accommodate big data by utilizing block distance matrix technique , parallel computing , and locality-sensitive hashing ( lsh ) . experiments on large datasets have shown the promising results of the proposed methods ."}
{"title": "the complex negotiation dialogue game", "abstract": "this position paper formalises an abstract model for complex negotiation dialogue . this model is to be used for the benchmark of optimisation algorithms ranging from reinforcement learning to stochastic games , through transfer learning , one-shot learning or others ."}
{"title": "model-based diagnosis with qualitative temporal uncertainty", "abstract": "in this paper we describe a framework for model-based diagnosis of dynamic systems , which extends previous work in this field by using and expressing temporal uncertainty in the form of qualitative interval relations a la allen . based on a logical framework extended by qualitative and quantitative temporal constraints we show how to describe behavioral models ( both consistency- and abductive-based ) , discuss how to use abstract observations and show how abstract temporal diagnoses are computed . this yields an expressive framework , which allows the representation of complex temporal behavior allowing us to represent temporal uncertainty . due to its abstraction capabilities computation is made independent of the number of observations and time points in a temporal setting . an example of hepatitis diagnosis is used throughout the paper ."}
{"title": "learning geometrically-constrained hidden markov models for robot navigation : bridging the topological-geometrical gap", "abstract": "hidden markov models ( hmms ) and partially observable markov decision processes ( pomdps ) provide useful tools for modeling dynamical systems . they are particularly useful for representing the topology of environments such as road networks and office buildings , which are typical for robot navigation and planning . the work presented here describes a formal framework for incorporating readily available odometric information and geometrical constraints into both the models and the algorithm that learns them . by taking advantage of such information , learning hmms/pomdps can be made to generate better solutions and require fewer iterations , while being robust in the face of data reduction . experimental results , obtained from both simulated and real robot data , demonstrate the effectiveness of the approach ."}
{"title": "learning to generate posters of scientific papers", "abstract": "researchers often summarize their work in the form of posters . posters provide a coherent and efficient way to convey core ideas from scientific papers . generating a good scientific poster , however , is a complex and time consuming cognitive task , since such posters need to be readable , informative , and visually aesthetic . in this paper , for the first time , we study the challenging problem of learning to generate posters from scientific papers . to this end , a data-driven framework , that utilizes graphical models , is proposed . specifically , given content to display , the key elements of a good poster , including panel layout and attributes of each panel , are learned and inferred from data . then , given inferred layout and attributes , composition of graphical elements within each panel is synthesized . to learn and validate our model , we collect and make public a poster-paper dataset , which consists of scientific papers and corresponding posters with exhaustively labelled panels and attributes . qualitative and quantitative results indicate the effectiveness of our approach ."}
{"title": "on the average similarity degree between solutions of random k-sat and random csps", "abstract": "to study the structure of solutions for random k-sat and random csps , this paper introduces the concept of average similarity degree to characterize how solutions are similar to each other . it is proved that under certain conditions , as r ( i.e . the ratio of constraints to variables ) increases , the limit of average similarity degree when the number of variables approaches infinity exhibits phase transitions at a threshold point , shifting from a smaller value to a larger value abruptly . for random k-sat this phenomenon will occur when k > 4 . it is further shown that this threshold point is also a singular point with respect to r in the asymptotic estimate of the second moment of the number of solutions . finally , we discuss how this work is helpful to understand the hardness of solving random instances and a possible application of it to the design of search algorithms ."}
{"title": "a new look at the easy-hard-easy pattern of combinatorial search difficulty", "abstract": "the easy-hard-easy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning . we test the generality of this explanation by examining one of its predictions : if the number of solutions is held fixed by the choice of problems , then increased pruning should lead to a monotonic decrease in search cost . instead , we find the easy-hard-easy pattern in median search cost even when the number of solutions is held constant , for some search methods . this generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost . in these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems , rather than changing numbers of solutions ."}
{"title": "workflow complexity for collaborative interactions : where are the metrics ? -- a challenge", "abstract": "in this paper , we introduce the problem of denoting and deriving the complexity of workflows ( plans , schedules ) in collaborative , planner-assisted settings where humans and agents are trying to jointly solve a task . the interactions -- and hence the workflows that connect the human and the agents -- may differ according to the domain and the kind of agents . we adapt insights from prior work in human-agent teaming and workflow analysis to suggest metrics for workflow complexity . the main motivation behind this work is to highlight metrics for human comprehensibility of plans and schedules . the planning community has seen its fair share of work on the synthesis of plans that take diversity into account -- what value do such plans hold if their generation is not guided at least in part by metrics that reflect the ease of engaging with and using those plans ?"}
{"title": "inducing honest reporting without observing outcomes : an application to the peer-review process", "abstract": "when eliciting opinions from a group of experts , traditional devices used to promote honest reporting assume that there is an observable future outcome . in practice , however , this assumption is not always reasonable . in this paper , we propose a scoring method built on strictly proper scoring rules to induce honest reporting without assuming observable outcomes . our method provides scores based on pairwise comparisons between the reports made by each pair of experts in the group . for ease of exposition , we introduce our scoring method by illustrating its application to the peer-review process . in order to do so , we start by modeling the peer-review process using a bayesian model where the uncertainty regarding the quality of the manuscript is taken into account . thereafter , we introduce our scoring method to evaluate the reported reviews . under the assumptions that reviewers are bayesian decision-makers and that they can not influence the reviews of other reviewers , we show that risk-neutral reviewers strictly maximize their expected scores by honestly disclosing their reviews . we also show how the group 's scores can be used to find a consensual review . experimental results show that encouraging honest reporting through the proposed scoring method creates more accurate reviews than the traditional peer-review process ."}
{"title": "refining recency search results with user click feedback", "abstract": "traditional machine-learned ranking systems for web search are often trained to capture stationary relevance of documents to queries , which has limited ability to track non-stationary user intention in a timely manner . in recency search , for instance , the relevance of documents to a query on breaking news often changes significantly over time , requiring effective adaptation to user intention . in this paper , we focus on recency search and study a number of algorithms to improve ranking results by leveraging user click feedback . our contributions are three-fold . first , we use real search sessions collected in a random exploration bucket for \\emph { reliable } offline evaluation of these algorithms , which provides an unbiased comparison across algorithms without online bucket tests . second , we propose a re-ranking approach to improve search results for recency queries using user clicks . third , our empirical comparison of a dozen algorithms on real-life search data suggests importance of a few algorithmic choices in these applications , including generalization across different query-document pairs , specialization to popular queries , and real-time adaptation of user clicks ."}
{"title": "directed information graphs", "abstract": "we propose a graphical model for representing networks of stochastic processes , the minimal generative model graph . it is based on reduced factorizations of the joint distribution over time . we show that under appropriate conditions , it is unique and consistent with another type of graphical model , the directed information graph , which is based on a generalization of granger causality . we demonstrate how directed information quantifies granger causality in a particular sequential prediction setting . we also develop efficient methods to estimate the topological structure from data that obviate estimating the joint statistics . one algorithm assumes upper-bounds on the degrees and uses the minimal dimension statistics necessary . in the event that the upper-bounds are not valid , the resulting graph is nonetheless an optimal approximation . another algorithm uses near-minimal dimension statistics when no bounds are known but the distribution satisfies a certain criterion . analogous to how structure learning algorithms for undirected graphical models use mutual information estimates , these algorithms use directed information estimates . we characterize the sample-complexity of two plug-in directed information estimators and obtain confidence intervals . for the setting when point estimates are unreliable , we propose an algorithm that uses confidence intervals to identify the best approximation that is robust to estimation error . lastly , we demonstrate the effectiveness of the proposed algorithms through analysis of both synthetic data and real data from the twitter network . in the latter case , we identify which news sources influence users in the network by merely analyzing tweet times ."}
{"title": "pragmatic-pedagogic value alignment", "abstract": "as intelligent systems gain autonomy and capability , it becomes vital to ensure that their objectives match those of their human users ; this is known as the value-alignment problem . in robotics , value alignment is key to the design of collaborative robots that can integrate into human workflows , successfully inferring and adapting to their users ' objectives as they go . we argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition , enabling robots to tap into people 's natural collaborative capabilities . we present a solution to the cooperative inverse reinforcement learning ( cirl ) dynamic game based on well-established cognitive models of decision making and theory of mind . the solution captures a key reciprocity relation : the human will not plan her actions in isolation , but rather reason pedagogically about how the robot might learn from them ; the robot , in turn , can anticipate this and interpret the human 's actions pragmatically . to our knowledge , this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models ."}
{"title": "a soft computing model for physicians ' decision process", "abstract": "in this paper the author presents a kind of soft computing technique , mainly an application of fuzzy set theory of prof. zadeh [ 16 ] , on a problem of medical experts systems . the choosen problem is on design of a physician 's decision model which can take crisp as well as fuzzy data as input , unlike the traditional models . the author presents a mathematical model based on fuzzy set theory for physician aided evaluation of a complete representation of information emanating from the initial interview including patient past history , present symptoms , and signs observed upon physical examination and results of clinical and diagnostic tests ."}
{"title": "inverse reinforcement learning with simultaneous estimation of rewards and dynamics", "abstract": "inverse reinforcement learning ( irl ) describes the problem of learning an unknown reward function of a markov decision process ( mdp ) from observed behavior of an agent . since the agent 's behavior originates in its policy and mdp policies depend on both the stochastic system dynamics as well as the reward function , the solution of the inverse problem is significantly influenced by both . current irl approaches assume that if the transition model is unknown , additional samples from the system 's dynamics are accessible , or the observed behavior provides enough samples of the system 's dynamics to solve the inverse problem accurately . these assumptions are often not satisfied . to overcome this , we present a gradient-based irl approach that simultaneously estimates the system 's dynamics . by solving the combined optimization problem , our approach takes into account the bias of the demonstrations , which stems from the generating policy . the evaluation on a synthetic mdp and a transfer learning task shows improvements regarding the sample efficiency as well as the accuracy of the estimated reward functions and transition models ."}
{"title": "complexity results for manipulation , bribery and control of the kemeny procedure in judgment aggregation", "abstract": "we study the computational complexity of several scenarios of strategic behavior for the kemeny procedure in the setting of judgment aggregation . in particular , we investigate ( 1 ) manipulation , where an individual aims to achieve a better group outcome by reporting an insincere individual opinion , ( 2 ) bribery , where an external agent aims to achieve an outcome with certain properties by bribing a number of individuals , and ( 3 ) control ( by adding or deleting issues ) , where an external agent aims to achieve an outcome with certain properties by influencing the set of issues in the judgment aggregation situation . we show that determining whether these types of strategic behavior are possible ( and if so , computing a policy for successful strategic behavior ) is complete for the second level of the polynomial hierarchy . that is , we show that these problems are $ \\sigma^p_2 $ -complete ."}
{"title": "ultimate intelligence part i : physical completeness and objectivity of induction", "abstract": "we propose that solomonoff induction is complete in the physical sense via several strong physical arguments . we also argue that solomonoff induction is fully applicable to quantum mechanics . we show how to choose an objective reference machine for universal induction by defining a physical message complexity and physical message probability , and argue that this choice dissolves some well-known objections to universal induction . we also introduce many more variants of physical message complexity based on energy and action , and discuss the ramifications of our proposals ."}
{"title": "independence concepts for convex sets of probabilities", "abstract": "in this paper we study different concepts of independence for convex sets of probabilities . there will be two basic ideas for independence . the first is irrelevance . two variables are independent when a change on the knowledge about one variable does not affect the other . the second one is factorization . two variables are independent when the joint convex set of probabilities can be decomposed on the product of marginal convex sets . in the case of the theory of probability , these two starting points give rise to the same definition . in the case of convex sets of probabilities , the resulting concepts will be strongly related , but they will not be equivalent . as application of the concept of independence , we shall consider the problem of building a global convex set from marginal convex sets of probabilities ."}
{"title": "introduction to neutrosophic measure , neutrosophic integral , and neutrosophic probability", "abstract": "in this paper , we introduce for the first time the notions of neutrosophic measure and neutrosophic integral , and we develop the 1995 notion of neutrosophic probability . we present many practical examples . it is possible to define the neutrosophic measure and consequently the neutrosophic integral and neutrosophic probability in many ways , because there are various types of indeterminacies , depending on the problem we need to solve . neutrosophics study the indeterminacy . indeterminacy is different from randomness . it can be caused by physical space materials and type of construction , by items involved in the space , etc ."}
{"title": "restricted causal inference algorithm", "abstract": "this paper proposes a new algorithm for recovery of belief network structure from data handling hidden variables . it consists essentially in an extension of the ci algorithm of spirtes et al . by restricting the number of conditional dependencies checked up to k variables and in an extension of the original ci by additional steps transforming so called partial including path graph into a belief network . its correctness is demonstrated ."}
{"title": "characterizing concept drift", "abstract": "most machine learning models are static , but the world is dynamic , and increasing online deployment of learned models gives increasing urgency to the development of efficient and effective mechanisms to address learning in the context of non-stationary distributions , or as it is commonly called concept drift . however , the key issue of characterizing the different types of drift that can occur has not previously been subjected to rigorous definition and analysis . in particular , while some qualitative drift categorizations have been proposed , few have been formally defined , and the quantitative descriptions required for precise and objective understanding of learner performance have not existed . we present the first comprehensive framework for quantitative analysis of drift . this supports the development of the first comprehensive set of formal definitions of types of concept drift . the formal definitions clarify ambiguities and identify gaps in previous definitions , giving rise to a new comprehensive taxonomy of concept drift types and a solid foundation for research into mechanisms to detect and address concept drift ."}
{"title": "intra-team strategies for teams negotiating against competitor , matchers , and conceders", "abstract": "under some circumstances , a group of individuals may need to negotiate together as a negotiation team against another party . unlike bilateral negotiation between two individuals , this type of negotiations entails to adopt an intra-team strategy for negotiation teams in order to make team decisions and accordingly negotiate with the opponent . it is crucial to be able to negotiate successfully with heterogeneous opponents since opponents ' negotiation strategy and behavior may vary in an open environment . while one opponent might collaborate and concede over time , another may not be inclined to concede . this paper analyzes the performance of recently proposed intra-team strategies for negotiation teams against different categories of opponents : competitors , matchers , and conceders . furthermore , it provides an extension of the negotiation tool genius for negotiation teams in bilateral settings . consequently , this work facilitates research in negotiation teams ."}
{"title": "debugging machine learning tasks", "abstract": "unlike traditional programs ( such as operating systems or word processors ) which have large amounts of code , machine learning tasks use programs with relatively small amounts of code ( written in machine learning libraries ) , but voluminous amounts of data . just like developers of traditional programs debug errors in their code , developers of machine learning tasks debug and fix errors in their data . however , algorithms and tools for debugging and fixing errors in data are less common , when compared to their counterparts for detecting and fixing errors in code . in this paper , we consider classification tasks where errors in training data lead to misclassifications in test points , and propose an automated method to find the root causes of such misclassifications . our root cause analysis is based on pearl 's theory of causation , and uses pearl 's ps ( probability of sufficiency ) as a scoring metric . our implementation , psi , encodes the computation of ps as a probabilistic program , and uses recent work on probabilistic programs and transformations on probabilistic programs ( along with gray-box models of machine learning algorithms ) to efficiently compute ps . psi is able to identify root causes of data errors in interesting data sets ."}
{"title": "self-organizing the abstract : canvas as a swarm habitat for collective memory , perception and cooperative distributed creativity", "abstract": "past experiences under the designation of `` swarm paintings '' conducted in 2001 , not only confirmed the possibility of realizing an artificial art ( thus non-human ) , as introduced into the process the questioning of creative migration , specifically from the computer monitors to the canvas via a robotic harm . in more recent self-organized based research we seek to develop and profound the initial ideas by using a swarm of autonomous robots ( artsbot project 2002-03 ) , that `` live '' avoiding the purpose of being merely a simple perpetrator of order streams coming from an external computer , but instead , that actually co-evolve within the canvas space , acting ( that is , laying ink ) according to simple inner threshold stimulus response functions , reacting simultaneously to the chromatic stimulus present in the canvas environment done by the passage of their team-mates , as well as by the distributed feedback , affecting their future collective behaviour . in parallel , and in what respects to certain types of collective systems , we seek to confirm , in a physically embedded way , that the emergence of order ( even as a concept ) seems to be found at a lower level of complexity , based on simple and basic interchange of information , and on the local dynamic of parts , who , by self-organizing mechanisms tend to form an lived whole , innovative and adapting , allowing for emergent open-ended creative and distributed production . keywords : artsbots project , swarm intelligence , stigmergy , unmanned art , symbiotic art , swarm paintings , robot paintings , non-human art , painting emergence and cooperation , art and complexity , artbots : the robot talent show ."}
{"title": "weighted spectral cluster ensemble", "abstract": "clustering explores meaningful patterns in the non-labeled data sets . cluster ensemble selection ( ces ) is a new approach , which can combine individual clustering results for increasing the performance of the final results . although ces can achieve better final results in comparison with individual clustering algorithms and cluster ensemble methods , its performance can be dramatically affected by its consensus diversity metric and thresholding procedure . there are two problems in ces : 1 ) most of the diversity metrics is based on heuristic shannon 's entropy and 2 ) estimating threshold values are really hard in practice . the main goal of this paper is proposing a robust approach for solving the above mentioned problems . accordingly , this paper develops a novel framework for clustering problems , which is called weighted spectral cluster ensemble ( wsce ) , by exploiting some concepts from community detection arena and graph based clustering . under this framework , a new version of spectral clustering , which is called two kernels spectral clustering , is used for generating graphs based individual clustering results . further , by using modularity , which is a famous metric in the community detection , on the transformed graph representation of individual clustering results , our approach provides an effective diversity estimation for individual clustering results . moreover , this paper introduces a new approach for combining the evaluated individual clustering results without the procedure of thresholding . experimental study on varied data sets demonstrates that the prosed approach achieves superior performance to state-of-the-art methods ."}
{"title": "dynamic bayesian networks to simulate occupant behaviours in office buildings related to indoor air quality", "abstract": "this paper proposes a new general approach based on bayesian networks to model the human behaviour . this approach represents human behaviour with probabilistic cause-effect relations based on knowledge , but also with conditional probabilities coming either from knowledge or deduced from observations . this approach has been applied to the co-simulation of the co2 concentration in an office coupled with human behaviour ."}
{"title": "ensemble uct needs high exploitation", "abstract": "recent results have shown that the mcts algorithm ( a new , adaptive , randomized optimization algorithm ) is effective in a remarkably diverse set of applications in artificial intelligence , operations research , and high energy physics . mcts can find good solutions without domain dependent heuristics , using the uct formula to balance exploitation and exploration . it has been suggested that the optimum in the exploitation- exploration balance differs for different search tree sizes : small search trees needs more exploitation ; large search trees need more exploration . small search trees occur in variations of mcts , such as parallel and ensemble approaches . this paper investigates the possibility of improving the performance of ensemble uct by increasing the level of exploitation . as the search trees becomes smaller we achieve an improved performance . the results are important for improving the performance of large scale parallelism of mcts ."}
{"title": "reinforcement learning with linear function approximation and lq control converges", "abstract": "reinforcement learning is commonly used with function approximation . however , very few positive results are known about the convergence of function approximation based rl control algorithms . in this paper we show that td ( 0 ) and sarsa ( 0 ) with linear function approximation is convergent for a simple class of problems , where the system is linear and the costs are quadratic ( the lq control problem ) . furthermore , we show that for systems with gaussian noise and non-completely observable states ( the lqg problem ) , the mentioned rl algorithms are still convergent , if they are combined with kalman filtering ."}
{"title": "spatial-spectral boosting analysis for stroke patients ' motor imagery eeg in rehabilitation training", "abstract": "current studies about motor imagery based rehabilitation training systems for stroke subjects lack an appropriate analytic method , which can achieve a considerable classification accuracy , at the same time detects gradual changes of imagery patterns during rehabilitation process and disinters potential mechanisms about motor function recovery . in this study , we propose an adaptive boosting algorithm based on the cortex plasticity and spectral band shifts . this approach models the usually predetermined spatial-spectral configurations in eeg study into variable preconditions , and introduces a new heuristic of stochastic gradient boost for training base learners under these preconditions . we compare our proposed algorithm with commonly used methods on datasets collected from 2 months ' clinical experiments . the simulation results demonstrate the effectiveness of the method in detecting the variations of stroke patients ' eeg patterns . by chronologically reorganizing the weight parameters of the learned additive model , we verify the spatial compensatory mechanism on impaired cortex and detect the changes of accentuation bands in spectral domain , which may contribute important prior knowledge for rehabilitation practice ."}
{"title": "robust learning of fixed-structure bayesian networks", "abstract": "we investigate the problem of learning bayesian networks in an agnostic model where an $ \\epsilon $ -fraction of the samples are adversarially corrupted . our agnostic learning model is similar to -- in fact , stronger than -- huber 's contamination model in robust statistics . in this work , we study the fully observable bernoulli case where the structure of the network is given . even in this basic setting , previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees . we provide the first computationally efficient agnostic learning algorithm for this problem with dimension-independent error guarantees . our algorithm has polynomial sample complexity , runs in polynomial time , and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples ."}
{"title": "lift-based bidding in ad selection", "abstract": "real-time bidding ( rtb ) has become one of the largest online advertising markets in the world . today the bid price per ad impression is typically decided by the expected value of how it can lead to a desired action event ( e.g. , registering an account or placing a purchase order ) to the advertiser . however , this industry standard approach to decide the bid price does not consider the actual effect of the ad shown to the user , which should be measured based on the performance lift among users who have been or have not been exposed to a certain treatment of ads . in this paper , we propose a new bidding strategy and prove that if the bid price is decided based on the performance lift rather than absolute performance value , advertisers can actually gain more action events . we describe the modeling methodology to predict the performance lift and demonstrate the actual performance gain through blind a/b test with real ad campaigns in an industry-leading demand-side platform ( dsp ) . we also discuss the relationship between attribution models and bidding strategies . we prove that , to move the dsps to bid based on performance lift , they should be rewarded according to the relative performance lift they contribute ."}
{"title": "a convenient category for higher-order probability theory", "abstract": "higher-order probabilistic programming languages allow programmers to write sophisticated models in machine learning and statistics in a succinct and structured way , but step outside the standard measure-theoretic formalization of probability theory . programs may use both higher-order functions and continuous distributions , or even define a probability distribution on functions . but standard probability theory does not handle higher-order functions well : the category of measurable spaces is not cartesian closed . here we introduce quasi-borel spaces . we show that these spaces : form a new formalization of probability theory replacing measurable spaces ; form a cartesian closed category and so support higher-order functions ; form a well-pointed category and so support good proof principles for equational reasoning ; and support continuous probability distributions . we demonstrate the use of quasi-borel spaces for higher-order functions and probability by : showing that a well-known construction of probability theory involving random functions gains a cleaner expression ; and generalizing de finetti 's theorem , that is a crucial theorem in probability theory , to quasi-borel spaces ."}
{"title": "symbolic dynamic programming for discrete and continuous state mdps", "abstract": "many real-world decision-theoretic planning problems can be naturally modeled with discrete and continuous state markov decision processes ( dc-mdps ) . while previous work has addressed automated decision-theoretic planning for dcmdps , optimal solutions have only been defined so far for limited settings , e.g. , dc-mdps having hyper-rectangular piecewise linear value functions . in this work , we extend symbolic dynamic programming ( sdp ) techniques to provide optimal solutions for a vastly expanded class of dcmdps . to address the inherent combinatorial aspects of sdp , we introduce the xadd - a continuous variable extension of the algebraic decision diagram ( add ) - that maintains compact representations of the exact value function . empirically , we demonstrate an implementation of sdp with xadds on various dc-mdps , showing the first optimal automated solutions to dcmdps with linear and nonlinear piecewise partitioned value functions and showing the advantages of constraint-based pruning for xadds ."}
{"title": "augur : a modeling language for data-parallel probabilistic inference", "abstract": "it is time-consuming and error-prone to implement inference procedures for each new probabilistic model . probabilistic programming addresses this problem by allowing a user to specify the model and having a compiler automatically generate an inference procedure for it . for this approach to be practical , it is important to generate inference code that has reasonable performance . in this paper , we present a probabilistic programming language and compiler for bayesian networks designed to make effective use of data-parallel architectures such as gpus . our language is fully integrated within the scala programming language and benefits from tools such as ide support , type-checking , and code completion . we show that the compiler can generate data-parallel inference code scalable to thousands of gpu cores by making use of the conditional independence relationships in the bayesian network ."}
{"title": "a structured argumentation framework for detaching conditional obligations", "abstract": "we present a general formal argumentation system for dealing with the detachment of conditional obligations . given a set of facts , constraints , and conditional obligations , we answer the question whether an unconditional obligation is detachable by considering reasons for and against its detachment . for the evaluation of arguments in favor of detaching obligations we use a dung-style argumentation-theoretical semantics . we illustrate the modularity of the general framework by considering some extensions , and we compare the framework to some related approaches from the literature ."}
{"title": "the missing ones : key ingredients towards effective ambient assisted living systems", "abstract": "the population of elderly people keeps increasing rapidly , which becomes a predominant aspect of our societies . as such , solutions both efficacious and cost-effective need to be sought . ambient assisted living ( aal ) is a new approach which promises to address the needs from elderly people . in this paper , we claim that human participation is a key ingredient towards effective aal systems , which not only saves social resources , but also has positive relapses on the psychological health of the elderly people . challenges in increasing the human participation in ambient assisted living are discussed in this paper and solutions to meet those challenges are also proposed . we use our proposed mutual assistance community , which is built with service oriented approach , as an example to demonstrate how to integrate human tasks in aal systems . our preliminary simulation results are presented , which support the effectiveness of human participation ."}
{"title": "sequence tutor : conservative fine-tuning of sequence generation models with kl-control", "abstract": "this paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network ( rnn ) , while maintaining information originally learned from data , as well as sample diversity . an rnn is first pre-trained on data using maximum likelihood estimation ( mle ) , and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy . another rnn is then trained using reinforcement learning ( rl ) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the mle rnn . to formalize this objective , we derive novel off-policy rl methods for rnns from kl-control . the effectiveness of the approach is demonstrated on two applications ; 1 ) generating novel musical melodies , and 2 ) computational molecular generation . for both problems , we show that the proposed method improves the desired properties and structure of the generated sequences , while maintaining information learned from data ."}
{"title": "possible and necessary winner problem in social polls", "abstract": "social networks are increasingly being used to conduct polls . we introduce a simple model of such social polling . we suppose agents vote sequentially , but the order in which agents choose to vote is not necessarily fixed . we also suppose that an agent 's vote is influenced by the votes of their friends who have already voted . despite its simplicity , this model provides useful insights into a number of areas including social polling , sequential voting , and manipulation . we prove that the number of candidates and the network structure affect the computational complexity of computing which candidate necessarily or possibly can win in such a social poll . for social networks with bounded treewidth and a bounded number of candidates , we provide polynomial algorithms for both problems . in other cases , we prove that computing which candidates necessarily or possibly win are computationally intractable ."}
{"title": "making decisions using sets of probabilities : updating , time consistency , and calibration", "abstract": "we consider how an agent should update her beliefs when her beliefs are represented by a set p of probability distributions , given that the agent makes decisions using the minimax criterion , perhaps the best-studied and most commonly-used criterion in the literature . we adopt a game-theoretic framework , where the agent plays against a bookie , who chooses some distribution from p. we consider two reasonable games that differ in what the bookie knows when he makes his choice . anomalies that have been observed before , like time inconsistency , can be understood as arising because different games are being played , against bookies with different information . we characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information . finally , we consider the relationship between updating and calibration when uncertainty is described by sets of probabilities . our results emphasize the key role of the rectangularity condition of epstein and schneider ."}
{"title": "note on combinatorial engineering frameworks for hierarchical modular systems", "abstract": "the paper briefly describes a basic set of special combinatorial engineering frameworks for solving complex problems in the field of hierarchical modular systems . the frameworks consist of combinatorial problems ( and corresponding models ) , which are interconnected/linked ( e.g. , by preference relation ) . mainly , hierarchical morphological system model is used . the list of basic standard combinatorial engineering ( technological ) frameworks is the following : ( 1 ) design of system hierarchical model , ( 2 ) combinatorial synthesis ( 'bottom-up ' process for system design ) , ( 3 ) system evaluation , ( 4 ) detection of system bottlenecks , ( 5 ) system improvement ( re-design , upgrade ) , ( 6 ) multi-stage design ( design of system trajectory ) , ( 7 ) combinatorial modeling of system evolution/development and system forecasting . the combinatorial engineering frameworks are targeted to maintenance of some system life cycle stages . the list of main underlaying combinatorial optimization problems involves the following : knapsack problem , multiple-choice problem , assignment problem , spanning trees , morphological clique problem ."}
{"title": "security , privacy and safety evaluation of dynamic and static fleets of drones", "abstract": "inter-connected objects , either via public or private networks are the near future of modern societies . such inter-connected objects are referred to as internet-of-things ( iot ) and/or cyber-physical systems ( cps ) . one example of such a system is based on unmanned aerial vehicles ( uavs ) . the fleet of such vehicles are prophesied to take on multiple roles involving mundane to high-sensitive , such as , prompt pizza or shopping deliveries to your homes to battlefield deployment for reconnaissance and combat missions . drones , as we refer to uavs in this paper , either can operate individually ( solo missions ) or part of a fleet ( group missions ) , with and without constant connection with the base station . the base station acts as the command centre to manage the activities of the drones . however , an independent , localised and effective fleet control is required , potentially based on swarm intelligence , for the reasons : 1 ) increase in the number of drone fleets , 2 ) number of drones in a fleet might be multiple of tens , 3 ) time-criticality in making decisions by such fleets in the wild , 4 ) potential communication congestions/lag , and 5 ) in some cases working in challenging terrains that hinders or mandates-limited communication with control centre ( i.e. , operations spanning long period of times or military usage of such fleets in enemy territory ) . this self-ware , mission-focused and independent fleet of drones that potential utilises swarm intelligence for a ) air-traffic and/or flight control management , b ) obstacle avoidance , c ) self-preservation while maintaining the mission criteria , d ) collaboration with other fleets in the wild ( autonomously ) and e ) assuring the security , privacy and safety of physical ( drones itself ) and virtual ( data , software ) assets . in this paper , we investigate the challenges faced by fleet of drones and propose a potential course of action on how to overcome them ."}
{"title": "deep learning interior tomography for region-of-interest reconstruction", "abstract": "interior tomography for the region-of-interest ( roi ) imaging has advantages of using a small detector and reducing x-ray radiation dose . however , standard analytic reconstruction suffers from severe cupping artifacts due to existence of null space in the truncated radon transform . existing penalized reconstruction methods may address this problem but they require extensive computations due to the iterative reconstruction . inspired by the recent deep learning approaches to low-dose and sparse view ct , here we propose a deep learning architecture that removes null space signals from the fbp reconstruction . experimental results have shown that the proposed method provides near-perfect reconstruction with about 7-10 db improvement in psnr over existing methods in spite of significantly reduced run-time complexity ."}
{"title": "using artificial intelligence for model selection", "abstract": "we apply the optimization algorithm adaptive simulated annealing ( asa ) to the problem of analyzing data on a large population and selecting the best model to predict that an individual with various traits will have a particular disease . we compare asa with traditional forward and backward regression on computer simulated data . we find that the traditional methods of modeling are better for smaller data sets whereas a numerically stable asa seems to perform better on larger and more complicated data sets ."}
{"title": "learning what to read : focused machine reading", "abstract": "recent efforts in bioinformatics have achieved tremendous progress in the machine reading of biomedical literature , and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways . however , batch machine reading of literature at today 's scale ( pubmed alone indexes over 1 million papers per year ) is unfeasible due to both cost and processing overhead . in this work , we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible . we introduce a family of algorithms for focused reading , including an intuitive , strong baseline , and a second approach which uses a reinforcement learning ( rl ) framework that learns when to explore ( widen the search ) or exploit ( narrow it ) . we demonstrate that the rl approach is capable of answering more queries than the baseline , while being more efficient , i.e. , reading fewer documents ."}
{"title": "exact map inference in general higher-order graphical models using linear programming", "abstract": "this paper is concerned with the problem of exact map inference in general higher-order graphical models by means of a traditional linear programming relaxation approach . in fact , the proof that we have developed in this paper is a rather simple algebraic proof being made straightforward , above all , by the introduction of two novel algebraic tools . indeed , on the one hand , we introduce the notion of delta-distribution which merely stands for the difference of two arbitrary probability distributions , and which mainly serves to alleviate the sign constraint inherent to a traditional probability distribution . on the other hand , we develop an approximation framework of general discrete functions by means of an orthogonal projection expressing in terms of linear combinations of function margins with respect to a given collection of point subsets , though , we rather exploit the latter approach for the purpose of modeling locally consistent sets of discrete functions from a global perspective . after that , as a first step , we develop from scratch the expectation optimization framework which is nothing else than a reformulation , on stochastic grounds , of the convex-hull approach , as a second step , we develop the traditional lp relaxation of such an expectation optimization approach , and we show that it enables to solve the map inference problem in graphical models under rather general assumptions . last but not least , we describe an algorithm which allows to compute an exact map solution from a perhaps fractional optimal ( probability ) solution of the proposed lp relaxation ."}
{"title": "issues in exploiting germanet as a resource in real applications", "abstract": "this paper reports about experiments with germanet as a resource within domain specific document analysis . the main question to be answered is : how is the coverage of germanet in a specific domain ? we report about results of a field test of germanet for analyses of autopsy protocols and present a sketch about the integration of germanet inside xdoc . our remarks will contribute to a germanet user 's wish list ."}
{"title": "obtaining reliable feedback for sanctioning reputation mechanisms", "abstract": "reputation mechanisms offer an effective alternative to verification authorities for building trust in electronic markets with moral hazard . future clients guide their business decisions by considering the feedback from past transactions ; if truthfully exposed , cheating behavior is sanctioned and thus becomes irrational . it therefore becomes important to ensure that rational clients have the right incentives to report honestly . as an alternative to side-payment schemes that explicitly reward truthful reports , we show that honesty can emerge as a rational behavior when clients have a repeated presence in the market . to this end we describe a mechanism that supports an equilibrium where truthful feedback is obtained . then we characterize the set of pareto-optimal equilibria of the mechanism , and derive an upper bound on the percentage of false reports that can be recorded by the mechanism . an important role in the existence of this bound is played by the fact that rational clients can establish a reputation for reporting honestly ."}
{"title": "a novel method for developing robotics via artificial intelligence and internet of things", "abstract": "this paper describe about a new methodology for developing and improving the robotics field via artificial intelligence and internet of things . now a day , we can say artificial intelligence take the world into robotics . almost all industries use robots for lot of works . they are use co-operative robots to make different kind of works . but there was some problem to make robot for multi tasks . so there was a necessary new methodology to made multi tasking robots . it will be done only by artificial intelligence and internet of things ."}
{"title": "neural aggregation network for video face recognition", "abstract": "this paper presents a neural aggregation network ( nan ) for video face recognition . the network takes a face video or face image set of a person with a variable number of face images as its input , and produces a compact , fixed-dimension feature representation for recognition . the whole network is composed of two modules . the feature embedding module is a deep convolutional neural network ( cnn ) which maps each face image to a feature vector . the aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them . due to the attention mechanism , the aggregation is invariant to the image order . our nan is trained with a standard classification or verification loss without any extra supervision signal , and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred , occluded and improperly exposed faces . the experiments on ijb-a , youtube face , celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy ."}
{"title": "on interestingness measures of formal concepts", "abstract": "formal concepts and closed itemsets proved to be of big importance for knowledge discovery , both as a tool for concise representation of association rules and a tool for clustering and constructing domain taxonomies and ontologies . exponential explosion makes it difficult to consider the whole concept lattice arising from data , one needs to select most useful and interesting concepts . in this paper interestingness measures of concepts are considered and compared with respect to various aspects , such as efficiency of computation and applicability to noisy data and performing ranking correlation ."}
{"title": "a novel approach of solving the cnf-sat problem", "abstract": "in this paper , we discussed cnf-sat problem ( np-complete problem ) and analysis two solutions that can solve the problem , the pl-resolution algorithm and the walksat algorithm . pl-resolution is a sound and complete algorithm that can be used to determine satisfiability and unsatisfiability with certainty . walksat can determine satisfiability if it finds a model , but it can not guarantee to find a model even there exists one . however , walksat is much faster than pl-resolution , which makes walksat more practical ; and we have analysis the performance between these two algorithms , and the performance of walksat is acceptable if the problem is not so hard ."}
{"title": "automated coin recognition system using ann", "abstract": "coins are integral part of our day to day life . we use coins everywhere like grocery store , banks , buses , trains etc . so it becomes a basic need that coins can be sorted and counted automatically . for this it is necessary that coins can be recognized automatically . in this paper we have developed an ann ( artificial neural network ) based automated coin recognition system for the recognition of indian coins of denomination rs . 1 , 2 , 5 and 10 with rotation invariance . we have taken images from both sides of coin . so this system is capable of recognizing coins from both sides . features are extracted from images using techniques of hough transformation , pattern averaging etc . then , the extracted features are passed as input to a trained neural network . 97.74 % recognition rate has been achieved during the experiments i.e . only 2.26 % miss recognition , which is quite encouraging ."}
{"title": "efficient open world reasoning for planning", "abstract": "we consider the problem of reasoning and planning with incomplete knowledge and deterministic actions . we introduce a knowledge representation scheme called psiplan that can effectively represent incompleteness of an agent 's knowledge while allowing for sound , complete and tractable entailment in domains where the set of all objects is either unknown or infinite . we present a procedure for state update resulting from taking an action in psiplan that is correct , complete and has only polynomial complexity . state update is performed without considering the set of all possible worlds corresponding to the knowledge state . as a result , planning with psiplan is done without direct manipulation of possible worlds . psiplan representation underlies the psipop planning algorithm that handles quantified goals with or without exceptions that no other domain independent planner has been shown to achieve . psiplan has been implemented in common lisp and used in an application on planning in a collaborative interface ."}
{"title": "new methods of analysis of narrative and semantics in support of interactivity", "abstract": "our work has focused on support for film or television scriptwriting . since this involves potentially varied story-lines , we note the implicit or latent support for interactivity . furthermore the film , television , games , publishing and other sectors are converging , so that cross-over and re-use of one form of product in another of these sectors is ever more common . technically our work has been largely based on mathematical algorithms for data clustering and display . operationally , we also discuss how our algorithms can support collective , distributed problem-solving ."}
{"title": "digital libraries , conceptual knowledge systems , and the nebula interface", "abstract": "concept analysis provides a principled approach to effective management of wide area information systems , such as the nebula file system and interface . this not only offers evidence to support the assertion that a digital library is a bounded collection of incommensurate information sources in a logical space , but also sheds light on techniques for collaboration through coordinated access to the shared organization of knowledge ."}
{"title": "an immune inspired network intrusion detection system utilising correlation context", "abstract": "network intrusion detection systems ( nids ) are computer systems which monitor a network with the aim of discerning malicious from benign activity on that network . while a wide range of approaches have met varying levels of success , most idss rely on having access to a database of known attack signatures which are written by security experts . nowadays , in order to solve problems with false positive alerts , correlation algorithms are used to add additional structure to sequences of ids alerts . however , such techniques are of no help in discovering novel attacks or variations of known attacks , something the human immune system ( his ) is capable of doing in its own specialised domain . this paper presents a novel immune algorithm for application to the ids problem . the goal is to discover packets containing novel variations of attacks covered by an existing signature base ."}
{"title": "toward the implementation of functions in the dlv system ( preliminary technical report )", "abstract": "this document describes the functions as they are treated in the dlv system . we give first the language , then specify the main implementation issues ."}
{"title": "theory refinement on bayesian networks", "abstract": "theory refinement is the task of updating a domain theory in the light of new cases , to be done automatically or with some expert assistance . the problem of theory refinement under uncertainty is reviewed here in the context of bayesian statistics , a theory of belief revision . the problem is reduced to an incremental learning task as follows : the learning system is initially primed with a partial theory supplied by a domain expert , and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data . algorithms for refinement of bayesian networks are presented to illustrate what is meant by `` partial theory '' , `` alternative theory representation '' , etc . the algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode ."}
{"title": "interval constraint solving for camera control and motion planning", "abstract": "many problems in robust control and motion planning can be reduced to either find a sound approximation of the solution space determined by a set of nonlinear inequalities , or to the `` guaranteed tuning problem '' as defined by jaulin and walter , which amounts to finding a value for some tuning parameter such that a set of inequalities be verified for all the possible values of some perturbation vector . a classical approach to solve these problems , which satisfies the strong soundness requirement , involves some quantifier elimination procedure such as collins ' cylindrical algebraic decomposition symbolic method . sound numerical methods using interval arithmetic and local consistency enforcement to prune the search space are presented in this paper as much faster alternatives for both soundly solving systems of nonlinear inequalities , and addressing the guaranteed tuning problem whenever the perturbation vector has dimension one . the use of these methods in camera control is investigated , and experiments with the prototype of a declarative modeller to express camera motion using a cinematic language are reported and commented ."}
{"title": "the grt planning system : backward heuristic construction in forward state-space planning", "abstract": "this paper presents grt , a domain-independent heuristic planning system for strips worlds . grt solves problems in two phases . in the pre-processing phase , it estimates the distance between each fact and the goals of the problem , in a backward direction . then , in the search phase , these estimates are used in order to further estimate the distance between each intermediate state and the goals , guiding so the search process in a forward direction and on a best-first basis . the paper presents the benefits from the adoption of opposite directions between the preprocessing and the search phases , discusses some difficulties that arise in the pre-processing phase and introduces techniques to cope with them . moreover , it presents several methods of improving the efficiency of the heuristic , by enriching the representation and by reducing the size of the problem . finally , a method of overcoming local optimal states , based on domain axioms , is proposed . according to it , difficult problems are decomposed into easier sub-problems that have to be solved sequentially . the performance results from various domains , including those of the recent planning competitions , show that grt is among the fastest planners ."}
{"title": "new candidates welcome ! possible winners with respect to the addition of new candidates", "abstract": "in voting contexts , some new candidates may show up in the course of the process . in this case , we may want to determine which of the initial candidates are possible winners , given that a fixed number $ k $ of new candidates will be added . we give a computational study of this problem , focusing on scoring rules , and we provide a formal comparison with related problems such as control via adding candidates or cloning ."}
{"title": "metaheuristics in flood disaster management and risk assessment", "abstract": "a conceptual area is divided into units or barangays , each was allowed to evolve under a physical constraint . a risk assessment method was then used to identify the flood risk in each community using the following risk factors : the area 's urbanized area ratio , literacy rate , mortality rate , poverty incidence , radio/tv penetration , and state of structural and non-structural measures . vulnerability is defined as a weighted-sum of these components . a penalty was imposed for reduced vulnerability . optimization comparison was done with matlab 's genetic algorithms and simulated annealing ; results showed 'extreme ' solutions and realistic designs , for simulated annealing and genetic algorithm , respectively ."}
{"title": "distance and similarity measures effect on the performance of k-nearest neighbor classifier - a review", "abstract": "the k-nearest neighbor ( knn ) classifier is one of the simplest and most common classifiers , yet its performance competes with the most complex classifiers in the literature . the core of this classifier depends mainly on measuring the distance or similarity between the tested example and the training examples . this raises a major question about which distance measures to be used for the knn classifier among a large number of distance and similarity measures ? this review attempts to answer the previous question through evaluating the performance ( measured by accuracy , precision and recall ) of the knn using a large number of distance measures , tested on a number of real world datasets , with and without adding different levels of noise . the experimental results show that the performance of knn classifier depends significantly on the distance used , the results showed large gaps between the performances of different distances . we found that a recently proposed non-convex distance performed the best when applied on most datasets comparing to the other tested distances . in addition , the performance of the knn degraded only about $ 20\\ % $ while the noise level reaches $ 90\\ % $ , this is true for all the distances used . this means that the knn classifier using any of the top $ 10 $ distances tolerate noise to a certain degree . moreover , the results show that some distances are less affected by the added noise comparing to other distances ."}
{"title": "determining the unithood of word sequences using a probabilistic approach", "abstract": "most research related to unithood were conducted as part of a larger effort for the determination of termhood . consequently , novelties are rare in this small sub-field of term extraction . in addition , existing work were mostly empirically motivated and derived . we propose a new probabilistically-derived measure , independent of any influences of termhood , that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from google search engine for the measurement of unithood . our comparative study using 1,825 test cases against an existing empirically-derived function revealed an improvement in terms of precision , recall and accuracy ."}
{"title": "a polynomial time subsumption algorithm for nominal safe $ \\mathcal { elo } _\\bot $ under rational closure", "abstract": "description logics ( dls ) under rational closure ( rc ) is a well-known framework for non-monotonic reasoning in dls . in this paper , we address the concept subsumption decision problem under rc for nominal safe $ \\mathcal { elo } _\\bot $ , a notable and practically important dl representative of the owl 2 profile owl 2 el . our contribution here is to define a polynomial time subsumption procedure for nominal safe $ \\mathcal { elo } _\\bot $ under rc that relies entirely on a series of classical , monotonic $ \\mathcal { el } _\\bot $ subsumption tests . therefore , any existing classical monotonic $ \\mathcal { el } _\\bot $ reasoner can be used as a black box to implement our method . we then also adapt the method to one of the known extensions of rc for dls , namely defeasible inheritance-based dls without losing the computational tractability ."}
{"title": "intelligent subset selection of power generators for economic dispatch", "abstract": "sustainable and economical generation of electrical power is an essential and mandatory component of infrastructure in today 's world . optimal generation ( generator subset selection ) of power requires a careful evaluation of various factors like type of source , generation , transmission & storage capacities , congestion among others which makes this a difficult task . we created a grid to simulate various conditions including stimuli like generator supply , weather and load demand using siemens pss/e software and this data is trained using deep learning methods and subsequently tested . the results are highly encouraging . as per our knowledge , this is the first paper to propose a working and scalable deep learning model for this problem ."}
{"title": "toward crowd-sensitive path planning", "abstract": "if a robot can predict crowds in parts of its environment that are inaccessible to its sensors , then it can plan to avoid them . this paper proposes a fast , online algorithm that learns average crowd densities in different areas . it also describes how these densities can be incorporated into existing navigation architectures . in simulation across multiple challenging crowd scenarios , the robot reaches its target faster , travels less , and risks fewer collisions than if it were to plan with the traditional a* algorithm ."}
{"title": "distance semantics for belief revision", "abstract": "a vast and interesting family of natural semantics for belief revision is defined . suppose one is given a distance d between any two models . one may then define the revision of a theory k by a formula a as the theory defined by the set of all those models of a that are closest , by d , to the set of models of k. this family is characterized by a set of rationality postulates that extends the agm postulates . the new postulates describe properties of iterated revisions ."}
{"title": "intelligent decision : towards interpreting the pe algorithm", "abstract": "the human intelligence lies in the algorithm , the nature of algorithm lies in the classification , and the classification is equal to outlier detection . a lot of algorithms have been proposed to detect outliers , meanwhile a lot of definitions . unsatisfying point is that definitions seem vague , which makes the solution an ad hoc one . we analyzed the nature of outliers , and give two clear definitions . we then develop an efficient rdd algorithm , which converts outlier problem to pattern and degree problem . furthermore , a collapse mechanism was introduced by iir algorithm , which can be united seamlessly with the rdd algorithm and serve for the final decision . both algorithms are originated from the study on general ai . the combined edition is named as pe algorithm , which is the basis of the intelligent decision . here we introduce longest k-turn subsequence problem and corresponding solution as an example to interpret the function of pe algorithm in detecting curve-type outliers . we also give a comparison between iir algorithm and pe algorithm , where we can get a better understanding at both algorithms . a short discussion about intelligence is added to demonstrate the function of the pe algorithm . related experimental results indicate its robustness ."}
{"title": "boolean hedonic games", "abstract": "we study hedonic games with dichotomous preferences . hedonic games are cooperative games in which players desire to form coalitions , but only care about the makeup of the coalitions of which they are members ; they are indifferent about the makeup of other coalitions . the assumption of dichotomous preferences means that , additionally , each player 's preference relation partitions the set of coalitions of which that player is a member into just two equivalence classes : satisfactory and unsatisfactory . a player is indifferent between satisfactory coalitions , and is indifferent between unsatisfactory coalitions , but strictly prefers any satisfactory coalition over any unsatisfactory coalition . we develop a succinct representation for such games , in which each player 's preference relation is represented by a propositional formula . we show how solution concepts for hedonic games with dichotomous preferences are characterised by propositional formulas ."}
{"title": "surprising properties of dropout in deep networks", "abstract": "we analyze dropout in deep networks with rectified linear units and the quadratic loss . our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay . for example , on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs . this provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights . we also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear , and that dropout is insensitive to various re-scalings of the input features , outputs , and network weights . this last insensitivity implies that there are no isolated local minima of the dropout training criterion . our work uncovers new properties of dropout , extends our understanding of why dropout succeeds , and lays the foundation for further progress ."}
{"title": "feasibility of random basis function approximators for modeling and control", "abstract": "we discuss the role of random basis function approximators in modeling and control . we analyze the published work on random basis function approximators and demonstrate that their favorable error rate of convergence o ( 1/n ) is guaranteed only with very substantial computational resources . we also discuss implications of our analysis for applications of neural networks in modeling and control ."}
{"title": "a recurrent neural model with attention for the recognition of chinese implicit discourse relations", "abstract": "we introduce an attention-based bi-lstm for chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches . our model benefits from a partial sampling scheme and is conceptually simple , yet achieves state-of-the-art performance on the chinese discourse treebank . we also visualize its attention activity to illustrate the model 's ability to selectively focus on the relevant parts of an input sequence ."}
{"title": "unsupervised activity discovery and characterization from event-streams", "abstract": "we present a framework to discover and characterize different classes of everyday activities from event-streams . we begin by representing activities as bags of event n-grams . this allows us to analyze the global structural information of activities , using their local event statistics . we demonstrate how maximal cliques in an undirected edge-weighted graph of activities , can be used for activity-class discovery in an unsupervised manner . we show how modeling an activity as a variable length markov process , can be used to discover recurrent event-motifs to characterize the discovered activity-classes . we present results over extensive data-sets , collected from multiple active environments , to show the competence and generalizability of our proposed framework ."}
{"title": "new results for the map problem in bayesian networks", "abstract": "this paper presents new results for the ( partial ) maximum a posteriori ( map ) problem in bayesian networks , which is the problem of querying the most probable state configuration of some of the network variables given evidence . first , it is demonstrated that the problem remains hard even in networks with very simple topology , such as binary polytrees and simple trees ( including the naive bayes structure ) . such proofs extend previous complexity results for the problem . inapproximability results are also derived in the case of trees if the number of states per variable is not bounded . although the problem is shown to be hard and inapproximable even in very simple scenarios , a new exact algorithm is described that is empirically fast in networks of bounded treewidth and bounded number of states per variable . the same algorithm is used as basis of a fully polynomial time approximation scheme for map under such assumptions . approximation schemes were generally thought to be impossible for this problem , but we show otherwise for classes of networks that are important in practice . the algorithms are extensively tested using some well-known networks as well as random generated cases to show their effectiveness ."}
{"title": "using data analytics to detect anomalous states in vehicles", "abstract": "vehicles are becoming more and more connected , this opens up a larger attack surface which not only affects the passengers inside vehicles , but also people around them . these vulnerabilities exist because modern systems are built on the comparatively less secure and old can bus framework which lacks even basic authentication . since a new protocol can only help future vehicles and not older vehicles , our approach tries to solve the issue as a data analytics problem and use machine learning techniques to secure cars . we develop a hidden markov model to detect anomalous states from real data collected from vehicles . using this model , while a vehicle is in operation , we are able to detect and issue alerts . our model could be integrated as a plug-n-play device in all new and old cars ."}
{"title": "visualising interactive inferences with idpd3", "abstract": "a large part of the use of knowledge base systems is the interpretation of the output by the end-users and the interaction with these users . even during the development process visualisations can be a great help to the developer . we created idpd3 as a library to visualise models of logic theories . idpd3 is a new version of $ id^ { p } _ { draw } $ and adds support for visualised interactive simulations ."}
{"title": "dialogue learning with human-in-the-loop", "abstract": "an important aspect of developing conversational agents is to give a bot the ability to improve through communicating with humans and to learn from the mistakes that it makes . most research has focused on learning from fixed training sets of labeled data rather than interacting with a dialogue partner in an online fashion . in this paper we explore this direction in a reinforcement learning setting where the bot improves its question-answering ability from feedback a teacher gives following its generated responses . we build a simulator that tests various aspects of such learning in a synthetic environment , and introduce models that work in this regime . finally , real experiments with mechanical turk validate the approach ."}
{"title": "a new look at bdds for pseudo-boolean constraints", "abstract": "pseudo-boolean constraints are omnipresent in practical applications , and thus a significant effort has been devoted to the development of good sat encoding techniques for them . some of these encodings first construct a binary decision diagram ( bdd ) for the constraint , and then encode the bdd into a propositional formula . these bdd-based approaches have some important advantages , such as not being dependent on the size of the coefficients , or being able to share the same bdd for representing many constraints . we first focus on the size of the resulting bdds , which was considered to be an open problem in our research community . we report on previous work where it was proved that there are pseudo-boolean constraints for which no polynomial bdd exists . we also give an alternative and simpler proof assuming that np is different from co-np . more interestingly , here we also show how to overcome the possible exponential blowup of bdds by phcoefficient decomposition . this allows us to give the first polynomial generalized arc-consistent robdd-based encoding for pseudo-boolean constraints . finally , we focus on practical issues : we show how to efficiently construct such robdds , how to encode them into sat with only 2 clauses per node , and present experimental results that confirm that our approach is competitive with other encodings and state-of-the-art pseudo-boolean solvers ."}
{"title": "naive bayes and exemplar-based approaches to word sense disambiguation revisited", "abstract": "this paper describes an experimental comparison between two standard supervised learning methods , namely naive bayes and exemplar-based classification , on the word sense disambiguation ( wsd ) problem . the aim of the work is twofold . firstly , it attempts to contribute to clarify some confusing information about the comparison between both methods appearing in the related literature . in doing so , several directions have been explored , including : testing several modifications of the basic learning algorithms and varying the feature space . secondly , an improvement of both algorithms is proposed , in order to deal with large attribute sets . this modification , which basically consists in using only the positive information appearing in the examples , allows to improve greatly the efficiency of the methods , with no loss in accuracy . the experiments have been performed on the largest sense-tagged corpus available containing the most frequent and ambiguous english words . results show that the exemplar-based approach to wsd is generally superior to the bayesian approach , especially when a specific metric for dealing with symbolic attributes is used ."}
{"title": "charda : causal hybrid automata recovery via dynamic analysis", "abstract": "we propose and evaluate a new technique for learning hybrid automata automatically by observing the runtime behavior of a dynamical system . working from a sequence of continuous state values and predicates about the environment , charda recovers the distinct dynamic modes , learns a model for each mode from a given set of templates , and postulates causal guard conditions which trigger transitions between modes . our main contribution is the use of information-theoretic measures ( 1 ) ~as a cost function for data segmentation and model selection to penalize over-fitting and ( 2 ) ~to determine the likely causes of each transition . charda is easily extended with different classes of model templates , fitting methods , or predicates . in our experiments on a complex videogame character , charda successfully discovers a reasonable over-approximation of the character 's true behaviors . our results also compare favorably against recent work in automatically learning probabilistic timed automata in an aircraft domain : charda exactly learns the modes of these simpler automata ."}
{"title": "lloyd-topor completion and general stable models", "abstract": "we investigate the relationship between the generalization of program completion defined in 1984 by lloyd and topor and the generalization of the stable model semantics introduced recently by ferraris et al . the main theorem can be used to characterize , in some cases , the general stable models of a logic program by a first-order formula . the proof uses truszczynski 's stable model semantics of infinitary propositional formulas ."}
{"title": "a note on adjusting $ r^2 $ for using with cross-validation", "abstract": "we show how to adjust the coefficient of determination ( $ r^2 $ ) when used for measuring predictive accuracy via leave-one-out cross-validation ."}
{"title": "extending sroiq with constraint networks and grounded circumscription", "abstract": "developments in semantic web technologies have promoted ontological encoding of knowledge from diverse domains . however , modelling many practical domains requires more expressiveness than what the standard description logics ( most prominently sroiq ) support . in this paper , we extend the expressive dl sroiq with constraint networks ( resulting in the logic sroiqc ) and grounded circumscription ( resulting in the logic gc-sroiq ) . applications of constraint modelling include embedding ontologies with temporal or spatial information , while those of grounded circumscription include defeasible inference and closed world reasoning . we describe the syntax and semantics of the logic formed by including constraint modelling constructs in sroiq , and provide a sound , complete and terminating tableau algorithm for it . we further provide an intuitive algorithm for grounded circumscription in sroiqc , which adheres to the general framework of grounded circumscription , and which can be applied to a whole range of expressive logics for which no such specific algorithm presently exists ."}
{"title": "advances in artificial intelligence require progress across all of computer science", "abstract": "advances in artificial intelligence require progress across all of computer science ."}
{"title": "dynamic shared context processing in an e-collaborative learning environment", "abstract": "in this paper , we propose a dynamic shared context processing method based on dsc ( dynamic shared context ) model , applied in an e-collaborative learning environment . firstly , we present the model . this is a way to measure the relevance between events and roles in collaborative environments . with this method , we can share the most appropriate event information for each role instead of sharing all information to all roles in a collaborative work environment . then , we apply and verify this method in our project with google app supported e-learning collaborative environment . during this experiment , we compared dsc method measured relevance of events and roles to manual measured relevance . and we describe the favorable points from this comparison and our finding . finally , we discuss our future research of a hybrid dsc method to make dynamical information shared more effective in a collaborative work environment ."}
{"title": "approximate decomposition : a method for bounding and estimating probabilistic and deterministic queries", "abstract": "in this paper , we introduce a method for approximating the solution to inference and optimization tasks in uncertain and deterministic reasoning . such tasks are in general intractable for exact algorithms because of the large number of dependency relationships in their structure . our method effectively maps such a dense problem to a sparser one which is in some sense `` closest '' . exact methods can be run on the sparser problem to derive bounds on the original answer , which can be quite sharp . we present empirical results demonstrating that our method works well on the tasks of belief inference and finding the probability of the most probable explanation in belief networks , and finding the cost of the solution that violates the smallest number of constraints in constraint satisfaction problems . on one large cpcs network , for example , we were able to calculate upper and lower bounds on the conditional probability of a variable , given evidence , that were almost identical in the average case ."}
{"title": "a system for probabilistic linking of thesauri and classification systems", "abstract": "this paper presents a system which creates and visualizes probabilistic semantic links between concepts in a thesaurus and classes in a classification system . for creating the links , we build on the polylingual labeled topic model ( pll-tm ) . pll-tm identifies probable thesaurus descriptors for each class in the classification system by using information from the natural language text of documents , their assigned thesaurus descriptors and their designated classes . the links are then presented to users of the system in an interactive visualization , providing them with an automatically generated overview of the relations between the thesaurus and the classification system ."}
{"title": "a bayesian perspective on generalization and stochastic gradient descent", "abstract": "we consider two questions at the heart of machine learning ; how can we predict if a minimum will generalize to the test set , and why does stochastic gradient descent find minima that generalize well ? our work responds to zhang et al . ( 2016 ) , who showed deep neural networks can easily memorize randomly labeled training data , despite generalizing well on real labels of the same inputs . we show that the same phenomenon occurs in small linear models . these observations are explained by the bayesian evidence , which penalizes sharp minima but is invariant to model parameterization . we also demonstrate that , when one holds the learning rate fixed , there is an optimum batch size which maximizes the test set accuracy . we propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large . interpreting stochastic gradient descent as a stochastic differential equation , we identify the `` noise scale '' $ g = \\epsilon ( \\frac { n } { b } - 1 ) \\approx \\epsilon n/b $ , where $ \\epsilon $ is the learning rate , $ n $ the training set size and $ b $ the batch size . consequently the optimum batch size is proportional to both the learning rate and the size of the training set , $ b_ { opt } \\propto \\epsilon n $ . we verify these predictions empirically ."}
{"title": "comparison of selection methods in on-line distributed evolutionary robotics", "abstract": "in this paper , we study the impact of selection methods in the context of on-line on-board distributed evolutionary algorithms . we propose a variant of the medea algorithm in which we add a selection operator , and we apply it in a taskdriven scenario . we evaluate four selection methods that induce different intensity of selection pressure in a multi-robot navigation with obstacle avoidance task and a collective foraging task . experiments show that a small intensity of selection pressure is sufficient to rapidly obtain good performances on the tasks at hand . we introduce different measures to compare the selection methods , and show that the higher the selection pressure , the better the performances obtained , especially for the more challenging food foraging task ."}
{"title": "a study of existing ontologies in the iot-domain", "abstract": "several domains have adopted the increasing use of iot-based devices to collect sensor data for generating abstractions and perceptions of the real world . this sensor data is multi-modal and heterogeneous in nature . this heterogeneity induces interoperability issues while developing cross-domain applications , thereby restricting the possibility of reusing sensor data to develop new applications . as a solution to this , semantic approaches have been proposed in the literature to tackle problems related to interoperability of sensor data . several ontologies have been proposed to handle different aspects of iot-based sensor data collection , ranging from discovering the iot sensors for data collection to applying reasoning on the collected sensor data for drawing inferences . in this paper , we survey these existing semantic ontologies to provide an overview of the recent developments in this field . we highlight the fundamental ontological concepts ( e.g. , sensor-capabilities and context-awareness ) required for an iot-based application , and survey the existing ontologies which include these concepts . based on our study , we also identify the shortcomings of currently available ontologies , which serves as a stepping stone to state the need for a common unified ontology for the iot domain ."}
{"title": "initial experiments with tptp-style automated theorem provers on acl2 problems", "abstract": "this paper reports our initial experiments with using external atp on some corpora built with the acl2 system . this is intended to provide the first estimate about the usefulness of such external reasoning and ai systems for solving acl2 problems ."}
{"title": "parallel belief revision", "abstract": "this paper describes a formal system of belief revision developed by wolfgang spohn and shows that this system has a parallel implementation that can be derived from an influence diagram in a manner similar to that in which bayesian networks are derived . the proof rests upon completeness results for an axiomatization of the notion of conditional independence , with the spohn system being used as a semantics for the relation of conditional independence ."}
{"title": "fuzzy answer set computation via satisfiability modulo theories", "abstract": "fuzzy answer set programming ( fasp ) combines two declarative frameworks , answer set programming and fuzzy logic , in order to model reasoning by default over imprecise information . several connectives are available to combine different expressions ; in particular the \\godel and \\luka fuzzy connectives are usually considered , due to their properties . although the \\godel conjunction can be easily eliminated from rule heads , we show through complexity arguments that such a simplification is infeasible in general for all other connectives . % , even if bodies are restricted to \\luka or \\godel conjunctions . the paper analyzes a translation of fasp programs into satisfiability modulo theories~ ( smt ) , which in general produces quantified formulas because of the minimality of the semantics . structural properties of many fasp programs allow to eliminate the quantification , or to sensibly reduce the number of quantified variables . indeed , integrality constraints can replace recursive rules commonly used to force boolean interpretations , and completion subformulas can guarantee minimality for acyclic programs with atomic heads . moreover , head cycle free rules can be replaced by shifted subprograms , whose structure depends on the eliminated head connective , so that ordered completion may replace the minimality check if also \\luka disjunction in rule bodies is acyclic . the paper also presents and evaluates a prototype system implementing these translations . to appear in theory and practice of logic programming ( tplp ) , proceedings of iclp 2015 ."}
{"title": "ask your neurons : a deep learning approach to visual question answering", "abstract": "we address a question answering task on real-world images that is set up as a visual turing test . by combining latest advances in image representation and natural language processing , we propose ask your neurons , a scalable , jointly trained , end-to-end formulation to this problem . in contrast to previous efforts , we are facing a multi-modal problem where the language output ( answer ) is conditioned on visual and natural language inputs ( image and question ) . we provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline . to study human consensus , which is related to the ambiguities inherent in this challenging task , we propose two novel metrics and collect additional answers which extend the original daquar dataset to daquar-consensus . moreover , we also extend our analysis to vqa , a large-scale question answering about images dataset , where we investigate some particular design choices and show the importance of stronger visual models . at the same time , we achieve strong performance of our model that still uses a global image representation . finally , based on such analysis , we refine our ask your neurons on daquar , which also leads to a better performance on this challenging task ."}
{"title": "an implementation of a method for computing the uncertainty in inferred probabilities in belief networks", "abstract": "in recent years the belief network has been used increasingly to model systems in al that must perform uncertain inference . the development of efficient algorithms for probabilistic inference in belief networks has been a focus of much research in ai . efficient algorithms for certain classes of belief networks have been developed , but the problem of reporting the uncertainty in inferred probabilities has received little attention . a system should not only be capable of reporting the values of inferred probabilities and/or the favorable choices of a decision ; it should report the range of possible error in the inferred probabilities and/or choices . two methods have been developed and implemented for determining the variance in inferred probabilities in belief networks . these methods , the approximate propagation method and the monte carlo integration method are discussed and compared in this paper ."}
{"title": "deep learning for medical image segmentation", "abstract": "this report provides an overview of the current state of the art deep learning architectures and optimisation techniques , and uses the adni hippocampus mri dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation , which is important in the diagnosis of alzheimer 's disease . we found that a slightly unconventional `` stacked 2d '' approach provides much better classification performance than simple 2d patches without requiring significantly more computational power . we also examined the popular `` tri-planar '' approach used in some recently published studies , and found that it provides much better results than the 2d approaches , but also with a moderate increase in computational power requirement . finally , we evaluated a full 3d convolutional architecture , and found that it provides marginally better results than the tri-planar approach , but at the cost of a very significant increase in computational power requirement ."}
{"title": "predicting rapid fire growth ( flashover ) using conditional generative adversarial networks", "abstract": "a flashover occurs when a fire spreads very rapidly through crevices due to intense heat . flashovers present one of the most frightening and challenging fire phenomena to those who regularly encounter them : firefighters . firefighters ' safety and lives often depend on their ability to predict flashovers before they occur . typical pre-flashover fire characteristics include dark smoke , high heat , and rollover ( `` angel fingers '' ) and can be quantified by color , size , and shape . using a color video stream from a firefighter 's body camera , we applied generative adversarial neural networks for image enhancement . the neural networks were trained to enhance very dark fire and smoke patterns in videos and monitor dynamic changes in smoke and fire areas . preliminary tests with limited flashover training videos showed that we predicted a flashover as early as 55 seconds before it occurred ."}
{"title": "modelling contractual arguments", "abstract": "one influential approach to assessing the `` goodness '' of arguments is offered by the pragma-dialectical school ( p-d ) ( eemeren & grootendorst 1992 ) . this can be compared with rhetorical structure theory ( rst ) ( mann & thompson 1988 ) , an approach that originates in discourse analysis . in p-d terms an argument is good if it avoids committing a fallacy , whereas in rst terms an argument is good if it is coherent . rst has been criticised ( snoeck henkemans 1997 ) for providing only a partially functional account of argument , and similar criticisms have been raised in the natural language generation ( nlg ) community-particularly by moore & pollack ( 1992 ) - with regards to its account of intentionality in text in general . mann and thompson themselves note that although rst can be successfully applied to a wide range of texts from diverse domains , it fails to characterise some types of text , most notably legal contracts . there is ongoing research in the artificial intelligence and law community exploring the potential for providing electronic support to contract negotiators , focusing on long-term , complex engineering agreements ( see for example daskalopulu & sergot 1997 ) . this paper provides a brief introduction to rst and illustrates its shortcomings with respect to contractual text . an alternative approach for modelling argument structure is presented which not only caters for contractual text , but also overcomes the aforementioned limitations of rst ."}
{"title": "a unit selection methodology for music generation using deep neural networks", "abstract": "several methods exist for a computer to generate music based on data including markov chains , recurrent neural networks , recombinancy , and grammars . we explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking , where , we consider a unit to be a variable length number of measures of music . we first examine whether a unit selection method , that is restricted to a finite size unit library , can be sufficient for encompassing a wide spectrum of music . we do this by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library . we then describe a generative model that combines a deep structured semantic model ( dssm ) with an lstm to predict the next unit , where units consist of four , two , and one measures of music . we evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task . we compare our model to a note-level generative baseline that consists of a stacked lstm trained to predict forward by one note ."}
{"title": "automating access control logics in simple type theory with leo-ii", "abstract": "garg and abadi recently proved that prominent access control logics can be translated in a sound and complete way into modal logic s4 . we have previously outlined how normal multimodal logics , including monomodal logics k and s4 , can be embedded in simple type theory ( which is also known as higher-order logic ) and we have demonstrated that the higher-order theorem prover leo-ii can automate reasoning in and about them . in this paper we combine these results and describe a sound and complete embedding of different access control logics in simple type theory . employing this framework we show that the off the shelf theorem prover leo-ii can be applied to automate reasoning in prominent access control logics ."}
{"title": "imitating driver behavior with generative adversarial networks", "abstract": "the ability to accurately predict and simulate human driving behavior is critical for the development of intelligent transportation systems . traditional modeling methods have employed simple parametric models and behavioral cloning . this paper adopts a method for overcoming the problem of cascading errors inherent in prior approaches , resulting in realistic behavior that is robust to trajectory perturbations . we extend generative adversarial imitation learning to the training of recurrent policies , and we demonstrate that our model outperforms rule-based controllers and maximum likelihood models in realistic highway simulations . our model both reproduces emergent behavior of human drivers , such as lane change rate , while maintaining realistic control over long time horizons ."}
{"title": "automatically selecting useful phrases for dialogue act tagging", "abstract": "we present an empirical investigation of various ways to automatically identify phrases in a tagged corpus that are useful for dialogue act tagging . we found that a new method ( which measures a phrase 's deviation from an optimally-predictive phrase ) , enhanced with a lexical filtering mechanism , produces significantly better cues than manually-selected cue phrases , the exhaustive set of phrases in a training corpus , and phrases chosen by traditional metrics , like mutual information and information gain ."}
{"title": "failover in cellular automata", "abstract": "a cellular automata ( ca ) configuration is constructed that exhibits emergent failover . the configuration is based on standard game of life rules . gliders and glider-guns form the core messaging structure in the configuration . the blinker is represented as the basic computational unit , and it is shown how it can be recreated in case of a failure . stateless failover using primary-backup mechanism is demonstrated . the details of the ca components used in the configuration and its working are described , and a simulation of the complete configuration is also presented ."}
{"title": "overlapping mixtures of gaussian processes for the data association problem", "abstract": "in this work we introduce a mixture of gps to address the data association problem , i.e . to label a group of observations according to the sources that generated them . unlike several previously proposed gp mixtures , the novel mixture has the distinct characteristic of using no gating function to determine the association of samples and mixture components . instead , all the gps in the mixture are global and samples are clustered following `` trajectories '' across input space . we use a non-standard variational bayesian algorithm to efficiently recover sample labels and learn the hyperparameters . we show how multi-object tracking problems can be disambiguated and also explore the characteristics of the model in traditional regression settings ."}
{"title": "taxonomy grounded aggregation of classifiers with different label sets", "abstract": "we describe the problem of aggregating the label predictions of diverse classifiers using a class taxonomy . such a taxonomy may not have been available or referenced when the individual classifiers were designed and trained , yet mapping the output labels into the taxonomy is desirable to integrate the effort spent in training the constituent classifiers . a hierarchical taxonomy representing some domain knowledge may be different from , but partially mappable to , the label sets of the individual classifiers . we present a heuristic approach and a principled graphical model to aggregate the label predictions by grounding them into the available taxonomy . our model aggregates the labels using the taxonomy structure as constraints to find the most likely hierarchically consistent class . we experimentally validate our proposed method on image and text classification tasks ."}
{"title": "learning sparse causal models is not np-hard", "abstract": "this paper shows that causal model discovery is not an np-hard problem , in the sense that for sparse graphs bounded by node degree k the sound and complete causal model can be obtained in worst case order n^ { 2 ( k+2 ) } independence tests , even when latent variables and selection bias may be present . we present a modification of the well-known fci algorithm that implements the method for an independence oracle , and suggest improvements for sample/real-world data versions . it does not contradict any known hardness results , and does not solve an np-hard problem : it just proves that sparse causal discovery is perhaps more complicated , but not as hard as learning minimal bayesian networks ."}
{"title": "residual belief propagation : informed scheduling for asynchronous message passing", "abstract": "inference for probabilistic graphical models is still very much a practical challenge in large domains . the commonly used and effective belief propagation ( bp ) algorithm and its generalizations often do not converge when applied to hard , real-life inference tasks . while it is widely recognized that the scheduling of messages in these algorithms may have significant consequences , this issue remains largely unexplored . in this work , we address the question of how to schedule messages for asynchronous propagation so that a fixed point is reached faster and more often . we first show that any reasonable asynchronous bp converges to a unique fixed point under conditions similar to those that guarantee convergence of synchronous bp . in addition , we show that the convergence rate of a simple round-robin schedule is at least as good as that of synchronous propagation . we then propose residual belief propagation ( rbp ) , a novel , easy-to-implement , asynchronous propagation algorithm that schedules messages in an informed way , that pushes down a bound on the distance from the fixed point . finally , we demonstrate the superiority of rbp over state-of-the-art methods for a variety of challenging synthetic and real-life problems : rbp converges significantly more often than other methods ; and it significantly reduces running time until convergence , even when other methods converge ."}
{"title": "an importance sampling algorithm based on evidence pre-propagation", "abstract": "precision achieved by stochastic sampling algorithms for bayesian networks typically deteriorates in face of extremely unlikely evidence . to address this problem , we propose the evidence pre-propagation importance sampling algorithm ( epis-bn ) , an importance sampling algorithm that computes an approximate importance function by the heuristic methods : loopy belief propagation and e-cutoff . we tested the performance of e-cutoff on three large real bayesian networks : andes , cpcs , and pathfinder . we observed that on each of these networks the epis-bn algorithm gives us a considerable improvement over the current state of the art algorithm , the ais-bn algorithm . in addition , it avoids the costly learning stage of the ais-bn algorithm ."}
{"title": "maximum entropy models and subjective interestingness : an application to tiles in binary databases", "abstract": "recent research has highlighted the practical benefits of subjective interestingness measures , which quantify the novelty or unexpectedness of a pattern when contrasted with any prior information of the data miner ( silberschatz and tuzhilin , 1995 ; geng and hamilton , 2006 ) . a key challenge here is the formalization of this prior information in a way that lends itself to the definition of an interestingness subjective measure that is both meaningful and practical . in this paper , we outline a general strategy of how this could be achieved , before working out the details for a use case that is important in its own right . our general strategy is based on considering prior information as constraints on a probabilistic model representing the uncertainty about the data . more specifically , we represent the prior information by the maximum entropy ( maxent ) distribution subject to these constraints . we briefly outline various measures that could subsequently be used to contrast patterns with this maxent model , thus quantifying their subjective interestingness ."}
{"title": "woah : preliminaries to zero-shot ontology learning for conversational agents", "abstract": "the present paper presents the weighted ontology approximation heuristic ( woah ) , a novel zero-shot approach to ontology estimation for conversational agents development environments . this methodology extracts verbs and nouns separately from data by distilling the dependencies obtained and applying similarity and sparsity metrics to generate an ontology estimation configurable in terms of the level of generalization ."}
{"title": "resource planning for rescue operations", "abstract": "after an earthquake , disaster sites pose a multitude of health and safety concerns . a rescue operation of people trapped in the ruins after an earthquake disaster requires a series of intelligent behavior , including planning . for a successful rescue operation , given a limited number of available actions and regulations , the role of planning in rescue operations is crucial . fortunately , recent developments in automated planning by artificial intelligence community can help different organization in this crucial task . due to the number of rules and regulations , we believe that a rule based system for planning can be helpful for this specific planning problem . in this research work , we use logic rules to represent rescue and related regular regulations , together with a logic based planner to solve this complicated problem . although this research is still in the prototyping and modeling stage , it clearly shows that rule based languages can be a good infrastructure for this computational task . the results of this research can be used by different organizations , such as iranian red crescent society and international institute of seismology and earthquake engineering ( iisee ) ."}
{"title": "semantic preserving embeddings for generalized graphs", "abstract": "a new approach to the study of generalized graphs as semantic data structures using machine learning techniques is presented . we show how vector representations maintaining semantic characteristics of the original data can be obtained from a given graph using neural encoding architectures and considering the topological properties of the graph . semantic features of these new representations are tested by using some machine learning tasks and new directions on efficient link discovery , entitity retrieval and long distance query methodologies on large relational datasets are investigated using real datasets . -- -- en este trabajo se presenta un nuevo enfoque en el contexto del aprendizaje autom\\'atico multi-relacional para el estudio de grafos generalizados . se muestra c\\'omo se pueden obtener representaciones vectoriales que mantienen caracter\\'isticas sem\\'anticas del grafo original utilizando codificadores neuronales y considerando las propiedades topol\\'ogicas del grafo . adem\\'as , se eval\\'uan las caracter\\'isticas sem\\'anticas capturadas por estas nuevas representaciones y se investigan nuevas metodolog\\'ias eficientes relacionadas con link discovery , entity retrieval y consultas a larga distancia en grandes conjuntos de datos relacionales haciendo uso de bases de datos reales ."}
{"title": "realizing rcc8 networks using convex regions", "abstract": "rcc8 is a popular fragment of the region connection calculus , in which qualitative spatial relations between regions , such as adjacency , overlap and parthood , can be expressed . while rcc8 is essentially dimensionless , most current applications are confined to reasoning about two-dimensional or three-dimensional physical space . in this paper , however , we are mainly interested in conceptual spaces , which typically are high-dimensional euclidean spaces in which the meaning of natural language concepts can be represented using convex regions . the aim of this paper is to analyze how the restriction to convex regions constrains the realizability of networks of rcc8 relations . first , we identify all ways in which the set of rcc8 base relations can be restricted to guarantee that consistent networks can be convexly realized in respectively 1d , 2d , 3d , and 4d . most surprisingly , we find that if the relation 'partially overlaps ' is disallowed , all consistent atomic rcc8 networks can be convexly realized in 4d . if instead refinements of the relation 'part of ' are disallowed , all consistent atomic rcc8 relations can be convexly realized in 3d . we furthermore show , among others , that any consistent rcc8 network with 2n+1 variables can be realized using convex regions in the n-dimensional euclidean space ."}
{"title": "automatic learning of gait signatures for people identification", "abstract": "this work targets people identification in video based on the way they walk ( i.e . gait ) . while classical methods typically derive gait signatures from sequences of binary silhouettes , in this work we explore the use of convolutional neural networks ( cnn ) for learning high-level descriptors from low-level motion features ( i.e . optical flow components ) . we carry out a thorough experimental evaluation of the proposed cnn architecture on the challenging tum-gaid dataset . the experimental results indicate that using spatio-temporal cuboids of optical flow as input data for cnn allows to obtain state-of-the-art results on the gait task with an image resolution eight times lower than the previously reported results ( i.e . 80x60 pixels ) ."}
{"title": "learning causal structures using regression invariance", "abstract": "we study causal inference in a multi-environment setting , in which the functional relations for producing the variables from their direct causes remain the same across environments , while the distribution of exogenous noises may vary . we introduce the idea of using the invariance of the functional relations of the variables to their causes across a set of environments . we define a notion of completeness for a causal inference algorithm in this setting and prove the existence of such algorithm by proposing the baseline algorithm . additionally , we present an alternate algorithm that has significantly improved computational and sample complexity compared to the baseline algorithm . the experiment results show that the proposed algorithm outperforms the other existing algorithms ."}
{"title": "proceedings fifteenth conference on theoretical aspects of rationality and knowledge", "abstract": "the 15th conference on theoretical aspects of rationality and knowledge ( tark ) took place in carnegie mellon university , pittsburgh , usa from june 4 to 6 , 2015. the mission of the tark conferences is to bring together researchers from a wide variety of fields , including artificial intelligence , cryptography , distributed computing , economics and game theory , linguistics , philosophy , and psychology , in order to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge . these proceedings consist of a subset of the papers / abstracts presented at the tark conference ."}
{"title": "left-right skip-densenets for coarse-to-fine object categorization", "abstract": "inspired by the recent neuroscience studies on the left-right asymmetry of the brains in the low and high spatial frequency processing , we introduce a novel type of network -- the left-right skip-densenets for coarse-to-fine object categorization . this network can enable both coarse and fine-grained classification in a single framework . we also for the first time propose the layer-skipping mechanism which learns a gating network to predict whether skip some layers in the testing stage . this layer-skipping mechanism assigns more flexibility and capability to our network for the categorization tasks . our network is evaluated on three widely used datasets ; the results show that our network is more promising in solving the coarse-to-fine object categorization than the competitors ."}
{"title": "improved gq-cnn : deep learning model for planning robust grasps", "abstract": "recent developments in the field of robot grasping have shown great improvements in the grasp success rates when dealing with unknown objects . in this work we improve on one of the most promising approaches , the grasp quality convolutional neural network ( gq-cnn ) trained on the dexnet 2.0 dataset . we propose a new architecture for the gq-cnn and describe practical improvements that increase the model validation accuracy from 92.2 % to 95.8 % and from 85.9 % to 88.0 % on respectively image-wise and object-wise training and validation splits ."}
{"title": "lifted graphical models : a survey", "abstract": "this article presents a survey of work on lifted graphical models . we review a general form for a lifted graphical model , a par-factor graph , and show how a number of existing statistical relational representations map to this formalism . we discuss inference algorithms , including lifted inference algorithms , that efficiently compute the answers to probabilistic queries . we also review work in learning lifted graphical models from data . it is our belief that the need for statistical relational models ( whether it goes by that name or another ) will grow in the coming decades , as we are inundated with data which is a mix of structured and unstructured , with entities and relations extracted in a noisy manner from text , and with the need to reason effectively with this data . we hope that this synthesis of ideas from many different research groups will provide an accessible starting point for new researchers in this expanding field ."}
{"title": "delta epsilon alpha star : a pac-admissible search algorithm", "abstract": "delta epsilon alpha star is a minimal coverage , real-time robotic search algorithm that yields a moderately aggressive search path with minimal backtracking . search performance is bounded by a placing a combinatorial bound , epsilon and delta , on the maximum deviation from the theoretical shortest path and the probability at which further deviations can occur . additionally , we formally define the notion of pac-admissibility -- a relaxed admissibility criteria for algorithms , and show that pac-admissible algorithms are better suited to robotic search situations than epsilon-admissible or strict algorithms ."}
{"title": "development of a cargo screening process simulator : a first approach", "abstract": "the efficiency of current cargo screening processes at sea and air ports is largely unknown as few benchmarks exists against which they could be measured . some manufacturers provide benchmarks for individual sensors but we found no benchmarks that take a holistic view of the overall screening procedures and no benchmarks that take operator variability into account . just adding up resources and manpower used is not an effective way for assessing systems where human decision-making and operator compliance to rules play a vital role . our aim is to develop a decision support tool ( cargo-screening system simulator ) that will map the right technology and manpower to the right commodity-threat combination in order to maximise detection rates . in this paper we present our ideas for developing such a system and highlight the research challenges we have identified . then we introduce our first case study and report on the progress we have made so far ."}
{"title": "deploying learning materials to game content for serious education game development : a case study", "abstract": "the ultimate goals of serious education games ( seg ) are to facilitate learning and maximizing enjoyment during playing segs . in seg development , there are normally two spaces to be taken into account : knowledge space regarding learning materials and content space regarding games to be used to convey learning materials . how to deploy the learning materials seamlessly and effectively into game content becomes one of the most challenging problems in seg development . unlike previous work where experts in education have to be used heavily , we proposed a novel approach that works toward minimizing the efforts of education experts in mapping learning materials to content space . for a proof-of-concept , we apply the proposed approach in developing an seg game , named \\emph { chem dungeon } , as a case study in order to demonstrate the effectiveness of our proposed approach . this seg game has been tested with a number of users , and the user survey suggests our method works reasonably well ."}
{"title": "dataset and neural recurrent sequence labeling model for open-domain factoid question answering", "abstract": "while question answering ( qa ) with neural network , i.e . neural qa , has achieved promising results in recent years , lacking of large scale real-word qa dataset is still a challenge for developing and evaluating neural qa system . to alleviate this problem , we propose a large scale human annotated real-world qa dataset webqa with more than 42k questions and 556k evidences . as existing neural qa methods resolve qa either as sequence generation or classification/ranking problem , they face challenges of expensive softmax computation , unseen answers handling or separate candidate answer generation component . in this work , we cast neural qa as a sequence labeling problem and propose an end-to-end sequence labeling model , which overcomes all the above challenges . experimental results on webqa show that our model outperforms the baselines significantly with an f1 score of 74.69 % with word-based input , and the performance drops only 3.72 f1 points with more challenging character-based input ."}
{"title": "deeptransport : learning spatial-temporal dependency for traffic condition forecasting", "abstract": "predicting traffic conditions has been recently explored as a way to relieve traffic congestion . several pioneering approaches have been proposed based on traffic observations of the target location as well as its adjacent regions , but they obtain somewhat limited accuracy due to lack of mining road topology . to address the effect attenuation problem , we propose to take account of the traffic of surrounding locations ( wider than adjacent range ) . we propose an end-to-end framework called deeptransport , in which convolutional neural networks ( cnn ) and recurrent neural networks ( rnn ) are utilized to obtain spatial-temporal traffic information within a transport network topology . in addition , attention mechanism is introduced to align spatial and temporal information . moreover , we constructed and released a real-world large traffic condition dataset with 5-minute resolution . our experiments on this dataset demonstrate our method captures the complex relationship in temporal and spatial domain . it significantly outperforms traditional statistical methods and a state-of-the-art deep learning method ."}
{"title": "muse csp : an extension to the constraint satisfaction problem", "abstract": "this paper describes an extension to the constraint satisfaction problem ( csp ) called muse csp ( multiply segmented constraint satisfaction problem ) . this extension is especially useful for those problems which segment into multiple sets of partially shared variables . such problems arise naturally in signal processing applications including computer vision , speech processing , and handwriting recognition . for these applications , it is often difficult to segment the data in only one way given the low-level information utilized by the segmentation algorithms . muse csp can be used to compactly represent several similar instances of the constraint satisfaction problem . if multiple instances of a csp have some common variables which have the same domains and constraints , then they can be combined into a single instance of a muse csp , reducing the work required to apply the constraints . we introduce the concepts of muse node consistency , muse arc consistency , and muse path consistency . we then demonstrate how muse csp can be used to compactly represent lexically ambiguous sentences and the multiple sentence hypotheses that are often generated by speech recognition algorithms so that grammar constraints can be used to provide parses for all syntactically correct sentences . algorithms for muse arc and path consistency are provided . finally , we discuss how to create a muse csp from a set of csps which are labeled to indicate when the same variable is shared by more than a single csp ."}
{"title": "weakly supervised plda training", "abstract": "plda is a popular normalization approach for the i-vector model , and it has delivered state-of-the-art performance in speaker verification . however , plda training requires a large amount of labelled development data , which is highly expensive in most cases . we present a cheap plda training approach , which assumes that speakers in the same session can be easily separated , and speakers in different sessions are simply different . this results in ` weak labels ' which are not fully accurate but cheap , leading to a weak plda training . our experimental results on real-life large-scale telephony customer service achieves demonstrated that the weak training can offer good performance when human-labelled data are limited . more interestingly , the weak training can be employed as a discriminative adaptation approach , which is more efficient than the prevailing unsupervised method when human-labelled data are insufficient ."}
{"title": "reddwarfdata : a simplified dataset of starcraft matches", "abstract": "the game starcraft is one of the most interesting arenas to test new machine learning and computational intelligence techniques ; however , starcraft matches take a long time and creating a good dataset for training can be hard . besides , analyzing match logs to extract the main characteristics can also be done in many different ways to the point that extracting and processing data itself can take an inordinate amount of time and of course , depending on what you choose , can bias learning algorithms . in this paper we present a simplified dataset extracted from the set of matches published by robinson and watson , which we have called reddwarfdata , containing several thousand matches processed to frames , so that temporal studies can also be undertaken . this dataset is available from github under a free license . an initial analysis and appraisal of these matches is also made ."}
{"title": "learning from synthetic data using a stacked multichannel autoencoder", "abstract": "learning from synthetic data has many important and practical applications . an example of application is photo-sketch recognition . using synthetic data is challenging due to the differences in feature distributions between synthetic and real data , a phenomenon we term synthetic gap . in this paper , we investigate and formalize a general framework-stacked multichannel autoencoder ( smcae ) that enables bridging the synthetic gap and learning from synthetic data more efficiently . in particular , we show that our smcae can not only transform and use synthetic data on the challenging face-sketch recognition task , but that it can also help simulate real images , which can be used for training classifiers for recognition . preliminary experiments validate the effectiveness of the framework ."}
{"title": "efficiency and sequenceability in fair division of indivisible goods with additive preferences", "abstract": "in fair division of indivisible goods , using sequences of sincere choices ( or picking sequences ) is a natural way to allocate the objects . the idea is the following : at each stage , a designated agent picks one object among those that remain . this paper , restricted to the case where the agents have numerical additive preferences over objects , revisits to some extent the seminal paper by brams and king [ 9 ] which was specific to ordinal and linear order preferences over items . we point out similarities and differences with this latter context . in particular , we show that any pareto-optimal allocation ( under additive preferences ) is sequenceable , but that the converse is not true anymore . this asymmetry leads naturally to the definition of a `` scale of efficiency '' having three steps : pareto-optimality , sequenceability without pareto-optimality , and non-sequenceability . finally , we investigate the links between these efficiency properties and the `` scale of fairness '' we have described in an earlier work [ 7 ] : we first show that an allocation can be envy-free and non-sequenceable , but that every competitive equilibrium with equal incomes is sequenceable . then we experimentally explore the links between the scales of efficiency and fairness ."}
{"title": "super logic programs", "abstract": "the autoepistemic logic of knowledge and belief ( aelb ) is a powerful nonmonotic formalism introduced by teodor przymusinski in 1994. in this paper , we specialize it to a class of theories called ` super logic programs ' . we argue that these programs form a natural generalization of standard logic programs . in particular , they allow disjunctions and default negation of arbibrary positive objective formulas . our main results are two new and powerful characterizations of the static semant ics of these programs , one syntactic , and one model-theoretic . the syntactic fixed point characterization is much simpler than the fixed point construction of the static semantics for arbitrary aelb theories . the model-theoretic characterization via kripke models allows one to construct finite representations of the inherently infinite static expansions . both characterizations can be used as the basis of algorithms for query answering under the static semantics . we describe a query-answering interpreter for super programs which we developed based on the model-theoretic characterization and which is available on the web ."}
{"title": "improving statistical multimedia information retrieval model by using ontology", "abstract": "a typical ir system that delivers and stores information is affected by problem of matching between user query and available content on web . use of ontology represents the extracted terms in form of network graph consisting of nodes , edges , index terms etc . the above mentioned ir approaches provide relevance thus satisfying users query . the paper also emphasis on analyzing multimedia documents and performs calculation for extracted terms using different statistical formulas . the proposed model developed reduces semantic gap and satisfies user needs efficiently ."}
{"title": "query-time entity resolution", "abstract": "entity resolution is the problem of reconciling database references corresponding to the same real-world entities . given the abundance of publicly available databases that have unresolved entities , we motivate the problem of query-time entity resolution quick and accurate resolution for answering queries over such unclean databases at query-time . since collective entity resolution approaches -- - where related references are resolved jointly -- - have been shown to be more accurate than independent attribute-based resolution for off-line entity resolution , we focus on developing new algorithms for collective resolution for answering entity resolution queries at query-time . for this purpose , we first formally show that , for collective resolution , precision and recall for individual entities follow a geometric progression as neighbors at increasing distances are considered . unfolding this progression leads naturally to a two stage expand and resolve query processing strategy . in this strategy , we first extract the related records for a query using two novel expansion operators , and then resolve the extracted records collectively . we then show how the same strategy can be adapted for query-time entity resolution by identifying and resolving only those database references that are the most helpful for processing the query . we validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing . we then show how the same queries can be answered in real-time using our adaptive approach while preserving the gains of collective resolution . in addition to experiments on real datasets , we use synthetically generated data to empirically demonstrate the validity of the performance trends predicted by our analysis of collective entity resolution over a wide range of structural characteristics in the data ."}
{"title": "estimation procedures for robust sensor control", "abstract": "many robotic sensor estimation problems can characterized in terms of nonlinear measurement systems . these systems are contaminated with noise and may be underdetermined from a single observation . in order to get reliable estimation results , the system must choose views which result in an overdetermined system . this is the sensor control problem . accurate and reliable sensor control requires an estimation procedure which yields both estimates and measures of its own performance . in the case of nonlinear measurement systems , computationally simple closed-form estimation solutions may not exist . however , approximation techniques provide viable alternatives . in this paper , we evaluate three estimation techniques : the extended kalman filter , a discrete bayes approximation , and an iterative bayes approximation . we present mathematical results and simulation statistics illustrating operating conditions where the extended kalman filter is inappropriate for sensor control , and discuss issues in the use of the discrete bayes approximation ."}
{"title": "tensors come of age : why the ai revolution will help hpc", "abstract": "this article discusses how the automation of tensor algorithms , based on a mathematics of arrays and psi calculus , and a new way to represent numbers , unum arithmetic , enables mechanically provable , scalable , portable , and more numerically accurate software ."}
{"title": "bandit algorithms for tree search", "abstract": "bandit based methods for tree search have recently gained popularity when applied to huge trees , e.g . in the game of go [ 6 ] . their efficient exploration of the tree enables to re- turn rapidly a good value , and improve preci- sion if more time is provided . the uct algo- rithm [ 8 ] , a tree search method based on up- per confidence bounds ( ucb ) [ 2 ] , is believed to adapt locally to the effective smoothness of the tree . however , we show that uct is `` over-optimistic '' in some sense , leading to a worst-case regret that may be very poor . we propose alternative bandit algorithms for tree search . first , a modification of uct us- ing a confidence sequence that scales expo- nentially in the horizon depth is analyzed . we then consider flat-ucb performed on the leaves and provide a finite regret bound with high probability . then , we introduce and analyze a bandit algorithm for smooth trees ( bast ) which takes into account ac- tual smoothness of the rewards for perform- ing efficient `` cuts '' of sub-optimal branches with high confidence . finally , we present an incremental tree expansion which applies when the full tree is too big ( possibly in- finite ) to be entirely represented and show that with high probability , only the optimal branches are indefinitely developed . we illus- trate these methods on a global optimization problem of a continuous function , given noisy values ."}
{"title": "neural network based next-song recommendation", "abstract": "recently , the next-item/basket recommendation system , which considers the sequential relation between bought items , has drawn attention of researchers . the utilization of sequential patterns has boosted performance on several kinds of recommendation tasks . inspired by natural language processing ( nlp ) techniques , we propose a novel neural network ( nn ) based next-song recommender , cnn-rec , in this paper . then , we compare the proposed system with several nn based and classic recommendation systems on the next-song recommendation task . verification results indicate the proposed system outperforms classic systems and has comparable performance with the state-of-the-art system ."}
{"title": "enhanced quantum synchronization via quantum machine learning", "abstract": "we study the quantum synchronization between a pair of two-level systems inside two coupledcavities . using a digital-analog decomposition of the master equation that rules the system dynamics , we show that this approach leads to quantum synchronization between both two-level systems . moreover , we can identify in this digital-analog block decomposition the fundamental elements of a quantum machine learning protocol , in which the agent and the environment ( learning units ) interact through a mediating system , namely , the register . if we can additionally equip this algorithm with a classical feedback mechanism , which consists of projective measurements in the register , reinitialization of the register state and local conditional operations on the agent and register subspace , a powerful and flexible quantum machine learning protocol emerges . indeed , numerical simulations show that this protocol enhances the synchronization process , even when every subsystem experience different loss/decoherence mechanisms , and give us flexibility to choose the synchronization state . finally , we propose an implementation based on current technologies in superconducting circuits ."}
{"title": "unicalc.lin : a linear constraint solver for the unicalc system", "abstract": "in this short paper we present a linear constraint solver for the unicalc system , an environment for reliable solution of mathematical modeling problems ."}
{"title": "conditions under which conditional independence and scoring methods lead to identical selection of bayesian network models", "abstract": "it is often stated in papers tackling the task of inferring bayesian network structures from data that there are these two distinct approaches : ( i ) apply conditional independence tests when testing for the presence or otherwise of edges ; ( ii ) search the model space using a scoring metric . here i argue that for complete data and a given node ordering this division is a myth , by showing that cross entropy methods for checking conditional independence are mathematically identical to methods based upon discriminating between models by their overall goodness-of-fit logarithmic scores ."}
{"title": "robust computer algebra , theorem proving , and oracle ai", "abstract": "in the context of superintelligent ai systems , the term `` oracle '' has two meanings . one refers to modular systems queried for domain-specific tasks . another usage , referring to a class of systems which may be useful for addressing the value alignment and ai control problems , is a superintelligent ai system that only answers questions . the aim of this manuscript is to survey contemporary research problems related to oracles which align with long-term research goals of ai safety . we examine existing question answering systems and argue that their high degree of architectural heterogeneity makes them poor candidates for rigorous analysis as oracles . on the other hand , we identify computer algebra systems ( cass ) as being primitive examples of domain-specific oracles for mathematics and argue that efforts to integrate computer algebra systems with theorem provers , systems which have largely been developed independent of one another , provide a concrete set of problems related to the notion of provable safety that has emerged in the ai safety community . we review approaches to interfacing cass with theorem provers , describe well-defined architectural deficiencies that have been identified with cass , and suggest possible lines of research and practical software projects for scientists interested in ai safety ."}
{"title": "a heuristic search approach to planning with continuous resources in stochastic domains", "abstract": "we consider the problem of optimal planning in stochastic domains with resource constraints , where the resources are continuous and the choice of action at each step depends on resource availability . we introduce the hao* algorithm , a generalization of the ao* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables , where the continuous variables represent monotonic resources . like other heuristic search algorithms , hao* leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy . we show that this approach is especially effective when resource constraints limit how much of the state space is reachable . experimental results demonstrate its effectiveness in the domain that motivates our research : automated planning for planetary exploration rovers ."}
{"title": "multi-platform version of starcraft : brood war in a docker container : technical report", "abstract": "we present a dockerized version of a real-time strategy game starcraft : brood war , commonly used as a domain for ai research , with a pre-installed collection of ai developement tools supporting all the major types of starcraft bots . this provides a convenient way to deploy starcraft ais on numerous hosts at once and across multiple platforms despite limited os support of starcraft . in this technical report , we describe the design of our docker images and present a few use cases ."}
{"title": "measuring similarity of graphs and their nodes by neighbor matching", "abstract": "the problem of measuring similarity of graphs and their nodes is important in a range of practical problems . there is a number of proposed measures , some of them being based on iterative calculation of similarity between two graphs and the principle that two nodes are as similar as their neighbors are . in our work , we propose one novel method of that sort , with a refined concept of similarity of two nodes that involves matching of their neighbors . we prove convergence of the proposed method and show that it has some additional desirable properties that , to our knowledge , the existing methods lack . we illustrate the method on two specific problems and empirically compare it to other methods ."}
{"title": "powerplay : training an increasingly general problem solver by continually searching for the simplest still unsolvable problem", "abstract": "most of computer science focuses on automatically solving given computational problems . i focus on automatically inventing or discovering problems in a way inspired by the playful behavior of animals and humans , to train a more and more general problem solver from scratch in an unsupervised fashion . consider the infinite set of all computable descriptions of tasks with possibly computable solutions . the novel algorithmic framework powerplay ( 2011 ) continually searches the space of possible pairs of new tasks and modifications of the current problem solver , until it finds a more powerful problem solver that provably solves all previously learned tasks plus the new one , while the unmodified predecessor does not . wow-effects are achieved by continually making previously learned skills more efficient such that they require less time and space . new skills may ( partially ) re-use previously learned skills . powerplay 's search orders candidate pairs of tasks and solver modifications by their conditional computational ( time & space ) complexity , given the stored experience so far . the new task and its corresponding task-solving skill are those first found and validated . the computational costs of validating new tasks need not grow with task repertoire size . powerplay 's ongoing search for novelty keeps breaking the generalization abilities of its present solver . this is related to goedel 's sequence of increasingly powerful formal theories based on adding formerly unprovable statements to the axioms without affecting previously provable theorems . the continually increasing repertoire of problem solving procedures can be exploited by a parallel search for solutions to additional externally posed tasks . powerplay may be viewed as a greedy but practical implementation of basic principles of creativity . a first experimental analysis can be found in separate papers [ 53,54 ] ."}
{"title": "flow-gan : combining maximum likelihood and adversarial learning in generative models", "abstract": "adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood . implicit models such as generative adversarial networks ( gan ) often generate better samples compared to explicit models trained by maximum likelihood . yet , gans sidestep the characterization of an explicit density which makes quantitative evaluations challenging . to bridge this gap , we propose flow-gans , a generative adversarial network for which we can perform exact likelihood evaluation , thus supporting both adversarial and maximum likelihood training . when trained adversarially , flow-gans generate high-quality samples but attain extremely poor log-likelihood scores , inferior even to a mixture model memorizing the training data ; the opposite is true when trained by maximum likelihood . results on mnist and cifar-10 demonstrate that hybrid training can attain high held-out likelihoods while retaining visual fidelity in the generated samples ."}
{"title": "optimal bangla keyboard layout using data mining technique", "abstract": "this paper presents an optimal bangla keyboard layout , which distributes the load equally on both hands so that maximizing the ease and minimizing the effort . bangla alphabet has a large number of letters , for this it is difficult to type faster using bangla keyboard . our proposed keyboard will maximize the speed of operator as they can type with both hands parallel . here we use the association rule of data mining to distribute the bangla characters in the keyboard . first , we analyze the frequencies of data consisting of monograph , digraph and trigraph , which are derived from data wire-house , and then used association rule of data mining to distribute the bangla characters in the layout . experimental results on several data show the effectiveness of the proposed approach with better performance ."}
{"title": "probability as a modal operator", "abstract": "this paper argues for a modal view of probability . the syntax and semantics of one particularly strong probability logic are discussed and some examples of the use of the logic are provided . we show that it is both natural and useful to think of probability as a modal operator . contrary to popular belief in ai , a probability ranging between 0 and 1 represents a continuum between impossibility and necessity , not between simple falsity and truth . the present work provides a clear semantics for quantification into the scope of the probability operator and for higher-order probabilities . probability logic is a language for expressing both probabilistic and logical concepts ."}
{"title": "s-shaped vs. v-shaped transfer functions for antlion optimization algorithm in feature selection problems", "abstract": "feature selection is an important preprocessing step for classification problems . it deals with selecting near optimal features in the original dataset . feature selection is an np-hard problem , so meta-heuristics can be more efficient than exact methods . in this work , ant lion optimizer ( alo ) , which is a recent metaheuristic algorithm , is employed as a wrapper feature selection method . six variants of alo are proposed where each employ a transfer function to map a continuous search space to a discrete search space . the performance of the proposed approaches is tested on eighteen uci datasets and compared to a number of existing approaches in the literature : particle swarm optimization , gravitational search algorithm , and two existing alo-based approaches . computational experiments show that the proposed approaches efficiently explore the feature space and select the most informative features , which help to improve the classification accuracy ."}
{"title": "principles of modal and vector theory of formal intelligence systems", "abstract": "the paper considers the class of information systems capable of solving heuristic problems on basis of formal theory that was termed modal and vector theory of formal intelligent systems ( fis ) . the paper justifies the construction of fis resolution algorithm , defines the main features of these systems and proves theorems that underlie the theory . the principle of representation diversity of fis construction is formulated . the paper deals with the main principles of constructing and functioning formal intelligent system ( fis ) on basis of fis modal and vector theory . the following phenomena are considered : modular architecture of fis presentation sub-system , algorithms of data processing at every step of the stage of creating presentations . besides the paper suggests the structure of neural elements , i.e . zone detectors and processors that are the basis for fis construction ."}
{"title": "managing change in graph-structured data using description logics ( long version with appendix )", "abstract": "in this paper , we consider the setting of graph-structured data that evolves as a result of operations carried out by users or applications . we study different reasoning problems , which range from ensuring the satisfaction of a given set of integrity constraints after a given sequence of updates , to deciding the ( non- ) existence of a sequence of actions that would take the data to an ( un ) desirable state , starting either from a specific data instance or from an incomplete description of it . we consider an action language in which actions are finite sequences of conditional insertions and deletions of nodes and labels , and use description logics for describing integrity constraints and ( partial ) states of the data . we then formalize the above data management problems as a static verification problem and several planning problems . we provide algorithms and tight complexity bounds for the formalized problems , both for an expressive dl and for a variant of dl-lite ."}
{"title": "a change-detection based framework for piecewise-stationary multi-armed bandit problem", "abstract": "the multi-armed bandit problem has been extensively studied under the stationary assumption . however in reality , this assumption often does not hold because the distributions of rewards themselves may change over time . in this paper , we propose a change-detection ( cd ) based framework for multi-armed bandit problems under the piecewise-stationary setting , and study a class of change-detection based ucb ( upper confidence bound ) policies , cd-ucb , that actively detects change points and restarts the ucb indices . we then develop cusum-ucb and pht-ucb , that belong to the cd-ucb class and use cumulative sum ( cusum ) and page-hinkley test ( pht ) to detect changes . we show that cusum-ucb obtains the best known regret upper bound under mild assumptions . we also demonstrate the regret reduction of the cd-ucb policies over arbitrary bernoulli rewards and yahoo ! datasets of webpage click-through rates ."}
{"title": "gakco : a fast gapped k-mer string kernel using counting", "abstract": "string kernel ( sk ) techniques , especially those using gapped $ k $ -mers as features ( gk ) , have obtained great success in classifying sequences like dna , protein , and text . however , the state-of-the-art gk-sk runs extremely slow when we increase the dictionary size ( $ \\sigma $ ) or allow more mismatches ( $ m $ ) . this is because current gk-sk uses a trie-based algorithm to calculate co-occurrence of mismatched substrings resulting in a time cost proportional to $ o ( \\sigma^ { m } ) $ . we propose a \\textbf { fast } algorithm for calculating \\underline { ga } pped $ k $ -mer \\underline { k } ernel using \\underline { co } unting ( gakco ) . gakco uses associative arrays to calculate the co-occurrence of substrings using cumulative counting . this algorithm is fast , scalable to larger $ \\sigma $ and $ m $ , and naturally parallelizable . we provide a rigorous asymptotic analysis that compares gakco with the state-of-the-art gk-sk . theoretically , the time cost of gakco is independent of the $ \\sigma^ { m } $ term that slows down the trie-based approach . experimentally , we observe that gakco achieves the same accuracy as the state-of-the-art and outperforms its speed by factors of 2 , 100 , and 4 , on classifying sequences of dna ( 5 datasets ) , protein ( 12 datasets ) , and character-based english text ( 2 datasets ) , respectively . gakco is shared as an open source tool at \\url { https : //github.com/qdata/gakco-svm }"}
{"title": "learning weighted association rules in human phenotype ontology", "abstract": "the human phenotype ontology ( hpo ) is a structured repository of concepts ( hpo terms ) that are associated to one or more diseases . the process of association is referred to as annotation . the relevance and the specificity of both hpo terms and annotations are evaluated by a measure defined as information content ( ic ) . the analysis of annotated data is thus an important challenge for bioinformatics . there exist different approaches of analysis . from those , the use of association rules ( ar ) may provide useful knowledge , and it has been used in some applications , e.g . improving the quality of annotations . nevertheless classical association rules algorithms do not take into account the source of annotation nor the importance yielding to the generation of candidate rules with low ic . this paper presents hpo-miner ( human phenotype ontology-based weighted association rules ) a methodology for extracting weighted association rules . hpo-miner can extract relevant rules from a biological point of view . a case study on using of hpo-miner on publicly available hpo annotation datasets is used to demonstrate the effectiveness of our methodology ."}
{"title": "csgnet : neural shape parser for constructive solid geometry", "abstract": "we present a neural architecture that takes as input a 2d or 3d shape and induces a program to generate it . the in- structions in our program are based on constructive solid geometry principles , i.e. , a set of boolean operations on shape primitives defined recursively . bottom-up techniques for this task that rely on primitive detection are inherently slow since the search space over possible primitive combi- nations is large . in contrast , our model uses a recurrent neural network conditioned on the input shape to produce a sequence of instructions in a top-down manner and is sig- nificantly faster . it is also more effective as a shape detec- tor than existing state-of-the-art detection techniques . we also demonstrate that our network can be trained on novel datasets without ground-truth program annotations through policy gradient techniques ."}
{"title": "bayesian policy reuse", "abstract": "a long-lived autonomous agent should be able to respond online to novel instances of tasks from a familiar domain . acting online requires 'fast ' responses , in terms of rapid convergence , especially when the task instance has a short duration , such as in applications involving interactions with humans . these requirements can be problematic for many established methods for learning to act . in domains where the agent knows that the task instance is drawn from a family of related tasks , albeit without access to the label of any given instance , it can choose to act through a process of policy reuse from a library , rather than policy learning from scratch . in policy reuse , the agent has prior knowledge of the class of tasks in the form of a library of policies that were learnt from sample task instances during an offline training phase . we formalise the problem of policy reuse , and present an algorithm for efficiently responding to a novel task instance by reusing a policy from the library of existing policies , where the choice is based on observed 'signals ' which correlate to policy performance . we achieve this by posing the problem as a bayesian choice problem with a corresponding notion of an optimal response , but the computation of that response is in many cases intractable . therefore , to reduce the computation cost of the posterior , we follow a bayesian optimisation approach and define a set of policy selection functions , which balance exploration in the policy library against exploitation of previously tried policies , together with a model of expected performance of the policy library on their corresponding task instances . we validate our method in several simulated domains of interactive , short-duration episodic tasks , showing rapid convergence in unknown task variations ."}
{"title": "multi-robot active information gathering with periodic communication", "abstract": "a team of robots sharing a common goal can benefit from coordination of the activities of team members , helping the team to reach the goal more reliably or quickly . we address the problem of coordinating the actions of a team of robots with periodic communication capability executing an information gathering task . we cast the problem as a multi-agent optimal decision-making problem with an information theoretic objective function . we show that appropriate techniques for solving decentralized partially observable markov decision processes ( dec-pomdps ) are applicable in such information gathering problems . we quantify the usefulness of coordinated information gathering through simulation studies , and demonstrate the feasibility of the method in a real-world target tracking domain ."}
{"title": "discriminative functional connectivity measures for brain decoding", "abstract": "we propose a statistical learning model for classifying cognitive processes based on distributed patterns of neural activation in the brain , acquired via functional magnetic resonance imaging ( fmri ) . in the proposed learning method , local meshes are formed around each voxel . the distance between voxels in the mesh is determined by using a functional neighbourhood concept . in order to define the functional neighbourhood , the similarities between the time series recorded for voxels are measured and functional connectivity matrices are constructed . then , the local mesh for each voxel is formed by including the functionally closest neighbouring voxels in the mesh . the relationship between the voxels within a mesh is estimated by using a linear regression model . these relationship vectors , called functional connectivity aware local relational features ( fc-lrf ) are then used to train a statistical learning machine . the proposed method was tested on a recognition memory experiment , including data pertaining to encoding and retrieval of words belonging to ten different semantic categories . two popular classifiers , namely k-nearest neighbour ( k-nn ) and support vector machine ( svm ) , are trained in order to predict the semantic category of the item being retrieved , based on activation patterns during encoding . the classification performance of the functional mesh learning model , which range in 62 % -71 % is superior to the classical multi-voxel pattern analysis ( mvpa ) methods , which range in 40 % -48 % , for ten semantic categories ."}
{"title": "efficient induction of finite state automata", "abstract": "this paper introduces a new algorithm for the induction if complex finite state automata from samples of behavior . the algorithm is based on information theoretic principles . the algorithm reduces the search space by many orders of magnitude over what was previously thought possible . we compare the algorithm with some existing induction techniques for finite state automata and show that the algorithm is much superior in both run time and quality of inductions ."}
{"title": "human trajectory prediction using spatially aware deep attention models", "abstract": "trajectory prediction of dynamic objects is a widely studied topic in the field of artificial intelligence . thanks to a large number of applications like predicting abnormal events , navigation system for the blind , etc . there have been many approaches to attempt learning patterns of motion directly from data using a wide variety of techniques ranging from hand-crafted features to sophisticated deep learning models for unsupervised feature learning . all these approaches have been limited by problems like inefficient features in the case of hand crafted features , large error propagation across the predicted trajectory and no information of static artefacts around the dynamic moving objects . we propose an end to end deep learning model to learn the motion patterns of humans using different navigational modes directly from data using the much popular sequence to sequence model coupled with a soft attention mechanism . we also propose a novel approach to model the static artefacts in a scene and using these to predict the dynamic trajectories . the proposed method , tested on trajectories of pedestrians , consistently outperforms previously proposed state of the art approaches on a variety of large scale data sets . we also show how our architecture can be naturally extended to handle multiple modes of movement ( say pedestrians , skaters , bikers and buses ) simultaneously ."}
{"title": "splitting and updating hybrid knowledge bases ( extended version )", "abstract": "over the years , nonmonotonic rules have proven to be a very expressive and useful knowledge representation paradigm . they have recently been used to complement the expressive power of description logics ( dls ) , leading to the study of integrative formal frameworks , generally referred to as hybrid knowledge bases , where both dl axioms and rules can be used to represent knowledge . the need to use these hybrid knowledge bases in dynamic domains has called for the development of update operators , which , given the substantially different way description logics and rules are usually updated , has turned out to be an extremely difficult task . in [ sl10 ] , a first step towards addressing this problem was taken , and an update operator for hybrid knowledge bases was proposed . despite its significance -- not only for being the first update operator for hybrid knowledge bases in the literature , but also because it has some applications - this operator was defined for a restricted class of problems where only the abox was allowed to change , which considerably diminished its applicability . many applications that use hybrid knowledge bases in dynamic scenarios require both dl axioms and rules to be updated . in this paper , motivated by real world applications , we introduce an update operator for a large class of hybrid knowledge bases where both the dl component as well as the rule component are allowed to dynamically change . we introduce splitting sequences and splitting theorem for hybrid knowledge bases , use them to define a modular update semantics , investigate its basic properties , and illustrate its use on a realistic example about cargo imports ."}
{"title": "learning semantic script knowledge with event embeddings", "abstract": "induction of common sense knowledge about prototypical sequences of events has recently received much attention . instead of inducing this knowledge in the form of graphs , as in much of the previous work , in our method , distributed representations of event realizations are computed based on distributed representations of predicates and their arguments , and then these representations are used to predict prototypical event orderings . the parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts . we show that this approach results in a substantial boost in ordering performance with respect to previous methods ."}
{"title": "empirical probabilities in monadic deductive databases", "abstract": "we address the problem of supporting empirical probabilities in monadic logic databases . though the semantics of multivalued logic programs has been studied extensively , the treatment of probabilities as results of statistical findings has not been studied in logic programming/deductive databases . we develop a model-theoretic characterization of logic databases that facilitates such a treatment . we present an algorithm for checking consistency of such databases and prove its total correctness . we develop a sound and complete query processing procedure for handling queries to such databases ."}
{"title": "pose flow : efficient online pose tracking", "abstract": "multi-person articulated pose tracking in complex unconstrained videos is an important and challenging problem . in this paper , going along the road of top-down approaches , we propose a decent and efficient pose tracker based on pose flows . first , we design an online optimization framework to build association of cross-frame poses and form pose flows . second , a novel pose flow non maximum suppression ( nms ) is designed to robustly reduce redundant pose flows and re-link temporal disjoint pose flows . extensive experiments show our method significantly outperforms best reported results on two standard pose tracking datasets ( posetrack dataset and posetrack challenge dataset ) by 13 map 25 mota and 6 map 3 mota respectively . moreover , in the case of working on detected poses in individual frames , the extra computation of proposed pose tracker is very minor , requiring 0.01 second per frame only ."}
{"title": "scalable l\u00e9vy process priors for spectral kernel learning", "abstract": "gaussian processes are rich distributions over functions , with generalization properties determined by a kernel function . when used for long-range extrapolation , predictions are particularly sensitive to the choice of kernel parameters . it is therefore critical to account for kernel uncertainty in our predictive distributions . we propose a distribution over kernels formed by modelling a spectral mixture density with a l\\'evy process . the resulting distribution has support for all stationary covariances -- including the popular rbf , periodic , and mat\\'ern kernels -- combined with inductive biases which enable automatic and data efficient learning , long-range extrapolation , and state of the art predictive performance . the proposed model also presents an approach to spectral regularization , as the l\\'evy process introduces a sparsity-inducing prior over mixture components , allowing automatic selection over model order and pruning of extraneous components . we exploit the algebraic structure of the proposed process for $ \\mathcal { o } ( n ) $ training and $ \\mathcal { o } ( 1 ) $ predictions . we perform extrapolations having reasonable uncertainty estimates on several benchmarks , show that the proposed model can recover flexible ground truth covariances and that it is robust to errors in initialization ."}
{"title": "formalising hypothesis virtues in knowledge graphs : a general theoretical framework and its validation in literature-based discovery experiments", "abstract": "we introduce an approach to discovery informatics that uses so called knowledge graphs as the essential representation structure . knowledge graph is an umbrella term that subsumes various approaches to tractable representation of large volumes of loosely structured knowledge in a graph form . it has been used primarily in the web and linked open data contexts , but is applicable to any other area dealing with knowledge representation . in the perspective of our approach motivated by the challenges of discovery informatics , knowledge graphs correspond to hypotheses . we present a framework for formalising so called hypothesis virtues within knowledge graphs . the framework is based on a classic work in philosophy of science , and naturally progresses from mostly informative foundational notions to actionable specifications of measures corresponding to particular virtues . these measures can consequently be used to determine refined sub-sets of knowledge graphs that have large relative potential for making discoveries . we validate the proposed framework by experiments in literature-based discovery . the experiments have demonstrated the utility of our work and its superiority w.r.t . related approaches ."}
{"title": "the harmonic theory ; a mathematical framework to build intelligent contextual and adaptive computing , cognition and sensory system", "abstract": "harmonic theory provides a mathematical framework to describe the structure , behavior , evolution and emergence of harmonic systems . a harmonic system is context aware , contains elements that manifest characteristics either collaboratively or independently according to system 's expression and can interact with its environment . this theory provides a fresh way to analyze emergence and collaboration of `` ad-hoc '' and complex systems ."}
{"title": "using automatic generation of relaxation constraints to improve the preimage attack on 39-step md4", "abstract": "in this paper we construct preimage attack on the truncated variant of the md4 hash function . specifically , we study the md4-39 function defined by the first 39 steps of the md4 algorithm . we suggest a new attack on md4-39 , which develops the ideas proposed by h. dobbertin in 1998. namely , the special relaxation constraints are introduced in order to simplify the equations corresponding to the problem of finding a preimage for an arbitrary md4-39 hash value . the equations supplemented with the relaxation constraints are then reduced to the boolean satisfiability problem ( sat ) and solved using the state-of-the-art sat solvers . we show that the effectiveness of a set of relaxation constraints can be evaluated using the black-box function of a special kind . thus , we suggest automatic method of relaxation constraints generation by applying the black-box optimization to this function . the proposed method made it possible to find new relaxation constraints that contribute to a sat-based preimage attack on md4-39 which significantly outperforms the competition ."}
{"title": "towards the patterns of hard csps with association rule mining", "abstract": "the hardness of finite domain constraint satisfaction problems ( csps ) is a very important research area in constraint programming ( cp ) community . however , this problem has not yet attracted much attention from the researchers in the association rule mining community . as a popular data mining technique , association rule mining has an extremely wide application area and it has already been successfully applied to many interdisciplines . in this paper , we study the association rule mining techniques and propose a cascaded approach to extract the interesting patterns of the hard csps . as far as we know , this problem is investigated with the data mining techniques for the first time . specifically , we generate the random csps and collect their characteristics by solving all the csp instances , and then apply the data mining techniques on the data set and further to discover the interesting patterns of the hardness of the randomly generated csps"}
{"title": "a novel representation of neural networks", "abstract": "deep neural networks ( dnns ) have become very popular for prediction in many areas . their strength is in representation with a high number of parameters that are commonly learned via gradient descent or similar optimization methods . however , the representation is non-standardized , and the gradient calculation methods are often performed using component-based approaches that break parameters down into scalar units , instead of considering the parameters as whole entities . in this work , these problems are addressed . standard notation is used to represent dnns in a compact framework . gradients of dnn loss functions are calculated directly over the inner product space on which the parameters are defined . this framework is general and is applied to two common network types : the multilayer perceptron and the deep autoencoder ."}
{"title": "using t-norm based uncertainty calculi in a naval situation assessment application", "abstract": "rum ( reasoning with uncertainty module ) , is an integrated software tool based on a kee , a frame system implemented in an object oriented language . rum 's architecture is composed of three layers : representation , inference , and control . the representation layer is based on frame-like data structures that capture the uncertainty information used in the inference layer and the uncertainty meta-information used in the control layer . the inference layer provides a selection of five t-norm based uncertainty calculi with which to perform the intersection , detachment , union , and pooling of information . the control layer uses the meta-information to select the appropriate calculus for each context and to resolve eventual ignorance or conflict in the information . this layer also provides a context mechanism that allows the system to focus on the relevant portion of the knowledge base , and an uncertain-belief revision system that incrementally updates the certainty values of well-formed formulae ( wffs ) in an acyclic directed deduction graph . rum has been tested and validated in a sequence of experiments in both naval and aerial situation assessment ( sa ) , consisting of correlating reports and tracks , locating and classifying platforms , and identifying intents and threats . an example of naval situation assessment is illustrated . the testbed environment for developing these experiments has been provided by lotta , a symbolic simulator implemented in flavors . this simulator maintains time-varying situations in a multi-player antagonistic game where players must make decisions in light of uncertain and incomplete data . rum has been used to assist one of the lotta players to perform the sa task ."}
{"title": "totally corrective boosting for regularized risk minimization", "abstract": "consideration of the primal and dual problems together leads to important new insights into the characteristics of boosting algorithms . in this work , we propose a general framework that can be used to design new boosting algorithms . a wide variety of machine learning problems essentially minimize a regularized risk functional . we show that the proposed boosting framework , termed cgboost , can accommodate various loss functions and different regularizers in a totally-corrective optimization fashion . we show that , by solving the primal rather than the dual , a large body of totally-corrective boosting algorithms can actually be efficiently solved and no sophisticated convex optimization solvers are needed . we also demonstrate that some boosting algorithms like adaboost can be interpreted in our framework -- even their optimization is not totally corrective . we empirically show that various boosting algorithms based on the proposed framework perform similarly on the ucirvine machine learning datasets [ 1 ] that we have used in the experiments ."}
{"title": "encoding markov logic networks in possibilistic logic", "abstract": "markov logic uses weighted formulas to compactly encode a probability distribution over possible worlds . despite the use of logical formulas , markov logic networks ( mlns ) can be difficult to interpret , due to the often counter-intuitive meaning of their weights . to address this issue , we propose a method to construct a possibilistic logic theory that exactly captures what can be derived from a given mln using maximum a posteriori ( map ) inference . unfortunately , the size of this theory is exponential in general . we therefore also propose two methods which can derive compact theories that still capture map inference , but only for specific types of evidence . these theories can be used , among others , to make explicit the hidden assumptions underlying an mln or to explain the predictions it makes ."}
{"title": "racing multi-objective selection probabilities", "abstract": "in the context of noisy multi-objective optimization , dealing with uncertainties requires the decision maker to define some preferences about how to handle them , through some statistics ( e.g. , mean , median ) to be used to evaluate the qualities of the solutions , and define the corresponding pareto set . approximating these statistics requires repeated samplings of the population , drastically increasing the overall computational cost . to tackle this issue , this paper proposes to directly estimate the probability of each individual to be selected , using some hoeffding races to dynamically assign the estimation budget during the selection step . the proposed racing approach is validated against static budget approaches with nsga-ii on noisy versions of the zdt benchmark functions ."}
{"title": "a matrix approach for computing extensions of argumentation frameworks", "abstract": "the matrices and their sub-blocks are introduced into the study of determining various extensions in the sense of dung 's theory of argumentation frameworks . it is showed that each argumentation framework has its matrix representations , and the core semantics defined by dung can be characterized by specific sub-blocks of the matrix . furthermore , the elementary permutations of a matrix are employed by which an efficient matrix approach for finding out all extensions under a given semantics is obtained . different from several established approaches , such as the graph labelling algorithm , constraint satisfaction problem algorithm , the matrix approach not only put the mathematic idea into the investigation for finding out various extensions , but also completely achieve the goal to compute all the extensions needed ."}
{"title": "parallel strategies selection", "abstract": "we consider the problem of selecting the best variable-value strategy for solving a given problem in constraint programming . we show that the recent embarrassingly parallel search method ( eps ) can be used for this purpose . eps proposes to solve a problem by decomposing it in a lot of subproblems and to give them on-demand to workers which run in parallel . our method uses a part of these subproblems as a simple sample as defined in statistics for comparing some strategies in order to select the most promising one that will be used for solving the remaining subproblems . for each subproblem of the sample , the parallelism helps us to control the running time of the strategies because it gives us the possibility to introduce timeouts by stopping a strategy when it requires more than twice the time of the best one . thus , we can deal with the great disparity in solving times for the strategies . the selections we made are based on the wilcoxon signed rank tests because no assumption has to be made on the distribution of the solving times and because these tests can deal with the censored data that we obtain after introducing timeouts . the experiments we performed on a set of classical benchmarks for satisfaction and optimization problems show that our method obtain good performance by selecting almost all the time the best variable-value strategy and by almost never choosing a variable-value strategy which is dramatically slower than the best one . our method also outperforms the portfolio approach consisting in running some strategies in parallel and is competitive with the multi armed bandit framework ."}
{"title": "quantitative analysis of whether machine intelligence can surpass human intelligence", "abstract": "whether the machine intelligence can surpass the human intelligence is a controversial issue . on the basis of traditional iq , this article presents the universal iq test method suitable for both the machine intelligence and the human intelligence . with the method , machine and human intelligences were divided into 4 major categories and 15 subcategories . a total of 50 search engines across the world and 150 persons at different ages were subject to the relevant test . and then , the universal iq ranking list of 2014 for the test objects was obtained ."}
{"title": "spatio-temporal data mining : a survey of problems and methods", "abstract": "large volumes of spatio-temporal data are increasingly collected and studied in diverse domains including , climate science , social sciences , neuroscience , epidemiology , transportation , mobile health , and earth sciences . spatio-temporal data differs from relational data for which computational approaches are developed in the data mining community for multiple decades , in that both spatial and temporal attributes are available in addition to the actual measurements/attributes . the presence of these attributes introduces additional challenges that needs to be dealt with . approaches for mining spatio-temporal data have been studied for over a decade in the data mining community . in this article we present a broad survey of this relatively young field of spatio-temporal data mining . we discuss different types of spatio-temporal data and the relevant data mining questions that arise in the context of analyzing each of these datasets . based on the nature of the data mining problem studied , we classify literature on spatio-temporal data mining into six major categories : clustering , predictive learning , change detection , frequent pattern mining , anomaly detection , and relationship mining . we discuss the various forms of spatio-temporal data mining problems in each of these categories ."}
{"title": "gibbs sampling in open-universe stochastic languages", "abstract": "languages for open-universe probabilistic models ( oupms ) can represent situations with an unknown number of objects and iden- tity uncertainty . while such cases arise in a wide range of important real-world appli- cations , existing general purpose inference methods for oupms are far less efficient than those available for more restricted lan- guages and model classes . this paper goes some way to remedying this deficit by in- troducing , and proving correct , a generaliza- tion of gibbs sampling to partial worlds with possibly varying model structure . our ap- proach draws on and extends previous generic oupm inference methods , as well as aux- iliary variable samplers for nonparametric mixture models . it has been implemented for blog , a well-known oupm language . combined with compile-time optimizations , the resulting algorithm yields very substan- tial speedups over existing methods on sev- eral test cases , and substantially improves the practicality of oupm languages generally ."}
{"title": "on the notion of cognition", "abstract": "we discuss philosophical issues concerning the notion of cognition basing ourselves in experimental results in cognitive sciences , especially in computer simulations of cognitive systems . there have been debates on the `` proper '' approach for studying cognition , but we have realized that all approaches can be in theory equivalent . different approaches model different properties of cognitive systems from different perspectives , so we can only learn from all of them . we also integrate ideas from several perspectives for enhancing the notion of cognition , such that it can contain other definitions of cognition as special cases . this allows us to propose a simple classification of different types of cognition ."}
{"title": "learning robust dialog policies in noisy environments", "abstract": "modern virtual personal assistants provide a convenient interface for completing daily tasks via voice commands . an important consideration for these assistants is the ability to recover from automatic speech recognition ( asr ) and natural language understanding ( nlu ) errors . in this paper , we focus on learning robust dialog policies to recover from these errors . to this end , we develop a user simulator which interacts with the assistant through voice commands in realistic scenarios with noisy audio , and use it to learn dialog policies through deep reinforcement learning . we show that dialogs generated by our simulator are indistinguishable from human generated dialogs , as determined by human evaluators . furthermore , preliminary experimental results show that the learned policies in noisy environments achieve the same execution success rate with fewer dialog turns compared to fixed rule-based policies ."}
{"title": "inferring cognitive models from data using approximate bayesian computation", "abstract": "an important problem for hci researchers is to estimate the parameter values of a cognitive model from behavioral data . this is a difficult problem , because of the substantial complexity and variety in human behavioral strategies . we report an investigation into a new approach using approximate bayesian computation ( abc ) to condition model parameters to data and prior knowledge . as the case study we examine menu interaction , where we have click time data only to infer a cognitive model that implements a search behaviour with parameters such as fixation duration and recall probability . our results demonstrate that abc ( i ) improves estimates of model parameter values , ( ii ) enables meaningful comparisons between model variants , and ( iii ) supports fitting models to individual users . abc provides ample opportunities for theoretical hci research by allowing principled inference of model parameter values and their uncertainty ."}
{"title": "on the equivalence of causal models", "abstract": "scientists often use directed acyclic graphs ( days ) to model the qualitative structure of causal theories , allowing the parameters to be estimated from observational data . two causal models are equivalent if there is no experiment which could distinguish one from the other . a canonical representation for causal models is presented which yields an efficient graphical criterion for deciding equivalence , and provides a theoretical basis for extracting causal structures from empirical data . this representation is then extended to the more general case of an embedded causal model , that is , a dag in which only a subset of the variables are observable . the canonical representation presented here yields an efficient algorithm for determining when two embedded causal models reflect the same dependency information . this algorithm leads to a model theoretic definition of causation in terms of statistical dependencies ."}
{"title": "understanding design fundamentals : how synthesis and analysis drive creativity , resulting in emergence", "abstract": "this paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design . human design activities are surveyed , and popular computer-aided design methodologies are examined . it is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use . reviewing related work in philosophy , psychology , and cognitive science provides a general and encompassing vision of the creativity phenomenon . basic notions of algebraic semiotics are given and explained in terms of design . this is to define a model of the design creative process , which is seen as a process of semiosis , where concepts and their attributes represented as signs organized into systems are evolved , blended , and analyzed , resulting in the development of new concepts . the model allows us to formally describe and investigate essential properties of the design process , namely its dynamics and non-determinism inherent in creative thinking . a stable pattern of creative thought - analogical and metaphorical reasoning - is specified to demonstrate the expressive power of the modeling approach ; illustrative examples are given . the developed theory is applied to clarify the nature of emergence in design : it is shown that while emergent properties of a product may influence its creative value , emergence can simply be seen as a by-product of the creative process . concluding remarks summarize the research , point to some unresolved issues , and outline directions for future work ."}
{"title": "bayesian inference in monte-carlo tree search", "abstract": "monte-carlo tree search ( mcts ) methods are drawing great interest after yielding breakthrough results in computer go . this paper proposes a bayesian approach to mcts that is inspired by distributionfree approaches such as uct [ 13 ] , yet significantly differs in important respects . the bayesian framework allows potentially much more accurate ( bayes-optimal ) estimation of node values and node uncertainties from a limited number of simulation trials . we further propose propagating inference in the tree via fast analytic gaussian approximation methods : this can make the overhead of bayesian inference manageable in domains such as go , while preserving high accuracy of expected-value estimates . we find substantial empirical outperformance of uct in an idealized bandit-tree test environment , where we can obtain valuable insights by comparing with known ground truth . additionally we rigorously prove on-policy and off-policy convergence of the proposed methods ."}
{"title": "ethical considerations in artificial intelligence courses", "abstract": "the recent surge in interest in ethics in artificial intelligence may leave many educators wondering how to address moral , ethical , and philosophical issues in their ai courses . as instructors we want to develop curriculum that not only prepares students to be artificial intelligence practitioners , but also to understand the moral , ethical , and philosophical impacts that artificial intelligence will have on society . in this article we provide practical case studies and links to resources for use by ai educators . we also provide concrete suggestions on how to integrate ai ethics into a general artificial intelligence course and how to teach a stand-alone artificial intelligence ethics course ."}
{"title": "geomrdf : a geodata converter with a fine-grained structured representation of geometry in the web", "abstract": "in recent years , with the advent of the web of data , a growing number of national mapping agencies tend to publish their geospatial data as linked data . however , differences between traditional gis data models and linked data model can make the publication process more complicated . besides , it may require , to be done , the setting of several parameters and some expertise in the semantic web technologies . in addition , the use of standards like geosparql ( or ad hoc predicates ) is mandatory to perform spatial queries on published geospatial data . in this paper , we present geomrdf , a tool that helps users to convert spatial data from traditional gis formats to rdf model easily . it generates geometries represented as geosparql wkt literal but also as structured geometries that can be exploited by using only the rdf query language , sparql . geomrdf was implemented as a module in the rdf publication platform datalift . a validation of geomrdf has been realized against the french administrative units dataset ( provided by ign france ) ."}
{"title": "google 's neural machine translation system : bridging the gap between human and machine translation", "abstract": "neural machine translation ( nmt ) is an end-to-end learning approach for automated translation , with the potential to overcome many of the weaknesses of conventional phrase-based translation systems . unfortunately , nmt systems are known to be computationally expensive both in training and in translation inference . also , most nmt systems have difficulty with rare words . these issues have hindered nmt 's use in practical deployments and services , where both accuracy and speed are essential . in this work , we present gnmt , google 's neural machine translation system , which attempts to address many of these issues . our model consists of a deep lstm network with 8 encoder and 8 decoder layers using attention and residual connections . to improve parallelism and therefore decrease training time , our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder . to accelerate the final translation speed , we employ low-precision arithmetic during inference computations . to improve handling of rare words , we divide words into a limited set of common sub-word units ( `` wordpieces '' ) for both input and output . this method provides a good balance between the flexibility of `` character '' -delimited models and the efficiency of `` word '' -delimited models , naturally handles translation of rare words , and ultimately improves the overall accuracy of the system . our beam search technique employs a length-normalization procedure and uses a coverage penalty , which encourages generation of an output sentence that is most likely to cover all the words in the source sentence . on the wmt'14 english-to-french and english-to-german benchmarks , gnmt achieves competitive results to state-of-the-art . using a human side-by-side evaluation on a set of isolated simple sentences , it reduces translation errors by an average of 60 % compared to google 's phrase-based production system ."}
{"title": "proceedings of the 12th international colloquium on implementation of constraint and logic programming systems", "abstract": "this volume contains the papers presented at ciclops'12 : 12th international colloquium on implementation of constraint and logic programming systems held on tueseday september 4th , 2012 in budapest . the program included 1 invited talk , 9 technical presentations and a panel discussion on prolog open standards ( open.pl ) . each programme paper was reviewed by 3 reviewers . ciclops'12 continues a tradition of successful workshops on implementations of logic programming systems , previously held in budapest ( 1993 ) and ithaca ( 1994 ) , the compulog net workshops on parallelism and implementation technologies held in madrid ( 1993 and 1994 ) , utrecht ( 1995 ) and bonn ( 1996 ) , the workshop on parallelism and implementation technology for ( constraint ) logic programming languages held in port jefferson ( 1997 ) , manchester ( 1998 ) , las cruces ( 1999 ) , and london ( 2000 ) , and more recently the colloquium on implementation of constraint and logic programming systems in paphos ( 2001 ) , copenhagen ( 2002 ) , mumbai ( 2003 ) , saint malo ( 2004 ) , sitges ( 2005 ) , seattle ( 2006 ) , porto ( 2007 ) , udine ( 2008 ) , pasadena ( 2009 ) , edinburgh ( 2010 ) - together with wlpe , lexington ( 2011 ) . we would like to thank all the authors , tom schrijvers for his invited talk , the programme committee members , and the iclp 2012 organisers . we would like to also thank arxiv.org for providing permanent hosting ."}
{"title": "a linear-programming approximation of ac power flows", "abstract": "linear active-power-only dc power flow approximations are pervasive in the planning and control of power systems . however , these approximations fail to capture reactive power and voltage magnitudes , both of which are necessary in many applications to ensure voltage stability and ac power flow feasibility . this paper proposes linear-programming models ( the lpac models ) that incorporate reactive power and voltage magnitudes in a linear power flow approximation . the lpac models are built on a convex approximation of the cosine terms in the ac equations , as well as taylor approximations of the remaining nonlinear terms . experimental comparisons with ac solutions on a variety of standard ieee and matpower benchmarks show that the lpac models produce accurate values for active and reactive power , phase angles , and voltage magnitudes . the potential benefits of the lpac models are illustrated on two `` proof-of-concept '' studies in power restoration and capacitor placement ."}
{"title": "towards combining htn planning and geometric task planning", "abstract": "in this paper we present an interface between a symbolic planner and a geometric task planner , which is different to a standard trajectory planner in that the former is able to perform geometric reasoning on abstract entities -- -tasks . we believe that this approach facilitates a more principled interface to symbolic planning , while also leaving more room for the geometric planner to make independent decisions . we show how the two planners could be interfaced , and how their planning and backtracking could be interleaved . we also provide insights for a methodology for using the combined system , and experimental results to use as a benchmark with future extensions to both the combined system , as well as to the geometric task planner ."}
{"title": "explanations based on the missing : towards contrastive explanations with pertinent negatives", "abstract": "in this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network . given an input we find what should be minimally and sufficiently present ( viz . important object pixels in an image ) to justify its classification and analogously what should be minimally and necessarily \\emph { absent } ( viz . certain background pixels ) . we argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology . what is minimally but critically \\emph { absent } is an important part of an explanation , which to the best of our knowledge , has not been touched upon by current explanation methods that attempt to explain predictions of neural networks . we validate our approach on three real datasets obtained from diverse domains ; namely , a handwritten digits dataset mnist , a large procurement fraud dataset and an fmri brain imaging dataset . in all three cases , we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate ."}
{"title": "databright : towards a global exchange for decentralized data ownership and trusted computation", "abstract": "it is safe to assume that , for the foreseeable future , machine learning , especially deep learning will remain both data- and computation-hungry . in this paper , we ask : can we build a global exchange where everyone can contribute computation and data to train the next generation of machine learning applications ? we present an early , but running prototype of databright , a system that turns the creation of training examples and the sharing of computation into an investment mechanism . unlike most crowdsourcing platforms , where the contributor gets paid when they submit their data , databright pays dividends whenever a contributor 's data or hardware is used by someone to train a machine learning model . the contributor becomes a shareholder in the dataset they created . to enable the measurement of usage , a computation platform that contributors can trust is also necessary . databright thus merges both a data market and a trusted computation market . we illustrate that trusted computation can enable the creation of an ai market , where each data point has an exact value that should be paid to its creator . databright allows data creators to retain ownership of their contribution and attaches to it a measurable value . the value of the data is given by its utility in subsequent distributed computation done on the databright computation market . the computation market allocates tasks and subsequent payments to pooled hardware . this leads to the creation of a decentralized ai cloud . our experiments show that trusted hardware such as intel sgx can be added to the usual ml pipeline with no additional costs . we use this setting to orchestrate distributed computation that enables the creation of a computation market . databright is available for download at https : //github.com/ds3lab/databright ."}
{"title": "analysis of supervised and semi-supervised growcut applied to segmentation of masses in mammography images", "abstract": "breast cancer is already one of the most common form of cancer worldwide . mammography image analysis is still the most effective diagnostic method to promote the early detection of breast cancer . accurately segmenting tumors in digital mammography images is important to improve diagnosis capabilities of health specialists and avoid misdiagnosis . in this work , we evaluate the feasibility of applying growcut to segment regions of tumor and we propose two growcut semi-supervised versions . all the analysis was performed by evaluating the application of segmentation techniques to a set of images obtained from the mini-mias mammography image database . growcut segmentation was compared to region growing , active contours , random walks and graph cut techniques . experiments showed that growcut , when compared to the other techniques , was able to acquire better results for the metrics analyzed . moreover , the proposed semi-supervised versions of growcut was proved to have a clinically satisfactory quality of segmentation ."}
{"title": "intention-net : integrating planning and deep learning for goal-directed autonomous navigation", "abstract": "how can a delivery robot navigate reliably to a destination in a new office building , with minimal prior information ? to tackle this challenge , this paper introduces a two-level hierarchical approach , which integrates model-free deep learning and model-based path planning . at the low level , a neural-network motion controller , called the intention-net , is trained end-to-end to provide robust local navigation . the intention-net maps images from a single monocular camera and `` intentions '' directly to robot controls . at the high level , a path planner uses a crude map , e.g. , a 2-d floor plan , to compute a path from the robot 's current location to the goal . the planned path provides intentions to the intention-net . preliminary experiments suggest that the learned motion controller is robust against perceptual uncertainty and by integrating with a path planner , it generalizes effectively to new environments and goals ."}
{"title": "parallel computation is ess", "abstract": "there are enormous amount of examples of computation in nature , exemplified across multiple species in biology . one crucial aim for these computations across all life forms their ability to learn and thereby increase the chance of their survival . in the current paper a formal definition of autonomous learning is proposed . from that definition we establish a turing machine model for learning , where rule tables can be added or deleted , but can not be modified . sequential and parallel implementations of this model are discussed . it is found that for general purpose learning based on this model , the implementations capable of parallel execution would be evolutionarily stable . this is proposed to be of the reasons why in nature parallelism in computation is found in abundance ."}
{"title": "implementing an intelligent version of the classical sliding-puzzle game for unix terminals using golang 's concurrency primitives", "abstract": "an intelligent version of the sliding-puzzle game is developed using the new go programming language , which uses a concurrent version of the a* informed search algorithm to power solver-bot that runs in the background . the game runs in computer system 's terminals . mainly , it was developed for unix-type systems but it works pretty well in nearly all the operating systems because of cross-platform compatibility of the programming language used . the game uses language 's concurrency primitives to simplify most of the hefty parts of the game . a real-time notification delivery architecture is developed using language 's built-in concurrency support , which performs similar to event based context aware invocations like we see on the web platform ."}
{"title": "learning from richer human guidance : augmenting comparison-based learning with feature queries", "abstract": "we focus on learning the desired objective function for a robot . although trajectory demonstrations can be very informative of the desired objective , they can also be difficult for users to provide . answers to comparison queries , asking which of two trajectories is preferable , are much easier for users , and have emerged as an effective alternative . unfortunately , comparisons are far less informative . we propose that there is much richer information that users can easily provide and that robots ought to leverage . we focus on augmenting comparisons with feature queries , and introduce a unified formalism for treating all answers as observations about the true desired reward . we derive an active query selection algorithm , and test these queries in simulation and on real users . we find that richer , feature-augmented queries can extract more information faster , leading to robots that better match user preferences in their behavior ."}
{"title": "mcts based on simple regret", "abstract": "uct , a state-of-the art algorithm for monte carlo tree search ( mcts ) in games and markov decision processes , is based on ucb , a sampling policy for the multi-armed bandit problem ( mab ) that minimizes the cumulative regret . however , search differs from mab in that in mcts it is usually only the final `` arm pull '' ( the actual move selection ) that collects a reward , rather than all `` arm pulls '' . therefore , it makes more sense to minimize the simple regret , as opposed to the cumulative regret . we begin by introducing policies for multi-armed bandits with lower finite-time and asymptotic simple regret than ucb , using it to develop a two-stage scheme ( sr+cr ) for mcts which outperforms uct empirically . optimizing the sampling process is itself a metareasoning problem , a solution of which can use value of information ( voi ) techniques . although the theory of voi for search exists , applying it to mcts is non-trivial , as typical myopic assumptions fail . lacking a complete working voi theory for mcts , we nevertheless propose a sampling scheme that is `` aware '' of voi , achieving an algorithm that in empirical evaluation outperforms both uct and the other proposed algorithms ."}
{"title": "informledge system : a modified knowledge network with autonomous nodes using multi-lateral links", "abstract": "research in the field of artificial intelligence is continually progressing to simulate the human knowledge into automated intelligent knowledge base , which can encode and retrieve knowledge efficiently along with the capability of being is consistent and scalable at all times . however , there is no system at hand that can match the diversified abilities of human knowledge base . in this position paper , we put forward a theoretical model of a different system that intends to integrate pieces of knowledge , informledge system ( ils ) . ils would encode the knowledge , by virtue of knowledge units linked across diversified domains . the proposed ils comprises of autonomous knowledge units termed as knowledge network node ( knn ) , which would help in efficient cross-linking of knowledge units to encode fresh knowledge . these links are reasoned and inferred by the parser and link manager , which are part of knn ."}
{"title": "samplernn : an unconditional end-to-end neural audio generation model", "abstract": "in this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time . we show that our model , which profits from combining memory-less modules , namely autoregressive multilayer perceptrons , and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans , on three datasets of different nature . human evaluation on the generated samples indicate that our model is preferred over competing models . we also show how each component of the model contributes to the exhibited performance ."}
{"title": "knowledge-based programs as plans : succinctness and the complexity of plan existence", "abstract": "knowledge-based programs ( kbps ) are high-level protocols describing the course of action an agent should perform as a function of its knowledge . the use of kbps for expressing action policies in ai planning has been surprisingly overlooked . given that to each kbp corresponds an equivalent plan and vice versa , kbps are typically more succinct than standard plans , but imply more on-line computation time . here we make this argument formal , and prove that there exists an exponential succinctness gap between knowledge-based programs and standard plans . then we address the complexity of plan existence . some results trivially follow from results already known from the literature on planning under incomplete knowledge , but many were unknown so far ."}
{"title": "the ibmap approach for markov networks structure learning", "abstract": "in this work we consider the problem of learning the structure of markov networks from data . we present an approach for tackling this problem called ibmap , together with an efficient instantiation of the approach : the ibmap-hc algorithm , designed for avoiding important limitations of existing independence-based algorithms . these algorithms proceed by performing statistical independence tests on data , trusting completely the outcome of each test . in practice tests may be incorrect , resulting in potential cascading errors and the consequent reduction in the quality of the structures learned . ibmap contemplates this uncertainty in the outcome of the tests through a probabilistic maximum-a-posteriori approach . the approach is instantiated in the ibmap-hc algorithm , a structure selection strategy that performs a polynomial heuristic local search in the space of possible structures . we present an extensive empirical evaluation on synthetic and real data , showing that our algorithm outperforms significantly the current independence-based algorithms , in terms of data efficiency and quality of learned structures , with equivalent computational complexities . we also show the performance of ibmap-hc in a real-world application of knowledge discovery : edas , which are evolutionary algorithms that use structure learning on each generation for modeling the distribution of populations . the experiments show that when ibmap-hc is used to learn the structure , edas improve the convergence to the optimum ."}
{"title": "aligned image-word representations improve inductive transfer across vision-language tasks", "abstract": "an important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks . in this paper , we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning . in particular , the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings . we show this leads to greater inductive transfer from recognition to vqa than standard multitask learning . visual recognition also improves , especially for categories that have relatively few recognition training labels but appear often in the vqa setting . thus , our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable , flexible , and trainable core representations ."}
{"title": "speaker identification in each of the neutral and shouted talking environments based on gender-dependent approach using sphmms", "abstract": "it is well known that speaker identification performs extremely well in the neutral talking environments ; however , the identification performance is declined sharply in the shouted talking environments . this work aims at proposing , implementing and testing a new approach to enhance the declined performance in the shouted talking environments . the new proposed approach is based on gender-dependent speaker identification using suprasegmental hidden markov models ( sphmms ) as classifiers . this proposed approach has been tested on two different and separate speech databases : our collected database and the speech under simulated and actual stress ( susas ) database . the results of this work show that gender-dependent speaker identification based on sphmms outperforms gender-independent speaker identification based on the same models and gender-dependent speaker identification based on hidden markov models ( hmms ) by about 6 % and 8 % , respectively . the results obtained based on the proposed approach are close to those obtained in subjective evaluation by human judges ."}
{"title": "eight maximal tractable subclasses of allen 's algebra with metric time", "abstract": "this paper combines two important directions of research in temporal resoning : that of finding maximal tractable subclasses of allen 's interval algebra , and that of reasoning with metric temporal information . eight new maximal tractable subclasses of allen 's interval algebra are presented , some of them subsuming previously reported tractable algebras . the algebras allow for metric temporal constraints on interval starting or ending points , using the recent framework of horn dlrs . two of the algebras can express the notion of sequentiality between intervals , being the first such algebras admitting both qualitative and metric time ."}
{"title": "specifying nonspecific evidence", "abstract": "in an earlier article [ j. schubert , on nonspecific evidence , int . j. intell . syst . 8 ( 6 ) , 711-725 ( 1993 ) ] we established within dempster-shafer theory a criterion function called the metaconflict function . with this criterion we can partition into subsets a set of several pieces of evidence with propositions that are weakly specified in the sense that it may be uncertain to which event a proposition is referring . each subset in the partitioning is representing a separate event . the metaconflict function was derived as the plausibility that the partitioning is correct when viewing the conflict in dempster 's rule within each subset as a newly constructed piece of metalevel evidence with a proposition giving support against the entire partitioning . in this article we extend the results of the previous article . we will not only find the most plausible subset for each piece of evidence as was done in the earlier article . in addition we will specify each piece of nonspecific evidence , in the sense that we find to which events the proposition might be referring , by finding the plausibility for every subset that this piece of evidence belong to the subset . in doing this we will automatically receive indication that some evidence might be false . we will then develop a new methodology to exploit these newly specified pieces of evidence in a subsequent reasoning process . this will include methods to discount evidence based on their degree of falsity and on their degree of credibility due to a partial specification of affiliation , as well as a refined method to infer the event of each subset ."}
{"title": "determining health utilities through data mining of social media", "abstract": "'health utilities ' measure patient preferences for perfect health compared to specific unhealthy states , such as asthma , a fractured hip , or colon cancer . when integrated over time , these estimations are called quality adjusted life years ( qalys ) . until now , characterizing health utilities ( hus ) required detailed patient interviews or written surveys . while reliable and specific , this data remained costly due to efforts to locate , enlist and coordinate participants . thus the scope , context and temporality of diseases examined has remained limited . now that more than a billion people use social media , we propose a novel strategy : use natural language processing to analyze public online conversations for signals of the severity of medical conditions and correlate these to known hus using machine learning . in this work , we filter a dataset that originally contained 2 billion tweets for relevant content on 60 diseases . using this data , our algorithm successfully distinguished mild from severe diseases , which had previously been categorized only by traditional techniques . this represents progress towards two related applications : first , predicting hus where such information is nonexistent ; and second , ( where rich hu data already exists ) estimating temporal or geographic patterns of disease severity through data mining ."}
{"title": "multi-label annotation aggregation in crowdsourcing", "abstract": "as a means of human-based computation , crowdsourcing has been widely used to annotate large-scale unlabeled datasets . one of the obvious challenges is how to aggregate these possibly noisy labels provided by a set of heterogeneous annotators . another challenge stems from the difficulty in evaluating the annotator reliability without even knowing the ground truth , which can be used to build incentive mechanisms in crowdsourcing platforms . when each instance is associated with many possible labels simultaneously , the problem becomes even harder because of its combinatorial nature . in this paper , we present new flexible bayesian models and efficient inference algorithms for multi-label annotation aggregation by taking both annotator reliability and label dependency into account . extensive experiments on real-world datasets confirm that the proposed methods outperform other competitive alternatives , and the model can recover the type of the annotators with high accuracy . besides , we empirically find that the mixture of multiple independent bernoulli distribution is able to accurately capture label dependency in this unsupervised multi-label annotation aggregation scenario ."}
{"title": "on the application of hierarchical coevolutionary genetic algorithms : recombination and evaluation partners", "abstract": "this paper examines the use of a hierarchical coevolutionary genetic algorithm under different partnering strategies . cascading clusters of sub-populations are built from the bottom up , with higher-level sub-populations optimising larger parts of the problem . hence higher-level sub-populations potentially search a larger search space with a lower resolution whilst lower-level sub-populations search a smaller search space with a higher resolution . the effects of different partner selection schemes amongst the sub-populations on solution quality are examined for two constrained optimisation problems . we examine a number of recombination partnering strategies in the construction of higher-level individuals and a number of related schemes for evaluating sub-solutions . it is shown that partnering strategies that exploit problem-specific knowledge are superior and can counter inappropriate ( sub ) fitness measurements ."}
{"title": "deriving and combining continuous possibility functions in the framework of evidential reasoning", "abstract": "to develop an approach to utilizing continuous statistical information within the dempster- shafer framework , we combine methods proposed by strat and by shafero we first derive continuous possibility and mass functions from probability-density functions . then we propose a rule for combining such evidence that is simpler and more efficiently computed than dempster 's rule . we discuss the relationship between dempster 's rule and our proposed rule for combining evidence over continuous frames ."}
{"title": "a maximal tractable class of soft constraints", "abstract": "many researchers in artificial intelligence are beginning to explore the use of soft constraints to express a set of ( possibly conflicting ) problem requirements . a soft constraint is a function defined on a collection of variables which associates some measure of desirability with each possible combination of values for those variables . however , the crucial question of the computational complexity of finding the optimal solution to a collection of soft constraints has so far received very little attention . in this paper we identify a class of soft binary constraints for which the problem of finding the optimal solution is tractable . in other words , we show that for any given set of such constraints , there exists a polynomial time algorithm to determine the assignment having the best overall combined measure of desirability . this tractable class includes many commonly-occurring soft constraints , such as 'as near as possible ' or 'as soon as possible after ' , as well as crisp constraints such as 'greater than ' . finally , we show that this tractable class is maximal , in the sense that adding any other form of soft binary constraint which is not in the class gives rise to a class of problems which is np-hard ."}
{"title": "eigenoption discovery through the deep successor representation", "abstract": "options in reinforcement learning allow agents to hierarchically decompose a task into subtasks , having the potential to speed up learning and planning . however , autonomously learning effective sets of options is still a major challenge in the field . in this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process . specifically , we look at eigenoptions , options obtained from representations that encode diffusive information flow in the environment . we extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available . we propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels . it exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation . we use traditional tabular domains to provide intuition about our approach and atari 2600 games to demonstrate its potential ."}
{"title": "eliciting implicit assumptions of proofs in the mizar mathematical library by property omission", "abstract": "when formalizing proofs with interactive theorem provers , it often happens that extra background knowledge ( declarative or procedural ) about mathematical concepts is employed without the formalizer explicitly invoking it , to help the formalizer focus on the relevant details of the proof . in the contexts of producing and studying a formalized mathematical argument , such mechanisms are clearly valuable . but we may not always wish to suppress background knowledge . for certain purposes , it is important to know , as far as possible , precisely what background knowledge was implicitly employed in a formal proof . in this note we describe an experiment conducted on the mizar mathematical library of formal mathematical proofs to elicit one such class of implicitly employed background knowledge : properties of functions and relations ( e.g. , commutativity , asymmetry , etc . ) ."}
{"title": "task-based end-to-end model learning in stochastic optimization", "abstract": "with the increasing popularity of machine learning techniques , it has become common to see prediction algorithms operating within some larger process . however , the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them . this paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used , within the context of stochastic programming . we present three experimental evaluations of the proposed approach : a classical inventory stock problem , a real-world electrical grid scheduling task , and a real-world energy storage arbitrage task . we show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications ."}
{"title": "distributed self management for distributed security systems", "abstract": "distributed system as e.g . artificial immune systems , complex adaptive systems , or multi-agent systems are widely used in computer science , e.g . for network security , optimisations , or simulations . in these systems , small entities move through the network and perform certain tasks . at some time , the entities move to another place and require therefore information where to move is most profitable . common used systems do not provide any information or use a centralised approach where a center delegates the entities . this article discusses whether small information about the neighbours enhances the performance of the overall system or not . therefore , two information-protocols are introduced and analysed . in addition , the protocols are implemented and tested using the artificial immune system sana that protects a network against intrusions ."}
{"title": "sat-based preprocessing for maxsat ( extended version )", "abstract": "state-of-the-art algorithms for industrial instances of maxsat problem rely on iterative calls to a sat solver . preprocessing is crucial for the acceleration of sat solving , and the key preprocessing techniques rely on the application of resolution and subsumption elimination . additionally , satisfiability-preserving clause elimination procedures are often used . since maxsat computation typically involves a large number of sat calls , we are interested in whether an input instance to a maxsat problem can be preprocessed up-front , i.e . prior to running the maxsat solver , rather than ( or , in addition to ) during each iterative sat solver call . the key requirement in this setting is that the preprocessing has to be sound , i.e . so that the solution can be reconstructed correctly and efficiently after the execution of a maxsat algorithm on the preprocessed instance . while , as we demonstrate in this paper , certain clause elimination procedures are sound for maxsat , it is well-known that this is not the case for resolution and subsumption elimination . in this paper we show how to adapt these preprocessing techniques to maxsat . to achieve this we recast the maxsat problem in a recently introduced labelled-cnf framework , and show that within the framework the preprocessing techniques can be applied soundly . furthermore , we show that maxsat algorithms restated in the framework have a natural implementation on top of an incremental sat solver . we evaluate the prototype implementation of a maxsat algorithm wmsu1 in this setting , demonstrate the effectiveness of preprocessing , and show overall improvement with respect to non-incremental versions of the algorithm on some classes of problems ."}
{"title": "physics informed deep learning ( part i ) : data-driven solutions of nonlinear partial differential equations", "abstract": "we introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations . in this two part treatise , we present our developments in the context of solving two main classes of problems : data-driven solution and data-driven discovery of partial differential equations . depending on the nature and arrangement of the available data , we devise two distinct classes of algorithms , namely continuous time and discrete time models . the resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information . in this first part , we demonstrate how these networks can be used to infer solutions to partial differential equations , and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters ."}
{"title": "modelling constraint solver architecture design as a constraint problem", "abstract": "designing component-based constraint solvers is a complex problem . some components are required , some are optional and there are interdependencies between the components . because of this , previous approaches to solver design and modification have been ad-hoc and limited . we present a system that transforms a description of the components and the characteristics of the target constraint solver into a constraint problem . solving this problem yields the description of a valid solver . our approach represents a significant step towards the automated design and synthesis of constraint solvers that are specialised for individual constraint problem classes or instances ."}
{"title": "applying the wizard-of-oz technique to multimodal human-robot dialogue", "abstract": "our overall program objective is to provide more natural ways for soldiers to interact and communicate with robots , much like how soldiers communicate with other soldiers today . we describe how the wizard-of-oz ( woz ) method can be applied to multimodal human-robot dialogue in a collaborative exploration task . while the woz method can help design robot behaviors , traditional approaches place the burden of decisions on a single wizard . in this work , we consider two wizards to stand in for robot navigation and dialogue management software components . the scenario used to elicit data is one in which a human-robot team is tasked with exploring an unknown environment : a human gives verbal instructions from a remote location and the robot follows them , clarifying possible misunderstandings as needed via dialogue . we found the division of labor between wizards to be workable , which holds promise for future software development ."}
{"title": "quizz : targeted crowdsourcing with a billion ( potential ) users", "abstract": "we describe quizz , a gamified crowdsourcing system that simultaneously assesses the knowledge of users and acquires new knowledge from them . quizz operates by asking users to complete short quizzes on specific topics ; as a user answers the quiz questions , quizz estimates the user 's competence . to acquire new knowledge , quizz also incorporates questions for which we do not have a known answer ; the answers given by competent users provide useful signals for selecting the correct answers for these questions . quizz actively tries to identify knowledgeable users on the internet by running advertising campaigns , effectively leveraging the targeting capabilities of existing , publicly available , ad placement services . quizz quantifies the contributions of the users using information theory and sends feedback to the advertisingsystem about each user . the feedback allows the ad targeting mechanism to further optimize ad placement . our experiments , which involve over ten thousand users , confirm that we can crowdsource knowledge curation for niche and specialized topics , as the advertising network can automatically identify users with the desired expertise and interest in the given topic . we present controlled experiments that examine the effect of various incentive mechanisms , highlighting the need for having short-term rewards as goals , which incentivize the users to contribute . finally , our cost-quality analysis indicates that the cost of our approach is below that of hiring workers through paid-crowdsourcing platforms , while offering the additional advantage of giving access to billions of potential users all over the planet , and being able to reach users with specialized expertise that is not typically available through existing labor marketplaces ."}
{"title": "syntax-based default reasoning as probabilistic model-based diagnosis", "abstract": "we view the syntax-based approaches to default reasoning as a model-based diagnosis problem , where each source giving a piece of information is considered as a component . it is formalized in the atms framework ( each source corresponds to an assumption ) . we assume then that all sources are independent and `` fail '' with a very small probability . this leads to a probability assignment on the set of candidates , or equivalently on the set of consistent environments . this probability assignment induces a dempster-shafer belief function which measures the probability that a proposition can be deduced from the evidence . this belief function can be used in several different ways to define a non-monotonic consequence relation . we study and compare these consequence relations . the -case of prioritized knowledge bases is briefly considered ."}
{"title": "accelerating reinforcement learning by composing solutions of automatically identified subtasks", "abstract": "this paper discusses a system that accelerates reinforcement learning by using transfer from related tasks . without such transfer , even if two tasks are very similar at some abstract level , an extensive re-learning effort is required . the system achieves much of its power by transferring parts of previously learned solutions rather than a single complete solution . the system exploits strong features in the multi-dimensional function produced by reinforcement learning in solving a particular task . these features are stable and easy to recognize early in the learning process . they generate a partitioning of the state space and thus the function . the partition is represented as a graph . this is used to index and compose functions stored in a case base to form a close approximation to the solution of the new task . experiments demonstrate that function composition often produces more than an order of magnitude increase in learning rate compared to a basic reinforcement learning algorithm ."}
{"title": "a discrete and bounded envy-free cake cutting protocol for any number of agents", "abstract": "we consider the well-studied cake cutting problem in which the goal is to find an envy-free allocation based on queries from $ n $ agents . the problem has received attention in computer science , mathematics , and economics . it has been a major open problem whether there exists a discrete and bounded envy-free protocol . we resolve the problem by proposing a discrete and bounded envy-free protocol for any number of agents . the maximum number of queries required by the protocol is $ n^ { n^ { n^ { n^ { n^n } } } } $ . we additionally show that even if we do not run our protocol to completion , it can find in at most $ n^3 { ( n^2 ) } ^n $ queries a partial allocation of the cake that achieves proportionality ( each agent gets at least $ 1/n $ of the value of the whole cake ) and envy-freeness . finally we show that an envy-free partial allocation can be computed in at most $ n^3 { ( n^2 ) } ^n $ queries such that each agent gets a connected piece that gives the agent at least $ 1/ ( 3n ) $ of the value of the whole cake ."}
{"title": "a fuzzy model for analogical problem solving", "abstract": "in this paper we develop a fuzzy model for the description of the process of analogical reasoning by representing its main steps as fuzzy subsets of a set of linguistic labels characterizing the individuals ' performance in each step and we use the shannon- wiener diversity index as a measure of the individuals ' abilities in analogical problem solving . this model is compared with a stochastic model presented in author 's earlier papers by introducing a finite markov chain on the steps of the process of analogical reasoning . a classroom experiment is also presented to illustrate the use of our results in practice ."}
{"title": "ontology for cellular communication", "abstract": "the lack of interoperability between mobile cellular access networks has long been a challenging obstacle , which telecommunication engineering is trying to overcome . in second generation networks for example , this problem lies in the fact that there are multiple standards . each of these standards can operate in the same frequency range . however , each utilizes a different radio technology and modulation scheme , which are characteristics of the standard . therefore , the lack of interoperability in 2g occurs because of the lack of standardization . interoperability within 3g networks is limited to a few operating modes using different radio transmission technologies that are not inter-operable . thus , interoperability remains an issue for 3g . 4g technology even being successful in its various trials can not guarantee the interoperability . this is within each network generation ; meanwhile between heterogeneous network generations the situation seems to be worst . this approach is first to analyze the structure , inputs , and outputs of three different cellular technologies , performing a domain analysis ( of this subset of technologies ) and producing a feature model of the domain . finally , we sought to build an ontology capable of providing a common view of the domain , providing an effective representation of relations between representations of corresponding concepts in different cellular technologies ."}
{"title": "deductive algorithmic knowledge", "abstract": "the framework of algorithmic knowledge assumes that agents use algorithms to compute the facts they explicitly know . in many cases of interest , a deductive system , rather than a particular algorithm , captures the formal reasoning used by the agents to compute what they explicitly know . we introduce a logic for reasoning about both implicit and explicit knowledge with the latter defined with respect to a deductive system formalizing a logical theory for agents . the highly structured nature of deductive systems leads to very natural axiomatizations of the resulting logic when interpreted over any fixed deductive system . the decision problem for the logic , in the presence of a single agent , is np-complete in general , no harder than propositional logic . it remains np-complete when we fix a deductive system that is decidable in nondeterministic polynomial time . these results extend in a straightforward way to multiple agents ."}
{"title": "hybrid collaborative filtering with autoencoders", "abstract": "collaborative filtering aims at exploiting the feedback of users to provide personalised recommendations . such algorithms look for latent variables in a large sparse matrix of ratings . they can be enhanced by adding side information to tackle the well-known cold start problem . while neu-ral networks have tremendous success in image and speech recognition , they have received less attention in collaborative filtering . this is all the more surprising that neural networks are able to discover latent variables in large and heterogeneous datasets . in this paper , we introduce a collaborative filtering neural network architecture aka cfn which computes a non-linear matrix factorization from sparse rating inputs and side information . we show experimentally on the movielens and douban dataset that cfn outper-forms the state of the art and benefits from side information . we provide an implementation of the algorithm as a reusable plugin for torch , a popular neural network framework ."}
{"title": "stationary probability density of stochastic search processes in global optimization", "abstract": "a method for the construction of approximate analytical expressions for the stationary marginal densities of general stochastic search processes is proposed . by the marginal densities , regions of the search space that with high probability contain the global optima can be readily defined . the density estimation procedure involves a controlled number of linear operations , with a computational cost per iteration that grows linearly with problem size ."}
{"title": "beyond turing machines", "abstract": "this paper discusses `` computational '' systems capable of `` computing '' functions not computable by predefined turing machines if the systems are not isolated from their environment . roughly speaking , these systems can change their finite descriptions by interacting with their environment ."}
{"title": "aspmt ( qs ) : non-monotonic spatial reasoning with answer set programming modulo theories", "abstract": "the systematic modelling of \\emph { dynamic spatial systems } [ 9 ] is a key requirement in a wide range of application areas such as comonsense cognitive robotics , computer-aided architecture design , dynamic geographic information systems . we present aspmt ( qs ) , a novel approach and fully-implemented prototype for non-monotonic spatial reasoning -- -a crucial requirement within dynamic spatial systems -- based on answer set programming modulo theories ( aspmt ) . aspmt ( qs ) consists of a ( qualitative ) spatial representation module ( qs ) and a method for turning tight aspmt instances into sat modulo theories ( smt ) instances in order to compute stable models by means of smt solvers . we formalise and implement concepts of default spatial reasoning and spatial frame axioms using choice formulas . spatial reasoning is performed by encoding spatial relations as systems of polynomial constraints , and solving via smt with the theory of real nonlinear arithmetic . we empirically evaluate aspmt ( qs ) in comparison with other prominent contemporary spatial reasoning systems . our results show that aspmt ( qs ) is the only existing system that is capable of reasoning about indirect spatial effects ( i.e . addressing the ramification problem ) , and integrating geometric and qualitative spatial information within a non-monotonic spatial reasoning context ."}
{"title": "chameleon : a hybrid secure computation framework for machine learning applications", "abstract": "we present chameleon , a novel hybrid ( mixed-protocol ) framework for secure function evaluation ( sfe ) which enables two parties to jointly compute a function without disclosing their private inputs . chameleon combines the best aspects of generic sfe protocols with the ones that are based upon additive secret sharing . in particular , the framework performs linear operations in the ring $ \\mathbb { z } _ { 2^l } $ using additively secret shared values and nonlinear operations using yao 's garbled circuits or the goldreich-micali-wigderson protocol . chameleon departs from the common assumption of additive or linear secret sharing models where three or more parties need to communicate in the online phase : the framework allows two parties with private inputs to communicate in the online phase under the assumption of a third node generating correlated randomness in an offline phase . almost all of the heavy cryptographic operations are precomputed in an offline phase which substantially reduces the communication overhead . chameleon is both scalable and significantly more efficient than the aby framework ( ndss'15 ) it is based on . our framework supports signed fixed-point numbers . in particular , chameleon 's vector dot product of signed fixed-point numbers improves the efficiency of mining and classification of encrypted data for algorithms based upon heavy matrix multiplications . our evaluation of chameleon on a 5 layer convolutional deep neural network shows 133x and 4.2x faster executions than microsoft cryptonets ( icml'16 ) and minionn ( ccs'17 ) , respectively ."}
{"title": "soft constraint programming to analysing security protocols", "abstract": "security protocols stipulate how the remote principals of a computer network should interact in order to obtain specific security goals . the crucial goals of confidentiality and authentication may be achieved in various forms , each of different strength . using soft ( rather than crisp ) constraints , we develop a uniform formal notion for the two goals . they are no longer formalised as mere yes/no properties as in the existing literature , but gain an extra parameter , the security level . for example , different messages can enjoy different levels of confidentiality , or a principal can achieve different levels of authentication with different principals . the goals are formalised within a general framework for protocol analysis that is amenable to mechanisation by model checking . following the application of the framework to analysing the asymmetric needham-schroeder protocol , we have recently discovered a new attack on that protocol as a form of retaliation by principals who have been attacked previously . having commented on that attack , we then demonstrate the framework on a bigger , largely deployed protocol consisting of three phases , kerberos ."}
{"title": "an application of the generalized rectangular fuzzy model to critical thinking assessment", "abstract": "the authors apply the generalized rectangular model to assessing critical thinking skills and its relations with their language competency ."}
{"title": "agm-style revision of beliefs and intentions from a database perspective ( preliminary version )", "abstract": "we introduce a logic for temporal beliefs and intentions based on shoham 's database perspective . we separate strong beliefs from weak beliefs . strong beliefs are independent from intentions , while weak beliefs are obtained by adding intentions to strong beliefs and everything that follows from that . we formalize coherence conditions on strong beliefs and intentions . we provide agm-style postulates for the revision of strong beliefs and intentions . we show in a representation theorem that a revision operator satisfying our postulates can be represented by a pre-order on interpretations of the beliefs , together with a selection function for the intentions ."}
{"title": "neural network ensembles : evaluation of aggregation algorithms", "abstract": "ensembles of artificial neural networks show improved generalization capabilities that outperform those of single networks . however , for aggregation to be effective , the individual networks must be as accurate and diverse as possible . an important problem is , then , how to tune the aggregate members in order to have an optimal compromise between these two conflicting conditions . we present here an extensive evaluation of several algorithms for ensemble construction , including new proposals and comparing them with standard methods in the literature . we also discuss a potential problem with sequential aggregation algorithms : the non-frequent but damaging selection through their heuristics of particularly bad ensemble members . we introduce modified algorithms that cope with this problem by allowing individual weighting of aggregate members . our algorithms and their weighted modifications are favorably tested against other methods in the literature , producing a sensible improvement in performance on most of the standard statistical databases used as benchmarks ."}
{"title": "a formal methods approach to pattern synthesis in reaction diffusion systems", "abstract": "we propose a technique to detect and generate patterns in a network of locally interacting dynamical systems . central to our approach is a novel spatial superposition logic , whose semantics is defined over the quad-tree of a partitioned image . we show that formulas in this logic can be efficiently learned from positive and negative examples of several types of patterns . we also demonstrate that pattern detection , which is implemented as a model checking algorithm , performs very well for test data sets different from the learning sets . we define a quantitative semantics for the logic and integrate the model checking algorithm with particle swarm optimization in a computational framework for synthesis of parameters leading to desired patterns in reaction-diffusion systems ."}
{"title": "recurrent network-based deterministic policy gradient for solving bipedal walking challenge on rugged terrains", "abstract": "this paper presents the learning algorithm based on the recurrent network-based deterministic policy gradient . the long-short term memory is utilized to enable the partially observed markov decision process framework . the novelty are improvements of lstm networks : update of multi-step temporal difference , removal of backpropagation through time on actor , initialisation of hidden state using past trajectory scanning , and injection of external experiences learned by other agents . our methods benefit the reinforcement learning agent on inferring the desirable action by referring the trajectories of both past observations and actions . the proposed algorithm was implemented to solve the bipedal-walker challenge in openai virtual environment where only partial state information is available . the validation on the extremely rugged terrain demonstrates the effectiveness of the proposed algorithm by achieving a new record of highest rewards in the challenge . the autonomous behaviors generated by our agent are highly adaptive to a variety of obstacles as shown in the simulation results ."}
{"title": "learning chordal markov networks by constraint satisfaction", "abstract": "we investigate the problem of learning the structure of a markov network from data . it is shown that the structure of such networks can be described in terms of constraints which enables the use of existing solver technology with optimization capabilities to compute optimal networks starting from initial scores computed from the data . to achieve efficient encodings , we develop a novel characterization of markov network structure using a balancing condition on the separators between cliques forming the network . the resulting translations into propositional satisfiability and its extensions such as maximum satisfiability , satisfiability modulo theories , and answer set programming , enable us to prove optimal certain network structures which have been previously found by stochastic search ."}
{"title": "role of deep lstm neural networks and wifi networks in support of occupancy prediction in smart buildings", "abstract": "knowing how many people occupy a building , and where they are located , is a key component of smart building services . commercial , industrial and residential buildings often incorporate systems used to determine occupancy . however , relatively simple sensor technology and control algorithms limit the effectiveness of smart building services . in this paper we propose to replace sensor technology with time series models that can predict the number of occupants at a given location and time . we use wi-fi data sets readily available in abundance for smart building services and train auto regression integrating moving average ( arima ) models and long short-term memory ( lstm ) time series models . as a use case scenario of smart building services , these models allow forecasting of the number of people at a given time and location in 15 , 30 and 60 minutes time intervals at building as well as access point ( ap ) level . for lstm , we build our models in two ways : a separate model for every time scale , and a combined model for the three time scales . our experiments show that lstm combined model reduced the computational resources with respect to the number of neurons by 74.48 % for the ap level , and by 67.13 % for the building level . further , the root mean square error ( rmse ) was reduced by 88.2 % - 93.4 % for lstm in comparison to arima for the building levels models and by 80.9 % - 87 % for the ap level models ."}
{"title": "kernel feature selection via conditional covariance minimization", "abstract": "we propose a framework for feature selection that employs kernel-based measures of independence to find a subset of covariates that is maximally predictive of the response . building on past work in kernel dimension reduction , we formulate our approach as a constrained optimization problem involving the trace of the conditional covariance operator , and additionally provide some consistency results . we then demonstrate on a variety of synthetic and real data sets that our method compares favorably with other state-of-the-art algorithms ."}
{"title": "dropoutdagger : a bayesian approach to safe imitation learning", "abstract": "while imitation learning is becoming common practice in robotics , this approach often suffers from data mismatch and compounding errors . dagger is an iterative algorithm that addresses these issues by continually aggregating training data from both the expert and novice policies , but does not consider the impact of safety . we present a probabilistic extension to dagger , which uses the distribution over actions provided by the novice policy , for a given observation . our method , which we call dropoutdagger , uses dropout to train the novice as a bayesian neural network that provides insight to its confidence . using the distribution over the novice 's actions , we estimate a probabilistic measure of safety with respect to the expert action , tuned to balance exploration and exploitation . the utility of this approach is evaluated on the mujoco halfcheetah and in a simple driving experiment , demonstrating improved performance and safety compared to other dagger variants and classic imitation learning ."}
{"title": "a model approximation scheme for planning in partially observable stochastic domains", "abstract": "partially observable markov decision processes ( pomdps ) are a natural model for planning problems where effects of actions are nondeterministic and the state of the world is not completely observable . it is difficult to solve pomdps exactly . this paper proposes a new approximation scheme . the basic idea is to transform a pomdp into another one where additional information is provided by an oracle . the oracle informs the planning agent that the current state of the world is in a certain region . the transformed pomdp is consequently said to be region observable . it is easier to solve than the original pomdp . we propose to solve the transformed pomdp and use its optimal policy to construct an approximate policy for the original pomdp . by controlling the amount of additional information that the oracle provides , it is possible to find a proper tradeoff between computational time and approximation quality . in terms of algorithmic contributions , we study in details how to exploit region observability in solving the transformed pomdp . to facilitate the study , we also propose a new exact algorithm for general pomdps . the algorithm is conceptually simple and yet is significantly more efficient than all previous exact algorithms ."}
{"title": "unsupervised domain adaptation for face recognition in unlabeled videos", "abstract": "despite rapid advances in face recognition , there remains a clear gap between the performance of still image-based face recognition and video-based face recognition , due to the vast difference in visual quality between the domains and the difficulty of curating diverse large-scale video datasets . this paper addresses both of those challenges , through an image to video feature-level domain adaptation approach , to learn discriminative video frame representations . the framework utilizes large-scale unlabeled video data to reduce the gap between different domains while transferring discriminative knowledge from large-scale labeled still images . given a face recognition network that is pretrained in the image domain , the adaptation is achieved by ( i ) distilling knowledge from the network to a video adaptation network through feature matching , ( ii ) performing feature restoration through synthetic data augmentation and ( iii ) learning a domain-invariant feature through a domain adversarial discriminator . we further improve performance through a discriminator-guided feature fusion that boosts high-quality frames while eliminating those degraded by video domain-specific factors . experiments on the youtube faces and ijb-a datasets demonstrate that each module contributes to our feature-level domain adaptation framework and substantially improves video face recognition performance to achieve state-of-the-art accuracy . we demonstrate qualitatively that the network learns to suppress diverse artifacts in videos such as pose , illumination or occlusion without being explicitly trained for them ."}
{"title": "combining spatial and temporal logics : expressiveness vs. complexity", "abstract": "in this paper , we construct and investigate a hierarchy of spatio-temporal formalisms that result from various combinations of propositional spatial and temporal logics such as the propositional temporal logic ptl , the spatial logics rcc-8 , brcc-8 , s4u and their fragments . the obtained results give a clear picture of the trade-off between expressiveness and computational realisability within the hierarchy . we demonstrate how different combining principles as well as spatial and temporal primitives can produce np- , pspace- , expspace- , 2expspace-complete , and even undecidable spatio-temporal logics out of components that are at most np- or pspace-complete ."}
{"title": "concept-based recommendations for internet advertisement", "abstract": "the problem of detecting terms that can be interesting to the advertiser is considered . if a company has already bought some advertising terms which describe certain services , it is reasonable to find out the terms bought by competing companies . a part of them can be recommended as future advertising terms to the company . the goal of this work is to propose better interpretable recommendations based on fca and association rules ."}
{"title": "learning is planning : near bayes-optimal reinforcement learning via monte-carlo tree search", "abstract": "bayes-optimal behavior , while well-defined , is often difficult to achieve . recent advances in the use of monte-carlo tree search ( mcts ) have shown that it is possible to act near-optimally in markov decision processes ( mdps ) with very large or infinite state spaces . bayes-optimal behavior in an unknown mdp is equivalent to optimal behavior in the known belief-space mdp , although the size of this belief-space mdp grows exponentially with the amount of history retained , and is potentially infinite . we show how an agent can use one particular mcts algorithm , forward search sparse sampling ( fsss ) , in an efficient way to act nearly bayes-optimally for all but a polynomial number of steps , assuming that fsss can be used to act efficiently in any possible underlying mdp ."}
{"title": "comparing dataset characteristics that favor the apriori , eclat or fp-growth frequent itemset mining algorithms", "abstract": "frequent itemset mining is a popular data mining technique . apriori , eclat , and fp-growth are among the most common algorithms for frequent itemset mining . considerable research has been performed to compare the relative performance between these three algorithms , by evaluating the scalability of each algorithm as the dataset size increases . while scalability as data size increases is important , previous papers have not examined the performance impact of similarly sized datasets that contain different itemset characteristics . this paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms . to perform this empirical analysis , a dataset generator is created to measure the effects of frequent item density and the maximum transaction size on performance . the generated datasets contain the same number of rows . this provides some insight into dataset characteristics that are conducive to each algorithm . the results of this paper 's research demonstrate eclat and fp-growth both handle increases in maximum transaction size and frequent itemset density considerably better than the apriori algorithm . this paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms . to perform this empirical analysis , a dataset generator is created to measure the effects of frequent item density and the maximum transaction size on performance . the generated datasets contain the same number of rows . this provides some insight into dataset characteristics that are conducive to each algorithm . the results of this paper 's research demonstrate eclat and fp-growth both handle increases in maximum transaction size and frequent itemset density considerably better than the apriori algorithm ."}
{"title": "a visibility graph averaging aggregation operator", "abstract": "the problem of aggregation is considerable importance in many disciplines . in this paper , a new type of operator called visibility graph averaging ( vga ) aggregation operator is proposed . this proposed operator is based on the visibility graph which can convert a time series into a graph . the weights are obtained according to the importance of the data in the visibility graph . finally , the vga operator is used in the analysis of the taiex database to illustrate that it is practical and compared with the classic aggregation operators , it shows its advantage that it not only implements the aggregation of the data purely , but also conserves the time information , and meanwhile , the determination of the weights is more reasonable ."}
{"title": "automation of mathematical induction as part of the history of logic", "abstract": "we review the history of the automation of mathematical induction"}
{"title": "what does newcomb 's paradox teach us ?", "abstract": "in newcomb 's paradox you choose to receive either the contents of a particular closed box , or the contents of both that closed box and another one . before you choose , a prediction algorithm deduces your choice , and fills the two boxes based on that deduction . newcomb 's paradox is that game theory appears to provide two conflicting recommendations for what choice you should make in this scenario . we analyze newcomb 's paradox using a recent extension of game theory in which the players set conditional probability distributions in a bayes net . we show that the two game theory recommendations in newcomb 's scenario have different presumptions for what bayes net relates your choice and the algorithm 's prediction . we resolve the paradox by proving that these two bayes nets are incompatible . we also show that the accuracy of the algorithm 's prediction , the focus of much previous work , is irrelevant . in addition we show that newcomb 's scenario only provides a contradiction between game theory 's expected utility and dominance principles if one is sloppy in specifying the underlying bayes net . we also show that newcomb 's paradox is time-reversal invariant ; both the paradox and its resolution are unchanged if the algorithm makes its ` prediction ' after you make your choice rather than before ."}
{"title": "mura dataset : towards radiologist-level abnormality detection in musculoskeletal radiographs", "abstract": "we introduce mura , a large dataset of musculoskeletal radiographs containing 40,895 images from 14,982 studies , where each study is manually labeled by radiologists as either normal or abnormal . on this dataset , we train a 169-layer densely connected convolutional network to detect and localize abnormalities . to evaluate our model robustly and to get an estimate of radiologist performance , we collect additional labels from board-certified stanford radiologists on the test set , consisting of 209 musculoskeletal studies . we compared our model and radiologists on the cohen 's kappa statistic , which expresses the agreement of our model and of each radiologist with the gold standard , defined as the majority vote of a disjoint group of radiologists . we find that our model achieves performance comparable to that of radiologists . model performance is higher than the best radiologist performance in detecting abnormalities on finger studies and equivalent on wrist studies . however , model performance is lower than best radiologist performance in detecting abnormalities on elbow , forearm , hand , humerus , and shoulder studies , indicating that the task is a good challenge for future research . to encourage advances , we have made our dataset freely available at https : //stanfordmlgroup.github.io/projects/mura"}
{"title": "bound propagation", "abstract": "in this article we present an algorithm to compute bounds on the marginals of a graphical model . for several small clusters of nodes upper and lower bounds on the marginal values are computed independently of the rest of the network . the range of allowed probability distributions over the surrounding nodes is restricted using earlier computed bounds . as we will show , this can be considered as a set of constraints in a linear programming problem of which the objective function is the marginal probability of the center nodes . in this way knowledge about the maginals of neighbouring clusters is passed to other clusters thereby tightening the bounds on their marginals . we show that sharp bounds can be obtained for undirected and directed graphs that are used for practical applications , but for which exact computations are infeasible ."}
{"title": "super-polynomial and exponential improvements for quantum-enhanced reinforcement learning", "abstract": "recent work on quantum machine learning has demonstrated that quantum computers can offer dramatic improvements over classical devices for data mining , prediction and classification . however , less is known about the advantages using quantum computers may bring in the more general setting of reinforcement learning , where learning is achieved via interaction with a task environment that provides occasional rewards . reinforcement learning can incorporate data-analysis-oriented learning settings as special cases , but also includes more complex situations where , e.g. , reinforcing feedback is delayed . in a few recent works , grover-type amplification has been utilized to construct quantum agents that achieve up-to-quadratic improvements in learning efficiency . these encouraging results have left open the key question of whether super-polynomial improvements in learning times are possible for genuine reinforcement learning problems , that is problems that go beyond the other more restricted learning paradigms . in this work , we provide a family of such genuine reinforcement learning tasks . we construct quantum-enhanced learners which learn super-polynomially , and even exponentially faster than any classical reinforcement learning model , and we discuss the potential impact our results may have on future technologies ."}
{"title": "a reduction of imitation learning and structured prediction to no-regret online learning", "abstract": "sequential prediction problems such as imitation learning , where future observations depend on previous predictions ( actions ) , violate the common i.i.d . assumptions made in statistical learning . this leads to poor performance in theory and often in practice . some recent approaches provide stronger guarantees in this setting , but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations . in this paper , we propose a new iterative algorithm , which trains a stationary deterministic policy , that can be seen as a no regret algorithm in an online learning setting . we show that any such no regret algorithm , combined with additional reduction assumptions , must find a policy with good performance under the distribution of observations it induces in such sequential settings . we demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem ."}
{"title": "human action recognition system using good features and multilayer perceptron network", "abstract": "human action recognition involves the characterization of human actions through the automated analysis of video data and is integral in the development of smart computer vision systems . however , several challenges like dynamic backgrounds , camera stabilization , complex actions , occlusions etc . make action recognition in a real time and robust fashion difficult . several complex approaches exist but are computationally intensive . this paper presents a novel approach of using a combination of good features along with iterative optical flow algorithm to compute feature vectors which are classified using a multilayer perceptron ( mlp ) network . the use of multiple features for motion descriptors enhances the quality of tracking . resilient backpropagation algorithm is used for training the feedforward neural network reducing the learning time . the overall system accuracy is improved by optimizing the various parameters of the multilayer perceptron network ."}
{"title": "use of the triangular fuzzy numbers for student assessment", "abstract": "in an earlier work we have used the triangular fuzzy numbers ( tfns ) as an assessment tool of student skills.this approach led to an approximate linguistic characterization of the students ' overall performance , but it was not proved to be sufficient in all cases for comparing the performance of two different student groups , since tywo tfns are not always comparable . in the present paper we complete the above fuzzy assessment approach by presenting a defuzzification method of tfns based on the center of gravity ( cog ) technique , which enables the required comparison . in addition we extend our results by using the trapezoidal fuzzy numbers ( tpfns ) too , which are a generalization of the tfns , for student assessment and we present suitable examples illustrating our new results in practice ."}
{"title": "a hybrid lp-rpg heuristic for modelling numeric resource flows in planning", "abstract": "although the use of metric fluents is fundamental to many practical planning problems , the study of heuristics to support fully automated planners working with these fluents remains relatively unexplored . the most widely used heuristic is the relaxation of metric fluents into interval-valued variables -- - an idea first proposed a decade ago . other heuristics depend on domain encodings that supply additional information about fluents , such as capacity constraints or other resource-related annotations . a particular challenge to these approaches is in handling interactions between metric fluents that represent exchange , such as the transformation of quantities of raw materials into quantities of processed goods , or trading of money for materials . the usual relaxation of metric fluents is often very poor in these situations , since it does not recognise that resources , once spent , are no longer available to be spent again . we present a heuristic for numeric planning problems building on the propositional relaxed planning graph , but using a mathematical program for numeric reasoning . we define a class of producer -- consumer planning problems and demonstrate how the numeric constraints in these can be modelled in a mixed integer program ( mip ) . this mip is then combined with a metric relaxed planning graph ( rpg ) heuristic to produce an integrated hybrid heuristic . the mip tracks resource use more accurately than the usual relaxation , but relaxes the ordering of actions , while the rpg captures the causal propositional aspects of the problem . we discuss how these two components interact to produce a single unified heuristic and go on to explore how further numeric features of planning problems can be integrated into the mip . we show that encoding a limited subset of the propositional problem to augment the mip can yield more accurate guidance , partly by exploiting structure such as propositional landmarks and propositional resources . our results show that the use of this heuristic enhances scalability on problems where numeric resource interaction is key in finding a solution ."}
{"title": "generating optimal plans in highly-dynamic domains", "abstract": "generating optimal plans in highly dynamic environments is challenging . plans are predicated on an assumed initial state , but this state can change unexpectedly during plan generation , potentially invalidating the planning effort . in this paper we make three contributions : ( 1 ) we propose a novel algorithm for generating optimal plans in settings where frequent , unexpected events interfere with planning . it is able to quickly distinguish relevant from irrelevant state changes , and to update the existing planning search tree if necessary . ( 2 ) we argue for a new criterion for evaluating plan adaptation techniques : the relative running time compared to the `` size '' of changes . this is significant since during recovery more changes may occur that need to be recovered from subsequently , and in order for this process of repeated recovery to terminate , recovery time has to converge . ( 3 ) we show empirically that our approach can converge and find optimal plans in environments that would ordinarily defy planning due to their high dynamics ."}
{"title": "algebraic semantics of proto-transitive rough sets", "abstract": "rough sets over generalized transitive relations like proto-transitive ones had been initiated by the present author in the year 2012. subsequently , approximation of proto-transitive relations by other relations was investigated and the relation with rough approximations was developed towards constructing semantics that can handle fragments of structure . it was also proved that difference of approximations induced by some approximate relations need not induce rough structures . in this research we develop different semantics of proto transitive rough sets ( prax ) after characterizing the structure of rough objects and also develop a theory of dependence for general rough sets and use it to internalize the nelson-algebra based approximate semantics developed earlier . the theory of rough dependence initiated later by the present author is extended in the process . this monograph is reasonably self-contained and includes proofs and extensions of representation of objects that were not part of earlier papers ."}
{"title": "sub-structural niching in estimation of distribution algorithms", "abstract": "we propose a sub-structural niching method that fully exploits the problem decomposition capability of linkage-learning methods such as the estimation of distribution algorithms and concentrate on maintaining diversity at the sub-structural level . the proposed method consists of three key components : ( 1 ) problem decomposition and sub-structure identification , ( 2 ) sub-structure fitness estimation , and ( 3 ) sub-structural niche preservation . the sub-structural niching method is compared to restricted tournament selection ( rts ) -- a niching method used in hierarchical bayesian optimization algorithm -- with special emphasis on sustained preservation of multiple global solutions of a class of boundedly-difficult , additively-separable multimodal problems . the results show that sub-structural niching successfully maintains multiple global optima over large number of generations and does so with significantly less population than rts . additionally , the market share of each of the niche is much closer to the expected level in sub-structural niching when compared to rts ."}
{"title": "a novel approach for data-driven automatic site recommendation and selection", "abstract": "this paper presents a novel , generic , and automatic method for data-driven site selection . site selection is one of the most crucial and important decisions made by any company . such a decision depends on various factors of sites , including socio-economic , geographical , ecological , as well as specific requirements of companies . the existing approaches for site selection ( commonly used by economists ) are manual , subjective , and not scalable , especially to big data . the presented method for site selection is robust , efficient , scalable , and is capable of handling challenges emerging in big data . to assess the effectiveness of the presented method , it is evaluated on real data ( collected from federal statistical office of germany ) of around 200 influencing factors which are considered by economists for site selection of supermarkets in germany ( lidl , edeka , and np ) . evaluation results show that there is a big overlap ( 86.4 \\ % ) between the sites of existing supermarkets and the sites recommended by the presented method . in addition , the method also recommends many sites ( 328 ) for supermarket where a store should be opened ."}
{"title": "lpar-05 workshop : empirically successfull automated reasoning in higher-order logic ( eshol )", "abstract": "this workshop brings together practioners and researchers who are involved in the everyday aspects of logical systems based on higher-order logic . we hope to create a friendly and highly interactive setting for discussions around the following four topics . implementation and development of proof assistants based on any notion of impredicativity , automated theorem proving tools for higher-order logic reasoning systems , logical framework technology for the representation of proofs in higher-order logic , formal digital libraries for storing , maintaining and querying databases of proofs . we envision attendees that are interested in fostering the development and visibility of reasoning systems for higher-order logics . we are particularly interested in a discusssion on the development of a higher-order version of the tptp and in comparisons of the practical strengths of automated higher-order reasoning systems . additionally , the workshop includes system demonstrations . eshol is the successor of the escar and esfor workshops held at cade 2005 and ijcar 2004 ."}
{"title": "geometric lattice structure of covering and its application to attribute reduction through matroids", "abstract": "the reduction of covering decision systems is an important problem in data mining , and covering-based rough sets serve as an efficient technique to process the problem . geometric lattices have been widely used in many fields , especially greedy algorithm design which plays an important role in the reduction problems . therefore , it is meaningful to combine coverings with geometric lattices to solve the optimization problems . in this paper , we obtain geometric lattices from coverings through matroids and then apply them to the issue of attribute reduction . first , a geometric lattice structure of a covering is constructed through transversal matroids . then its atoms are studied and used to describe the lattice . second , considering that all the closed sets of a finite matroid form a geometric lattice , we propose a dependence space through matroids and study the attribute reduction issues of the space , which realizes the application of geometric lattices to attribute reduction . furthermore , a special type of information system is taken as an example to illustrate the application . in a word , this work points out an interesting view , namely , geometric lattice to study the attribute reduction issues of information systems ."}
{"title": "learning overcomplete hmms", "abstract": "we study the problem of learning overcomplete hmms -- -those that have many hidden states but a small output alphabet . despite having significant practical importance , such hmms are poorly understood with no known positive or negative results for efficient learning . in this paper , we present several new results -- -both positive and negative -- -which help define the boundaries between the tractable and intractable settings . specifically , we show positive results for a large subclass of hmms whose transition matrices are sparse , well-conditioned , and have small probability mass on short cycles . on the other hand , we show that learning is impossible given only a polynomial number of samples for hmms with a small output alphabet and whose transition matrices are random regular graphs with large degree . we also discuss these results in the context of learning hmms which can capture long-term dependencies ."}
{"title": "label-free supervision of neural networks with physics and domain knowledge", "abstract": "in many machine learning applications , labeled data is scarce and obtaining more labels is expensive . we introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space , rather than direct examples of input-output pairs . these constraints are derived from prior domain knowledge , e.g. , from known laws of physics . we demonstrate the effectiveness of this approach on real world and simulated computer vision tasks . we are able to train a convolutional neural network to detect and track objects without any labeled examples . our approach can significantly reduce the need for labeled training data , but introduces new challenges for encoding prior knowledge into appropriate loss functions ."}
{"title": "fast k-nearest neighbour search via dynamic continuous indexing", "abstract": "existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality . we argue this is caused in part by inherent deficiencies of space partitioning , which is the underlying strategy used by most existing methods . we devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset . the proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis , automatically adapts to variations in data density , supports dynamic updates to the dataset and is easy-to-implement . we show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing ( lsh ) in terms of approximation quality , speed and space efficiency ."}
{"title": "real-time anomaly detection for streaming analytics", "abstract": "much of the worlds data is streaming , time-series data , where anomalies give significant information in critical situations . yet detecting anomalies in streaming data is a difficult task , requiring detectors to process data in real-time , and learn while simultaneously making predictions . we present a novel anomaly detection technique based on an on-line sequence memory algorithm called hierarchical temporal memory ( htm ) . we show results from a live application that detects anomalies in financial metrics in real-time . we also test the algorithm on nab , a published benchmark for real-time anomaly detection , where our algorithm achieves best-in-class results ."}
{"title": "meta-reinforcement learning of structured exploration strategies", "abstract": "exploration is a fundamental challenge in reinforcement learning ( rl ) . many of the current exploration methods for deep rl use task-agnostic objectives , such as information gain or bonuses based on state visitation . however , many practical applications of rl involve learning more than a single task , and prior tasks can be used to inform how exploration should be performed in new tasks . in this work , we explore how prior tasks can inform an agent about how to explore effectively in new situations . we introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise ( maesn ) -- to learn exploration strategies from prior experience . the prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy , producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise . we show that maesn is more effective at learning exploration strategies when compared to prior meta-rl methods , rl without learned exploration strategies , and task-agnostic exploration methods . we evaluate our method on a variety of simulated tasks : locomotion with a wheeled robot , locomotion with a quadrupedal walker , and object manipulation ."}
{"title": "3d object dense reconstruction from a single depth view", "abstract": "in this paper , we propose a novel approach , 3d-recgan++ , which reconstructs the complete 3d structure of a given object from a single arbitrary depth view using generative adversarial networks . unlike existing work which typically requires multiple views of the same object or class labels to recover the full 3d geometry , the proposed 3d-recgan++ only takes the voxel grid representation of a depth view of the object as input , and is able to generate the complete 3d occupancy grid with a high resolution of 256^3 by recovering the occluded/missing regions . the key idea is to combine the generative capabilities of autoencoders and the conditional generative adversarial networks ( gan ) framework , to infer accurate and fine-grained 3d structures of objects in high-dimensional voxel space . extensive experiments on large synthetic datasets and real-world kinect datasets show that the proposed 3d-recgan++ significantly outperforms the state of the art in single view 3d object reconstruction , and is able to reconstruct unseen types of objects ."}
{"title": "a study of associative evidential reasoning", "abstract": "evidential reasoning is cast as the problem of simplifying the evidence-hypothesis relation and constructing combination formulas that possess certain testable properties . important classes of evidence as identifiers , annihilators , and idempotents and their roles in determining binary operations on intervals of reals are discussed . the appropriate way of constructing formulas for combining evidence and their limitations , for instance , in robustness , are presented ."}
{"title": "'indifference ' methods for managing agent rewards", "abstract": "` indifference ' refers to a class of methods that are used to control a reward based agent . these methods of control work even if the implications of the agent 's reward are otherwise not fully understood . though they all come out of similar ideas , indifference techniques can be classified as way of achieving one or more of three distinct goals : rewards dependent on certain events ( with no motivation for the agent to manipulate the probability of those events ) , effective disbelief that an event will ever occur , and seamless transition from one behaviour to another . this paper analyses methods of achieving these goals in the pomdp setting , and establishes their uses , strengths , and limitations . it aims to make the tools of indifference generally accessible and usable to agent designers ."}
{"title": "evolution and the structure of learning agents", "abstract": "this paper presents the thesis that all learning agents of finite information size are limited by their informational structure in what goals they can efficiently learn to achieve in a complex environment . evolutionary change is critical for creating the required structure for all learning agents in any complex environment . the thesis implies that there is no efficient universal learning algorithm . an agent can go past the learning limits imposed by its structure only by slow evolutionary change or blind search which in a very complex environment can only give an agent an inefficient universal learning capability that can work only in evolutionary timescales or improbable luck ."}
{"title": "submodular hamming metrics", "abstract": "we show that there is a largely unexplored class of functions ( positive polymatroids ) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over . by exploiting submodularity , we are able to give hardness results and approximation algorithms for optimizing over such metrics . additionally , we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task ( a form of clustering ) and also a metric maximization task ( generating diverse k-best lists ) ."}
{"title": "a description logic primer", "abstract": "this paper provides a self-contained first introduction to description logics ( dls ) . the main concepts and features are explained with examples before syntax and semantics of the dl sroiq are defined in detail . additional sections review light-weight dl languages , discuss the relationship to the web ontology language owl and give pointers to further reading ."}
{"title": "region-based incremental pruning for pomdps", "abstract": "we present a major improvement to the incremental pruning algorithm for solving partially observable markov decision processes . our technique targets the cross-sum step of the dynamic programming ( dp ) update , a key source of complexity in pomdp algorithms . instead of reasoning about the whole belief space when pruning the cross-sums , our algorithm divides the belief space into smaller regions and performs independent pruning in each region . we evaluate the benefits of the new technique both analytically and experimentally , and show that it produces very significant performance gains . the results contribute to the scalability of pomdp algorithms to domains that can not be handled by the best existing techniques ."}
{"title": "achievements in answer set programming ( preliminary report )", "abstract": "this paper describes an approach to the methodology of answer set programming ( asp ) that can facilitate the design of encodings that are easy to understand and provably correct . under this approach , after appending a rule or a small group of rules to the emerging program we include a comment that states what has been `` achieved '' so far . this strategy allows us to set out our understanding of the design of the program by describing the roles of small parts of the program in a mathematically precise way ."}
{"title": "an agent based classification model", "abstract": "the major function of this model is to access the uci wisconsin breast can- cer data-set [ 1 ] and classify the data items into two categories , which are normal and anomalous . this kind of classifi cation can be referred as anomaly detection , which discriminates anomalous behaviour from normal behaviour in computer systems . one popular solution for anomaly detection is artifi cial immune sys- tems ( ais ) . ais are adaptive systems inspired by theoretical immunology and observed immune functions , principles and models which are applied to prob- lem solving . the dendritic cell algorithm ( dca ) [ 2 ] is an ais algorithm that is developed specifi cally for anomaly detection . it has been successfully applied to intrusion detection in computer security . it is believed that agent-based mod- elling is an ideal approach for implementing ais , as intelligent agents could be the perfect representations of immune entities in ais . this model evaluates the feasibility of re-implementing the dca in an agent-based simulation environ- ment called anylogic , where the immune entities in the dca are represented by intelligent agents . if this model can be successfully implemented , it makes it possible to implement more complicated and adaptive ais models in the agent-based simulation environment ."}
{"title": "bin completion algorithms for multicontainer packing , knapsack , and covering problems", "abstract": "many combinatorial optimization problems such as the bin packing and multiple knapsack problems involve assigning a set of discrete objects to multiple containers . these problems can be used to model task and resource allocation problems in multi-agent systems and distributed systms , and can also be found as subproblems of scheduling problems . we propose bin completion , a branch-and-bound strategy for one-dimensional , multicontainer packing problems . bin completion combines a bin-oriented search space with a powerful dominance criterion that enables us to prune much of the space . the performance of the basic bin completion framework can be enhanced by using a number of extensions , including nogood-based pruning techniques that allow further exploitation of the dominance criterion . bin completion is applied to four problems : multiple knapsack , bin covering , min-cost covering , and bin packing . we show that our bin completion algorithms yield new , state-of-the-art results for the multiple knapsack , bin covering , and min-cost covering problems , outperforming previous algorithms by several orders of magnitude with respect to runtime on some classes of hard , random problem instances . for the bin packing problem , we demonstrate significant improvements compared to most previous results , but show that bin completion is not competitive with current state-of-the-art cutting-stock based approaches ."}
{"title": "cautious propagation in bayesian networks", "abstract": "consider the situation where some evidence e has been entered to a bayesian network . when performing conflict analysis , sensitivity analysis , or when answering questions like `` what if the finding on x had been y instead of x ? '' you need probabilities p ( e'| h ) , where e ' is a subset of e , and h is a configuration of a ( possibly empty ) set of variables . cautious propagation is a modification of hugin propagation into a shafer-shenoy-like architecture . it is less efficient than hugin propagation ; however , it provides easy access to p ( e'| h ) for a great deal of relevant subsets e ' ."}
{"title": "ontology-driven information extraction", "abstract": "homogeneous unstructured data ( hud ) are collections of unstructured documents that share common properties , such as similar layout , common file format , or common domain of values . building on such properties , it would be desirable to automatically process hud to access the main information through a semantic layer -- typically an ontology -- called semantic view . hence , we propose an ontology-based approach for extracting semantically rich information from hud , by integrating and extending recent technologies and results from the fields of classical information extraction , table recognition , ontologies , text annotation , and logic programming . moreover , we design and implement a system , named knowrex , that has been successfully applied to curriculum vitae in the europass style to offer a semantic view of them , and be able , for example , to select those which exhibit required skills ."}
{"title": "an indirect genetic algorithm for set covering problems", "abstract": "this paper presents a new type of genetic algorithm for the set covering problem . it differs from previous evolutionary approaches first because it is an indirect algorithm , i.e . the actual solutions are found by an external decoder function . the genetic algorithm itself provides this decoder with permutations of the solution variables and other parameters . second , it will be shown that results can be further improved by adding another indirect optimisation layer . the decoder will not directly seek out low cost solutions but instead aims for good exploitable solutions . these are then post optimised by another hill-climbing algorithm . although seemingly more complicated , we will show that this three-stage approach has advantages in terms of solution quality , speed and adaptability to new types of problems over more direct approaches . extensive computational results are presented and compared to the latest evolutionary and other heuristic approaches to the same data instances ."}
{"title": "challenges for distributional compositional semantics", "abstract": "this paper summarises the current state-of-the art in the study of compositionality in distributional semantics , and major challenges for this area . we single out generalised quantifiers and intensional semantics as areas on which to focus attention for the development of the theory . once suitable theories have been developed , algorithms will be needed to apply the theory to tasks . evaluation is a major problem ; we single out application to recognising textual entailment and machine translation for this purpose ."}
{"title": "a vague improved markov model approach for web page prediction", "abstract": "today most of the information in all areas is available over the web . it increases the web utilization as well as attracts the interest of researchers to improve the effectiveness of web access and web utilization . as the number of web clients gets increased , the bandwidth sharing is performed that decreases the web access efficiency . web page prefetching improves the effectiveness of web access by availing the next required web page before the user demand . it is an intelligent predictive mining that analyze the user web access history and predict the next page . in this work , vague improved markov model is presented to perform the prediction . in this work , vague rules are suggested to perform the pruning at different levels of markov model . once the prediction table is generated , the association mining will be implemented to identify the most effective next page . in this paper , an integrated model is suggested to improve the prediction accuracy and effectiveness ."}
{"title": "beating atari with natural language guided reinforcement learning", "abstract": "we introduce the first deep reinforcement learning agent that learns to beat atari games with the aid of natural language instructions . the agent uses a multimodal embedding between environment observations and natural language to self-monitor progress through a list of english instructions , granting itself reward for completing instructions in addition to increasing the game score . our agent significantly outperforms deep q-networks ( dqns ) , asynchronous advantage actor-critic ( a3c ) agents , and the best agents posted to openai gym on what is often considered the hardest atari 2600 environment : montezuma 's revenge ."}
{"title": "diversifying sparsity using variational determinantal point processes", "abstract": "we propose a novel diverse feature selection method based on determinantal point processes ( dpps ) . our model enables one to flexibly define diversity based on the covariance of features ( similar to orthogonal matching pursuit ) or alternatively based on side information . we introduce our approach in the context of bayesian sparse regression , employing a dpp as a variational approximation to the true spike and slab posterior distribution . we subsequently show how this variational dpp approximation generalizes and extends mean-field approximation , and can be learned efficiently by exploiting the fast sampling properties of dpps . our motivating application comes from bioinformatics , where we aim to identify a diverse set of genes whose expression profiles predict a tumor type where the diversity is defined with respect to a gene-gene interaction network . we also explore an application in spatial statistics . in both cases , we demonstrate that the proposed method yields significantly more diverse feature sets than classic sparse methods , without compromising accuracy ."}
{"title": "optimal stochastic delivery planning in full-truckload and less-than-truckload delivery", "abstract": "with an increasing demand from emerging logistics businesses , vehicle routing problem with private fleet and common carrier ( vrppc ) has been introduced to manage package delivery services from a supplier to customers . however , almost all of existing studies focus on the deterministic problem that assumes all parameters are known perfectly at the time when the planning and routing decisions are made . in reality , some parameters are random and unknown . therefore , in this paper , we consider vrppc with hard time windows and random demand , called optimal delivery planning ( odp ) . the proposed odp aims to minimize the total package delivery cost while meeting the customer time window constraints . we use stochastic integer programming to formulate the optimization problem incorporating the customer demand uncertainty . moreover , we evaluate the performance of the odp using test data from benchmark dataset and from actual singapore road map ."}
{"title": "binary matrix guessing problem", "abstract": "we introduce the binary matrix guessing problem and provide two algorithms to solve this problem . the first algorithm we introduce is elementwise probing algorithm ( epa ) which is very fast under a score which utilizes frobenius distance . the second algorithm is additive reinforcement learning algorithm which combines ideas from perceptron algorithm and reinforcement learning algorithm . this algorithm is significantly slower compared to first one , but less restrictive and generalizes better . we compare computational performance of both algorithms and provide numerical results ."}
{"title": "web-star : towards a visual web-based ide for a story comprehension system", "abstract": "in this work , we present web-star , an online platform for story understanding built on top of the star ( story comprehension through argumentation ) reasoning engine . this platform includes a web-based ide , integration with the star system and a web service infrastructure to support integration with other systems that rely on story understanding functionality to complete their tasks . the platform also delivers a number of `` social '' features like public story sharing with a built-in commenting system , a public repository for sharing stories with the community and collaboration tools that can be used from both project team members for development and educators for teaching . moreover , we discuss the ongoing work on adding new features and functionality to this platform ."}
{"title": "optnet : differentiable optimization as a layer in neural networks", "abstract": "this paper presents optnet , a network architecture that integrates optimization problems ( here , specifically in the form of quadratic programs ) as individual layers in larger end-to-end trainable deep networks . these layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often can not capture . in this paper , we explore the foundations for such an architecture : we show how techniques from sensitivity analysis , bilevel optimization , and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters ; we develop a highly efficient solver for these layers that exploits fast gpu-based batch solves within a primal-dual interior point method , and which provides backpropagation gradients with virtually no additional cost on top of the solve ; and we highlight the application of these approaches in several problems . in one notable example , we show that the method is capable of learning to play mini-sudoku ( 4x4 ) given just input and output games , with no a priori information about the rules of the game ; this highlights the ability of our architecture to learn hard constraints better than other neural architectures ."}
{"title": "a fuzzy logic system to analyze a student 's lifestyle", "abstract": "a college student 's life can be primarily categorized into domains such as education , health , social and other activities which may include daily chores and travelling time . time management is crucial for every student . a self realisation of one 's daily time expenditure in various domains is therefore essential to maximize one 's effective output . this paper presents how a mobile application using fuzzy logic and global positioning system ( gps ) analyzes a student 's lifestyle and provides recommendations and suggestions based on the results ."}
{"title": "composite strategy for multicriteria ranking/sorting ( methodological issues , examples )", "abstract": "the paper addresses the modular design of composite solving strategies for multicriteria ranking ( sorting ) . here a 'scale of creativity ' that is close to creative levels proposed by altshuller is used as the reference viewpoint : ( i ) a basic object , ( ii ) a selected object , ( iii ) a modified object , and ( iv ) a designed object ( e.g. , composition of object components ) . these levels maybe used in various parts of decision support systems ( dss ) ( e.g. , information , operations , user ) . the paper focuses on the more creative above-mentioned level ( i.e. , composition or combinatorial synthesis ) for the operational part ( i.e. , composite solving strategy ) . this is important for a search/exploration mode of decision making process with usage of various procedures and techniques and analysis/integration of obtained results . the paper describes methodological issues of decision technology and synthesis of composite strategy for multicriteria ranking . the synthesis of composite strategies is based on 'hierarchical morphological multicriteria design ' ( hmmd ) which is based on selection and combination of design alternatives ( das ) ( here : local procedures or techniques ) while taking into account their quality and quality of their interconnections ( ic ) . a new version of hmmd with interval multiset estimates for das is used . the operational environment of dss combi for multicriteria ranking , consisting of a morphology of local procedures or techniques ( as design alternatives das ) , is examined as a basic one ."}
{"title": "from restful services to rdf : connecting the web and the semantic web", "abstract": "restful services on the web expose information through retrievable resource representations that represent self-describing descriptions of resources , and through the way how these resources are interlinked through the hyperlinks that can be found in those representations . this basic design of restful services means that for extracting the most useful information from a service , it is necessary to understand a service 's representations , which means both the semantics in terms of describing a resource , and also its semantics in terms of describing its linkage with other resources . based on the resource linking language ( rell ) , this paper describes a framework for how restful services can be described , and how these descriptions can then be used to harvest information from these services . building on this framework , a layered model of restful service semantics allows to represent a service 's information in rdf/owl . because rest is based on the linkage between resources , the same model can be used for aggregating and interlinking multiple services for extracting rdf data from sets of restful services ."}
{"title": "causal inference for cloud computing", "abstract": "cloud computing involves complex technical and economical systems and interactions . this brings about various challenges , two of which are : ( 1 ) debugging and control of computing systems , based on heterogeneous data , and ( 2 ) prediction of performance and price of `` spot '' resources , allocated via auctions . in this paper , we first establish two theoretical results on approximate causal inference . we then use the first one , approximate counterfactuals , along with established causal methodology , to outline a general framework to address ( 1 ) . to address ( 2 ) , we show how the second one , approximate integration of causal knowledge , can in principle provide a tool for cloud clients to trade off privacy against predictability of cloud costs . we report experiments on simulated and real data ."}
{"title": "adaptive branching for constraint satisfaction problems", "abstract": "the two standard branching schemes for csps are d-way and 2-way branching . although it has been shown that in theory the latter can be exponentially more effective than the former , there is a lack of empirical evidence showing such differences . to investigate this , we initially make an experimental comparison of the two branching schemes over a wide range of benchmarks . experimental results verify the theoretical gap between d-way and 2-way branching as we move from a simple variable ordering heuristic like dom to more sophisticated ones like dom/ddeg . however , perhaps surprisingly , experiments also show that when state-of-the-art variable ordering heuristics like dom/wdeg are used then d-way can be clearly more efficient than 2-way branching in many cases . motivated by this observation , we develop two generic heuristics that can be applied at certain points during search to decide whether 2-way branching or a restricted version of 2-way branching , which is close to d-way branching , will be followed . the application of these heuristics results in an adaptive branching scheme . experiments with instantiations of the two generic heuristics confirm that search with adaptive branching outperforms search with a fixed branching scheme on a wide range of problems ."}
{"title": "non-characterizability of belief revision : an application of finite model theory", "abstract": "a formal framework is given for the characterizability of a class of belief revision operators , defined using minimization over a class of partial preorders , by postulates . it is shown that for partial orders characterizability implies a definability property of the class of partial orders in monadic second-order logic . based on a non-definability result for a class of partial orders , an example is given of a non-characterizable class of revision operators . this appears to be the first non-characterizability result in belief revision ."}
{"title": "premise selection for theorem proving by deep graph embedding", "abstract": "we propose a deep learning-based approach to the problem of premise selection : selecting mathematical statements relevant for proving a given conjecture . we represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information . we then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering . our approach achieves state-of-the-art results on the holstep dataset , improving the classification accuracy from 83 % to 90.3 % ."}
{"title": "on the relation between accuracy and fairness in binary classification", "abstract": "our study revisits the problem of accuracy-fairness tradeoff in binary classification . we argue that comparison of non-discriminatory classifiers needs to account for different rates of positive predictions , otherwise conclusions about performance may be misleading , because accuracy and discrimination of naive baselines on the same dataset vary with different rates of positive predictions . we provide methodological recommendations for sound comparison of non-discriminatory classifiers , and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination ."}
{"title": "hugs : combining exact inference and gibbs sampling in junction trees", "abstract": "dawid , kjaerulff and lauritzen ( 1994 ) provided a preliminary description of a hybrid between monte-carlo sampling methods and exact local computations in junction trees . utilizing the strengths of both methods , such hybrid inference methods has the potential of expanding the class of problems which can be solved under bounded resources as well as solving problems which otherwise resist exact solutions . the paper provides a detailed description of a particular instance of such a hybrid scheme ; namely , combination of exact inference and gibbs sampling in discrete bayesian networks . we argue that this combination calls for an extension of the usual message passing scheme of ordinary junction trees ."}
{"title": "using abduction in markov logic networks for root cause analysis", "abstract": "it infrastructure is a crucial part in most of today 's business operations . high availability and reliability , and short response times to outages are essential . thus a high amount of tool support and automation in risk management is desirable to decrease outages . we propose a new approach for calculating the root cause for an observed failure in an it infrastructure . our approach is based on abduction in markov logic networks . abduction aims to find an explanation for a given observation in the light of some background knowledge . in failure diagnosis , the explanation corresponds to the root cause , the observation to the failure of a component , and the background knowledge to the dependency graph extended by potential risks . we apply a method to extend a markov logic network in order to conduct abductive reasoning , which is not naturally supported in this formalism . our approach exhibits a high amount of reusability and enables users without specific knowledge of a concrete infrastructure to gain viable insights in the case of an incident . we implemented the method in a tool and illustrate its suitability for root cause analysis by applying it to a sample scenario ."}
{"title": "dynamic global constraints : a first view", "abstract": "global constraints proved themselves to be an efficient tool for modelling and solving large-scale real-life combinatorial problems . they encapsulate a set of binary constraints and using global reasoning about this set they filter the domains of involved variables better than arc consistency among the set of binary constraints . moreover , global constraints exploit semantic information to achieve more efficient filtering than generalised consistency algorithms for n-ary constraints . continued expansion of constraint programming ( cp ) to various application areas brings new challenges for design of global constraints . in particular , application of cp to advanced planning and scheduling ( aps ) requires dynamic additions of new variables and constraints during the process of constraint satisfaction and , thus , it would be helpful if the global constraints could adopt new variables . in the paper , we give a motivation for such dynamic global constraints and we describe a dynamic version of the well-known alldifferent constraint ."}
{"title": "rhythm transcription of polyphonic piano music based on merged-output hmm for multiple voices", "abstract": "in a recent conference paper , we have reported a rhythm transcription method based on a merged-output hidden markov model ( hmm ) that explicitly describes the multiple-voice structure of polyphonic music . this model solves a major problem of conventional methods that could not properly describe the nature of multiple voices as in polyrhythmic scores or in the phenomenon of loose synchrony between voices . in this paper we present a complete description of the proposed model and develop an inference technique , which is valid for any merged-output hmms for which output probabilities depend on past events . we also examine the influence of the architecture and parameters of the method in terms of accuracies of rhythm transcription and voice separation and perform comparative evaluations with six other algorithms . using midi recordings of classical piano pieces , we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non-polyrhythmic performances . this reveals the state-of-the-art methods of rhythm transcription for the first time in the literature . publicly available source codes are also provided for future comparisons ."}
{"title": "a classification approach to word prediction", "abstract": "the eventual goal of a language model is to accurately predict the value of a missing word given its context . we present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context . this approach raises a few new questions that we address . first , in order to learn good word representations it is necessary to use an expressive representation of the context . we present a way that uses external knowledge to generate expressive context representations , along with a learning method capable of handling the large number of features generated this way that can , potentially , contribute to each prediction . second , since the number of words `` competing '' for each prediction is large , there is a need to `` focus the attention '' on a smaller subset of these . we exhibit the contribution of a `` focus of attention '' mechanism to the performance of the word predictor . finally , we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks ."}
{"title": "mamadroid : detecting android malware by building markov chains of behavioral models ( extended version )", "abstract": "as android becomes increasingly popular , so does malware targeting it , this motivating the research community to propose many different detection techniques . however , the constant evolution of the android ecosystem , and of malware itself , makes it hard to design robust tools that can operate for long periods of time without the need for modifications or costly re-training . aiming to address this issue , we set to detect malware from a behavioral point of view , modeled as the sequence of abstracted api calls . we introduce mamadroid , a static-analysis based system that abstracts app 's api calls to their class , package , or family , and builds a model from their sequences obtained from the call graph of an app as markov chains . this ensures that the model is more resilient to api changes and the features set is of manageable size . we evaluate mamadroid using a dataset of 8.5k benign and 35.5k malicious apps collected over a period of six years , showing that it effectively detects malware ( with up to 0.99 f-measure ) and keeps its detection capabilities for long periods of time ( up to 0.87 f-measure two years after training ) . we also show that mamadroid remarkably improves over droidapiminer , a state-of-the-art detection system that relies on the frequency of ( raw ) api calls . aiming to assess whether mamadroid 's effectiveness mainly stems from the api abstraction or from the sequencing modeling , we also evaluate a variant of it that uses frequency ( instead of sequences ) , of abstracted api calls . we find that it is not as accurate , failing to capture maliciousness when trained on malware samples including api calls that are equally or more frequently used by benign apps ."}
{"title": "decompositions of all different , global cardinality and related constraints", "abstract": "we show that some common and important global constraints like all-different and gcc can be decomposed into simple arithmetic constraints on which we achieve bound or range consistency , and in some cases even greater pruning . these decompositions can be easily added to new solvers . they also provide other constraints with access to the state of the propagator by sharing of variables . such sharing can be used to improve propagation between constraints . we report experiments with our decomposition in a pseudo-boolean solver ."}
{"title": "a new pruning method for solving decision trees and game trees", "abstract": "the main goal of this paper is to describe a new pruning method for solving decision trees and game trees . the pruning method for decision trees suggests a slight variant of decision trees that we call scenario trees . in scenario trees , we do not need a conditional probability for each edge emanating from a chance node . instead , we require a joint probability for each path from the root node to a leaf node . we compare the pruning method to the traditional rollback method for decision trees and game trees . for problems that require bayesian revision of probabilities , a scenario tree representation with the pruning method is more efficient than a decision tree representation with the rollback method . for game trees , the pruning method is more efficient than the rollback method ."}
{"title": "proceedings of the 2017 adkdd & targetad workshop", "abstract": "proceedings of the 2017 adkdd and targetad workshop held in conjunction with the 23rd acm sigkdd conference on knowledge discovery and data mining halifax , nova scotia , canada ."}
{"title": "evolution strategies as a scalable alternative to reinforcement learning", "abstract": "we explore the use of evolution strategies ( es ) , a class of black box optimization algorithms , as an alternative to popular mdp-based rl techniques such as q-learning and policy gradients . experiments on mujoco and atari show that es is a viable solution strategy that scales extremely well with the number of cpus available : by using a novel communication strategy based on common random numbers , our es implementation only needs to communicate scalars , making it possible to scale to over a thousand parallel workers . this allows us to solve 3d humanoid walking in 10 minutes and obtain competitive results on most atari games after one hour of training . in addition , we highlight several advantages of es as a black box optimization technique : it is invariant to action frequency and delayed rewards , tolerant of extremely long horizons , and does not need temporal discounting or value function approximation ."}
{"title": "hierarchical compositional feature learning", "abstract": "we introduce the hierarchical compositional network ( hcn ) , a directed generative model able to discover and disentangle , without supervision , the building blocks of a set of binary images . the building blocks are binary features defined hierarchically as a composition of some of the features in the layer immediately below , arranged in a particular manner . at a high level , hcn is similar to a sigmoid belief network with pooling . inference and learning in hcn are very challenging and existing variational approximations do not work satisfactorily . a main contribution of this work is to show that both can be addressed using max-product message passing ( mpmp ) with a particular schedule ( no em required ) . also , using mpmp as an inference engine for hcn makes new tasks simple : adding supervision information , classifying images , or performing inpainting all correspond to clamping some variables of the model to their known values and running mpmp on the rest . when used for classification , fast inference with hcn has exactly the same functional form as a convolutional neural network ( cnn ) with linear activations and binary weights . however , hcn 's features are qualitatively very different ."}
{"title": "probabilistic data analysis with probabilistic programming", "abstract": "probabilistic techniques are central to data analysis , but different approaches can be difficult to apply , combine , and compare . this paper introduces composable generative population models ( cgpms ) , a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques . examples include hierarchical bayesian models , multivariate kernel methods , discriminative machine learning , clustering algorithms , dimensionality reduction , and arbitrary probabilistic programs . we also demonstrate the integration of cgpms into bayesdb , a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language . the practical value is illustrated in two ways . first , cgpms are used in an analysis that identifies satellite data records which probably violate kepler 's third law , by composing causal probabilistic programs with non-parametric bayes in under 50 lines of probabilistic code . second , for several representative data analysis tasks , we report on lines of code and accuracy measurements of various cgpms , plus comparisons with standard baseline solutions from python and matlab libraries ."}
{"title": "self-organizing traffic lights : a realistic simulation", "abstract": "we have previously shown in an abstract simulation ( gershenson , 2005 ) that self-organizing traffic lights can improve greatly traffic flow for any density . in this paper , we extend these results to a realistic setting , implementing self-organizing traffic lights in an advanced traffic simulator using real data from a brussels avenue . on average , for different traffic densities , travel waiting times are reduced by 50 % compared to the current green wave method ."}
{"title": "tensor completion algorithms in big data analytics", "abstract": "tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors . due to the multidimensional character of tensors in describing complex datasets , tensor completion algorithms and their applications have received wide attention and achievement in data mining , computer vision , signal processing , and neuroscience , etc . in this survey , we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety , large volume , and high velocity . towards a better comprehension and comparison of vast existing advances , we summarize and categorize them into four groups including general tensor completion algorithms , tensor completion with auxiliary information ( variety ) , scalable tensor completion algorithms ( volume ) and dynamic tensor completion algorithms ( velocity ) . besides , we introduce their applications on real-world data-driven problems and present an open-source package covering several widely used tensor decomposition and completion algorithms . our goal is to summarize these popular methods and introduce them to researchers for promoting the research process in this field and give an available repository for practitioners . in the end , we also discuss some challenges and promising research directions in this community for future explorations ."}
{"title": "a parameterised hierarchy of argumentation semantics for extended logic programming and its application to the well-founded semantics", "abstract": "argumentation has proved a useful tool in defining formal semantics for assumption-based reasoning by viewing a proof as a process in which proponents and opponents attack each others arguments by undercuts ( attack to an argument 's premise ) and rebuts ( attack to an argument 's conclusion ) . in this paper , we formulate a variety of notions of attack for extended logic programs from combinations of undercuts and rebuts and define a general hierarchy of argumentation semantics parameterised by the notions of attack chosen by proponent and opponent . we prove the equivalence and subset relationships between the semantics and examine some essential properties concerning consistency and the coherence principle , which relates default negation and explicit negation . most significantly , we place existing semantics put forward in the literature in our hierarchy and identify a particular argumentation semantics for which we prove equivalence to the paraconsistent well-founded semantics with explicit negation , wfsx $ _p $ . finally , we present a general proof theory , based on dialogue trees , and show that it is sound and complete with respect to the argumentation semantics ."}
{"title": "semi-supervised online structure learning for composite event recognition", "abstract": "online structure learning approaches , such as those stemming from statistical relational learning , enable the discovery of complex relations in noisy data streams . however , these methods assume the existence of fully-labelled training data , which is unrealistic for most real-world applications . we present a novel approach for completing the supervision of a semi-supervised structure learning task . we incorporate graph cut minimisation , a technique that derives labels for unlabelled data , based on their distance to their labelled counterparts . in order to adapt graph cut minimisation to first order logic , we employ a suitable structural distance for measuring the distance between sets of logical atoms . the labelling process is achieved online ( single-pass ) by means of a caching mechanism and the hoeffding bound , a statistical tool to approximate globally-optimal decisions from locally-optimal ones . we evaluate our approach on the task of composite event recognition by using a benchmark dataset for human activity recognition , as well as a real dataset for maritime monitoring . the evaluation suggests that our approach can effectively complete the missing labels and eventually , improve the accuracy of the underlying structure learning system ."}
{"title": "a backwards view for assessment", "abstract": "much artificial intelligence research focuses on the problem of deducing the validity of unobservable propositions or hypotheses from observable evidence. ! many of the knowledge representation techniques designed for this problem encode the relationship between evidence and hypothesis in a directed manner . moreover , the direction in which evidence is stored is typically from evidence to hypothesis ."}
{"title": "causal modeling", "abstract": "causal models are like dependency graphs and belief nets in that they provide a structure and a set of assumptions from which a joint distribution can , in principle , be computed . unlike dependency graphs , causal models are models of hierarchical and/or parallel processes , rather than models of distributions ( partially ) known to a model builder through some sort of gestalt . as such , causal models are more modular , easier to build , more intuitive , and easier to understand than dependency graph models . causal models are formally defined and dependency graph models are shown to be a special case of them . algorithms supporting inference are presented . parsimonious methods for eliciting dependent probabilities are presented ."}
{"title": "optimal ordered problem solver", "abstract": "we present a novel , general , optimally fast , incremental way of searching for a universal algorithm that solves each task in a sequence of tasks . the optimal ordered problem solver ( oops ) continually organizes and exploits previously found solutions to earlier tasks , efficiently searching not only the space of domain-specific algorithms , but also the space of search algorithms . essentially we extend the principles of optimal nonincremental universal search to build an incremental universal learner that is able to improve itself through experience . in illustrative experiments , our self-improver becomes the first general system that learns to solve all n disk towers of hanoi tasks ( solution size 2^n-1 ) for n up to 30 , profiting from previously solved , simpler tasks involving samples of a simple context free language ."}
{"title": "monte-carlo planning : theoretically fast convergence meets practical efficiency", "abstract": "popular monte-carlo tree search ( mcts ) algorithms for online planning , such as epsilon-greedy tree search and uct , aim at rapidly identifying a reasonably good action , but provide rather poor worst-case guarantees on performance improvement over time . in contrast , a recently introduced mcts algorithm brue guarantees exponential-rate improvement over time , yet it is not geared towards identifying reasonably good choices right at the go . we take a stand on the individual strengths of these two classes of algorithms , and show how they can be effectively connected . we then rationalize a principle of `` selective tree expansion '' , and suggest a concrete implementation of this principle within mcts . the resulting algorithm , s favorably compete with other mcts algorithms under short planning times , while preserving the attractive convergence properties of brue ."}
{"title": "neuromorphic deep learning machines", "abstract": "an ongoing challenge in neuromorphic computing is to devise general and computationally efficient models of inference and learning which are compatible with the spatial and temporal constraints of the brain . one increasingly popular and successful approach is to take inspiration from inference and learning algorithms used in deep neural networks . however , the workhorse of deep learning , the gradient descent back propagation ( bp ) rule , often relies on the immediate availability of network-wide information stored with high-precision memory , and precise operations that are difficult to realize in neuromorphic hardware . remarkably , recent work showed that exact backpropagated weights are not essential for learning deep representations . random bp replaces feedback weights with random ones and encourages the network to adjust its feed-forward weights to learn pseudo-inverses of the ( random ) feedback weights . building on these results , we demonstrate an event-driven random bp ( erbp ) rule that uses an error-modulated synaptic plasticity for learning deep representations in neuromorphic computing hardware . the rule requires only one addition and two comparisons for each synaptic weight using a two-compartment leaky integrate & fire ( i & f ) neuron , making it very suitable for implementation in digital or mixed-signal neuromorphic hardware . our results show that using erbp , deep representations are rapidly learned , achieving nearly identical classification accuracies compared to artificial neural network simulations on gpus , while being robust to neural and synaptic state quantizations during learning ."}
{"title": "the network of french legal codes", "abstract": "we propose an analysis of the codified law of france as a structured system . fifty two legal codes are selected on the basis of explicit legal criteria and considered as vertices with their mutual quotations forming the edges in a network which properties are analyzed relying on graph theory . we find that a group of 10 codes are simultaneously the most citing and the most cited by other codes , and are also strongly connected together so forming a `` rich club '' sub-graph . three other code communities are also found that somewhat partition the legal field is distinct thematic sub-domains . the legal interpretation of this partition is opening new untraditional lines of research . we also conjecture that many legal systems are forming such new kind of networks that share some properties in common with small worlds but are far denser . we propose to call `` concentrated world '' ."}
{"title": "on type-aware entity retrieval", "abstract": "today , the practice of returning entities from a knowledge base in response to search queries has become widespread . one of the distinctive characteristics of entities is that they are typed , i.e. , assigned to some hierarchically organized type system ( type taxonomy ) . the primary objective of this paper is to gain a better understanding of how entity type information can be utilized in entity retrieval . we perform this investigation in an idealized `` oracle '' setting , assuming that we know the distribution of target types of the relevant entities for a given query . we perform a thorough analysis of three main aspects : ( i ) the choice of type taxonomy , ( ii ) the representation of hierarchical type information , and ( iii ) the combination of type-based and term-based similarity in the retrieval model . using a standard entity search test collection based on dbpedia , we find that type information proves most useful when using large type taxonomies that provide very specific types . we provide further insights on the extensional coverage of entities and on the utility of target types ."}
{"title": "bandit-based random mutation hill-climbing", "abstract": "the random mutation hill-climbing algorithm is a direct search technique mostly used in discrete domains . it repeats the process of randomly selecting a neighbour of a best-so-far solution and accepts the neighbour if it is better than or equal to it . in this work , we propose to use a novel method to select the neighbour solution using a set of independent multi- armed bandit-style selection units which results in a bandit-based random mutation hill-climbing algorithm . the new algorithm significantly outperforms random mutation hill-climbing in both onemax ( in noise-free and noisy cases ) and royal road problems ( in the noise-free case ) . the algorithm shows particular promise for discrete optimisation problems where each fitness evaluation is expensive ."}
{"title": "ecmdd : evidential c-medoids clustering with multiple prototypes", "abstract": "in this work , a new prototype-based clustering method named evidential c-medoids ( ecmdd ) , which belongs to the family of medoid-based clustering for proximity data , is proposed as an extension of fuzzy c-medoids ( fcmdd ) on the theoretical framework of belief functions . in the application of fcmdd and original ecmdd , a single medoid ( prototype ) , which is supposed to belong to the object set , is utilized to represent one class . for the sake of clarity , this kind of ecmdd using a single medoid is denoted by secmdd . in real clustering applications , using only one pattern to capture or interpret a class may not adequately model different types of group structure and hence limits the clustering performance . in order to address this problem , a variation of ecmdd using multiple weighted medoids , denoted by wecmdd , is presented . unlike secmdd , in wecmdd objects in each cluster carry various weights describing their degree of representativeness for that class . this mechanism enables each class to be represented by more than one object . experimental results in synthetic and real data sets clearly demonstrate the superiority of secmdd and wecmdd . moreover , the clustering results by wecmdd can provide richer information for the inner structure of the detected classes with the help of prototype weights ."}
{"title": "on geometric algebra representation of binary spatter codes", "abstract": "kanerva 's binary spatter codes are reformulated in terms of geometric algebra . the key ingredient of the construction is the representation of xor binding in terms of geometric product ."}
{"title": "a new intelligence based approach for computer-aided diagnosis of dengue fever", "abstract": "identification of the influential clinical symptoms and laboratory features that help in the diagnosis of dengue fever in early phase of the illness would aid in designing effective public health management and virological surveillance strategies . keeping this as our main objective we develop in this paper , a new computational intelligence based methodology that predicts the diagnosis in real time , minimizing the number of false positives and false negatives . our methodology consists of three major components ( i ) a novel missing value imputation procedure that can be applied on any data set consisting of categorical ( nominal ) and/or numeric ( real or integer ) ( ii ) a wrapper based features selection method with genetic search for extracting a subset of most influential symptoms that can diagnose the illness and ( iii ) an alternating decision tree method that employs boosting for generating highly accurate decision rules . the predictive models developed using our methodology are found to be more accurate than the state-of-the-art methodologies used in the diagnosis of the dengue fever ."}
{"title": "causal graph justifications of logic programs", "abstract": "in this work we propose a multi-valued extension of logic programs under the stable models semantics where each true atom in a model is associated with a set of justifications . these justifications are expressed in terms of causal graphs formed by rule labels and edges that represent their application ordering . for positive programs , we show that the causal justifications obtained for a given atom have a direct correspon- dence to ( relevant ) syntactic proofs of that atom using the program rules involved in the graphs . the most interesting contribution is that this causal information is obtained in a purely semantic way , by algebraic op- erations ( product , sum and application ) on a lattice of causal values whose ordering relation expresses when a justification is stronger than another . finally , for programs with negation , we define the concept of causal stable model by introducing an analogous transformation to gelfond and lifschitz 's program reduct . as a result , default negation behaves as `` absence of proof '' and no justification is derived from negative liter"}
{"title": "contact state analysis using nfis and som", "abstract": "this paper reports application of neuro- fuzzy inference system ( nfis ) and self organizing feature map neural networks ( som ) on detection of contact state in a block system . in this manner , on a simple system , the evolution of contact states , by parallelization of dda , has been investigated . so , a comparison between nfis and som results has been presented . the results show applicability of the proposed methods , by different accuracy , on detection of contact 's distribution ."}
{"title": "continuous occurrence theory", "abstract": "usually gradual and continuous changes in entities will lead to appear events . but usually it is supposed that an event is occurred at once . in this research an integrated framework called continuous occurrence theory ( cot ) is presented to investigate respective path leading to occurrence of the events in the real world . for this purpose initially fundamental concepts are defined . afterwards , the appropriate tools such as occurrence variables computations , occurrence dependency function and occurrence model are introduced and explained in a systematic manner . indeed , cot provides the possibility to : ( a ) monitor occurrence of events during time ; ( b ) study background of the events ; ( c ) recognize the relevant issues of each event ; and ( d ) understand how these issues affect on the considered event . the developed framework ( cot ) provides the necessary context to analyze accurately continual changes of the issues and the relevant events in the various branches of science and business . finally , typical applications of cot and an applied modeling example of it have been explained and a mathematical programming example is modeled in the occurrence based environment ."}
{"title": "a spectrum of applications of automated reasoning", "abstract": "the likelihood of an automated reasoning program being of substantial assistance for a wide spectrum of applications rests with the nature of the options and parameters it offers on which to base needed strategies and methodologies . this article focuses on such a spectrum , featuring w. mccune 's program otter , discussing widely varied successes in answering open questions , and touching on some of the strategies and methodologies that played a key role . the applications include finding a first proof , discovering single axioms , locating improved axiom systems , and simplifying existing proofs . the last application is directly pertinent to the recently found ( by r. thiele ) hilbert 's twenty-fourth problem -- which is extremely amenable to attack with the appropriate automated reasoning program -- a problem concerned with proof simplification . the methodologies include those for seeking shorter proofs and for finding proofs that avoid unwanted lemmas or classes of term , a specific option for seeking proofs with smaller equational or formula complexity , and a different option to address the variable richness of a proof . the type of proof one obtains with the use of otter is hilbert-style axiomatic , including details that permit one sometimes to gain new insights . we include questions still open and challenges that merit consideration ."}
{"title": "can deep reinforcement learning solve erdos-selfridge-spencer games ?", "abstract": "deep reinforcement learning has achieved many recent successes , but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior , and correspondingly diagnose individual actions against such a characterization . here we consider a family of combinatorial games , arising from work of erdos , selfridge , and spencer , and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning . these games have a number of appealing features : they are challenging for current learning approaches , but they form ( i ) a low-dimensional , simply parametrized environment where ( ii ) there is a linear closed form solution for optimal behavior from any state , and ( iii ) the difficulty of the game can be tuned by changing environment parameters in an interpretable way . we use these erdos-selfridge-spencer games not only to compare different algorithms , but also to compare approaches based on supervised and reinforcement learning , to analyze the power of multi-agent approaches in improving performance , and to evaluate generalization to environments outside the training set ."}
{"title": "cheating for problem solving : a genetic algorithm with social interactions", "abstract": "we propose a variation of the standard genetic algorithm that incorporates social interaction between the individuals in the population . our goal is to understand the evolutionary role of social systems and its possible application as a non-genetic new step in evolutionary algorithms . in biological populations , ie animals , even human beings and microorganisms , social interactions often affect the fitness of individuals . it is conceivable that the perturbation of the fitness via social interactions is an evolutionary strategy to avoid trapping into local optimum , thus avoiding a fast convergence of the population . we model the social interactions according to game theory . the population is , therefore , composed by cooperator and defector individuals whose interactions produce payoffs according to well known game models ( prisoner 's dilemma , chicken game , and others ) . our results on knapsack problems show , for some game models , a significant performance improvement as compared to a standard genetic algorithm ."}
{"title": "using learning-based filters to detect rule-based filtering obsolescence", "abstract": "for years , caisse des depots et consignations has produced information filtering applications . to be operational , these applications require high filtering performances which are achieved by using rule-based filters . with this technique , an administrator has to tune a set of rules for each topic . however , filters become obsolescent over time . the decrease of their performances is due to diachronic polysemy of terms that involves a loss of precision and to diachronic polymorphism of concepts that involves a loss of recall . to help the administrator to maintain his filters , we have developed a method which automatically detects filtering obsolescence . it consists in making a learning-based control filter using a set of documents which have already been categorised as relevant or not relevant by the rule-based filter . the idea is to supervise this filter by processing a differential comparison of its outcomes with those of the control one . this method has many advantages . it is simple to implement since the training set used by the learning is supplied by the rule-based filter . thus , both the making and the use of the control filter are fully automatic . with automatic detection of obsolescence , learning-based filtering finds a rich application which offers interesting prospects ."}
{"title": "potential functions based sampling heuristic for optimal path planning", "abstract": "rapidly-exploring random tree star ( rrt* ) is a recently proposed extension of rapidly-exploring random tree ( rrt ) algorithm that provides a collision-free , asymptotically optimal path regardless of obstacle 's geometry in a given environment . however , one of the limitations in the rrt* algorithm is slow convergence to optimal path solution . as a result , it consumes high memory as well as time due to a large number of iterations utilised in achieving optimal path solution . to overcome these limitations , we propose the potential function based-rrt* ( p-rrt* ) that incorporates the artificial potential field algorithm in rrt* . the proposed algorithm allows a considerable decrease in the number of iterations and thus leads to more efficient memory utilization and an accelerated convergence rate . in order to illustrate the usefulness of the proposed algorithm in terms of space execution and convergence rate , this paper presents rigorous simulation based comparisons between the proposed techniques and rrt* under different environmental conditions . moreover , both algorithms are also tested and compared under non-holonomic differential constraints ."}
{"title": "identifying reasoning patterns in games", "abstract": "we present an algorithm that identifies the reasoning patterns of agents in a game , by iteratively examining the graph structure of its multi-agent influence diagram ( maid ) representation . if the decision of an agent participates in no reasoning patterns , then we can effectively ignore that decision for the purpose of calculating a nash equilibrium for the game . in some cases , this can lead to exponential time savings in the process of equilibrium calculation . moreover , our algorithm can be used to enumerate the reasoning patterns in a game , which can be useful for constructing more effective computerized agents interacting with humans ."}
{"title": "an agent-driven semantical identifier using radial basis neural networks and reinforcement learning", "abstract": "due to the huge availability of documents in digital form , and the deception possibility raise bound to the essence of digital documents and the way they are spread , the authorship attribution problem has constantly increased its relevance . nowadays , authorship attribution , for both information retrieval and analysis , has gained great importance in the context of security , trust and copyright preservation . this work proposes an innovative multi-agent driven machine learning technique that has been developed for authorship attribution . by means of a preprocessing for word-grouping and time-period related analysis of the common lexicon , we determine a bias reference level for the recurrence frequency of the words within analysed texts , and then train a radial basis neural networks ( rbpnn ) -based classifier to identify the correct author . the main advantage of the proposed approach lies in the generality of the semantic analysis , which can be applied to different contexts and lexical domains , without requiring any modification . moreover , the proposed system is able to incorporate an external input , meant to tune the classifier , and then self-adjust by means of continuous learning reinforcement ."}
{"title": "a generic framework for interesting subspace cluster detection in multi-attributed networks", "abstract": "detection of interesting ( e.g. , coherent or anomalous ) clusters has been studied extensively on plain or univariate networks , with various applications . recently , algorithms have been extended to networks with multiple attributes for each node in the real-world . in a multi-attributed network , often , a cluster of nodes is only interesting for a subset ( subspace ) of attributes , and this type of clusters is called subspace clusters . however , in the current literature , few methods are capable of detecting subspace clusters , which involves concurrent feature selection and network cluster detection . these relevant methods are mostly heuristic-driven and customized for specific application scenarios . in this work , we present a generic and theoretical framework for detection of interesting subspace clusters in large multi-attributed networks . specifically , we propose a subspace graph-structured matching pursuit algorithm , namely , sg-pursuit , to address a broad class of such problems for different score functions ( e.g. , coherence or anomalous functions ) and topology constraints ( e.g. , connected subgraphs and dense subgraphs ) . we prove that our algorithm 1 ) runs in nearly-linear time on the network size and the total number of attributes and 2 ) enjoys rigorous guarantees ( geometrical convergence rate and tight error bound ) analogous to those of the state-of-the-art algorithms for sparse feature selection problems and subgraph detection problems . as a case study , we specialize sg-pursuit to optimize a number of well-known score functions for two typical tasks , including detection of coherent dense and anomalous connected subspace clusters in real-world networks . empirical evidence demonstrates that our proposed generic algorithm sg-pursuit performs superior over state-of-the-art methods that are designed specifically for these two tasks ."}
{"title": "perspectival knowledge in psoa ruleml : representation , model theory , and translation", "abstract": "in positional-slotted object-applicative ( psoa ) ruleml , a predicate application ( atom ) can have an object identifier ( oid ) and descriptors that may be positional arguments ( tuples ) or attribute-value pairs ( slots ) . psoa ruleml 1.0 specifies for each descriptor whether it is to be interpreted under the perspective of the predicate in whose scope it occurs . this perspectivity dimension refines the space between oidless , positional atoms ( relationships ) and oidful , slotted atoms ( frames ) : while relationships use only a predicate-scope-sensitive ( predicate-dependent ) tuple and frames use only predicate-scope-insensitive ( predicate-independent ) slots , psoa ruleml 1.0 uses a systematics of orthogonal constructs also permitting atoms with ( predicate- ) independent tuples and atoms with ( predicate- ) dependent slots . this supports data and knowledge representation where a slot attribute can have different values depending on the predicate . psoa thus extends object-oriented multi-membership and multiple inheritance . based on objectification , psoa laws are given : besides unscoping and centralization , the semantic restriction and transformation of describution permits rescoping of one atom 's independent descriptors to another atom with the same oid but a different predicate . for inheritance , default descriptors are realized by rules . on top of a metamodel and a grailog visualization , psoa 's atom systematics for facts , queries , and rules is explained . the presentation and ( xml- ) serialization syntaxes of psoa ruleml 1.0 are introduced . its model-theoretic semantics is formalized by extending the earlier interpretation functions for dependent descriptors . the open-source psoatransrun 1.3 system realizes psoa ruleml 1.0 by a translator to runtime predicates , including for dependent tuples ( prdtupterm ) and slots ( prdsloterm ) . our tests show efficiency advantages of dependent and tupled modeling ."}
{"title": "communicative capital for prosthetic agents", "abstract": "this work presents an overarching perspective on the role that machine intelligence can play in enhancing human abilities , especially those that have been diminished due to injury or illness . as a primary contribution , we develop the hypothesis that assistive devices , and specifically artificial arms and hands , can and should be viewed as agents in order for us to most effectively improve their collaboration with their human users . we believe that increased agency will enable more powerful interactions between human users and next generation prosthetic devices , especially when the sensorimotor space of the prosthetic technology greatly exceeds the conventional control and communication channels available to a prosthetic user . to more concretely examine an agency-based view on prosthetic devices , we propose a new schema for interpreting the capacity of a human-machine collaboration as a function of both the human 's and machine 's degrees of agency . we then introduce the idea of communicative capital as a way of thinking about the communication resources developed by a human and a machine during their ongoing interaction . using this schema of agency and capacity , we examine the benefits and disadvantages of increasing the agency of a prosthetic limb . to do so , we present an analysis of examples from the literature where building communicative capital has enabled a progression of fruitful , task-directed interactions between prostheses and their human users . we then describe further work that is needed to concretely evaluate the hypothesis that prostheses are best thought of as agents . the agent-based viewpoint developed in this article significantly extends current thinking on how best to support the natural , functional use of increasingly complex prosthetic enhancements , and opens the door for more powerful interactions between humans and their assistive technologies ."}
{"title": "ask your neurons : a neural-based approach to answering questions about images", "abstract": "we address a question answering task on real-world images that is set up as a visual turing test . by combining latest advances in image representation and natural language processing , we propose neural-image-qa , an end-to-end formulation to this problem for which all parts are trained jointly . in contrast to previous efforts , we are facing a multi-modal problem where the language output ( answer ) is conditioned on visual and natural language input ( image and question ) . our approach neural-image-qa doubles the performance of the previous best approach on this problem . we provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline . to study human consensus , which is related to the ambiguities inherent in this challenging task , we propose two novel metrics and collect additional answers which extends the original daquar dataset to daquar-consensus ."}
{"title": "in principle determination of generic priors", "abstract": "probability theory as extended logic is completed such that essentially any probability may be determined . this is done by considering propositional logic ( as opposed to predicate logic ) as syntactically suffcient and imposing a symmetry from propositional logic . it is shown how the notions of ` possibility ' and ` property ' may be suffciently represented in propositional logic such that 1 ) the principle of indifference drops out and becomes essentially combinatoric in nature and 2 ) one may appropriately represent assumptions where one assumes there is a space of possibilities but does not assume the size of the space ."}
{"title": "context encoders : feature learning by inpainting", "abstract": "we present an unsupervised visual feature learning algorithm driven by context-based pixel prediction . by analogy with auto-encoders , we propose context encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings . in order to succeed at this task , context encoders need to both understand the content of the entire image , as well as produce a plausible hypothesis for the missing part ( s ) . when training context encoders , we have experimented with both a standard pixel-wise reconstruction loss , as well as a reconstruction plus an adversarial loss . the latter produces much sharper results because it can better handle multiple modes in the output . we found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures . we quantitatively demonstrate the effectiveness of our learned features for cnn pre-training on classification , detection , and segmentation tasks . furthermore , context encoders can be used for semantic inpainting tasks , either stand-alone or as initialization for non-parametric methods ."}
{"title": "bootstrapping incremental dialogue systems : using linguistic knowledge to learn from minimal data", "abstract": "we present a method for inducing new dialogue systems from very small amounts of unannotated dialogue data , showing how word-level exploration using reinforcement learning ( rl ) , combined with an incremental and semantic grammar - dynamic syntax ( ds ) - allows systems to discover , generate , and understand many new dialogue variants . the method avoids the use of expensive and time-consuming dialogue act annotations , and supports more natural ( incremental ) dialogues than turn-based systems . here , language generation and dialogue management are treated as a joint decision/optimisation problem , and the mdp model for rl is constructed automatically . with an implemented system , we show that this method enables a wide range of dialogue variations to be automatically captured , even when the system is trained from only a single dialogue . the variants include question-answer pairs , over- and under-answering , self- and other-corrections , clarification interaction , split-utterances , and ellipsis . this generalisation property results from the structural knowledge and constraints present within the ds grammar , and highlights some limitations of recent systems built using machine learning techniques only ."}
{"title": "despot : online pomdp planning with regularization", "abstract": "the partially observable markov decision process ( pomdp ) provides a principled general framework for planning under uncertainty , but solving pomdps optimally is computationally intractable , due to the `` curse of dimensionality '' and the `` curse of history '' . to overcome these challenges , we introduce the determinized sparse partially observable tree ( despot ) , a sparse approximation of the standard belief tree , for online planning under uncertainty . a despot focuses online planning on a set of randomly sampled scenarios and compactly captures the `` execution '' of all policies under these scenarios . we show that the best policy obtained from a despot is near-optimal , with a regret bound that depends on the representation size of the optimal policy . leveraging this result , we give an anytime online planning algorithm , which searches a despot for a policy that optimizes a regularized objective function . regularization balances the estimated value of a policy under the sampled scenarios and the policy size , thus avoiding overfitting . the algorithm demonstrates strong experimental results , compared with some of the best online pomdp algorithms available . it has also been incorporated into an autonomous driving system for real-time vehicle control . the source code for the algorithm is available online ."}
{"title": "particle filters in robotics ( invited talk )", "abstract": "this presentation will introduce the audience to a new , emerging body of research on sequential monte carlo techniques in robotics . in recent years , particle filters have solved several hard perceptual robotic problems . early successes were limited to low-dimensional problems , such as the problem of robot localization in environments with known maps . more recently , researchers have begun exploiting structural properties of robotic domains that have led to successful particle filter applications in spaces with as many as 100,000 dimensions . the presentation will discuss specific tricks necessary to make these techniques work in real - world domains , and also discuss open challenges for researchers in the uai community ."}
{"title": "feature-based matrix factorization", "abstract": "recommender system has been more and more popular and widely used in many applications recently . the increasing information available , not only in quantities but also in types , leads to a big challenge for recommender system that how to leverage these rich information to get a better performance . most traditional approaches try to design a specific model for each scenario , which demands great efforts in developing and modifying models . in this technical report , we describe our implementation of feature-based matrix factorization . this model is an abstract of many variants of matrix factorization models , and new types of information can be utilized by simply defining new features , without modifying any lines of code . using the toolkit , we built the best single model reported on track 1 of kddcup'11 ."}
{"title": "a semantic enhanced model for effective spatial information retrieval", "abstract": "a lot of information on the web is geographically referenced . discovering and retrieving this geographic information to satisfy various users needs across both open and distributed spatial data infrastructures ( sdi ) poses eminent research challenges . however , this is mostly caused by semantic heterogeneity in users query and lack of semantic referencing of the geographic information ( gi ) metadata . to addressing these challenges , this paper discusses ontology based semantic enhanced model , which explicitly represents gi metadata , and provides linked rdf instances of each entity . the system focuses on semantic search , ontology , and efficient spatial information retrieval . in particular , an integrated model that uses specific domain information extraction to improve the searching and retrieval of ranked spatial search results ."}
{"title": "non-monotonic spatial reasoning with answer set programming modulo theories", "abstract": "the systematic modelling of dynamic spatial systems is a key requirement in a wide range of application areas such as commonsense cognitive robotics , computer-aided architecture design , and dynamic geographic information systems . we present aspmt ( qs ) , a novel approach and fully-implemented prototype for non-monotonic spatial reasoning -a crucial requirement within dynamic spatial systems- based on answer set programming modulo theories ( aspmt ) . aspmt ( qs ) consists of a ( qualitative ) spatial representation module ( qs ) and a method for turning tight aspmt instances into satisfiability modulo theories ( smt ) instances in order to compute stable models by means of smt solvers . we formalise and implement concepts of default spatial reasoning and spatial frame axioms . spatial reasoning is performed by encoding spatial relations as systems of polynomial constraints , and solving via smt with the theory of real nonlinear arithmetic . we empirically evaluate aspmt ( qs ) in comparison with other contemporary spatial reasoning systems both within and outside the context of logic programming . aspmt ( qs ) is currently the only existing system that is capable of reasoning about indirect spatial effects ( i.e. , addressing the ramification problem ) , and integrating geometric and qualitative spatial information within a non-monotonic spatial reasoning context . this paper is under consideration for publication in tplp ."}
{"title": "the observer-assisted method for adjusting hyper-parameters in deep learning algorithms", "abstract": "this paper presents a concept of a novel method for adjusting hyper-parameters in deep learning ( dl ) algorithms . an external agent-observer monitors a performance of a selected deep learning algorithm . the observer learns to model the dl algorithm using a series of random experiments . consequently , it may be used for predicting a response of the dl algorithm in terms of a selected quality measurement to a set of hyper-parameters . this allows to construct an ensemble composed of a series of evaluators which constitute an observer-assisted architecture . the architecture may be used to gradually iterate towards to the best achievable quality score in tiny steps governed by a unit of progress . the algorithm is stopped when the maximum number of steps is reached or no further progress is made ."}
{"title": "compliant conditions for polynomial time approximation of operator counts", "abstract": "in this paper , we develop a computationally simpler version of the operator count heuristic for a particular class of domains . the contribution of this abstract is threefold , we ( 1 ) propose an efficient closed form approximation to the operator count heuristic using the lagrangian dual ; ( 2 ) leverage compressed sensing techniques to obtain an integer approximation for operator counts in polynomial time ; and ( 3 ) discuss the relationship of the proposed formulation to existing heuristics and investigate properties of domains where such approaches appear to be useful ."}
{"title": "the conditional analogy gan : swapping fashion articles on people images", "abstract": "we present a novel method to solve image analogy problems : it allows to learn the relation between paired images present in training data , and then generalize and generate images that correspond to the relation , but were never seen in the training set . therefore , we call the method conditional analogy generative adversarial network ( cagan ) , as it is based on adversarial training and employs deep convolutional neural networks . an especially interesting application of that technique is automatic swapping of clothing on fashion model photos . our work has the following contributions . first , the definition of the end-to-end trainable cagan architecture , which implicitly learns segmentation masks without expensive supervised labeling data . second , experimental results show plausible segmentation masks and often convincing swapped images , given the target article . finally , we discuss the next steps for that technique : neural network architecture improvements and more advanced applications ."}
{"title": "emotion : mod\u00e8le d'appraisal-coping pour le probl\u00e8me des cascades", "abstract": "modeling emotion has become a challenge nowadays . therefore , several models have been produced in order to express human emotional activity . however , only a few of them are currently able to express the close relationship existing between emotion and cognition . an appraisal-coping model is presented here , with the aim to simulate the emotional impact caused by the evaluation of a particular situation ( appraisal ) , along with the consequent cognitive reaction intended to face the situation ( coping ) . this model is applied to the ? cascades ? problem , a small arithmetical exercise designed for ten-year-old pupils . the goal is to create a model corresponding to a child 's behavior when solving the problem using his own strategies ."}
{"title": "systematic vs. non-systematic algorithms for solving the mpe task", "abstract": "the paper continues the study of partitioning based inference of heuristics for search in the context of solving the most probable explanation task in bayesian networks . we compare two systematic branch and bound search algorithms , bbbt ( for which the heuristic information is constructed during search and allows dynamic variable/value ordering ) and its predecessor bbmb ( for which the heuristic information is pre-compiled ) , against a number of popular local search algorithms for the mpe problem . we show empirically that , when viewed as approximation schemes , bbbt/bbmb are superior to all of these best known sls algorithms , especially when the domain sizes increase beyond 2. this is in contrast with the performance of sls vs. systematic search on csp/sat problems , where sls often significantly outperforms systematic algorithms . as far as we know , bbbt/bbmb are currently the best performing algorithms for solving the mpe task ."}
{"title": "an empirical approach for modeling fuzzy geographical descriptors", "abstract": "we present a novel heuristic approach that defines fuzzy geographical descriptors using data gathered from a survey with human subjects . the participants were asked to provide graphical interpretations of the descriptors ` north ' and ` south ' for the galician region ( spain ) . based on these interpretations , our approach builds fuzzy descriptors that are able to compute membership degrees for geographical locations . we evaluated our approach in terms of efficiency and precision . the fuzzy descriptors are meant to be used as the cornerstones of a geographical referring expression generation algorithm that is able to linguistically characterize geographical locations and regions . this work is also part of a general research effort that intends to establish a methodology which reunites the empirical studies traditionally practiced in data-to-text and the use of fuzzy sets to model imprecision and vagueness in words and expressions for text generation purposes ."}
{"title": "empirical learning aided by weak domain knowledge in the form of feature importance", "abstract": "standard hybrid learners that use domain knowledge require stronger knowledge that is hard and expensive to acquire . however , weaker domain knowledge can benefit from prior knowledge while being cost effective . weak knowledge in the form of feature relative importance ( fri ) is presented and explained . feature relative importance is a real valued approximation of a feature 's importance provided by experts . advantage of using this knowledge is demonstrated by iann , a modified multilayer neural network algorithm . iann is a very simple modification of standard neural network algorithm but attains significant performance gains . experimental results in the field of molecular biology show higher performance over other empirical learning algorithms including standard backpropagation and support vector machines . iann performance is even comparable to a theory refinement system kbann that uses stronger domain knowledge . this shows feature relative importance can improve performance of existing empirical learning algorithms significantly with minimal effort ."}
{"title": "looking for plausibility", "abstract": "in the interpretation of experimental data , one is actually looking for plausible explanations . we look for a measure of plausibility , with which we can compare different possible explanations , and which can be combined when there are different sets of data . this is contrasted to the conventional measure for probabilities as well as to the proposed measure of possibilities . we define what characteristics this measure of plausibility should have . in getting to the conception of this measure , we explore the relation of plausibility to abductive reasoning , and to bayesian probabilities . we also compare with the dempster-schaefer theory of evidence , which also has its own definition for plausibility . abduction can be associated with biconditionality in inference rules , and this provides a platform to relate to the collins-michalski theory of plausibility . finally , using a formalism for wiring logic onto hopfield neural networks , we ask if this is relevant in obtaining this measure ."}
{"title": "computational narrative intelligence : a human-centered goal for artificial intelligence", "abstract": "narrative intelligence is the ability to craft , tell , understand , and respond affectively to stories . we argue that instilling artificial intelligences with computational narrative intelligence affords a number of applications beneficial to humans . we lay out some of the machine learning challenges necessary to solve to achieve computational narrative intelligence . finally , we argue that computational narrative is a practical step towards machine enculturation , the teaching of sociocultural values to machines ."}
{"title": "learning to blend computer game levels", "abstract": "we present an approach to generate novel computer game levels that blend different game concepts in an unsupervised fashion . our primary contribution is an analogical reasoning process to construct blends between level design models learned from gameplay videos . the models represent probabilistic relationships between elements in the game . an analogical reasoning process maps features between two models to produce blended models that can then generate new level chunks . as a proof-of-concept we train our system on the classic platformer game super mario bros. due to its highly-regarded and well understood level design . we evaluate the extent to which the models represent stylistic level design knowledge and demonstrate the ability of our system to explain levels that were blended by human expert designers ."}
{"title": "elementary epistemological features of machine intelligence", "abstract": "theoretical analysis of machine intelligence ( mi ) is useful for defining a common platform in both theoretical and applied artificial intelligence ( ai ) . the goal of this paper is to set canonical definitions that can assist pragmatic research in both strong and weak ai . described epistemological features of machine intelligence include relationship between intelligent behavior , intelligent and unintelligent machine characteristics , observable and unobservable entities and classification of intelligence . the paper also establishes algebraic definitions of efficiency and accuracy of mi tests as their quality measure . the last part of the paper addresses the learning process with respect to the traditional epistemology and the epistemology of mi described here . the proposed views on mi positively correlate to the hegelian monistic epistemology and contribute towards amalgamating idealistic deliberations with the ai theory , particularly in a local frame of reference ."}
{"title": "inference for belief networks using coupling from the past", "abstract": "inference for belief networks using gibbs sampling produces a distribution for unobserved variables that differs from the correct distribution by a ( usually ) unknown error , since convergence to the right distribution occurs only asymptotically . the method of `` coupling from the past '' samples from exactly the correct distribution by ( conceptually ) running dependent gibbs sampling simulations from every possible starting state from a time far enough in the past that all runs reach the same state at time t=0 . explicitly considering every possible state is intractable for large networks , however . we propose a method for layered noisy-or networks that uses a compact , but often imprecise , summary of a set of states . this method samples from exactly the correct distribution , and requires only about twice the time per step as ordinary gibbs sampling , but it may require more simulation steps than would be needed if chains were tracked exactly ."}
{"title": "paramils : an automatic algorithm configuration framework", "abstract": "the identification of performance-optimizing parameter settings is an important part of the development and application of algorithms . we describe an automatic framework for this algorithm configuration problem . more formally , we provide methods for optimizing a target algorithm 's performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters . we review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations . we describe the results of a comprehensive experimental evaluation of our methods , based on the configuration of prominent complete and incomplete algorithms for sat . we also present what is , to our knowledge , the first published work on automatically configuring the cplex mixed integer programming solver . all the algorithms we considered had default parameter settings that were manually identified with considerable effort . nevertheless , using our automated algorithm configuration procedures , we achieved substantial and consistent performance improvements ."}
{"title": "improved branch-and-bound for low autocorrelation binary sequences", "abstract": "the low autocorrelation binary sequence problem has applications in telecommunications , is of theoretical interest to physicists , and has inspired many optimisation researchers . metaheuristics for the problem have progressed greatly in recent years but complete search has not progressed since a branch-and-bound method of 1996. in this paper we find four ways of improving branch-and-bound , leading to a tighter relaxation , faster convergence to optimality , and better empirical scalability ."}
{"title": "soft goals can be compiled away", "abstract": "soft goals extend the classical model of planning with a simple model of preferences . the best plans are then not the ones with least cost but the ones with maximum utility , where the utility of a plan is the sum of the utilities of the soft goals achieved minus the plan cost . finding plans with high utility appears to involve two linked problems : choosing a subset of soft goals to achieve and finding a low-cost plan to achieve them . new search algorithms and heuristics have been developed for planning with soft goals , and a new track has been introduced in the international planning competition ( ipc ) to test their performance . in this note , we show however that these extensions are not needed : soft goals do not increase the expressive power of the basic model of planning with action costs , as they can easily be compiled away . we apply this compilation to the problems of the net-benefit track of the most recent ipc , and show that optimal and satisficing cost-based planners do better on the compiled problems than optimal and satisficing net-benefit planners on the original problems with explicit soft goals . furthermore , we show that penalties , or negative preferences expressing conditions to avoid , can also be compiled away using a similar idea ."}
{"title": "abduction , asp and open logic programs", "abstract": "open logic programs and open entailment have been recently proposed as an abstract framework for the verification of incomplete specifications based upon normal logic programs and the stable model semantics . there are obvious analogies between open predicates and abducible predicates . however , despite superficial similarities , there are features of open programs that have no immediate counterpart in the framework of abduction and viceversa . similarly , open programs can not be immediately simulated with answer set programming ( asp ) . in this paper we start a thorough investigation of the relationships between open inference , abduction and asp . we shall prove that open programs generalize the other two frameworks . the generalized framework suggests interesting extensions of abduction under the generalized stable model semantics . in some cases , we will be able to reduce open inference to abduction and asp , thereby estimating its computational complexity . at the same time , the aforementioned reduction opens the way to new applications of abduction and asp ."}
{"title": "use of python and phoenix-m interface in robotics", "abstract": "in this paper i will show how to use python programming with a computer interface such as phoenix-m 1 to drive simple robots . in my quest towards artificial intelligence ( ai ) i am experimenting with a lot of different possibilities in robotics . this one will try to mimic the working of a simple insect 's nervous system using hard wiring and some minimal software usage . this is the precursor to my advanced robotics and ai integration where i plan to use a new paradigm of ai based on machine learning and self consciousness via knowledge feedback and update process ."}
{"title": "computational complexity and simulation of rare events of ising spin glasses", "abstract": "we discuss the computational complexity of random 2d ising spin glasses , which represent an interesting class of constraint satisfaction problems for black box optimization . two extremal cases are considered : ( 1 ) the +/- j spin glass , and ( 2 ) the gaussian spin glass . we also study a smooth transition between these two extremal cases . the computational complexity of all studied spin glass systems is found to be dominated by rare events of extremely hard spin glass samples . we show that complexity of all studied spin glass systems is closely related to frechet extremal value distribution . in a hybrid algorithm that combines the hierarchical bayesian optimization algorithm ( hboa ) with a deterministic bit-flip hill climber , the number of steps performed by both the global searcher ( hboa ) and the local searcher follow frechet distributions . nonetheless , unlike in methods based purely on local search , the parameters of these distributions confirm good scalability of hboa with local search . we further argue that standard performance measures for optimization algorithms -- such as the average number of evaluations until convergence -- can be misleading . finally , our results indicate that for highly multimodal constraint satisfaction problems , such as ising spin glasses , recombination-based search can provide qualitatively better results than mutation-based search ."}
{"title": "neural generation of regular expressions from natural language with minimal domain knowledge", "abstract": "this paper explores the task of translating natural language queries into regular expressions which embody their meaning . in contrast to prior work , the proposed neural model does not utilize domain-specific crafting , learning to translate directly from a parallel corpus . to fully explore the potential of neural models , we propose a methodology for collecting a large corpus of regular expression , natural language pairs . our resulting model achieves a performance gain of 19.6 % over previous state-of-the-art models ."}
{"title": "a framework for control strategies in uncertain inference networks", "abstract": "control strategies for hierarchical tree-like probabilistic inference networks are formulated and investigated . strategies that utilize staged look-ahead and temporary focus on subgoals are formalized and refined using the depth vector concept that serves as a tool for defining the 'virtual tree ' regarded by the control strategy . the concept is illustrated by four types of control strategies for three-level trees that are characterized according to their depth vector , and according to the way they consider intermediate nodes and the role that they let these nodes play . inferenti is a computerized inference system written in prolog , which provides tools for exercising a variety of control strategies . the system also provides tools for simulating test data and for comparing the relative average performance under different strategies ."}
{"title": "causal model analysis using collider v-structure with negative percentage mapping", "abstract": "a major problem of causal inference is the arrangement of dependent nodes in a directed acyclic graph ( dag ) with path coefficients and observed confounders . path coefficients do not provide the units to measure the strength of information flowing from one node to the other . here we proposed the method of causal structure learning using collider v-structures ( cvs ) with negative percentage mapping ( npm ) to get selective thresholds of information strength , to direct the edges and subjective confounders in a dag . the npm is used to scale the strength of information passed through nodes in units of percentage from interval from 0 to 1. the causal structures are constructed by bottom up approach using path coefficients , causal directions and confounders , derived implementing collider v-structure and npm . the method is self-sufficient to observe all the latent confounders present in the causal model and capable of detecting every responsible causal direction . the results are tested for simulated datasets of non-gaussian distributions and compared with directlingam and ica-lingam to check efficiency of the proposed method ."}
{"title": "deep rewiring : training very sparse deep networks", "abstract": "neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them . but also generic hardware and software implementations of deep learning run more efficiently for sparse networks . several methods exist for pruning connections of a neural network after it was trained without connectivity constraints . we present an algorithm , deep r , that enables us to train directly a sparsely connected neural network . deep r automatically rewires the network during supervised training so that connections are there where they are most needed for the task , while its total number is all the time strictly bounded . we demonstrate that deep r can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance . deep r is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior ."}
{"title": "dynamic key-value memory networks for knowledge tracing", "abstract": "knowledge tracing ( kt ) is a task of tracing evolving knowledge state of students with respect to one or more concepts as they engage in a sequence of learning activities . one important purpose of kt is to personalize the practice sequence to help students learn knowledge concepts efficiently . however , existing methods such as bayesian knowledge tracing and deep knowledge tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with . to solve these problems , this work introduces a new model called dynamic key-value memory networks ( dkvmn ) that can exploit the relationships between underlying concepts and directly output a student 's mastery level of each concept . unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices , our model has one static matrix called key , which stores the knowledge concepts and the other dynamic matrix called value , which stores and updates the mastery levels of corresponding concepts . experiments show that our model consistently outperforms the state-of-the-art model in a range of kt datasets . moreover , the dkvmn model can automatically discover underlying concepts of exercises typically performed by human annotations and depict the changing knowledge state of a student ."}
{"title": "towards the bio-personalization of music recommendation systems : a single-sensor eeg biomarker of subjective music preference", "abstract": "recent advances in biosensors technology and mobile electroencephalographic ( eeg ) interfaces have opened new application fields for cognitive monitoring . a computable biomarker for the assessment of spontaneous aesthetic brain responses during music listening is introduced here . it derives from well-established measures of cross-frequency coupling ( cfc ) and quantifies the music-induced alterations in the dynamic relationships between brain rhythms . during a stage of exploratory analysis , and using the signals from a suitably designed experiment , we established the biomarker , which acts on brain activations recorded over the left prefrontal cortex and focuses on the functional coupling between high-beta and low-gamma oscillations . based on data from an additional experimental paradigm , we validated the introduced biomarker and showed its relevance for expressing the subjective aesthetic appreciation of a piece of music . our approach resulted in an affordable tool that can promote human-machine interaction and , by serving as a personalized music annotation strategy , can be potentially integrated into modern flexible music recommendation systems . keywords : cross-frequency coupling ; human-computer interaction ; brain-computer interface"}
{"title": "deep residual learning for weakly-supervised relation extraction", "abstract": "deep residual learning ( resnet ) is a new method for training very deep neural networks using identity map-ping for shortcut connections . resnet has won the imagenet ilsvrc 2015 classification task , and achieved state-of-the-art performances in many computer vision tasks . however , the effect of residual learning on noisy natural language processing tasks is still not well understood . in this paper , we design a novel convolutional neural network ( cnn ) with residual learning , and investigate its impacts on the task of distantly supervised noisy relation extraction . in contradictory to popular beliefs that resnet only works well for very deep networks , we found that even with 9 layers of cnns , using identity mapping could significantly improve the performance for distantly-supervised relation extraction ."}
{"title": "the libra toolkit for probabilistic models", "abstract": "the libra toolkit is a collection of algorithms for learning and inference with discrete probabilistic models , including bayesian networks , markov networks , dependency networks , and sum-product networks . compared to other toolkits , libra places a greater emphasis on learning the structure of tractable models in which exact inference is efficient . it also includes a variety of algorithms for learning graphical models in which inference is potentially intractable , and for performing exact and approximate inference . libra is released under a 2-clause bsd license to encourage broad use in academia and industry ."}
{"title": "imagining probabilistic belief change as imaging ( technical report )", "abstract": "imaging is a form of probabilistic belief change which could be employed for both revision and update . in this paper , we propose a new framework for probabilistic belief change based on imaging , called expected distance imaging ( edi ) . edi is sufficiently general to define bayesian conditioning and other forms of imaging previously defined in the literature . we argue that , and investigate how , edi can be used for both revision and update . edi 's definition depends crucially on a weight function whose properties are studied and whose effect on belief change operations is analysed . finally , four edi instantiations are proposed , two for revision and two for update , and probabilistic rationality postulates are suggested for their analysis ."}
{"title": "learning multiagent communication with backpropagation", "abstract": "many tasks in ai require the collaboration of multiple agents . typically , the communication protocol between agents is manually specified and not altered during training . in this paper we explore a simple neural model , called commnet , that uses continuous communication for fully cooperative tasks . the model consists of multiple agents and the communication between them is learned alongside their policy . we apply this model to a diverse set of tasks , demonstrating the ability of the agents to learn to communicate amongst themselves , yielding improved performance over non-communicative agents and baselines . in some cases , it is possible to interpret the language devised by the agents , revealing simple but effective strategies for solving the task at hand ."}
{"title": "social emotion mining techniques for facebook posts reaction prediction", "abstract": "as of february 2016 facebook allows users to express their experienced emotions about a post by using five so-called ` reactions ' . this research paper proposes and evaluates alternative methods for predicting these reactions to user posts on public pages of firms/companies ( like supermarket chains ) . for this purpose , we collected posts ( and their reactions ) from facebook pages of large supermarket chains and constructed a dataset which is available for other researches . in order to predict the distribution of reactions of a new post , neural network architectures ( convolutional and recurrent neural networks ) were tested using pretrained word embeddings . results of the neural networks were improved by introducing a bootstrapping approach for sentiment and emotion mining on the comments for each post . the final model ( a combination of neural network and a baseline emotion miner ) is able to predict the reaction distribution on facebook posts with a mean squared error ( or misclassification rate ) of 0.135 ."}
{"title": "simple and efficient architecture search for convolutional neural networks", "abstract": "neural networks have recently had a lot of success for many tasks . however , neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process . we propose a new method to automatically search for well-performing cnn architectures based on a simple hill climbing procedure whose operators apply network morphisms , followed by short optimization runs by cosine annealing . surprisingly , this simple method yields competitive results , despite only requiring resources in the same order of magnitude as training a single network . e.g. , on cifar-10 , our method designs and trains networks with an error rate below 6 % in only 12 hours on a single gpu ; training for one day reduces this error further , to almost 5 % ."}
{"title": "shadows and headless shadows : a worlds-based , autobiographical approach to reasoning", "abstract": "many cognitive systems deploy multiple , closed , individually consistent models which can represent interpretations of the present state of the world , moments in the past , possible futures or alternate versions of reality . while they appear under different names , these structures can be grouped under the general term of worlds . the xapagy architecture is a story-oriented cognitive system which relies exclusively on the autobiographical memory implemented as a raw collection of events organized into world-type structures called { \\em scenes } . the system performs reasoning by shadowing current events with events from the autobiography . the shadows are then extrapolated into headless shadows corresponding to predictions , hidden events or inferred relations ."}
{"title": "a modified vortex search algorithm for numerical function optimization", "abstract": "the vortex search ( vs ) algorithm is one of the recently proposed metaheuristic algorithms which was inspired from the vortical flow of the stirred fluids . although the vs algorithm is shown to be a good candidate for the solution of certain optimization problems , it also has some drawbacks . in the vs algorithm , candidate solutions are generated around the current best solution by using a gaussian distribution at each iteration pass . this provides simplicity to the algorithm but it also leads to some problems along . especially , for the functions those have a number of local minimum points , to select a single point to generate candidate solutions leads the algorithm to being trapped into a local minimum point . due to the adaptive step-size adjustment scheme used in the vs algorithm , the locality of the created candidate solutions is increased at each iteration pass . therefore , if the algorithm can not escape a local point as quickly as possible , it becomes much more difficult for the algorithm to escape from that point in the latter iterations . in this study , a modified vortex search algorithm ( mvs ) is proposed to overcome above mentioned drawback of the existing vs algorithm . in the mvs algorithm , the candidate solutions are generated around a number of points at each iteration pass . computational results showed that with the help of this modification the global search ability of the existing vs algorithm is improved and the mvs algorithm outperformed the existing vs algorithm , pso2011 and abc algorithms for the benchmark numerical function set ."}
{"title": "gaussian approximation of collective graphical models", "abstract": "the collective graphical model ( cgm ) models a population of independent and identically distributed individuals when only collective statistics ( i.e. , counts of individuals ) are observed . exact inference in cgms is intractable , and previous work has explored markov chain monte carlo ( mcmc ) and map approximations for learning and inference . this paper studies gaussian approximations to the cgm . as the population grows large , we show that the cgm distribution converges to a multivariate gaussian distribution ( gcgm ) that maintains the conditional independence properties of the original cgm . if the observations are exact marginals of the cgm or marginals that are corrupted by gaussian noise , inference in the gcgm approximation can be computed efficiently in closed form . if the observations follow a different noise model ( e.g. , poisson ) , then expectation propagation provides efficient and accurate approximate inference . the accuracy and speed of gcgm inference is compared to the mcmc and map methods on a simulated bird migration problem . the gcgm matches or exceeds the accuracy of the map method while being significantly faster ."}
{"title": "talplanner in ipc-2002 : extensions and control rules", "abstract": "talplanner is a forward-chaining planner that relies on domain knowledge in the shape of temporal logic formulas in order to prune irrelevant parts of the search space . talplanner recently participated in the third international planning competition , which had a clear emphasis on increasing the complexity of the problem domains being used as benchmark tests and the expressivity required to represent these domains in a planning system . like many other planners , talplanner had support for some but not all aspects of this increase in expressivity , and a number of changes to the planner were required . after a short introduction to talplanner , this article describes some of the changes that were made before and during the competition . we also describe the process of introducing suitable domain knowledge for several of the competition domains ."}
{"title": "a tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing", "abstract": "dual decomposition , and more generally lagrangian relaxation , is a classical method for combinatorial optimization ; it has recently been applied to several inference problems in natural language processing ( nlp ) . this tutorial gives an overview of the technique . we describe example algorithms , describe formal guarantees for the method , and describe practical issues in implementing the algorithms . while our examples are predominantly drawn from the nlp literature , the material should be of general relevance to inference problems in machine learning . a central theme of this tutorial is that lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms , allowing inference in models that go significantly beyond previous work on lagrangian relaxation for inference in graphical models ."}
{"title": "supervisory control for behavior composition", "abstract": "we relate behavior composition , a synthesis task studied in ai , to supervisory control theory from the discrete event systems field . in particular , we show that realizing ( i.e. , implementing ) a target behavior module ( e.g. , a house surveillance system ) by suitably coordinating a collection of available behaviors ( e.g. , automatic blinds , doors , lights , cameras , etc . ) amounts to imposing a supervisor onto a special discrete event system . such a link allows us to leverage on the solid foundations and extensive work on discrete event systems , including borrowing tools and ideas from that field . as evidence of that we show how simple it is to introduce preferences in the mapped framework ."}
{"title": "first experiments with powerplay", "abstract": "like a scientist or a playing child , powerplay not only learns new skills to solve given problems , but also invents new interesting problems by itself . by design , it continually comes up with the fastest to find , initially novel , but eventually solvable tasks . it also continually simplifies or compresses or speeds up solutions to previous tasks . here we describe first experiments with powerplay . a self-delimiting recurrent neural network slim rnn is used as a general computational problem solving architecture . its connection weights can encode arbitrary , self-delimiting , halting or non-halting programs affecting both environment ( through effectors ) and internal states encoding abstractions of event sequences . our powerplay-driven slim rnn learns to become an increasingly general solver of self-invented problems , continually adding new problem solving procedures to its growing skill repertoire . extending a recent conference paper , we identify interesting , emerging , developmental stages of our open-ended system . we also show how it automatically self-modularizes , frequently re-using code for previously invented skills , always trying to invent novel tasks that can be quickly validated because they do not require too many weight changes affecting too many previous tasks ."}
{"title": "probabilistic hybrid action models for predicting concurrent percept-driven robot behavior", "abstract": "this article develops probabilistic hybrid action models ( phams ) , a realistic causal model for predicting the behavior generated by modern percept-driven robot plans . phams represent aspects of robot behavior that can not be represented by most action models used in ai planning : the temporal structure of continuous control processes , their non-deterministic effects , several modes of their interferences , and the achievement of triggering conditions in closed-loop robot plans . the main contributions of this article are : ( 1 ) phams , a model of concurrent percept-driven behavior , its formalization , and proofs that the model generates probably , qualitatively accurate predictions ; and ( 2 ) a resource-efficient inference method for phams based on sampling projections from probabilistic action models and state descriptions . we show how phams can be applied to planning the course of action of an autonomous robot office courier based on analytical and experimental results ."}
{"title": "a tube-and-droplet-based approach for representing and analyzing motion trajectories", "abstract": "trajectory analysis is essential in many applications . in this paper , we address the problem of representing motion trajectories in a highly informative way , and consequently utilize it for analyzing trajectories . our approach first leverages the complete information from given trajectories to construct a thermal transfer field which provides a context-rich way to describe the global motion pattern in a scene . then , a 3d tube is derived which depicts an input trajectory by integrating its surrounding motion patterns contained in the thermal transfer field . the 3d tube effectively : 1 ) maintains the movement information of a trajectory , 2 ) embeds the complete contextual motion pattern around a trajectory , 3 ) visualizes information about a trajectory in a clear and unified way . we further introduce a droplet-based process . it derives a droplet vector from a 3d tube , so as to characterize the high-dimensional 3d tube information in a simple but effective way . finally , we apply our tube-and-droplet representation to trajectory analysis applications including trajectory clustering , trajectory classification & abnormality detection , and 3d action recognition . experimental comparisons with state-of-the-art algorithms demonstrate the effectiveness of our approach ."}
{"title": "graph-grammar assistance for automated generation of influence diagrams", "abstract": "one of the most difficult aspects of modeling complex dilemmas in decision-analytic terms is composing a diagram of relevance relations from a set of domain concepts . decision models in domains such as medicine , however , exhibit certain prototypical patterns that can guide the modeling process . medical concepts can be classified according to semantic types that have characteristic positions and typical roles in an influence-diagram model . we have developed a graph-grammar production system that uses such inherent interrelationships among medical terms to facilitate the modeling of medical decisions ."}
{"title": "two-stage metric learning", "abstract": "in this paper , we present a novel two-stage metric learning algorithm . we first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points . then , we define the distance in the input data space as the fisher information distance on the associated statistical manifold . this induces in the input data space a new family of distance metric with unique properties . unlike kernelized metric learning , we do not require the similarity measure to be positive semi-definite . moreover , it can also be interpreted as a local metric learning algorithm with well defined distance approximation . we evaluate its performance on a number of datasets . it outperforms significantly other metric learning methods and svm ."}
{"title": "decision making under uncertainty : a quasimetric approach", "abstract": "we propose a new approach for solving a class of discrete decision making problems under uncertainty with positive cost . this issue concerns multiple and diverse fields such as engineering , economics , artificial intelligence , cognitive science and many others . basically , an agent has to choose a single or series of actions from a set of options , without knowing for sure their consequences . schematically , two main approaches have been followed : either the agent learns which option is the correct one to choose in a given situation by trial and error , or the agent already has some knowledge on the possible consequences of his decisions ; this knowledge being generally expressed as a conditional probability distribution . in the latter case , several optimal or suboptimal methods have been proposed to exploit this uncertain knowledge in various contexts . in this work , we propose following a different approach , based on the geometric intuition of distance . more precisely , we define a goal independent quasimetric structure on the state space , taking into account both cost function and transition probability . we then compare precision and computation time with classical approaches ."}
{"title": "n-gram opcode analysis for android malware detection", "abstract": "android malware has been on the rise in recent years due to the increasing popularity of android and the proliferation of third party application markets . emerging android malware families are increasingly adopting sophisticated detection avoidance techniques and this calls for more effective approaches for android malware detection . hence , in this paper we present and evaluate an n-gram opcode features based approach that utilizes machine learning to identify and categorize android malware . this approach enables automated feature discovery without relying on prior expert or domain knowledge for pre-determined features . furthermore , by using a data segmentation technique for feature selection , our analysis is able to scale up to 10-gram opcodes . our experiments on a dataset of 2520 samples showed an f-measure of 98 % using the n-gram opcode based approach . we also provide empirical findings that illustrate factors that have probable impact on the overall n-gram opcodes performance trends ."}
{"title": "bridging lstm architecture and the neural dynamics during reading", "abstract": "recently , the long short-term memory neural network ( lstm ) has attracted wide interest due to its success in many tasks . lstm architecture consists of a memory cell and three gates , which looks similar to the neuronal networks in the brain . however , there still lacks the evidence of the cognitive plausibility of lstm architecture as well as its working mechanism . in this paper , we study the cognitive plausibility of lstm by aligning its internal architecture with the brain activity observed via fmri when the subjects read a story . experiment results show that the artificial memory vector in lstm can accurately predict the observed sequential brain activities , indicating the correlation between lstm architecture and the cognitive process of story reading ."}
{"title": "parallel complexity of forward and backward propagation", "abstract": "we show that the forward and backward propagation can be formulated as a solution of lower and upper triangular systems of equations . for standard feedforward ( fnns ) and recurrent neural networks ( rnns ) the triangular systems are always block bi-diagonal , while for a general computation graph ( directed acyclic graph ) they can have a more complex triangular sparsity pattern . we discuss direct and iterative parallel algorithms that can be used for their solution and interpreted as different ways of performing model parallelism . also , we show that for fnns and rnns with $ k $ layers and $ \\tau $ time steps the backward propagation can be performed in parallel in o ( $ \\log k $ ) and o ( $ \\log k \\log \\tau $ ) steps , respectively . finally , we outline the generalization of this technique using jacobians that potentially allows us to handle arbitrary layers ."}
{"title": "on rho in a decision-theoretic apparatus of dempster-shafer theory", "abstract": "thomas m. strat has developed a decision-theoretic apparatus for dempster-shafer theory ( decision analysis using belief functions , intern . j. approx . reason . 4 ( 5/6 ) , 391-417 , 1990 ) . in this apparatus , expected utility intervals are constructed for different choices . the choice with the highest expected utility is preferable to others . however , to find the preferred choice when the expected utility interval of one choice is included in that of another , it is necessary to interpolate a discerning point in the intervals . this is done by the parameter rho , defined as the probability that the ambiguity about the utility of every nonsingleton focal element will turn out as favorable as possible . if there are several different decision makers , we might sometimes be more interested in having the highest expected utility among the decision makers rather than only trying to maximize our own expected utility regardless of choices made by other decision makers . the preference of each choice is then determined by the probability of yielding the highest expected utility . this probability is equal to the maximal interval length of rho under which an alternative is preferred . we must here take into account not only the choices already made by other decision makers but also the rational choices we can assume to be made by later decision makers . in strats apparatus , an assumption , unwarranted by the evidence at hand , has to be made about the value of rho . we demonstrate that no such assumption is necessary . it is sufficient to assume a uniform probability distribution for rho to be able to discern the most preferable choice . we discuss when this approach is justifiable ."}
{"title": "memory enriched big bang big crunch optimization algorithm for data clustering", "abstract": "cluster analysis plays an important role in decision making process for many knowledge-based systems . there exist a wide variety of different approaches for clustering applications including the heuristic techniques , probabilistic models , and traditional hierarchical algorithms . in this paper , a novel heuristic approach based on big bang-big crunch algorithm is proposed for clustering problems . the proposed method not only takes advantage of heuristic nature to alleviate typical clustering algorithms such as k-means , but it also benefits from the memory based scheme as compared to its similar heuristic techniques . furthermore , the performance of the proposed algorithm is investigated based on several benchmark test functions as well as on the well-known datasets . the experimental results show the significant superiority of the proposed method over the similar algorithms ."}
{"title": "learning python code suggestion with a sparse pointer network", "abstract": "to enhance developer productivity , all modern integrated development environments ( ides ) include code suggestion functionality that proposes likely next tokens at the cursor . while current ides work well for statically-typed languages , their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages . moreover , suggestion engines in modern ides do not propose expressions or multi-statement idiomatic code . recent work has shown that language models can improve code suggestion systems by learning from software repositories . this paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies . we release a large-scale code suggestion corpus of 41m lines of python code crawled from github . on this corpus , we found standard neural language models to perform well at suggesting local phenomena , but struggle to refer to identifiers that are introduced many tokens in the past . by augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers , we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an lstm baseline . in fact , this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers . furthermore , a qualitative analysis shows this model indeed captures interesting long-range dependencies , like referring to a class member defined over 60 tokens in the past ."}
{"title": "implicit robot-human communication in adversarial and collaborative environments", "abstract": "users of ai systems may rely upon them to produce plans for achieving desired objectives . such ai systems should be able to compute obfuscated plans whose execution in adversarial situations protects privacy as well as legible plans which are easy for team-members to understand in collaborative situations . we develop a unified framework that addresses these dual problems by computing plans with a desired level of comprehensibility from the point of view of a partially informed observer . our approach produces obfuscated plans with observations that are consistent with at least 'k ' goals from a given set of decoy goals . in addition , when the goal is known to the observer , our approach generates obfuscated plans with observations that are diverse with at least 'l ' candidate plans . our approach for plan legibility produces plans that achieve a goal while being consistent with at most 'j ' goals in a given set of confounding goals . we provide an empirical evaluation to show the feasibility and usefulness of our approaches ."}
{"title": "when ignorance is bliss", "abstract": "it is commonly-accepted wisdom that more information is better , and that information should never be ignored . here we argue , using both a bayesian and a non-bayesian analysis , that in some situations you are better off ignoring information if your uncertainty is represented by a set of probability measures . these include situations in which the information is relevant for the prediction task at hand . in the non-bayesian analysis , we show how ignoring information avoids dilation , the phenomenon that additional pieces of information sometimes lead to an increase in uncertainty . in the bayesian analysis , we show that for small sample sizes and certain prediction tasks , the bayesian posterior based on a noninformative prior yields worse predictions than simply ignoring the given information ."}
{"title": "numerical sensitivity and efficiency in the treatment of epistemic and aleatory uncertainty", "abstract": "the treatment of both aleatory and epistemic uncertainty by recent methods often requires an high computational effort . in this abstract , we propose a numerical sampling method allowing to lighten the computational burden of treating the information by means of so-called fuzzy random variables ."}
{"title": "logic programming applications : what are the abstractions and implementations ?", "abstract": "this article presents an overview of applications of logic programming , classifying them based on the abstractions and implementations of logic languages that support the applications . the three key abstractions are join , recursion , and constraint . their essential implementations are for-loops , fixed points , and backtracking , respectively . the corresponding kinds of applications are database queries , inductive analysis , and combinatorial search , respectively . we also discuss language extensions and programming paradigms , summarize example application problems by application areas , and touch on example systems that support variants of the abstractions with different implementations ."}
{"title": "unsatisfiable cores for constraint programming", "abstract": "constraint programming ( cp ) solvers typically tackle optimization problems by repeatedly finding solutions to a problem while placing tighter and tighter bounds on the solution cost . this approach is somewhat naive , especially for soft-constraint optimization problems in which the soft constraints are mostly satisfied . unsatisfiable-core approaches to solving soft constraint problems in boolean satisfiability ( e.g . maxsat ) force all soft constraints to hold initially . when solving fails they return an unsatisfiable core , as a set of soft constraints that can not hold simultaneously . using this information the problem is relaxed to allow certain soft constraint ( s ) to be violated and solving continues . since lazy clause generation ( lcg ) solvers can also return unsatisfiable cores we can adapt the maxsat unsatisfiable core approach to cp . we implement the original maxsat unsatisfiable core solving algorithms wpm1 , msu3 in a state-of-the-art lcg solver and show that there exist problems which benefit from this hybrid approach ."}
{"title": "normalized information distance", "abstract": "the normalized information distance is a universal distance measure for objects of all kinds . it is based on kolmogorov complexity and thus uncomputable , but there are ways to utilize it . first , compression algorithms can be used to approximate the kolmogorov complexity if the objects have a string representation . second , for names and abstract concepts , page count statistics from the world wide web can be used . these practical realizations of the normalized information distance can then be applied to machine learning tasks , expecially clustering , to perform feature-free and parameter-free data mining . this chapter discusses the theoretical foundations of the normalized information distance and both practical realizations . it presents numerous examples of successful real-world applications based on these distance measures , ranging from bioinformatics to music clustering to machine translation ."}
{"title": "a distributional perspective on reinforcement learning", "abstract": "in this paper we argue for the fundamental importance of the value distribution : the distribution of the random return received by a reinforcement learning agent . this is in contrast to the common approach to reinforcement learning which models the expectation of this return , or value . although there is an established body of literature studying the value distribution , thus far it has always been used for a specific purpose such as implementing risk-aware behaviour . we begin with theoretical results in both the policy evaluation and control settings , exposing a significant distributional instability in the latter . we then use the distributional perspective to design a new algorithm which applies bellman 's equation to the learning of approximate value distributions . we evaluate our algorithm using the suite of games from the arcade learning environment . we obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning . finally , we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting ."}
{"title": "a stronger foundation for computer science and p=np", "abstract": "this article constructs a turing machine which can solve for $ \\beta^ { ' } $ which is re-complete . such a machine is only possible if there is something wrong with the foundations of computer science and mathematics . we therefore check our work by looking very closely at cantor 's diagonalization and construct a novel formal language as an abelian group which allows us , through equivalence relations , to provide a non-trivial counterexample to cantor 's argument . as if that was n't enough , we then discover that the impredicative nature of g\\ '' odel 's diagonalization lemma leads to logical tautology , invalidating any meaning behind the method , leaving no doubt that diagonalization is flawed . our discovery in regards to these foundational arguments opens the door to solving the p vs np problem ."}
{"title": "relating complexity-theoretic parameters with sat solver performance", "abstract": "over the years complexity theorists have proposed many structural parameters to explain the surprising efficiency of conflict-driven clause-learning ( cdcl ) sat solvers on a wide variety of large industrial boolean instances . while some of these parameters have been studied empirically , until now there has not been a unified comparative study of their explanatory power on a comprehensive benchmark . we correct this state of affairs by conducting a large-scale empirical evaluation of cdcl sat solver performance on nearly 7000 industrial and crafted formulas against several structural parameters such as backdoors , treewidth , backbones , and community structure . our study led us to several results . first , we show that while such parameters only weakly correlate with cdcl solving time , certain combinations of them yield much better regression models . second , we show how some parameters can be used as a `` lens '' to better understand the efficiency of different solving heuristics . finally , we propose a new complexity-theoretic parameter , which we call learning-sensitive with restarts ( lsr ) backdoors , that extends the notion of learning-sensitive ( ls ) backdoors to incorporate restarts and discuss algorithms to compute them . we mathematically prove that for certain class of instances minimal lsr-backdoors are exponentially smaller than minimal-ls backdoors ."}
{"title": "modeling creativity : case studies in python", "abstract": "modeling creativity ( doctoral dissertation , 2013 ) explores how creativity can be represented using computational approaches . our aim is to construct computer models that exhibit creativity in an artistic context , that is , that are capable of generating or evaluating an artwork ( visual or linguistic ) , an interesting new idea , a subjective opinion . the research was conducted in 2008-2012 at the computational linguistics research group ( clips , university of antwerp ) under the supervision of prof. walter daelemans . prior research was also conducted at the experimental media research group ( emrg , st. lucas university college of art & design antwerp ) under the supervision of lucas nijs . modeling creativity examines creativity in a number of different perspectives : from its origins in nature , which is essentially blind , to humans and machines , and from generating creative ideas to evaluating and learning their novelty and usefulness . we will use a hands-on approach with case studies and examples in the python programming language ."}
{"title": "a behavioral distance for fuzzy-transition systems", "abstract": "in contrast to the existing approaches to bisimulation for fuzzy systems , we introduce a behavioral distance to measure the behavioral similarity of states in a nondeterministic fuzzy-transition system . this behavioral distance is defined as the greatest fixed point of a suitable monotonic function and provides a quantitative analogue of bisimilarity . the behavioral distance has the important property that two states are at zero distance if and only if they are bisimilar . moreover , for any given threshold , we find that states with behavioral distances bounded by the threshold are equivalent . in addition , we show that two system combinators -- -parallel composition and product -- -are non-expansive with respect to our behavioral distance , which makes compositional verification possible ."}
{"title": "complexity of the description logic alcm", "abstract": "in this paper we show that the problem of checking consistency of a knowledge base in the description logic alcm is exptime-complete . the m stands for meta-modelling as defined by motz , rohrer and severi . to show our main result , we define an exptime tableau algorithm as an extension of an algorithm for checking consistency of a knowledge base in alc by nguyen and szalas ."}
{"title": "bayes ' bluff : opponent modelling in poker", "abstract": "poker is a challenging problem for artificial intelligence , with non-deterministic dynamics , partial observability , and the added difficulty of unknown adversaries . modelling all of the uncertainties in this domain is not an easy task . in this paper we present a bayesian probabilistic model for a broad class of poker games , separating the uncertainty in the game dynamics from the uncertainty of the opponent 's strategy . we then describe approaches to two key subproblems : ( i ) inferring a posterior over opponent strategies given a prior distribution and observations of their play , and ( ii ) playing an appropriate response to that distribution . we demonstrate the overall approach on a reduced version of poker using dirichlet priors and then on the full game of texas hold'em using a more informed prior . we demonstrate methods for playing effective responses to the opponent , based on the posterior ."}
{"title": "efficient rank aggregation via lehmer codes", "abstract": "we propose a novel rank aggregation method based on converting permutations into their corresponding lehmer codes or other subdiagonal images . lehmer codes , also known as inversion vectors , are vector representations of permutations in which each coordinate can take values not restricted by the values of other coordinates . this transformation allows for decoupling of the coordinates and for performing aggregation via simple scalar median or mode computations . we present simulation results illustrating the performance of this completely parallelizable approach and analytically prove that both the mode and median aggregation procedure recover the correct centroid aggregate with small sample complexity when the permutations are drawn according to the well-known mallows models . the proposed lehmer code approach may also be used on partial rankings , with similar performance guarantees ."}
{"title": "the ariadne 's clew algorithm", "abstract": "we present a new approach to path planning , called the `` ariadne 's clew algorithm '' . it is designed to find paths in high-dimensional continuous spaces and applies to robots with many degrees of freedom in static , as well as dynamic environments - ones where obstacles may move . the ariadne 's clew algorithm comprises two sub-algorithms , called search and explore , applied in an interleaved manner . explore builds a representation of the accessible space while search looks for the target . both are posed as optimization problems . we describe a real implementation of the algorithm to plan paths for a six degrees of freedom arm in a dynamic environment where another six degrees of freedom arm is used as a moving obstacle . experimental results show that a path is found in about one second without any pre-processing ."}
{"title": "tuned models of peer assessment in moocs", "abstract": "in massive open online courses ( moocs ) , peer grading serves as a critical tool for scaling the grading of complex , open-ended assignments to courses with tens or hundreds of thousands of students . but despite promising initial trials , it does not always deliver accurate results compared to human experts . in this paper , we develop algorithms for estimating and correcting for grader biases and reliabilities , showing significant improvement in peer grading accuracy on real data with 63,199 peer grades from coursera 's hci course offerings -- - the largest peer grading networks analysed to date . we relate grader biases and reliabilities to other student factors such as student engagement , performance as well as commenting style . we also show that our model can lead to more intelligent assignment of graders to gradees ."}
{"title": "learning classifier systems with memory condition to solve non-markov problems", "abstract": "in the family of learning classifier systems , the classifier system xcs has been successfully used for many applications . however , the standard xcs has no memory mechanism and can only learn optimal policy in markov environments , where the optimal action is determined solely by the state of current sensory input . in practice , most environments are partially observable environments on agent 's sensation , which are also known as non-markov environments . within these environments , xcs either fails , or only develops a suboptimal policy , since it has no memory . in this work , we develop a new classifier system based on xcs to tackle this problem . it adds an internal message list to xcs as the memory list to record input sensation history , and extends a small number of classifiers with memory conditions . the classifier 's memory condition , as a foothold to disambiguate non-markov states , is used to sense a specified element in the memory list . besides , a detection method is employed to recognize non-markov states in environments , to avoid these states controlling over classifiers ' memory conditions . furthermore , four sets of different complex maze environments have been tested by the proposed method . experimental results show that our system is one of the best techniques to solve partially observable environments , compared with some well-known classifier systems proposed for these environments ."}
{"title": "prasp report", "abstract": "this technical report describes the usage , syntax , semantics and core algorithms of the probabilistic inductive logic programming framework prasp . prasp is a research software which integrates non-monotonic reasoning based on answer set programming ( asp ) , probabilistic inference and parameter learning . in contrast to traditional approaches to probabilistic ( inductive ) logic programming , our framework imposes only little restrictions on probabilistic logic programs . in particular , prasp allows for asp as well as first-order logic syntax , and for the annotation of formulas with point probabilities as well as interval probabilities . a range of widely configurable inference algorithms can be combined in a pipeline-like fashion , in order to cover a variety of use cases ."}
{"title": "deep neural networks under stress", "abstract": "in recent years , deep architectures have been used for transfer learning with state-of-the-art performance in many datasets . the properties of their features remain , however , largely unstudied under the transfer perspective . in this work , we present an extensive analysis of the resiliency of feature vectors extracted from deep models , with special focus on the trade-off between performance and compression rate . by introducing perturbations to image descriptions extracted from a deep convolutional neural network , we change their precision and number of dimensions , measuring how it affects the final score . we show that deep features are more robust to these disturbances when compared to classical approaches , achieving a compression rate of 98.4 % , while losing only 0.88 % of their original score for pascal voc 2007 ."}
{"title": "fashioning with networks : neural style transfer to design clothes", "abstract": "convolutional neural networks have been highly successful in performing a host of computer vision tasks such as object recognition , object detection , image segmentation and texture synthesis . in 2015 , gatys et . al [ 7 ] show how the style of a painter can be extracted from an image of the painting and applied to another normal photograph , thus recreating the photo in the style of the painter . the method has been successfully applied to a wide range of images and has since spawned multiple applications and mobile apps . in this paper , the neural style transfer algorithm is applied to fashion so as to synthesize new custom clothes . we construct an approach to personalize and generate new custom clothes based on a users preference and by learning the users fashion choices from a limited set of clothes from their closet . the approach is evaluated by analyzing the generated images of clothes and how well they align with the users fashion style ."}
{"title": "structural controllability and observability in influence diagrams", "abstract": "influence diagram is a graphical representation of belief networks with uncertainty . this article studies the structural properties of a probabilistic model in an influence diagram . in particular , structural controllability theorems and structural observability theorems are developed and algorithms are formulated . controllability and observability are fundamental concepts in dynamic systems ( luenberger 1979 ) . controllability corresponds to the ability to control a system while observability analyzes the inferability of its variables . both properties can be determined by the ranks of the system matrices . structural controllability and observability , on the other hand , analyze the property of a system with its structure only , without the specific knowledge of the values of its elements ( tin 1974 , shields and pearson 1976 ) . the structural analysis explores the connection between the structure of a model and the functional dependence among its elements . it is useful in comprehending problem and formulating solution by challenging the underlying intuitions and detecting inconsistency in a model . this type of qualitative reasoning can sometimes provide insight even when there is insufficient numerical information in a model ."}
{"title": "combining voting rules together", "abstract": "we propose a simple method for combining together voting rules that performs a run-off between the different winners of each voting rule . we prove that this combinator has several good properties . for instance , even if just one of the base voting rules has a desirable property like condorcet consistency , the combination inherits this property . in addition , we prove that combining voting rules together in this way can make finding a manipulation more computationally difficult . finally , we study the impact of this combinator on approximation methods that find close to optimal manipulations ."}
{"title": "assessing the reach and impact of game-based learning approaches to cultural competency and behavioural change", "abstract": "as digital games continue to be explored as solutions to educational and behavioural challenges , the need for evaluation methodologies which support both the unique nature of the format and the need for comparison with other approaches continues to increase . in this workshop paper , a range of challenges are described related specifically to the case of cultural learning using digital games , in terms of how it may best be assessed , understood , and sustained through an iterative process supported by research . an evaluation framework is proposed , identifying metrics for reach and impact and their associated challenges , as well as presenting ethical considerations and the means to utilize evaluation outcomes within an iterative cycle , and to provide feedback to learners . presenting as a case study a serious game from the mobile assistance for social inclusion and empowerment of immigrants with persuasive learning technologies and social networks ( maseltov ) project , the use of the framework in the context of an integrative project is discussed , with emphasis on the need to view game-based learning as a blended component of the cultural learning process , rather than a standalone solution . the particular case of mobile gaming is also considered within this case study , providing a platform by which to deliver and update content in response to evaluation outcomes . discussion reflects upon the general challenges related to the assessment of cultural learning , and behavioural change in more general terms , suggesting future work should address the need to provide sustainable , research-driven platforms for game-based learning content ."}
{"title": "average size of implicational bases", "abstract": "implicational bases are objects of interest in formal concept analysis and its applications . unfortunately , even the smallest base , the duquenne-guigues base , has an exponential size in the worst case . in this paper , we use results on the average number of minimal transversals in random hypergraphs to show that the base of proper premises is , on average , of quasi-polynomial size ."}
{"title": "on seeking consensus between document similarity measures", "abstract": "this paper investigates the application of consensus clustering and meta-clustering to the set of all possible partitions of a data set . we show that when using a `` complement '' of rand index as a measure of cluster similarity , the total-separation partition , putting each element in a separate set , is chosen ."}
{"title": "prototype matching networks for large-scale multi-label genomic sequence classification", "abstract": "one of the fundamental tasks in understanding genomics is the problem of predicting transcription factor binding sites ( tfbss ) . with more than hundreds of transcription factors ( tfs ) as labels , genomic-sequence based tfbs prediction is a challenging multi-label classification task . there are two major biological mechanisms for tf binding : ( 1 ) sequence-specific binding patterns on genomes known as `` motifs '' and ( 2 ) interactions among tfs known as co-binding effects . in this paper , we propose a novel deep architecture , the prototype matching network ( pmn ) to mimic the tf binding mechanisms . our pmn model automatically extracts prototypes ( `` motif '' -like features ) for each tf through a novel prototype-matching loss . borrowing ideas from few-shot matching models , we use the notion of support set of prototypes and an lstm to learn how tfs interact and bind to genomic sequences . on a reference tfbs dataset with $ 2.1 $ $ million $ genomic sequences , pmn significantly outperforms baselines and validates our design choices empirically . to our knowledge , this is the first deep learning architecture that introduces prototype learning and considers tf-tf interactions for large-scale tfbs prediction . not only is the proposed architecture accurate , but it also models the underlying biology ."}
{"title": "intelligent search heuristics for cost based scheduling", "abstract": "nurse scheduling is a difficult optimization problem with multiple constraints . there is extensive research in the literature solving the problem using meta-heuristics approaches . in this paper , we will investigate an intelligent search heuristics that handles cost based scheduling problem . the heuristics demonstrated superior performances compared to the original algorithms used to solve the problems described in li et . al . ( 2003 ) and ozkarahan ( 1989 ) in terms of time needed to establish a feasible solution . both problems can be formulated as a cost problem . the search heuristic consists of several phrases of search and input based on the cost of each assignment and how the assignment will interact with the cost of the resources ."}
{"title": "finding inner outliers in high dimensional space", "abstract": "outlier detection in a large-scale database is a significant and complex issue in knowledge discovering field . as the data distributions are obscure and uncertain in high dimensional space , most existing solutions try to solve the issue taking into account the two intuitive points : first , outliers are extremely far away from other points in high dimensional space ; second , outliers are detected obviously different in projected-dimensional subspaces . however , for a complicated case that outliers are hidden inside the normal points in all dimensions , existing detection methods fail to find such inner outliers . in this paper , we propose a method with twice dimension-projections , which integrates primary subspace outlier detection and secondary point-projection between subspaces , and sums up the multiple weight values for each point . the points are computed with local density ratio separately in twice-projected dimensions . after the process , outliers are those points scoring the largest values of weight . the proposed method succeeds to find all inner outliers on the synthetic test datasets with the dimension varying from 100 to 10000. the experimental results also show that the proposed algorithm can work in low dimensional space and can achieve perfect performance in high dimensional space . as for this reason , our proposed approach has considerable potential to apply it in multimedia applications helping to process images or video with large-scale attributes ."}
{"title": "continuous deep q-learning with model-based acceleration", "abstract": "model-free reinforcement learning has been successfully applied to a range of challenging problems , and has recently been extended to handle large neural network policies and value functions . however , the sample complexity of model-free algorithms , particularly when using high-dimensional function approximators , tends to limit their applicability to physical systems . in this paper , we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks . we propose two complementary techniques for improving the efficiency of such algorithms . first , we derive a continuous variant of the q-learning algorithm , which we call normalized adantage functions ( naf ) , as an alternative to the more commonly used policy gradient and actor-critic methods . naf representation allows us to apply q-learning with experience replay to continuous tasks , and substantially improves performance on a set of simulated robotic control tasks . to further improve the efficiency of our approach , we explore the use of learned models for accelerating model-free reinforcement learning . we show that iteratively refitted local linear models are especially effective for this , and demonstrate substantially faster learning on domains where such models are applicable ."}
{"title": "assigning satisfaction values to constraints : an algorithm to solve dynamic meta-constraints", "abstract": "the model of dynamic meta-constraints has special activity constraints which can activate other constraints . it also has meta-constraints which range over other constraints . an algorithm is presented in which constraints can be assigned one of five different satisfaction values , which leads to the assignment of domain values to the variables in the csp . an outline of the model and the algorithm is presented , followed by some initial results for two problems : a simple classic csp and the car configuration problem . the algorithm is shown to perform few backtracks per solution , but to have overheads in the form of historical records required for the implementation of state ."}
{"title": "answer set planning under action costs", "abstract": "recently , planning based on answer set programming has been proposed as an approach towards realizing declarative planning systems . in this paper , we present the language kc , which extends the declarative planning language k by action costs . kc provides the notion of admissible and optimal plans , which are plans whose overall action costs are within a given limit resp . minimum over all plans ( i.e. , cheapest plans ) . as we demonstrate , this novel language allows for expressing some nontrivial planning tasks in a declarative way . furthermore , it can be utilized for representing planning problems under other optimality criteria , such as computing `` shortest '' plans ( with the least number of steps ) , and refinement combinations of cheapest and fastest plans . we study complexity aspects of the language kc and provide a transformation to logic programs , such that planning problems are solved via answer set programming . furthermore , we report experimental results on selected problems . our experience is encouraging that answer set planning may be a valuable approach to expressive planning systems in which intricate planning problems can be naturally specified and solved ."}
{"title": "asymmetric move selection strategies in monte-carlo tree search : minimizing the simple regret at max nodes", "abstract": "the combination of multi-armed bandit ( mab ) algorithms with monte-carlo tree search ( mcts ) has made a significant impact in various research fields . the uct algorithm , which combines the ucb bandit algorithm with mcts , is a good example of the success of this combination . the recent breakthrough made by alphago , which incorporates convolutional neural networks with bandit algorithms in mcts , also highlights the necessity of bandit algorithms in mcts . however , despite the various investigations carried out on mcts , nearly all of them still follow the paradigm of treating every node as an independent instance of the mab problem , and applying the same bandit algorithm and heuristics on every node . as a result , this paradigm may leave some properties of the game tree unexploited . in this work , we propose that max nodes and min nodes have different concerns regarding their value estimation , and different bandit algorithms should be applied accordingly . we develop the asymmetric-mcts algorithm , which is an mcts variant that applies a simple regret algorithm on max nodes , and the ucb algorithm on min nodes . we will demonstrate the performance of the asymmetric-mcts algorithm on the game of $ 9\\times 9 $ go , $ 9\\times 9 $ nogo , and othello ."}
{"title": "belief revision in the propositional closure of a qualitative algebra", "abstract": "belief revision is an operation that aims at modifying old be-liefs so that they become consistent with new ones . the issue of belief revision has been studied in various formalisms , in particular , in qualitative algebras ( qas ) in which the result is a disjunction of belief bases that is not necessarily repre-sentable in a qa . this motivates the study of belief revision in formalisms extending qas , namely , their propositional clo-sures : in such a closure , the result of belief revision belongs to the formalism . moreover , this makes it possible to define a contraction operator thanks to the harper identity . belief revision in the propositional closure of qas is studied , an al-gorithm for a family of revision operators is designed , and an open-source implementation is made freely available on the web ."}
{"title": "applications of data mining ( dm ) in science and engineering : state of the art and perspectives", "abstract": "the continuous increase in the availability of data of any kind , coupled with the development of networks of high-speed communications , the popularization of cloud computing and the growth of data centers and the emergence of high-performance computing does essential the task to develop techniques that allow more efficient data processing and analyzing of large volumes datasets and extraction of valuable information . in the following pages we will discuss about development of this field in recent decades , and its potential and applicability present in the various branches of scientific research . also , we try to review briefly the different families of algorithms that are included in data mining research area , its scalability with increasing dimensionality of the input data and how they can be addressed and what behavior different methods in a scenario in which the information is distributed or decentralized processed so as to increment performance optimization in heterogeneous environments ."}
{"title": "learning undirected graphical models with structure penalty", "abstract": "in undirected graphical models , learning the graph structure and learning the functions that relate the predictive variables ( features ) to the responses given the structure are two topics that have been widely investigated in machine learning and statistics . learning graphical models in two stages will have problems because graph structure may change after considering the features . the main contribution of this paper is the proposed method that learns the graph structure and functions on the graph at the same time . general graphical models with binary outcomes conditioned on predictive variables are proved to be equivalent to multivariate bernoulli model . the reparameterization of the potential functions in graphical model by conditional log odds ratios in multivariate bernoulli model offers advantage in the representation of the conditional independence structure in the model . additionally , we impose a structure penalty on groups of conditional log odds ratios to learn the graph structure . these groups of functions are designed with overlaps to enforce hierarchical function selection . in this way , we are able to shrink higher order interactions to obtain a sparse graph structure . simulation studies show that the method is able to recover the graph structure . the analysis of county data from census bureau gives interesting relations between unemployment rate , crime and others discovered by the model ."}
{"title": "stochastic global optimization algorithms : a systematic formal approach", "abstract": "as we know , some global optimization problems can not be solved using analytic methods , so numeric/algorithmic approaches are used to find near to the optimal solutions for them . a stochastic global optimization algorithm ( sgoal ) is an iterative algorithm that generates a new population ( a set of candidate solutions ) from a previous population using stochastic operations . although some research works have formalized sgoals using markov kernels , such formalization is not general and sometimes is blurred . in this paper , we propose a comprehensive and systematic formal approach for studying sgoals . first , we present the required theory of probability ( \\sigma-algebras , measurable functions , kernel , markov chain , products , convergence and so on ) and prove that some algorithmic functions like swapping and projection can be represented by kernels . then , we introduce the notion of join-kernel as a way of characterizing the combination of stochastic methods . next , we define the optimization space , a formal structure ( a set with a \\sigma-algebra that contains strict \\epsilon-optimal states ) for studying sgoals , and we develop kernels , like sort and permutation , on such structure . finally , we present some popular sgoals in terms of the developed theory , we introduce sufficient conditions for convergence of a sgoal , and we prove convergence of some popular sgoals ."}
{"title": "the information-collecting vehicle routing problem : stochastic optimization for emergency storm response", "abstract": "utilities face the challenge of responding to power outages due to storms and ice damage , but most power grids are not equipped with sensors to pinpoint the precise location of the faults causing the outage . instead , utilities have to depend primarily on phone calls ( trouble calls ) from customers who have lost power to guide the dispatching of utility trucks . in this paper , we develop a policy that routes a utility truck to restore outages in the power grid as quickly as possible , using phone calls to create beliefs about outages , but also using utility trucks as a mechanism for collecting additional information . this means that routing decisions change not only the physical state of the truck ( as it moves from one location to another ) and the grid ( as the truck performs repairs ) , but also our belief about the network , creating the first stochastic vehicle routing problem that explicitly models information collection and belief modeling . we address the problem of managing a single utility truck , which we start by formulating as a sequential stochastic optimization model which captures our belief about the state of the grid . we propose a stochastic lookahead policy , and use monte carlo tree search ( mcts ) to produce a practical policy that is asymptotically optimal . simulation results show that the developed policy restores the power grid much faster compared to standard industry heuristics ."}
{"title": "fault classification in cylinders using multilayer perceptrons , support vector machines and guassian mixture models", "abstract": "gaussian mixture models ( gmm ) and support vector machines ( svm ) are introduced to classify faults in a population of cylindrical shells . the proposed procedures are tested on a population of 20 cylindrical shells and their performance is compared to the procedure , which uses multi-layer perceptrons ( mlp ) . the modal properties extracted from vibration data are used to train the gmm , svm and mlp . it is observed that the gmm produces 98 % , svm produces 94 % classification accuracy while the mlp produces 88 % classification rates ."}
{"title": "order-of-magnitude influence diagrams", "abstract": "in this paper , we develop a qualitative theory of influence diagrams that can be used to model and solve sequential decision making tasks when only qualitative ( or imprecise ) information is available . our approach is based on an order-of-magnitude approximation of both probabilities and utilities and allows for specifying partially ordered preferences via sets of utility values . we also propose a dedicated variable elimination algorithm that can be applied for solving order-of-magnitude influence diagrams ."}
{"title": "match-srnn : modeling the recursive matching structure with spatial rnn", "abstract": "semantic matching , which aims to determine the matching degree between two texts , is a fundamental problem for many nlp applications . recently , deep learning approach has been applied to this problem and significant improvements have been achieved . in this paper , we propose to view the generation of the global interaction between two texts as a recursive process : i.e . the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position . based on this idea , we propose a novel deep architecture , namely match-srnn , to model the recursive matching structure . firstly , a tensor is constructed to capture the word level interactions . then a spatial rnn is applied to integrate the local interactions recursively , with importance determined by four types of gates . finally , the matching score is calculated based on the global interaction . we show that , after degenerated to the exact matching scenario , match-srnn can approximate the dynamic programming process of longest common subsequence . thus , there exists a clear interpretation for match-srnn . our experiments on two semantic matching tasks showed the effectiveness of match-srnn , and its ability of visualizing the learned matching structure ."}
{"title": "evidential reasoning in parallel hierarchical vision programs", "abstract": "this paper presents an efficient adaptation and application of the dempster-shafer theory of evidence , one that can be used effectively in a massively parallel hierarchical system for visual pattern perception . it describes the techniques used , and shows in an extended example how they serve to improve the system 's performance as it applies a multiple-level set of processes ."}
{"title": "specification inference from demonstrations", "abstract": "learning from expert demonstrations has received a lot of attention in artificial intelligence and machine learning . the goal is to infer the underlying reward function that an agent is optimizing given a set of observations of the agent 's behavior over time in a variety of circumstances , the system state trajectories , and a plant model specifying the evolution of the system state for different agent 's actions . the system is often modeled as a markov decision process , that is , the next state depends only on the current state and agent 's action , and the the agent 's choice of action depends only on the current state . while the former is a markovian assumption on the evolution of system state , the later assumes that the target reward function is itself markovian . in this work , we explore learning a class of non-markovian reward functions , known in the formal methods literature as specifications . these specifications offer better composition , transferability , and interpretability . we then show that inferring the specification can be done efficiently without unrolling the transition system . we demonstrate on a 2-d grid world example ."}
{"title": "ikbt : solving closed-form inverse kinematics with behavior tree", "abstract": "serial robot arms have complicated kinematic equations which must be solved to write effective arm planning and control software ( the inverse kinematics problem ) . existing software packages for inverse kinematics often rely on numerical methods which have significant shortcomings . here we report a new symbolic inverse kinematics solver which overcomes the limitations of numerical methods , and the shortcomings of previous symbolic software packages . we integrate behavior trees , an execution planning framework previously used for controlling intelligent robot behavior , to organize the equation solving process , and a modular architecture for each solution technique . the system successfully solved , generated a latex report , and generated a python code template for 18 out of 19 example robots of 4-6 dof . the system is readily extensible , maintainable , and multi-platform with few dependencies . the complete package is available with a modified bsd license on github ."}
{"title": "learning to learn with backpropagation of hebbian plasticity", "abstract": "hebbian plasticity is a powerful principle that allows biological brains to learn from their lifetime experience . by contrast , artificial neural networks trained with backpropagation generally have fixed connection weights that do not change once training is complete . while recent methods can endow neural networks with long-term memories , hebbian plasticity is currently not amenable to gradient descent . here we derive analytical expressions for activity gradients in neural networks with hebbian plastic connections . using these expressions , we can use backpropagation to train not just the baseline weights of the connections , but also their plasticity . as a result , the networks `` learn how to learn '' in order to solve the problem at hand : the trained networks automatically perform fast learning of unpredictable environmental features during their lifetime , expanding the range of solvable problems . we test the algorithm on various on-line learning tasks , including pattern completion , one-shot learning , and reversal learning . the algorithm successfully learns how to learn the relevant associations from one-shot instruction , and fine-tunes the temporal dynamics of plasticity to allow for continual learning in response to changing environmental parameters . we conclude that backpropagation of hebbian plasticity offers a powerful model for lifelong learning ."}
{"title": "a survey on independence-based markov networks learning", "abstract": "this work reports the most relevant technical aspects in the problem of learning the \\emph { markov network structure } from data . such problem has become increasingly important in machine learning , and many other application fields of machine learning . markov networks , together with bayesian networks , are probabilistic graphical models , a widely used formalism for handling probability distributions in intelligent systems . learning graphical models from data have been extensively applied for the case of bayesian networks , but for markov networks learning it is not tractable in practice . however , this situation is changing with time , given the exponential growth of computers capacity , the plethora of available digital data , and the researching on new learning technologies . this work stresses on a technology called independence-based learning , which allows the learning of the independence structure of those networks from data in an efficient and sound manner , whenever the dataset is sufficiently large , and data is a representative sampling of the target distribution . in the analysis of such technology , this work surveys the current state-of-the-art algorithms for learning markov networks structure , discussing its current limitations , and proposing a series of open problems where future works may produce some advances in the area in terms of quality and efficiency . the paper concludes by opening a discussion about how to develop a general formalism for improving the quality of the structures learned , when data is scarce ."}
{"title": "online event recognition from moving vessel trajectories", "abstract": "we present a system for online monitoring of maritime activity over streaming positions from numerous vessels sailing at sea . it employs an online tracking module for detecting important changes in the evolving trajectory of each vessel across time , and thus can incrementally retain concise , yet reliable summaries of its recent movement . in addition , thanks to its complex event recognition module , this system can also offer instant notification to marine authorities regarding emergency situations , such as risk of collisions , suspicious moves in protected zones , or package picking at open sea . not only did our extensive tests validate the performance , efficiency , and robustness of the system against scalable volumes of real-world and synthetically enlarged datasets , but its deployment against online feeds from vessels has also confirmed its capabilities for effective , real-time maritime surveillance ."}
{"title": "learning model-based planning from scratch", "abstract": "conventional wisdom holds that model-based planning is a powerful approach to sequential decision-making . it is often very challenging in practice , however , because while a model can be used to evaluate a plan , it does not prescribe how to construct a plan . here we introduce the `` imagination-based planner '' , the first model-based , sequential decision-making agent that can learn to construct , evaluate , and execute plans . before any action , it can perform a variable number of imagination steps , which involve proposing an imagined action and evaluating it with its model-based imagination . all imagined actions and outcomes are aggregated , iteratively , into a `` plan context '' which conditions future real and imagined actions . the agent can even decide how to imagine : testing out alternative imagined actions , chaining sequences of actions together , or building a more complex `` imagination tree '' by navigating flexibly among the previously imagined states using a learned policy . and our agent can learn to plan economically , jointly optimizing for external rewards and computational costs associated with using its imagination . we show that our architecture can learn to solve a challenging continuous control problem , and also learn elaborate planning strategies in a discrete maze-solving task . our work opens a new direction toward learning the components of a model-based planning system and how to use them ."}
{"title": "pattern-based classification : a unifying perspective", "abstract": "the use of patterns in predictive models is a topic that has received a lot of attention in recent years . pattern mining can help to obtain models for structured domains , such as graphs and sequences , and has been proposed as a means to obtain more accurate and more interpretable models . despite the large amount of publications devoted to this topic , we believe however that an overview of what has been accomplished in this area is missing . this paper presents our perspective on this evolving area . we identify the principles of pattern mining that are important when mining patterns for models and provide an overview of pattern-based classification methods . we categorize these methods along the following dimensions : ( 1 ) whether they post-process a pre-computed set of patterns or iteratively execute pattern mining algorithms ; ( 2 ) whether they select patterns model-independently or whether the pattern selection is guided by a model . we summarize the results that have been obtained for each of these methods ."}
{"title": "system dynamics modelling of the processes involving the maintenance of the naive t cell repertoire", "abstract": "the study of immune system aging , i.e . immunosenescence , is a relatively new research topic . it deals with understanding the processes of immunodegradation that indicate signs of functionality loss possibly leading to death . even though it is not possible to prevent immunosenescence , there is great benefit in comprehending its causes , which may help to reverse some of the damage done and thus improve life expectancy . one of the main factors influencing the process of immunosenescence is the number and phenotypical variety of naive t cells in an individual . this work presents a review of immunosenescence , proposes system dynamics modelling of the processes involving the maintenance of the naive t cell repertoire and presents some preliminary results ."}
{"title": "distributed robust subspace recovery", "abstract": "we study robust subspace recovery ( rsr ) in distributed settings . we consider a huge data set in an ad hoc network without a central processor , where each node has access only to one chunk of the data set . we assume that part of the whole data set lies around a low-dimensional subspace and the other part is composed of outliers that lie away from that subspace . the goal is to recover the underlying subspace for the whole data set , without transferring the data itself between the nodes . we apply the consensus based gradient method for the geometric median subspace algorithm for rsr . we propose an iterative solution for the local dual minimization problem and establish its $ r $ -linear convergence . we show that this mathematical framework also extends to two simpler problems : principal component analysis and the geometric median . we also explain how to distributedly implement the reaper and fast median subspace algorithms for rsr . we demonstrate the competitive performance of our algorithms for both synthetic and real data ."}
{"title": "on the scaling window of model rb", "abstract": "this paper analyzes the scaling window of a random csp model ( i.e . model rb ) for which we can identify the threshold points exactly , denoted by $ r_ { cr } $ or $ p_ { cr } $ . for this model , we establish the scaling window $ w ( n , \\delta ) = ( r_ { - } ( n , \\delta ) , r_ { + } ( n , \\delta ) ) $ such that the probability of a random instance being satisfiable is greater than $ 1-\\delta $ for $ r < r_ { - } ( n , \\delta ) $ and is less than $ \\delta $ for $ r > r_ { + } ( n , \\delta ) $ . specifically , we obtain the following result $ $ w ( n , \\delta ) = ( r_ { cr } -\\theta ( \\frac { 1 } { n^ { 1-\\epsilon } \\ln n } ) , \\ r_ { cr } +\\theta ( \\frac { 1 } { n\\ln n } ) ) , $ $ where $ 0\\leq\\epsilon < 1 $ is a constant . a similar result with respect to the other parameter $ p $ is also obtained . since the instances generated by model rb have been shown to be hard at the threshold , this is the first attempt , as far as we know , to analyze the scaling window of such a model with hard instances ."}
{"title": "maximum entropy models for generation of expressive music", "abstract": "in the context of contemporary monophonic music , expression can be seen as the difference between a musical performance and its symbolic representation , i.e . a musical score . in this paper , we show how maximum entropy ( maxent ) models can be used to generate musical expression in order to mimic a human performance . as a training corpus , we had a professional pianist play about 150 melodies of jazz , pop , and latin jazz . the results show a good predictive power , validating the choice of our model . additionally , we set up a listening test whose results reveal that on average , people significantly prefer the melodies generated by the maxent model than the ones without any expression , or with fully random expression . furthermore , in some cases , maxent melodies are almost as popular as the human performed ones ."}
{"title": "formula-based probabilistic inference", "abstract": "computing the probability of a formula given the probabilities or weights associated with other formulas is a natural extension of logical inference to the probabilistic setting . surprisingly , this problem has received little attention in the literature to date , particularly considering that it includes many standard inference problems as special cases . in this paper , we propose two algorithms for this problem : formula decomposition and conditioning , which is an exact method , and formula importance sampling , which is an approximate method . the latter is , to our knowledge , the first application of model counting to approximate probabilistic inference . unlike conventional variable-based algorithms , our algorithms work in the dual realm of logical formulas . theoretically , we show that our algorithms can greatly improve efficiency by exploiting the structural information in the formulas . empirically , we show that they are indeed quite powerful , often achieving substantial performance gains over state-of-the-art schemes ."}
{"title": "sufficient and necessary causation are dual", "abstract": "causation has been the issue of philosophic debate since hippocrates . recent work defines actual causation in terms of pearl/halpern 's causality framework , formalizing necessary causes ( ijcai'15 ) . this has inspired causality notions in the security domain ( csf'15 ) , which , perhaps surprisingly , formalize sufficient causes instead . we provide an explicit relation between necessary and sufficient causes ."}
{"title": "hybrid model for solving multi-objective problems using evolutionary algorithm and tabu search", "abstract": "this paper presents a new multi-objective hybrid model that makes cooperation between the strength of research of neighborhood methods presented by the tabu search ( ts ) and the important exploration capacity of evolutionary algorithm . this model was implemented and tested in benchmark functions ( zdt1 , zdt2 , and zdt3 ) , using a network of computers ."}
{"title": "joint extraction of entities and relations based on a novel tagging scheme", "abstract": "joint extraction of entities and relations is an important task in information extraction . to tackle this problem , we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem . then , based on our tagging scheme , we study different end-to-end models to extract entities and their relations directly , without identifying entities and relations separately . we conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods . what 's more , the end-to-end model proposed in this paper , achieves the best results on the public dataset ."}
{"title": "understanding model counting for $ \u03b2 $ -acyclic cnf-formulas", "abstract": "we extend the knowledge about so-called structural restrictions of $ \\mathrm { \\ # sat } $ by giving a polynomial time algorithm for $ \\beta $ -acyclic $ \\mathrm { \\ # sat } $ . in contrast to previous algorithms in the area , our algorithm does not proceed by dynamic programming but works along an elimination order , solving a weighted version of constraint satisfaction . moreover , we give evidence that this deviation from more standard algorithm is not a coincidence , but that there is likely no dynamic programming algorithm of the usual style for $ \\beta $ -acyclic $ \\mathrm { \\ # sat } $ ."}
{"title": "induction of first-order decision lists : results on learning the past tense of english verbs", "abstract": "this paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists , defined as ordered lists of clauses each ending in a cut . the method , called foidl , is based on foil ( quinlan , 1990 ) but employs intensional background knowledge and avoids the need for explicit negative examples . it is particularly useful for problems that involve rules with specific exceptions , such as learning the past-tense of english verbs , a task widely studied in the context of the symbolic/connectionist debate . foidl is able to learn concise , accurate programs for this problem from significantly fewer examples than previous methods ( both connectionist and symbolic ) ."}
{"title": "answer set solving with bounded treewidth revisited", "abstract": "parameterized algorithms are a way to solve hard problems more efficiently , given that a specific parameter of the input is small . in this paper , we apply this idea to the field of answer set programming ( asp ) . to this end , we propose two kinds of graph representations of programs to exploit their treewidth as a parameter . treewidth roughly measures to which extent the internal structure of a program resembles a tree . our main contribution is the design of parameterized dynamic programming algorithms , which run in linear time if the treewidth and weights of the given program are bounded . compared to previous work , our algorithms handle the full syntax of asp . finally , we report on an empirical evaluation that shows good runtime behaviour for benchmark instances of low treewidth , especially for counting answer sets ."}
{"title": "robust propensity score computation method based on machine learning with label-corrupted data", "abstract": "in biostatistics , propensity score is a common approach to analyze the imbalance of covariate and process confounding covariates to eliminate differences between groups . while there are an abundant amount of methods to compute propensity score , a common issue of them is the corrupted labels in the dataset . for example , the data collected from the patients could contain samples that are treated mistakenly , and the computing methods could incorporate them as a misleading information . in this paper , we propose a machine learning-based method to handle the problem . specifically , we utilize the fact that the majority of sample should be labeled with the correct instance and design an approach to first cluster the data with spectral clustering and then sample a new dataset with a distribution processed from the clustering results . the propensity score is computed by xgboost , and a mathematical justification of our method is provided in this paper . the experimental results illustrate that xgboost propensity scores computing with the data processed by our method could outperform the same method with original data , and the advantages of our method increases as we add some artificial corruptions to the dataset . meanwhile , the implementation of xgboost to compute propensity score for multiple treatments is also a pioneering work in the area ."}
{"title": "a new algorithm for identity verification based on the analysis of a handwritten dynamic signature", "abstract": "identity verification based on authenticity assessment of a handwritten signature is an important issue in biometrics . there are many effective methods for signature verification taking into account dynamics of a signing process . methods based on partitioning take a very important place among them . in this paper we propose a new approach to signature partitioning . its most important feature is the possibility of selecting and processing of hybrid partitions in order to increase a precision of the test signature analysis . partitions are formed by a combination of vertical and horizontal sections of the signature . vertical sections correspond to the initial , middle , and final time moments of the signing process . in turn , horizontal sections correspond to the signature areas associated with high and low pen velocity and high and low pen pressure on the surface of a graphics tablet . our previous research on vertical and horizontal sections of the dynamic signature ( created independently ) led us to develop the algorithm presented in this paper . selection of sections , among others , allows us to define the stability of the signing process in the partitions , promoting signature areas of greater stability ( and vice versa ) . in the test of the proposed method two databases were used : public mcyt-100 and paid biosecure ."}
{"title": "saliency weighted convolutional features for instance search", "abstract": "this work explores attention models to weight the contribution of local convolutional representations for the instance search task . we present a retrieval framework based on bags of local convolutional features ( blcf ) that benefits from saliency weighting to build an efficient image representation . the use of human visual attention models ( saliency ) allows significant improvements in retrieval performance without the need to conduct region analysis or spatial verification , and without requiring any feature fine tuning . we investigate the impact of different saliency models , finding that higher performance on saliency benchmarks does not necessarily equate to improved performance when used in instance search tasks . the proposed approach outperforms the state-of-the-art on the challenging instre benchmark by a large margin , and provides similar performance on the oxford and paris benchmarks compared to more complex methods that use off-the-shelf representations . the source code used in this project is available at https : //imatge-upc.github.io/salbow/"}
{"title": "a comprehensive performance evaluation of deformable face tracking `` in-the-wild ''", "abstract": "recently , technologies such as face detection , facial landmark localisation and face recognition and verification have matured enough to provide effective and efficient solutions for imagery captured under arbitrary conditions ( referred to as `` in-the-wild '' ) . this is partially attributed to the fact that comprehensive `` in-the-wild '' benchmarks have been developed for face detection , landmark localisation and recognition/verification . a very important technology that has not been thoroughly evaluated yet is deformable face tracking `` in-the-wild '' . until now , the performance has mainly been assessed qualitatively by visually assessing the result of a deformable face tracking technology on short videos . in this paper , we perform the first , to the best of our knowledge , thorough evaluation of state-of-the-art deformable face tracking pipelines using the recently introduced 300vw benchmark . we evaluate many different architectures focusing mainly on the task of on-line deformable face tracking . in particular , we compare the following general strategies : ( a ) generic face detection plus generic facial landmark localisation , ( b ) generic model free tracking plus generic facial landmark localisation , as well as ( c ) hybrid approaches using state-of-the-art face detection , model free tracking and facial landmark localisation technologies . our evaluation reveals future avenues for further research on the topic ."}
{"title": "rational competitive analysis", "abstract": "much work in computer science has adopted competitive analysis as a tool for decision making under uncertainty . in this work we extend competitive analysis to the context of multi-agent systems . unlike classical competitive analysis where the behavior of an agent 's environment is taken to be arbitrary , we consider the case where an agent 's environment consists of other agents . these agents will usually obey some ( minimal ) rationality constraints . this leads to the definition of rational competitive analysis . we introduce the concept of rational competitive analysis , and initiate the study of competitive analysis for multi-agent systems . we also discuss the application of rational competitive analysis to the context of bidding games , as well as to the classical one-way trading problem ."}
{"title": "man is to computer programmer as woman is to homemaker ? debiasing word embeddings", "abstract": "the blind application of machine learning runs the risk of amplifying biases present in data . such a danger is facing us with word embedding , a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks . we show that even word embeddings trained on google news articles exhibit female/male gender stereotypes to a disturbing extent . this raises concerns because their widespread use , as we describe , often tends to amplify these biases . geometrically , gender bias is first shown to be captured by a direction in the word embedding . second , gender neutral words are shown to be linearly separable from gender definition words in the word embedding . using these properties , we provide a methodology for modifying an embedding to remove gender stereotypes , such as the association between between the words receptionist and female , while maintaining desired associations such as between the words queen and female . we define metrics to quantify both direct and indirect gender biases in embeddings , and develop algorithms to `` debias '' the embedding . using crowd-worker evaluation as well as standard benchmarks , we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks . the resulting embeddings can be used in applications without amplifying gender bias ."}
{"title": "predicting performance during tutoring with models of recent performance", "abstract": "in educational technology and learning sciences , there are multiple uses for a predictive model of whether a student will perform a task correctly or not . for example , an intelligent tutoring system may use such a model to estimate whether or not a student has mastered a skill . we analyze the significance of data recency in making such predictions , i.e. , asking whether relatively more recent observations of a student 's performance matter more than relatively older observations . we develop a new recent-performance factors analysis model that takes data recency into account . the new model significantly improves predictive accuracy over both existing logistic-regression performance models and over novel baseline models in evaluations on real-world and synthetic datasets . as a secondary contribution , we demonstrate how the widely used cross-validation with 0-1 loss is inferior to aic and to cross-validation with l1 prediction error loss as a measure of model performance ."}
{"title": "a mip backend for the idp system", "abstract": "the idp knowledge base system currently uses minisat ( id ) as its backend constraint programming ( cp ) solver . a few similar systems have used a mixed integer programming ( mip ) solver as backend . however , so far little is known about when the mip solver is preferable . this paper explores this question . it describes the use of cplex as a backend for idp and reports on experiments comparing both backends ."}
{"title": "logic programming approaches for representing and solving constraint satisfaction problems : a comparison", "abstract": "many logic programming based approaches can be used to describe and solve combinatorial search problems . on the one hand there is constraint logic programming which computes a solution as an answer substitution to a query containing the variables of the constraint satisfaction problem . on the other hand there are systems based on stable model semantics , abductive systems , and first order logic model generators which compute solutions as models of some theory . this paper compares these different approaches from the point of view of knowledge representation ( how declarative are the programs ) and from the point of view of performance ( how good are they at solving typical problems ) ."}
{"title": "monaural audio speaker separation with source contrastive estimation", "abstract": "we propose an algorithm to separate simultaneously speaking persons from each other , the `` cocktail party problem '' , using a single microphone . our approach involves a deep recurrent neural networks regression to a vector space that is descriptive of independent speakers . such a vector space can embed empirically determined speaker characteristics and is optimized by distinguishing between speaker masks . we call this technique source-contrastive estimation . the methodology is inspired by negative sampling , which has seen success in natural language processing , where an embedding is learned by correlating and de-correlating a given input vector with output weights . although the matrix determined by the output weights is dependent on a set of known speakers , we only use the input vectors during inference . doing so will ensure that source separation is explicitly speaker-independent . our approach is similar to recent deep neural network clustering and permutation-invariant training research ; we use weighted spectral features and masks to augment individual speaker frequencies while filtering out other speakers . we avoid , however , the severe computational burden of other approaches with our technique . furthermore , by training a vector space rather than combinations of different speakers or differences thereof , we avoid the so-called permutation problem during training . our algorithm offers an intuitive , computationally efficient response to the cocktail party problem , and most importantly boasts better empirical performance than other current techniques ."}
{"title": "valuation-based systems for discrete optimization", "abstract": "this paper describes valuation-based systems for representing and solving discrete optimization problems . in valuation-based systems , we represent information in an optimization problem using variables , sample spaces of variables , a set of values , and functions that map sample spaces of sets of variables to the set of values . the functions , called valuations , represent the factors of an objective function . solving the optimization problem involves using two operations called combination and marginalization . combination tells us how to combine the factors of the joint objective function . marginalization is either maximization or minimization . solving an optimization problem can be simply described as finding the marginal of the joint objective function for the empty set . we state some simple axioms that combination and marginalization need to satisfy to enable us to solve an optimization problem using local computation . for optimization problems , the solution method of valuation-based systems reduces to non-serial dynamic programming . thus our solution method for vbs can be regarded as an abstract description of dynamic programming . and our axioms can be viewed as conditions that permit the use of dynamic programming ."}
{"title": "proceedings of the twenty-second conference on uncertainty in artificial intelligence ( 2006 )", "abstract": "this is the proceedings of the twenty-second conference on uncertainty in artificial intelligence , which was held in cambridge , ma , july 13 - 16 2006 ."}
{"title": "quantum cognition goes beyond-quantum : modeling the collective participant in psychological measurements", "abstract": "in psychological measurements , two levels should be distinguished : the 'individual level ' , relative to the different participants in a given cognitive situation , and the 'collective level ' , relative to the overall statistics of their outcomes , which we propose to associate with a notion of 'collective participant ' . when the distinction between these two levels is properly formalized , it reveals why the modeling of the collective participant generally requires beyond-quantum - non-bornian - probabilistic models , when sequential measurements at the individual level are considered , and this though a pure quantum description remains valid for single measurement situations ."}
{"title": "low-rank and sparse soft targets to learn better dnn acoustic models", "abstract": "conventional deep neural networks ( dnn ) for speech acoustic modeling rely on gaussian mixture models ( gmm ) and hidden markov model ( hmm ) to obtain binary class labels as the targets for dnn training . subword classes in speech recognition systems correspond to context-dependent tied states or senones . the present work addresses some limitations of gmm-hmm senone alignments for dnn training . we hypothesize that the senone probabilities obtained from a dnn trained with binary labels can provide more accurate targets to learn better acoustic models . however , dnn outputs bear inaccuracies which are exhibited as high dimensional unstructured noise , whereas the informative components are structured and low-dimensional . we exploit principle component analysis ( pca ) and sparse coding to characterize the senone subspaces . enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for dnn acoustic modeling , that also enables training with untranscribed data . experiments conducted on ami corpus shows 4.6 % relative reduction in word error rate ."}
{"title": "fuzzy rankings : properties and applications", "abstract": "in practice , a ranking of objects with respect to given set of criteria is of considerable importance . however , due to lack of knowledge , information of time pressure , decision makers might not be able to provide a ( crisp ) ranking of objects from the top to the bottom . instead , some objects might be ranked equally , or better than other objects only to some degree . in such cases , a generalization of crisp rankings to fuzzy rankings can be more useful . the aim of the article is to introduce the notion of a fuzzy ranking and to discuss its several properties , namely orderings , similarity and indecisiveness . the proposed approach can be used both for group decision making or multiple criteria decision making when uncertainty is involved ."}
{"title": "why pairdiff works ? -- a mathematical analysis of bilinear relational compositional operators for analogy detection", "abstract": "representing the semantic relations that exist between two given words ( or entities ) is an important first step in a wide-range of nlp applications such as analogical reasoning , knowledge base completion and relational information retrieval . a simple , yet surprisingly accurate method for representing a relation between two words is to compute the vector offset ( \\pairdiff ) between their corresponding word embeddings . despite the empirical success , it remains unclear as to whether \\pairdiff is the best operator for obtaining a relational representation from word embeddings . we conduct a theoretical analysis of generalised bilinear operators that can be used to measure the $ \\ell_ { 2 } $ relational distance between two word-pairs . we show that , if the word embeddings are standardised and uncorrelated , such an operator will be independent of bilinear terms , and can be simplified to a linear form , where \\pairdiff is a special case . for numerous word embedding types , we empirically verify the uncorrelation assumption , demonstrating the general applicability of our theoretical result . moreover , we experimentally discover \\pairdiff from the bilinear relation composition operator on several benchmark analogy datasets ."}
{"title": "asynchronous decentralized algorithm for space-time cooperative pathfinding", "abstract": "cooperative pathfinding is a multi-agent path planning problem where a group of vehicles searches for a corresponding set of non-conflicting space-time trajectories . many of the practical methods for centralized solving of cooperative pathfinding problems are based on the prioritized planning strategy . however , in some domains ( e.g. , multi-robot teams of unmanned aerial vehicles , autonomous underwater vehicles , or unmanned ground vehicles ) a decentralized approach may be more desirable than a centralized one due to communication limitations imposed by the domain and/or privacy concerns . in this paper we present an asynchronous decentralized variant of prioritized planning adpp and its interruptible version iadpp . the algorithm exploits the inherent parallelism of distributed systems and allows for a speed up of the computation process . unlike the synchronized planning approaches , the algorithm allows an agent to react to updates about other agents ' paths immediately and invoke its local spatio-temporal path planner to find the best trajectory , as response to the other agents ' choices . we provide a proof of correctness of the algorithms and experimentally evaluate them on synthetic domains ."}
{"title": "accounting for hidden common causes when inferring cause and effect from observational data", "abstract": "identifying causal relationships from observation data is difficult , in large part , due to the presence of hidden common causes . in some cases , where just the right patterns of conditional independence and dependence lie in the data -- -for example , y-structures -- -it is possible to identify cause and effect . in other cases , the analyst deliberately makes an uncertain assumption that hidden common causes are absent , and infers putative causal relationships to be tested in a randomized trial . here , we consider a third approach , where there are sufficient clues in the data such that hidden common causes can be inferred ."}
{"title": "integrating active sensing into reactive synthesis with temporal logic constraints under partial observations", "abstract": "we introduce the notion of online reactive planning with sensing actions for systems with temporal logic constraints in partially observable and dynamic environments . with incomplete information on the dynamic environment , reactive controller synthesis amounts to solving a two-player game with partial observations , which has impractically computational complexity . to alleviate the high computational burden , online replanning via sensing actions avoids solving the strategy in the reactive system under partial observations . instead , we only solve for a strategy that ensures a given temporal logic specification can be satisfied had the system have complete observations of its environment . such a strategy is then transformed into one which makes control decisions based on the observed sequence of states ( of the interacting system and its environment ) . when the system encounters a belief -- -a set including all possible hypotheses the system has for the current state -- -for which the observation-based strategy is undefined , a sequence of sensing actions are triggered , chosen by an active sensing strategy , to reduce the uncertainty in the system 's belief . we show that by alternating between the observation-based strategy and the active sensing strategy , under a mild technical assumption of the set of sensors in the system , the given temporal logic specification can be satisfied with probability 1 ."}
{"title": "anusaaraka : machine translation in stages", "abstract": "fully-automatic general-purpose high-quality machine translation systems ( fgh-mt ) are extremely difficult to build . in fact , there is no system in the world for any pair of languages which qualifies to be called fgh-mt . the reasons are not far to seek . translation is a creative process which involves interpretation of the given text by the translator . translation would also vary depending on the audience and the purpose for which it is meant . this would explain the difficulty of building a machine translation system . since , the machine is not capable of interpreting a general text with sufficient accuracy automatically at present - let alone re-expressing it for a given audience , it fails to perform as fgh-mt . footnote { the major difficulty that the machine faces in interpreting a given text is the lack of general world knowledge or common sense knowledge . }"}
{"title": "learning bayesian network equivalence classes with ant colony optimization", "abstract": "bayesian networks are a useful tool in the representation of uncertain knowledge . this paper proposes a new algorithm called aco-e , to learn the structure of a bayesian network . it does this by conducting a search through the space of equivalence classes of bayesian networks using ant colony optimization ( aco ) . to this end , two novel extensions of traditional aco techniques are proposed and implemented . firstly , multiple types of moves are allowed . secondly , moves can be given in terms of indices that are not based on construction graph nodes . the results of testing show that aco-e performs better than a greedy search and other state-of-the-art and metaheuristic algorithms whilst searching in the space of equivalence classes ."}
{"title": "improved use of continuous attributes in c4.5", "abstract": "a reported weakness of c4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes . an mdl-inspired penalty is applied to such tests , eliminating some of them from consideration and altering the relative desirability of all tests . empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies . results also confirm that a new version of c4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits ."}
{"title": "support vector machine in prediction of building energy demand using pseudo dynamic approach", "abstract": "building 's energy consumption prediction is a major concern in the recent years and many efforts have been achieved in order to improve the energy management of buildings . in particular , the prediction of energy consumption in building is essential for the energy operator to build an optimal operating strategy , which could be integrated to building 's energy management system ( bems ) . this paper proposes a prediction model for building energy consumption using support vector machine ( svm ) . data-driven model , for instance , svm is very sensitive to the selection of training data . thus the relevant days data selection method based on dynamic time warping is used to train svm model . in addition , to encompass thermal inertia of building , pseudo dynamic model is applied since it takes into account information of transition of energy consumption effects and occupancy profile . relevant days data selection and whole training data model is applied to the case studies of ecole des mines de nantes , france office building . the results showed that support vector machine based on relevant data selection method is able to predict the energy consumption of building with a high accuracy in compare to whole data training . in addition , relevant data selection method is computationally cheaper ( around 8 minute training time ) in contrast to whole data training ( around 31 hour for weekend and 116 hour for working days ) and reveals realistic control implementation for online system as well ."}
{"title": "multiscale inverse reinforcement learning using diffusion wavelets", "abstract": "this work presents a multiscale framework to solve an inverse reinforcement learning ( irl ) problem for continuous-time/state stochastic systems . we take advantage of a diffusion wavelet representation of the associated markov chain to abstract the state space . this not only allows for effectively handling the large ( and geometrically complex ) decision space but also provides more interpretable representations of the demonstrated state trajectories and also of the resulting policy of irl . in the proposed framework , the problem is divided into the global and local irl , where the global approximation of the optimal value functions are obtained using coarse features and the local details are quantified using fine local features . an illustrative numerical example on robot path control in a complex environment is presented to verify the proposed method ."}
{"title": "should one compute the temporal difference fix point or minimize the bellman residual ? the unified oblique projection view", "abstract": "we investigate projection methods , for evaluating a linear approximation of the value function of a policy in a markov decision process context . we consider two popular approaches , the one-step temporal difference fix-point computation ( td ( 0 ) ) and the bellman residual ( br ) minimization . we describe examples , where each method outperforms the other . we highlight a simple relation between the objective function they minimize , and show that while br enjoys a performance guarantee , td ( 0 ) does not in general . we then propose a unified view in terms of oblique projections of the bellman equation , which substantially simplifies and extends the characterization of ( schoknecht,2002 ) and the recent analysis of ( yu & bertsekas , 2008 ) . eventually , we describe some simulations that suggest that if the td ( 0 ) solution is usually slightly better than the br solution , its inherent numerical instability makes it very bad in some cases , and thus worse on average ."}
{"title": "coupling control and human-centered automation in mathematical models of complex systems", "abstract": "in this paper we analyze mathematically how human factors can be effectively incorporated into the analysis and control of complex systems . as an example , we focus our discussion around one of the key problems in the intelligent transportation systems ( its ) theory and practice , the problem of speed control , considered here as a decision making process with limited information available . the problem is cast mathematically in the general framework of control problems and is treated in the context of dynamically changing environments where control is coupled to human-centered automation . since in this case control might not be limited to a small number of control settings , as it is often assumed in the control literature , serious difficulties arise in the solution of this problem . we demonstrate that the problem can be reduced to a set of hamilton-jacobi-bellman equations where human factors are incorporated via estimations of the system hamiltonian . in the its context , these estimations can be obtained with the use of on-board equipment like sensors/receivers/actuators , in-vehicle communication devices , etc . the proposed methodology provides a way to integrate human factor into the solving process of the models for other complex dynamic systems ."}
{"title": "resource matchmaking algorithm using dynamic rough set in grid environment", "abstract": "grid environment is a service oriented infrastructure in which many heterogeneous resources participate to provide the high performance computation . one of the bug issues in the grid environment is the vagueness and uncertainty between advertised resources and requested resources . furthermore , in an environment such as grid dynamicity is considered as a crucial issue which must be dealt with . classical rough set have been used to deal with the uncertainty and vagueness . but it can just be used on the static systems and can not support dynamicity in a system . in this work we propose a solution , called dynamic rough set resource discovery ( drsrd ) , for dealing with cases of vagueness and uncertainty problems based on dynamic rough set theory which considers dynamic features in this environment . in this way , requested resource properties have a weight as priority according to which resource matchmaking and ranking process is done . we also report the result of the solution obtained from the simulation in gridsim simulator . the comparison has been made between drsrd , classical rough set theory based algorithm , and uddi and owl s combined algorithm . drsrd shows much better precision for the cases with vagueness and uncertainty in a dynamic system such as the grid rather than the classical rough set theory based algorithm , and uddi and owl s combined algorithm ."}
{"title": "enigma : efficient learning-based inference guiding machine", "abstract": "enigma is a learning-based method for guiding given clause selection in saturation-based theorem provers . clauses from many proof searches are classified as positive and negative based on their participation in the proofs . an efficient classification model is trained on this data , using fast feature-based characterization of the clauses . the learned model is then tightly linked with the core prover and used as a basis of a new parameterized evaluation heuristic that provides fast ranking of all generated clauses . the approach is evaluated on the e prover and the casc 2016 aim benchmark , showing a large increase of e 's performance ."}
{"title": "mutual transformation of information and knowledge", "abstract": "information and knowledge are transformable into each other . information transformation into knowledge by the example of rule generation from owl ( web ontology language ) ontology has been shown during the development of the swes ( semantic web expert system ) . the swes is expected as an expert system for searching owl ontologies from the web , generating rules from the found ontologies and supplementing the swes knowledge base with these rules . the purpose of this paper is to show knowledge transformation into information by the example of ontology generation from rules ."}
{"title": "networked fairness in cake cutting", "abstract": "we introduce a graphical framework for fair division in cake cutting , where comparisons between agents are limited by an underlying network structure . we generalize the classical fairness notions of envy-freeness and proportionality to this graphical setting . given a simple undirected graph g , an allocation is envy-free on g if no agent envies any of her neighbor 's share , and is proportional on g if every agent values her own share no less than the average among her neighbors , with respect to her own measure . these generalizations open new research directions in developing simple and efficient algorithms that can produce fair allocations under specific graph structures . on the algorithmic frontier , we first propose a moving-knife algorithm that outputs an envy-free allocation on trees . the algorithm is significantly simpler than the discrete and bounded envy-free algorithm recently designed by aziz and mackenzie for complete graphs . next , we give a discrete and bounded algorithm for computing a proportional allocation on descendant graphs , a class of graphs by taking a rooted tree and connecting all its ancestor-descendant pairs ."}
{"title": "ecg feature extraction techniques - a survey approach", "abstract": "ecg feature extraction plays a significant role in diagnosing most of the cardiac diseases . one cardiac cycle in an ecg signal consists of the p-qrs-t waves . this feature extraction scheme determines the amplitudes and intervals in the ecg signal for subsequent analysis . the amplitudes and intervals value of p-qrs-t segment determines the functioning of heart of every human . recently , numerous research and techniques have been developed for analyzing the ecg signal . the proposed schemes were mostly based on fuzzy logic methods , artificial neural networks ( ann ) , genetic algorithm ( ga ) , support vector machines ( svm ) , and other signal analysis techniques . all these techniques and algorithms have their advantages and limitations . this proposed paper discusses various techniques and transformations proposed earlier in literature for extracting feature from an ecg signal . in addition this paper also provides a comparative study of various methods proposed by researchers in extracting the feature from ecg signal ."}
{"title": "rational deployment of multiple heuristics in ida*", "abstract": "recent advances in metareasoning for search has shown its usefulness in improving numerous search algorithms . this paper applies rational metareasoning to ida* when several admissible heuristics are available . the obvious basic approach of taking the maximum of the heuristics is improved upon by lazy evaluation of the heuristics , resulting in a variant known as lazy ida* . we introduce a rational version of lazy ida* that decides whether to compute the more expensive heuristics or to bypass it , based on a myopic expected regret estimate . empirical evaluation in several domains supports the theoretical results , and shows that rational lazy ida* is a state-of-the-art heuristic combination method ."}
{"title": "value elimination : bayesian inference via backtracking search", "abstract": "backtracking search is a powerful algorithmic paradigm that can be used to solve many problems . it is in a certain sense the dual of variable elimination ; but on many problems , e.g. , sat , it is vastly superior to variable elimination in practice . motivated by this we investigate the application of backtracking search to the problem of bayesian inference ( bayes ) . we show that natural generalizations of known techniques allow backtracking search to achieve performance guarantees similar to standard algorithms for bayes , and that there exist problems on which backtracking can in fact do much better . we also demonstrate that these ideas can be applied to implement a bayesian inference engine whose performance is competitive with standard algorithms . since backtracking search can very naturally take advantage of context specific structure , the potential exists for performance superior to standard algorithms on many problems ."}
{"title": "learning link-probabilities in causal trees", "abstract": "a learning algorithm is presented which given the structure of a causal tree , will estimate its link probabilities by sequential measurements on the leaves only . internal nodes of the tree represent conceptual ( hidden ) variables inaccessible to observation . the method described is incremental , local , efficient , and remains robust to measurement imprecisions ."}
{"title": "context aware dynamic traffic signal optimization", "abstract": "conventional urban traffic control systems have been based on historical traffic data . later advancements made use of detectors , which enabled the gathering of real time traffic data , in order to reorganize and calibrate traffic signalization programs . further evolvement provided the ability to forecast traffic conditions , in order to develop traffic signalization programs and strategies precomputed and applied at the most appropriate time frame for the optimal control of the current traffic conditions . we , propose the next generation of traffic control systems based on principles of artificial intelligence and context awareness . most of the existing algorithms use average waiting time or length of the queue to assess an algorithms performance . however , a low average waiting time may come at the cost of delaying other vehicles indefinitely . in our algorithm , besides the vehicle queue , we use fairness also as an important performance metric to assess an algorithms performance ."}
{"title": "bayesian network learning with cutting planes", "abstract": "the problem of learning the structure of bayesian networks from complete discrete data with a limit on parent set size is considered . learning is cast explicitly as an optimisation problem where the goal is to find a bn structure which maximises log marginal likelihood ( bde score ) . integer programming , specifically the scip framework , is used to solve this optimisation problem . acyclicity constraints are added to the integer program ( ip ) during solving in the form of cutting planes . finding good cutting planes is the key to the success of the approach -the search for such cutting planes is effected using a sub-ip . results show that this is a particularly fast method for exact bn learning ."}
{"title": "a semantic model for historical manuscripts", "abstract": "the study and publication of historical scientific manuscripts are com- plex tasks that involve , among others , the explicit representation of the text mean- ings and reasoning on temporal entities . in this paper we present the first results of an interdisciplinary project dedicated to the study of saussure 's manuscripts . these results aim to fulfill requirements elaborated with saussurean humanists . they comprise a model for the representation of time-varying statements and time-varying domain knowledge ( in particular terminologies ) as well as imple- mentation techniques for the semantic indexing of manuscripts and for temporal reasoning on knowledge extracted from the manuscripts ."}
{"title": "consciousness is pattern recognition", "abstract": "this is a proof of the strong ai hypothesis , i.e . that machines can be conscious . it is a phenomenological proof that pattern-recognition and subjective consciousness are the same activity in different terms . therefore , it proves that essential subjective processes of consciousness are computable , and identifies significant traits and requirements of a conscious system . since husserl , many philosophers have accepted that consciousness consists of memories of logical connections between an ego and external objects . these connections are called `` intentions . '' pattern recognition systems are achievable technical artifacts . the proof links this respected introspective philosophical theory of consciousness with technical art . the proof therefore endorses the strong ai hypothesis and may therefore also enable a theoretically-grounded form of artificial intelligence called a `` synthetic intentionality , '' able to synthesize , generalize , select and repeat intentions . if the pattern recognition is reflexive , able to operate on the set of intentions , and flexible , with several methods of synthesizing intentions , an si may be a particularly strong form of ai . similarities and possible applications to several ai paradigms are discussed . the article then addresses some problems : the proof 's limitations , reflexive cognition , searles ' chinese room , and how an si could `` understand '' `` meanings '' and `` be creative . ''"}
{"title": "a bayesian method for causal modeling and discovery under selection", "abstract": "this paper describes a bayesian method for learning causal networks using samples that were selected in a non-random manner from a population of interest . examples of data obtained by non-random sampling include convenience samples and case-control data in which a fixed number of samples with and without some condition is collected ; such data are not uncommon . the paper describes a method for combining data under selection with prior beliefs in order to derive a posterior probability for a model of the causal processes that are generating the data in the population of interest . the priors include beliefs about the nature of the non-random sampling procedure . although exact application of the method would be computationally intractable for most realistic datasets , efficient special-case and approximation methods are discussed . finally , the paper describes how to combine learning under selection with previous methods for learning from observational and experimental data that are obtained on random samples of the population of interest . the net result is a bayesian methodology that supports causal modeling and discovery from a rich mixture of different types of data ."}
{"title": "a logical characterization of iterated admissibility", "abstract": "brandenburger , friedenberg , and keisler provide an epistemic characterization of iterated admissibility ( i.e. , iterated deletion of weakly dominated strategies ) where uncertainty is represented using lpss ( lexicographic probability sequences ) . their characterization holds in a rich structure called a complete structure , where all types are possible . here , a logical charaacterization of iterated admisibility is given that involves only standard probability and holds in all structures , not just complete structures . a stronger notion of strong admissibility is then defined . roughly speaking , strong admissibility is meant to capture the intuition that `` all the agent knows '' is that the other agents satisfy the appropriate rationality assumptions . strong admissibility makes it possible to relate admissibility , canonical structures ( as typically considered in completeness proofs in modal logic ) , complete structures , and the notion of `` all i know '' ."}
{"title": "achieving fully proportional representation : approximability results", "abstract": "we study the complexity of ( approximate ) winner determination under the monroe and chamberlin -- courant multiwinner voting rules , which determine the set of representatives by optimizing the total ( dis ) satisfaction of the voters with their representatives . the total ( dis ) satisfaction is calculated either as the sum of individual ( dis ) satisfactions ( the utilitarian case ) or as the ( dis ) satisfaction of the worst off voter ( the egalitarian case ) . we provide good approximation algorithms for the satisfaction-based utilitarian versions of the monroe and chamberlin -- courant rules , and inapproximability results for the dissatisfaction-based utilitarian versions of them and also for all egalitarian cases . our algorithms are applicable and particularly appealing when voters submit truncated ballots . we provide experimental evaluation of the algorithms both on real-life preference-aggregation data and on synthetic data . these experiments show that our simple and fast algorithms can in many cases find near-perfect solutions ."}
{"title": "an approach to multi-agent planning with incomplete information", "abstract": "multi-agent planning ( map ) approaches have been typically conceived for independent or loosely-coupled problems to enhance the benefits of distributed planning between autonomous agents as solving this type of problems require less coordination between the agents ' sub-plans . however , when it comes to tightly-coupled agents ' tasks , map has been relegated in favour of centralized approaches and little work has been done in this direction . in this paper , we present a general-purpose map capable to efficiently handle planning problems with any level of coupling between agents . we propose a cooperative refinement planning approach , built upon the partial-order planning paradigm , that allows agents to work with incomplete information and to have incomplete views of the world , i.e . being ignorant of other agents ' information , as well as maintaining their own private information . we show various experiments to compare the performance of our system with a distributed csp-based map approach over a suite of problems ."}
{"title": "efficient test selection in active diagnosis via entropy approximation", "abstract": "we consider the problem of diagnosing faults in a system represented by a bayesian network , where diagnosis corresponds to recovering the most likely state of unobserved nodes given the outcomes of tests ( observed nodes ) . finding an optimal subset of tests in this setting is intractable in general . we show that it is difficult even to compute the next most-informative test using greedy test selection , as it involves several entropy terms whose exact computation is intractable . we propose an approximate approach that utilizes the loopy belief propagation infrastructure to simultaneously compute approximations of marginal and conditional entropies on multiple subsets of nodes . we apply our method to fault diagnosis in computer networks , and show the algorithm to be very effective on realistic internet-like topologies . we also provide theoretical justification for the greedy test selection approach , along with some performance guarantees ."}
{"title": "technical report : graph-structured sparse optimization for connected subgraph detection", "abstract": "structured sparse optimization is an important and challenging problem for analyzing high-dimensional data in a variety of applications such as bioinformatics , medical imaging , social networks , and astronomy . although a number of structured sparsity models have been explored , such as trees , groups , clusters , and paths , connected subgraphs have been rarely explored in the current literature . one of the main technical challenges is that there is no structured sparsity-inducing norm that can directly model the space of connected subgraphs , and there is no exact implementation of a projection oracle for connected subgraphs due to its np-hardness . in this paper , we explore efficient approximate projection oracles for connected subgraphs , and propose two new efficient algorithms , namely , graph-iht and graph-ghtp , to optimize a generic nonlinear objective function subject to connectivity constraint on the support of the variables . our proposed algorithms enjoy strong guarantees analogous to several current methods for sparsity-constrained optimization , such as projected gradient descent ( pgd ) , approximate model iterative hard thresholding ( am-iht ) , and gradient hard thresholding pursuit ( ghtp ) with respect to convergence rate and approximation accuracy . we apply our proposed algorithms to optimize several well-known graph scan statistics in several applications of connected subgraph detection as a case study , and the experimental results demonstrate that our proposed algorithms outperform state-of-the-art methods ."}
{"title": "multibiometrics belief fusion", "abstract": "this paper proposes a multimodal biometric system through gaussian mixture model ( gmm ) for face and ear biometrics with belief fusion of the estimated scores characterized by gabor responses and the proposed fusion is accomplished by dempster-shafer ( ds ) decision theory . face and ear images are convolved with gabor wavelet filters to extracts spatially enhanced gabor facial features and gabor ear features . further , gmm is applied to the high-dimensional gabor face and gabor ear responses separately for quantitive measurements . expectation maximization ( em ) algorithm is used to estimate density parameters in gmm . this produces two sets of feature vectors which are then fused using dempster-shafer theory . experiments are conducted on multimodal database containing face and ear images of 400 individuals . it is found that use of gabor wavelet filters along with gmm and ds theory can provide robust and efficient multimodal fusion strategy ."}
{"title": "a database and lexicon of scripts for thoughttreasure", "abstract": "since scripts were proposed in the 1970 's as an inferencing mechanism for ai and natural language processing programs , there have been few attempts to build a database of scripts . this paper describes a database and lexicon of scripts that has been added to the thoughttreasure commonsense platform . the database provides the following information about scripts : sequence of events , roles , props , entry conditions , results , goals , emotions , places , duration , frequency , and cost . english and french words and phrases are linked to script concepts ."}
{"title": "conditional indifference and conditional preservation", "abstract": "the idea of preserving conditional beliefs emerged recently as a new paradigm apt to guide the revision of epistemic states . conditionals are substantially different from propositional beliefs and need specific treatment . in this paper , we present a new approach to conditionals , capturing particularly well their dynamic part as revision policies . we thoroughly axiomatize a principle of conditional preservation as an indifference property with respect to conditional structures of worlds . this principle is developed in a semi-quantitative setting , so as to reveal its fundamental meaning for belief revision in quantitative as well as in qualitative frameworks . in fact , it is shown to cover other proposed approaches to conditional preservation ."}
{"title": "efficiently discovering locally exceptional yet globally representative subgroups", "abstract": "subgroup discovery is a local pattern mining technique to find interpretable descriptions of sub-populations that stand out on a given target variable . that is , these sub-populations are exceptional with regard to the global distribution . in this paper we argue that in many applications , such as scientific discovery , subgroups are only useful if they are additionally representative of the global distribution with regard to a control variable . that is , when the distribution of this control variable is the same , or almost the same , as over the whole data . we formalise this objective function and give an efficient algorithm to compute its tight optimistic estimator for the case of a numeric target and a binary control variable . this enables us to use the branch-and-bound framework to efficiently discover the top- $ k $ subgroups that are both exceptional as well as representative . experimental evaluation on a wide range of datasets shows that with this algorithm we discover meaningful representative patterns and are up to orders of magnitude faster in terms of node evaluations as well as time ."}
{"title": "toward a characterization of uncertainty measure for the dempster-shafer theory", "abstract": "this is a working paper summarizing results of an ongoing research project whose aim is to uniquely characterize the uncertainty measure for the dempster-shafer theory . a set of intuitive axiomatic requirements is presented , some of their implications are shown , and the proof is given of the minimality of recently proposed measure au among all measures satisfying the proposed requirements ."}
{"title": "logical stochastic optimization", "abstract": "we present a logical framework to represent and reason about stochastic optimization problems based on probability answer set programming . this is established by allowing probability optimization aggregates , e.g. , minimum and maximum in the language of probability answer set programming to allow minimization or maximization of some desired criteria under the probabilistic environments . we show the application of the proposed logical stochastic optimization framework under the probability answer set programming to two stages stochastic optimization problems with recourse ."}
{"title": "rapport technique du projet ogre", "abstract": "this repport concerns automatic understanding of ( french ) iterative sentences , i.e . sentences where one single verb has to be interpreted by a more or less regular plurality of events . a linguistic analysis is proposed along an extension of reichenbach 's theory , several formal representations are considered and a corpus of 18000 newspaper extracts is described ."}
{"title": "tournament versus fitness uniform selection", "abstract": "in evolutionary algorithms a critical parameter that must be tuned is that of selection pressure . if it is set too low then the rate of convergence towards the optimum is likely to be slow . alternatively if the selection pressure is set too high the system is likely to become stuck in a local optimum due to a loss of diversity in the population . the recent fitness uniform selection scheme ( fuss ) is a conceptually simple but somewhat radical approach to addressing this problem - rather than biasing the selection towards higher fitness , fuss biases selection towards sparsely populated fitness levels . in this paper we compare the relative performance of fuss with the well known tournament selection scheme on a range of problems ."}
{"title": "on simulated annealing dedicated to maximin latin hypercube designs", "abstract": "the goal of our research was to enhance local search heuristics used to construct latin hypercube designs . first , we introduce the \\textit { 1d-move } perturbation to improve the space exploration performed by these algorithms . second , we propose a new evaluation function $ \\psi_ { p , \\sigma } $ specifically targeting the maximin criterion . exhaustive series of experiments with simulated annealing , which we used as a typically well-behaving local search heuristics , confirm that our goal was reached as the result we obtained surpasses the best scores reported in the literature . furthermore , the $ \\psi_ { p , \\sigma } $ function seems very promising for a wide spectrum of optimization problems through the maximin criterion ."}
{"title": "automatic taxonomy generation - a use-case in the legal domain", "abstract": "a key challenge in the legal domain is the adaptation and representation of the legal knowledge expressed through texts , in order for legal practitioners and researchers to access this information easier and faster to help with compliance related issues . one way to approach this goal is in the form of a taxonomy of legal concepts . while this task usually requires a manual construction of terms and their relations by domain experts , this paper describes a methodology to automatically generate a taxonomy of legal noun concepts . we apply and compare two approaches on a corpus consisting of statutory instruments for uk , wales , scotland and northern ireland laws ."}
{"title": "modeling computations in a semantic network", "abstract": "semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the semantic web initiative . the semantic web effort has brought forth an array of technologies that support the encoding , storage , and querying of the semantic network data structure at the world stage . currently , the popular conception of the semantic web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways . however , new models have emerged that explicitly encode procedural information within the semantic network substrate . with these new technologies , the semantic web has evolved from a data modeling medium to a computational medium . this article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain ."}
{"title": "systematic testing of convolutional neural networks for autonomous driving", "abstract": "we present a framework to systematically analyze convolutional neural networks ( cnns ) used in classification of cars in autonomous vehicles . our analysis procedure comprises an image generator that produces synthetic pictures by sampling in a lower dimension image modification subspace and a suite of visualization tools . the image generator produces images which can be used to test the cnn and hence expose its vulnerabilities . the presented framework can be used to extract insights of the cnn classifier , compare across classification models , or generate training and validation datasets ."}
{"title": "credit assignment in adaptive evolutionary algorithms", "abstract": "in this paper , a new method for assigning credit to search operators is presented . starting with the principle of optimizing search bias , search operators are selected based on an ability to create solutions that are historically linked to future generations . using a novel framework for defining performance measurements , distributing credit for performance , and the statistical interpretation of this credit , a new adaptive method is developed and shown to outperform a variety of adaptive and non-adaptive competitors ."}
{"title": "networks of influence diagrams : a formalism for representing agents ' beliefs and decision-making processes", "abstract": "this paper presents networks of influence diagrams ( nid ) , a compact , natural and highly expressive language for reasoning about agents beliefs and decision-making processes . nids are graphical structures in which agents mental models are represented as nodes in a network ; a mental model for an agent may itself use descriptions of the mental models of other agents . nids are demonstrated by examples , showing how they can be used to describe conflicting and cyclic belief structures , and certain forms of bounded rationality . in an opponent modeling domain , nids were able to outperform other computational agents whose strategies were not known in advance . nids are equivalent in representation to bayesian games but they are more compact and structured than this formalism . in particular , the equilibrium definition for nids makes an explicit distinction between agents optimal strategies , and how they actually behave in reality ."}
{"title": "cooperative searching for stochastic targets", "abstract": "spatial search problems abound in the real world , from locating hidden nuclear or chemical sources to finding skiers after an avalanche . we exemplify the formalism and solution for spatial searches involving two agents that may or may not choose to share information during a search . for certain classes of tasks , sharing information between multiple searchers makes cooperative searching advantageous . in some examples , agents are able to realize synergy by aggregating information and moving based on local judgments about maximal information gathering expectations . we also explore one- and two-dimensional simplified situations analytically and numerically to provide a framework for analyzing more complex problems . these general considerations provide a guide for designing optimal algorithms for real-world search problems ."}
{"title": "another perspective on default reasoning", "abstract": "the lexicographic closure of any given finite set d of normal defaults is defined . a conditional assertion `` if a then b '' is in this lexicographic closure if , given the defaults d and the fact a , one would conclude b. the lexicographic closure is essentially a rational extension of d , and of its rational closure , defined in a previous paper . it provides a logic of normal defaults that is different from the one proposed by r. reiter and that is rich enough not to require the consideration of non-normal defaults . a large number of examples are provided to show that the lexicographic closure corresponds to the basic intuitions behind reiter 's logic of defaults ."}
{"title": "communication-based decomposition mechanisms for decentralized mdps", "abstract": "multi-agent planning in stochastic environments can be framed formally as a decentralized markov decision problem . many real-life distributed problems that arise in manufacturing , multi-robot coordination and information gathering scenarios can be formalized using this framework . however , finding the optimal solution in the general case is hard , limiting the applicability of recently developed algorithms . this paper provides a practical approach for solving decentralized control problems when communication among the decision makers is possible , but costly . we develop the notion of communication-based mechanism that allows us to decompose a decentralized mdp into multiple single-agent problems . in this framework , referred to as decentralized semi-markov decision process with direct communication ( dec-smdp-com ) , agents operate separately between communications . we show that finding an optimal mechanism is equivalent to solving optimally a dec-smdp-com . we also provide a heuristic search algorithm that converges on the optimal decomposition . restricting the decomposition to some specific types of local behaviors reduces significantly the complexity of planning . in particular , we present a polynomial-time algorithm for the case in which individual agents perform goal-oriented behaviors between communications . the paper concludes with an additional tractable algorithm that enables the introduction of human knowledge , thereby reducing the overall problem to finding the best time to communicate . empirical results show that these approaches provide good approximate solutions ."}
{"title": "the path inference filter : model-based low-latency map matching of probe vehicle data", "abstract": "we consider the problem of reconstructing vehicle trajectories from sparse sequences of gps points , for which the sampling interval is between 10 seconds and 2 minutes . we introduce a new class of algorithms , called altogether path inference filter ( pif ) , that maps gps data in real time , for a variety of trade-offs and scenarios , and with a high throughput . numerous prior approaches in map-matching can be shown to be special cases of the path inference filter presented in this article . we present an efficient procedure for automatically training the filter on new data , with or without ground truth observations . the framework is evaluated on a large san francisco taxi dataset and is shown to improve upon the current state of the art . this filter also provides insights about driving patterns of drivers . the path inference filter has been deployed at an industrial scale inside the mobile millennium traffic information system , and is used to map fleets of data in san francisco , sacramento , stockholm and porto ."}
{"title": "exact inference of hidden structure from sample data in noisy-or networks", "abstract": "in the literature on graphical models , there has been increased attention paid to the problems of learning hidden structure ( see heckerman [ h96 ] for survey ) and causal mechanisms from sample data [ h96 , p88 , s93 , p95 , f98 ] . in most settings we should expect the former to be difficult , and the latter potentially impossible without experimental intervention . in this work , we examine some restricted settings in which perfectly reconstruct the hidden structure solely on the basis of observed sample data ."}
{"title": "a spectral algorithm for learning hidden markov models", "abstract": "hidden markov models ( hmms ) are one of the most fundamental and widely used statistical tools for modeling discrete time series . in general , learning hmms from data is computationally hard ( under cryptographic assumptions ) , and practitioners typically resort to search heuristics which suffer from the usual local optima issues . we prove that under a natural separation condition ( bounds on the smallest singular value of the hmm parameters ) , there is an efficient and provably correct algorithm for learning hmms . the sample complexity of the algorithm does not explicitly depend on the number of distinct ( discrete ) observations -- -it implicitly depends on this quantity through spectral properties of the underlying hmm . this makes the algorithm particularly applicable to settings with a large number of observations , such as those in natural language processing where the space of observation is sometimes the words in a language . the algorithm is also simple , employing only a singular value decomposition and matrix multiplications ."}
{"title": "firefly algorithm : recent advances and applications", "abstract": "nature-inspired metaheuristic algorithms , especially those based on swarm intelligence , have attracted much attention in the last ten years . firefly algorithm appeared in about five years ago , its literature has expanded dramatically with diverse applications . in this paper , we will briefly review the fundamentals of firefly algorithm together with a selection of recent publications . then , we discuss the optimality associated with balancing exploration and exploitation , which is essential for all metaheuristic algorithms . by comparing with intermittent search strategy , we conclude that metaheuristics such as firefly algorithm are better than the optimal intermittent search strategy . we also analyse algorithms and their implications for higher-dimensional optimization problems ."}
{"title": "intelligent bidirectional rapidly-exploring random trees for optimal motion planning in complex cluttered environments", "abstract": "the sampling based motion planning algorithm known as rapidly-exploring random trees ( rrt ) has gained the attention of many researchers due to their computational efficiency and effectiveness . recently , a variant of rrt called rrt* has been proposed that ensures asymptotic optimality . subsequently its bidirectional version has also been introduced in the literature known as bidirectional-rrt* ( b-rrt* ) . we introduce a new variant called intelligent bidirectional-rrt* ( ib-rrt* ) which is an improved variant of the optimal rrt* and bidirectional version of rrt* ( b-rrt* ) algorithms and is specially designed for complex cluttered environments . ib-rrt* utilizes the bidirectional trees approach and introduces intelligent sample insertion heuristic for fast convergence to the optimal path solution using uniform sampling heuristics . the proposed algorithm is evaluated theoretically and experimental results are presented that compares ib-rrt* with rrt* and b-rrt* . moreover , experimental results demonstrate the superior efficiency of ib-rrt* in comparison with rrt* and b-rrt in complex cluttered environments ."}
{"title": "towards an unanimous international regulatory body for responsible use of artificial intelligence [ uirb-ai ]", "abstract": "artificial intelligence ( ai ) , is once again in the phase of drastic advancements . unarguably , the technology itself can revolutionize the way we live our everyday life . but the exponential growth of technology poses a daunting task for policy researchers and law makers in making amendments to the existing norms . in addition , not everyone in the society is studying the potential socio-economic intricacies and cultural drifts that ai can bring about . it is prudence to reflect from our historical past to propel the development of technology in the right direction . to benefit the society of the present and future , i scientifically explore the societal impact of ai . while there are many public and private partnerships working on similar aspects , here i describe the necessity for an unanimous international regulatory body for all applications of ai ( uirb-ai ) . i also discuss the benefits and drawbacks of such an organization . to combat any drawbacks in the formation of an uirb-ai , both idealistic and pragmatic perspectives are discussed alternatively . the paper further advances the discussion by proposing novel policies on how such organization should be structured and how it can bring about a win-win situation for everyone in the society ."}
{"title": "multi-focus attention network for efficient deep reinforcement learning", "abstract": "deep reinforcement learning ( drl ) has shown incredible performance in learning various tasks to the human level . however , unlike human perception , current drl models connect the entire low-level sensory input to the state-action values rather than exploiting the relationship between and among entities that constitute the sensory input . because of this difference , drl needs vast amount of experience samples to learn . in this paper , we propose a multi-focus attention network ( manet ) which mimics human ability to spatially abstract the low-level sensory input into multiple entities and attend to them simultaneously . the proposed method first divides the low-level input into several segments which we refer to as partial states . after this segmentation , parallel attention layers attend to the partial states relevant to solving the task . our model estimates state-action values using these attended partial states . in our experiments , manet attains highest scores with significantly less experience samples . additionally , the model shows higher performance compared to the deep q-network and the single attention model as benchmarks . furthermore , we extend our model to attentive communication model for performing multi-agent cooperative tasks . in multi-agent cooperative task experiments , our model shows 20 % faster learning than existing state-of-the-art model ."}
{"title": "metaheuristic algorithms for convolution neural network", "abstract": "a typical modern optimization technique is usually either heuristic or metaheuristic . this technique has managed to solve some optimization problems in the research area of science , engineering , and industry . however , implementation strategy of metaheuristic for accuracy improvement on convolution neural networks ( cnn ) , a famous deep learning method , is still rarely investigated . deep learning relates to a type of machine learning technique , where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human . in this paper , we propose the implementation strategy of three popular metaheuristic approaches , that is , simulated annealing , differential evolution , and harmony search , to optimize cnn . the performances of these metaheuristic methods in optimizing cnn on classifying mnist and cifar dataset were evaluated and compared . furthermore , the proposed methods are also compared with the original cnn . although the proposed methods show an increase in the computation time , their accuracy has also been improved ( up to 7.14 percent ) ."}
{"title": "application of pso , artificial bee colony and bacterial foraging optimization algorithms to economic load dispatch : an analysis", "abstract": "this paper illustrates successful implementation of three evolutionary algorithms , namely- particle swarm optimization ( pso ) , artificial bee colony ( abc ) and bacterial foraging optimization ( bfo ) algorithms to economic load dispatch problem ( eld ) . power output of each generating unit and optimum fuel cost obtained using all three algorithms have been compared . the results obtained show that abc and bfo algorithms converge to optimal fuel cost with reduced computational time when compared to pso for the two example problems considered ."}
{"title": "crowd labeling : a survey", "abstract": "recently , there has been a burst in the number of research projects on human computation via crowdsourcing . multiple choice ( or labeling ) questions could be referred to as a common type of problem which is solved by this approach . as an application , crowd labeling is applied to find true labels for large machine learning datasets . since crowds are not necessarily experts , the labels they provide are rather noisy and erroneous . this challenge is usually resolved by collecting multiple labels for each sample , and then aggregating them to estimate the true label . although the mechanism leads to high-quality labels , it is not actually cost-effective . as a result , efforts are currently made to maximize the accuracy in estimating true labels , while fixing the number of acquired labels . this paper surveys methods to aggregate redundant crowd labels in order to estimate unknown true labels . it presents a unified statistical latent model where the differences among popular methods in the field correspond to different choices for the parameters of the model . afterwards , algorithms to make inference on these models will be surveyed . moreover , adaptive methods which iteratively collect labels based on the previously collected labels and estimated models will be discussed . in addition , this paper compares the distinguished methods , and provides guidelines for future work required to address the current open issues ."}
{"title": "dyna-style planning with linear function approximation and prioritized sweeping", "abstract": "we consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world . this paper develops an explicitly model-based approach extending the dyna architecture to linear function approximation . dynastyle planning proceeds by generating imaginary experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions . our main results are to prove that linear dyna-style planning converges to a unique solution independent of the generating distribution , under natural conditions . in the policy evaluation setting , we prove that the limit point is the least-squares ( lstd ) solution . an implication of our results is that prioritized-sweeping can be soundly extended to the linear approximation case , backing up to preceding features rather than to preceding states . we introduce two versions of prioritized sweeping with linear dyna and briefly illustrate their performance empirically on the mountain car and boyan chain problems ."}
{"title": "an investigation into mathematical programming for finite horizon decentralized pomdps", "abstract": "decentralized planning in uncertain environments is a complex task generally dealt with by using a decision-theoretic approach , mainly through the framework of decentralized partially observable markov decision processes ( dec-pomdps ) . although dec-pomdps are a general and powerful modeling tool , solving them is a task with an overwhelming complexity that can be doubly exponential . in this paper , we study an alternate formulation of dec-pomdps relying on a sequence-form representation of policies . from this formulation , we show how to derive mixed integer linear programming ( milp ) problems that , once solved , give exact optimal solutions to the dec-pomdps . we show that these milps can be derived either by using some combinatorial characteristics of the optimal solutions of the dec-pomdps or by using concepts borrowed from game theory . through an experimental validation on classical test problems from the dec-pomdp literature , we compare our approach to existing algorithms . results show that mathematical programming outperforms dynamic programming but is less efficient than forward search , except for some particular problems . the main contributions of this work are the use of mathematical programming for dec-pomdps and a better understanding of dec-pomdps and of their solutions . besides , we argue that our alternate representation of dec-pomdps could be helpful for designing novel algorithms looking for approximate solutions to dec-pomdps ."}
{"title": "combining fuzzy cognitive maps and discrete random variables", "abstract": "in this paper we propose an extension to the fuzzy cognitive maps ( fcms ) that aims at aggregating a number of reasoning tasks into a one parallel run . the described approach consists in replacing real-valued activation levels of concepts ( and further influence weights ) by random variables . such extension , followed by the implemented software tool , allows for determining ranges reached by concept activation levels , sensitivity analysis as well as statistical analysis of multiple reasoning results . we replace multiplication and addition operators appearing in the fcm state equation by appropriate convolutions applicable for discrete random variables . to make the model computationally feasible , it is further augmented with aggregation operations for discrete random variables . we discuss four implemented aggregators , as well as we report results of preliminary tests ."}
{"title": "self-organizing traffic lights", "abstract": "steering traffic in cities is a very complex task , since improving efficiency involves the coordination of many actors . traditional approaches attempt to optimize traffic lights for a particular density and configuration of traffic . the disadvantage of this lies in the fact that traffic densities and configurations change constantly . traffic seems to be an adaptation problem rather than an optimization problem . we propose a simple and feasible alternative , in which traffic lights self-organize to improve traffic flow . we use a multi-agent simulation to study three self-organizing methods , which are able to outperform traditional rigid and adaptive methods . using simple rules and no direct communication , traffic lights are able to self-organize and adapt to changing traffic conditions , reducing waiting times , number of stopped cars , and increasing average speeds ."}
{"title": "training for fast sequential prediction using dynamic feature selection", "abstract": "we present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many nlp components . this is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features . parameter estimation is arranged to maximize accuracy and early confidence in this sequence . we present experiments in left-to-right part-of-speech tagging on wsj , demonstrating that we can preserve accuracy above 97 % with over a five-fold reduction in run-time ."}
{"title": "adaptive lambda least-squares temporal difference learning", "abstract": "temporal difference learning or td ( $ \\lambda $ ) is a fundamental algorithm in the field of reinforcement learning . however , setting td 's $ \\lambda $ parameter , which controls the timescale of td updates , is generally left up to the practitioner . we formalize the $ \\lambda $ selection problem as a bias-variance trade-off where the solution is the value of $ \\lambda $ that leads to the smallest mean squared value error ( msve ) . to solve this trade-off we suggest applying leave-one-trajectory-out cross-validation ( loto-cv ) to search the space of $ \\lambda $ values . unfortunately , this approach is too computationally expensive for most practical applications . for least squares td ( lstd ) we show that loto-cv can be implemented efficiently to automatically tune $ \\lambda $ and apply function optimization methods to efficiently search the space of $ \\lambda $ values . the resulting algorithm , allstd , is parameter free and our experiments demonstrate that allstd is significantly computationally faster than the na\\ '' { i } ve loto-cv implementation while achieving similar performance ."}
{"title": "mob-esp and other improvements in probability estimation", "abstract": "a key prerequisite to optimal reasoning under uncertainty in intelligent systems is to start with good class probability estimates . this paper improves on the current best probability estimation trees ( bagged-pets ) and also presents a new ensemble-based algorithm ( mob-esp ) . comparisons are made using several benchmark datasets and multiple metrics . these experiments show that mob-esp outputs significantly more accurate class probabilities than either the baseline bpets algorithm or the enhanced version presented here ( eb-pets ) . these results are based on metrics closely associated with the average accuracy of the predictions . mob-esp also provides much better probability rankings than b-pets . the paper further suggests how these estimation techniques can be applied in concert with a broader category of classifiers ."}
{"title": "idiotypic immune networks in mobile robot control", "abstract": "jerne 's idiotypic network theory postulates that the immune response involves inter-antibody stimulation and suppression as well as matching to antigens . the theory has proved the most popular artificial immune system ( ais ) model for incorporation into behavior-based robotics but guidelines for implementing idiotypic selection are scarce . furthermore , the direct effects of employing the technique have not been demonstrated in the form of a comparison with non-idiotypic systems . this paper aims to address these issues . a method for integrating an idiotypic ais network with a reinforcement learning based control system ( rl ) is described and the mechanisms underlying antibody stimulation and suppression are explained in detail . some hypotheses that account for the network advantage are put forward and tested using three systems with increasing idiotypic complexity . the basic rl , a simplified hybrid ais-rl that implements idiotypic selection independently of derived concentration levels and a full hybrid ais-rl scheme are examined . the test bed takes the form of a simulated pioneer robot that is required to navigate through maze worlds detecting and tracking door markers ."}
{"title": "cross-linguistic differences and similarities in image descriptions", "abstract": "automatic image description systems are commonly trained and evaluated on large image description datasets . recently , researchers have started to collect such datasets for languages other than english . an unexplored question is how different these datasets are from english and , if there are any differences , what causes them to differ . this paper provides a cross-linguistic comparison of dutch , english , and german image descriptions . we find that these descriptions are similar in many respects , but the familiarity of crowd workers with the subjects of the images has a noticeable influence on description specificity ."}
{"title": "a multi-criteria neutrosophic group decision making metod based topsis for supplier selection", "abstract": "the process of multiple criteria decision making ( mcdm ) is of determining the best choice among all of the probable alternatives . the problem of supplier selection on which decision maker has usually vague and imprecise knowledge is a typical example of multi criteria group decision-making problem . the conventional crisp techniques has not much effective for solving mcdm problems because of imprecise or fuzziness nature of the linguistic assessments . to find the exact values for mcdm problems is both difficult and impossible in more cases in real world . so , it is more reasonable to consider the values of alternatives according to the criteria as single valued neutrosophic sets ( svns ) . this paper deal with the technique for order preference by similarity to ideal solution ( topsis ) approach and extend the topsis method to mcdm problem with single valued neutrosophic information . the value of each alternative and the weight of each criterion are characterized by single valued neutrosophic numbers . here , the importance of criteria and alternatives is identified by aggregating individual opinions of decision makers ( dms ) via single valued neutrosophic weighted averaging ( ifwa ) operator . the proposed method is , easy use , precise and practical for solving mcdm problem with single valued neutrosophic data . finally , to show the applicability of the developed method , a numerical experiment for supplier choice is given as an application of single valued neutrosophic topsis method at end of this paper ."}
{"title": "fast bayesian optimization of machine learning hyperparameters on large datasets", "abstract": "bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms , such as support vector machines or deep neural networks . despite its success , for large datasets , training and validating a single configuration often takes hours , days , or even weeks , which limits the achievable performance . to accelerate hyperparameter optimization , we propose a generative model for the validation error as a function of training set size , which is learned during the optimization process and allows exploration of preliminary configurations on small subsets , by extrapolating to the full dataset . we construct a bayesian optimization procedure , dubbed fabolas , which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost . experiments optimizing support vector machines and deep neural networks show that fabolas often finds high-quality solutions 10 to 100 times faster than other state-of-the-art bayesian optimization methods or the recently proposed bandit strategy hyperband ."}
{"title": "quantifying creativity in art networks", "abstract": "can we develop a computer algorithm that assesses the creativity of a painting given its context within art history ? this paper proposes a novel computational framework for assessing the creativity of creative products , such as paintings , sculptures , poetry , etc . we use the most common definition of creativity , which emphasizes the originality of the product and its influential value . the proposed computational framework is based on constructing a network between creative products and using this network to infer about the originality and influence of its nodes . through a series of transformations , we construct a creativity implication network . we show that inference about creativity in this network reduces to a variant of network centrality problems which can be solved efficiently . we apply the proposed framework to the task of quantifying creativity of paintings ( and sculptures ) . we experimented on two datasets with over 62k paintings to illustrate the behavior of the proposed framework . we also propose a methodology for quantitatively validating the results of the proposed algorithm , which we call the `` time machine experiment '' ."}
{"title": "share : a web service based framework for distributed querying and reasoning on the semantic web", "abstract": "here we describe the share system , a web service based framework for distributed querying and reasoning on the semantic web . the main innovations of share are : ( 1 ) the extension of a sparql query engine to perform on-demand data retrieval from web services , and ( 2 ) the extension of an owl reasoner to test property restrictions by means of web service invocations . in addition to enabling queries across distributed datasets , the system allows for a target dataset that is significantly larger than is possible under current , centralized approaches . although the architecture is equally applicable to all types of data , the share system targets bioinformatics , due to the large number of interoperable web services that are already available in this area . share is built entirely on semantic web standards , and is the successor of the biomoby project ."}
{"title": "proceedings first workshop on formal verification of autonomous vehicles", "abstract": "these are the proceedings of the workshop on formal verification of autonomous vehicles , held on september 19th , 2017 in turin , italy , as an affiliated workshop of the international conference on integrated formal methods ( ifm 2017 ) . the workshop aim is to bring together researchers from the formal verification community that are developing formal methods for autonomous vehicles as well as researchers working , e.g. , in the area of control theory or robotics , interested in applying verification techniques for designing and developing of autonomous vehicles ."}
{"title": "borrowing treasures from the wealthy : deep transfer learning through selective joint fine-tuning", "abstract": "deep neural networks require a large amount of labeled training data during supervised learning . however , collecting and labeling so much data might be infeasible in many cases . in this paper , we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data . in this scheme , a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data . however , the source learning task does not use all existing training data . our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task , and jointly fine-tune shared convolutional layers for both tasks . specifically , we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks , and use such descriptors to search for a desired subset of training samples for the source learning task . experiments demonstrate that our selective joint fine-tuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning . such tasks include caltech 256 , mit indoor 67 , oxford flowers 102 and stanford dogs 120. in comparison to fine-tuning without a source domain , the proposed method can improve the classification accuracy by 2 % - 10 % using a single model ."}
{"title": "characterising equilibrium logic and nested logic programs : reductions and complexity", "abstract": "equilibrium logic is an approach to nonmonotonic reasoning that extends the stable-model and answer-set semantics for logic programs . in particular , it includes the general case of nested logic programs , where arbitrary boolean combinations are permitted in heads and bodies of rules , as special kinds of theories . in this paper , we present polynomial reductions of the main reasoning tasks associated with equilibrium logic and nested logic programs into quantified propositional logic , an extension of classical propositional logic where quantifications over atomic formulas are permitted . we provide reductions not only for decision problems , but also for the central semantical concepts of equilibrium logic and nested logic programs . in particular , our encodings map a given decision problem into some formula such that the latter is valid precisely in case the former holds . the basic tasks we deal with here are the consistency problem , brave reasoning , and skeptical reasoning . additionally , we also provide encodings for testing equivalence of theories or programs under different notions of equivalence , viz . ordinary , strong , and uniform equivalence . for all considered reasoning tasks , we analyse their computational complexity and give strict complexity bounds ."}
{"title": "deep learning works in practice . but does it work in theory ?", "abstract": "deep learning relies on a very specific kind of neural networks : those superposing several neural layers . in the last few years , deep learning achieved major breakthroughs in many tasks such as image analysis , speech recognition , natural language processing , and so on . yet , there is no theoretical explanation of this success . in particular , it is not clear why the deeper the network , the better it actually performs . we argue that the explanation is intimately connected to a key feature of the data collected from our surrounding universe to feed the machine learning algorithms : large non-parallelizable logical depth . roughly speaking , we conjecture that the shortest computational descriptions of the universe are algorithms with inherently large computation times , even when a large number of computers are available for parallelization . interestingly , this conjecture , combined with the folklore conjecture in theoretical computer science that $ p \\neq nc $ , explains the success of deep learning ."}
{"title": "the logical difference for the lightweight description logic el", "abstract": "we study a logic-based approach to versioning of ontologies . under this view , ontologies provide answers to queries about some vocabulary of interest . the difference between two versions of an ontology is given by the set of queries that receive different answers . we investigate this approach for terminologies given in the description logic el extended with role inclusions and domain and range restrictions for three distinct types of queries : subsumption , instance , and conjunctive queries . in all three cases , we present polynomial-time algorithms that decide whether two terminologies give the same answers to queries over a given vocabulary and compute a succinct representation of the difference if it is non- empty . we present an implementation , cex2 , of the developed algorithms for subsumption and instance queries and apply it to distinct versions of snomed ct and the nci ontology ."}
{"title": "the dependent doors problem : an investigation into sequential decisions without feedback", "abstract": "we introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions , without receiving feedback information on the effectiveness of previously made actions . informally , the problem considers a set of $ d $ doors that are initially closed , and the aim is to open all of them as fast as possible . to open a door , the algorithm knocks on it and it might open or not according to some probability distribution . this distribution may depend on which other doors are currently open , as well as on which other doors were open during each of the previous knocks on that door . the algorithm aims to minimize the expected time until all doors open . crucially , it must act at any time without knowing whether or which other doors have already opened . in this work , we focus on scenarios where dependencies between doors are both positively correlated and acyclic.the fundamental distribution of a door describes the probability it opens in the best of conditions ( with respect to other doors being open or closed ) . we show that if in two configurations of $ d $ doors corresponding doors share the same fundamental distribution , then these configurations have the same optimal running time up to a universal constant , no matter what are the dependencies between doors and what are the distributions . we also identify algorithms that are optimal up to a universal constant factor . for the case in which all doors share the same fundamental distribution we additionally provide a simpler algorithm , and a formula to calculate its running time . we furthermore analyse the price of lacking feedback for several configurations governed by standard fundamental distributions . in particular , we show that the price is logarithmic in $ d $ for memoryless doors , but can potentially grow to be linear in $ d $ for other distributions.we then turn our attention to investigate precise bounds . even for the case of two doors , identifying the optimal sequence is an intriguing combinatorial question . here , we study the case of two cascading memoryless doors . that is , the first door opens on each knock independently with probability $ p\\_1 $ . the second door can only open if the first door is open , in which case it will open on each knock independently with probability $ p\\_2 $ . we solve this problem almost completely by identifying algorithms that are optimal up to an additive term of 1 ."}
{"title": "renewal strings for cleaning astronomical databases", "abstract": "large astronomical databases obtained from sky surveys such as the supercosmos sky surveys ( sss ) invariably suffer from a small number of spurious records coming from artefactual effects of the telescope , satellites and junk objects in orbit around earth and physical defects on the photographic plate or ccd . though relatively small in number these spurious records present a significant problem in many situations where they can become a large proportion of the records potentially of interest to a given astronomer . in this paper we focus on the four most common causes of unwanted records in the sss : satellite or aeroplane tracks , scratches fibres and other linear phenomena introduced to the plate , circular halos around bright stars due to internal reflections within the telescope and diffraction spikes near to bright stars . accurate and robust techniques are needed for locating and flagging such spurious objects . we have developed renewal strings , a probabilistic technique combining the hough transform , renewal processes and hidden markov models which have proven highly effective in this context . the methods are applied to the sss data to develop a dataset of spurious object detections , along with confidence measures , which can allow this unwanted data to be removed from consideration . these methods are general and can be adapted to any future astronomical survey data ."}
{"title": "dcn+ : mixed objective and deep residual coattention for question answering", "abstract": "traditional models for question answering optimize using cross entropy loss , which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate . we propose a mixed objective that combines cross entropy loss with self-critical policy learning . the objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective . in addition to the mixed objective , we improve dynamic coattention networks ( dcn ) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks . our proposals improve model performance across question types and input lengths , especially for long questions that requires the ability to capture long-term dependencies . on the stanford question answering dataset , our model achieves state-of-the-art results with 75.1 % exact match accuracy and 83.1 % f1 , while the ensemble obtains 78.9 % exact match accuracy and 86.0 % f1 ."}
{"title": "an internet-enabled technology to support evolutionary design", "abstract": "this paper discusses the systematic use of product feedback information to support life-cycle design approaches and provides guidelines for developing a design at both the product and the system levels . design activities are surveyed in the light of the product life cycle , and the design information flow is interpreted from a semiotic perspective . the natural evolution of a design is considered , the notion of design expectations is introduced , and the importance of evaluation of these expectations in dynamic environments is argued . possible strategies for reconciliation of the expectations and environmental factors are described . an internet-enabled technology is proposed to monitor product functionality , usage , and operational environment and supply the designer with relevant information . a pilot study of assessing design expectations of a refrigerator is outlined , and conclusions are drawn ."}
{"title": "solving the `` false positives '' problem in fraud prediction", "abstract": "in this paper , we present an automated feature engineering based approach to dramatically reduce false positives in fraud prediction . false positives plague the fraud prediction industry . it is estimated that only 1 in 5 declared as fraud are actually fraud and roughly 1 in every 6 customers have had a valid transaction declined in the past year . to address this problem , we use the deep feature synthesis algorithm to automatically derive behavioral features based on the historical data of the card associated with a transaction . we generate 237 features ( > 100 behavioral patterns ) for each transaction , and use a random forest to learn a classifier . we tested our machine learning model on data from a large multinational bank and compared it to their existing solution . on an unseen data of 1.852 million transactions , we were able to reduce the false positives by 54 % and provide a savings of 190k euros . we also assess how to deploy this solution , and whether it necessitates streaming computation for real time scoring . we found that our solution can maintain similar benefits even when historical features are computed once every 7 days ."}
{"title": "a necessary and sufficient condition for graph matching being equivalent to the maximum weight clique problem", "abstract": "this paper formulates a necessary and sufficient condition for a generic graph matching problem to be equivalent to the maximum vertex and edge weight clique problem in a derived association graph . the consequences of this results are threefold : first , the condition is general enough to cover a broad range of practical graph matching problems ; second , a proof to establish equivalence between graph matching and clique search reduces to showing that a given graph matching problem satisfies the proposed condition ; and third , the result sets the scene for generic continuous solutions for a broad range of graph matching problems . to illustrate the mathematical framework , we apply it to a number of graph matching problems , including the problem of determining the graph edit distance ."}
{"title": "matroidal structure of rough sets based on serial and transitive relations", "abstract": "the theory of rough sets is concerned with the lower and upper approximations of objects through a binary relation on a universe . it has been applied to machine learning , knowledge discovery and data mining . the theory of matroids is a generalization of linear independence in vector spaces . it has been used in combinatorial optimization and algorithm design . in order to take advantages of both rough sets and matroids , in this paper we propose a matroidal structure of rough sets based on a serial and transitive relation on a universe . we define the family of all minimal neighborhoods of a relation on a universe , and prove it satisfy the circuit axioms of matroids when the relation is serial and transitive . in order to further study this matroidal structure , we investigate the inverse of this construction : inducing a relation by a matroid . the relationships between the upper approximation operators of rough sets based on relations and the closure operators of matroids in the above two constructions are studied . moreover , we investigate the connections between the above two constructions ."}
{"title": "model-agnostic meta-learning for fast adaptation of deep networks", "abstract": "we propose an algorithm for meta-learning that is model-agnostic , in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems , including classification , regression , and reinforcement learning . the goal of meta-learning is to train a model on a variety of learning tasks , such that it can solve new learning tasks using only a small number of training samples . in our approach , the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task . in effect , our method trains the model to be easy to fine-tune . we demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks , produces good results on few-shot regression , and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies ."}
{"title": "meta-learning evolutionary artificial neural networks", "abstract": "in this paper , we present mleann ( meta-learning evolutionary artificial neural network ) , an automatic computational framework for the adaptive optimization of artificial neural networks wherein the neural network architecture , activation function , connection weights ; learning algorithm and its parameters are adapted according to the problem . we explored the performance of mleann and conventionally designed artificial neural networks for function approximation problems . to evaluate the comparative performance , we used three different well-known chaotic time series . we also present the state of the art popular neural network learning algorithms and some experimentation results related to convergence speed and generalization performance . we explored the performance of backpropagation algorithm ; conjugate gradient algorithm , quasi-newton algorithm and levenberg-marquardt algorithm for the three chaotic time series . performances of the different learning algorithms were evaluated when the activation functions and architecture were changed . we further present the theoretical background , algorithm , design strategy and further demonstrate how effective and inevitable is the proposed mleann framework to design a neural network , which is smaller , faster and with a better generalization performance ."}
{"title": "fuzzy bayesian learning", "abstract": "in this paper we propose a novel approach for learning from data using rule based fuzzy inference systems where the model parameters are estimated using bayesian inference and markov chain monte carlo ( mcmc ) techniques . we show the applicability of the method for regression and classification tasks using synthetic data-sets and also a real world example in the financial services industry . then we demonstrate how the method can be extended for knowledge extraction to select the individual rules in a bayesian way which best explains the given data . finally we discuss the advantages and pitfalls of using this method over state-of-the-art techniques and highlight the specific class of problems where this would be useful ."}
{"title": "synthesis of shared control protocols with provable safety and performance guarantees", "abstract": "we formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications . more specifically , we introduce a modeling formalism in which both a human and an autonomy protocol can issue commands to a robot towards performing a certain task . these commands are blended into a joint input to the robot . the autonomy protocol is synthesized using an abstraction of possible human commands accounting for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand . the synthesis is designed to ensure that the resulting robot behavior satisfies given safety and performance specifications , e.g. , in temporal logic . our solution is based on nonlinear programming and we address the inherent scalability issue by presenting alternative methods . we assess the feasibility and the scalability of the approach by an experimental evaluation ."}
{"title": "mathematical model of interest matchmaking in electronic social networks", "abstract": "the problem of matchmaking in electronic social networks is formulated as an optimization problem . in particular , a function measuring the matching degree of fields of interest of a search profile with those of an advertising profile is proposed ."}
{"title": "modifying bayesian networks by probability constraints", "abstract": "this paper deals with the following problem : modify a bayesian network to satisfy a given set of probability constraints by only change its conditional probability tables , and the probability distribution of the resulting network should be as close as possible to that of the original network . we propose to solve this problem by extending ipfp ( iterative proportional fitting procedure ) to probability distributions represented by bayesian networks . the resulting algorithm e-ipfp is further developed to d-ipfp , which reduces the computational cost by decomposing a global eipfp into a set of smaller local e-ipfp problems . limited analysis is provided , including the convergence proofs of the two algorithms . computer experiments were conducted to validate the algorithms . the results are consistent with the theoretical analysis ."}
{"title": "toward negotiable reinforcement learning : shifting priorities in pareto optimal sequential decision-making", "abstract": "existing multi-objective reinforcement learning ( morl ) algorithms do not account for objectives that arise from players with differing beliefs . concretely , consider two players with different beliefs and utility functions who may cooperate to build a machine that takes actions on their behalf . a representation is needed for how much the machine 's policy will prioritize each player 's interests over time . assuming the players have reached common knowledge of their situation , this paper derives a recursion that any pareto optimal policy must satisfy . two qualitative observations can be made from the recursion : the machine must ( 1 ) use each player 's own beliefs in evaluating how well an action will serve that player 's utility function , and ( 2 ) shift the relative priority it assigns to each player 's expected utilities over time , by a factor proportional to how well that player 's beliefs predict the machine 's inputs . observation ( 2 ) represents a substantial divergence from na\\ '' { i } ve linear utility aggregation ( as in harsanyi 's utilitarian theorem , and existing morl algorithms ) , which is shown here to be inadequate for pareto optimal sequential decision-making on behalf of players with different beliefs ."}
{"title": "a fuzzy relation-based extension of reggia 's relational model for diagnosis handling uncertain and incomplete information", "abstract": "relational models for diagnosis are based on a direct description of the association between disorders and manifestations . this type of model has been specially used and developed by reggia and his co-workers in the late eighties as a basic starting point for approaching diagnosis problems . the paper proposes a new relational model which includes reggia 's model as a particular case and which allows for a more expressive representation of the observations and of the manifestations associated with disorders . the model distinguishes , i ) between manifestations which are certainly absent and those which are not ( yet ) observed , and ii ) between manifestations which can not be caused by a given disorder and manifestations for which we do not know if they can or can not be caused by this disorder . this new model , which can handle uncertainty in a non-probabilistic way , is based on possibility theory and so-called twofold fuzzy sets , previously introduced by the authors ."}
{"title": "bin packing under multiple objectives - a heuristic approximation approach", "abstract": "the article proposes a heuristic approximation approach to the bin packing problem under multiple objectives . in addition to the traditional objective of minimizing the number of bins , the heterogeneousness of the elements in each bin is minimized , leading to a biobjective formulation of the problem with a tradeoff between the number of bins and their heterogeneousness . an extension of the best-fit approximation algorithm is presented to solve the problem . experimental investigations have been carried out on benchmark instances of different size , ranging from 100 to 1000 items . encouraging results have been obtained , showing the applicability of the heuristic approach to the described problem ."}
{"title": "a complete calculus for possibilistic logic programming with fuzzy propositional variables", "abstract": "in this paper we present a propositional logic programming language for reasoning under possibilistic uncertainty and representing vague knowledge . formulas are represented by pairs ( a , c ) , where a is a many-valued proposition and c is value in the unit interval [ 0,1 ] which denotes a lower bound on the belief on a in terms of necessity measures . belief states are modeled by possibility distributions on the set of all many-valued interpretations . in this framework , ( i ) we define a syntax and a semantics of the general underlying uncertainty logic ; ( ii ) we provide a modus ponens-style calculus for a sublanguage of horn-rules and we prove that it is complete for determining the maximum degree of possibilistic belief with which a fuzzy propositional variable can be entailed from a set of formulas ; and finally , ( iii ) we show how the computation of a partial matching between fuzzy propositional variables , in terms of necessity measures for fuzzy sets , can be included in our logic programming system ."}
{"title": "on how percolation threshold affects pso performance", "abstract": "statistical evidence of the influence of neighborhood topology on the performance of particle swarm optimization ( pso ) algorithms has been shown in many works . however , little has been done about the implications could have the percolation threshold in determining the topology of this neighborhood . this work addresses this problem for individuals that , like robots , are able to sense in a limited neighborhood around them . based on the concept of percolation threshold , and more precisely , the disk percolation model in 2d , we show that better results are obtained for low values of radius , when individuals occasionally ask others their best visited positions , with the consequent decrease of computational complexity . on the other hand , since percolation threshold is a universal measure , it could have a great interest to compare the performance of different hybrid pso algorithms ."}
{"title": "hierarchically-attentive rnn for album summarization and storytelling", "abstract": "we address the problem of end-to-end visual storytelling . given a photo album , our model first selects the most representative ( summary ) photos , and then composes a natural language story for the album . for this task , we make use of the visual storytelling dataset and a model composed of three hierarchically-attentive recurrent neural nets ( rnns ) to : encode the album photos , select representative ( summary ) photos , and compose the story . automatic and human evaluations show our model achieves better performance on selection , generation , and retrieval than baselines ."}
{"title": "hadamard product for low-rank bilinear pooling", "abstract": "bilinear models provide rich representations compared with linear models . they have been applied in various visual tasks , such as object recognition , segmentation , and visual question-answering , to get state-of-the-art performances taking advantage of the expanded representations . however , bilinear representations tend to be high-dimensional , limiting the applicability to computationally complex tasks . we propose low-rank bilinear pooling using hadamard product for an efficient attention mechanism of multimodal learning . we show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the vqa dataset , having a better parsimonious property ."}
{"title": "multi-agents dynamic case based reasoning and the inverse longest common sub-sequence and individualized follow-up of learners in the cehl", "abstract": "in e-learning , there is still the problem of knowing how to ensure an individualized and continuous learner 's follow-up during learning process , indeed among the numerous tools proposed , very few systems concentrate on a real time learner 's follow-up . our work in this field develops the design and implementation of a multi-agents system based on dynamic case based reasoning which can initiate learning and provide an individualized follow-up of learner . when interacting with the platform , every learner leaves his/her traces in the machine . these traces are stored in a basis under the form of scenarios which enrich collective past experience . the system monitors , compares and analyses these traces to keep a constant intelligent watch and therefore detect difficulties hindering progress and/or avoid possible dropping out . the system can support any learning subject . the success of a case-based reasoning system depends critically on the performance of the retrieval step used and , more specifically , on similarity measure used to retrieve scenarios that are similar to the course of the learner ( traces in progress ) . we propose a complementary similarity measure , named inverse longest common sub-sequence ( ilcss ) . to help and guide the learner , the system is equipped with combined virtual and human tutors ."}
{"title": "un mod\u00e8le pour la repr\u00e9sentation des connaissances temporelles dans les documents historiques", "abstract": "processing and publishing the data of the historical sciences in the semantic web is an interesting challenge in which the representation of temporal aspects plays a key role . we propose in this paper a model of temporal knowledge representation adapted to work on historical documents . this model is based on the notion of fluent that is represented in rdf graphs . we show how this model allows to represent the knowledge necessary to the historians and how it can be used to reason on this knowledge using the swrl and sparql languages . this model is being used in a project to digitize , study and publish the manuscripts of linguist ferdinand de saussure ."}
{"title": "syntax-preserving belief change operators for logic programs", "abstract": "recent methods have adapted the well-established agm and belief base frameworks for belief change to cover belief revision in logic programs . in this study here , we present two new sets of belief change operators for logic programs . they focus on preserving the explicit relationships expressed in the rules of a program , a feature that is missing in purely semantic approaches that consider programs only in their entirety . in particular , operators of the latter class fail to satisfy preservation and support , two important properties for belief change in logic programs required to ensure intuitive results . we address this shortcoming of existing approaches by introducing partial meet and ensconcement constructions for logic program belief change , which allow us to define syntax-preserving operators that satisfy preservation and support . our work is novel in that our constructions not only preserve more information from a logic program during a change operation than existing ones , but they also facilitate natural definitions of contraction operators , the first in the field to the best of our knowledge . in order to evaluate the rationality of our operators , we translate the revision and contraction postulates from the agm and belief base frameworks to the logic programming setting . we show that our operators fully comply with the belief base framework and formally state the interdefinability between our operators . we further propose an algorithm that is based on modularising a logic program to reduce partial meet and ensconcement revisions or contractions to performing the operation only on the relevant modules of that program . finally , we compare our approach to two state-of-the-art logic program revision methods and demonstrate that our operators address the shortcomings of one and generalise the other method ."}
{"title": "bounded optimal exploration in mdp", "abstract": "within the framework of probably approximately correct markov decision processes ( pac-mdp ) , much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration . however , practical concerns require the attainment of satisfactory behavior within a short period of time . in this paper , we relax the pac-mdp conditions to reconcile theoretically driven exploration methods and practical needs . we propose simple algorithms for discrete and continuous state spaces , and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples . our algorithms also maintain anytime error bounds and average loss bounds . our approach accommodates both bayesian and non-bayesian methods ."}
{"title": "building hierarchies of concepts via crowdsourcing", "abstract": "hierarchies of concepts are useful in many applications from navigation to organization of objects . usually , a hierarchy is created in a centralized manner by employing a group of domain experts , a time-consuming and expensive process . the experts often design one single hierarchy to best explain the semantic relationships among the concepts , and ignore the natural uncertainty that may exist in the process . in this paper , we propose a crowdsourcing system to build a hierarchy and furthermore capture the underlying uncertainty . our system maintains a distribution over possible hierarchies and actively selects questions to ask using an information gain criterion . we evaluate our methodology on simulated data and on a set of real world application domains . experimental results show that our system is robust to noise , efficient in picking questions , cost-effective and builds high quality hierarchies ."}
{"title": "multiresolution recurrent neural networks : an application to dialogue response generation", "abstract": "we introduce the multiresolution recurrent neural network , which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes : a sequence of high-level coarse tokens , and a sequence of natural language tokens . there are many ways to estimate or learn the high-level coarse tokens , but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics . such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences . in contrast to the standard log- likelihood objective w.r.t . natural language tokens ( word perplexity ) , optimizing the joint log-likelihood biases the model towards modeling high-level abstractions . we apply the proposed model to the task of dialogue response generation in two challenging domains : the ubuntu technical support domain , and twitter conversations . on ubuntu , the model outperforms competing approaches by a substantial margin , achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study . on twitter , the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics . finally , our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure ."}
{"title": "low-rank matrix factorization with attributes", "abstract": "we develop a new collaborative filtering ( cf ) method that combines both previously known users ' preferences , i.e . standard cf , as well as product/user attributes , i.e . classical function approximation , to predict a given user 's interest in a particular product . our method is a generalized low rank matrix completion problem , where we learn a function whose inputs are pairs of vectors -- the standard low rank matrix completion problem being a special case where the inputs to the function are the row and column indices of the matrix . we solve this generalized matrix completion problem using tensor product kernels for which we also formally generalize standard kernel properties . benchmark experiments on movie ratings show the advantages of our generalized matrix completion method over the standard matrix completion one with no information about movies or people , as well as over standard multi-task or single task learning methods ."}
{"title": "distributed flexible nonlinear tensor factorization", "abstract": "tensor factorization is a powerful tool to analyse multi-way data . compared with traditional multi-linear methods , nonlinear tensor factorization models are capable of capturing more complex relationships in the data . however , they are computationally expensive and may suffer severe learning bias in case of extreme data sparsity . to overcome these limitations , in this paper we propose a distributed , flexible nonlinear tensor factorization model . our model can effectively avoid the expensive computations and structural restrictions of the kronecker-product in existing tgp formulations , allowing an arbitrary subset of tensorial entries to be selected to contribute to the training . at the same time , we derive a tractable and tight variational evidence lower bound ( elbo ) that enables highly decoupled , parallel computations and high-quality inference . based on the new bound , we develop a distributed inference algorithm in the mapreduce framework , which is key-value-free and can fully exploit the memory cache mechanism in fast mapreduce systems such as spark . experimental results fully demonstrate the advantages of our method over several state-of-the-art approaches , in terms of both predictive performance and computational efficiency . moreover , our approach shows a promising potential in the application of click-through-rate ( ctr ) prediction for online advertising ."}
{"title": "model-based planning in discrete action spaces", "abstract": "planning actions using learned and differentiable forward models of the world is a general approach which has a number of desirable properties , including improved sample complexity over model-free rl methods , reuse of learned models across different tasks , and the ability to perform efficient gradient-based optimization in continuous action spaces . however , this approach does not apply straightforwardly when the action space is discrete , which may have limited its adoption . in this work , we introduce two discrete planning tasks inspired by existing question-answering datasets and show that it is in fact possible to effectively perform planning via backprop in discrete action spaces using two simple yet principled modifications . our experiments show that this approach can significantly outperform model-free rl based methods and supervised imitation learners ."}
{"title": "dealing with uncertainty on the initial state of a petri net", "abstract": "this paper proposes a method to find the actual state of a complex dynamic system from information coming from the sensors on the system himself , or on its environment . the nominal evolution of the system is a priori known and can be modeled ( by an expert , for example ) , by different methods . in this paper , the petri nets have been chosen . contrary to the usual use of the petri nets , the initial state of the system is unknown . so a degree of belief is bound to each places , or set of places . the theory used to model this uncertainty is the dempster-shafer 's one which is well adapted to this type of problems . from the given petri net characterizing the nominal evolution of the dynamic system , and from the observation inputs , the proposed method allows to determine according to the reliability of the model and the inputs , the state of the system at any time ."}
{"title": "a temporal bayesian network for diagnosis and prediction", "abstract": "diagnosis and prediction in some domains , like medical and industrial diagnosis , require a representation that combines uncertainty management and temporal reasoning . based on the fact that in many cases there are few state changes in the temporal range of interest , we propose a novel representation called temporal nodes bayesian networks ( tnbn ) . in a tnbn each node represents an event or state change of a variable , and an arc corresponds to a causal-temporal relationship . the temporal intervals can differ in number and size for each temporal node , so this allows multiple granularity . our approach is contrasted with a dynamic bayesian network for a simple medical example . an empirical evaluation is presented for a more complex problem , a subsystem of a fossil power plant , in which this approach is used for fault diagnosis and prediction with good results ."}
{"title": "lexis : an optimization framework for discovering the hierarchical structure of sequential data", "abstract": "data represented as strings abounds in biology , linguistics , document mining , web search and many other fields . such data often have a hierarchical structure , either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings . we propose a framework , referred to as `` lexis '' , that produces an optimized hierarchical representation of a given set of `` target '' strings . the resulting hierarchy , `` lexis-dag '' , shows how to construct each target through the concatenation of intermediate substrings , minimizing the total number of such concatenations or dag edges . the lexis optimization problem is related to the smallest grammar problem . after we prove its np-hardness for two cost formulations , we propose an efficient greedy algorithm for the construction of lexis-dags . we also consider the problem of identifying the set of intermediate nodes ( substrings ) that collectively form the `` core '' of a lexis-dag , which is important in the analysis of lexis-dags . we show that the lexis framework can be applied in diverse applications such as optimized synthesis of dna fragments in genomic libraries , hierarchical structure discovery in protein sequences , dictionary-based text compression , and feature extraction from a set of documents ."}
{"title": "variations of the turing test in the age of internet and virtual reality", "abstract": "inspired by hofstadter 's coffee-house conversation ( 1982 ) and by the science fiction short story sam by schattschneider ( 1988 ) , we propose and discuss criteria for non-mechanical intelligence . firstly , we emphasize the practical need for such tests in view of massively multiuser online role-playing games ( mmorpgs ) and virtual reality systems like second life . secondly , we demonstrate second life as a useful framework for implementing ( some iterations of ) that test ."}
{"title": "throwing fuel on the embers : probability or dichotomy , cognitive or linguistic ?", "abstract": "prof. robert berwick 's abstract for his forthcoming invited talk at the acl2016 workshop on cognitive aspects of computational language learning revives an ancient debate . entitled `` why take a chance ? `` , berwick seems to refer implicitly to chomsky 's critique of the statistical approach of harris as well as the currently dominant paradigms in conll . berwick avoids chomsky 's use of `` innate '' but states that `` the debate over the existence of sophisticated mental grammars was settled with chomsky 's logical structure of linguistic theory ( 1957/1975 ) '' , acknowledging that `` this debate has often been revived '' . this paper agrees with the view that this debate has long since been settled , but with the opposite outcome ! given the embers have not yet died away , and the questions remain fundamental , perhaps it is appropriate to refuel the debate , so i would like to join bob in throwing fuel on this fire by reviewing the evidence against the chomskian position !"}
{"title": "on a formal model of safe and scalable self-driving cars", "abstract": "in recent years , car makers and tech companies have been racing towards self driving cars . it seems that the main parameter in this race is who will have the first car on the road . the goal of this paper is to add to the equation two additional crucial parameters . the first is standardization of safety assurance -- - what are the minimal requirements that every self-driving car must satisfy , and how can we verify these requirements . the second parameter is scalability -- - engineering solutions that lead to unleashed costs will not scale to millions of cars , which will push interest in this field into a niche academic corner , and drive the entire field into a `` winter of autonomous driving '' . in the first part of the paper we propose a white-box , interpretable , mathematical model for safety assurance , which we call responsibility-sensitive safety ( rss ) . in the second part we describe a design of a system that adheres to our safety assurance requirements and is scalable to millions of cars ."}
{"title": "a characterization of the combined effects of overlap and imbalance on the svm classifier", "abstract": "in this paper we demonstrate that two common problems in machine learning -- -imbalanced and overlapping data distributions -- -do not have independent effects on the performance of svm classifiers . this result is notable since it shows that a model of either of these factors must account for the presence of the other . our study of the relationship between these problems has lead to the discovery of a previously unreported form of `` covert '' overfitting which is resilient to commonly used empirical regularization techniques . we demonstrate the existance of this covert phenomenon through several methods based around the parametric regularization of trained svms . our findings in this area suggest a possible approach to quantifying overlap in real world data sets ."}
{"title": "a bayesian method for constructing bayesian belief networks from databases", "abstract": "this paper presents a bayesian method for constructing bayesian belief networks from a database of cases . potential applications include computer-assisted hypothesis testing , automated scientific discovery , and automated construction of probabilistic expert systems . results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases . we relate the methods in this paper to previous work , and we discuss open problems ."}
{"title": "on the relative succinctness of sentential decision diagrams", "abstract": "sentential decision diagrams ( sdds ) introduced by darwiche in 2011 are a promising representation type used in knowledge compilation . the relative succinctness of representation types is an important subject in this area . the aim of the paper is to identify which kind of boolean functions can be represented by sdds of small size with respect to the number of variables the functions are defined on . for this reason the sets of boolean functions representable by different representation types in polynomial size are investigated and sdds are compared with representation types from the classical knowledge compilation map of darwiche and marquis . ordered binary decision diagrams ( obdds ) which are a popular data structure for boolean functions are one of these representation types . sdds are more general than obdds by definition but only recently , a boolean function was presented with polynomial sdd size but exponential obdd size . this result is strengthened in several ways . the main result is a quasipolynomial simulation of sdds by equivalent unambiguous nondeterministic obdds , a nondeterministic variant where there exists exactly one accepting computation for each satisfying input . as a side effect an open problem about the relative succinctness between sdds and free binary decision diagrams ( fbdds ) which are more general than obdds is answered ."}
{"title": "an anthropic argument against the future existence of superintelligent artificial intelligence", "abstract": "this paper uses anthropic reasoning to argue for a reduced likelihood that superintelligent ai will come into existence in the future . to make this argument , a new principle is introduced : the super-strong self-sampling assumption ( ssssa ) , building on the self-sampling assumption ( ssa ) and the strong self-sampling assumption ( sssa ) . ssa uses as its sample the relevant observers , whereas sssa goes further by using observer-moments . ssssa goes further still and weights each sample proportionally , according to the size of a mind in cognitive terms . ssssa is required for human observer-samples to be typical , given by how much non-human animals outnumber humans . given ssssa , the assumption that humans experience typical observer-samples relies on a future where superintelligent ai does not dominate , which in turn reduces the likelihood of it being created at all ."}
{"title": "approximate linear programming for first-order mdps", "abstract": "we introduce a new approximate solution technique for first-order markov decision processes ( fomdps ) . representing the value function linearly w.r.t . a set of first-order basis functions , we compute suitable weights by casting the corresponding optimization as a first-order linear program and show how off-the-shelf theorem prover and lp software can be effectively used . this technique allows one to solve fomdps independent of a specific domain instantiation ; furthermore , it allows one to determine bounds on approximation error that apply equally to all domain instantiations . we apply this solution technique to the task of elevator scheduling with a rich feature space and multi-criteria additive reward , and demonstrate that it outperforms a number of intuitive , heuristicallyguided policies ."}
{"title": "on kernelization of supervised mahalanobis distance learners", "abstract": "this paper focuses on the problem of kernelizing an existing supervised mahalanobis distance learner . the following features are included in the paper . firstly , three popular learners , namely , `` neighborhood component analysis '' , `` large margin nearest neighbors '' and `` discriminant neighborhood embedding '' , which do not have kernel versions are kernelized in order to improve their classification performances . secondly , an alternative kernelization framework called `` kpca trick '' is presented . implementing a learner in the new framework gains several advantages over the standard framework , e.g . no mathematical formulas and no reprogramming are required for a kernel implementation , the framework avoids troublesome problems such as singularity , etc . thirdly , while the truths of representer theorems are just assumptions in previous papers related to ours , here , representer theorems are formally proven . the proofs validate both the kernel trick and the kpca trick in the context of mahalanobis distance learning . fourthly , unlike previous works which always apply brute force methods to select a kernel , we investigate two approaches which can be efficiently adopted to construct an appropriate kernel for a given dataset . finally , numerical results on various real-world datasets are presented ."}
{"title": "arrhythmia classification from the abductive interpretation of short single-lead ecg records", "abstract": "in this work we propose a new method for the rhythm classification of short single-lead ecg records , using a set of high-level and clinically meaningful features provided by the abductive interpretation of the records . these features include morphological and rhythm-related features that are used to build two classifiers : one that evaluates the record globally , using aggregated values for each feature ; and another one that evaluates the record as a sequence , using a recurrent neural network fed with the individual features for each detected heartbeat . the two classifiers are finally combined using the stacking technique , providing an answer by means of four target classes : normal sinus rhythm , atrial fibrillation , other anomaly , and noisy . the approach has been validated against the 2017 physionet/cinc challenge dataset , obtaining a final score of 0.83 and ranking first in the competition ."}
{"title": "minimal exploration in structured stochastic bandits", "abstract": "this paper introduces and addresses a wide class of stochastic bandit problems where the function mapping the arm to the corresponding reward exhibits some known structural properties . most existing structures ( e.g . linear , lipschitz , unimodal , combinatorial , dueling , ... ) are covered by our framework . we derive an asymptotic instance-specific regret lower bound for these problems , and develop ossb , an algorithm whose regret matches this fundamental limit . ossb is not based on the classical principle of `` optimism in the face of uncertainty '' or on thompson sampling , and rather aims at matching the minimal exploration rates of sub-optimal arms as characterized in the derivation of the regret lower bound . we illustrate the efficiency of ossb using numerical experiments in the case of the linear bandit problem and show that ossb outperforms existing algorithms , including thompson sampling ."}
{"title": "integrating defeasible argumentation and machine learning techniques", "abstract": "the field of machine learning ( ml ) is concerned with the question of how to construct algorithms that automatically improve with experience . in recent years many successful ml applications have been developed , such as datamining programs , information-filtering systems , etc . although ml algorithms allow the detection and extraction of interesting patterns of data for several kinds of problems , most of these algorithms are based on quantitative reasoning , as they rely on training data in order to infer so-called target functions . in the last years defeasible argumentation has proven to be a sound setting to formalize common-sense qualitative reasoning . this approach can be combined with other inference techniques , such as those provided by machine learning theory . in this paper we outline different alternatives for combining defeasible argumentation and machine learning techniques . we suggest how different aspects of a generic argument-based framework can be integrated with other ml-based approaches ."}
{"title": "a goal-directed implementation of query answering for hybrid mknf knowledge bases", "abstract": "ontologies and rules are usually loosely coupled in knowledge representation formalisms . in fact , ontologies use open-world reasoning while the leading semantics for rules use non-monotonic , closed-world reasoning . one exception is the tightly-coupled framework of minimal knowledge and negation as failure ( mknf ) , which allows statements about individuals to be jointly derived via entailment from an ontology and inferences from rules . nonetheless , the practical usefulness of mknf has not always been clear , although recent work has formalized a general resolution-based method for querying mknf when rules are taken to have the well-founded semantics , and the ontology is modeled by a general oracle . that work leaves open what algorithms should be used to relate the entailments of the ontology and the inferences of rules . in this paper we provide such algorithms , and describe the implementation of a query-driven system , cdf-rules , for hybrid knowledge bases combining both ( non-monotonic ) rules under the well-founded semantics and a ( monotonic ) ontology , represented by a cdf type-1 ( alq ) theory . to appear in theory and practice of logic programming ( tplp )"}
{"title": "bayesian neural networks", "abstract": "this paper describes and discusses bayesian neural network ( bnn ) . the paper showcases a few different applications of them for classification and regression problems . bnns are comprised of a probabilistic model and a neural network . the intent of such a design is to combine the strengths of neural networks and stochastic modeling . neural networks exhibit continuous function approximator capabilities . stochastic models allow direct specification of a model with known interaction between parameters to generate data . during the prediction phase , stochastic models generate a complete posterior distribution and produce probabilistic guarantees on the predictions . thus bnns are a unique combination of neural network and stochastic models with the stochastic model forming the core of this integration . bnns can then produce probabilistic guarantees on it 's predictions and also generate the distribution of parameters that it has learnt from the observations . that means , in the parameter space , one can deduce the nature and shape of the neural network 's learnt parameters . these two characteristics makes them highly attractive to theoreticians as well as practitioners . recently there has been a lot of activity in this area , with the advent of numerous probabilistic programming libraries such as : pymc3 , edward , stan etc . further this area is rapidly gaining ground as a standard machine learning approach for numerous problems"}
{"title": "( dual ) hoops have unique halving", "abstract": "continuous logic extends the multi-valued lukasiewicz logic by adding a halving operator on propositions . this extension is designed to give a more satisfactory model theory for continuous structures . the semantics of these logics can be given using specialisations of algebraic structures known as hoops . as part of an investigation into the metatheory of propositional continuous logic , we were indebted to prover9 for finding a proof of an important algebraic law ."}
{"title": "structure and problem hardness : goal asymmetry and dpll proofs in < br > sat-based planning", "abstract": "in verification and in ( optimal ) ai planning , a successful method is to formulate the application as boolean satisfiability ( sat ) , and solve it with state-of-the-art dpll-based procedures . there is a lack of understanding of why this works so well . focussing on the planning context , we identify a form of problem structure concerned with the symmetrical or asymmetrical nature of the cost of achieving the individual planning goals . we quantify this sort of structure with a simple numeric parameter called asymratio , ranging between 0 and 1. we run experiments in 10 benchmark domains from the international planning competitions since 2000 ; we show that asymratio is a good indicator of sat solver performance in 8 of these domains . we then examine carefully crafted synthetic planning domains that allow control of the amount of structure , and that are clean enough for a rigorous analysis of the combinatorial search space . the domains are parameterized by size , and by the amount of structure . the cnfs we examine are unsatisfiable , encoding one planning step less than the length of the optimal plan . we prove upper and lower bounds on the size of the best possible dpll refutations , under different settings of the amount of structure , as a function of size . we also identify the best possible sets of branching variables ( backdoors ) . with minimum asymratio , we prove exponential lower bounds , and identify minimal backdoors of size linear in the number of variables . with maximum asymratio , we identify logarithmic dpll refutations ( and backdoors ) , showing a doubly exponential gap between the two structural extreme cases . the reasons for this behavior -- the proof arguments -- illuminate the prototypical patterns of structure causing the empirical behavior observed in the competition benchmarks ."}
{"title": "revisiting simple neural networks for learning representations of knowledge graphs", "abstract": "we address the problem of learning vector representations for entities and relations in knowledge graphs ( kgs ) for knowledge base completion ( kbc ) . this problem has received significant attention in the past few years and multiple methods have been proposed . most of the existing methods in the literature use a predefined characteristic scoring function for evaluating the correctness of kg triples . these scoring functions distinguish correct triples ( high score ) from incorrect ones ( low score ) . however , their performance vary across different datasets . in this work , we demonstrate that a simple neural network based score function can consistently achieve near start-of-the-art performance on multiple datasets . we also quantitatively demonstrate biases in standard benchmark datasets , and highlight the need to perform evaluation spanning various datasets ."}
{"title": "maskgan : better text generation via filling in the______", "abstract": "neural text generation models are often autoregressive language models or seq2seq models . these models generate text by sampling words sequentially , with each word conditioned on the previous word , and are state-of-the-art for several machine translation and summarization benchmarks . these benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text . additionally , these models are typically trained via maxi- mum likelihood and teacher forcing . these methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time . we propose to improve sample quality using generative adversarial networks ( gans ) , which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation . gans were originally designed to output differentiable values , so discrete language generation is challenging for them . we claim that validation perplexity alone is not indicative of the quality of text generated by a model . we introduce an actor-critic conditional gan that fills in missing text conditioned on the surrounding context . we show qualitatively and quantitatively , evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model ."}
{"title": "hyflex : a benchmark framework for cross-domain heuristic search", "abstract": "automating the design of heuristic search methods is an active research field within computer science , artificial intelligence and operational research . in order to make these methods more generally applicable , it is important to eliminate or reduce the role of the human expert in the process of designing an effective methodology to solve a given computational search problem . researchers developing such methodologies are often constrained on the number of problem domains on which to test their adaptive , self-configuring algorithms ; which can be explained by the inherent difficulty of implementing their corresponding domain specific software components . this paper presents hyflex , a software framework for the development of cross-domain search methodologies . the framework features a common software interface for dealing with different combinatorial optimisation problems , and provides the algorithm components that are problem specific . in this way , the algorithm designer does not require a detailed knowledge the problem domains , and thus can concentrate his/her efforts in designing adaptive general-purpose heuristic search algorithms . four hard combinatorial problems are fully implemented ( maximum satisfiability , one dimensional bin packing , permutation flow shop and personnel scheduling ) , each containing a varied set of instance data ( including real-world industrial applications ) and an extensive set of problem specific heuristics and search operators . the framework forms the basis for the first international cross-domain heuristic search challenge ( chesc ) , and it is currently in use by the international research community . in summary , hyflex represents a valuable new benchmark of heuristic search generality , with which adaptive cross-domain algorithms are being easily developed , and reliably compared ."}
{"title": "dynamic coattention networks for question answering", "abstract": "several deep learning models have been proposed for question answering . however , due to their single-pass nature , they have no way to recover from local maxima corresponding to incorrect answers . to address this problem , we introduce the dynamic coattention network ( dcn ) for question answering . the dcn first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both . then a dynamic pointing decoder iterates over potential answer spans . this iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers . on the stanford question answering dataset , a single dcn model improves the previous state of the art from 71.0 % f1 to 75.9 % , while a dcn ensemble obtains 80.4 % f1 ."}
{"title": "deep reinforcement learning from raw pixels in doom", "abstract": "using current reinforcement learning methods , it has recently become possible to learn to play unknown 3d games from raw pixels . in this work , we study the challenges that arise in such complex environments , and summarize current methods to approach these . we choose a task within the doom game , that has not been approached yet . the goal for the agent is to fight enemies in a 3d world consisting of five rooms . we train the dqn and lstm-a3c algorithms on this task . results show that both algorithms learn sensible policies , but fail to achieve high scores given the amount of training . we provide insights into the learned behavior , which can serve as a valuable starting point for further research in the doom domain ."}
{"title": "towards a theory of granular sets", "abstract": "motivated by the application problem of sensor fusion the author introduced the concept of graded set . it is reasoned that in classification problem arising in an information system ( represented by information table ) , a novel set called granular set naturally arises . it is realized that in any hierarchical classification problem , granular set naturally arises . also when the target set of objects forms a graded set the lower and upper approximations of target sets form a graded set . this generalizes the concept of rough set . it is hoped that a detailed theory of granular/ graded sets finds several applications ."}
{"title": "localized partial evaluation of belief networks", "abstract": "most algorithms for propagating evidence through belief networks have been exact and exhaustive : they produce an exact ( point-valued ) marginal probability for every node in the network . often , however , an application will not need information about every n ode in the network nor will it need exact probabilities . we present the localized partial evaluation ( lpe ) propagation algorithm , which computes interval bounds on the marginal probability of a specified query node by examining a subset of the nodes in the entire network . conceptually , lpe ignores parts of the network that are `` too far away '' from the queried node to have much impact on its value . lpe has the `` anytime '' property of being able to produce better solutions ( tighter intervals ) given more time to consider more of the network ."}
{"title": "reputation-based incentive protocols in crowdsourcing applications", "abstract": "crowdsourcing websites ( e.g . yahoo ! answers , amazon mechanical turk , and etc . ) emerged in recent years that allow requesters from all around the world to post tasks and seek help from an equally global pool of workers . however , intrinsic incentive problems reside in crowdsourcing applications as workers and requester are selfish and aim to strategically maximize their own benefit . in this paper , we propose to provide incentives for workers to exert effort using a novel game-theoretic model based on repeated games . as there is always a gap in the social welfare between the non-cooperative equilibria emerging when workers pursue their self-interests and the desirable pareto efficient outcome , we propose a novel class of incentive protocols based on social norms which integrates reputation mechanisms into the existing pricing schemes currently implemented on crowdsourcing websites , in order to improve the performance of the non-cooperative equilibria emerging in such applications . we first formulate the exchanges on a crowdsourcing website as a two-sided market where requesters and workers are matched and play gift-giving games repeatedly . subsequently , we study the protocol designer 's problem of finding an optimal and sustainable ( equilibrium ) protocol which achieves the highest social welfare for that website . we prove that the proposed incentives protocol can make the website operate close to pareto efficiency . moreover , we also examine an alternative scenario , where the protocol designer aims at maximizing the revenue of the website and evaluate the performance of the optimal protocol ."}
{"title": "comparison of ontology alignment algorithms across single matching task via the mcnemar test", "abstract": "ontology alignment is widely used to find the correspondences between different ontologies in diverse fields . after discovering the alignment by methods , several performance scores are available to evaluate them . the scores require the produced alignment by a method and the reference alignment containing the underlying actual correspondences of the given ontologies . the current trend in alignment evaluation is to put forward a new score and to compare various alignments by juxtaposing their performance scores . however , it is substantially provocative to select one performance score among others for comparison . on top of that , claiming if one method has a better performance than one another can not be substantiated by solely comparing the scores . in this paper , we propose the statistical procedures which enable us to theoretically favor one method over one another . the mcnemar test is considered as a reliable and suitable means for comparing two ontology alignment methods over one matching task . the test applies to a 2 x 2 contingency table which can be constructed in two different ways based on the alignments , each of which has their own merits/pitfalls . the ways of the contingency table construction and various apposite statistics from the mcnemar test are elaborated in minute detail . in the case of having more than two alignment methods for comparison , the family-wise error rate is expected to happen . thus , the ways of preventing such an error are also discussed . a directed graph visualizes the outcome of the mcnemar test in the presence of multiple alignment methods . from this graph , it is readily understood if one method is better than one another or if their differences are imperceptible . our investigation on the methods participated in the anatomy track of oaei 2016 demonstrates that aml and cromatcher are the top two methods and dkp-aom and alin are the bottom two ones ."}
{"title": "accomplishable tasks in knowledge representation", "abstract": "knowledge representation ( kr ) is traditionally based on the logic of facts , expressed in boolean logic . however , facts about an agent can also be seen as a set of accomplished tasks by the agent . this paper proposes a new approach to kr : the notion of task logical kr based on computability logic . this notion allows the user to represent both accomplished tasks and accomplishable tasks by the agent . this notion allows us to build sophisticated krs about many interesting agents , which have not been supported by previous logical languages ."}
{"title": "relation variables in qualitative spatial reasoning", "abstract": "we study an alternative to the prevailing approach to modelling qualitative spatial reasoning ( qsr ) problems as constraint satisfaction problems . in the standard approach , a relation between objects is a constraint whereas in the alternative approach it is a variable . the relation-variable approach greatly simplifies integration and implementation of qsr . to substantiate this point , we discuss several qsr algorithms from the literature which in the relation-variable approach reduce to the customary constraint propagation algorithm enforcing generalised arc-consistency ."}
{"title": "estimation of linear , non-gaussian causal models in the presence of confounding latent variables", "abstract": "the estimation of linear causal models ( also known as structural equation models ) from data is a well-known problem which has received much attention in the past . most previous work has , however , made an explicit or implicit assumption of gaussianity , limiting the identifiability of the models . we have recently shown ( shimizu et al , 2005 ; hoyer et al , 2006 ) that for non-gaussian distributions the full causal model can be estimated in the no hidden variables case . in this contribution , we discuss the estimation of the model when confounding latent variables are present . although in this case uniqueness is no longer guaranteed , there is at most a finite set of models which can fit the data . we develop an algorithm for estimating this set , and describe numerical simulations which confirm the theoretical arguments and demonstrate the practical viability of the approach . full matlab code is provided for all simulations ."}
{"title": "science question answering using instructional materials", "abstract": "we provide a solution for elementary science test using instructional materials . we posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures ( given a corpus of question-answer pairs and instructional materials ) , and uses what it learns to answer novel elementary science questions . our evaluation shows that our framework outperforms several strong baselines ."}
{"title": "how to elicit many probabilities", "abstract": "in building bayesian belief networks , the elicitation of all probabilities required can be a major obstacle . we learned the extent of this often-cited observation in the construction of the probabilistic part of a complex influence diagram in the field of cancer treatment . based upon our negative experiences with existing methods , we designed a new method for probability elicitation from domain experts . the method combines various ideas , among which are the ideas of transcribing probabilities and of using a scale with both numerical and verbal anchors for marking assessments . in the construction of the probabilistic part of our influence diagram , the method proved to allow for the elicitation of many probabilities in little time ."}
{"title": "biomimetic use of genetic algorithms", "abstract": "genetic algorithms are considered as an original way to solve problems , probably because of their generality and of their `` blind '' nature . but gas are also unusual since the features of many implementations ( among all that could be thought of ) are principally led by the biological metaphor , while efficiency measurements intervene only afterwards . we propose here to examine the relevance of these biomimetic aspects , by pointing out some fundamental similarities and divergences between gas and the genome of living beings shaped by natural selection . one of the main differences comes from the fact that gas rely principally on the so-called implicit parallelism , while giving to the mutation/selection mechanism the second role . such differences could suggest new ways of employing gas on complex problems , using complex codings and starting from nearly homogeneous populations ."}
{"title": "reinforcement learning based recommender system using biclustering technique", "abstract": "a recommender system aims to recommend items that a user is interested in among many items . the need for the recommender system has been expanded by the information explosion . various approaches have been suggested for providing meaningful recommendations to users . one of the proposed approaches is to consider a recommender system as a markov decision process ( mdp ) problem and try to solve it using reinforcement learning ( rl ) . however , existing rl-based methods have an obvious drawback . to solve an mdp in a recommender system , they encountered a problem with the large number of discrete actions that bring rl to a larger class of problems . in this paper , we propose a novel rl-based recommender system . we formulate a recommender system as a gridworld game by using a biclustering technique that can reduce the state and action space significantly . using biclustering not only reduces space but also improves the recommendation quality effectively handling the cold-start problem . in addition , our approach can provide users with some explanation why the system recommends certain items . lastly , we examine the proposed algorithm on a real-world dataset and achieve a better performance than the widely used recommendation algorithm ."}
{"title": "technique for designing a domain ontology", "abstract": "the article describes the technique for designing a domain ontology , shows the flowchart of algorithm design and example of constructing a fragment of the ontology of the subject area of computer science is considered ."}
{"title": "optimal convergence for distributed learning with stochastic gradient methods and spectral-regularization algorithms", "abstract": "we study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel hilbert space ( rkhs ) . we first investigate distributed stochastic gradient methods ( sgm ) , with mini-batches and multi-passes over the data . we show that optimal generalization error bounds can be retained for distributed sgm provided that the partition level is not too large . we then extend our results to spectral-regularization algorithms ( sra ) , including kernel ridge regression ( krr ) , kernel principal component analysis , and gradient methods . our results are superior to the state-of-the-art theory . particularly , our results show that distributed sgm has a smaller theoretical computational complexity , compared with distributed krr and classic sgm . moreover , even for non-distributed sra , they provide the first optimal , capacity-dependent convergence rates , considering the case that the regression function may not be in the rkhs ."}
{"title": "processing information in quantum decision theory", "abstract": "a survey is given summarizing the state of the art of describing information processing in quantum decision theory , which has been recently advanced as a novel variant of decision making , based on the mathematical theory of separable hilbert spaces . this mathematical structure captures the effect of superposition of composite prospects , including many incorporated intended actions . the theory characterizes entangled decision making , non-commutativity of subsequent decisions , and intention interference . the self-consistent procedure of decision making , in the frame of the quantum decision theory , takes into account both the available objective information as well as subjective contextual effects . this quantum approach avoids any paradox typical of classical decision theory . conditional maximization of entropy , equivalent to the minimization of an information functional , makes it possible to connect the quantum and classical decision theories , showing that the latter is the limit of the former under vanishing interference terms ."}
{"title": "representing knowledge base into database for wap and web-based expert system", "abstract": "expert system is developed as consulting service for users spread or public requires affordable access . the internet has become a medium for such services , but presence of mobile devices make the access becomes more widespread by utilizing mobile web and wap ( wireless application protocol ) . applying expert systems applications over the web and wap requires a knowledge base representation that can be accessed simultaneously . this paper proposes single database to accommodate the knowledge representation with decision tree mapping approach . because of the database exist , consulting application through both web and wap can access it to provide expert system services options for more affordable for public ."}
{"title": "multi-armed bandits for intelligent tutoring systems", "abstract": "we present an approach to intelligent tutoring systems which adaptively personalizes sequences of learning activities to maximize skills acquired by students , taking into account the limited time and motivational resources . at a given point in time , the system proposes to the students the activity which makes them progress faster . we introduce two algorithms that rely on the empirical estimation of the learning progress , riarit that uses information about the difficulty of each exercise and zpdes that uses much less knowledge about the problem . the system is based on the combination of three approaches . first , it leverages recent models of intrinsically motivated learning by transposing them to active teaching , relying on empirical estimation of learning progress provided by specific activities to particular students . second , it uses state-of-the-art multi-arm bandit ( mab ) techniques to efficiently manage the exploration/exploitation challenge of this optimization process . third , it leverages expert knowledge to constrain and bootstrap initial exploration of the mab , while requiring only coarse guidance information of the expert and allowing the system to deal with didactic gaps in its knowledge . the system is evaluated in a scenario where 7-8 year old schoolchildren learn how to decompose numbers while manipulating money . systematic experiments are presented with simulated students , followed by results of a user study across a population of 400 school children ."}
{"title": "self-supervised visual planning with temporal skip connections", "abstract": "in order to autonomously learn wide repertoires of complex skills , robots must be able to learn from their own autonomously collected data , without human supervision . one learning signal that is always available for autonomously collected data is prediction : if a robot can learn to predict the future , it can use this predictive model to take actions to produce desired outcomes , such as moving an object to a particular location . however , in complex open-world scenarios , designing a representation for prediction is difficult . in this work , we instead aim to enable self-supervised robotic learning through direct video prediction : instead of attempting to design a good representation , we directly predict what the robot will see next , and then use this model to achieve desired goals . a key challenge in video prediction for robotic manipulation is handling complex spatial arrangements such as occlusions . to that end , we introduce a video prediction model that can keep track of objects through occlusion by incorporating temporal skip-connections . together with a novel planning criterion and action space formulation , we demonstrate that this model substantially outperforms prior work on video prediction-based control . our results show manipulation of objects not seen during training , handling multiple objects , and pushing objects around obstructions . these results represent a significant advance in the range and complexity of skills that can be performed entirely with self-supervised robotic learning ."}
{"title": "properties of answer set programming with convex generalized atoms", "abstract": "in recent years , answer set programming ( asp ) , logic programming under the stable model or answer set semantics , has seen several extensions by generalizing the notion of an atom in these programs : be it aggregate atoms , hex atoms , generalized quantifiers , or abstract constraints , the idea is to have more complicated satisfaction patterns in the lattice of herbrand interpretations than traditional , simple atoms . in this paper we refer to any of these constructs as generalized atoms . several semantics with differing characteristics have been proposed for these extensions , rendering the big picture somewhat blurry . in this paper , we analyze the class of programs that have convex generalized atoms ( originally proposed by liu and truszczynski in [ 10 ] ) in rule bodies and show that for this class many of the proposed semantics coincide . this is an interesting result , since recently it has been shown that this class is the precise complexity boundary for the flp semantics . we investigate whether similar results also hold for other semantics , and discuss the implications of our findings ."}
{"title": "magic sets for disjunctive datalog programs", "abstract": "in this paper , a new technique for the optimization of ( partially ) bound queries over disjunctive datalog programs with stratified negation is presented . the technique exploits the propagation of query bindings and extends the magic set ( ms ) optimization technique . an important feature of disjunctive datalog is nonmonotonicity , which calls for nondeterministic implementations , such as backtracking search . a distinguishing characteristic of the new method is that the optimization can be exploited also during the nondeterministic phase . in particular , after some assumptions have been made during the computation , parts of the program may become irrelevant to a query under these assumptions . this allows for dynamic pruning of the search space . in contrast , the effect of the previously defined ms methods for disjunctive datalog is limited to the deterministic portion of the process . in this way , the potential performance gain by using the proposed method can be exponential , as could be observed empirically . the correctness of ms is established thanks to a strong relationship between ms and unfounded sets that has not been studied in the literature before . this knowledge allows for extending the method also to programs with stratified negation in a natural way . the proposed method has been implemented in dlv and various experiments have been conducted . experimental results on synthetic data confirm the utility of ms for disjunctive datalog , and they highlight the computational gain that may be obtained by the new method w.r.t . the previously proposed ms methods for disjunctive datalog programs . further experiments on real-world data show the benefits of ms within an application scenario that has received considerable attention in recent years , the problem of answering user queries over possibly inconsistent databases originating from integration of autonomous sources of information ."}
{"title": "automated game design learning", "abstract": "while general game playing is an active field of research , the learning of game design has tended to be either a secondary goal of such research or it has been solely the domain of humans . we propose a field of research , automated game design learning ( agdl ) , with the direct purpose of learning game designs directly through interaction with games in the mode that most people experience games : via play . we detail existing work that touches the edges of this field , describe current successful projects in agdl and the theoretical foundations that enable them , point to promising applications enabled by agdl , and discuss next steps for this exciting area of study . the key moves of agdl are to use game programs as the ultimate source of truth about their own design , and to make these design properties available to other systems and avenues of inquiry ."}
{"title": "compositional distributional semantics with compact closed categories and frobenius algebras", "abstract": "this thesis contributes to ongoing research related to the categorical compositional model for natural language of coecke , sadrzadeh and clark in three ways : firstly , i propose a concrete instantiation of the abstract framework based on frobenius algebras ( joint work with sadrzadeh ) . the theory improves shortcomings of previous proposals , extends the coverage of the language , and is supported by experimental work that improves existing results . the proposed framework describes a new class of compositional models that find intuitive interpretations for a number of linguistic phenomena . secondly , i propose and evaluate in practice a new compositional methodology which explicitly deals with the different levels of lexical ambiguity ( joint work with pulman ) . a concrete algorithm is presented , based on the separation of vector disambiguation from composition in an explicit prior step . extensive experimental work shows that the proposed methodology indeed results in more accurate composite representations for the framework of coecke et al . in particular and every other class of compositional models in general . as a last contribution , i formalize the explicit treatment of lexical ambiguity in the context of the categorical framework by resorting to categorical quantum mechanics ( joint work with coecke ) . in the proposed extension , the concept of a distributional vector is replaced with that of a density matrix , which compactly represents a probability distribution over the potential different meanings of the specific word . composition takes the form of quantum measurements , leading to interesting analogies between quantum physics and linguistics ."}
{"title": "the imprecisions of precision measures in process mining", "abstract": "in process mining , precision measures are used to quantify how much a process model overapproximates the behavior seen in an event log . although several measures have been proposed throughout the years , no research has been done to validate whether these measures achieve the intended aim of quantifying over-approximation in a consistent way for all models and logs . this paper fills this gap by postulating a number of axioms for quantifying precision consistently for any log and any model . further , we show through counter-examples that none of the existing measures consistently quantifies precision ."}
{"title": "safe probability", "abstract": "we formalize the idea of probability distributions that lead to reliable predictions about some , but not all aspects of a domain . the resulting notion of ` safety ' provides a fresh perspective on foundational issues in statistics , providing a middle ground between imprecise probability and multiple-prior models on the one hand and strictly bayesian approaches on the other . it also allows us to formalize fiducial distributions in terms of the set of random variables that they can safely predict , thus taking some of the sting out of the fiducial idea . by restricting probabilistic inference to safe uses , one also automatically avoids paradoxes such as the monty hall problem . safety comes in a variety of degrees , such as `` validity '' ( the strongest notion ) , `` calibration '' , `` confidence safety '' and `` unbiasedness '' ( almost the weakest notion ) ."}
{"title": "definition and properties to assess multi-agent environments as social intelligence tests", "abstract": "social intelligence in natural and artificial systems is usually measured by the evaluation of associated traits or tasks that are deemed to represent some facets of social behaviour . the amalgamation of these traits is then used to configure the intuitive notion of social intelligence . instead , in this paper we start from a parametrised definition of social intelligence as the expected performance in a set of environments with several agents , and we assess and derive tests from it . this definition makes several dependencies explicit : ( 1 ) the definition depends on the choice ( and weight ) of environments and agents , ( 2 ) the definition may include both competitive and cooperative behaviours depending on how agents and rewards are arranged into teams , ( 3 ) the definition mostly depends on the abilities of other agents , and ( 4 ) the actual difference between social intelligence and general intelligence ( or other abilities ) depends on these choices . as a result , we address the problem of converting this definition into a more precise one where some fundamental properties ensuring social behaviour ( such as action and reward dependency and anticipation on competitive/cooperative behaviours ) are met as well as some other more instrumental properties ( such as secernment , boundedness , symmetry , validity , reliability , efficiency ) , which are convenient to convert the definition into a practical test . from the definition and the formalised properties , we take a look at several representative multi-agent environments , tests and games to see whether they meet these properties ."}
{"title": "model-based influence diagrams for machine vision", "abstract": "we show an approach to automated control of machine vision systems based on incremental creation and evaluation of a particular family of influence diagrams that represent hypotheses of imagery interpretation and possible subsequent processing decisions . in our approach , model-based machine vision techniques are integrated with hierarchical bayesian inference to provide a framework for representing and matching instances of objects and relationships in imagery and for accruing probabilities to rank order conflicting scene interpretations . we extend a result of tatman and shachter to show that the sequence of processing decisions derived from evaluating the diagrams at each stage is the same as the sequence that would have been derived by evaluating the final influence diagram that contains all random variables created during the run of the vision system ."}
{"title": "value of evidence on influence diagrams", "abstract": "in this paper , we introduce evidence propagation operations on influence diagrams and a concept of value of evidence , which measures the value of experimentation . evidence propagation operations are critical for the computation of the value of evidence , general update and inference operations in normative expert systems which are based on the influence diagram ( generalized bayesian network ) paradigm . the value of evidence allows us to compute directly an outcome sensitivity , a value of perfect information and a value of control which are used in decision analysis ( the science of decision making under uncertainty ) . more specifically , the outcome sensitivity is the maximum difference among the values of evidence , the value of perfect information is the expected value of the values of evidence , and the value of control is the optimal value of the values of evidence . we also discuss an implementation and a relative computational efficiency issues related to the value of evidence and the value of perfect information ."}
{"title": "exploiting layerwise convexity of rectifier networks with sign constrained weights", "abstract": "by introducing sign constraints on the weights , this paper proposes sign constrained rectifier networks ( scrns ) , whose training can be solved efficiently by the well known majorization-minimization ( mm ) algorithms . we prove that the proposed two-hidden-layer scrns , which exhibit negative weights in the second hidden layer and negative weights in the output layer , are capable of separating any two ( or more ) disjoint pattern sets . furthermore , the proposed two-hidden-layer scrns can decompose the patterns of each class into several clusters so that each cluster is convexly separable from all the patterns from the other classes . this provides a means to learn the pattern structures and analyse the discriminant factors between different classes of patterns ."}
{"title": "updating probabilities", "abstract": "as examples such as the monty hall puzzle show , applying conditioning to update a probability distribution on a `` naive space ' , which does not take into account the protocol used , can often lead to counterintuitive results . here we examine why . a criterion known as car ( coarsening at random ) in the statistical literature characterizes when `` naive ' conditioning in a naive space works . we show that the car condition holds rather infrequently . we then consider more generalized notions of update such as jeffrey conditioning and minimizing relative entropy ( mre ) . we give a generalization of the car condition that characterizes when jeffrey conditioning leads to appropriate answers , but show that there are no such conditions for mre . this generalizes and interconnects previous results obtained in the literature on car and mre ."}
{"title": "neural networks in 3d medical scan visualization", "abstract": "for medical volume visualization , one of the most important tasks is to reveal clinically relevant details from the 3d scan ( ct , mri ... ) , e.g . the coronary arteries , without obscuring them with less significant parts . these volume datasets contain different materials which are difficult to extract and visualize with 1d transfer functions based solely on the attenuation coefficient . multi-dimensional transfer functions allow a much more precise classification of data which makes it easier to separate different surfaces from each other . unfortunately , setting up multi-dimensional transfer functions can become a fairly complex task , generally accomplished by trial and error . this paper explains neural networks , and then presents an efficient way to speed up visualization process by semi-automatic transfer function generation . we describe how to use neural networks to detect distinctive features shown in the 2d histogram of the volume data and how to use this information for data classification ."}
{"title": "use of statistical outlier detection method in adaptive evolutionary algorithms", "abstract": "in this paper , the issue of adapting probabilities for evolutionary algorithm ( ea ) search operators is revisited . a framework is devised for distinguishing between measurements of performance and the interpretation of those measurements for purposes of adaptation . several examples of measurements and statistical interpretations are provided . probability value adaptation is tested using an ea with 10 search operators against 10 test problems with results indicating that both the type of measurement and its statistical interpretation play significant roles in ea performance . we also find that selecting operators based on the prevalence of outliers rather than on average performance is able to provide considerable improvements to adaptive methods and soundly outperforms the non-adaptive case ."}
{"title": "the stochastic firefighter problem", "abstract": "the dynamics of infectious diseases spread is crucial in determining their risk and offering ways to contain them . we study sequential vaccination of individuals in networks . in the original ( deterministic ) version of the firefighter problem , a fire breaks out at some node of a given graph . at each time step , b nodes can be protected by a firefighter and then the fire spreads to all unprotected neighbors of the nodes on fire . the process ends when the fire can no longer spread . we extend the firefighter problem to a probabilistic setting , where the infection is stochastic . we devise a simple policy that only vaccinates neighbors of infected nodes and is optimal on regular trees and on general graphs for a sufficiently large budget . we derive methods for calculating upper and lower bounds of the expected number of infected individuals , as well as provide estimates on the budget needed for containment in expectation . we calculate these explicitly on trees , d-dimensional grids , and erd\\h { o } s r\\ ' { e } nyi graphs . finally , we construct a state-dependent budget allocation strategy and demonstrate its superiority over constant budget allocation on real networks following a first order acquaintance vaccination policy ."}
{"title": "complexity of propositional abduction for restricted sets of boolean functions", "abstract": "abduction is a fundamental and important form of non-monotonic reasoning . given a knowledge base explaining how the world behaves it aims at finding an explanation for some observed manifestation . in this paper we focus on propositional abduction , where the knowledge base and the manifestation are represented by propositional formulae . the problem of deciding whether there exists an explanation has been shown to be sigmap2-complete in general . we consider variants obtained by restricting the allowed connectives in the formulae to certain sets of boolean functions . we give a complete classification of the complexity for all considerable sets of boolean functions . in this way , we identify easier cases , namely np-complete and polynomial cases ; and we highlight sources of intractability . further , we address the problem of counting the explanations and draw a complete picture for the counting complexity ."}
{"title": "drawing and analyzing causal dags with dagitty", "abstract": "dagitty is a software for drawing and analyzing causal diagrams , also known as directed acyclic graphs ( dags ) . functions include identification of minimal sufficient adjustment sets for estimating causal effects , diagnosis of insufficient or invalid adjustment via the identification of biasing paths , identification of instrumental variables , and derivation of testable implications . dagitty is provided in the hope that it is useful for researchers and students in epidemiology , sociology , psychology , and other empirical disciplines . the software should run in any web browser that supports modern javascript , html , and svg . this is the user manual for dagitty version 2.3. the manual is updated with every release of a new stable version . dagitty is available at dagitty.net ."}
{"title": "solving pomdps by searching the space of finite policies", "abstract": "solving partially observable markov decision processes ( pomdps ) is highly intractable in general , at least in part because the optimal policy may be infinitely large . in this paper , we explore the problem of finding the optimal policy from a restricted set of policies , represented as finite state automata of a given size . this problem is also intractable , but we show that the complexity can be greatly reduced when the pomdp and/or policy are further constrained . we demonstrate good empirical results with a branch-and-bound method for finding globally optimal deterministic policies , and a gradient-ascent method for finding locally optimal stochastic policies ."}
{"title": "template matching advances and applications in image analysis", "abstract": "in most computer vision and image analysis problems , it is necessary to define a similarity measure between two or more different objects or images . template matching is a classic and fundamental method used to score similarities between objects using certain mathematical algorithms . in this paper , we reviewed the basic concept of matching , as well as advances in template matching and applications such as invariant features or novel applications in medical image analysis . additionally , deformable models and templates originating from classic template matching were discussed . these models have broad applications in image registration , and they are a fundamental aspect of novel machine vision or deep learning algorithms , such as convolutional neural networks ( cnn ) , which perform shift and scale invariant functions followed by classification . in general , although template matching methods have restrictions which limit their application , they are recommended for use with other object recognition methods as pre- or post-processing steps . combining a template matching technique such as normalized cross-correlation or dice coefficient with a robust decision-making algorithm yields a significant improvement in the accuracy rate for object detection and recognition ."}
{"title": "learning to attend via word-aspect associative fusion for aspect-based sentiment analysis", "abstract": "aspect-based sentiment analysis ( absa ) tries to predict the polarity of a given document with respect to a given aspect entity . while neural network architectures have been successful in predicting the overall polarity of sentences , aspect-specific sentiment analysis still remains as an open problem . in this paper , we propose a novel method for integrating aspect information into the neural model . more specifically , we incorporate aspect information into the neural model by modeling word-aspect relationships . our novel model , \\textit { aspect fusion lstm } ( af-lstm ) learns to attend based on associative relationships between sentence words and aspect which allows our model to adaptively focus on the correct words given an aspect term . this ameliorates the flaws of other state-of-the-art models that utilize naive concatenations to model word-aspect similarity . instead , our model adopts circular convolution and circular correlation to model the similarity between aspect and words and elegantly incorporates this within a differentiable neural attention framework . finally , our model is end-to-end differentiable and highly related to convolution-correlation ( holographic like ) memories . our proposed neural model achieves state-of-the-art performance on benchmark datasets , outperforming atae-lstm by $ 4\\ % -5\\ % $ on average across multiple datasets ."}
{"title": "systems theoretic techniques for modeling , control , and decision support in complex dynamic systems", "abstract": "we discuss the problems of modeling , control , and decision support in complex dynamic systems from a general system theoretic point of view . the main characteristics of complex systems and of system approach to complex system study are considered . we provide an overview and analysis of known existing paradigms and methods of mathematical modeling and simulation of complex systems , which support the processes of control and decision making . then we continue with the general dynamic modeling and simulation technique for complex hierarchical systems functioning in control loop . architectural and structural models of computer information system intended for simulation and decision support in complex systems are presented ."}
{"title": "on the failure of the finite model property in some fuzzy description logics", "abstract": "fuzzy description logics ( dls ) are a family of logics which allow the representation of ( and the reasoning with ) structured knowledge affected by vagueness . although most of the not very expressive crisp dls , such as alc , enjoy the finite model property ( fmp ) , this is not the case once we move into the fuzzy case . in this paper we show that if we allow arbitrary knowledge bases , then the fuzzy dls alc under lukasiewicz and product fuzzy logics do not verify the fmp even if we restrict to witnessed models ; in other words , finite satisfiability and witnessed satisfiability are different for arbitrary knowledge bases . the aim of this paper is to point out the failure of fmp because it affects several algorithms published in the literature for reasoning under fuzzy alc ."}
{"title": "the world as evolving information", "abstract": "this paper discusses the benefits of describing the world as information , especially in the study of the evolution of life and cognition . traditional studies encounter problems because it is difficult to describe life and cognition in terms of matter and energy , since their laws are valid only at the physical scale . however , if matter and energy , as well as life and cognition , are described in terms of information , evolution can be described consistently as information becoming more complex . the paper presents eight tentative laws of information , valid at multiple scales , which are generalizations of darwinian , cybernetic , thermodynamic , psychological , philosophical , and complexity principles . these are further used to discuss the notions of life , cognition and their evolution ."}
{"title": "parameterized complexity results for plan reuse", "abstract": "planning is a notoriously difficult computational problem of high worst-case complexity . researchers have been investing significant efforts to develop heuristics or restrictions to make planning practically feasible . case-based planning is a heuristic approach where one tries to reuse previous experience when solving similar problems in order to avoid some of the planning effort . plan reuse may offer an interesting alternative to plan generation in some settings . we provide theoretical results that identify situations in which plan reuse is provably tractable . we perform our analysis in the framework of parameterized complexity , which supports a rigorous worst-case complexity analysis that takes structural properties of the input into account in terms of parameters . a central notion of parameterized complexity is fixed-parameter tractability which extends the classical notion of polynomial-time tractability by utilizing the effect of structural properties of the problem input . we draw a detailed map of the parameterized complexity landscape of several variants of problems that arise in the context of case-based planning . in particular , we consider the problem of reusing an existing plan , imposing various restrictions in terms of parameters , such as the number of steps that can be added to the existing plan to turn it into a solution of the planning instance at hand ."}
{"title": "exact phase transitions in random constraint satisfaction problems", "abstract": "in this paper we propose a new type of random csp model , called model rb , which is a revision to the standard model b. it is proved that phase transitions from a region where almost all problems are satisfiable to a region where almost all problems are unsatisfiable do exist for model rb as the number of variables approaches infinity . moreover , the critical values at which the phase transitions occur are also known exactly . by relating the hardness of model rb to model b , it is shown that there exist a lot of hard instances in model rb ."}
{"title": "automatic generation of constraint propagation algorithms for small finite domains", "abstract": "we study here constraint satisfaction problems that are based on predefined , explicitly given finite constraints . to solve them we propose a notion of rule consistency that can be expressed in terms of rules derived from the explicit representation of the initial constraints . this notion of local consistency is weaker than arc consistency for constraints of arbitrary arity but coincides with it when all domains are unary or binary . for boolean constraints rule consistency coincides with the closure under the well-known propagation rules for boolean constraints . by generalizing the format of the rules we obtain a characterization of arc consistency in terms of so-called inclusion rules . the advantage of rule consistency and this rule based characterization of the arc consistency is that the algorithms that enforce both notions can be automatically generated , as chr rules . so these algorithms could be integrated into constraint logic programming systems such as eclipse . we illustrate the usefulness of this approach to constraint propagation by discussing the implementations of both algorithms and their use on various examples , including boolean constraints , three valued logic of kleene , constraints dealing with waltz 's language for describing polyhedreal scenes , and allen 's qualitative approach to temporal logic ."}
{"title": "mancalog : a logic for multi-attribute network cascades ( technical report )", "abstract": "the modeling of cascade processes in multi-agent systems in the form of complex networks has in recent years become an important topic of study due to its many applications : the adoption of commercial products , spread of disease , the diffusion of an idea , etc . in this paper , we begin by identifying a desiderata of seven properties that a framework for modeling such processes should satisfy : the ability to represent attributes of both nodes and edges , an explicit representation of time , the ability to represent non-markovian temporal relationships , representation of uncertain information , the ability to represent competing cascades , allowance of non-monotonic diffusion , and computational tractability . we then present the mancalog language , a formalism based on logic programming that satisfies all these desiderata , and focus on algorithms for finding minimal models ( from which the outcome of cascades can be obtained ) as well as how this formalism can be applied in real world scenarios . we are not aware of any other formalism in the literature that meets all of the above requirements ."}
{"title": "geometric algebra model of distributed representations", "abstract": "formalism based on ga is an alternative to distributed representation models developed so far -- - smolensky 's tensor product , holographic reduced representations ( hrr ) and binary spatter code ( bsc ) . convolutions are replaced by geometric products , interpretable in terms of geometry which seems to be the most natural language for visualization of higher concepts . this paper recalls the main ideas behind the ga model and investigates recognition test results using both inner product and a clipped version of matrix representation . the influence of accidental blade equality on recognition is also studied . finally , the efficiency of the ga model is compared to that of previously developed models ."}
{"title": "moba : a new arena for game ai", "abstract": "games have always been popular testbeds for artificial intelligence ( ai ) . in the last decade , we have seen the rise of the multiple online battle arena ( moba ) games , which are the most played games nowadays . in spite of this , there are few works that explore moba as a testbed for ai research . in this paper we present and discuss the main features and opportunities offered by moba games to game ai research . we describe the various challenges faced along the game and also propose a discrete model that can be used to better understand and explore the game . with this , we aim to encourage the use of moba as a novel research platform for game ai ."}
{"title": "deep binaries : encoding semantic-rich cues for efficient textual-visual cross retrieval", "abstract": "cross-modal hashing is usually regarded as an effective technique for large-scale textual-visual cross retrieval , where data from different modalities are mapped into a shared hamming space for matching . most of the traditional textual-visual binary encoding methods only consider holistic image representations and fail to model descriptive sentences . this renders existing methods inappropriate to handle the rich semantics of informative cross-modal data for quality textual-visual search tasks . to address the problem of hashing cross-modal data with semantic-rich cues , in this paper , a novel integrated deep architecture is developed to effectively encode the detailed semantics of informative images and long descriptive sentences , named as textual-visual deep binaries ( tvdb ) . in particular , region-based convolutional networks with long short-term memory units are introduced to fully explore image regional details while semantic cues of sentences are modeled by a text convolutional network . additionally , we propose a stochastic batch-wise training routine , where high-quality binary codes and deep encoding functions are efficiently optimized in an alternating manner . experiments are conducted on three multimedia datasets , i.e . microsoft coco , iapr tc-12 , and inria web queries , where the proposed tvdb model significantly outperforms state-of-the-art binary coding methods in the task of cross-modal retrieval ."}
{"title": "an all-in-one network for dehazing and beyond", "abstract": "this paper proposes an image dehazing model built with a convolutional neural network ( cnn ) , called all-in-one dehazing network ( aod-net ) . it is designed based on a re-formulated atmospheric scattering model . instead of estimating the transmission matrix and the atmospheric light separately as most previous models did , aod-net directly generates the clean image through a light-weight cnn . such a novel end-to-end design makes it easy to embed aod-net into other deep models , e.g. , faster r-cnn , for improving high-level task performance on hazy images . experimental results on both synthesized and natural hazy image datasets demonstrate our superior performance than the state-of-the-art in terms of psnr , ssim and the subjective visual quality . furthermore , when concatenating aod-net with faster r-cnn and training the joint pipeline from end to end , we witness a large improvement of the object detection performance on hazy images ."}
{"title": "application of support vector regression to interpolation of sparse shock physics data sets", "abstract": "shock physics experiments are often complicated and expensive . as a result , researchers are unable to conduct as many experiments as they would like - leading to sparse data sets . in this paper , support vector machines for regression are applied to velocimetry data sets for shock damaged and melted tin metal . some success at interpolating between data sets is achieved . implications for future work are discussed ."}
{"title": "speculation on graph computation architectures and computing via synchronization", "abstract": "a speculative overview of a future topic of research . the paper is a collection of ideas concerning two related areas : 1 ) graph computation machines ( `` computing with graphs '' ) . this is the class of models of computation in which the state of the computation is represented as a graph or network . 2 ) arc-based neural networks , which store information not as activation in the nodes , but rather by adding and deleting arcs . sometimes the arcs may be interpreted as synchronization . warnings to readers : this is not the sort of thing that one might submit to a journal or conference . no proofs are presented . the presentation is informal , and written at an introductory level . you 'll probably want to wait for a more concise presentation ."}
{"title": "a modification to evidential probability", "abstract": "selecting the right reference class and the right interval when faced with conflicting candidates and no possibility of establishing subset style dominance has been a problem for kyburg 's evidential probability system . various methods have been proposed by loui and kyburg to solve this problem in a way that is both intuitively appealing and justifiable within kyburg 's framework . the scheme proposed in this paper leads to stronger statistical assertions without sacrificing too much of the intuitive appeal of kyburg 's latest proposal ."}
{"title": "hybridminer : mining maximal frequent itemsets using hybrid database representation approach", "abstract": "in this paper we present a novel hybrid ( arraybased layout and vertical bitmap layout ) database representation approach for mining complete maximal frequent itemset ( mfi ) on sparse and large datasets . our work is novel in terms of scalability , item search order and two horizontal and vertical projection techniques . we also present a maximal algorithm using this hybrid database representation approach . different experimental results on real and sparse benchmark datasets show that our approach is better than previous state of art maximal algorithms ."}
{"title": "device placement optimization with reinforcement learning", "abstract": "the past few years have witnessed a growth in size and computational requirements for training and inference with neural networks . currently , a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as cpus and gpus . importantly , the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions . in this paper , we propose a method which learns to optimize device placement for tensorflow computational graphs . key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a tensorflow graph should run on which of the available devices . the execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model . our main result is that on inception-v3 for imagenet classification , and on rnn lstm , for language modeling and neural machine translation , our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods ."}
{"title": "a neuro-fuzzy approach for modelling electricity demand in victoria", "abstract": "neuro-fuzzy systems have attracted growing interest of researchers in various scientific and engineering areas due to the increasing need of intelligent systems . this paper evaluates the use of two popular soft computing techniques and conventional statistical approach based on box -- jenkins autoregressive integrated moving average ( arima ) model to predict electricity demand in the state of victoria , australia . the soft computing methods considered are an evolving fuzzy neural network ( efunn ) and an artificial neural network ( ann ) trained using scaled conjugate gradient algorithm ( cga ) and backpropagation ( bp ) algorithm . the forecast accuracy is compared with the forecasts used by victorian power exchange ( vpx ) and the actual energy demand . to evaluate , we considered load demand patterns for 10 consecutive months taken every 30 min for training the different prediction models . test results show that the neuro-fuzzy system performed better than neural networks , arima model and the vpx forecasts ."}
{"title": "vector quantization as sparse least square optimization", "abstract": "vector quantization aims to form new vectors/matrices with shared values close to the original . it could compress data with acceptable information loss , and could be of great usefulness in areas like image processing , pattern recognition and machine learning . in recent years , the importance of quantization has been soaring as it has been discovered huge potentials in deploying practical neural networks , which is among one of the most popular research topics . conventional vector quantization methods usually suffer from their own flaws : hand-coding domain rules quantization could produce poor results when encountering complex data , and clustering-based algorithms have the problem of inexact solution and high time consumption . in this paper , we explored vector quantization problem from a new perspective of sparse least square optimization and designed multiple algorithms with their program implementations . specifically , deriving from a sparse form of coefficient matrix , three types of sparse least squares , with $ l_0 $ , $ l_1 $ , and generalized $ l_1 + l_2 $ penalizations , are designed and implemented respectively . in addition , to produce quantization results with given amount of quantized values ( instead of penalization coefficient $ \\lambda $ ) , this paper proposed a cluster-based least square quantization method , which could also be regarded as an improvement of information preservation of conventional clustering algorithm . the algorithms were tested on various data and tasks and their computational properties were analyzed . the paper offers a new perspective to probe the area of vector quantization , while the algorithms proposed could provide more appropriate options for quantization tasks under different circumstances ."}
{"title": "learning of human-like algebraic reasoning using deep feedforward neural networks", "abstract": "there is a wide gap between symbolic reasoning and deep learning . in this research , we explore the possibility of using deep learning to improve symbolic reasoning . briefly , in a reasoning system , a deep feedforward neural network is used to guide rewriting processes after learning from algebraic reasoning examples produced by humans . to enable the neural network to recognise patterns of algebraic expressions with non-deterministic sizes , reduced partial trees are used to represent the expressions . also , to represent both top-down and bottom-up information of the expressions , a centralisation technique is used to improve the reduced partial trees . besides , symbolic association vectors and rule application records are used to improve the rewriting processes . experimental results reveal that the algebraic reasoning examples can be accurately learnt only if the feedforward neural network has enough hidden layers . also , the centralisation technique , the symbolic association vectors and the rule application records can reduce error rates of reasoning . in particular , the above approaches have led to 4.6 % error rate of reasoning on a dataset of linear equations , differentials and integrals ."}
{"title": "a modification of particle swarm optimization using random walk", "abstract": "particle swarm optimization comes under lot of changes after james kennedy and russell eberhart first proposes the idea in 1995. the changes has been done mainly on inertia parameters in velocity updating equation so that the convergence rate will be higher . we are proposing a novel approach where particles movement will not be depend on its velocity rather it will be decided by constrained biased random walk of particles . in random walk every particles movement based on two significant parameters , one is random process like toss of a coin and other is how much displacement a particle should have . in our approach we exploit this idea by performing a biased random operation and based on the outcome of that random operation , pso particles choose the direction of the path and move non-uniformly into the solution space . this constrained , non-uniform movement helps the random walking particle to converge quicker then classical pso . in our constrained biased random walking approach , we no longer needed velocity term ( vi ) , rather we introduce a new parameter ( k ) which is a probabilistic function . no global best particle ( pgbest ) , local best particle ( plbest ) , constriction parameter ( w ) are required rather we use a new term called ptarg which is loosely influenced by pgbest.we test our algorithm on five different benchmark functions , and also compare its performance with classical pso and quantum particle swarm optimization ( qpso ) .this new approach have been shown significantly better than basic pso and sometime outperform qpso in terms of convergence , search space , number of iterations ."}
{"title": "evaluating variable length markov chain models for analysis of user web navigation sessions", "abstract": "markov models have been widely used to represent and analyse user web navigation data . in previous work we have proposed a method to dynamically extend the order of a markov chain model and a complimentary method for assessing the predictive power of such a variable length markov chain . herein , we review these two methods and propose a novel method for measuring the ability of a variable length markov model to summarise user web navigation sessions up to a given length . while the summarisation ability of a model is important to enable the identification of user navigation patterns , the ability to make predictions is important in order to foresee the next link choice of a user after following a given trail so as , for example , to personalise a web site . we present an extensive experimental evaluation providing strong evidence that prediction accuracy increases linearly with summarisation ability ."}
{"title": "simple , robust and optimal ranking from pairwise comparisons", "abstract": "we consider data in the form of pairwise comparisons of n items , with the goal of precisely identifying the top k items for some value of k < n , or alternatively , recovering a ranking of all the items . we analyze the copeland counting algorithm that ranks the items in order of the number of pairwise comparisons won , and show it has three attractive features : ( a ) its computational efficiency leads to speed-ups of several orders of magnitude in computation time as compared to prior work ; ( b ) it is robust in that theoretical guarantees impose no conditions on the underlying matrix of pairwise-comparison probabilities , in contrast to some prior work that applies only to the btl parametric model ; and ( c ) it is an optimal method up to constant factors , meaning that it achieves the information-theoretic limits for recovering the top k-subset . we extend our results to obtain sharp guarantees for approximate recovery under the hamming distortion metric , and more generally , to any arbitrary error requirement that satisfies a simple and natural monotonicity condition ."}
{"title": "rapid : a reachable anytime planner for imprecisely-sensed domains", "abstract": "despite the intractability of generic optimal partially observable markov decision process planning , there exist important problems that have highly structured models . previous researchers have used this insight to construct more efficient algorithms for factored domains , and for domains with topological structure in the flat state dynamics model . in our work , motivated by findings from the education community relevant to automated tutoring , we consider problems that exhibit a form of topological structure in the factored dynamics model . our reachable anytime planner for imprecisely-sensed domains ( rapid ) leverages this structure to efficiently compute a good initial envelope of reachable states under the optimal mdp policy in time linear in the number of state variables . rapid performs partially-observable planning over the limited envelope of states , and slowly expands the state space considered as time allows . rapid performs well on a large tutoring-inspired problem simulation with 122 state variables , corresponding to a flat state space of over 10^30 states ."}
{"title": "disan : directional self-attention network for rnn/cnn-free language understanding", "abstract": "recurrent neural nets ( rnn ) and convolutional neural nets ( cnn ) are widely used on nlp tasks to capture the long-term and local dependencies , respectively . attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation , significantly less training time , and flexibility in modeling dependencies . we propose a novel attention mechanism in which the attention between elements from input sequence ( s ) is directional and multi-dimensional ( i.e. , feature-wise ) . a light-weight neural net , `` directional self-attention network ( disan ) '' , is then proposed to learn sentence embedding , based solely on the proposed attention without any rnn/cnn structure . disan is only composed of a directional self-attention with temporal order encoded , followed by a multi-dimensional attention that compresses the sequence into a vector representation . despite its simple form , disan outperforms complicated rnn models on both prediction quality and time efficiency . it achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02 % on the stanford natural language inference ( snli ) dataset , and shows state-of-the-art test accuracy on the stanford sentiment treebank ( sst ) , multi-genre natural language inference ( multinli ) , sentences involving compositional knowledge ( sick ) , customer review , mpqa , trec question-type classification and subjectivity ( subj ) datasets ."}
{"title": "an infinite hidden markov model with similarity-biased transitions", "abstract": "we describe a generalization of the hierarchical dirichlet process hidden markov model ( hdp-hmm ) which is able to encode prior information that state transitions are more likely between `` nearby '' states . this is accomplished by defining a similarity function on the state space and scaling transition probabilities by pair-wise similarities , thereby inducing correlations among the transition distributions . we present an augmented data representation of the model as a markov jump process in which : ( 1 ) some jump attempts fail , and ( 2 ) the probability of success is proportional to the similarity between the source and destination states . this augmentation restores conditional conjugacy and admits a simple gibbs sampler . we evaluate the model and inference method on a speaker diarization task and a `` harmonic parsing '' task using four-part chorale data , as well as on several synthetic datasets , achieving favorable comparisons to existing models ."}
{"title": "learning robot activities from first-person human videos using convolutional future regression", "abstract": "we design a new approach that allows robot learning of new activities from unlabeled human example videos . given videos of humans executing the same activity from a human 's viewpoint ( i.e. , first-person videos ) , our objective is to make the robot learn the temporal structure of the activity as its future regression network , and learn to transfer such model for its own motor execution . we present a new deep learning model : we extend the state-of-the-art convolutional object detection network for the representation/estimation of human hands in training videos , and newly introduce the concept of using a fully convolutional network to regress ( i.e. , predict ) the intermediate scene representation corresponding to the future frame ( e.g. , 1-2 seconds later ) . combining these allows direct prediction of future locations of human hands and objects , which enables the robot to infer the motor control plan using our manipulation network . we experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible , and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input ."}
{"title": "translation of pronominal anaphora between english and spanish : discrepancies and evaluation", "abstract": "this paper evaluates the different tasks carried out in the translation of pronominal anaphora in a machine translation ( mt ) system . the mt interlingua approach named agir ( anaphora generation with an interlingua representation ) improves upon other proposals presented to date because it is able to translate intersentential anaphors , detect co-reference chains , and translate spanish zero pronouns into english -- -issues hardly considered by other systems . the paper presents the resolution and evaluation of these anaphora problems in agir with the use of different kinds of knowledge ( lexical , morphological , syntactic , and semantic ) . the translation of english and spanish anaphoric third-person personal pronouns ( including spanish zero pronouns ) into the target language has been evaluated on unrestricted corpora . we have obtained a precision of 80.4 % and 84.8 % in the translation of spanish and english pronouns , respectively . although we have only studied the spanish and english languages , our approach can be easily extended to other languages such as portuguese , italian , or japanese ."}
{"title": "a savage-like axiomatization for nonstandard expected utility", "abstract": "since leonard savage 's epoch-making `` foundations of statistics '' , subjective expected utility theory has been the presumptive model for decision-making . savage provided an act-based axiomatization of standard expected utility theory . in this article , we provide a savage-like axiomatization of nonstandard expected utility theory . it corresponds to a weakening of savage 's 6th axiom ."}
{"title": "model-based value estimation for efficient model-free reinforcement learning", "abstract": "recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity . such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks . unfortunately , they rely on heuristics that limit usage of the dynamics model . we present model-based value expansion , which controls for uncertainty in the model by only allowing imagination to fixed depth . by enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm , we improve value estimation , which , in turn , reduces the sample complexity of learning ."}
{"title": "task scheduling system for uav operations in indoor environment", "abstract": "application of uav in indoor environment is emerging nowadays due to the advancements in technology . uav brings more space-flexibility in an occupied or hardly-accessible indoor environment , e.g. , shop floor of manufacturing industry , greenhouse , nuclear powerplant . uav helps in creating an autonomous manufacturing system by executing tasks with less human intervention in time-efficient manner . consequently , a scheduler is one essential component to be focused on ; yet the number of reported studies on uav scheduling has been minimal . this work proposes a methodology with a heuristic ( based on earliest available time algorithm ) which assigns tasks to uavs with an objective of minimizing the makespan . in addition , a quick response towards uncertain events and a quick creation of new high-quality feasible schedule are needed . hence , the proposed heuristic is incorporated with particle swarm optimization ( pso ) algorithm to find a quick near optimal schedule . this proposed methodology is implemented into a scheduler and tested on a few scales of datasets generated based on a real flight demonstration . performance evaluation of scheduler is discussed in detail and the best solution obtained from a selected set of parameters is reported ."}
{"title": "structure and complexity in planning with unary operators", "abstract": "unary operator domains -- i.e. , domains in which operators have a single effect -- arise naturally in many control problems . in its most general form , the problem of strips planning in unary operator domains is known to be as hard as the general strips planning problem -- both are pspace-complete . however , unary operator domains induce a natural structure , called the domain 's causal graph . this graph relates between the preconditions and effect of each domain operator . causal graphs were exploited by williams and nayak in order to analyze plan generation for one of the controllers in nasa 's deep-space one spacecraft . there , they utilized the fact that when this graph is acyclic , a serialization ordering over any subgoal can be obtained quickly . in this paper we conduct a comprehensive study of the relationship between the structure of a domain 's causal graph and the complexity of planning in this domain . on the positive side , we show that a non-trivial polynomial time plan generation algorithm exists for domains whose causal graph induces a polytree with a constant bound on its node indegree . on the negative side , we show that even plan existence is hard when the graph is a directed-path singly connected dag . more generally , we show that the number of paths in the causal graph is closely related to the complexity of planning in the associated domain . finally we relate our results to the question of complexity of planning with serializable subgoals ."}
{"title": "sensitivity analysis for probability assessments in bayesian networks", "abstract": "when eliciting probability models from experts , knowledge engineers may compare the results of the model with expert judgment on test scenarios , then adjust model parameters to bring the behavior of the model more in line with the expert 's intuition . this paper presents a methodology for analytic computation of sensitivity values to measure the impact of small changes in a network parameter on a target probability value or distribution . these values can be used to guide knowledge elicitation . they can also be used in a gradient descent algorithm to estimate parameter values that maximize a measure of goodness-of-fit to both local and holistic probability assessments ."}
{"title": "improved heterogeneous distance functions", "abstract": "instance-based learning techniques typically handle continuous and linear input values well , but often do not handle nominal input attributes appropriately . the value difference metric ( vdm ) was designed to find reasonable distance values between nominal attribute values , but it largely ignores continuous attributes , requiring discretization to map continuous values into nominal values . this paper proposes three new heterogeneous distance functions , called the heterogeneous value difference metric ( hvdm ) , the interpolated value difference metric ( ivdm ) , and the windowed value difference metric ( wvdm ) . these new distance functions are designed to handle applications with nominal attributes , continuous attributes , or both . in experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes ."}
{"title": "conquering the rating bound problem in neighborhood-based collaborative filtering : a function recovery approach", "abstract": "as an important tool for information filtering in the era of socialized web , recommender systems have witnessed rapid development in the last decade . as benefited from the better interpretability , neighborhood-based collaborative filtering techniques , such as item-based collaborative filtering adopted by amazon , have gained a great success in many practical recommender systems . however , the neighborhood-based collaborative filtering method suffers from the rating bound problem , i.e. , the rating on a target item that this method estimates is bounded by the observed ratings of its all neighboring items . therefore , it can not accurately estimate the unobserved rating on a target item , if its ground truth rating is actually higher ( lower ) than the highest ( lowest ) rating over all items in its neighborhood . in this paper , we address this problem by formalizing rating estimation as a task of recovering a scalar rating function . with a linearity assumption , we infer all the ratings by optimizing the low-order norm , e.g. , the $ l_1/2 $ -norm , of the second derivative of the target scalar function , while remaining its observed ratings unchanged . experimental results on three real datasets , namely douban , goodreads and movielens , demonstrate that the proposed approach can well overcome the rating bound problem . particularly , it can significantly improve the accuracy of rating estimation by 37 % than the conventional neighborhood-based methods ."}
{"title": "a comparison of non-stationary , type-2 and dual surface fuzzy control", "abstract": "type-1 fuzzy logic has frequently been used in control systems . however this method is sometimes shown to be too restrictive and unable to adapt in the presence of uncertainty . in this paper we compare type-1 fuzzy control with several other fuzzy approaches under a range of uncertain conditions . interval type-2 and non-stationary fuzzy controllers are compared , along with 'dual surface ' type-2 control , named due to utilising both the lower and upper values produced from standard interval type-2 systems . we tune a type-1 controller , then derive the membership functions and footprints of uncertainty from the type-1 system and evaluate them using a simulated autonomous sailing problem with varying amounts of environmental uncertainty . we show that while these more sophisticated controllers can produce better performance than the type-1 controller , this is not guaranteed and that selection of footprint of uncertainty ( fou ) size has a large effect on this relative performance ."}
{"title": "memetic artificial bee colony algorithm for large-scale global optimization", "abstract": "memetic computation ( mc ) has emerged recently as a new paradigm of efficient algorithms for solving the hardest optimization problems . on the other hand , artificial bees colony ( abc ) algorithms demonstrate good performances when solving continuous and combinatorial optimization problems . this study tries to use these technologies under the same roof . as a result , a memetic abc ( mabc ) algorithm has been developed that is hybridized with two local search heuristics : the nelder-mead algorithm ( nma ) and the random walk with direction exploitation ( rwde ) . the former is attended more towards exploration , while the latter more towards exploitation of the search space . the stochastic adaptation rule was employed in order to control the balancing between exploration and exploitation . this mabc algorithm was applied to a special suite on large scale continuous global optimization at the 2012 ieee congress on evolutionary computation . the obtained results the mabc are comparable with the results of decc-g , decc-g* , and mlcc ."}
{"title": "accnet : actor-coordinator-critic net for `` learning-to-communicate '' with deep multi-agent reinforcement learning", "abstract": "communication is a critical factor for the big multi-agent world to stay organized and productive . typically , most previous multi-agent `` learning-to-communicate '' studies try to predefine the communication protocols or use technologies such as tabular reinforcement learning and evolutionary algorithm , which can not generalize to changing environment or large collection of agents . in this paper , we propose an actor-coordinator-critic net ( accnet ) framework for solving `` learning-to-communicate '' problem . the accnet naturally combines the powerful actor-critic reinforcement learning technology with deep learning technology . it can efficiently learn the communication protocols even from scratch under partially observable environment . we demonstrate that the accnet can achieve better results than several baselines under both continuous and discrete action space environments . we also analyse the learned protocols and discuss some design considerations ."}
{"title": "learning what matters - sampling interesting patterns", "abstract": "in the field of exploratory data mining , local structure in data can be described by patterns and discovered by mining algorithms . although many solutions have been proposed to address the redundancy problems in pattern mining , most of them either provide succinct pattern sets or take the interests of the user into account-but not both . consequently , the analyst has to invest substantial effort in identifying those patterns that are relevant to her specific interests and goals . to address this problem , we propose a novel approach that combines pattern sampling with interactive data mining . in particular , we introduce the letsip algorithm , which builds upon recent advances in 1 ) weighted sampling in sat and 2 ) learning to rank in interactive pattern mining . specifically , it exploits user feedback to directly learn the parameters of the sampling distribution that represents the user 's interests . we compare the performance of the proposed algorithm to the state-of-the-art in interactive pattern mining by emulating the interests of a user . the resulting system allows efficient and interleaved learning and sampling , thus user-specific anytime data exploration . finally , letsip demonstrates favourable trade-offs concerning both quality-diversity and exploitation-exploration when compared to existing methods ."}
{"title": "a polynomial-time algorithm for deciding markov equivalence of directed cyclic graphical models", "abstract": "although the concept of d-separation was originally defined for directed acyclic graphs ( see pearl 1988 ) , there is a natural extension of he concept to directed cyclic graphs . when exactly the same set of d-separation relations hold in two directed graphs , no matter whether respectively cyclic or acyclic , we say that they are markov equivalent . in other words , when two directed cyclic graphs are markov equivalent , the set of distributions that satisfy a natural extension of the global directed markov condition ( lauritzen et al . 1990 ) is exactly the same for each graph . there is an obvious exponential ( in the number of vertices ) time algorithm for deciding markov equivalence of two directed cyclic graphs ; simply chech all of the d-separation relations in each graph . in this paper i state a theorem that gives necessary and sufficient conditions for the markov equivalence of two directed cyclic graphs , where each of the conditions can be checked in polynomial time . hence , the theorem can be easily adapted into a polynomial time algorithm for deciding the markov equivalence of two directed cyclic graphs . although space prohibits inclusion of correctness proofs , they are fully described in richardson ( 1994b ) ."}
{"title": "making early predictions of the accuracy of machine learning applications", "abstract": "the accuracy of machine learning systems is a widely studied research topic . established techniques such as cross-validation predict the accuracy on unseen data of the classifier produced by applying a given learning method to a given training data set . however , they do not predict whether incurring the cost of obtaining more data and undergoing further training will lead to higher accuracy . in this paper we investigate techniques for making such early predictions . we note that when a machine learning algorithm is presented with a training set the classifier produced , and hence its error , will depend on the characteristics of the algorithm , on training set 's size , and also on its specific composition . in particular we hypothesise that if a number of classifiers are produced , and their observed error is decomposed into bias and variance terms , then although these components may behave differently , their behaviour may be predictable . we test our hypothesis by building models that , given a measurement taken from the classifier created from a limited number of samples , predict the values that would be measured from the classifier produced when the full data set is presented . we create separate models for bias , variance and total error . our models are built from the results of applying ten different machine learning algorithms to a range of data sets , and tested with `` unseen '' algorithms and datasets . we analyse the results for various numbers of initial training samples , and total dataset sizes . results show that our predictions are very highly correlated with the values observed after undertaking the extra training . finally we consider the more complex case where an ensemble of heterogeneous classifiers is trained , and show how we can accurately estimate an upper bound on the accuracy achievable after further training ."}
{"title": "semantic parsing of mathematics by context-based learning from aligned corpora and theorem proving", "abstract": "we study methods for automated parsing of informal mathematical expressions into formal ones , a main prerequisite for deep computer understanding of informal mathematical texts . we propose a context-based parsing approach that combines efficient statistical learning of deep parse trees with their semantic pruning by type checking and large-theory automated theorem proving . we show that the methods very significantly improve on previous results in parsing theorems from the flyspeck corpus ."}
{"title": "efficient multi-task feature and relationship learning", "abstract": "in this paper we propose a multi-convex framework for multi-task learning that improves predictions by learning relationships both between tasks and between features . our framework is a generalization of related methods in multi-task learning , that either learn task relationships , or feature relationships , but not both . we start with a hierarchical bayesian model , and use the empirical bayes method to transform the underlying inference problem into a multi-convex optimization problem . we propose a coordinate-wise minimization algorithm that has a closed form solution for each block subproblem . naively these solutions would be expensive to compute , but by using the theory of doubly stochastic matrices , we are able to reduce the underlying matrix optimization subproblem into a minimum weight perfect matching problem on a complete bipartite graph , and solve it analytically and efficiently . to solve the weight learning subproblem , we propose three different strategies , including a gradient descent method with linear convergence guarantee when the instances are not shared by multiple tasks , and a numerical solution based on sylvester equation when instances are shared . we demonstrate the efficiency of our method on both synthetic datasets and real-world datasets . experiments show that the proposed optimization method is orders of magnitude faster than an off-the-shelf projected gradient method , and our model is able to exploit the correlation structures among multiple tasks and features ."}
{"title": "probabilistic argumentation with epistemic extensions and incomplete information", "abstract": "abstract argumentation offers an appealing way of representing and evaluating arguments and counterarguments . this approach can be enhanced by a probability assignment to each argument . there are various interpretations that can be ascribed to this assignment . in this paper , we regard the assignment as denoting the belief that an agent has that an argument is justifiable , i.e. , that both the premises of the argument and the derivation of the claim of the argument from its premises are valid . this leads to the notion of an epistemic extension which is the subset of the arguments in the graph that are believed to some degree ( which we defined as the arguments that have a probability assignment greater than 0.5 ) . we consider various constraints on the probability assignment . some constraints correspond to standard notions of extensions , such as grounded or stable extensions , and some constraints give us new kinds of extensions ."}
{"title": "robust multi-cellular developmental design", "abstract": "this paper introduces a continuous model for multi-cellular developmental design . the cells are fixed on a 2d grid and exchange `` chemicals '' with their neighbors during the growth process . the quantity of chemicals that a cell produces , as well as the differentiation value of the cell in the phenotype , are controlled by a neural network ( the genotype ) that takes as inputs the chemicals produced by the neighboring cells at the previous time step . in the proposed model , the number of iterations of the growth process is not pre-determined , but emerges during evolution : only organisms for which the growth process stabilizes give a phenotype ( the stable state ) , others are declared nonviable . the optimization of the controller is done using the neat algorithm , that optimizes both the topology and the weights of the neural networks . though each cell only receives local information from its neighbors , the experimental results of the proposed approach on the 'flags ' problems ( the phenotype must match a given 2d pattern ) are almost as good as those of a direct regression approach using the same model with global information . moreover , the resulting multi-cellular organisms exhibit almost perfect self-healing characteristics ."}
{"title": "an algorithm for computing probabilistic propositions", "abstract": "a method for computing probabilistic propositions is presented . it assumes the availability of a single external routine for computing the probability of one instantiated variable , given a conjunction of other instantiated variables . in particular , the method allows belief network algorithms to calculate general probabilistic propositions over nodes in the network . although in the worst case the time complexity of the method is exponential in the size of a query , it is polynomial in the size of a number of common types of queries ."}
{"title": "bag-of-words representation for biomedical time series classification", "abstract": "automatic analysis of biomedical time series such as electroencephalogram ( eeg ) and electrocardiographic ( ecg ) signals has attracted great interest in the community of biomedical engineering due to its important applications in medicine . in this work , a simple yet effective bag-of-words representation that is able to capture both local and global structure similarity information is proposed for biomedical time series representation . in particular , similar to the bag-of-words model used in text document domain , the proposed method treats a time series as a text document and extracts local segments from the time series as words . the biomedical time series is then represented as a histogram of codewords , each entry of which is the count of a codeword appeared in the time series . although the temporal order of the local segments is ignored , the bag-of-words representation is able to capture high-level structural information because both local and global structural information are well utilized . the performance of the bag-of-words model is validated on three datasets extracted from real eeg and ecg signals . the experimental results demonstrate that the proposed method is not only insensitive to parameters of the bag-of-words model such as local segment length and codebook size , but also robust to noise ."}
{"title": "a sequence of relaxations constraining hidden variable models", "abstract": "many widely studied graphical models with latent variables lead to nontrivial constraints on the distribution of the observed variables . inspired by the bell inequalities in quantum mechanics , we refer to any linear inequality whose violation rules out some latent variable model as a `` hidden variable test '' for that model . our main contribution is to introduce a sequence of relaxations which provides progressively tighter hidden variable tests . we demonstrate applicability to mixtures of sequences of i.i.d . variables , bell inequalities , and homophily models in social networks . for the last , we demonstrate that our method provides a test that is able to rule out latent homophily as the sole explanation for correlations on a real social network that are known to be due to influence ."}
{"title": "gap analysis of natural language processing systems with respect to linguistic modality", "abstract": "modality is one of the important components of grammar in linguistics . it lets speaker to express attitude towards , or give assessment or potentiality of state of affairs . it implies different senses and thus has different perceptions as per the context . this paper presents an account showing the gap in the functionality of the current state of art natural language processing ( nlp ) systems . the contextual nature of linguistic modality is studied . in this paper , the works and logical approaches employed by natural language processing systems dealing with modality are reviewed . it sees human cognition and intelligence as multi-layered approach that can be implemented by intelligent systems for learning . lastly , current flow of research going on within this field is talked providing futurology ."}
{"title": "discovering markov blanket from multiple interventional datasets", "abstract": "in this paper , we study the problem of discovering the markov blanket ( mb ) of a target variable from multiple interventional datasets . datasets attained from interventional experiments contain richer causal information than passively observed data ( observational data ) for mb discovery . however , almost all existing mb discovery methods are designed for finding mbs from a single observational dataset . to identify mbs from multiple interventional datasets , we face two challenges : ( 1 ) unknown intervention variables ; ( 2 ) nonidentical data distributions . to tackle the challenges , we theoretically analyze ( a ) under what conditions we can find the correct mb of a target variable , and ( b ) under what conditions we can identify the causes of the target variable via discovering its mb . based on the theoretical analysis , we propose a new algorithm for discovering mbs from multiple interventional datasets , and present the conditions/assumptions which assure the correctness of the algorithm . to our knowledge , this work is the first to present the theoretical analyses about the conditions for mb discovery in multiple interventional datasets and the algorithm to find the mbs in relation to the conditions . using benchmark bayesian networks and real-world datasets , the experiments have validated the effectiveness and efficiency of the proposed algorithm in the paper ."}
{"title": "post-proceedings of the first international workshop on learning and nonmonotonic reasoning", "abstract": "knowledge representation and reasoning and machine learning are two important fields in ai . nonmonotonic logic programming ( nmlp ) and answer set programming ( asp ) provide formal languages for representing and reasoning with commonsense knowledge and realize declarative problem solving in ai . on the other side , inductive logic programming ( ilp ) realizes machine learning in logic programming , which provides a formal background to inductive learning and the techniques have been applied to the fields of relational learning and data mining . generally speaking , nmlp and asp realize nonmonotonic reasoning while lack the ability of learning . by contrast , ilp realizes inductive learning while most techniques have been developed under the classical monotonic logic . with this background , some researchers attempt to combine techniques in the context of nonmonotonic ilp . such combination will introduce a learning mechanism to programs and would exploit new applications on the nmlp side , while on the ilp side it will extend the representation language and enable us to use existing solvers . cross-fertilization between learning and nonmonotonic reasoning can also occur in such as the use of answer set solvers for ilp , speed-up learning while running answer set solvers , learning action theories , learning transition rules in dynamical systems , abductive learning , learning biological networks with inhibition , and applications involving default and negation . this workshop is the first attempt to provide an open forum for the identification of problems and discussion of possible collaborations among researchers with complementary expertise . the workshop was held on september 15th of 2013 in corunna , spain . this post-proceedings contains five technical papers ( out of six accepted papers ) and the abstract of the invited talk by luc de raedt ."}
{"title": "fast lexically constrained viterbi algorithm ( flcva ) : simultaneous optimization of speed and memory", "abstract": "lexical constraints on the input of speech and on-line handwriting systems improve the performance of such systems . a significant gain in speed can be achieved by integrating in a digraph structure the different hidden markov models ( hmm ) corresponding to the words of the relevant lexicon . this integration avoids redundant computations by sharing intermediate results between hmm 's corresponding to different words of the lexicon . in this paper , we introduce a token passing method to perform simultaneously the computation of the a posteriori probabilities of all the words of the lexicon . the coding scheme that we introduce for the tokens is optimal in the information theory sense . the tokens use the minimum possible number of bits . overall , we optimize simultaneously the execution speed and the memory requirement of the recognition systems ."}
{"title": "a bayesian approach to constraint based causal inference", "abstract": "we target the problem of accuracy and robustness in causal inference from finite data sets . some state-of-the-art algorithms produce clear output complete with solid theoretical guarantees but are susceptible to propagating erroneous decisions , while others are very adept at handling and representing uncertainty , but need to rely on undesirable assumptions . our aim is to combine the inherent robustness of the bayesian approach with the theoretical strength and clarity of constraint-based methods . we use a bayesian score to obtain probability estimates on the input statements used in a constraint-based procedure . these are subsequently processed in decreasing order of reliability , letting more reliable decisions take precedence in case of con icts , until a single output model is obtained . tests show that a basic implementation of the resulting bayesian constraint-based causal discovery ( bccd ) algorithm already outperforms established procedures such as fci and conservative pc . it can also indicate which causal decisions in the output have high reliability and which do not ."}
{"title": "discrete bayesian networks : the exact posterior marginal distributions", "abstract": "in a bayesian network , we wish to evaluate the marginal probability of a query variable , which may be conditioned on the observed values of some evidence variables . here we first present our `` border algorithm , '' which converts a bn into a directed chain . for the polytrees , we then present in details , with some modifications and within the border algorithm framework , the `` revised polytree algorithm '' by peot & shachter ( 1991 ) . finally , we present our `` parentless polytree method , '' which , coupled with the border algorithm , converts any bayesian network into a polytree , rendering the complexity of our inferences independent of the size of network , and linear with the number of its evidence and query variables . all quantities in this paper have probabilistic interpretations ."}
{"title": "improved relation extraction with feature-rich compositional embedding models", "abstract": "compositional embedding models build a representation ( or embedding ) for a linguistic structure based on its component word embeddings . we propose a feature-rich compositional embedding model ( fcm ) for relation extraction that is expressive , generalizes to new domains , and is easy-to-implement . the key idea is to combine both ( unlexicalized ) hand-crafted features with learned word embeddings . the model is able to directly tackle the difficulties met by traditional compositional embeddings models , such as handling arbitrary types of sentence annotations and utilizing global information for composition . we test the proposed model on two relation extraction tasks , and demonstrate that our model outperforms both previous compositional models and traditional feature rich models on the ace 2005 relation extraction task , and the semeval 2010 relation classification task . the combination of our model and a log-linear classifier with hand-crafted features gives state-of-the-art results ."}
{"title": "visual inference specification methods for modularized rulebases . overview and integration proposal", "abstract": "the paper concerns selected rule modularization techniques . three visual methods for inference specification for modularized rule- bases are described : drools flow , bpmn and xtt2 . drools flow is a popular technology for workflow or process modeling , bpmn is an omg standard for modeling business processes , and xtt2 is a hierarchical tab- ular system specification method . because of some limitations of these solutions , several proposals of their integration are given ."}
{"title": "reinforcement learning based on active learning method", "abstract": "in this paper , a new reinforcement learning approach is proposed which is based on a powerful concept named active learning method ( alm ) in modeling . alm expresses any multi-input-single-output system as a fuzzy combination of some single-input-singleoutput systems . the proposed method is an actor-critic system similar to generalized approximate reasoning based intelligent control ( garic ) structure to adapt the alm by delayed reinforcement signals . our system uses temporal difference ( td ) learning to model the behavior of useful actions of a control system . the goodness of an action is modeled on reward- penalty-plane . ids planes will be updated according to this plane . it is shown that the system can learn with a predefined fuzzy system or without it ( through random actions ) ."}
{"title": "taming numbers and durations in the model checking integrated planning system", "abstract": "the model checking integrated planning system ( mips ) is a temporal least commitment heuristic search planner based on a flexible object-oriented workbench architecture . its design clearly separates explicit and symbolic directed exploration algorithms from the set of on-line and off-line computed estimates and associated data structures . mips has shown distinguished performance in the last two international planning competitions . in the last event the description language was extended from pure propositional planning to include numerical state variables , action durations , and plan quality objective functions . plans were no longer sequences of actions but time-stamped schedules . as a participant of the fully automated track of the competition , mips has proven to be a general system ; in each track and every benchmark domain it efficiently computed plans of remarkable quality . this article introduces and analyzes the most important algorithmic novelties that were necessary to tackle the new layers of expressiveness in the benchmark problems and to achieve a high level of performance . the extensions include critical path analysis of sequentially generated plans to generate corresponding optimal parallel plans . the linear time algorithm to compute the parallel plan bypasses known np hardness results for partial ordering by scheduling plans with respect to the set of actions and the imposed precedence relations . the efficiency of this algorithm also allows us to improve the exploration guidance : for each encountered planning state the corresponding approximate sequential plan is scheduled . one major strength of mips is its static analysis phase that grounds and simplifies parameterized predicates , functions and operators , that infers knowledge to minimize the state description length , and that detects domain object symmetries . the latter aspect is analyzed in detail . mips has been developed to serve as a complete and optimal state space planner , with admissible estimates , exploration engines and branching cuts . in the competition version , however , certain performance compromises had to be made , including floating point arithmetic , weighted heuristic search exploration according to an inadmissible estimate and parameterized optimization ."}
{"title": "dynamics of knowledge in delp through argument theory change", "abstract": "this article is devoted to the study of methods to change defeasible logic programs ( de.l.p.s ) which are the knowledge bases used by the defeasible logic programming ( delp ) interpreter . delp is an argumentation formalism that allows to reason over potentially inconsistent de.l.p.s . argument theory change ( atc ) studies certain aspects of belief revision in order to make them suitable for abstract argumentation systems . in this article , abstract arguments are rendered concrete by using the particular rule-based defeasible logic adopted by delp . the objective of our proposal is to define prioritized argument revision operators \\ ` a la atc for de.l.p.s , in such a way that the newly inserted argument ends up undefeated after the revision , thus warranting its conclusion . in order to ensure this warrant , the de.l.p . has to be changed in concordance with a minimal change principle . to this end , we discuss different minimal change criteria that could be adopted . finally , an algorithm is presented , implementing the argument revision operations ."}
{"title": "objection-based causal networks", "abstract": "this paper introduces the notion of objection-based causal networks which resemble probabilistic causal networks except that they are quantified using objections . an objection is a logical sentence and denotes a condition under which a , causal dependency does not exist . objection-based causal networks enjoy almost all the properties that make probabilistic causal networks popular , with the added advantage that objections are , arguably more intuitive than probabilities ."}
{"title": "back analysis based on som-rst system", "abstract": "this paper describes application of information granulation theory , on the back analysis of jeffrey mine southeast wall quebec . in this manner , using a combining of self organizing map ( som ) and rough set theory ( rst ) , crisp and rough granules are obtained . balancing of crisp granules and sub rough granules is rendered in close-open iteration . combining of hard and soft computing , namely finite difference method ( fdm ) and computational intelligence and taking in to account missing information are two main benefits of the proposed method . as a practical example , reverse analysis on the failure of the southeast wall jeffrey mine is accomplished ."}
{"title": "a transformational characterization of markov equivalence for directed acyclic graphs with latent variables", "abstract": "different directed acyclic graphs ( dags ) may be markov equivalent in the sense that they entail the same conditional independence relations among the observed variables . chickering ( 1995 ) provided a transformational characterization of markov equivalence for dags ( with no latent variables ) , which is useful in deriving properties shared by markov equivalent dags , and , with certain generalization , is needed to prove the asymptotic correctness of a search procedure over markov equivalence classes , known as the ges algorithm . for dag models with latent variables , maximal ancestral graphs ( mags ) provide a neat representation that facilitates model search . however , no transformational characterization -- analogous to chickering 's -- of markov equivalent mags is yet available . this paper establishes such a characterization for directed mags , which we expect will have similar uses as it does for dags ."}
{"title": "the possibilities and limitations of private prediction markets", "abstract": "we consider the design of private prediction markets , financial markets designed to elicit predictions about uncertain events without revealing too much information about market participants ' actions or beliefs . our goal is to design market mechanisms in which participants ' trades or wagers influence the market 's behavior in a way that leads to accurate predictions , yet no single participant has too much influence over what others are able to observe . we study the possibilities and limitations of such mechanisms using tools from differential privacy . we begin by designing a private one-shot wagering mechanism in which bettors specify a belief about the likelihood of a future event and a corresponding monetary wager . wagers are redistributed among bettors in a way that more highly rewards those with accurate predictions . we provide a class of wagering mechanisms that are guaranteed to satisfy truthfulness , budget balance in expectation , and other desirable properties while additionally guaranteeing epsilon-joint differential privacy in the bettors ' reported beliefs , and analyze the trade-off between the achievable level of privacy and the sensitivity of a bettor 's payment to her own report . we then ask whether it is possible to obtain privacy in dynamic prediction markets , focusing our attention on the popular cost-function framework in which securities with payments linked to future events are bought and sold by an automated market maker . we show that under general conditions , it is impossible for such a market maker to simultaneously achieve bounded worst-case loss and epsilon-differential privacy without allowing the privacy guarantee to degrade extremely quickly as the number of trades grows , making such markets impractical in settings in which privacy is valued . we conclude by suggesting several avenues for potentially circumventing this lower bound ."}
{"title": "general-purpose computing on a semantic network substrate", "abstract": "this article presents a model of general-purpose computing on a semantic network substrate . the concepts presented are applicable to any semantic network representation . however , due to the standards and technological infrastructure devoted to the semantic web effort , this article is presented from this point of view . in the proposed model of computing , the application programming interface , the run-time program , and the state of the computing virtual machine are all represented in the resource description framework ( rdf ) . the implementation of the concepts presented provides a practical computing paradigm that leverages the highly-distributed and standardized representational-layer of the semantic web ."}
{"title": "spook : a system for probabilistic object-oriented knowledge representation", "abstract": "in previous work , we pointed out the limitations of standard bayesian networks as a modeling framework for large , complex domains . we proposed a new , richly structured modeling language , { em object-oriented bayesian netorks } , that we argued would be able to deal with such domains . however , it turns out that oobns are not expressive enough to model many interesting aspects of complex domains : the existence of specific named objects , arbitrary relations between objects , and uncertainty over domain structure . these aspects are crucial in real-world domains such as battlefield awareness . in this paper , we present spook , an implemented system that addresses these limitations . spook implements a more expressive language that allows it to represent the battlespace domain naturally and compactly . we present a new inference algorithm that utilizes the model structure in a fundamental way , and show empirically that it achieves orders of magnitude speedup over existing approaches ."}
{"title": "assessing the players'performance in the game of bridge : a fuzzy logic approach", "abstract": "contract bridge occupies nowadays a position of great prestige being , together with chess , the only mind games officially recognized by the international olympic committee . in the present paper an innovative method for assessing the total performance of bridge- players ' belonging to groups of special interest ( e.g . different bridge clubs during a tournament , men and women , new and old players , etc ) is introduced , which is based on principles of fuzzy logic . for this , the cohorts under assessment are represented as fuzzy subsets of a set of linguistic labels characterizing their performance and the centroid defuzzification method is used to convert the fuzzy data collected from the game to a crisp number . this new method of assessment could be used informally as a complement of the official bridge-scoring methods for statistical and other obvious reasons . two real applications related to simultaneous tournaments with pre-dealt boards , organized by the hellenic bridge federation , are also presented , illustrating the importance of our results in practice ."}
{"title": "non-markovian control with gated end-to-end memory policy networks", "abstract": "partially observable environments present an important open challenge in the domain of sequential control learning with delayed rewards . despite numerous attempts during the two last decades , the majority of reinforcement learning algorithms and associated approximate models , applied to this context , still assume markovian state transitions . in this paper , we explore the use of a recently proposed attention-based model , the gated end-to-end memory network , for sequential control . we call the resulting model the gated end-to-end memory policy network . more precisely , we use a model-free value-based algorithm to learn policies for partially observed domains using this memory-enhanced neural network . this model is end-to-end learnable and it features unbounded memory . indeed , because of its attention mechanism and associated non-parametric memory , the proposed model allows us to define an attention mechanism over the observation stream unlike recurrent models . we show encouraging results that illustrate the capability of our attention-based model in the context of the continuous-state non-stationary control problem of stock trading . we also present an openai gym environment for simulated stock exchange and explain its relevance as a benchmark for the field of non-markovian decision process learning ."}
{"title": "sort story : sorting jumbled images and captions into stories", "abstract": "temporal common sense has applications in ai tasks such as qa , multi-document summarization , and human-ai communication . we propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story , the task is to sort them such that the output sequence forms a coherent story . we present multiple approaches , via unary ( position ) and pairwise ( order ) predictions , and their ensemble-based combinations , achieving strong results on this task . we use both text-based and image-based features , which depict complementary improvements . using qualitative examples , we demonstrate that our models have learnt interesting aspects of temporal common sense ."}
{"title": "adjacency-faithfulness and conservative causal inference", "abstract": "most causal inference algorithms in the literature ( e.g. , pearl ( 2000 ) , spirtes et al . ( 2000 ) , heckerman et al . ( 1999 ) ) exploit an assumption usually referred to as the causal faithfulness or stability condition . in this paper , we highlight two components of the condition used in constraint-based algorithms , which we call `` adjacency-faithfulness '' and `` orientation-faithfulness '' . we point out that assuming adjacency-faithfulness is true , it is in principle possible to test the validity of orientation-faithfulness . based on this observation , we explore the consequence of making only the adjacency-faithfulness assumption . we show that the familiar pc algorithm has to be modified to be ( asymptotically ) correct under the weaker , adjacency-faithfulness assumption . roughly the modified algorithm , called conservative pc ( cpc ) , checks whether orientation-faithfulness holds in the orientation phase , and if not , avoids drawing certain causal conclusions the pc algorithm would draw . however , if the stronger , standard causal faithfulness condition actually obtains , the cpc algorithm is shown to output the same pattern as the pc algorithm does in the large sample limit . we also present a simulation study showing that the cpc algorithm runs almost as fast as the pc algorithm , and outputs significantly fewer false causal arrowheads than the pc algorithm does on realistic sample sizes . we end our paper by discussing how score-based algorithms such as ges perform when the adjacency-faithfulness but not the standard causal faithfulness condition holds , and how to extend our work to the fci algorithm , which allows for the possibility of latent variables ."}
{"title": "a comparative analysis of classification data mining techniques : deriving key factors useful for predicting students performance", "abstract": "students opting for engineering as their discipline is increasing rapidly . but due to various factors and inappropriate primary education in india , failure rates are high . students are unable to excel in core engineering because of complex and mathematical subjects . hence , they fail in such subjects . with the help of data mining techniques , we can predict the performance of students in terms of grades and failure in subjects . this paper performs a comparative analysis of various classification techniques , such as na\\ '' ive bayes , libsvm , j48 , random forest , and jrip and tries to choose best among these . based on the results obtained , we found that na\\ '' ive bayes is the most accurate method in terms of students failure prediction and jrip is most accurate in terms of students grade prediction . we also found that jrip marginally differs from na\\ '' ive bayes in terms of accuracy for students failure prediction and gives us a set of rules from which we derive the key factors influencing students performance . finally , we suggest various ways to mitigate these factors . this study is limited to indian education system scenarios . however , the factors found can be helpful in other scenarios as well ."}
{"title": "projective simulation for classical learning agents : a comprehensive investigation", "abstract": "we study the model of projective simulation ( ps ) , a novel approach to artificial intelligence based on stochastic processing of episodic memory which was recently introduced [ h.j . briegel and g. de las cuevas . sci . rep. 2 , 400 , ( 2012 ) ] . here we provide a detailed analysis of the model and examine its performance , including its achievable efficiency , its learning times and the way both properties scale with the problems ' dimension . in addition , we situate the ps agent in different learning scenarios , and study its learning abilities . a variety of new scenarios are being considered , thereby demonstrating the model 's flexibility . furthermore , to put the ps scheme in context , we compare its performance with those of q-learning and learning classifier systems , two popular models in the field of reinforcement learning . it is shown that ps is a competitive artificial intelligence model of unique properties and strengths ."}
{"title": "synergy : a linear planner based on genetic programming", "abstract": "in this paper we describe synergy , which is a highly parallelizable , linear planning system that is based on the genetic programming paradigm . rather than reasoning about the world it is planning for , synergy uses artificial selection , recombination and fitness measure to generate linear plans that solve conjunctive goals . we ran synergy on several domains ( e.g. , the briefcase problem and a few variants of the robot navigation problem ) , and the experimental results show that our planner is capable of handling problem instances that are one to two orders of magnitude larger than the ones solved by ucpop . in order to facilitate the search reduction and to enhance the expressive power of synergy , we also propose two major extensions to our planning system : a formalism for using hierarchical planning operators , and a framework for planning in dynamic environments ."}
{"title": "on the benefits of inoculation , an example in train scheduling", "abstract": "the local reconstruction of a railway schedule following a small perturbation of the traffic , seeking minimization of the total accumulated delay , is a very difficult and tightly constrained combinatorial problem . notoriously enough , the railway company 's public image degrades proportionally to the amount of daily delays , and the same goes for its profit ! this paper describes an inoculation procedure which greatly enhances an evolutionary algorithm for train re-scheduling . the procedure consists in building the initial population around a pre-computed solution based on problem-related information available beforehand . the optimization is performed by adapting times of departure and arrival , as well as allocation of tracks , for each train at each station . this is achieved by a permutation-based evolutionary algorithm that relies on a semi-greedy heuristic scheduler to gradually reconstruct the schedule by inserting trains one after another . experimental results are presented on various instances of a large real-world case involving around 500 trains and more than 1 million constraints . in terms of competition with commercial math ematical programming tool ilog cplex , it appears that within a large class of instances , excluding trivial instances as well as too difficult ones , and with very few exceptions , a clever initialization turns an encouraging failure into a clear-cut success auguring of substantial financial savings ."}
{"title": "a new combination approach based on improved evidence distance", "abstract": "dempster-shafer evidence theory is a powerful tool in information fusion . when the evidence are highly conflicting , the counter-intuitive results will be presented . to adress this open issue , a new method based on evidence distance of jousselme and hausdorff distance is proposed . weight of each evidence can be computed , preprocess the original evidence to generate a new evidence . the dempster 's combination rule is used to combine the new evidence . comparing with the existing methods , the new proposed method is efficient ."}
{"title": "mcmc assisted by belief propagaion", "abstract": "markov chain monte carlo ( mcmc ) and belief propagation ( bp ) are the most popular algorithms for computational inference in graphical models ( gm ) . in principle , mcmc is an exact probabilistic method which , however , often suffers from exponentially slow mixing . in contrast , bp is a deterministic method , which is typically fast , empirically very successful , however in general lacking control of accuracy over loopy graphs . in this paper , we introduce mcmc algorithms correcting the approximation error of bp , i.e. , we provide a way to compensate for bp errors via a consecutive bp-aware mcmc . our framework is based on the loop calculus ( lc ) approach which allows to express the bp error as a sum of weighted generalized loops . although the full series is computationally intractable , it is known that a truncated series , summing up all 2-regular loops , is computable in polynomial-time for planar pair-wise binary gms and it also provides a highly accurate approximation empirically . motivated by this , we first propose a polynomial-time approximation mcmc scheme for the truncated series of general ( non-planar ) pair-wise binary models . our main idea here is to use the worm algorithm , known to provide fast mixing in other ( related ) problems , and then design an appropriate rejection scheme to sample 2-regular loops . furthermore , we also design an efficient rejection-free mcmc scheme for approximating the full series . the main novelty underlying our design is in utilizing the concept of cycle basis , which provides an efficient decomposition of the generalized loops . in essence , the proposed mcmc schemes run on transformed gm built upon the non-trivial bp solution , and our experiments show that this synthesis of bp and mcmc outperforms both direct mcmc and bare bp schemes ."}
{"title": "transfer learning to learn with multitask neural model search", "abstract": "deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task . the exploration of the model design space is often made by a human expert , and optimized using a combination of grid search and search heuristics over a large space of possible choices . neural architecture search ( nas ) is a reinforcement learning approach that has been proposed to automate architecture design . nas has been successfully applied to generate neural networks that rival the best human-designed architectures . however , nas requires sampling , constructing , and training hundreds to thousands of models to achieve well-performing architectures . this procedure needs to be executed from scratch for each new task . the application of nas to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks . in this paper , we present the multitask neural model search ( mnms ) controller . our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks , thus significantly speeding up the search for new tasks . we demonstrate that mnms can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing , specialized models for each task . we then show that pre-trained mnms controllers can transfer learning to new tasks . by leveraging knowledge from previous searches , we find that pre-trained mnms models start from a better location in the search space and reduce search time on unseen tasks , while still discovering models that outperform published human-designed models ."}
{"title": "optimal experiment design for causal discovery from fixed number of experiments", "abstract": "we study the problem of causal structure learning over a set of random variables when the experimenter is allowed to perform at most $ m $ experiments in a non-adaptive manner . we consider the optimal learning strategy in terms of minimizing the portions of the structure that remains unknown given the limited number of experiments in both bayesian and minimax setting . we characterize the theoretical optimal solution and propose an algorithm , which designs the experiments efficiently in terms of time complexity . we show that for bounded degree graphs , in the minimax case and in the bayesian case with uniform priors , our proposed algorithm is a $ \\rho $ -approximation algorithm , where $ \\rho $ is independent of the order of the underlying graph . simulations on both synthetic and real data show that the performance of our algorithm is very close to the optimal solution ."}
{"title": "the role of commutativity in constraint propagation algorithms", "abstract": "constraint propagation algorithms form an important part of most of the constraint programming systems . we provide here a simple , yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way . in this framework we proceed in two steps . first , we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting . then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms . in particular , using the notions commutativity and semi-commutativity , we show that the { \\tt ac-3 } , { \\tt pc-2 } , { \\tt dac } and { \\tt dpc } algorithms for achieving ( directional ) arc consistency and ( directional ) path consistency are instances of a single generic algorithm . the work reported here extends and simplifies that of apt \\citeyear { apt99b } ."}
{"title": "adaptive load balancing : a study in multi-agent learning", "abstract": "we study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system , without use of either central coordination or explicit communication . we first define a precise framework in which to study adaptive load balancing , important features of which are its stochastic nature and the purely local information available to individual agents . given this framework , we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency . we then investigate the properties of adaptive load balancing in heterogeneous populations , and address the issue of exploration vs. exploitation in that context . finally , we show that naive use of communication may not improve , and might even harm system efficiency ."}
{"title": "human uncertainty and ranking error -- the secret of successful evaluation in predictive data mining", "abstract": "one of the most crucial issues in data mining is to model human behaviour in order to provide personalisation , adaptation and recommendation . this usually involves implicit or explicit knowledge , either by observing user interactions , or by asking users directly . but these sources of information are always subject to the volatility of human decisions , making utilised data uncertain to a particular extent . in this contribution , we elaborate on the impact of this human uncertainty when it comes to comparative assessments of different data mining approaches . in particular , we reveal two problems : ( 1 ) biasing effects on various metrics of model-based prediction and ( 2 ) the propagation of uncertainty and its thus induced error probabilities for algorithm rankings . for this purpose , we introduce a probabilistic view and prove the existence of those problems mathematically , as well as provide possible solution strategies . we exemplify our theory mainly in the context of recommender systems along with the metric rmse as a prominent example of precision quality measures ."}
{"title": "computing datalog rewritings beyond horn ontologies", "abstract": "rewriting-based approaches for answering queries over an owl 2 dl ontology have so far been developed mainly for horn fragments of owl 2 dl . in this paper , we study the possibilities of answering queries over non-horn ontologies using datalog rewritings . we prove that this is impossible in general even for very simple ontology languages , and even if ptime = np . furthermore , we present a resolution-based procedure for $ \\shi $ ontologies that , in case it terminates , produces a datalog rewriting of the ontology . our procedure necessarily terminates on dl-lite_ { bool } ^h ontologies -- -an extension of owl 2 ql with transitive roles and boolean connectives ."}
{"title": "convergent approximate solving of first-order constraints by approximate quantifiers", "abstract": "exactly solving first-order constraints ( i.e. , first-order formulas over a certain predefined structure ) can be a very hard , or even undecidable problem . in continuous structures like the real numbers it is promising to compute approximate solutions instead of exact ones . however , the quantifiers of the first-order predicate language are an obstacle to allowing approximations to arbitrary small error bounds . in this paper we solve the problem by modifying the first-order language and replacing the classical quantifiers with approximate quantifiers . these also have two additional advantages : first , they are tunable , in the sense that they allow the user to decide on the trade-off between precision and efficiency . second , they introduce additional expressivity into the first-order language by allowing reasoning over the size of solution sets ."}
{"title": "network fragments : representing knowledge for constructing probabilistic models", "abstract": "in most current applications of belief networks , domain knowledge is represented by a single belief network that applies to all problem instances in the domain . in more complex domains , problem-specific models must be constructed from a knowledge base encoding probabilistic relationships in the domain . most work in knowledge-based model construction takes the rule as the basic unit of knowledge . we present a knowledge representation framework that permits the knowledge base designer to specify knowledge in larger semantically meaningful units which we call network fragments . our framework provides for representation of asymmetric independence and canonical intercausal interaction . we discuss the combination of network fragments to form problem-specific models to reason about particular problem instances . the framework is illustrated using examples from the domain of military situation awareness ."}
{"title": "sequential prediction of social media popularity with deep temporal context networks", "abstract": "prediction of popularity has profound impact for social media , since it offers opportunities to reveal individual preference and public attention from evolutionary social systems . previous research , although achieves promising results , neglects one distinctive characteristic of social data , i.e. , sequentiality . for example , the popularity of online content is generated over time with sequential post streams of social media . to investigate the sequential prediction of popularity , we propose a novel prediction framework called deep temporal context networks ( dtcn ) by incorporating both temporal context and temporal attention into account . our dtcn contains three main components , from embedding , learning to predicting . with a joint embedding network , we obtain a unified deep representation of multi-modal user-post data in a common embedding space . then , based on the embedded data sequence over time , temporal context learning attempts to recurrently learn two adaptive temporal contexts for sequential popularity . finally , a novel temporal attention is designed to predict new popularity ( the popularity of a new user-post pair ) with temporal coherence across multiple time-scales . experiments on our released image dataset with about 600k flickr photos demonstrate that dtcn outperforms state-of-the-art deep prediction algorithms , with an average of 21.51 % relative performance improvement in the popularity prediction ( spearman ranking correlation ) ."}
{"title": "the topological fusion of bayes nets", "abstract": "bayes nets are relatively recent innovations . as a result , most of their theoretical development has focused on the simplest class of single-author models . the introduction of more sophisticated multiple-author settings raises a variety of interesting questions . one such question involves the nature of compromise and consensus . posterior compromises let each model process all data to arrive at an independent response , and then split the difference . prior compromises , on the other hand , force compromise to be reached on all points before data is observed . this paper introduces prior compromises in a bayes net setting . it outlines the problem and develops an efficient algorithm for fusing two directed acyclic graphs into a single , consensus structure , which may then be used as the basis of a prior compromise ."}
{"title": "learning to organize knowledge with n-gram machines", "abstract": "deep neural networks ( dnns ) had great success on nlp tasks such as language modeling , machine translation and certain question answering ( qa ) tasks . however , the success is limited at more knowledge intensive tasks such as qa from a big corpus . existing end-to-end deep qa models ( miller et al. , 2016 ; weston et al. , 2014 ) need to read the entire text after observing the question , and therefore their complexity in responding a question is linear in the text size . this is prohibitive for practical tasks such as qa from wikipedia , a novel , or the web . we propose to solve this scalability issue by using symbolic meaning representations , which can be indexed and retrieved efficiently with complexity that is independent of the text size . more specifically , we use sequence-to-sequence models to encode knowledge symbolically and generate programs to answer questions from the encoded knowledge . we apply our approach , called the n-gram machine ( ngm ) , to the babi tasks ( weston et al. , 2015 ) and a special version of them ( `` life-long babi '' ) which has stories of up to 10 million sentences . our experiments show that ngm can successfully solve both of these tasks accurately and efficiently . unlike fully differentiable memory models , ngm 's time complexity and answering quality are not affected by the story length . the whole system of ngm is trained end-to-end with reinforce ( williams , 1992 ) . to avoid high variance in gradient estimation , which is typical in discrete latent variable models , we use beam search instead of sampling . to tackle the exponentially large search space , we use a stabilized auto-encoding objective and a structure tweak procedure to iteratively reduce and refine the search space ."}
{"title": "the extended parameter filter", "abstract": "the parameters of temporal models , such as dynamic bayesian networks , may be modelled in a bayesian context as static or atemporal variables that influence transition probabilities at every time step . particle filters fail for models that include such variables , while methods that use gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence . storvik devised a method for incremental computation of exact sufficient statistics that , for some cases , reduces the per-sample cost to a constant . in this paper , we demonstrate a connection between storvik 's filter and a kalman filter in parameter space and establish more general conditions under which storvik 's filter works . drawing on an analogy to the extended kalman filter , we develop and analyze , both theoretically and experimentally , a taylor approximation to the parameter posterior that allows storvik 's method to be applied to a broader class of models . our experiments on both synthetic examples and real applications show improvement over existing methods ."}
{"title": "rio : minimizing user interaction in ontology debugging", "abstract": "efficient ontology debugging is a cornerstone for many activities in the context of the semantic web , especially when automatic tools produce ( parts of ) ontologies such as in the field of ontology matching . the best currently known interactive debugging systems rely upon some meta information in terms of fault probabilities , which can speed up the debugging procedure in the good case , but can also have negative impact on the performance in the bad case . the problem is that assessment of the meta information is only possible a-posteriori . consequently , as long as the actual fault is unknown , there is always some risk of suboptimal interactive diagnoses discrimination . as an alternative , one might prefer to rely on a tool which pursues a no-risk strategy . in this case , however , possibly well-chosen meta information can not be exploited , resulting again in inefficient debugging actions . in this work we present a reinforcement learning strategy that continuously adapts its behavior depending on the performance achieved and minimizes the risk of using low-quality meta information . therefore , this method is suitable for application scenarios where reliable a-priori fault estimates are difficult to obtain . using problematic ontologies in the field of ontology matching , we show that the proposed risk-aware query strategy outperforms both active learning approaches and no-risk strategies on average in terms of required amount of user interaction ."}
{"title": "an empirical evaluation of portfolios approaches for solving csps", "abstract": "recent research in areas such as sat solving and integer linear programming has shown that the performances of a single arbitrarily efficient solver can be significantly outperformed by a portfolio of possibly slower on-average solvers . we report an empirical evaluation and comparison of portfolio approaches applied to constraint satisfaction problems ( csps ) . we compared models developed on top of off-the-shelf machine learning algorithms with respect to approaches used in the sat field and adapted for csps , considering different portfolio sizes and using as evaluation metrics the number of solved problems and the time taken to solve them . results indicate that the best sat approaches have top performances also in the csp field and are slightly more competitive than simple models built on top of classification algorithms ."}
{"title": "towards a quantum world wide web", "abstract": "we elaborate a quantum model for the meaning associated with corpora of written documents , like the pages forming the world wide web . to that end , we are guided by how physicists constructed quantum theory for microscopic entities , which unlike classical objects can not be fully represented in our spatial theater . we suggest that a similar construction needs to be carried out by linguists and computational scientists , to capture the full meaning carried by collections of documental entities . more precisely , we show how to associate a quantum-like 'entity of meaning ' to a 'language entity formed by printed documents ' , considering the latter as the collection of traces that are left by the former , in specific results of search actions that we describe as measurements . in other words , we offer a perspective where a collection of documents , like the web , is described as the space of manifestation of a more complex entity - the qweb - which is the object of our modeling , drawing its inspiration from previous studies on operational-realistic approaches to quantum physics and quantum modeling of human cognition and decision-making . we emphasize that a consistent qweb model needs to account for the observed correlations between words appearing in printed documents , e.g. , co-occurrences , as the latter would depend on the 'meaning connections ' existing between the concepts that are associated with these words . in that respect , we show that both 'context and interference ( quantum ) effects ' are required to explain the probabilities calculated by counting the relative number of documents containing certain words and co-ocurrrences of words ."}
{"title": "refinement and coarsening of bayesian networks", "abstract": "in almost all situation assessment problems , it is useful to dynamically contract and expand the states under consideration as assessment proceeds . contraction is most often used to combine similar events or low probability events together in order to reduce computation . expansion is most often used to make distinctions of interest which have significant probability in order to improve the quality of the assessment . although other uncertainty calculi , notably dempster-shafer [ shafer , 1976 ] , have addressed these operations , there has not yet been any approach of refining and coarsening state spaces for the bayesian network technology . this paper presents two operations for refining and coarsening the state space in bayesian networks . we also discuss their practical implications for knowledge acquisition ."}
{"title": "teleo-reactive programs for agent control", "abstract": "a formalism is presented for computing and organizing actions for autonomous agents in dynamic environments . we introduce the notion of teleo-reactive ( t-r ) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based . in addition to continuous feedback , t-r programs support parameter binding and recursion . a primary difference between t-r programs and many other circuit-based systems is that the circuitry of t-r programs is more compact ; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs . in addition , t-r programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods . we briefly describe some experimental applications of t-r programs in the control of simulated and actual mobile robots ."}
{"title": "structure in the value function of two-player zero-sum games of incomplete information", "abstract": "zero-sum stochastic games provide a rich model for competitive decision making . however , under general forms of state uncertainty as considered in the partially observable stochastic game ( posg ) , such decision making problems are still not very well understood . this paper makes a contribution to the theory of zero-sum posgs by characterizing structure in their value function . in particular , we introduce a new formulation of the value function for zs-posgs as a function of the `` plan-time sufficient statistics '' ( roughly speaking the information distribution in the posg ) , which has the potential to enable generalization over such information distributions . we further delineate this generalization capability by proving a structural result on the shape of value function : it exhibits concavity and convexity with respect to appropriately chosen marginals of the statistic space . this result is a key pre-cursor for developing solution methods that may be able to exploit such structure . finally , we show how these results allow us to reduce a zs-posg to a `` centralized '' model with shared observations , thereby transferring results for the latter , narrower class , to games with individual ( private ) observations ."}
{"title": "loopy belief propagation , bethe free energy and graph zeta function", "abstract": "we propose a new approach to the theoretical analysis of loopy belief propagation ( lbp ) and the bethe free energy ( bfe ) by establishing a formula to connect lbp and bfe with a graph zeta function . the proposed approach is applicable to a wide class of models including multinomial and gaussian types . the connection derives a number of new theoretical results on lbp and bfe . this paper focuses two of such topics . one is the analysis of the region where the hessian of the bethe free energy is positive definite , which derives the non-convexity of bfe for graphs with multiple cycles , and a condition of convexity on a restricted set . this analysis also gives a new condition for the uniqueness of the lbp fixed point . the other result is to clarify the relation between the local stability of a fixed point of lbp and local minima of the bfe , which implies , for example , that a locally stable fixed point of the gaussian lbp is a local minimum of the gaussian bethe free energy ."}
{"title": "can : creative adversarial networks , generating `` art '' by learning about styles and deviating from style norms", "abstract": "we propose a new system for generating art . the system generates art by looking at art and learning about style ; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles . we build over generative adversarial networks ( gan ) , which have shown the ability to learn to generate novel images simulating a given distribution . we argue that such networks are limited in their ability to generate creative products in their original design . we propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing deviation from art distribution . we conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists . the results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs . human subjects even rated the generated images higher on various scales ."}
{"title": "using linear constraints for logic program termination analysis", "abstract": "it is widely acknowledged that function symbols are an important feature in answer set programming , as they make modeling easier , increase the expressive power , and allow us to deal with infinite domains . the main issue with their introduction is that the evaluation of a program might not terminate and checking whether it terminates or not is undecidable . to cope with this problem , several classes of logic programs have been proposed where the use of function symbols is restricted but the program evaluation termination is guaranteed . despite the significant body of work in this area , current approaches do not include many simple practical programs whose evaluation terminates . in this paper , we present the novel classes of rule-bounded and cycle-bounded programs , which overcome different limitations of current approaches by performing a more global analysis of how terms are propagated from the body to the head of rules . results on the correctness , the complexity , and the expressivity of the proposed approach are provided ."}
{"title": "labeled directed acyclic graphs : a generalization of context-specific independence in directed graphical models", "abstract": "we introduce a novel class of labeled directed acyclic graph ( ldag ) models for finite sets of discrete variables . ldags generalize earlier proposals for allowing local structures in the conditional probability distribution of a node , such that unrestricted label sets determine which edges can be deleted from the underlying directed acyclic graph ( dag ) for a given context . several properties of these models are derived , including a generalization of the concept of markov equivalence classes . efficient bayesian learning of ldags is enabled by introducing an ldag-based factorization of the dirichlet prior for the model parameters , such that the marginal likelihood can be calculated analytically . in addition , we develop a novel prior distribution for the model structures that can appropriately penalize a model for its labeling complexity . a non-reversible markov chain monte carlo algorithm combined with a greedy hill climbing approach is used for illustrating the useful properties of ldag models for both real and synthetic data sets ."}
{"title": "applying deep bidirectional lstm and mixture density network for basketball trajectory prediction", "abstract": "data analytics helps basketball teams to create tactics . however , manual data collection and analytics are costly and ineffective . therefore , we applied a deep bidirectional long short-term memory ( blstm ) and mixture density network ( mdn ) approach . this model is not only capable of predicting a basketball trajectory based on real data , but it also can generate new trajectory samples . it is an excellent application to help coaches and players decide when and where to shoot . its structure is particularly suitable for dealing with time series problems . blstm receives forward and backward information at the same time , while stacking multiple blstms further increases the learning ability of the model . combined with blstms , mdn is used to generate a multi-modal distribution of outputs . thus , the proposed model can , in principle , represent arbitrary conditional probability distributions of output variables . we tested our model with two experiments on three-pointer datasets from nba sportvu data . in the hit-or-miss classification experiment , the proposed model outperformed other models in terms of the convergence speed and accuracy . in the trajectory generation experiment , eight model-generated trajectories at a given time closely matched real trajectories ."}
{"title": "feature evaluation of deep convolutional neural networks for object recognition and detection", "abstract": "in this paper , we evaluate convolutional neural network ( cnn ) features using the alexnet architecture and very deep convolutional network ( vggnet ) architecture . to date , most cnn researchers have employed the last layers before output , which were extracted from the fully connected feature layers . however , since it is unlikely that feature representation effectiveness is dependent on the problem , this study evaluates additional convolutional layers that are adjacent to fully connected layers , in addition to executing simple tuning for feature concatenation ( e.g. , layer 3 + layer 5 + layer 7 ) and transformation , using tools such as principal component analysis . in our experiments , we carried out detection and classification tasks using the caltech 101 and daimler pedestrian benchmark datasets ."}
{"title": "film : visual reasoning with a general conditioning layer", "abstract": "we introduce a general-purpose conditioning method for neural networks called film : feature-wise linear modulation . film layers influence neural network computation via a simple , feature-wise affine transformation based on conditioning information . we show that film layers are highly effective for visual reasoning - answering image-related questions which require a multi-step , high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning . specifically , we show on visual reasoning tasks that film layers 1 ) halve state-of-the-art error for the clevr benchmark , 2 ) modulate features in a coherent manner , 3 ) are robust to ablations and architectural modifications , and 4 ) generalize well to challenging , new data from few examples or even zero-shot ."}
{"title": "the inductive constraint programming loop", "abstract": "constraint programming is used for a variety of real-world optimisation problems , such as planning , scheduling and resource allocation problems . at the same time , one continuously gathers vast amounts of data about these problems . current constraint programming software does not exploit such data to update schedules , resources and plans . we propose a new framework , that we call the inductive constraint programming loop . in this approach data is gathered and analyzed systematically , in order to dynamically revise and adapt constraints and optimization criteria . inductive constraint programming aims at bridging the gap between the areas of data mining and machine learning on the one hand , and constraint programming on the other hand ."}
{"title": "adaptive regularization for weight matrices", "abstract": "algorithms for learning distributions over weight-vectors , such as arow were recently shown empirically to achieve state-of-the-art performance at various problems , with strong theoretical guaranties . extending these algorithms to matrix models pose challenges since the number of free parameters in the covariance of the distribution scales as $ n^4 $ with the dimension $ n $ of the matrix , and $ n $ tends to be large in real applications . we describe , analyze and experiment with two new algorithms for learning distribution of matrix models . our first algorithm maintains a diagonal covariance over the parameters and can handle large covariance matrices . the second algorithm factors the covariance to capture inter-features correlation while keeping the number of parameters linear in the size of the original matrix . we analyze both algorithms in the mistake bound model and show a superior precision performance of our approach over other algorithms in two tasks : retrieving similar images , and ranking similar documents . the factored algorithm is shown to attain faster convergence rate ."}
{"title": "proceedings sixteenth conference on theoretical aspects of rationality and knowledge", "abstract": "this volume consists of papers presented at the sixteenth conference on theoretical aspects of rationality and knowledge ( tark ) held at the university of liverpool , uk , from july 24 to 26 , 2017. tark conferences bring together researchers from a wide variety of fields , including computer science ( especially , artificial intelligence , cryptography , distributed computing ) , economics ( especially , decision theory , game theory , social choice theory ) , linguistics , philosophy ( especially , philosophical logic ) , and cognitive psychology , in order to further understand the issues involving reasoning about rationality and knowledge ."}
{"title": "grasp and path-relinking for coalition structure generation", "abstract": "in artificial intelligence with coalition structure generation ( csg ) one refers to those cooperative complex problems that require to find an optimal partition , maximising a social welfare , of a set of entities involved in a system into exhaustive and disjoint coalitions . the solution of the csg problem finds applications in many fields such as machine learning ( covering machines , clustering ) , data mining ( decision tree , discretization ) , graph theory , natural language processing ( aggregation ) , semantic web ( service composition ) , and bioinformatics . the problem of finding the optimal coalition structure is np-complete . in this paper we present a greedy adaptive search procedure ( grasp ) with path-relinking to efficiently search the space of coalition structures . experiments and comparisons to other algorithms prove the validity of the proposed method in solving this hard combinatorial problem ."}
{"title": "self-organization of the neuron collective of optimal complexity", "abstract": "the optimal complexity of neural networks is achieved when the self-organization principles is used to eliminate the contradictions existing in accordance with the k. godel theorem about incompleteness of the systems based on axiomatics . the principle of s. beer exterior addition the heuristic group method of data handling by a. ivakhnenko realized is used ."}
{"title": "prioritized planning algorithms for trajectory coordination of multiple mobile robots", "abstract": "an important capability of autonomous multi-robot systems is to prevent collision among the individual robots . one approach to this problem is to plan conflict-free trajectories and let each of the robots follow its pre-planned trajectory . a widely used practical method for multi-robot trajectory planning is prioritized planning , which has been shown to be effective in practice , but is in general incomplete . formal analysis of instances that are provably solvable by prioritized planning is still missing . moreover , prioritized planning is a centralized algorithm , which may be in many situations undesirable . in this paper we a ) propose a revised version of prioritized planning and characterize the class of instances that are provably solvable by the algorithm and b ) propose an asynchronous decentralized variant of prioritized planning , which maintains the desirable properties of the centralized version and in the same time exploits the distributed computational power of the individual robots , which in most situations allows to find the joint trajectories faster . the experimental evaluation performed on real-world indoor maps shows that a ) the revised version of prioritized planning reliably solves a wide class of instances on which both classical prioritized planning and popular reactive technique orca fail and b ) the asynchronous decentralized algorithm provides solution faster than the previously proposed synchronized decentralized algorithm ."}
{"title": "adversarial sets for regularising neural link predictors", "abstract": "in adversarial training , a set of models learn together by pursuing competing goals , usually defined on single data instances . however , in relational learning and other non-i.i.d domains , goals can also be defined over sets of instances . for example , a link predictor for the is-a relation needs to be consistent with the transitivity property : if is-a ( x_1 , x_2 ) and is-a ( x_2 , x_3 ) hold , is-a ( x_1 , x_3 ) needs to hold as well . here we use such assumptions for deriving an inconsistency loss , measuring the degree to which the model violates the assumptions on an adversarially-generated set of examples . the training objective is defined as a minimax problem , where an adversary finds the most offending adversarial examples by maximising the inconsistency loss , and the model is trained by jointly minimising a supervised loss and the inconsistency loss on the adversarial examples . this yields the first method that can use function-free horn clauses ( as in datalog ) to regularise any neural link predictor , with complexity independent of the domain size . we show that for several link prediction models , the optimisation problem faced by the adversary has efficient closed-form solutions . experiments on link prediction benchmarks indicate that given suitable prior knowledge , our method can significantly improve neural link predictors on all relevant metrics ."}
{"title": "automatic extraction of paraphrastic phrases from medium size corpora", "abstract": "this paper presents a versatile system intended to acquire paraphrastic phrases from a representative corpus . in order to decrease the time spent on the elaboration of resources for nlp system ( for example information extraction , ie hereafter ) , we suggest to use a machine learning system that helps defining new templates and associated resources . this knowledge is automatically derived from the text collection , in interaction with a large semantic network ."}
{"title": "the automatic training of rule bases that use numerical uncertainty representations", "abstract": "the use of numerical uncertainty representations allows better modeling of some aspects of human evidential reasoning . it also makes knowledge acquisition and system development , test , and modification more difficult . we propose that where possible , the assignment and/or refinement of rule weights should be performed automatically . we present one approach to performing this training - numerical optimization - and report on the results of some preliminary tests in training rule bases . we also show that truth maintenance can be used to make training more efficient and ask some epistemological questions raised by training rule weights ."}
{"title": "causal inference in observational data", "abstract": "our aging population increasingly suffers from multiple chronic diseases simultaneously , necessitating the comprehensive treatment of these conditions . finding the optimal set of drugs for a combinatorial set of diseases is a combinatorial pattern exploration problem . association rule mining is a popular tool for such problems , but the requirement of health care for finding causal , rather than associative , patterns renders association rule mining unsuitable . to address this issue , we propose a novel framework based on the rubin-neyman causal model for extracting causal rules from observational data , correcting for a number of common biases . specifically , given a set of interventions and a set of items that define subpopulations ( e.g. , diseases ) , we wish to find all subpopulations in which effective intervention combinations exist and in each such subpopulation , we wish to find all intervention combinations such that dropping any intervention from this combination will reduce the efficacy of the treatment . a key aspect of our framework is the concept of closed intervention sets which extend the concept of quantifying the effect of a single intervention to a set of concurrent interventions . we also evaluated our causal rule mining framework on the electronic health records ( ehr ) data of a large cohort of patients from mayo clinic and showed that the patterns we extracted are sufficiently rich to explain the controversial findings in the medical literature regarding the effect of a class of cholesterol drugs on type-ii diabetes mellitus ( t2dm ) ."}
{"title": "knowledge reduction of dynamic covering decision information systems with immigration of more objects", "abstract": "in practical situations , it is of interest to investigate computing approximations of sets as an important step of knowledge reduction of dynamic covering decision information systems . in this paper , we present incremental approaches to computing the type-1 and type-2 characteristic matrices of dynamic coverings whose cardinalities increase with immigration of more objects . we also present the incremental algorithms of computing the second and sixth lower and upper approximations of sets in dynamic covering approximation spaces ."}
{"title": "on the existence and convergence computable universal priors", "abstract": "solomonoff unified occam 's razor and epicurus ' principle of multiple explanations to one elegant , formal , universal theory of inductive inference , which initiated the field of algorithmic information theory . his central result is that the posterior of his universal semimeasure m converges rapidly to the true sequence generating posterior mu , if the latter is computable . hence , m is eligible as a universal predictor in case of unknown mu . we investigate the existence and convergence of computable universal ( semi ) measures for a hierarchy of computability classes : finitely computable , estimable , enumerable , and approximable . for instance , m is known to be enumerable , but not finitely computable , and to dominate all enumerable semimeasures . we define seven classes of ( semi ) measures based on these four computability concepts . each class may or may not contain a ( semi ) measure which dominates all elements of another class . the analysis of these 49 cases can be reduced to four basic cases , two of them being new . the results hold for discrete and continuous semimeasures . we also investigate more closely the types of convergence , possibly implied by universality : in difference and in ratio , with probability 1 , in mean sum , and for martin-loef random sequences . we introduce a generalized concept of randomness for individual sequences and use it to exhibit difficulties regarding these issues ."}
{"title": "using modified partitioning around medoids clustering technique in mobile network planning", "abstract": "every cellular network deployment requires planning and optimization in order to provide adequate coverage , capacity , and quality of service ( qos ) . optimization mobile radio network planning is a very complex task , as many aspects must be taken into account . with the rapid development in mobile network we need effective network planning tool to satisfy the need of customers . however , deciding upon the optimum placement for the base stations ( bs s ) to achieve best services while reducing the cost is a complex task requiring vast computational resource . this paper introduces the spatial clustering to solve the mobile networking planning problem . it addresses antenna placement problem or the cell planning problem , involves locating and configuring infrastructure for mobile networks by modified the original partitioning around medoids pam algorithm . m-pam ( modified partitioning around medoids ) has been proposed to satisfy the requirements and constraints . pam needs to specify number of clusters ( k ) before starting to search for the best locations of base stations . the m-pam algorithm uses the radio network planning to determine k. we calculate for each cluster its coverage and capacity and determine if they satisfy the mobile requirements , if not we will increase ( k ) and reapply algorithms depending on two methods for clustering . implementation of this algorithm to a real case study is presented . experimental results and analysis indicate that the m-pam algorithm when applying method two is effective in case of heavy load distribution , and leads to minimum number of base stations , which directly affected onto the cost of planning the network ."}
{"title": "fmap : distributed cooperative multi-agent planning", "abstract": "this paper proposes fmap ( forward multi-agent planning ) , a fully-distributed multi-agent planning method that integrates planning and coordination . although fmap is specifically aimed at solving problems that require cooperation among agents , the flexibility of the domain-independent planning model allows fmap to tackle multi-agent planning tasks of any type . in fmap , agents jointly explore the plan space by building up refinement plans through a complete and flexible forward-chaining partial-order planner . the search is guided by $ h_ { dtg } $ , a novel heuristic function that is based on the concepts of domain transition graph and frontier state and is optimized to evaluate plans in distributed environments . agents in fmap apply an advanced privacy model that allows them to adequately keep private information while communicating only the data of the refinement plans that is relevant to each of the participating agents . experimental results show that fmap is a general-purpose approach that efficiently solves tightly-coupled domains that have specialized agents and cooperative goals as well as loosely-coupled problems . specifically , the empirical evaluation shows that fmap outperforms current map systems at solving complex planning tasks that are adapted from the international planning competition benchmarks ."}
{"title": "what counterfactuals can be tested", "abstract": "counterfactual statements , e.g. , `` my headache would be gone had i taken an aspirin '' are central to scientific discourse , and are formally interpreted as statements derived from `` alternative worlds '' . however , since they invoke hypothetical states of affairs , often incompatible with what is actually known or observed , testing counterfactuals is fraught with conceptual and practical difficulties . in this paper , we provide a complete characterization of `` testable counterfactuals , '' namely , counterfactual statements whose probabilities can be inferred from physical experiments . we provide complete procedures for discerning whether a given counterfactual is testable and , if so , expressing its probability in terms of experimental data ."}
{"title": "random drift particle swarm optimization", "abstract": "the random drift particle swarm optimization ( rdpso ) algorithm , inspired by the free electron model in metal conductors placed in an external electric field , is presented , systematically analyzed and empirically studied in this paper . the free electron model considers that electrons have both a thermal and a drift motion in a conductor that is placed in an external electric field . the motivation of the rdpso algorithm is described first , and the velocity equation of the particle is designed by simulating the thermal motion as well as the drift motion of the electrons , both of which lead the electrons to a location with minimum potential energy in the external electric field . then , a comprehensive analysis of the algorithm is made , in order to provide a deep insight into how the rdpso algorithm works . it involves a theoretical analysis and the simulation of the stochastic dynamical behavior of a single particle in the rdpso algorithm . the search behavior of the algorithm itself is also investigated in detail , by analyzing the interaction between the particles . some variants of the rdpso algorithm are proposed by incorporating different random velocity components with different neighborhood topologies . finally , empirical studies on the rdpso algorithm are performed by using a set of benchmark functions from the cec2005 benchmark suite . based on the theoretical analysis of the particle 's behavior , two methods of controlling the algorithmic parameters are employed , followed by an experimental analysis on how to select the parameter values , in order to obtain a good overall performance of the rdpso algorithm and its variants in real-world applications . a further performance comparison between the rdpso algorithms and other variants of pso is made to prove the efficiency of the rdpso algorithms ."}
{"title": "a model for safety case confidence assessment", "abstract": "building a safety case is a common approach to make expert judgement explicit about safety of a system . the issue of confidence in such argumentation is still an open research field . providing quantitative estimation of confidence is an interesting approach to manage complexity of arguments . this paper explores the main current approaches , and proposes a new model for quantitative confidence estimation based on belief theory for its definition , and on bayesian belief networks for its propagation in safety case networks ."}
{"title": "users constraints in itemset mining", "abstract": "discovering significant itemsets is one of the fundamental problems in data mining . it has recently been shown that constraint programming is a flexible way to tackle data mining tasks . with a constraint programming approach , we can easily express and efficiently answer queries with users constraints on items . however , in many practical cases it is possible that queries also express users constraints on the dataset itself . for instance , asking for a particular itemset in a particular part of the dataset . this paper presents a general constraint programming model able to handle any kind of query on the items or the dataset for itemset mining ."}
{"title": "optimal rectangle packing : an absolute placement approach", "abstract": "we consider the problem of finding all enclosing rectangles of minimum area that can contain a given set of rectangles without overlap . our rectangle packer chooses the x-coordinates of all the rectangles before any of the y-coordinates . we then transform the problem into a perfect-packing problem with no empty space by adding additional rectangles . to determine the y-coordinates , we branch on the different rectangles that can be placed in each empty position . our packer allows us to extend the known solutions for a consecutive-square benchmark from 27 to 32 squares . we also introduce three new benchmarks , avoiding properties that make a benchmark easy , such as rectangles with shared dimensions . our third benchmark consists of rectangles of increasingly high precision . to pack them efficiently , we limit the rectangles coordinates and the bounding box dimensions to the set of subset sums of the rectangles dimensions . overall , our algorithms represent the current state-of-the-art for this problem , outperforming other algorithms by orders of magnitude , depending on the benchmark ."}
{"title": "expert gate : lifelong learning with a network of experts", "abstract": "in this paper we introduce a model of lifelong learning , based on a network of experts . new tasks / experts are learned and added to the model sequentially , building on what was learned before . to ensure scalability of this process , data from previous tasks can not be stored and hence is not available when learning a new task . a critical issue in such context , not addressed in the literature so far , relates to the decision which expert to deploy at test time . we introduce a set of gating autoencoders that learn a representation for the task at hand , and , at test time , automatically forward the test sample to the relevant expert . this also brings memory efficiency as only one expert network has to be loaded into memory at any given time . further , the autoencoders inherently capture the relatedness of one task to another , based on which the most relevant prior model to be used for training a new expert , with finetuning or learning without-forgetting , can be selected . we evaluate our method on image classification and video prediction problems ."}
{"title": "a fixed-parameter algorithm for random instances of weighted d-cnf satisfiability", "abstract": "we study random instances of the weighted $ d $ -cnf satisfiability problem ( weighted $ d $ -sat ) , a generic w [ 1 ] -complete problem . a random instance of the problem consists of a fixed parameter $ k $ and a random $ d $ -cnf formula $ \\weicnf { n } { p } { k , d } $ generated as follows : for each subset of $ d $ variables and with probability $ p $ , a clause over the $ d $ variables is selected uniformly at random from among the $ 2^d - 1 $ clauses that contain at least one negated literals . we show that random instances of weighted $ d $ -sat can be solved in $ o ( k^2n + n^ { o ( 1 ) } ) $ -time with high probability , indicating that typical instances of weighted $ d $ -sat under this instance distribution are fixed-parameter tractable . the result also hold for random instances from the model $ \\weicnf { n } { p } { k , d } ( d ' ) $ where clauses containing less than $ d ' ( 1 < d ' < d ) $ negated literals are forbidden , and for random instances of the renormalized ( miniaturized ) version of weighted $ d $ -sat in certain range of the random model 's parameter $ p ( n ) $ . this , together with our previous results on the threshold behavior and the resolution complexity of unsatisfiable instances of $ \\weicnf { n } { p } { k , d } $ , provides an almost complete characterization of the typical-case behavior of random instances of weighted $ d $ -sat ."}
{"title": "resource discovery in trilogy", "abstract": "trilogy is a collaborative project whose key aim is the development of an integrated virtual laboratory to support research training within each institution and collaborative projects between the partners . in this paper , the architecture and underpinning platform of the system is described with particular emphasis being placed on the structure and the integration of the distributed database . a key element is the ontology that provides the multi-agent system with a conceptualisation specification of the domain ; this ontology is explained , accompanied by a discussion how such a system is integrated and used within the virtual laboratory . although in this paper , telecommunications and in particular broadband networks are used as exemplars , the underlying system principles are applicable to any domain where a combination of experimental and literature-based resources are required ."}
{"title": "transforming business rules into natural language text", "abstract": "the aim of the project presented in this paper is to design a system for an nlg architecture , which supports the documentation process of ebusiness models . a major task is to enrich the formal description of an ebusiness model with additional information needed in an nlg task ."}
{"title": "incorporating expressive graphical models in variational approximations : chain-graphs and hidden variables", "abstract": "global variational approximation methods in graphical models allow efficient approximate inference of complex posterior distributions by using a simpler model . the choice of the approximating model determines a tradeoff between the complexity of the approximation procedure and the quality of the approximation . in this paper , we consider variational approximations based on two classes of models that are richer than standard bayesian networks , markov networks or mixture models . as such , these classes allow to find better tradeoffs in the spectrum of approximations . the first class of models are chain graphs , which capture distributions that are partially directed . the second class of models are directed graphs ( bayesian networks ) with additional latent variables . both classes allow representation of multi-variable dependencies that can not be easily represented within a bayesian network ."}
{"title": "a new approach for scalable analysis of microbial communities", "abstract": "microbial communities play important roles in the function and maintenance of various biosystems , ranging from human body to the environment . current methods for analysis of microbial communities are typically based on taxonomic phylogenetic alignment using 16s rrna metagenomic or whole genome sequencing data . in typical characterizations of microbial communities , studies deal with billions of micobial sequences , aligning them to a phylogenetic tree . we introduce a new approach for the efficient analysis of microbial communities . our new reference-free analysis tech- nique is based on n-gram sequence analysis of 16s rrna data and reduces the processing data size dramatically ( by 105 fold ) , without requiring taxonomic alignment . the proposed approach is applied to characterize phenotypic microbial community differ- ences in different settings . specifically , we applied this approach in classification of microbial com- munities across different body sites , characterization of oral microbiomes associated with healthy and diseased individuals , and classification of microbial communities longitudinally during the develop- ment of infants . different dimensionality reduction methods are introduced that offer a more scalable analysis framework , while minimizing the loss in classification accuracies . among dimensionality re- duction techniques , we propose a continuous vector representation for microbial communities , which can widely be used for deep learning applications in microbial informatics ."}
{"title": "discrete optimization of statistical sample sizes in simulation by using the hierarchical bootstrap method", "abstract": "the bootstrap method application in simulation supposes that value of random variables are not generated during the simulation process but extracted from available sample populations . in the case of hierarchical bootstrap the function of interest is calculated recurrently using the calculation tree . in the present paper we consider the optimization of sample sizes in each vertex of the calculation tree . the dynamic programming method is used for this aim . proposed method allows to decrease a variance of system characteristic estimators ."}
{"title": "evidence combination for a large number of sources", "abstract": "the theory of belief functions is an effective tool to deal with the multiple uncertain information . in recent years , many evidence combination rules have been proposed in this framework , such as the conjunctive rule , the cautious rule , the pcr ( proportional conflict redistribution ) rules and so on . these rules can be adopted for different types of sources . however , most of these rules are not applicable when the number of sources is large . this is due to either the complexity or the existence of an absorbing element ( such as the total conflict mass function for the conjunctive-based rules when applied on unreliable evidence ) . in this paper , based on the assumption that the majority of sources are reliable , a combination rule for a large number of sources , named lns ( stands for large number of sources ) , is proposed on the basis of a simple idea : the more common ideas one source shares with others , the morereliable the source is . this rule is adaptable for aggregating a large number of sources among which some are unreliable . it will keep the spirit of the conjunctive rule to reinforce the belief on the focal elements with which the sources are in agreement . the mass on the empty set will be kept as an indicator of the conflict . moreover , it can be used to elicit the major opinion among the experts . the experimental results on synthetic mass functionsverify that the rule can be effectively used to combine a large number of mass functions and to elicit the major opinion ."}
{"title": "an efficient protocol for negotiation over combinatorial domains with incomplete information", "abstract": "we study the problem of agent-based negotiation in combinatorial domains . it is difficult to reach optimal agreements in bilateral or multi-lateral negotiations when the agents ' preferences for the possible alternatives are not common knowledge . self-interested agents often end up negotiating inefficient agreements in such situations . in this paper , we present a protocol for negotiation in combinatorial domains which can lead rational agents to reach optimal agreements under incomplete information setting . our proposed protocol enables the negotiating agents to identify efficient solutions using distributed search that visits only a small subspace of the whole outcome space . moreover , the proposed protocol is sufficiently general that it is applicable to most preference representation models in combinatorial domains . we also present results of experiments that demonstrate the feasibility and computational efficiency of our approach ."}
{"title": "solver scheduling via answer set programming", "abstract": "although boolean constraint technology has made tremendous progress over the last decade , the efficacy of state-of-the-art solvers is known to vary considerably across different types of problem instances and is known to depend strongly on algorithm parameters . this problem was addressed by means of a simple , yet effective approach using handmade , uniform and unordered schedules of multiple solvers in ppfolio , which showed very impressive performance in the 2011 sat competition . inspired by this , we take advantage of the modeling and solving capacities of answer set programming ( asp ) to automatically determine more refined , that is , non-uniform and ordered solver schedules from existing benchmarking data . we begin by formulating the determination of such schedules as multi-criteria optimization problems and provide corresponding asp encodings . the resulting encodings are easily customizable for different settings and the computation of optimum schedules can mostly be done in the blink of an eye , even when dealing with large runtime data sets stemming from many solvers on hundreds to thousands of instances . also , the fact that our approach can be customized easily enabled us to swiftly adapt it to generate parallel schedules for multi-processor machines ."}
{"title": "keyphrase extraction using sequential labeling", "abstract": "keyphrases efficiently summarize a document 's content and are used in various document processing and retrieval tasks . several unsupervised techniques and classifiers exist for extracting keyphrases from text documents . most of these methods operate at a phrase-level and rely on part-of-speech ( pos ) filters for candidate phrase generation . in addition , they do not directly handle keyphrases of varying lengths . we overcome these modeling shortcomings by addressing keyphrase extraction as a sequential labeling task in this paper . we explore a basic set of features commonly used in nlp tasks as well as predictions from various unsupervised methods to train our taggers . in addition to a more natural modeling for the keyphrase extraction problem , we show that tagging models yield significant performance benefits over existing state-of-the-art extraction methods ."}
{"title": "accbench : a framework for comparing causality algorithms", "abstract": "modern socio-technical systems are increasingly complex . a fundamental problem is that the borders of such systems are often not well-defined a-priori , which among other problems can lead to unwanted behavior during runtime . ideally , unwanted behavior should be prevented . if this is not possible the system shall at least be able to help determine potential cause ( s ) a-posterori , identify responsible parties and make them accountable for their behavior . recently , several algorithms addressing these concepts have been proposed . however , the applicability of the corresponding approaches , specifically their effectiveness and performance , is mostly unknown . therefore , in this paper , we propose accbench , a benchmark tool that allows to compare and evaluate causality algorithms under a consistent setting . furthermore , we contribute an implementation of the two causality algorithms by g\\ '' o { \\ss } ler and metayer and g\\ '' o { \\ss } ler and astefanoaei as well as of a policy compliance approach based on some concepts of main et al . lastly , we conduct a case study of an intelligent door control system , which exposes concrete strengths and weaknesses of all algorithms under different aspects . in the course of this , we show that the effectiveness of the algorithms in terms of cause detection as well as their performance differ to some extent . in addition , our analysis reports on some qualitative aspects that should be considered when evaluating each algorithm . for example , the human effort needed to configure the algorithm and model the use case is analyzed ."}
{"title": "probabilistic zero-shot classification with semantic rankings", "abstract": "in this paper we propose a non-metric ranking-based representation of semantic similarity that allows natural aggregation of semantic information from multiple heterogeneous sources . we apply the ranking-based representation to zero-shot learning problems , and present deterministic and probabilistic zero-shot classifiers which can be built from pre-trained classifiers without retraining . we demonstrate their the advantages on two large real-world image datasets . in particular , we show that aggregating different sources of semantic information , including crowd-sourcing , leads to more accurate classification ."}
{"title": "modification of the elite ant system in order to avoid local optimum points in the traveling salesman problem", "abstract": "this article presents a new algorithm which is a modified version of the elite ant system ( eas ) algorithm . the new version utilizes an effective criterion for escaping from the local optimum points . in contrast to the classical eac algorithms , the proposed algorithm uses only a global updating , which will increase pheromone on the edges of the best ( i.e . the shortest ) route and will at the same time decrease the amount of pheromone on the edges of the worst ( i.e . the longest ) route . in order to assess the efficiency of the new algorithm , some standard traveling salesman problems ( tsps ) were studied and their results were compared with classical eac and other well-known meta-heuristic algorithms . the results indicate that the proposed algorithm has been able to improve the efficiency of the algorithms in all instances and it is competitive with other algorithms ."}
{"title": "observation subset selection as local compilation of performance profiles", "abstract": "deciding what to sense is a crucial task , made harder by dependencies and by a nonadditive utility function . we develop approximation algorithms for selecting an optimal set of measurements , under a dependency structure modeled by a tree-shaped bayesian network ( bn ) . our approach is a generalization of composing anytime algorithm represented by conditional performance profiles . this is done by relaxing the input monotonicity assumption , and extending the local compilation technique to more general classes of performance profiles ( pps ) . we apply the extended scheme to selecting a subset of measurements for choosing a maximum expectation variable in a binary valued bn , and for minimizing the worst variance in a gaussian bn ."}
{"title": "pseudorehearsal in value function approximation", "abstract": "catastrophic forgetting is of special importance in reinforcement learning , as the data distribution is generally non-stationary over time . we study and compare several pseudorehearsal approaches for q-learning with function approximation in a pole balancing task . we have found that pseudorehearsal seems to assist learning even in such very simple problems , given proper initialization of the rehearsal parameters ."}
{"title": "towards better response times and higher-quality queries in interactive knowledge base debugging", "abstract": "many ai applications rely on knowledge encoded in a locigal knowledge base ( kb ) . the most essential benefit of such logical kbs is the opportunity to perform automatic reasoning which however requires a kb to meet some minimal quality criteria such as consistency . without adequate tool assistance , the task of resolving such violated quality criteria in a kb can be extremely hard , especially when the problematic kb is large and complex . to this end , interactive kb debuggers have been introduced which ask a user queries whether certain statements must or must not hold in the intended domain . the given answers help to gradually restrict the search space for kb repairs . existing interactive debuggers often rely on a pool-based strategy for query computation . a pool of query candidates is precomputed , from which the best candidate according to some query quality criterion is selected to be shown to the user . this often leads to the generation of many unnecessary query candidates and thus to a high number of expensive calls to logical reasoning services . we tackle this issue by an in-depth mathematical analysis of diverse real-valued active learning query selection measures in order to determine qualitative criteria that make a query favorable . these criteria are the key to devising efficient heuristic query search methods . the proposed methods enable for the first time a completely reasoner-free query generation for interactive kb debugging while at the same time guaranteeing optimality conditions , e.g . minimal cardinality or best understandability for the user , of the generated query that existing methods can not realize . further , we study different relations between active learning measures . the obtained picture gives a hint about which measures are more favorable in which situation or which measures always lead to the same outcomes , based on given types of queries ."}
{"title": "refining adverse drug reaction signals by incorporating interaction variables identified using emergent pattern mining", "abstract": "purpose : to develop a framework for identifying and incorporating candidate confounding interaction terms into a regularised cox regression analysis to refine adverse drug reaction signals obtained via longitudinal observational data . methods : we considered six drug families that are commonly associated with myocardial infarction in observational healthcare data , but where the causal relationship ground truth is known ( adverse drug reaction or not ) . we applied emergent pattern mining to find itemsets of drugs and medical events that are associated with the development of myocardial infarction . these are the candidate confounding interaction terms . we then implemented a cohort study design using regularised cox regression that incorporated and accounted for the candidate confounding interaction terms . results the methodology was able to account for signals generated due to confounding and a cox regression with elastic net regularisation correctly ranked the drug families known to be true adverse drug reactions above those ."}
{"title": "25 tweets to know you : a new model to predict personality with social media", "abstract": "predicting personality is essential for social applications supporting human-centered activities , yet prior modeling methods with users written text require too much input data to be realistically used in the context of social media . in this work , we aim to drastically reduce the data requirement for personality modeling and develop a model that is applicable to most users on twitter . our model integrates word embedding features with gaussian processes regression . based on the evaluation of over 1.3k users on twitter , we find that our model achieves comparable or better accuracy than state of the art techniques with 8 times fewer data ."}
{"title": "tighter linear program relaxations for high order graphical models", "abstract": "graphical models with high order potentials ( hops ) have received considerable interest in recent years . while there are a variety of approaches to inference in these models , nearly all of them amount to solving a linear program ( lp ) relaxation with unary consistency constraints between the hop and the individual variables . in many cases , the resulting relaxations are loose , and in these cases the results of inference can be poor . it is thus desirable to look for more accurate ways of performing inference in these models . in this work , we study the lp relaxations that result from enforcing additional consistency constraints between the hop and the rest of the model . we address theoretical questions about the strength of the resulting relaxations compared to the relaxations that arise in standard approaches , and we develop practical and efficient message passing algorithms for optimizing the lps . empirically , we show that the lps with additional consistency constraints lead to more accurate inference on some challenging problems that include a combination of low order and high order terms ."}
{"title": "loop calculus and bootstrap-belief propagation for perfect matchings on arbitrary graphs", "abstract": "this manuscript discusses computation of the partition function ( pf ) and the minimum weight perfect matching ( mwpm ) on arbitrary , non-bipartite graphs . we present two novel problem formulations - one for computing the pf of a perfect matching ( pm ) and one for finding mwpms - that build upon the inter-related bethe free energy , belief propagation ( bp ) , loop calculus ( lc ) , integer linear programming ( ilp ) and linear programming ( lp ) frameworks . first , we describe an extension of the lc framework to the pm problem . the resulting formulas , coined ( fractional ) bootstrap-bp , express the pf of the original model via the bfe of an alternative pm problem . we then study the zero-temperature version of this bootstrap-bp formula for approximately solving the mwpm problem . we do so by leveraging the bootstrap-bp formula to construct a sequence of mwpm problems , where each new problem in the sequence is formed by contracting odd-sized cycles ( or blossoms ) from the previous problem . this bootstrap-and-contract procedure converges reliably and generates an empirically tight upper bound for the mwpm . we conclude by discussing the relationship between our iterative procedure and the famous blossom algorithm of edmonds '65 and demonstrate the performance of the bootstrap-and-contract approach on a variety of weighted pm problems ."}
{"title": "interactive knowledge base population", "abstract": "most work on building knowledge bases has focused on collecting entities and facts from as large a collection of documents as possible . we argue for and describe a new paradigm where the focus is on a high-recall extraction over a small collection of documents under the supervision of a human expert , that we call interactive knowledge base population ( ikbp ) ."}
{"title": "online continuous submodular maximization", "abstract": "in this paper , we consider an online optimization process , where the objective functions are not convex ( nor concave ) but instead belong to a broad class of continuous submodular functions . we first propose a variant of the frank-wolfe algorithm that has access to the full gradient of the objective functions . we show that it achieves a regret bound of $ o ( \\sqrt { t } ) $ ( where $ t $ is the horizon of the online optimization problem ) against a $ ( 1-1/e ) $ -approximation to the best feasible solution in hindsight . however , in many scenarios , only an unbiased estimate of the gradients are available . for such settings , we then propose an online stochastic gradient ascent algorithm that also achieves a regret bound of $ o ( \\sqrt { t } ) $ regret , albeit against a weaker $ 1/2 $ -approximation to the best feasible solution in hindsight . we also generalize our results to $ \\gamma $ -weakly submodular functions and prove the same sublinear regret bounds . finally , we demonstrate the efficiency of our algorithms on a few problem instances , including non-convex/non-concave quadratic programs , multilinear extensions of submodular set functions , and d-optimal design ."}
{"title": "parseval networks : improving robustness to adversarial examples", "abstract": "we introduce parseval networks , a form of deep neural networks in which the lipschitz constant of linear , convolutional and aggregation layers is constrained to be smaller than 1. parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation . the most important feature of parseval networks is to maintain weight matrices of linear and convolutional layers to be ( approximately ) parseval tight frames , which are extensions of orthogonal matrices to non-square matrices . we describe how these constraints can be maintained efficiently during sgd . we show that parseval networks match the state-of-the-art in terms of accuracy on cifar-10/100 and street view house numbers ( svhn ) while being more robust than their vanilla counterpart against adversarial examples . incidentally , parseval networks also tend to train faster and make a better usage of the full capacity of the networks ."}
{"title": "exploring word embeddings for unsupervised textual user-generated content normalization", "abstract": "text normalization techniques based on rules , lexicons or supervised training requiring large corpora are not scalable nor domain interchangeable , and this makes them unsuitable for normalizing user-generated content ( ugc ) . current tools available for brazilian portuguese make use of such techniques . in this work we propose a technique based on distributed representation of words ( or word embeddings ) . it generates continuous numeric vectors of high-dimensionality to represent words . the vectors explicitly encode many linguistic regularities and patterns , as well as syntactic and semantic word relationships . words that share semantic similarity are represented by similar vectors . based on these features , we present a totally unsupervised , expandable and language and domain independent method for learning normalization lexicons from word embeddings . our approach obtains high correction rate of orthographic errors and internet slang in product reviews , outperforming the current available tools for brazilian portuguese ."}
{"title": "guided deep reinforcement learning for swarm systems", "abstract": "in this paper , we investigate how to learn to control a group of cooperative agents with limited sensing capabilities such as robot swarms . the agents have only very basic sensor capabilities , yet in a group they can accomplish sophisticated tasks , such as distributed assembly or search and rescue tasks . learning a policy for a group of agents is difficult due to distributed partial observability of the state . here , we follow a guided approach where a critic has central access to the global state during learning , which simplifies the policy evaluation problem from a reinforcement learning point of view . for example , we can get the positions of all robots of the swarm using a camera image of a scene . this camera image is only available to the critic and not to the control policies of the robots . we follow an actor-critic approach , where the actors base their decisions only on locally sensed information . in contrast , the critic is learned based on the true global state . our algorithm uses deep reinforcement learning to approximate both the q-function and the policy . the performance of the algorithm is evaluated on two tasks with simple simulated 2d agents : 1 ) finding and maintaining a certain distance to each others and 2 ) locating a target ."}
{"title": "information directed sampling for stochastic bandits with graph feedback", "abstract": "we consider stochastic multi-armed bandit problems with graph feedback , where the decision maker is allowed to observe the neighboring actions of the chosen action . we allow the graph structure to vary with time and consider both deterministic and erd\\h { o } s-r\\'enyi random graph models . for such a graph feedback model , we first present a novel analysis of thompson sampling that leads to tighter performance bound than existing work . next , we propose new information directed sampling based policies that are graph-aware in their decision making . under the deterministic graph case , we establish a bayesian regret bound for the proposed policies that scales with the clique cover number of the graph instead of the number of actions . under the random graph case , we provide a bayesian regret bound for the proposed policies that scales with the ratio of the number of actions over the expected number of observations per iteration . to the best of our knowledge , this is the first analytical result for stochastic bandits with random graph feedback . finally , using numerical evaluations , we demonstrate that our proposed ids policies outperform existing approaches , including adaptions of upper confidence bound , $ \\epsilon $ -greedy and exp3 algorithms ."}
{"title": "on the optimality of tree-reweighted max-product message-passing", "abstract": "tree-reweighted max-product ( trw ) message passing is a modified form of the ordinary max-product algorithm for attempting to find minimal energy configurations in markov random field with cycles . for a trw fixed point satisfying the strong tree agreement condition , the algorithm outputs a configuration that is provably optimal . in this paper , we focus on the case of binary variables with pairwise couplings , and establish stronger properties of trw fixed points that satisfy only the milder condition of weak tree agreement ( wta ) . first , we demonstrate how it is possible to identify part of the optimal solution|i.e. , a provably optimal solution for a subset of nodes| without knowing a complete solution . second , we show that for submodular functions , a wta fixed point always yields a globally optimal solution . we establish that for binary variables , any wta fixed point always achieves the global maximum of the linear programming relaxation underlying the trw method ."}
{"title": "learning ( predictive ) risk scores in the presence of censoring due to interventions", "abstract": "a large and diverse set of measurements are regularly collected during a patient 's hospital stay to monitor their health status . tools for integrating these measurements into severity scores , that accurately track changes in illness severity , can improve clinicians ability to provide timely interventions . existing approaches for creating such scores either 1 ) rely on experts to fully specify the severity score , or 2 ) train a predictive score , using supervised learning , by regressing against a surrogate marker of severity such as the presence of downstream adverse events . the first approach does not extend to diseases where an accurate score can not be elicited from experts . the second approach often produces scores that suffer from bias due to treatment-related censoring ( paxton , 2013 ) . we propose a novel ranking based framework for disease severity score learning ( dssl ) . dssl exploits the following key observation : while it is challenging for experts to quantify the disease severity at any given time , it is often easy to compare the disease severity at two different times . extending existing ranking algorithms , dssl learns a function that maps a vector of patient 's measurements to a scalar severity score such that the resulting score is temporally smooth and consistent with the expert 's ranking of pairs of disease states . we apply dssl to the problem of learning a sepsis severity score using a large , real-world dataset . the learned scores significantly outperform state-of-the-art clinical scores in ranking patient states by severity and in early detection of future adverse events . we also show that the learned disease severity trajectories are consistent with clinical expectations of disease evolution . further , using simulated datasets , we show that dssl exhibits better generalization performance to changes in treatment patterns compared to the above approaches ."}
{"title": "top-n recommendation with novel rank approximation", "abstract": "the importance of accurate recommender systems has been widely recognized by academia and industry . however , the recommendation quality is still rather low . recently , a linear sparse and low-rank representation of the user-item matrix has been applied to produce top-n recommendations . this approach uses the nuclear norm as a convex relaxation for the rank function and has achieved better recommendation accuracy than the state-of-the-art methods . in the past several years , solving rank minimization problems by leveraging nonconvex relaxations has received increasing attention . some empirical results demonstrate that it can provide a better approximation to original problems than convex relaxation . in this paper , we propose a novel rank approximation to enhance the performance of top-n recommendation systems , where the approximation error is controllable . experimental results on real data show that the proposed rank approximation improves the top- $ n $ recommendation accuracy substantially ."}
{"title": "a complete classification of tractability in rcc-5", "abstract": "we investigate the computational properties of the spatial algebra rcc-5 which is a restricted version of the rcc framework for spatial reasoning . the satisfiability problem for rcc-5 is known to be np-complete but not much is known about its approximately four billion subclasses . we provide a complete classification of satisfiability for all these subclasses into polynomial and np-complete respectively . in the process , we identify all maximal tractable subalgebras which are four in total ."}
{"title": "automatable evaluation method oriented toward behaviour believability for video games", "abstract": "classic evaluation methods of believable agents are time-consuming because they involve many human to judge agents . they are well suited to validate work on new believable behaviours models . however , during the implementation , numerous experiments can help to improve agents ' believability . we propose a method which aim at assessing how much an agent 's behaviour looks like humans ' behaviours . by representing behaviours with vectors , we can store data computed for humans and then evaluate as many agents as needed without further need of humans . we present a test experiment which shows that even a simple evaluation following our method can reveal differences between quite believable agents and humans . this method seems promising although , as shown in our experiment , results ' analysis can be difficult ."}
{"title": "mtd ( f ) , a minimax algorithm faster than negascout", "abstract": "mtd ( f ) is a new minimax search algorithm , simpler and more efficient than previous algorithms . in tests with a number of tournament game playing programs for chess , checkers and othello it performed better , on average , than negascout/pvs ( the alphabeta variant used in practically all good chess , checkers , and othello programs ) . one of the strongest chess programs of the moment , mit 's parallel chess program cilkchess uses mtd ( f ) as its search algorithm , replacing negascout , which was used in starsocrates , the previous version of the program ."}
{"title": "inverse graphics with probabilistic cad models", "abstract": "recently , multiple formulations of vision problems as probabilistic inversions of generative models based on computer graphics have been proposed . however , applications to 3d perception from natural images have focused on low-dimensional latent scenes , due to challenges in both modeling and inference . accounting for the enormous variability in 3d object shape and 2d appearance via realistic generative models seems intractable , as does inverting even simple versions of the many-to-many computations that link 3d scenes to 2d images . this paper proposes and evaluates an approach that addresses key aspects of both these challenges . we show that it is possible to solve challenging , real-world 3d vision problems by approximate inference in generative models for images based on rendering the outputs of probabilistic cad ( pcad ) programs . our pcad object geometry priors generate deformable 3d meshes corresponding to plausible objects and apply affine transformations to place them in a scene . image likelihoods are based on similarity in a feature space based on standard mid-level image representations from the vision literature . our inference algorithm integrates single-site and locally blocked metropolis-hastings proposals , hamiltonian monte carlo and discriminative data-driven proposals learned from training data generated from our models . we apply this approach to 3d human pose estimation and object shape reconstruction from single images , achieving quantitative and qualitative performance improvements over state-of-the-art baselines ."}
{"title": "cognitive architecture for direction of attention founded on subliminal memory searches , pseudorandom and nonstop", "abstract": "by way of explaining how a brain works logically , human associative memory is modeled with logical and memory neurons , corresponding to standard digital circuits . the resulting cognitive architecture incorporates basic psychological elements such as short term and long term memory . novel to the architecture are memory searches using cues chosen pseudorandomly from short term memory . recalls alternated with sensory images , many tens per second , are analyzed subliminally as an ongoing process , to determine a direction of attention in short term memory ."}
{"title": "machine learner for automated reasoning 0.4 and 0.5", "abstract": "machine learner for automated reasoning ( malarea ) is a learning and reasoning system for proving in large formal libraries where thousands of theorems are available when attacking a new conjecture , and a large number of related problems and proofs can be used to learn specific theorem-proving knowledge . the last version of the system has by a large margin won the 2013 casc ltb competition . this paper describes the motivation behind the methods used in malarea , discusses the general approach and the issues arising in evaluation of such system , and describes the mizar @ turing100 and casc'24 versions of malarea ."}
{"title": "about norms and causes", "abstract": "knowing the norms of a domain is crucial , but there exist no repository of norms . we propose a method to extract them from texts : texts generally do not describe a norm , but rather how a state-of-affairs differs from it . answers concerning the cause of the state-of-affairs described often reveal the implicit norm . we apply this idea to the domain of driving , and validate it by designing algorithms that identify , in a text , the `` basic '' norms to which it refers implicitly ."}
{"title": "learning continuous user representations through hybrid filtering with doc2vec", "abstract": "players in the online ad ecosystem are struggling to acquire the user data required for precise targeting . audience look-alike modeling has the potential to alleviate this issue , but models ' performance strongly depends on quantity and quality of available data . in order to maximize the predictive performance of our look-alike modeling algorithms , we propose two novel hybrid filtering techniques that utilize the recent neural probabilistic language model algorithm doc2vec . we apply these methods to data from a large mobile ad exchange and additional app metadata acquired from the apple app store and google play store . first , we model mobile app users through their app usage histories and app descriptions ( user2vec ) . second , we introduce context awareness to that model by incorporating additional user and app-related metadata in model training ( context2vec ) . our findings are threefold : ( 1 ) the quality of recommendations provided by user2vec is notably higher than current state-of-the-art techniques . ( 2 ) user representations generated through hybrid filtering using doc2vec prove to be highly valuable features in supervised machine learning models for look-alike modeling . this represents the first application of hybrid filtering user models using neural probabilistic language models , specifically doc2vec , in look-alike modeling . ( 3 ) incorporating context metadata in the doc2vec model training process to introduce context awareness has positive effects on performance and is superior to directly including the data as features in the downstream supervised models ."}
{"title": "a possibilistic handling of partially ordered information", "abstract": "in a standard possibilistic logic , prioritized information are encoded by means of weighted knowledge base . this paper proposes an extension of possibilistic logic for dealing with partially ordered information . we show that all basic notions of standard possibilitic logic ( sumbsumption , syntactic and semantic inference , etc . ) have natural couterparts when dealing with partially ordered information . we also propose an algorithm which computes possibilistic conclusions of a partial knowledge base of a partially ordered knowlege base ."}
{"title": "minos : multimodal indoor simulator for navigation in complex environments", "abstract": "we present minos , a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments . the simulator leverages large datasets of complex 3d environments and supports flexible configuration of multimodal sensor suites . we use minos to benchmark deep-learning-based navigation methods , to analyze the influence of environmental complexity on navigation performance , and to carry out a controlled study of multimodality in sensorimotor learning . the experiments show that current deep reinforcement learning approaches fail in large realistic environments . the experiments also indicate that multimodality is beneficial in learning to navigate cluttered scenes . minos is released open-source to the research community at http : //minosworld.org . a video that shows minos can be found at https : //youtu.be/c0ml9k64q84"}
{"title": "probabilistic warnings in national security crises : pearl harbor revisited", "abstract": "imagine a situation where a group of adversaries is preparing an attack on the united states or u.s. interests . an intelligence analyst has observed some signals , but the situation is rapidly changing . the analyst faces the decision to alert a principal decision maker that an attack is imminent , or to wait until more is known about the situation . this warning decision is based on the analyst 's observation and evaluation of signals , independent or correlated , and on her updating of the prior probabilities of possible scenarios and their outcomes . the warning decision also depends on the analyst 's assessment of the crisis ' dynamics and perception of the preferences of the principal decision maker , as well as the lead time needed for an appropriate response . this article presents a model to support this analyst 's dynamic warning decision . as with most problems involving warning , the key is to manage the tradeoffs between false positives and false negatives given the probabilities and the consequences of intelligence failures of both types . the model is illustrated by revisiting the case of the attack on pearl harbor in december 1941. it shows that the radio silence of the japanese fleet carried considerable information ( sir arthur conan doyle 's `` dog in the night '' problem ) , which was misinterpreted at the time . even though the probabilities of different attacks were relatively low , their consequences were such that the bayesian dynamic reasoning described here may have provided valuable information to key decision makers ."}
{"title": "batch reinforcement learning on the industrial benchmark : first experiences", "abstract": "the particle swarm optimization policy ( pso-p ) has been recently introduced and proven to produce remarkable results on interacting with academic reinforcement learning benchmarks in an off-policy , batch-based setting . to further investigate the properties and feasibility on real-world applications , this paper investigates pso-p on the so-called industrial benchmark ( ib ) , a novel reinforcement learning ( rl ) benchmark that aims at being realistic by including a variety of aspects found in industrial applications , like continuous state and action spaces , a high dimensional , partially observable state space , delayed effects , and complex stochasticity . the experimental results of pso-p on ib are compared to results of closed-form control policies derived from the model-based recurrent control neural network ( rcnn ) and the model-free neural fitted q-iteration ( nfq ) . experiments show that pso-p is not only of interest for academic benchmarks , but also for real-world industrial applications , since it also yielded the best performing policy in our ib setting . compared to other well established rl techniques , pso-p produced outstanding results in performance and robustness , requiring only a relatively low amount of effort in finding adequate parameters or making complex design decisions ."}
{"title": "efficient methods for qualitative spatial reasoning", "abstract": "the theoretical properties of qualitative spatial reasoning in the rcc8 framework have been analyzed extensively . however , no empirical investigation has been made yet . our experiments show that the adaption of the algorithms used for qualitative temporal reasoning can solve large rcc8 instances , even if they are in the phase transition region -- provided that one uses the maximal tractable subsets of rcc8 that have been identified by us . in particular , we demonstrate that the orthogonal combination of heuristic methods is successful in solving almost all apparently hard instances in the phase transition region up to a certain size in reasonable time ."}
{"title": "truth validation with evidence", "abstract": "in the modern era , abundant information is easily accessible from various sources , however only a few of these sources are reliable as they mostly contain unverified contents . we develop a system to validate the truthfulness of a given statement together with underlying evidence . the proposed system provides supporting evidence when the statement is tagged as false . our work relies on an inference method on a knowledge graph ( kg ) to identify the truthfulness of statements . in order to extract the evidence of falseness , the proposed algorithm takes into account combined knowledge from kg and ontologies . the system shows very good results as it provides valid and concise evidence . the quality of kg plays a role in the performance of the inference method which explicitly affects the performance of our evidence-extracting algorithm ."}
{"title": "fuzzy model on human emotions recognition", "abstract": "this paper discusses a fuzzy model for multi-level human emotions recognition by computer systems through keyboard keystrokes , mouse and touchscreen interactions . this model can also be used to detect the other possible emotions at the time of recognition . accuracy measurements of human emotions by the fuzzy model are discussed through two methods ; the first is accuracy analysis and the second is false positive rate analysis . this fuzzy model detects more emotions , but on the other hand , for some of emotions , a lower accuracy was obtained with the comparison with the non-fuzzy human emotions detection methods . this system was trained and tested by support vector machine ( svm ) to recognize the users ' emotions . overall , this model represents a closer similarity between human brain detection of emotions and computer systems ."}
{"title": "path consistency learning in tsallis entropy regularized mdps", "abstract": "we study the sparse entropy-regularized reinforcement learning ( erl ) problem in which the entropy term is a special form of the tsallis entropy . the optimal policy of this formulation is sparse , i.e. , ~at each state , it has non-zero probability for only a small number of actions . this addresses the main drawback of the standard shannon entropy-regularized rl ( soft erl ) formulation , in which the optimal policy is softmax , and thus , may assign a non-negligible probability mass to non-optimal actions . this problem is aggravated as the number of actions is increased . in this paper , we follow the work of nachum et al . ( 2017 ) in the soft erl setting , and propose a class of novel path consistency learning ( pcl ) algorithms , called { \\em sparse pcl } , for the sparse erl problem that can work with both on-policy and off-policy data . we first derive a { \\em sparse consistency } equation that specifies a relationship between the optimal value function and policy of the sparse erl along any system trajectory . crucially , a weak form of the converse is also true , and we quantify the sub-optimality of a policy which satisfies sparse consistency , and show that as we increase the number of actions , this sub-optimality is better than that of the soft erl optimal policy . we then use this result to derive the sparse pcl algorithms . we empirically compare sparse pcl with its soft counterpart , and show its advantage , especially in problems with a large number of actions ."}
{"title": "molecular de novo design through deep reinforcement learning", "abstract": "this work introduces a method to tune a sequence-based generative model for molecular de novo design that through augmented episodic likelihood can learn to generate structures with certain specified desirable properties . we demonstrate how this model can execute a range of tasks such as generating analogues to a query structure and generating compounds predicted to be active against a biological target . as a proof of principle , the model is first trained to generate molecules that do not contain sulphur . as a second example , the model is trained to generate analogues to the drug celecoxib , a technique that could be used for scaffold hopping or library expansion starting from a single molecule . finally , when tuning the model towards generating compounds predicted to be active against the dopamine receptor type 2 , the model generates structures of which more than 95 % are predicted to be active , including experimentally confirmed actives that have not been included in either the generative model nor the activity prediction model ."}
{"title": "pulcinella : a general tool for propagating uncertainty in valuation networks", "abstract": "we present pulcinella and its use in comparing uncertainty theories . pulcinella is a general tool for propagating uncertainty based on the local computation technique of shafer and shenoy . it may be specialized to different uncertainty theories : at the moment , pulcinella can propagate probabilities , belief functions , boolean values , and possibilities . moreover , pulcinella allows the user to easily define his own specializations . to illustrate pulcinella , we analyze two examples by using each of the four theories above . in the first one , we mainly focus on intrinsic differences between theories . in the second one , we take a knowledge engineer viewpoint , and check the adequacy of each theory to a given problem ."}
{"title": "multiobjective tactical planning under uncertainty for air traffic flow and capacity management", "abstract": "we investigate a method to deal with congestion of sectors and delays in the tactical phase of air traffic flow and capacity management . it relies on temporal objectives given for every point of the flight plans and shared among the controllers in order to create a collaborative environment . this would enhance the transition from the network view of the flow management to the local view of air traffic control . uncertainty is modeled at the trajectory level with temporal information on the boundary points of the crossed sectors and then , we infer the probabilistic occupancy count . therefore , we can model the accuracy of the trajectory prediction in the optimization process in order to fix some safety margins . on the one hand , more accurate is our prediction ; more efficient will be the proposed solutions , because of the tighter safety margins . on the other hand , when uncertainty is not negligible , the proposed solutions will be more robust to disruptions . furthermore , a multiobjective algorithm is used to find the tradeoff between the delays and congestion , which are antagonist in airspace with high traffic density . the flow management position can choose manually , or automatically with a preference-based algorithm , the adequate solution . this method is tested against two instances , one with 10 flights and 5 sectors and one with 300 flights and 16 sectors ."}
{"title": "a counterexample to the forward recursion in fuzzy critical path analysis under discrete fuzzy sets", "abstract": "fuzzy logic is an alternate approach for quantifying uncertainty relating to activity duration . the fuzzy version of the backward recursion has been shown to produce results that incorrectly amplify the level of uncertainty . however , the fuzzy version of the forward recursion has been widely proposed as an approach for determining the fuzzy set of critical path lengths . in this paper , the direct application of the extension principle leads to a proposition that must be satisfied in fuzzy critical path analysis . using a counterexample it is demonstrated that the fuzzy forward recursion when discrete fuzzy sets are used to represent activity durations produces results that are not consistent with the theory presented . the problem is shown to be the application of the fuzzy maximum . several methods presented in the literature are described and shown to provide results that are consistent with the extension principle ."}
{"title": "achieving privacy in the adversarial multi-armed bandit", "abstract": "in this paper , we improve the previously best known regret bound to achieve $ \\epsilon $ -differential privacy in oblivious adversarial bandits from $ \\mathcal { o } { ( t^ { 2/3 } /\\epsilon ) } $ to $ \\mathcal { o } { ( \\sqrt { t } \\ln t /\\epsilon ) } $ . this is achieved by combining a laplace mechanism with exp3 . we show that though exp3 is already differentially private , it leaks a linear amount of information in $ t $ . however , we can improve this privacy by relying on its intrinsic exponential mechanism for selecting actions . this allows us to reach $ \\mathcal { o } { ( \\sqrt { \\ln t } ) } $ -dp , with a regret of $ \\mathcal { o } { ( t^ { 2/3 } ) } $ that holds against an adaptive adversary , an improvement from the best known of $ \\mathcal { o } { ( t^ { 3/4 } ) } $ . this is done by using an algorithm that run exp3 in a mini-batch loop . finally , we run experiments that clearly demonstrate the validity of our theoretical analysis ."}
{"title": "a data-parallel version of aleph", "abstract": "this is to present work on modifying the aleph ilp system so that it evaluates the hypothesised clauses in parallel by distributing the data-set among the nodes of a parallel or distributed machine . the paper briefly discusses mpi , the interface used to access message- passing libraries for parallel computers and clusters . it then proceeds to describe an extension of yap prolog with an mpi interface and an implementation of data-parallel clause evaluation for aleph through this interface . the paper concludes by testing the data-parallel aleph on artificially constructed data-sets ."}
{"title": "updating probabilities", "abstract": "as examples such as the monty hall puzzle show , applying conditioning to update a probability distribution on a `` naive space '' , which does not take into account the protocol used , can often lead to counterintuitive results . here we examine why . a criterion known as car ( `` coarsening at random '' ) in the statistical literature characterizes when `` naive '' conditioning in a naive space works . we show that the car condition holds rather infrequently , and we provide a procedural characterization of it , by giving a randomized algorithm that generates all and only distributions for which car holds . this substantially extends previous characterizations of car . we also consider more generalized notions of update such as jeffrey conditioning and minimizing relative entropy ( mre ) . we give a generalization of the car condition that characterizes when jeffrey conditioning leads to appropriate answers , and show that there exist some very simple settings in which mre essentially never gives the right results . this generalizes and interconnects previous results obtained in the literature on car and mre ."}
{"title": "hybrid fuzzy-linear programming approach for multi criteria decision making problems", "abstract": "the purpose of this paper is to point to the usefulness of applying a linear mathematical formulation of fuzzy multiple criteria objective decision methods in organising business activities . in this respect fuzzy parameters of linear programming are modelled by preference-based membership functions . this paper begins with an introduction and some related research followed by some fundamentals of fuzzy set theory and technical concepts of fuzzy multiple objective decision models . further a real case study of a manufacturing plant and the implementation of the proposed technique is presented . empirical results clearly show the superiority of the fuzzy technique in optimising individual objective functions when compared to non-fuzzy approach . furthermore , for the problem considered , the optimal solution helps to infer that by incorporating fuzziness in a linear programming model either in constraints , or both in objective functions and constraints , provides a similar ( or even better ) level of satisfaction for obtained results compared to non-fuzzy linear programming ."}
{"title": "an algorithmic framework to control bias in bandit-based personalization", "abstract": "personalization is pervasive in the online space as it leads to higher efficiency and revenue by allowing the most relevant content to be served to each user . however , recent studies suggest that personalization methods can propagate societal or systemic biases and polarize opinions ; this has led to calls for regulatory mechanisms and algorithms to combat bias and inequality . algorithmically , bandit optimization has enjoyed great success in learning user preferences and personalizing content or feeds accordingly . we propose an algorithmic framework that allows for the possibility to control bias or discrimination in such bandit-based personalization . our model allows for the specification of general fairness constraints on the sensitive types of the content that can be displayed to a user . the challenge , however , is to come up with a scalable and low regret algorithm for the constrained optimization problem that arises . our main technical contribution is a provably fast and low-regret algorithm for the fairness-constrained bandit optimization problem . our proofs crucially leverage the special structure of our problem . experiments on synthetic and real-world data sets show that our algorithmic framework can control bias with only a minor loss to revenue ."}
{"title": "distance function of d numbers", "abstract": "dempster-shafer theory is widely applied in uncertainty modelling and knowledge reasoning due to its ability of expressing uncertain information . a distance between two basic probability assignments ( bpas ) presents a measure of performance for identification algorithms based on the evidential theory of dempster-shafer . however , some conditions lead to limitations in practical application for dempster-shafer theory , such as exclusiveness hypothesis and completeness constraint . to overcome these shortcomings , a novel theory called d numbers theory is proposed . a distance function of d numbers is proposed to measure the distance between two d numbers . the distance function of d numbers is an generalization of distance between two bpas , which inherits the advantage of dempster-shafer theory and strengthens the capability of uncertainty modeling . an illustrative case is provided to demonstrate the effectiveness of the proposed function ."}
{"title": "improving problem solving by exploiting the concept of symmetry", "abstract": "we investigate the concept of symmetry and its role in problem solving . this paper first defines precisely the elements that constitute a `` problem '' and its `` solution , '' and gives several examples to illustrate these definitions . given precise definitions of problems , it is relatively straightforward to construct a search process for finding solutions . finally this paper attempts to exploit the concept of symmetry in improving problem solving ."}
{"title": "a novice looks at emotional cognition", "abstract": "modeling emotional-cognition is in a nascent stage and therefore wide-open for new ideas and discussions . in this paper the author looks at the modeling problem by bringing in ideas from axiomatic mathematics , information theory , computer science , molecular biology , non-linear dynamical systems and quantum computing and explains how ideas from these disciplines may have applications in modeling emotional-cognition ."}
{"title": "examplers based image fusion features for face recognition", "abstract": "examplers of a face are formed from multiple gallery images of a person and are used in the process of classification of a test image . we incorporate such examplers in forming a biologically inspired local binary decisions on similarity based face recognition method . as opposed to single model approaches such as face averages the exampler based approach results in higher recognition accu- racies and stability . using multiple training samples per person , the method shows the following recognition accuracies : 99.0 % on ar , 99.5 % on feret , 99.5 % on orl , 99.3 % on eyale , 100.0 % on yale and 100.0 % on caltech face databases . in addition to face recognition , the method also detects the natural variability in the face images which can find application in automatic tagging of face images ."}
{"title": "a bio-inspired algorithm for fuzzy user equilibrium problem by aid of physarum polycephalum", "abstract": "the user equilibrium in traffic assignment problem is based on the fact that travelers choose the minimum-cost path between every origin-destination pair and on the assumption that such a behavior will lead to an equilibrium of the traffic network . in this paper , we consider this problem when the traffic network links are fuzzy cost . therefore , a physarum-type algorithm is developed to unify the physarum network and the traffic network for taking full of advantage of physarum polycephalum 's adaptivity in network design to solve the user equilibrium problem . eventually , some experiments are used to test the performance of this method . the results demonstrate that our approach is competitive when compared with other existing algorithms ."}
{"title": "a general approach to belief change in answer set programming", "abstract": "we address the problem of belief change in ( nonmonotonic ) logic programming under answer set semantics . unlike previous approaches to belief change in logic programming , our formal techniques are analogous to those of distance-based belief revision in propositional logic . in developing our results , we build upon the model theory of logic programs furnished by se models . since se models provide a formal , monotonic characterisation of logic programs , we can adapt techniques from the area of belief revision to belief change in logic programs . we introduce methods for revising and merging logic programs , respectively . for the former , we study both subset-based revision as well as cardinality-based revision , and we show that they satisfy the majority of the agm postulates for revision . for merging , we consider operators following arbitration merging and ic merging , respectively . we also present encodings for computing the revision as well as the merging of logic programs within the same logic programming framework , giving rise to a direct implementation of our approach in terms of off-the-shelf answer set solvers . these encodings reflect in turn the fact that our change operators do not increase the complexity of the base formalism ."}
{"title": "fml-based prediction agent and its application to game of go", "abstract": "in this paper , we present a robotic prediction agent including a darkforest go engine , a fuzzy markup language ( fml ) assessment engine , an fml-based decision support engine , and a robot engine for game of go application . the knowledge base and rule base of fml assessment engine are constructed by referring the information from the darkforest go engine located in nutn and opu , for example , the number of mcts simulations and winning rate prediction . the proposed robotic prediction agent first retrieves the database of go competition website , and then the fml assessment engine infers the winning possibility based on the information generated by darkforest go engine . the fml-based decision support engine computes the winning possibility based on the partial game situation inferred by fml assessment engine . finally , the robot engine combines with the human-friendly robot partner palro , produced by fujisoft incorporated , to report the game situation to human go players . experimental results show that the fml-based prediction agent can work effectively ."}
{"title": "deep learning can reverse photon migration for diffuse optical tomography", "abstract": "can artificial intelligence ( ai ) learn complicated non-linear physics ? here we propose a novel deep learning approach that learns non-linear photon scattering physics and obtains accurate 3d distribution of optical anomalies . in contrast to the traditional black-box deep learning approaches to inverse problems , our deep network learns to invert the lippmann-schwinger integral equation which describes the essential physics of photon migration of diffuse near-infrared ( nir ) photons in turbid media . as an example for clinical relevance , we applied the method to our prototype diffuse optical tomography ( dot ) . we show that our deep neural network , trained with only simulation data , can accurately recover the location of anomalies within biomimetic phantoms and live animals without the use of an exogenous contrast agent ."}
{"title": "deepheart : semi-supervised sequence learning for cardiovascular risk prediction", "abstract": "we train and validate a semi-supervised , multi-task lstm on 57,675 person-weeks of data from off-the-shelf wearable heart rate sensors , showing high accuracy at detecting multiple medical conditions , including diabetes ( 0.8451 ) , high cholesterol ( 0.7441 ) , high blood pressure ( 0.8086 ) , and sleep apnea ( 0.8298 ) . we compare two semi-supervised train- ing methods , semi-supervised sequence learning and heuristic pretraining , and show they outperform hand-engineered biomarkers from the medical literature . we believe our work suggests a new approach to patient risk stratification based on cardiovascular risk scores derived from popular wearables such as fitbit , apple watch , or android wear ."}
{"title": "incremental maintenance of association rules under support threshold change", "abstract": "maintenance of association rules is an interesting problem . several incremental maintenance algorithms were proposed since the work of ( cheung et al , 1996 ) . the majority of these algorithms maintain rule bases assuming that support threshold does n't change . in this paper , we present incremental maintenance algorithm under support threshold change . this solution allows user to maintain its rule base under any support threshold ."}
{"title": "enhancenet : single image super-resolution through automated texture synthesis", "abstract": "single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input . traditionally , the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio ( psnr ) which have been shown to correlate poorly with the human perception of image quality . as a result , algorithms minimizing these metrics tend to produce over-smoothed images that lack high-frequency textures and do not look natural despite yielding high psnr values . we propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixel-accurate reproduction of ground truth images during training . by using feed-forward fully convolutional neural networks in an adversarial training setting , we achieve a significant boost in image quality at high magnification ratios . extensive experiments on a number of datasets show the effectiveness of our approach , yielding state-of-the-art results in both quantitative and qualitative benchmarks ."}
{"title": "circumspect descent prevails in solving random constraint satisfaction problems", "abstract": "we study the performance of stochastic local search algorithms for random instances of the $ k $ -satisfiability ( $ k $ -sat ) problem . we introduce a new stochastic local search algorithm , chainsat , which moves in the energy landscape of a problem instance by { \\em never going upwards } in energy . chainsat is a \\emph { focused } algorithm in the sense that it considers only variables occurring in unsatisfied clauses . we show by extensive numerical investigations that chainsat and other focused algorithms solve large $ k $ -sat instances almost surely in linear time , up to high clause-to-variable ratios $ \\alpha $ ; for example , for k=4 we observe linear-time performance well beyond the recently postulated clustering and condensation transitions in the solution space . the performance of chainsat is a surprise given that by design the algorithm gets trapped into the first local energy minimum it encounters , yet no such minima are encountered . we also study the geometry of the solution space as accessed by stochastic local search algorithms ."}
{"title": "interdefinability of defeasible logic and logic programming under the well-founded semantics", "abstract": "we provide a method of translating theories of nute 's defeasible logic into logic programs , and a corresponding translation in the opposite direction . under certain natural restrictions , the conclusions of defeasible theories under the ambiguity propagating defeasible logic adl correspond to those of the well-founded semantics for normal logic programs , and so it turns out that the two formalisms are closely related . using the same translation of logic programs into defeasible theories , the semantics for the ambiguity blocking defeasible logic ndl can be seen as indirectly providing an ambiguity blocking semantics for logic programs . we also provide antimonotone operators for both adl and ndl , each based on the gelfond-lifschitz ( gl ) operator for logic programs . for defeasible theories without defeaters or priorities on rules , the operator for adl corresponds to the gl operator and so can be seen as partially capturing the consequences according to adl . similarly , the operator for ndl captures the consequences according to ndl , though in this case no restrictions on theories apply . both operators can be used to define stable model semantics for defeasible theories ."}
{"title": "order-consistent programs are cautiously monotonic", "abstract": "some normal logic programs under the answer set ( stable model ) semantics lack the appealing property of `` cautious monotonicity . '' that is , augmenting a program with one of its consequences may cause it to lose another of its consequences . the syntactic condition of `` order-consistency '' was shown by fages to guarantee existence of an answer set . this note establishes that order-consistent programs are not only consistent , but cautiously monotonic . from this it follows that they are also `` cumulative . '' that is , augmenting an order-consistent with some of its consequences does not alter its consequences . in fact , as we show , its answer sets remain unchanged ."}
{"title": "probabilistic models for query approximation with large sparse binary datasets", "abstract": "large sparse sets of binary transaction data with millions of records and thousands of attributes occur in various domains : customers purchasing products , users visiting web pages , and documents containing words are just three typical examples . real-time query selectivity estimation ( the problem of estimating the number of rows in the data satisfying a given predicate ) is an important practical problem for such databases . we investigate the application of probabilistic models to this problem . in particular , we study a markov random field ( mrf ) approach based on frequent sets and maximum entropy , and compare it to the independence model and the chow-liu tree model . we find that the mrf model provides substantially more accurate probability estimates than the other methods but is more expensive from a computational and memory viewpoint . to alleviate the computational requirements we show how one can apply bucket elimination and clique tree approaches to take advantage of structure in the models and in the queries . we provide experimental results on two large real-world transaction datasets ."}
{"title": "arguing for decisions : a qualitative model of decision making", "abstract": "we develop a qualitative model of decision making with two aims : to describe how people make simple decisions and to enable computer programs to do the same . current approaches based on planning or decisions theory either ignore uncertainty and tradeoffs , or provide languages and algorithms that are too complex for this task . the proposed model provides a language based on rules , a semantics based on high probabilities and lexicographical preferences , and a transparent decision procedure where reasons for and against decisions interact . the model is no substitude for decision theory , yet for decisions that people find easy to explain it may provide an appealing alternative ."}
{"title": "a survey of paraphrasing and textual entailment methods", "abstract": "paraphrasing methods recognize , generate , or extract phrases , sentences , or longer natural language expressions that convey almost the same information . textual entailment methods , on the other hand , recognize , generate , or extract pairs of natural language expressions , such that a human who reads ( and trusts ) the first element of a pair would most likely infer that the other element is also true . paraphrasing can be seen as bidirectional textual entailment and methods from the two areas are often similar . both kinds of methods are useful , at least in principle , in a wide range of natural language processing applications , including question answering , summarization , text generation , and machine translation . we summarize key ideas from the two areas by considering in turn recognition , generation , and extraction methods , also pointing to prominent articles and resources ."}
{"title": "on the measure of conflicts : a mus-decomposition based framework", "abstract": "measuring inconsistency is viewed as an important issue related to handling inconsistencies . good measures are supposed to satisfy a set of rational properties . however , defining sound properties is sometimes problematic . in this paper , we emphasize one such property , named decomposability , rarely discussed in the literature due to its modeling difficulties . to this end , we propose an independent decomposition which is more intuitive than existing proposals . to analyze inconsistency in a more fine-grained way , we introduce a graph representation of a knowledge base and various musdecompositions . one particular mus-decomposition , named distributable mus-decomposition leads to an interesting partition of inconsistencies in a knowledge base such that multiple experts can check inconsistencies in parallel , which is impossible under existing measures . such particular musdecomposition results in an inconsistency measure that satisfies a number of desired properties . moreover , we give an upper bound complexity of the measure that can be computed using 0/1 linear programming or min cost satisfiability problems , and conduct preliminary experiments to show its feasibility ."}
{"title": "regret bounds for opportunistic channel access", "abstract": "we consider the task of opportunistic channel access in a primary system composed of independent gilbert-elliot channels where the secondary ( or opportunistic ) user does not dispose of a priori information regarding the statistical characteristics of the system . it is shown that this problem may be cast into the framework of model-based learning in a specific class of partially observed markov decision processes ( pomdps ) for which we introduce an algorithm aimed at striking an optimal tradeoff between the exploration ( or estimation ) and exploitation requirements . we provide finite horizon regret bounds for this algorithm as well as a numerical evaluation of its performance in the single channel model as well as in the case of stochastically identical channels ."}
{"title": "solving relational mdps with exogenous events and additive rewards", "abstract": "we formalize a simple but natural subclass of service domains for relational planning problems with object-centered , independent exogenous events and additive rewards capturing , for example , problems in inventory control . focusing on this subclass , we present a new symbolic planning algorithm which is the first algorithm that has explicit performance guarantees for relational mdps with exogenous events . in particular , under some technical conditions , our planning algorithm provides a monotonic lower bound on the optimal value function . to support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams , a knowledge representation for real-valued functions over relational world states . our planning algorithm uses a set of focus states , which serves as a training set , to simplify and approximate the symbolic solution , and can thus be seen to perform learning for planning . a preliminary experimental evaluation demonstrates the validity of our approach ."}
{"title": "humanoid robots as agents of human consciousness expansion", "abstract": "the `` loving ai '' project involves developing software enabling humanoid robots to interact with people in loving and compassionate ways , and to promote people ' self-understanding and self-transcendence . currently the project centers on the hanson robotics robot `` sophia '' -- specifically , on supplying sophia with personality content and cognitive , linguistic , perceptual and behavioral content aimed at enabling loving interactions supportive of human self-transcendence . in september 2017 a small pilot study was conducted , involving the sophia robot leading human subjects through dialogues and exercises focused on meditation , visualization and relaxation . the pilot was an apparent success , qualitatively demonstrating the viability of the approach and the ability of appropriate human-robot interaction to increase human well-being and advance human consciousness ."}
{"title": "a gradient descent technique coupled with a dynamic simulation to determine the near optimum orientation of floor plan designs", "abstract": "a prototype tool to assist architects during the early design stage of floor plans has been developed , consisting of an evolutionary program for the space allocation problem ( epsap ) , which generates sets of floor plan alternatives according to the architect 's preferences ; and a floor plan performance optimization program ( fpop ) , which optimizes the selected solutions according to thermal performance criteria . the design variables subject to optimization are window position and size , overhangs , fins , wall positioning , and building orientation . a procedure using a transformation operator with gradient descent , such as behavior , coupled with a dynamic simulation engine was developed for the thermal evaluation and optimization process . however , the need to evaluate all possible alternatives regarding designing variables being used during the optimization process leads to an intensive use of thermal simulation , which dramatically increases the simulation time , rendering it unpractical . an alternative approach is a smart optimization approach , which utilizes an oriented and adaptive search technique to efficiently find the near optimum solution . this paper presents the search methodology for the building orientation of floor plan designs , and the corresponding efficiency and effectiveness indicators . the calculations are based on 100 floor plan designs generated by epsap . all floor plans have the same design program , location , and weather data , changing only their geometry . dynamic simulation of buildings was effectively used together with the optimization procedure in this approach to significantly improve the designs . the use of the orientation variable has been included in the algorithm ."}
{"title": "on the complexity of semantic integration of owl ontologies", "abstract": "we propose a new mechanism for integration of owl ontologies using semantic import relations . in contrast to the standard owl importing , we do not require all axioms of the imported ontologies to be taken into account for reasoning tasks , but only their logical implications over a chosen signature . this property comes natural in many ontology integration scenarios , especially when the number of ontologies is large . in this paper , we study the complexity of reasoning over ontologies with semantic import relations and establish a range of tight complexity bounds for various fragments of owl ."}
{"title": "improving naive bayes for regression with optimised artificial surrogate data", "abstract": "can we evolve better training data for machine learning algorithms ? to investigate this question we use population-based optimisation algorithms to generate artificial surrogate training data for naive bayes for regression . we demonstrate that the generalisation performance of naive bayes for regression models is enhanced by training them on the artificial data as opposed to the real data . these results are important for two reasons . firstly , naive bayes models are simple and interpretable but frequently underperform compared to more complex `` black box '' models , and therefore new methods of enhancing accuracy are called for . secondly , the idea of using the real training data indirectly in the construction of the artificial training data , as opposed to directly for model training , is a novel twist on the usual machine learning paradigm ."}
{"title": "modeling of social transitions using intelligent systems", "abstract": "in this study , we reproduce two new hybrid intelligent systems , involve three prominent intelligent computing and approximate reasoning methods : self organizing feature map ( som ) , neruo-fuzzy inference system and rough set theory ( rst ) , called sonfis and sorst . we show how our algorithms can be construed as a linkage of government-society interactions , where government catches various states of behaviors : solid ( absolute ) or flexible . so , transition of society , by changing of connectivity parameters ( noise ) from order to disorder is inferred ."}
{"title": "the search for computational intelligence", "abstract": "we define and explore in simulation several rules for the local evolution of generative rules for 1d and 2d cellular automata . our implementation uses strategies from conceptual blending . we discuss potential applications to modelling social dynamics ."}
{"title": "a survey of available corpora for building data-driven dialogue systems", "abstract": "during the past decade , several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models . in the area of dialogue systems , the trend is less obvious , and most practical systems are still built through significant engineering and expert knowledge . nevertheless , several recent results suggest that data-driven approaches are feasible and quite promising . to facilitate research in this area , we have carried out a wide survey of publicly available datasets suitable for data-driven learning of dialogue systems . we discuss important characteristics of these datasets , how they can be used to learn diverse dialogue strategies , and their other potential uses . we also examine methods for transfer learning between datasets and the use of external knowledge . finally , we discuss appropriate choice of evaluation metrics for the learning objective ."}
{"title": "robust estimation via robust gradient estimation", "abstract": "we provide a new computationally-efficient class of estimators for risk minimization . we show that these estimators are robust for general statistical models : in the classical huber epsilon-contamination model and in heavy-tailed settings . our workhorse is a novel robust variant of gradient descent , and we provide conditions under which our gradient descent variant provides accurate estimators in a general convex risk minimization problem . we provide specific consequences of our theory for linear regression , logistic regression and for estimation of the canonical parameters in an exponential family . these results provide some of the first computationally tractable and provably robust estimators for these canonical statistical models . finally , we study the empirical performance of our proposed methods on synthetic and real datasets , and find that our methods convincingly outperform a variety of baselines ."}
{"title": "saten : an object-oriented web-based revision and extraction engine", "abstract": "saten is an object-oriented web-based extraction and belief revision engine . it runs on any computer via a java 1.1 enabled browser such as netscape 4. saten performs belief revision based on the agm approach . the extraction and belief revision reasoning engines operate on a user specified ranking of information . one of the features of saten is that it can be used to integrate mutually inconsistent commensuate rankings into a consistent ranking ."}
{"title": "multi-modal human-machine communication for instructing robot grasping tasks", "abstract": "a major challenge for the realization of intelligent robots is to supply them with cognitive abilities in order to allow ordinary users to program them easily and intuitively . one way of such programming is teaching work tasks by interactive demonstration . to make this effective and convenient for the user , the machine must be capable to establish a common focus of attention and be able to use and integrate spoken instructions , visual perceptions , and non-verbal clues like gestural commands . we report progress in building a hybrid architecture that combines statistical methods , neural networks , and finite state machines into an integrated system for instructing grasping tasks by man-machine interaction . the system combines the gravis-robot for visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation , and an modality fusion module to allow multi-modal task-oriented man-machine communication with respect to dextrous robot manipulation of objects ."}
{"title": "a generalization of convolutional neural networks to graph-structured data", "abstract": "this paper introduces a generalization of convolutional neural networks ( cnns ) from low-dimensional grid data , such as images , to graph-structured data . we propose a novel spatial convolution utilizing a random walk to uncover the relations within the input , analogous to the way the standard convolution uses the spatial neighborhood of a pixel on the grid . the convolution has an intuitive interpretation , is efficient and scalable and can also be used on data with varying graph structure . furthermore , this generalization can be applied to many standard regression or classification problems , by learning the the underlying graph . we empirically demonstrate the performance of the proposed cnn on mnist , and challenge the state-of-the-art on merck molecular activity data set ."}
{"title": "dynamic jointrees", "abstract": "it is well known that one can ignore parts of a belief network when computing answers to certain probabilistic queries . it is also well known that the ignorable parts ( if any ) depend on the specific query of interest and , therefore , may change as the query changes . algorithms based on jointrees , however , do not seem to take computational advantage of these facts given that they typically construct jointrees for worst-case queries ; that is , queries for which every part of the belief network is considered relevant . to address this limitation , we propose in this paper a method for reconfiguring jointrees dynamically as the query changes . the reconfiguration process aims at maintaining a jointree which corresponds to the underlying belief network after it has been pruned given the current query . our reconfiguration method is marked by three characteristics : ( a ) it is based on a non-classical definition of jointrees ; ( b ) it is relatively efficient ; and ( c ) it can reuse some of the computations performed before a jointree is reconfigured . we present preliminary experimental results which demonstrate significant savings over using static jointrees when query changes are considerable ."}
{"title": "bit-pragmatic deep neural network computing", "abstract": "we quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in deep neural networks ( dnns ) and propose pragmatic ( pra ) , an architecture that exploits it improving performance and energy efficiency . the source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms , that is , products of the multiplicand and powers of two , which added together produce the final product [ 1 ] . at runtime , many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator . while conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency , pra calculates only the non-zero terms using a ) on-the-fly conversion of the multiplicator representation into an explicit list of powers of two , and b ) hybrid bit-parallel multplicand/bit-serial multiplicator processing units . pra exploits two sources of ineffectual computations : 1 ) the aforementioned zero product terms which are the result of the lack of explicitness in the multiplicator representation , and 2 ) the excess in the representation precision used for both multiplicants and multiplicators , e.g. , [ 2 ] . measurements demonstrate that for the convolutional layers , a straightforward variant of pra improves performance by 2.6x over the dadianao ( dadn ) accelerator [ 3 ] and by 1.4x over str [ 4 ] . similarly , pra improves energy efficiency by 28 % and 10 % on average compared to dadn and str . an improved cross lane synchronication scheme boosts performance improvements to 3.1x over dadn . finally , pragmatic benefits persist even with an 8-bit quantized representation [ 5 ] ."}
{"title": "human perception of performance", "abstract": "humans are routinely asked to evaluate the performance of other individuals , separating success from failure and affecting outcomes from science to education and sports . yet , in many contexts , the metrics driving the human evaluation process remain unclear . here we analyse a massive dataset capturing players ' evaluations by human judges to explore human perception of performance in soccer , the world 's most popular sport . we use machine learning to design an artificial judge which accurately reproduces human evaluation , allowing us to demonstrate how human observers are biased towards diverse contextual features . by investigating the structure of the artificial judge , we uncover the aspects of the players ' behavior which attract the attention of human judges , demonstrating that human evaluation is based on a noticeability heuristic where only feature values far from the norm are considered to rate an individual 's performance ."}
{"title": "online optimization methods for the quantification problem", "abstract": "the estimation of class prevalence , i.e. , the fraction of a population that belongs to a certain class , is a very useful tool in data analytics and learning , and finds applications in many domains such as sentiment analysis , epidemiology , etc . for example , in sentiment analysis , the objective is often not to estimate whether a specific text conveys a positive or a negative sentiment , but rather estimate the overall distribution of positive and negative sentiments during an event window . a popular way of performing the above task , often dubbed quantification , is to use supervised learning to train a prevalence estimator from labeled data . contemporary literature cites several performance measures used to measure the success of such prevalence estimators . in this paper we propose the first online stochastic algorithms for directly optimizing these quantification-specific performance measures . we also provide algorithms that optimize hybrid performance measures that seek to balance quantification and classification performance . our algorithms present a significant advancement in the theory of multivariate optimization and we show , by a rigorous theoretical analysis , that they exhibit optimal convergence . we also report extensive experiments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization techniques used for these performance measures ."}
{"title": "predicting the energy output of wind farms based on weather data : important variables and their correlation", "abstract": "wind energy plays an increasing role in the supply of energy world-wide . the energy output of a wind farm is highly dependent on the weather condition present at the wind farm . if the output can be predicted more accurately , energy suppliers can coordinate the collaborative production of different energy sources more efficiently to avoid costly overproductions . with this paper , we take a computer science perspective on energy prediction based on weather data and analyze the important parameters as well as their correlation on the energy output . to deal with the interaction of the different parameters we use symbolic regression based on the genetic programming tool datamodeler . our studies are carried out on publicly available weather and energy data for a wind farm in australia . we reveal the correlation of the different variables for the energy output . the model obtained for energy prediction gives a very reliable prediction of the energy output for newly given weather data ."}
{"title": "reasoning support for risk prediction and prevention in independent living", "abstract": "in recent years there has been growing interest in solutions for the delivery of clinical care for the elderly , due to the large increase in aging population . monitoring a patient in his home environment is necessary to ensure continuity of care in home settings , but , to be useful , this activity must not be too invasive for patients and a burden for caregivers . we prototyped a system called sindi ( secure and independent living ) , focused on i ) collecting a limited amount of data about the person and the environment through wireless sensor networks ( wsn ) , and ii ) inferring from these data enough information to support caregivers in understanding patients ' well being and in predicting possible evolutions of their health . our hierarchical logic-based model of health combines data from different sources , sensor data , tests results , common-sense knowledge and patient 's clinical profile at the lower level , and correlation rules between health conditions across upper levels . the logical formalization and the reasoning process are based on answer set programming . the expressive power of this logic programming paradigm makes it possible to reason about health evolution even when the available information is incomplete and potentially incoherent , while declarativity simplifies rules specification by caregivers and allows automatic encoding of knowledge . this paper describes how these issues have been targeted in the application scenario of the sindi system ."}
{"title": "supervised quantum learning without measurements", "abstract": "we propose a quantum machine learning algorithm for efficiently solving a class of problems encoded in quantum controlled unitary operations . the central physical mechanism of the protocol is the iteration of a quantum time-delayed equation that introduces feedback in the dynamics and eliminates the necessity of intermediate measurements . the performance of the quantum algorithm is analyzed by comparing the results obtained in numerical simulations with the outcome of classical machine learning methods for the same problem . the use of time-delayed equations enhances the toolbox of the field of quantum machine learning , which may enable unprecedented applications in quantum technologies ."}
{"title": "dempster-shafer for anomaly detection", "abstract": "in this paper , we implement an anomaly detection system using the dempster-shafer method . using two standard benchmark problems we show that by combining multiple signals it is possible to achieve better results than by using a single signal . we further show that by applying this approach to a real-world email dataset the algorithm works for email worm detection . dempster-shafer can be a promising method for anomaly detection problems with multiple features ( data sources ) , and two or more classes ."}
{"title": "fast generation of best interval patterns for nonmonotonic constraints", "abstract": "in pattern mining , the main challenge is the exponential explosion of the set of patterns . typically , to solve this problem , a constraint for pattern selection is introduced . one of the first constraints proposed in pattern mining is support ( frequency ) of a pattern in a dataset . frequency is an anti-monotonic function , i.e. , given an infrequent pattern , all its superpatterns are not frequent . however , many other constraints for pattern selection are not ( anti- ) monotonic , which makes it difficult to generate patterns satisfying these constraints . in this paper we introduce the notion of projection-antimonotonicity and $ \\theta $ - $ \\sigma\\o\\phi\\iota\\alpha $ algorithm that allows efficient generation of the best patterns for some nonmonotonic constraints . in this paper we consider stability and $ \\delta $ -measure , which are nonmonotonic constraints , and apply them to interval tuple datasets . in the experiments , we compute best interval tuple patterns w.r.t . these measures and show the advantage of our approach over postfiltering approaches . keywords : pattern mining , nonmonotonic constraints , interval tuple data"}
{"title": "seer : empowering software defined networking with data analytics", "abstract": "network complexity is increasing , making network control and orchestration a challenging task . the proliferation of network information and tools for data analytics can provide an important insight into resource provisioning and optimisation . the network knowledge incorporated in software defined networking can facilitate the knowledge driven control , leveraging the network programmability . we present seer : a flexible , highly configurable data analytics platform for network intelligence based on software defined networking and big data principles . seer combines a computational engine with a distributed messaging system to provide a scalable , fault tolerant and real-time platform for knowledge extraction . our first prototype uses apache spark for streaming analytics and open network operating system ( onos ) controller to program a network in real-time . the first application we developed aims to predict the mobility pattern of mobile devices inside a smart city environment ."}
{"title": "generating time-based label refinements to discover more precise process models", "abstract": "process mining is a research field focused on the analysis of event data with the aim of extracting insights related to dynamic behavior . applying process mining techniques on data from smart home environments has the potential to provide valuable insights in ( un ) healthy habits and to contribute to ambient assisted living solutions . finding the right event labels to enable the application of process mining techniques is however far from trivial , as simply using the triggering sensor as the label for sensor events results in uninformative models that allow for too much behavior ( overgeneralizing ) . refinements of sensor level event labels suggested by domain experts have been shown to enable discovery of more precise and insightful process models . however , there exists no automated approach to generate refinements of event labels in the context of process mining . in this paper we propose a framework for the automated generation of label refinements based on the time attribute of events , allowing us to distinguish behaviourally different instances of the same event type based on their time attribute . we show on a case study with real life smart home event data that using automatically generated refined labels in process discovery , we can find more specific , and therefore more insightful , process models . we observe that one label refinement could have an effect on the usefulness of other label refinements when used together . therefore , we explore four strategies to generate useful combinations of multiple label refinements and evaluate those on three real life smart home event logs ."}
{"title": "deductive nonmonotonic inference operations : antitonic representations", "abstract": "we provide a characterization of those nonmonotonic inference operations c for which c ( x ) may be described as the set of all logical consequences of x together with some set of additional assumptions s ( x ) that depends anti-monotonically on x ( i.e. , x is a subset of y implies that s ( y ) is a subset of s ( x ) ) . the operations represented are exactly characterized in terms of properties most of which have been studied in freund-lehmann ( cs.ai/0202031 ) . similar characterizations of right-absorbing and cumulative operations are also provided . for cumulative operations , our results fit in closely with those of freund . we then discuss extending finitary operations to infinitary operations in a canonical way and discuss co-compactness properties . our results provide a satisfactory notion of pseudo-compactness , generalizing to deductive nonmonotonic operations the notion of compactness for monotonic operations . they also provide an alternative , more elegant and more general , proof of the existence of an infinitary deductive extension for any finitary deductive operation ( theorem 7.9 of freund-lehmann ) ."}
{"title": "deep-learned collision avoidance policy for distributed multi-agent navigation", "abstract": "high-speed , low-latency obstacle avoidance that is insensitive to sensor noise is essential for enabling multiple decentralized robots to function reliably in cluttered and dynamic environments . while other distributed multi-agent collision avoidance systems exist , these systems require online geometric optimization where tedious parameter tuning and perfect sensing are necessary . we present a novel end-to-end framework to generate reactive collision avoidance policy for efficient distributed multi-agent navigation . our method formulates an agent 's navigation strategy as a deep neural network mapping from the observed noisy sensor measurements to the agent 's steering commands in terms of movement velocity . we train the network on a large number of frames of collision avoidance data collected by repeatedly running a multi-agent simulator with different parameter settings . we validate the learned deep neural network policy in a set of simulated and real scenarios with noisy measurements and demonstrate that our method is able to generate a robust navigation strategy that is insensitive to imperfect sensing and works reliably in all situations . we also show that our method can be well generalized to scenarios that do not appear in our training data , including scenes with static obstacles and agents with different sizes . videos are available at https : //sites.google.com/view/deepmaca ."}
{"title": "gradual classical logic for attributed objects - extended in re-presentation", "abstract": "our understanding about things is conceptual . by stating that we reason about objects , it is in fact not the objects but concepts referring to them that we manipulate . now , so long just as we acknowledge infinitely extending notions such as space , time , size , colour , etc , - in short , any reasonable quality - into which an object is subjected , it becomes infeasible to affirm atomicity in the concept referring to the object . however , formal/symbolic logics typically presume atomic entities upon which other expressions are built . can we reflect our intuition about the concept onto formal/symbolic logics at all ? i assure that we can , but the usual perspective about the atomicity needs inspected . in this work , i present gradual logic which materialises the observation that we can not tell apart whether a so-regarded atomic entity is atomic or is just atomic enough not to be considered non-atomic . the motivation is to capture certain phenomena that naturally occur around concepts with attributes , including presupposition and contraries . i present logical particulars of the logic , which is then mapped onto formal semantics . two linguistically interesting semantics will be considered . decidability is shown ."}
{"title": "optimal detection of faulty traffic sensors used in route planning", "abstract": "in a smart city , real-time traffic sensors may be deployed for various applications , such as route planning . unfortunately , sensors are prone to failures , which result in erroneous traffic data . erroneous data can adversely affect applications such as route planning , and can cause increased travel time . to minimize the impact of sensor failures , we must detect them promptly and accurately . however , typical detection algorithms may lead to a large number of false positives ( i.e. , false alarms ) and false negatives ( i.e. , missed detections ) , which can result in suboptimal route planning . in this paper , we devise an effective detector for identifying faulty traffic sensors using a prediction model based on gaussian processes . further , we present an approach for computing the optimal parameters of the detector which minimize losses due to false-positive and false-negative errors . we also characterize critical sensors , whose failure can have high impact on the route planning application . finally , we implement our method and evaluate it numerically using a real-world dataset and the route planning platform opentripplanner ."}
{"title": "on backdoors to tractable constraint languages", "abstract": "in the context of csps , a strong backdoor is a subset of variables such that every complete assignment yields a residual instance guaranteed to have a specified property . if the property allows efficient solving , then a small strong backdoor provides a reasonable decomposition of the original instance into easy instances . an important challenge is the design of algorithms that can find quickly a small strong backdoor if one exists . we present a systematic study of the parameterized complexity of backdoor detection when the target property is a restricted type of constraint language defined by means of a family of polymorphisms . in particular , we show that under the weak assumption that the polymorphisms are idempotent , the problem is unlikely to be fpt when the parameter is either r ( the constraint arity ) or k ( the size of the backdoor ) unless p = np or fpt = w [ 2 ] . when the parameter is k+r , however , we are able to identify large classes of languages for which the problem of finding a small backdoor is fpt ."}
{"title": "loss functions for multiset prediction", "abstract": "we study the problem of multiset prediction . the goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items . unlike existing problems in supervised learning , such as classification , ranking and sequence generation , there is no known order among items in a target multiset , and each item in the multiset may appear more than once , making this problem extremely challenging . in this paper , we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making . the proposed multiset loss function is empirically evaluated on two families of datasets , one synthetic and the other real , with varying levels of difficulty , against various baseline loss functions including reinforcement learning , sequence , and aggregated distribution matching loss functions . the experiments reveal the effectiveness of the proposed loss function over the others ."}
{"title": "collaborative creativity with monte-carlo tree search and convolutional neural networks", "abstract": "we investigate a human-machine collaborative drawing environment in which an autonomous agent sketches images while optionally allowing a user to directly influence the agent 's trajectory . we combine monte carlo tree search with image classifiers and test both shallow models ( e.g . multinomial logistic regression ) and deep convolutional neural networks ( e.g . lenet , inception v3 ) . we found that using the shallow model , the agent produces a limited variety of images , which are noticably recogonisable by humans . however , using the deeper models , the agent produces a more diverse range of images , and while the agent remains very confident ( 99.99 % ) in having achieved its objective , to humans they mostly resemble unrecognisable 'random ' noise . we relate this to recent research which also discovered that 'deep neural networks are easily fooled ' \\cite { nguyen2015 } and we discuss possible solutions and future directions for the research ."}
{"title": "studies in lower bounding probabilities of evidence using the markov inequality", "abstract": "computing the probability of evidence even with known error bounds is np-hard . in this paper we address this hard problem by settling on an easier problem . we propose an approximation which provides high confidence lower bounds on probability of evidence but does not have any guarantees in terms of relative or absolute error . our proposed approximation is a randomized importance sampling scheme that uses the markov inequality . however , a straight-forward application of the markov inequality may lead to poor lower bounds . we therefore propose several heuristic measures to improve its performance in practice . empirical evaluation of our scheme with state-of- the-art lower bounding schemes reveals the promise of our approach ."}
{"title": "from qualitative to quantitative probabilistic networks", "abstract": "quantification is well known to be a major obstacle in the construction of a probabilistic network , especially when relying on human experts for this purpose . the construction of a qualitative probabilistic network has been proposed as an initial step in a network s quantification , since the qualitative network can be used to gain preliminary insight in the projected networks reasoning behaviour . we extend on this idea and present a new type of network in which both signs and numbers are specified ; we further present an associated algorithm for probabilistic inference . building upon these semi-qualitative networks , a probabilistic network can be quantified and studied in a stepwise manner . as a result , modelling inadequacies can be detected and amended at an early stage in the quantification process ."}
{"title": "adaptive neural compilation", "abstract": "this paper proposes an adaptive neural-compilation framework to address the problem of efficient program learning . traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics . in contrast , our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution . our approach is inspired by the recent works on differentiable representations of programs . we show that it is possible to compile programs written in a low-level language to a differentiable representation . we also show how programs in this representation can be optimised to make them efficient on a target distribution of inputs . experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate ."}
{"title": "the effects of relative importance of user constraints in cloud of things resource discovery : a case study", "abstract": "over the last few years , the number of smart objects connected to the internet has grown exponentially in comparison to the number of services and applications . the integration between cloud computing and internet of things , named as cloud of things , plays a key role in managing the connected things , their data and services . one of the main challenges in cloud of things is the resource discovery of the smart objects and their reuse in different contexts . most of the existent work uses some kind of multi-criteria decision analysis algorithm to perform the resource discovery , but do not evaluate the impact that the user constraints has in the final solution . in this paper , we analyse the behaviour of the saw , topsis and vikor multi-objective decision analyses algorithms and the impact of user constraints on them . we evaluated the quality of the proposed solutions using the pareto-optimality concept ."}
{"title": "proceedings of the eighth conference on uncertainty in artificial intelligence ( 1992 )", "abstract": "this is the proceedings of the eighth conference on uncertainty in artificial intelligence , which was held in stanford , ca , july 17-19 , 1992"}
{"title": "exploiting points and lines in regression forests for rgb-d camera relocalization", "abstract": "camera relocalization plays a vital role in many robotics and computer vision tasks , such as global localization , recovery from tracking failure and loop closure detection . recent random forests based methods exploit randomly sampled pixel comparison features to predict 3d world locations for 2d image locations to guide the camera pose optimization . however , these image features are only sampled randomly in the images , without considering the spatial structures or geometric information , leading to large errors or failure cases with the existence of poorly textured areas or in motion blur . line segment features are more robust in these environments . in this work , we propose to jointly exploit points and lines within the framework of uncertainty driven regression forests . the proposed approach is thoroughly evaluated on three publicly available datasets against several strong state-of-the-art baselines in terms of several different error metrics . experimental results prove the efficacy of our method , showing superior or on-par state-of-the-art performance ."}
{"title": "multiscale markov decision problems : compression , solution , and transfer learning", "abstract": "many problems in sequential decision making and stochastic control often have natural multiscale structure : sub-tasks are assembled together to accomplish complex goals . systematically inferring and leveraging hierarchical structure , particularly beyond a single level of abstraction , has remained a longstanding challenge . we describe a fast multiscale procedure for repeatedly compressing , or homogenizing , markov decision processes ( mdps ) , wherein a hierarchy of sub-problems at different scales is automatically determined . coarsened mdps are themselves independent , deterministic mdps , and may be solved using existing algorithms . the multiscale representation delivered by this procedure decouples sub-tasks from each other and can lead to substantial improvements in convergence rates both locally within sub-problems and globally across sub-problems , yielding significant computational savings . a second fundamental aspect of this work is that these multiscale decompositions yield new transfer opportunities across different problems , where solutions of sub-tasks at different levels of the hierarchy may be amenable to transfer to new problems . localized transfer of policies and potential operators at arbitrary scales is emphasized . finally , we demonstrate compression and transfer in a collection of illustrative domains , including examples involving discrete and continuous statespaces ."}
{"title": "on clustering time series using euclidean distance and pearson correlation", "abstract": "for time series comparisons , it has often been observed that z-score normalized euclidean distances far outperform the unnormalized variant . in this paper we show that a z-score normalized , squared euclidean distance is , in fact , equal to a distance based on pearson correlation . this has profound impact on many distance-based classification or clustering methods . in addition to this theoretically sound result we also show that the often used k-means algorithm formally needs a mod ification to keep the interpretation as pearson correlation strictly valid . experimental results demonstrate that in many cases the standard k-means algorithm generally produces the same results ."}
{"title": "autoencoder-augmented neuroevolution for visual doom playing", "abstract": "neuroevolution has proven effective at many reinforcement learning tasks , but does not seem to scale well to high-dimensional controller representations , which are needed for tasks where the input is raw pixel data . we propose a novel method where we train an autoencoder to create a comparatively low-dimensional representation of the environment observation , and then use cma-es to train neural network controllers acting on this input data . as the behavior of the agent changes the nature of the input data , the autoencoder training progresses throughout evolution . we test this method in the vizdoom environment built on the classic fps doom , where it performs well on a health-pack gathering task ."}
{"title": "training a feed-forward neural network with artificial bee colony based backpropagation method", "abstract": "back-propagation algorithm is one of the most widely used and popular techniques to optimize the feed forward neural network training . nature inspired meta-heuristic algorithms also provide derivative-free solution to optimize complex problem . artificial bee colony algorithm is a nature inspired meta-heuristic algorithm , mimicking the foraging or food source searching behaviour of bees in a bee colony and this algorithm is implemented in several applications for an improved optimized outcome . the proposed method in this paper includes an improved artificial bee colony algorithm based back-propagation neural network training method for fast and improved convergence rate of the hybrid neural network learning method . the result is analysed with the genetic algorithm based back-propagation method , and it is another hybridized procedure of its kind . analysis is performed over standard data sets , reflecting the light of efficiency of proposed method in terms of convergence speed and rate ."}
{"title": "query strategy for sequential ontology debugging", "abstract": "debugging of ontologies is an important prerequisite for their wide-spread application , especially in areas that rely upon everyday users to create and maintain knowledge bases , as in the case of the semantic web . recent approaches use diagnosis methods to identify causes of inconsistent or incoherent ontologies . however , in most debugging scenarios these methods return many alternative diagnoses , thus placing the burden of fault localization on the user . this paper demonstrates how the target diagnosis can be identified by performing a sequence of observations , that is , by querying an oracle about entailments of the target ontology . we exploit a-priori probabilities of typical user errors to formulate information-theoretic concepts for query selection . our evaluation showed that the proposed method significantly reduces the number of required queries compared to myopic strategies . we experimented with different probability distributions of user errors and different qualities of the a-priori probabilities . our measurements showed the advantageousness of information-theoretic approach to query selection even in cases where only a rough estimate of the priors is available ."}
{"title": "procedural urban environments for fps games", "abstract": "this paper presents a novel approach to procedural generation of urban maps for first person shooter ( fps ) games . a multi-agent evolutionary system is employed to place streets , buildings and other items inside the unity3d game engine , resulting in playable video game levels . a computational agent is trained using machine learning techniques to capture the intent of the game designer as part of the multi-agent system , and to enable a semi-automated aesthetic selection for the underlying genetic algorithm ."}
{"title": "a first approach on modelling staff proactiveness in retail simulation models", "abstract": "there has been a noticeable shift in the relative composition of the industry in the developed countries in recent years ; manufacturing is decreasing while the service sector is becoming more important . however , currently most simulation models for investigating service systems are still built in the same way as manufacturing simulation models , using a process-oriented world view , i.e . they model the flow of passive entities through a system . these kinds of models allow studying aspects of operational management but are not well suited for studying the dynamics that appear in service systems due to human behaviour . for these kinds of studies we require tools that allow modelling the system and entities using an object-oriented world view , where intelligent objects serve as abstract `` actors '' that are goal directed and can behave proactively . in our work we combine process-oriented discrete event simulation modelling and object-oriented agent based simulation modelling to investigate the impact of people management practices on retail productivity . in this paper , we reveal in a series of experiments what impact considering proactivity can have on the output accuracy of simulation models of human centric systems . the model and data we use for this investigation are based on a case study in a uk department store . we show that considering proactivity positively influences the validity of these kinds of models and therefore allows analysts to make better recommendations regarding strategies to apply people management practises ."}
{"title": "restricted global grammar constraints", "abstract": "we investigate the global grammar constraint over restricted classes of context free grammars like deterministic and unambiguous context-free grammars . we show that detecting disentailment for the grammar constraint in these cases is as hard as parsing an unrestricted context free grammar.we also consider the class of linear grammars and give a propagator that runs in quadratic time . finally , to demonstrate the use of linear grammars , we show that a weighted linear grammar constraint can efficiently encode the editdistance constraint , and a conjunction of the editdistance constraint and the regular constraint"}
{"title": "a fuzzy logic based certain trust model for e-commerce", "abstract": "trustworthiness especially for service oriented system is very important topic now a day in it field of the whole world . there are many successful e-commerce organizations presently run in the whole world , but e-commerce has not reached its full potential . the main reason behind this is lack of trust of people in e-commerce . again , proper models are still absent for calculating trust of different e-commerce organizations . most of the present trust models are subjective and have failed to account vagueness and ambiguity of different domain . in this paper we have proposed a new fuzzy logic based certain trust model which considers these ambiguity and vagueness of different domain . fuzzy based certain trust model depends on some certain values given by experts and developers . can be applied in a system like cloud computing , internet , website , e-commerce , etc . to ensure trustworthiness of these platforms . in this paper we show , although fuzzy works with uncertainties , proposed model works with some certain values . some experimental results and validation of the model with linguistics terms are shown at the last part of the paper ."}
{"title": "distributed autonomous online learning : regrets and intrinsic privacy-preserving properties", "abstract": "online learning has become increasingly popular on handling massive data . the sequential nature of online learning , however , requires a centralized learner to store data and update parameters . in this paper , we consider online learning with { \\em distributed } data sources . the autonomous learners update local parameters based on local data sources and periodically exchange information with a small subset of neighbors in a communication network . we derive the regret bound for strongly convex functions that generalizes the work by ram et al . ( 2010 ) for convex functions . most importantly , we show that our algorithm has \\emph { intrinsic } privacy-preserving properties , and we prove the sufficient and necessary conditions for privacy preservation in the network . these conditions imply that for networks with greater-than-one connectivity , a malicious learner can not reconstruct the subgradients ( and sensitive raw data ) of other learners , which makes our algorithm appealing in privacy sensitive applications ."}
{"title": "causal discovery from a mixture of experimental and observational data", "abstract": "this paper describes a bayesian method for combining an arbitrary mixture of observational and experimental data in order to learn causal bayesian networks . observational data are passively observed . experimental data , such as that produced by randomized controlled trials , result from the experimenter manipulating one or more variables ( typically randomly ) and observing the states of other variables . the paper presents a bayesian method for learning the causal structure and parameters of the underlying causal process that is generating the data , given that ( 1 ) the data contains a mixture of observational and experimental case records , and ( 2 ) the causal process is modeled as a causal bayesian network . this learning method was applied using as input various mixtures of experimental and observational data that were generated from the alarm causal bayesian network . in these experiments , the absolute and relative quantities of experimental and observational data were varied systematically . for each of these training datasets , the learning method was applied to predict the causal structure and to estimate the causal parameters that exist among randomly selected pairs of nodes in alarm that are not confounded . the paper reports how these structure predictions and parameter estimates compare with the true causal structures and parameters as given by the alarm network ."}
{"title": "how to maximize user satisfaction degree in multi-service ip networks", "abstract": "bandwidth allocation is a fundamental problem in communication networks . with current network moving towards the future internet model , the problem is further intensified as network traffic demanding far from exceeds network bandwidth capability . maintaining a certain user satisfaction degree therefore becomes a challenge research topic . in this paper , we deal with the problem by proposing basmin , a novel bandwidth allocation scheme that aims to maximize network user 's happiness . we also defined a new metric for evaluating network user satisfaction degree : network worth . a three-step evaluation process is then conducted to compare basmin efficiency with other three popular bandwidth allocation schemes . throughout the tests , we experienced basmin 's advantages over the others ; we even found out that one of the most widely used bandwidth allocation schemes , in fact , is not effective at all ."}
{"title": "supervised q-walk for learning vector representation of nodes in networks", "abstract": "automatic feature learning algorithms are at the forefront of modern day machine learning research . we present a novel algorithm , supervised q-walk , which applies q-learning to generate random walks on graphs such that the walks prove to be useful for learning node features suitable for tackling with the node classification problem . we present another novel algorithm , k-hops neighborhood based confidence values learner , which learns confidence values of labels for unlabelled nodes in the network without first learning the node embedding . these confidence values aid in learning an apt reward function for q-learning . we demonstrate the efficacy of supervised q-walk approach over existing state-of-the-art random walk based node embedding learners in solving the single / multi-label multi-class node classification problem using several real world datasets . summarising , our approach represents a novel state-of-the-art technique to learn features , for nodes in networks , tailor-made for dealing with the node classification problem ."}
{"title": "flow : architecture and benchmarking for reinforcement learning in traffic control", "abstract": "flow is a new computational framework , built to support a key need triggered by the rapid growth of autonomy in ground traffic : controllers for autonomous vehicles in the presence of complex nonlinear dynamics in traffic . leveraging recent advances in deep reinforcement learning ( rl ) , flow enables the use of rl methods such as policy gradient for traffic control and enables benchmarking the performance of classical ( including hand-designed ) controllers with learned policies ( control laws ) . flow integrates traffic microsimulator sumo with deep reinforcement learning library rllab and enables the easy design of traffic tasks , including different networks configurations and vehicle dynamics . we use flow to develop reliable controllers for complex problems , such as controlling mixed-autonomy traffic ( involving both autonomous and human-driven vehicles ) in a ring road . for this , we first show that state-of-the-art hand-designed controllers excel when in-distribution , but fail to generalize ; then , we show that even simple neural network policies can solve the stabilization task across density settings and generalize to out-of-distribution settings ."}
{"title": "overcoming hierarchical difficulty by hill-climbing the building block structure", "abstract": "the building block hypothesis suggests that genetic algorithms ( gas ) are well-suited for hierarchical problems , where efficient solving requires proper problem decomposition and assembly of solution from sub-solution with strong non-linear interdependencies . the paper proposes a hill-climber operating over the building block ( bb ) space that can efficiently address hierarchical problems . the new building block hill-climber ( bbhc ) uses past hill-climb experience to extract bb information and adapts its neighborhood structure accordingly . the perpetual adaptation of the neighborhood structure allows the method to climb the hierarchical structure solving successively the hierarchical levels . it is expected that for fully non deceptive hierarchical bb structures the bbhc can solve hierarchical problems in linearithmic time . empirical results confirm that the proposed method scales almost linearly with the problem size thus clearly outperforms population based recombinative methods ."}
{"title": "selectivity in probabilistic causality : drawing arrows from inputs to stochastic outputs", "abstract": "given a set of several inputs into a system ( e.g. , independent variables characterizing stimuli ) and a set of several stochastically non-independent outputs ( e.g. , random variables describing different aspects of responses ) , how can one determine , for each of the outputs , which of the inputs it is influenced by ? the problem has applications ranging from modeling pairwise comparisons to reconstructing mental processing architectures to conjoint testing . a necessary and sufficient condition for a given pattern of selective influences is provided by the joint distribution criterion , according to which the problem of `` what influences what '' is equivalent to that of the existence of a joint distribution for a certain set of random variables . for inputs and outputs with finite sets of values this criterion translates into a test of consistency of a certain system of linear equations and inequalities ( linear feasibility test ) which can be performed by means of linear programming . the joint distribution criterion also leads to a metatheoretical principle for generating a broad class of necessary conditions ( tests ) for diagrams of selective influences . among them is the class of distance-type tests based on the observation that certain functionals on jointly distributed random variables satisfy triangle inequality ."}
{"title": "bounded recursive self-improvement", "abstract": "we have designed a machine that becomes increasingly better at behaving in underspecified circumstances , in a goal-directed way , on the job , by modeling itself and its environment as experience accumulates . based on principles of autocatalysis , endogeny , and reflectivity , the work provides an architectural blueprint for constructing systems with high levels of operational autonomy in underspecified circumstances , starting from a small seed . through value-driven dynamic priority scheduling controlling the parallel execution of a vast number of reasoning threads , the system achieves recursive self-improvement after it leaves the lab , within the boundaries imposed by its designers . a prototype system has been implemented and demonstrated to learn a complex real-world task , real-time multimodal dialogue with humans , by on-line observation . our work presents solutions to several challenges that must be solved for achieving artificial general intelligence ."}
{"title": "on empirical meaning of randomness with respect to a real parameter", "abstract": "we study the empirical meaning of randomness with respect to a family of probability distributions $ p_\\theta $ , where $ \\theta $ is a real parameter , using algorithmic randomness theory . in the case when for a computable probability distribution $ p_\\theta $ an effectively strongly consistent estimate exists , we show that the levin 's a priory semicomputable semimeasure of the set of all $ p_\\theta $ -random sequences is positive if and only if the parameter $ \\theta $ is a computable real number . the different methods for generating `` meaningful '' $ p_\\theta $ -random sequences with noncomputable $ \\theta $ are discussed ."}
{"title": "the complexity of integer bound propagation", "abstract": "bound propagation is an important artificial intelligence technique used in constraint programming tools to deal with numerical constraints . it is typically embedded within a search procedure ( `` branch and prune '' ) and used at every node of the search tree to narrow down the search space , so it is critical that it be fast . the procedure invokes constraint propagators until a common fixpoint is reached , but the known algorithms for this have a pseudo-polynomial worst-case time complexity : they are fast indeed when the variables have a small numerical range , but they have the well-known problem of being prohibitively slow when these ranges are large . an important question is therefore whether strongly-polynomial algorithms exist that compute the common bound consistent fixpoint of a set of constraints . this paper answers this question . in particular we show that this fixpoint computation is in fact np-complete , even when restricted to binary linear constraints ."}
{"title": "computational scenario-based capability planning", "abstract": "scenarios are pen-pictures of plausible futures , used for strategic planning . the aim of this investigation is to expand the horizon of scenario-based planning through computational models that are able to aid the analyst in the planning process . the investigation builds upon the advances of information and communication technology ( ict ) to create a novel , flexible and customizable computational capability-based planning methodology that is practical and theoretically sound . we will show how evolutionary computation , in particular evolutionary multi-objective optimization , can play a central role - both as an optimizer and as a source for innovation ."}
{"title": "a new smooth approximation to the zero one loss with a probabilistic interpretation", "abstract": "we examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function . our approach is based on using the posterior mean of a novel generalized beta-bernoulli formulation . this leads to a generalized logistic function that approximates the zero one loss , but retains a probabilistic formulation conferring a number of useful properties . the approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction . we present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima . our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set . our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard uc irvine and libsvm evaluation datasets to product review predictions and a visual information extraction task . we observe that the approach : 1 ) is more robust to outliers compared to the logistic and hinge losses ; 2 ) outperforms comparable logistic and max margin models on larger scale benchmark problems ; 3 ) when combined with gaussian- laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than support vector machine classifiers ; and 4 ) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance ."}
{"title": "distance metric ensemble learning and the andrews-curtis conjecture", "abstract": "motivated by the search for a counterexample to the poincar\\'e conjecture in three and four dimensions , the andrews-curtis conjecture was proposed in 1965. it is now generally suspected that the andrews-curtis conjecture is false , but small potential counterexamples are not so numerous , and previous work has attempted to eliminate some via combinatorial search . progress has however been limited , with the most successful approach ( breadth-first-search using secondary storage ) being neither scalable nor heuristically-informed . a previous empirical analysis of problem structure examined several heuristic measures of search progress and determined that none of them provided any useful guidance for search . in this article , we induce new quality measures directly from the problem structure and combine them to produce a more effective search driver via ensemble machine learning . by this means , we eliminate 19 potential counterexamples , the status of which had been unknown for some years ."}
{"title": "trajectorynet : an embedded gps trajectory representation for point-based classification using recurrent neural networks", "abstract": "understanding and discovering knowledge from gps ( global positioning system ) traces of human activities is an essential topic in mobility-based urban computing . we propose trajectorynet-a neural network architecture for point-based trajectory classification to infer real world human transportation modes from gps traces . to overcome the challenge of capturing the underlying latent factors in the low-dimensional and heterogeneous feature space imposed by gps data , we develop a novel representation that embeds the original feature space into another space that can be understood as a form of basis expansion . we also enrich the feature space via segment-based information and use maxout activations to improve the predictive power of recurrent neural networks ( rnns ) . we achieve over 98 % classification accuracy when detecting four types of transportation modes , outperforming existing models without additional sensory data or location-based prior knowledge ."}
{"title": "introspective perception : learning to predict failures in vision systems", "abstract": "as robots aspire for long-term autonomous operations in complex dynamic environments , the ability to reliably take mission-critical decisions in ambiguous situations becomes critical . this motivates the need to build systems that have situational awareness to assess how qualified they are at that moment to make a decision . we call this self-evaluating capability as introspection . in this paper , we take a small step in this direction and propose a generic framework for introspective behavior in perception systems . our goal is to learn a model to reliably predict failures in a given system , with respect to a task , directly from input sensor data . we present this in the context of vision-based autonomous mav flight in outdoor natural environments , and show that it effectively handles uncertain situations ."}
{"title": "the divide-and-conquer subgoal-ordering algorithm for speeding up logic inference", "abstract": "it is common to view programs as a combination of logic and control : the logic part defines what the program must do , the control part -- how to do it . the logic programming paradigm was developed with the intention of separating the logic from the control . recently , extensive research has been conducted on automatic generation of control for logic programs . only a few of these works considered the issue of automatic generation of control for improving the efficiency of logic programs . in this paper we present a novel algorithm for automatic finding of lowest-cost subgoal orderings . the algorithm works using the divide-and-conquer strategy . the given set of subgoals is partitioned into smaller sets , based on co-occurrence of free variables . the subsets are ordered recursively and merged , yielding a provably optimal order . we experimentally demonstrate the utility of the algorithm by testing it in several domains , and discuss the possibilities of its cooperation with other existing methods ."}
{"title": "a meta-analysis of the anomaly detection problem", "abstract": "this article provides a thorough meta-analysis of the anomaly detection problem . to accomplish this we first identify approaches to benchmarking anomaly detection algorithms across the literature and produce a large corpus of anomaly detection benchmarks that vary in their construction across several dimensions we deem important to real-world applications : ( a ) point difficulty , ( b ) relative frequency of anomalies , ( c ) clusteredness of anomalies , and ( d ) relevance of features . we apply a representative set of anomaly detection algorithms to this corpus , yielding a very large collection of experimental results . we analyze these results to understand many phenomena observed in previous work . first we observe the effects of experimental design on experimental results . second , results are evaluated with two metrics , roc area under the curve and average precision . we employ statistical hypothesis testing to demonstrate the value ( or lack thereof ) of our benchmarks . we then offer several approaches to summarizing our experimental results , drawing several conclusions about the impact of our methodology as well as the strengths and weaknesses of some algorithms . last , we compare results against a trivial solution as an alternate means of normalizing the reported performance of algorithms . the intended contributions of this article are many ; in addition to providing a large publicly-available corpus of anomaly detection benchmarks , we provide an ontology for describing anomaly detection contexts , a methodology for controlling various aspects of benchmark creation , guidelines for future experimental design and a discussion of the many potential pitfalls of trying to measure success in this field ."}
{"title": "information extraction in illicit domains", "abstract": "extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact . such domains employ atypical language models , have ` long tails ' and suffer from the problem of concept drift . in this paper , we propose a lightweight , feature-agnostic information extraction ( ie ) paradigm specifically designed for such domains . our approach uses raw , unlabeled text from an initial corpus , and a few ( 12-120 ) seed annotations per domain-specific attribute , to learn robust ie models for unobserved pages and websites . empirically , we demonstrate that our approach can outperform feature-centric conditional random field baselines by over 18\\ % f-measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings . we also show that our approach is demonstrably robust to concept drift , and can be efficiently bootstrapped even in a serial computing environment ."}
{"title": "chi-square tests driven method for learning the structure of factored mdps", "abstract": "sdyna is a general framework designed to address large stochastic reinforcement learning problems . unlike previous model based methods in fmdps , it incrementally learns the structure and the parameters of a rl problem using supervised learning techniques . then , it integrates decision-theoric planning algorithms based on fmdps to compute its policy . spiti is an instanciation of sdyna that exploits iti , an incremental decision tree algorithm , to learn the reward function and the dynamic bayesian networks with local structures representing the transition function of the problem . these representations are used by an incremental version of the structured value iteration algorithm . in order to learn the structure , spiti uses chi-square tests to detect the independence between two probability distributions . thus , we study the relation between the threshold used in the chi-square test , the size of the model built and the relative error of the value function of the induced policy with respect to the optimal value . we show that , on stochastic problems , one can tune the threshold so as to generate both a compact model and an efficient policy . then , we show that spiti , while keeping its model compact , uses the generalization property of its learning method to perform better than a stochastic classical tabular algorithm in large rl problem with an unknown structure . we also introduce a new measure based on chi-square to qualify the accuracy of the model learned by spiti . we qualitatively show that the generalization property in spiti within the fmdp framework may prevent an exponential growth of the time required to learn the structure of large stochastic rl problems ."}
{"title": "a pre-semantics for counterfactual conditionals and similar logics", "abstract": "the elegant stalnaker/lewis semantics for counterfactual conditonals works with distances between models . but human beings certainly have no tables of models and distances in their head . we begin here an investigation using a more realistic picture , based on findings in neuroscience . we call it a pre-semantics , as its meaning is not a description of the world , but of the brain , whose structure is ( partly ) determined by the world it reasons about . in the final section , we reconsider the components , and postulate that there are no atomic pictures , we can always look inside ."}
{"title": "exponentiated gradient exploration for active learning", "abstract": "active learning strategies respond to the costly labelling task in a supervised classification by selecting the most useful unlabelled examples in training a predictive model . many conventional active learning algorithms focus on refining the decision boundary , rather than exploring new regions that can be more informative . in this setting , we propose a sequential algorithm named eg-active that can improve any active learning algorithm by an optimal random exploration . experimental results show a statistically significant and appreciable improvement in the performance of our new approach over the existing active feedback methods ."}
{"title": "a model-based approach to predicting predator-prey & friend-foe relationships in ant colonies", "abstract": "understanding predator-prey relationships among insects is a challenging task in the domain of insect-colony research . this is due to several factors involved , such as determining whether a particular behavior is the result of a predator-prey interaction , a friend-foe interaction or another kind of interaction . in this paper , we analyze a series of predator-prey and friend-foe interactions in two colonies of carpenter ants to better understand and predict such behavior . using the data gathered , we have also come up with a preliminary model for predicting such behavior under the specific conditions the experiment was conducted in . in this paper , we present the results of our data analysis as well as an overview of the processes involved ."}
{"title": "de-identification of patient notes with recurrent neural networks", "abstract": "objective : patient notes in electronic health records ( ehrs ) may contain critical information for medical investigations . however , the vast majority of medical investigators can only access de-identified notes , in order to protect the confidentiality of patients . in the united states , the health insurance portability and accountability act ( hipaa ) defines 18 types of protected health information ( phi ) that needs to be removed to de-identify patient notes . manual de-identification is impractical given the size of ehr databases , the limited number of researchers with access to the non-de-identified notes , and the frequent mistakes of human annotators . a reliable automated de-identification system would consequently be of high value . materials and methods : we introduce the first de-identification system based on artificial neural networks ( anns ) , which requires no handcrafted features or rules , unlike existing systems . we compare the performance of the system with state-of-the-art systems on two datasets : the i2b2 2014 de-identification challenge dataset , which is the largest publicly available de-identification dataset , and the mimic de-identification dataset , which we assembled and is twice as large as the i2b2 2014 dataset . results : our ann model outperforms the state-of-the-art systems . it yields an f1-score of 97.85 on the i2b2 2014 dataset , with a recall 97.38 and a precision of 97.32 , and an f1-score of 99.23 on the mimic de-identification dataset , with a recall 99.25 and a precision of 99.06. conclusion : our findings support the use of anns for de-identification of patient notes , as they show better performance than previously published systems while requiring no feature engineering ."}
{"title": "accelerating science : a computing research agenda", "abstract": "the emergence of `` big data '' offers unprecedented opportunities for not only accelerating scientific advances but also enabling new modes of discovery . scientific progress in many disciplines is increasingly enabled by our ability to examine natural phenomena through the computational lens , i.e. , using algorithmic or information processing abstractions of the underlying processes ; and our ability to acquire , share , integrate and analyze disparate types of data . however , there is a huge gap between our ability to acquire , store , and process data and our ability to make effective use of the data to advance discovery . despite successful automation of routine aspects of data management and analytics , most elements of the scientific process currently require considerable human expertise and effort . accelerating science to keep pace with the rate of data acquisition and data processing calls for the development of algorithmic or information processing abstractions , coupled with formal methods and tools for modeling and simulation of natural processes as well as major innovations in cognitive tools for scientists , i.e. , computational tools that leverage and extend the reach of human intellect , and partner with humans on a broad range of tasks in scientific discovery ( e.g. , identifying , prioritizing formulating questions , designing , prioritizing and executing experiments designed to answer a chosen question , drawing inferences and evaluating the results , and formulating new questions , in a closed-loop fashion ) . this calls for concerted research agenda aimed at : development , analysis , integration , sharing , and simulation of algorithmic or information processing abstractions of natural processes , coupled with formal methods and tools for their analyses and simulation ; innovations in cognitive tools that augment and extend human intellect and partner with humans in all aspects of science ."}
{"title": "semi-separable hamiltonian monte carlo for inference in bayesian hierarchical models", "abstract": "sampling from hierarchical bayesian models is often difficult for mcmc methods , because of the strong correlations between the model parameters and the hyperparameters . recent riemannian manifold hamiltonian monte carlo ( rmhmc ) methods have significant potential advantages in this setting , but are computationally expensive . we introduce a new rmhmc method , which we call semi-separable hamiltonian monte carlo , which uses a specially designed mass matrix that allows the joint hamiltonian over model parameters and hyperparameters to decompose into two simpler hamiltonians . this structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm . the resulting method can mix faster than simpler gibbs sampling while being simpler and more efficient than previous instances of rmhmc ."}
{"title": "upgrading ambiguous signs in qpns", "abstract": "wa qualitative probabilistic network models the probabilistic relationships between its variables by means of signs . non-monotonic influences have associated an ambiguous sign . these ambiguous signs typically lead to uninformative results upon inference . a non-monotonic influence can , however , be associated with a , more informative , sign that indicates its effect in the current state of the network . to capture this effect , we introduce the concept of situational sign . furthermore , if the network converts to a state in which all variables that provoke the non-monotonicity have been observed , a non-monotonic influence reduces to a monotonic influence . we study the persistence and propagation of situational signs upon inference and give a method to establish the sign of a reduced influence ."}
{"title": "non-linear label ranking for large-scale prediction of long-term user interests", "abstract": "we consider the problem of personalization of online services from the viewpoint of ad targeting , where we seek to find the best ad categories to be shown to each user , resulting in improved user experience and increased advertisers ' revenue . we propose to address this problem as a task of ranking the ad categories depending on a user 's preference , and introduce a novel label ranking approach capable of efficiently learning non-linear , highly accurate models in large-scale settings . experiments on a real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-k retrieval performance , strongly suggesting the benefit of using the proposed model on large-scale ranking problems ."}
{"title": "a definition of happiness for reinforcement learning agents", "abstract": "what is happiness for reinforcement learning agents ? we seek a formal definition satisfying a list of desiderata . our proposed definition of happiness is the temporal difference error , i.e . the difference between the value of the obtained reward and observation and the agent 's expectation of this value . this definition satisfies most of our desiderata and is compatible with empirical research on humans . we state several implications and discuss examples ."}
{"title": "models and algorithms for skip-free markov decision processes on trees", "abstract": "we introduce a class of models for multidimensional control problems which we call skip-free markov decision processes on trees . we describe and analyse an algorithm applicable to markov decision processes of this type that are skip-free in the negative direction . starting with the finite average cost case , we show that the algorithm combines the advantages of both value iteration and policy iteration -- it is guaranteed to converge to an optimal policy and optimal value function after a finite number of iterations but the computational effort required for each iteration step is comparable with that for value iteration . we show that the algorithm can also be used to solve discounted cost models and continuous time models , and that a suitably modified algorithm can be used to solve communicating models ."}
{"title": "learning the past tense of english verbs : the symbolic pattern associator vs. connectionist models", "abstract": "learning the past tense of english verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986 , and has become a landmark task for testing the adequacy of cognitive modeling . several artificial neural networks ( anns ) have been implemented , and a challenge for better symbolic models has been posed . in this paper , we present a general-purpose symbolic pattern associator ( spa ) based upon the decision-tree learning algorithm id3 . we conduct extensive head-to-head comparisons on the generalization ability between ann models and the spa under different representations . we conclude that the spa generalizes the past tense of unseen verbs better than ann models by a wide margin , and we offer insights as to why this should be the case . we also discuss a new default strategy for decision-tree learning algorithms ."}
{"title": "bib2vec : an embedding-based search system for bibliographic information", "abstract": "we propose a novel embedding model that represents relationships among several elements in bibliographic information with high representation ability and flexibility . based on this model , we present a novel search system that shows the relationships among the elements in the acl anthology reference corpus . the evaluation results show that our model can achieve a high prediction ability and produce reasonable search results ."}
{"title": "characterization of graphs for protein structure modeling and recognition of solubility", "abstract": "this paper deals with the relations among structural , topological , and chemical properties of the e.coli proteome from the vantage point of the solubility/aggregation propensity of proteins . each e.coli protein is initially represented according to its known folded 3d shape . this step consists in representing the available e.coli proteins in terms of graphs . we first analyze those graphs by considering pure topological characterizations , i.e. , by analyzing the mass fractal dimension and the distribution underlying both shortest paths and vertex degrees . results confirm the general architectural principles of proteins . successively , we focus on the statistical properties of a representation of such graphs in terms of vectors composed of several numerical features , which we extracted from their structural representation . we found that protein size is the main discriminator for the solubility , while however there are other factors that help explaining the solubility degree . we finally analyze such data through a novel one-class classifier , with the aim of discriminating among very and poorly soluble proteins . results are encouraging and consolidate the potential of pattern recognition techniques when employed to describe complex biological systems ."}
{"title": "multimodal meaning representation for generic dialogue systems architectures", "abstract": "an unified language for the communicative acts between agents is essential for the design of multi-agents architectures . whatever the type of interaction ( linguistic , multimodal , including particular aspects such as force feedback ) , whatever the type of application ( command dialogue , request dialogue , database querying ) , the concepts are common and we need a generic meta-model . in order to tend towards task-independent systems , we need to clarify the modules parameterization procedures . in this paper , we focus on the characteristics of a meta-model designed to represent meaning in linguistic and multimodal applications . this meta-model is called mmil for multimodal interface language , and has first been specified in the framework of the ist miamm european project . what we want to test here is how relevant is mmil for a completely different context ( a different task , a different interaction type , a different linguistic domain ) . we detail the exploitation of mmil in the framework of the ist ozone european project , and we draw the conclusions on the role of mmil in the parameterization of task-independent dialogue managers ."}
{"title": "understanding the social cascading of geekspeak and the upshots for social cognitive systems", "abstract": "barring swarm robotics , a substantial share of current machine-human and machine-machine learning and interaction mechanisms are being developed and fed by results of agent-based computer simulations , game-theoretic models , or robotic experiments based on a dyadic communication pattern . yet , in real life , humans no less frequently communicate in groups , and gain knowledge and take decisions basing on information cumulatively gleaned from more than one single source . these properties should be taken into consideration in the design of autonomous artificial cognitive systems construed to interact with learn from more than one contact or 'neighbour ' . to this end , significant practical import can be gleaned from research applying strict science methodology to human and social phenomena , e.g . to discovery of realistic creativity potential spans , or the 'exposure thresholds ' after which new information could be accepted by a cognitive agent . the results will be presented of a project analysing the social propagation of neologisms in a microblogging service . from local , low-level interactions and information flows between agents inventing and imitating discrete lexemes we aim to describe the processes of the emergence of more global systemic order and dynamics , using the latest methods of complexity science . whether in order to mimic them , or to 'enhance ' them , parameters gleaned from complexity science approaches to humans ' social and humanistic behaviour should subsequently be incorporated as points of reference in the field of robotics and human-machine interaction ."}
{"title": "information fusion in multi-task gaussian processes", "abstract": "this paper evaluates heterogeneous information fusion using multi-task gaussian processes in the context of geological resource modeling . specifically , it empirically demonstrates that information integration across heterogeneous information sources leads to superior estimates of all the quantities being modeled , compared to modeling them individually . multi-task gaussian processes provide a powerful approach for simultaneous modeling of multiple quantities of interest while taking correlations between these quantities into consideration . experiments are performed on large scale real sensor data ."}
{"title": "large collection of diverse gene set search queries recapitulate known protein-protein interactions and gene-gene functional associations", "abstract": "popular online enrichment analysis tools from the field of molecular systems biology provide users with the ability to submit their experimental results as gene sets for individual analysis . such queries are kept private , and have never before been considered as a resource for integrative analysis . by harnessing gene set query submissions from thousands of users , we aim to discover biological knowledge beyond the scope of an individual study . in this work , we investigated a large collection of gene sets submitted to the tool enrichr by thousands of users . based on co-occurrence , we constructed a global gene-gene association network . we interpret this inferred network as providing a summary of the structure present in this crowdsourced gene set library , and show that this network recapitulates known protein-protein interactions and functional associations between genes . this finding implies that this network also offers predictive value . furthermore , we visualize this gene-gene association network using a new edge-pruning algorithm that retains both the local and global structures of large-scale networks . our ability to make predictions for currently unknown gene associations , that may not be captured by individual researchers and data sources , is a demonstration of the potential of harnessing collective knowledge from users of popular tools in the field of molecular systems biology ."}
{"title": "counterfactual reasoning and learning systems", "abstract": "this work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system . such predictions allow both humans and algorithms to select changes that improve both the short-term and long-term performance of such systems . this work is illustrated by experiments carried out on the ad placement system associated with the bing search engine ."}
{"title": "the bayesian decision tree technique with a sweeping strategy", "abstract": "the uncertainty of classification outcomes is of crucial importance for many safety critical applications including , for example , medical diagnostics . in such applications the uncertainty of classification can be reliably estimated within a bayesian model averaging technique that allows the use of prior information . decision tree ( dt ) classification models used within such a technique gives experts additional information by making this classification scheme observable . the use of the markov chain monte carlo ( mcmc ) methodology of stochastic sampling makes the bayesian dt technique feasible to perform . however , in practice , the mcmc technique may become stuck in a particular dt which is far away from a region with a maximal posterior . sampling such dts causes bias in the posterior estimates , and as a result the evaluation of classification uncertainty may be incorrect . in a particular case , the negative effect of such sampling may be reduced by giving additional prior information on the shape of dts . in this paper we describe a new approach based on sweeping the dts without additional priors on the favorite shape of dts . the performances of bayesian dt techniques with the standard and sweeping strategies are compared on a synthetic data as well as on real datasets . quantitatively evaluating the uncertainty in terms of entropy of class posterior probabilities , we found that the sweeping strategy is superior to the standard strategy ."}
{"title": "advances in artificial intelligence : are you sure , we are on the right track ?", "abstract": "over the past decade , ai has made a remarkable progress . it is agreed that this is due to the recently revived deep learning technology . deep learning enables to process large amounts of data using simplified neuron networks that simulate the way in which the brain works . however , there is a different point of view , which posits that the brain is processing information , not data . this unresolved duality hampered ai progress for years . in this paper , i propose a notion of integrated information that hopefully will resolve the problem . i consider integrated information as a coupling between two separate entities - physical information ( that implies data processing ) and semantic information ( that provides physical information interpretation ) . in this regard , intelligence becomes a product of information processing . extending further this line of thinking , it can be said that information processing does not require more a human brain for its implementation . indeed , bacteria and amoebas exhibit intelligent behavior without any sign of a brain . that dramatically removes the need for ai systems to emulate the human brain complexity ! the paper tries to explore this shift in ai systems design philosophy ."}
{"title": "on the phase transition of finding a biclique in a larger bipartite graph", "abstract": "we report on the phase transition of finding a complete subgraph , of specified dimensions , in a bipartite graph . finding a complete subgraph in a bipartite graph is a problem that has growing attention in several domains , including bioinformatics , social network analysis and domain clustering . a key step for a successful phase transition study is identifying a suitable order parameter , when none is known . to this purpose , we have applied a decision tree classifier to real-world instances of this problem , in order to understand what problem features separate an instance that is hard to solve from those that is not . we have successfully identified one such order parameter and with it the phase transition of finding a complete bipartite subgraph of specified dimensions . our phase transition study shows an easy-to-hard-to-easy-to-hard-to-easy pattern . further , our results indicate that the hardest instances are in a region where it is more likely that the corresponding bipartite graph will have a complete subgraph of specified dimensions , a positive answer . by contrast , instances with a negative answer are more likely to appear in a region where the computational cost is negligible . this behaviour is remarkably similar for problems of a number of different sizes ."}
{"title": "relationship between the second type of covering-based rough set and matroid via closure operator", "abstract": "recently , in order to broad the application and theoretical areas of rough sets and matroids , some authors have combined them from many different viewpoints , such as circuits , rank function , spanning sets and so on . in this paper , we connect the second type of covering-based rough sets and matroids from the view of closure operators . on one hand , we establish a closure system through the fixed point family of the second type of covering lower approximation operator , and then construct a closure operator . for a covering of a universe , the closure operator is a closure one of a matroid if and only if the reduct of the covering is a partition of the universe . on the other hand , we investigate the sufficient and necessary condition that the second type of covering upper approximation operation is a closure one of a matroid ."}
{"title": "loss-sensitive training of probabilistic conditional random fields", "abstract": "we consider the problem of training probabilistic conditional random fields ( crfs ) in the context of a task where performance is measured using a specific loss function . while maximum likelihood is the most common approach to training crfs , it ignores the inherent structure of the task 's loss function . we describe alternatives to maximum likelihood which take that loss into account . these include a novel adaptation of a loss upper bound from the structured svms literature to the crf context , as well as a new loss-inspired kl divergence objective which relies on the probabilistic nature of crfs . these loss-sensitive objectives are compared to maximum likelihood using ranking as a benchmark task . this comparison confirms the importance of incorporating loss information in the probabilistic training of crfs , with the loss-inspired kl outperforming all other objectives ."}
{"title": "strategic positioning in tactical scenario planning", "abstract": "capability planning problems are pervasive throughout many areas of human interest with prominent examples found in defense and security . planning provides a unique context for optimization that has not been explored in great detail and involves a number of interesting challenges which are distinct from traditional optimization research . planning problems demand solutions that can satisfy a number of competing objectives on multiple scales related to robustness , adaptiveness , risk , etc . the scenario method is a key approach for planning . scenarios can be defined for long-term as well as short-term plans . this paper introduces computational scenario-based planning problems and proposes ways to accommodate strategic positioning within the tactical planning domain . we demonstrate the methodology in a resource planning problem that is solved with a multi-objective evolutionary algorithm . our discussion and results highlight the fact that scenario-based planning is naturally framed within a multi-objective setting . however , the conflicting objectives occur on different system levels rather than within a single system alone . this paper also contends that planning problems are of vital interest in many human endeavors and that evolutionary computation may be well positioned for this problem domain ."}
{"title": "separating topology and geometry in space planning", "abstract": "we are dealing with the problem of space layout planning here . we present an architectural conceptual cad approach . starting with design specifications in terms of constraints over spaces , a specific enumeration heuristics leads to a complete set of consistent conceptual design solutions named topological solutions . these topological solutions which do not presume any precise definitive dimension correspond to the sketching step that an architect carries out from the design specifications on a preliminary design phase in architecture ."}
{"title": "normalization and the representation of nonmonotonic knowledge in the theory of evidence", "abstract": "we discuss the dempster-shafer theory of evidence . we introduce a concept of monotonicity which is related to the diminution of the range between belief and plausibility . we show that the accumulation of knowledge in this framework exhibits a nonmonotonic property . we show how the belief structure can be used to represent typical or commonsense knowledge ."}
{"title": "multi-channel parallel adaptation theory for rule discovery", "abstract": "in this paper , we introduce a new machine learning theory based on multi-channel parallel adaptation for rule discovery . this theory is distinguished from the familiar parallel-distributed adaptation theory of neural networks in terms of channel-based convergence to the target rules . we show how to realize this theory in a learning system named cfrule . cfrule is a parallel weight-based model , but it departs from traditional neural computing in that its internal knowledge is comprehensible . furthermore , when the model converges upon training , each channel converges to a target rule . the model adaptation rule is derived by multi-level parallel weight optimization based on gradient descent . since , however , gradient descent only guarantees local optimization , a multi-channel regression-based optimization strategy is developed to effectively deal with this problem . formally , we prove that the cfrule model can explicitly and precisely encode any given rule set . also , we prove a property related to asynchronous parallel convergence , which is a critical element of the multi-channel parallel adaptation theory for rule learning . thanks to the quantizability nature of the cfrule model , rules can be extracted completely and soundly via a threshold-based mechanism . finally , the practical application of the theory is demonstrated in dna promoter recognition and hepatitis prognosis prediction ."}
{"title": "embedding lexical features via low-rank tensors", "abstract": "modern nlp models rely heavily on engineered features , which often combine word and contextual information into complex lexical features . such combination results in large numbers of features , which can lead to over-fitting . we present a new model that represents complex lexical features -- -comprised of parts for words , contextual information and labels -- -in a tensor that captures conjunction information among these parts . we apply low-rank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed . furthermore , we investigate two methods for handling features that include $ n $ -grams of mixed lengths . our model achieves state-of-the-art results on tasks in relation extraction , pp-attachment , and preposition disambiguation ."}
{"title": "shield : fast , practical defense and vaccination for deep learning using jpeg compression", "abstract": "the rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks ( dnns ) are highly vulnerable to adversarially generated images . this underscores the urgent need for practical defense that can be readily deployed to combat attacks in real-time . observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible , we place jpeg compression at the core of our proposed shield defense framework , utilizing its capability to effectively `` compress away '' such pixel manipulation . to immunize a dnn model from artifacts introduced by compression , shield `` vaccinates '' a model by re-training it with compressed images , where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense . on top of that , shield adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels , making it harder for an adversary to estimate the transformation performed . this novel combination of vaccination , ensembling , and randomization makes shield a fortified multi-pronged protection . we conducted extensive , large-scale experiments using the imagenet dataset , and show that our approaches eliminate up to 94 % of black-box attacks and 98 % of gray-box attacks delivered by the recent , strongest attacks , such as carlini-wagner 's l2 and deepfool . our approaches are fast and work without requiring knowledge about the model ."}
{"title": "physiological and behavioral profiling for nociceptive pain estimation using personalized multitask learning", "abstract": "pain is a subjective experience commonly measured through patient 's self report . while there exist numerous situations in which automatic pain estimation methods may be preferred , inter-subject variability in physiological and behavioral pain responses has hindered the development of such methods . in this work , we address this problem by introducing a novel personalized multitask machine learning method for pain estimation based on individual physiological and behavioral pain response profiles , and show its advantages in a dataset containing multimodal responses to nociceptive heat pain ."}
{"title": "intercausal reasoning with uninstantiated ancestor nodes", "abstract": "intercausal reasoning is a common inference pattern involving probabilistic dependence of causes of an observed common effect . the sign of this dependence is captured by a qualitative property called product synergy . the current definition of product synergy is insufficient for intercausal reasoning where there are additional uninstantiated causes of the common effect . we propose a new definition of product synergy and prove its adequacy for intercausal reasoning with direct and indirect evidence for the common effect . the new definition is based on a new property matrix half positive semi-definiteness , a weakened form of matrix positive semi-definiteness ."}
{"title": "decision under uncertainty", "abstract": "we derive axiomatically the probability function that should be used to make decisions given any form of underlying uncertainty ."}
{"title": "the sat phase transition", "abstract": "phase transition is an important feature of sat problem . for random k-sat model , it is proved that as r ( ratio of clauses to variables ) increases , the structure of solutions will undergo a sudden change like satisfiability phase transition when r reaches a threshold point . this phenomenon shows that the satisfying truth assignments suddenly shift from being relatively different from each other to being very similar to each other ."}
{"title": "guiding the search in continuous state-action spaces by learning an action sampling distribution from off-target samples", "abstract": "in robotics , it is essential to be able to plan efficiently in high-dimensional continuous state-action spaces for long horizons . for such complex planning problems , unguided uniform sampling of actions until a path to a goal is found is hopelessly inefficient , and gradient-based approaches often fall short when the optimization manifold of a given problem is not smooth . in this paper we present an approach that guides the search of a state-space planner , such as a* , by learning an action-sampling distribution that can generalize across different instances of a planning problem . the motivation is that , unlike typical learning approaches for planning for continuous action space that estimate a policy , an estimated action sampler is more robust to error since it has a planner to fall back on . we use a generative adversarial network ( gan ) , and address an important issue : search experience consists of a relatively large number of actions that are not on a solution path and a relatively small number of actions that actually are on a solution path . we introduce a new technique , based on an importance-ratio estimation method , for using samples from a non-target distribution to make gan learning more data-efficient . we provide theoretical guarantees and empirical evaluation in three challenging continuous robot planning problems to illustrate the effectiveness of our algorithm ."}
{"title": "replication and generalization of precise", "abstract": "this report describes an initial replication study of the precise system and develops a clearer , more formal description of the approach . based on our evaluation , we conclude that the precise results do not fully replicate . however the formalization developed here suggests a road map to further enhance and extend the approach pioneered by precise . after a long , productive discussion with ana-maria popescu ( one of the authors of precise ) we got more clarity on the precise approach and how the lexicon was authored for the geo evaluation . based on this we built a more direct implementation over a repaired formalism . although our new evaluation is not yet complete , it is clear that the system is performing much better now . we will continue developing our ideas and implementation and generate a future report/publication that more accurately evaluates precise like approaches ."}
{"title": "automated prediction of temporal relations", "abstract": "background : there has been growing research interest in automated answering of questions or generation of summary of free form text such as news article . in order to implement this task , the computer should be able to identify the sequence of events , duration of events , time at which event occurred and the relationship type between event pairs , time pairs or event-time pairs . specific problem : it is important to accurately identify the relationship type between combinations of event and time before the temporal ordering of events can be defined . the machine learning approach taken in mani et . al ( 2006 ) provides an accuracy of only 62.5 on the baseline data from timebank . the researchers used maximum entropy classifier in their methodology . timeml uses the tlink annotation to tag a relationship type between events and time . the time complexity is quadratic when it comes to tagging documents with tlink using human annotation . this research proposes using decision tree and parsing to improve the relationship type tagging . this research attempts to solve the gaps in human annotation by automating the task of relationship type tagging in an attempt to improve the accuracy of event and time relationship in annotated documents . scope information : the documents from the domain of news will be used . the tagging will be performed within the same document and not across documents . the relationship types will be identified only for a pair of event and time and not a chain of events . the research focuses on documents tagged using the timeml specification which contains tags such as event , tlink , and timex . each tag has attributes such as identifier , relation , pos , time etc ."}
{"title": "evaluating and modelling hanabi-playing agents", "abstract": "agent modelling involves considering how other agents will behave , in order to influence your own actions . in this paper , we explore the use of agent modelling in the hidden-information , collaborative card game hanabi . we implement a number of rule-based agents , both from the literature and of our own devising , in addition to an information set monte carlo tree search ( is-mcts ) agent . we observe poor results from is-mcts , so construct a new , predictor version that uses a model of the agents with which it is paired . we observe a significant improvement in game-playing strength from this agent in comparison to is-mcts , resulting from its consideration of what the other agents in a game would do . in addition , we create a flawed rule-based agent to highlight the predictor 's capabilities with such an agent ."}
{"title": "ct-nor : representing and reasoning about events in continuous time", "abstract": "we present a generative model for representing and reasoning about the relationships among events in continuous time . we apply the model to the domain of networked and distributed computing environments where we fit the parameters of the model from timestamp observations , and then use hypothesis testing to discover dependencies between the events and changes in behavior for monitoring and diagnosis . after introducing the model , we present an em algorithm for fitting the parameters and then present the hypothesis testing approach for both dependence discovery and change-point detection . we validate the approach for both tasks using real data from a trace of network events at microsoft research cambridge . finally , we formalize the relationship between the proposed model and the noisy-or gate for cases when time can be discretized ."}
{"title": "backdoors into heterogeneous classes of sat and csp", "abstract": "in this paper we extend the classical notion of strong and weak backdoor sets for sat and csp by allowing that different instantiations of the backdoor variables result in instances that belong to different base classes ; the union of the base classes forms a heterogeneous base class . backdoor sets to heterogeneous base classes can be much smaller than backdoor sets to homogeneous ones , hence they are much more desirable but possibly harder to find . we draw a detailed complexity landscape for the problem of detecting strong and weak backdoor sets into heterogeneous base classes for sat and csp ."}
{"title": "cluster-based specification techniques in dempster-shafer theory for an evidential intelligence analysis of multipletarget tracks ( thesis abstract )", "abstract": "in intelligence analysis it is of vital importance to manage uncertainty . intelligence data is almost always uncertain and incomplete , making it necessary to reason and taking decisions under uncertainty . one way to manage the uncertainty in intelligence analysis is dempster-shafer theory . this thesis contains five results regarding multiple target tracks and intelligence specification ."}
{"title": "on- and off-policy monotonic policy improvement", "abstract": "monotonic policy improvement and off-policy learning are two main desirable properties for reinforcement learning algorithms . in this paper , by lower bounding the performance difference of two policies , we show that the monotonic policy improvement is guaranteed from on- and off-policy mixture samples . an optimization procedure which applies the proposed bound can be regarded as an off-policy natural policy gradient method . in order to support the theoretical result , we provide a trust region policy optimization method using experience replay as a naive application of our bound , and evaluate its performance in two classical benchmark problems ."}
{"title": "identifying spatial relations in images using convolutional neural networks", "abstract": "traditional approaches to building a large scale knowledge graph have usually relied on extracting information ( entities , their properties , and relations between them ) from unstructured text ( e.g . dbpedia ) . recent advances in convolutional neural networks ( cnn ) allow us to shift our focus to learning entities and relations from images , as they build robust models that require little or no pre-processing of the images . in this paper , we present an approach to identify and extract spatial relations ( e.g. , the girl is standing behind the table ) from images using cnns . our research addresses two specific challenges : providing insight into how spatial relations are learned by the network and which parts of the image are used to predict these relations . we use the pre-trained network vggnet to extract features from an image and train a multi-layer perceptron ( mlp ) on a set of synthetic images and the sun09 dataset to extract spatial relations . the mlp predicts spatial relations without a bounding box around the objects or the space in the image depicting the relation . to understand how the spatial relations are represented in the network , a heatmap is overlayed on the image to show the regions that are deemed important by the network . also , we analyze the mlp to show the relationship between the activation of consistent groups of nodes and the prediction of a spatial relation . we show how the loss of these groups affects the networks ability to identify relations ."}
{"title": "on the intertranslatability of argumentation semantics", "abstract": "translations between different nonmonotonic formalisms always have been an important topic in the field , in particular to understand the knowledge-representation capabilities those formalisms offer . we provide such an investigation in terms of different semantics proposed for abstract argumentation frameworks , a nonmonotonic yet simple formalism which received increasing interest within the last decade . although the properties of these different semantics are nowadays well understood , there are no explicit results about intertranslatability . we provide such translations wrt . different properties and also give a few novel complexity results which underlie some negative results ."}
{"title": "energy saving additive neural network", "abstract": "in recent years , machine learning techniques based on neural networks for mobile computing become increasingly popular . classical multi-layer neural networks require matrix multiplications at each stage . multiplication operation is not an energy efficient operation and consequently it drains the battery of the mobile device . in this paper , we propose a new energy efficient neural network with the universal approximation property over space of lebesgue integrable functions . this network , called , additive neural network , is very suitable for mobile computing . the neural structure is based on a novel vector product definition , called ef-operator , that permits a multiplier-free implementation . in ef-operation , the `` product '' of two real numbers is defined as the sum of their absolute values , with the sign determined by the sign of the product of the numbers . this `` product '' is used to construct a vector product in $ r^n $ . the vector product induces the $ l_1 $ norm . the proposed additive neural network successfully solves the xor problem . the experiments on mnist dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi-layer perceptron and convolutional neural networks ( lenet ) ."}
{"title": "bayesian reinforcement learning : a survey", "abstract": "bayesian methods for machine learning have been widely investigated , yielding principled methods for incorporating prior information into inference algorithms . in this survey , we provide an in-depth review of the role of bayesian methods for the reinforcement learning ( rl ) paradigm . the major incentives for incorporating bayesian reasoning in rl are : 1 ) it provides an elegant approach to action-selection ( exploration/exploitation ) as a function of the uncertainty in learning ; and 2 ) it provides a machinery to incorporate prior knowledge into the algorithms . we first discuss models and methods for bayesian inference in the simple single-step bandit model . we then review the extensive recent literature on bayesian methods for model-based rl , where prior information can be expressed on the parameters of the markov model . we also present bayesian methods for model-free rl , where priors are expressed over the value function or policy class . the objective of the paper is to provide a comprehensive survey on bayesian rl algorithms and their theoretical and empirical properties ."}
{"title": "simultaneous object detection , tracking , and event recognition", "abstract": "the common internal structure and algorithmic organization of object detection , detection-based tracking , and event recognition facilitates a general approach to integrating these three components . this supports multidirectional information flow between these components allowing object detection to influence tracking and event recognition and event recognition to influence tracking and object detection . the performance of the combination can exceed the performance of the components in isolation . this can be done with linear asymptotic complexity ."}
{"title": "intuitionistic computability logic", "abstract": "computability logic ( cl ) is a systematic formal theory of computational tasks and resources , which , in a sense , can be seen as a semantics-based alternative to ( the syntactically introduced ) linear logic . with its expressive and flexible language , where formulas represent computational problems and `` truth '' is understood as algorithmic solvability , cl potentially offers a comprehensive logical basis for constructive applied theories and computing systems inherently requiring constructive and computationally meaningful underlying logics . among the best known constructivistic logics is heyting 's intuitionistic calculus int , whose language can be seen as a special fragment of that of cl . the constructivistic philosophy of int , however , has never really found an intuitively convincing and mathematically strict semantical justification . cl has good claims to provide such a justification and hence a materialization of kolmogorov 's known thesis `` int = logic of problems '' . the present paper contains a soundness proof for int with respect to the cl semantics . a comprehensive online source on cl is available at http : //www.cis.upenn.edu/~giorgi/cl.html"}
{"title": "embodied evolution in collective robotics : a review", "abstract": "this paper provides an overview of evolutionary robotics techniques applied to on-line distributed evolution for robot collectives -- namely , embodied evolution . it provides a definition of embodied evolution as well as a thorough description of the underlying concepts and mechanisms . the paper also presents a comprehensive summary of research published in the field since its inception ( 1999-2017 ) , providing various perspectives to identify the major trends . in particular , we identify a shift from considering embodied evolution as a parallel search method within small robot collectives ( fewer than 10 robots ) to embodied evolution as an on-line distributed learning method for designing collective behaviours in swarm-like collectives . the paper concludes with a discussion of applications and open questions , providing a milestone for past and an inspiration for future research ."}
{"title": "clause/term resolution and learning in the evaluation of quantified boolean formulas", "abstract": "resolution is the rule of inference at the basis of most procedures for automated reasoning . in these procedures , the input formula is first translated into an equisatisfiable formula in conjunctive normal form ( cnf ) and then represented as a set of clauses . deduction starts by inferring new clauses by resolution , and goes on until the empty clause is generated or satisfiability of the set of clauses is proven , e.g. , because no new clauses can be generated . in this paper , we restrict our attention to the problem of evaluating quantified boolean formulas ( qbfs ) . in this setting , the above outlined deduction process is known to be sound and complete if given a formula in cnf and if a form of resolution , called q-resolution , is used . we introduce q-resolution on terms , to be used for formulas in disjunctive normal form . we show that the computation performed by most of the available procedures for qbfs -- based on the davis-logemann-loveland procedure ( dll ) for propositional satisfiability -- corresponds to a tree in which q-resolution on terms and clauses alternate . this poses the theoretical bases for the introduction of learning , corresponding to recording q-resolution formulas associated with the nodes of the tree . we discuss the problems related to the introduction of learning in dll based procedures , and present solutions extending state-of-the-art proposals coming from the literature on propositional satisfiability . finally , we show that our dll based solver extended with learning , performs significantly better on benchmarks used in the 2003 qbf solvers comparative evaluation ."}
{"title": "ontoverbal : a generic tool and practical application to snomed ct", "abstract": "ontology development is a non-trivial task requiring expertise in the chosen ontological language . we propose a method for making the content of ontologies more transparent by presenting , through the use of natural language generation , naturalistic descriptions of ontology classes as textual paragraphs . the method has been implemented in a proof-of- concept system , ontoverbal , that automatically generates paragraph-sized textual descriptions of ontological classes expressed in owl . ontoverbal has been applied to ontologies that can be loaded into prot\\'eg\\'e and been evaluated with snomed ct , showing that it provides coherent , well-structured and accurate textual descriptions of ontology classes ."}
{"title": "a joint speaker-listener-reinforcer model for referring expressions", "abstract": "referring expressions are natural language constructions used to identify particular objects within a scene . in this paper , we propose a unified framework for the tasks of referring expression comprehension and generation . our model is composed of three modules : speaker , listener , and reinforcer . the speaker generates referring expressions , the listener comprehends referring expressions , and the reinforcer introduces a reward function to guide sampling of more discriminative expressions . the listener-speaker modules are trained jointly in an end-to-end learning framework , allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer 's feedback . we demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets . project and demo page : https : //vision.cs.unc.edu/refer"}
{"title": "full object boundary detection by applying scale invariant features in a region merging segmentation algorithm", "abstract": "object detection is a fundamental task in computer vision and has many applications in image processing . this paper proposes a new approach for object detection by applying scale invariant feature transform ( sift ) in an automatic segmentation algorithm . sift is an invariant algorithm respect to scale , translation and rotation . the features are very distinct and provide stable keypoints that can be used for matching an object in different images . at first , an object is trained with different aspects for finding best keypoints . the object can be recognized in the other images by using achieved keypoints . then , a robust segmentation algorithm is used to detect the object with full boundary based on sift keypoints . in segmentation algorithm , a merging role is defined to merge the regions in image with the assistance of keypoints . the results show that the proposed approach is reliable for object detection and can extract object boundary well ."}
{"title": "how ( not ) to train your generative model : scheduled sampling , likelihood , adversary ?", "abstract": "modern applications and progress in deep learning research have created renewed interest for generative models of text and of images . however , even today it is unclear what objective functions one should use to train and evaluate these models . in this paper we present two contributions . firstly , we present a critique of scheduled sampling , a state-of-the-art training method that contributed to the winning entry to the mscoco image captioning benchmark in 2015. here we show that despite this impressive empirical performance , the objective function underlying scheduled sampling is improper and leads to an inconsistent learning algorithm . secondly , we revisit the problems that scheduled sampling was meant to address , and present an alternative interpretation . we argue that maximum likelihood is an inappropriate training objective when the end-goal is to generate natural-looking samples . we go on to derive an ideal objective function to use in this situation instead . we introduce a generalisation of adversarial training , and show how such method can interpolate between maximum likelihood training and our ideal training objective . to our knowledge this is the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality ."}
{"title": "toward a dynamic programming solution for the 4-peg tower of hanoi problem with configurations", "abstract": "the frame-stewart algorithm for the 4-peg variant of the tower of hanoi , introduced in 1941 , partitions disks into intermediate towers before moving the remaining disks to their destination . algorithms that partition the disks have not been proven to be optimal , although they have been verified for up to 30 disks . this paper presents a dynamic programming approach to this algorithm , using tabling in b-prolog . this study uses a variation of the problem , involving configurations of disks , in order to contrast the tabling approach with the approaches utilized by other solvers . a comparison of different partitioning locations for the frame-stewart algorithm indicates that , although certain partitions are optimal for the classic problem , they need to be modified for certain configurations , and that random configurations might require an entirely new algorithm ."}
{"title": "evolution in groups : a deeper look at synaptic cluster driven evolution of deep neural networks", "abstract": "a promising paradigm for achieving highly efficient deep neural networks is the idea of evolutionary deep intelligence , which mimics biological evolution processes to progressively synthesize more efficient networks . a crucial design factor in evolutionary deep intelligence is the genetic encoding scheme used to simulate heredity and determine the architectures of offspring networks . in this study , we take a deeper look at the notion of synaptic cluster-driven evolution of deep neural networks which guides the evolution process towards the formation of a highly sparse set of synaptic clusters in offspring networks . utilizing a synaptic cluster-driven genetic encoding , the probabilistic encoding of synaptic traits considers not only individual synaptic properties but also inter-synaptic relationships within a deep neural network . this process results in highly sparse offspring networks which are particularly tailored for parallel computational devices such as gpus and deep neural network accelerator chips . comprehensive experimental results using four well-known deep neural network architectures ( lenet-5 , alexnet , resnet-56 , and detectnet ) on two different tasks ( object categorization and object detection ) demonstrate the efficiency of the proposed method . cluster-driven genetic encoding scheme synthesizes networks that can achieve state-of-the-art performance with significantly smaller number of synapses than that of the original ancestor network . ( $ \\sim $ 125-fold decrease in synapses for mnist ) . furthermore , the improved cluster efficiency in the generated offspring networks ( $ \\sim $ 9.71-fold decrease in clusters for mnist and a $ \\sim $ 8.16-fold decrease in clusters for kitti ) is particularly useful for accelerated performance on parallel computing hardware architectures such as those in gpus and deep neural network accelerator chips ."}
{"title": "personalized risk scoring for critical care prognosis using mixtures of gaussian processes", "abstract": "objective : in this paper , we develop a personalized real-time risk scoring algorithm that provides timely and granular assessments for the clinical acuity of ward patients based on their ( temporal ) lab tests and vital signs ; the proposed risk scoring system ensures timely intensive care unit ( icu ) admissions for clinically deteriorating patients . methods : the risk scoring system learns a set of latent patient subtypes from the offline electronic health record data , and trains a mixture of gaussian process ( gp ) experts , where each expert models the physiological data streams associated with a specific patient subtype . transfer learning techniques are used to learn the relationship between a patient 's latent subtype and her static admission information ( e.g . age , gender , transfer status , icd-9 codes , etc ) . results : experiments conducted on data from a heterogeneous cohort of 6,321 patients admitted to ronald reagan ucla medical center show that our risk score significantly and consistently outperforms the currently deployed risk scores , such as the rothman index , mews , apache and sofa scores , in terms of timeliness , true positive rate ( tpr ) , and positive predictive value ( ppv ) . conclusion : our results reflect the importance of adopting the concepts of personalized medicine in critical care settings ; significant accuracy and timeliness gains can be achieved by accounting for the patients ' heterogeneity . significance : the proposed risk scoring methodology can confer huge clinical and social benefits on more than 200,000 critically ill inpatient who exhibit cardiac arrests in the us every year ."}
{"title": "ik-pso , pso inverse kinematics solver with application to biped gait generation", "abstract": "this paper describes a new approach allowing the generation of a simplified biped gait . this approach combines a classical dynamic modeling with an inverse kinematics ' solver based on particle swarm optimization , pso . first , an inverted pendulum , ip , is used to obtain a simplified dynamic model of the robot and to compute the target position of a key point in biped locomotion , the centre of mass , com . the proposed algorithm , called ik-pso , inverse kinematics pso , returns and inverse kinematics solution corresponding to that com respecting the joints constraints . in this paper the inertia weight pso variant is used to generate a possible solution according to the stability based fitness function and a set of joints motions constraints . the method is applied with success to a leg motion generation . since based on a pre-calculated com , that satisfied the biped stability , the proposal allowed also to plan a walk with application on a small size biped robot ."}
{"title": "best practices for applying deep learning to novel applications", "abstract": "this report is targeted to groups who are subject matter experts in their application but deep learning novices . it contains practical advice for those interested in testing the use of deep neural networks on applications that are novel for deep learning . we suggest making your project more manageable by dividing it into phases . for each phase this report contains numerous recommendations and insights to assist novice practitioners ."}
{"title": "a nonclassical symbolic theory of working memory , mental computations , and mental set", "abstract": "the paper tackles four basic questions associated with human brain as a learning system . how can the brain learn to ( 1 ) mentally simulate different external memory aids , ( 2 ) perform , in principle , any mental computations using imaginary memory aids , ( 3 ) recall the real sensory and motor events and synthesize a combinatorial number of imaginary events , ( 4 ) dynamically change its mental set to match a combinatorial number of contexts ? we propose a uniform answer to ( 1 ) - ( 4 ) based on the general postulate that the human neocortex processes symbolic information in a `` nonclassical '' way . instead of manipulating symbols in a read/write memory , as the classical symbolic systems do , it manipulates the states of dynamical memory representing different temporary attributes of immovable symbolic structures stored in a long-term memory . the approach is formalized as the concept of e-machine . intuitively , an e-machine is a system that deals mainly with characteristic functions representing subsets of memory pointers rather than the pointers themselves . this nonclassical symbolic paradigm is turing universal , and , unlike the classical one , is efficiently implementable in homogeneous neural networks with temporal modulation topologically resembling that of the neocortex ."}
{"title": "dynamic non-bayesian decision making", "abstract": "the model of a non-bayesian agent who faces a repeated game with incomplete information against nature is an appropriate tool for modeling general agent-environment interactions . in such a model the environment state ( controlled by nature ) may change arbitrarily , and the feedback/reward function is initially unknown . the agent is not bayesian , that is he does not form a prior probability neither on the state selection strategy of nature , nor on his reward function . a policy for the agent is a function which assigns an action to every history of observations and actions . two basic feedback structures are considered . in one of them -- the perfect monitoring case -- the agent is able to observe the previous environment state as part of his feedback , while in the other -- the imperfect monitoring case -- all that is available to the agent is the reward obtained . both of these settings refer to partially observable processes , where the current environment state is unknown . our main result refers to the competitive ratio criterion in the perfect monitoring case . we prove the existence of an efficient stochastic policy that ensures that the competitive ratio is obtained at almost all stages with an arbitrarily high probability , where efficiency is measured in terms of rate of convergence . it is further shown that such an optimal policy does not exist in the imperfect monitoring case . moreover , it is proved that in the perfect monitoring case there does not exist a deterministic policy that satisfies our long run optimality criterion . in addition , we discuss the maxmin criterion and prove that a deterministic efficient optimal strategy does exist in the imperfect monitoring case under this criterion . finally we show that our approach to long-run optimality can be viewed as qualitative , which distinguishes it from previous work in this area ."}
{"title": "comparing human and automated evaluation of open-ended student responses to questions of evolution", "abstract": "written responses can provide a wealth of data in understanding student reasoning on a topic . yet they are time- and labor-intensive to score , requiring many instructors to forego them except as limited parts of summative assessments at the end of a unit or course . recent developments in machine learning ( ml ) have produced computational methods of scoring written responses for the presence or absence of specific concepts . here , we compare the scores from one particular ml program -- evograder -- to human scoring of responses to structurally- and content-similar questions that are distinct from the ones the program was trained on . we find that there is substantial inter-rater reliability between the human and ml scoring . however , sufficient systematic differences remain between the human and ml scoring that we advise only using the ml scoring for formative , rather than summative , assessment of student reasoning ."}
{"title": "a novel multicriteria group decision making approach with intuitionistic fuzzy sir method", "abstract": "the superiority and inferiority ranking ( sir ) method is a generation of the well-known promethee method , which can be more efficient to deal with multi-criterion decision making ( mcdm ) problem . intuitionistic fuzzy sets ( ifss ) , as an important extension of fuzzy sets ( ifs ) , include both membership functions and non-membership functions and can be used to , more precisely describe uncertain information . in real world , decision situations are usually under uncertain environment and involve multiple individuals who have their own points of view on handing of decision problems . in order to solve uncertainty group mcdm problem , we propose a novel intuitionistic fuzzy sir method in this paper . this approach uses intuitionistic fuzzy aggregation operators and sir ranking methods to handle uncertain information ; integrate individual opinions into group opinions ; make decisions on multiple-criterion ; and finally structure a specific decision map . the proposed approach is illustrated in a simulation of group decision making problem related to supply chain management ."}
{"title": "a unified game-theoretic approach to multiagent reinforcement learning", "abstract": "to achieve general intelligence , agents must learn how to interact with others in a shared environment : this is the challenge of multiagent reinforcement learning ( marl ) . the simplest form is independent reinforcement learning ( inrl ) , where each agent treats its experience as part of its ( non-stationary ) environment . in this paper , we first observe that policies learned using inrl can overfit to the other agents ' policies during training , failing to sufficiently generalize during execution . we introduce a new metric , joint-policy correlation , to quantify this effect . we describe an algorithm for general marl , based on approximate best responses to mixtures of policies generated using deep reinforcement learning , and empirical game-theoretic analysis to compute meta-strategies for policy selection . the algorithm generalizes previous ones such as inrl , iterated best response , double oracle , and fictitious play . then , we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers . finally , we demonstrate the generality of the resulting policies in two partially observable settings : gridworld coordination games and poker ."}
{"title": "action-based character ai in video-games with cogbots architecture : a preliminary report", "abstract": "in this paper we propose an architecture for specifying the interaction of non-player characters ( npcs ) in the game-world in a way that abstracts common tasks in four main conceptual components , namely perception , deliberation , control , action . we argue that this architecture , inspired by ai research on autonomous agents and robots , can offer a number of benefits in the form of abstraction , modularity , re-usability and higher degrees of personalization for the behavior of each npc . we also show how this architecture can be used to tackle a simple scenario related to the navigation of npcs under incomplete information about the obstacles that may obstruct the various way-points in the game , in a simple and effective way ."}
{"title": "solution of the decision making problems using fuzzy soft relations", "abstract": "the fuzzy modeling has been applied in a wide variety of fields such as engineering and management sciences and social sciences to solve a number decision making problems which involve impreciseness , uncertainty and vagueness in data . in particular , applications of this modeling technique in decision making problems have remarkable significance . these problems have been tackled using various theories such as probability theory , fuzzy set theory , rough set theory , vague set theory , approximate reasoning theory etc . which lack in parameterization of the tools due to which they could not be applied successfully to such problems . the concept of soft set has a promising potential for giving an optimal solution for these problems . with the motivation of this new concept , in this paper we define the concepts of soft relation and fuzzy soft relation and then apply them to solve a number of decision making problems . the advantages of fuzzy soft relation compared to other paradigms are discussed . to the best of our knowledge this is the first work on the application of fuzzy soft relation to the decision making problems ."}
{"title": "generating graphical chain by mutual matching of bayesian network and extracted rules of bayesian network using genetic algorithm", "abstract": "with the technology development , the need of analyze and extraction of useful information is increasing . bayesian networks contain knowledge from data and experts that could be used for decision making processes but they are not easily understandable thus the rule extraction methods have been used but they have high computation costs . to overcome this problem we extract rules from bayesian network using genetic algorithm . then we generate the graphical chain by mutually matching the extracted rules and bayesian network . this graphical chain could shows the sequence of events that lead to the target which could help the decision making process . the experimental results on small networks show that the proposed method has comparable results with brute force method which has a significantly higher computation cost ."}
{"title": "learning network of multivariate hawkes processes : a time series approach", "abstract": "learning the influence structure of multiple time series data is of great interest to many disciplines . this paper studies the problem of recovering the causal structure in network of multivariate linear hawkes processes . in such processes , the occurrence of an event in one process affects the probability of occurrence of new events in some other processes . thus , a natural notion of causality exists between such processes captured by the support of the excitation matrix . we show that the resulting causal influence network is equivalent to the directed information graph ( dig ) of the processes , which encodes the causal factorization of the joint distribution of the processes . furthermore , we present an algorithm for learning the support of excitation matrix ( or equivalently the dig ) . the performance of the algorithm is evaluated on synthesized multivariate hawkes networks as well as a stock market and memetracker real-world dataset ."}
{"title": "on modeling profiles instead of values", "abstract": "we consider the problem of estimating the distribution underlying an observed sample of data . instead of maximum likelihood , which maximizes the probability of the ob served values , we propose a different estimate , the high-profile distribution , which maximizes the probability of the observed profile the number of symbols appearing any given number of times . we determine the high-profile distribution of several data samples , establish some of its general properties , and show that when the number of distinct symbols observed is small compared to the data size , the high-profile and maximum-likelihood distributions are roughly the same , but when the number of symbols is large , the distributions differ , and high-profile better explains the data ."}
{"title": "towards `` alphachem '' : chemical synthesis planning with tree search and deep neural network policies", "abstract": "retrosynthesis is a technique to plan the chemical synthesis of organic molecules , for example drugs , agro- and fine chemicals . in retrosynthesis , a search tree is built by analysing molecules recursively and dissecting them into simpler molecular building blocks until one obtains a set of known building blocks . the search space is intractably large , and it is difficult to determine the value of retrosynthetic positions . here , we propose to model retrosynthesis as a markov decision process . in combination with a deep neural network policy learned from essentially the complete published knowledge of chemistry , monte carlo tree search ( mcts ) can be used to evaluate positions . in exploratory studies , we demonstrate that mcts with neural network policies outperforms the traditionally used best-first search with hand-coded heuristics ."}
{"title": "unsupervised learning in a framework of information compression by multiple alignment , unification and search", "abstract": "this paper describes a novel approach to unsupervised learning that has been developed within a framework of `` information compression by multiple alignment , unification and search '' ( icmaus ) , designed to integrate learning with other ai functions such as parsing and production of language , fuzzy pattern recognition , probabilistic and exact forms of reasoning , and others ."}
{"title": "applying cooperative machine learning to speed up the annotation of social signals in large multi-modal corpora", "abstract": "scientific disciplines , such as behavioural psychology , anthropology and recently social signal processing are concerned with the systematic exploration of human behaviour . a typical work-flow includes the manual annotation ( also called coding ) of social signals in multi-modal corpora of considerable size . for the involved annotators this defines an exhausting and time-consuming task . in the article at hand we present a novel method and also provide the tools to speed up the coding procedure . to this end , we suggest and evaluate the use of cooperative machine learning ( cml ) techniques to reduce manual labelling efforts by combining the power of computational capabilities and human intelligence . the proposed cml strategy starts with a small number of labelled instances and concentrates on predicting local parts first . afterwards , a session-independent classification model is created to finish the remaining parts of the database . confidence values are computed to guide the manual inspection and correction of the predictions . to bring the proposed approach into application we introduce nova - an open-source tool for collaborative and machine-aided annotations . in particular , it gives labellers immediate access to cml strategies and directly provides visual feedback on the results . our experiments show that the proposed method has the potential to significantly reduce human labelling efforts ."}
{"title": "a simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling", "abstract": "we introduce a simple and accurate neural model for dependency-based semantic role labeling . our model predicts predicate-argument dependencies relying on states of a bidirectional lstm encoder . the semantic role labeler achieves competitive performance on english , even without any kind of syntactic information and only using local inference . however , when automatically predicted part-of-speech tags are provided as input , it substantially outperforms all previous local models and approaches the best reported results on the english conll-2009 dataset . we also consider chinese , czech and spanish where our approach also achieves competitive results . syntactic parsers are unreliable on out-of-domain data , so standard ( i.e. , syntactically-informed ) srl models are hindered when tested in this setting . our syntax-agnostic model appears more robust , resulting in the best reported results on standard out-of-domain test sets ."}
{"title": "modelling information incorporation in markets , with application to detecting and explaining events", "abstract": "we develop a model of how information flows into a market , and derive algorithms for automatically detecting and explaining relevant events . we analyze data from twenty-two `` political stock markets '' ( i.e. , betting markets on political outcomes ) on the iowa electronic market ( iem ) . we prove that , under certain efficiency assumptions , prices in such betting markets will on average approach the correct outcomes over time , and show that iem data conforms closely to the theory . we present a simple model of a betting market where information is revealed over time , and show a qualitative correspondence between the model and real market data . we also present an algorithm for automatically detecting significant events and generating semantic explanations of their origin . the algorithm operates by discovering significant changes in vocabulary on online news sources ( using expected entropy loss ) that align with major price spikes in related betting markets ."}
{"title": "the ( final ) countdown", "abstract": "the countdown game is one of the oldest tv show running in the world . it started broadcasting in 1972 on the french television and in 1982 on british channel 4 , and it has been running since in both countries . the game , while extremely popular , never received any serious scientific attention , probably because it seems too simple at first sight . we present in this article an in-depth analysis of the numbers round of the countdown game . this includes a complexity analysis of the game , an analysis of existing algorithms , the presentation of a new algorithm that increases resolution speed by a factor of 20. it also includes some leads on how to turn the game into a more difficult one , both for a human player and for a computer , and even to transform it into a probably undecidable problem ."}
{"title": "transition systems for model generators - a unifying approach", "abstract": "a fundamental task for propositional logic is to compute models of propositional formulas . programs developed for this task are called satisfiability solvers . we show that transition systems introduced by nieuwenhuis , oliveras , and tinelli to model and analyze satisfiability solvers can be adapted for solvers developed for two other propositional formalisms : logic programming under the answer-set semantics , and the logic pc ( id ) . we show that in each case the task of computing models can be seen as `` satisfiability modulo answer-set programming , '' where the goal is to find a model of a theory that also is an answer set of a certain program . the unifying perspective we develop shows , in particular , that solvers clasp and minisatid are closely related despite being developed for different formalisms , one for answer-set programming and the latter for the logic pc ( id ) ."}
{"title": "analysis of first prototype universal intelligence tests : evaluating and comparing ai algorithms and humans", "abstract": "today , available methods that assess ai systems are focused on using empirical techniques to measure the performance of algorithms in some specific tasks ( e.g. , playing chess , solving mazes or land a helicopter ) . however , these methods are not appropriate if we want to evaluate the general intelligence of ai and , even less , if we compare it with human intelligence . the anynt project has designed a new method of evaluation that tries to assess ai systems using well known computational notions and problems which are as general as possible . this new method serves to assess general intelligence ( which allows us to learn how to solve any new kind of problem we face ) and not only to evaluate performance on a set of specific tasks . this method not only focuses on measuring the intelligence of algorithms , but also to assess any intelligent system ( human beings , animals , ai , aliens ? , ... ) , and letting us to place their results on the same scale and , therefore , to be able to compare them . this new approach will allow us ( in the future ) to evaluate and compare any kind of intelligent system known or even to build/find , be it artificial or biological . this master thesis aims at ensuring that this new method provides consistent results when evaluating ai algorithms , this is done through the design and implementation of prototypes of universal intelligence tests and their application to different intelligent systems ( ai algorithms and humans beings ) . from the study we analyze whether the results obtained by two different intelligent systems are properly located on the same scale and we propose changes and refinements to these prototypes in order to , in the future , being able to achieve a truly universal intelligence test ."}
{"title": "optimal sparse linear auto-encoders and sparse pca", "abstract": "principal components analysis ( pca ) is the optimal linear auto-encoder of data , and it is often used to construct features . enforcing sparsity on the principal components can promote better generalization , while improving the interpretability of the features . we study the problem of constructing optimal sparse linear auto-encoders . two natural questions in such a setting are : i ) given a level of sparsity , what is the best approximation to pca that can be achieved ? ii ) are there low-order polynomial-time algorithms which can asymptotically achieve this optimal tradeoff between the sparsity and the approximation quality ? in this work , we answer both questions by giving efficient low-order polynomial-time algorithms for constructing asymptotically \\emph { optimal } linear auto-encoders ( in particular , sparse features with near-pca reconstruction error ) and demonstrate the performance of our algorithms on real data ."}
{"title": "temporal reasoning about uncertain worlds", "abstract": "we present a program that manages a database of temporally scoped beliefs . the basic functionality of the system includes maintaining a network of constraints among time points , supporting a variety of fetches , mediating the application of causal rules , monitoring intervals of time for the addition of new facts , and managing data dependencies that keep the database consistent . at this level the system operates independent of any measure of belief or belief calculus . we provide an example of how an application program mi9ght use this functionality to implement a belief calculus ."}
{"title": "adaptive importance sampling for estimation in structured domains", "abstract": "sampling is an important tool for estimating large , complex sums and integrals over high dimensional spaces . for instance , important sampling has been used as an alternative to exact methods for inference in belief networks . ideally , we want to have a sampling distribution that provides optimal-variance estimators . in this paper , we present methods that improve the sampling distribution by systematically adapting it as we obtain information from the samples . we present a stochastic-gradient-descent method for sequentially updating the sampling distribution based on the direct minization of the variance . we also present other stochastic-gradient-descent methods based on the minimizationof typical notions of distance between the current sampling distribution and approximations of the target , optimal distribution . we finally validate and compare the different methods empirically by applying them to the problem of action evaluation in influence diagrams ."}
{"title": "domain modelling in computational persuasion for behaviour change in healthcare", "abstract": "the aim of behaviour change is to help people to change aspects of their behaviour for the better ( e.g. , to decrease calorie intake , to drink in moderation , to take more exercise , to complete a course of antibiotics once started , etc. ) . in current persuasion technology for behaviour change , the emphasis is on helping people to explore their issues ( e.g. , through questionnaires or game playing ) or to remember to follow a behaviour change plan ( e.g. , diaries and email reminders ) . however , recent developments in computational persuasion are leading to an argument-centric approach to persuasion that can potentially be harnessed in behaviour change applications . in this paper , we review developments in computational persuasion , and then focus on domain modelling as a key component . we present a multi-dimensional approach to domain modelling . at the core of this proposal is an ontology which provides a representation of key factors , in particular kinds of belief , which we have identified in the behaviour change literature as being important in diverse behaviour change initiatives . our proposal for domain modelling is intended to facilitate the acquisition and representation of the arguments that can be used in persuasion dialogues , together with meta-level information about them which can be used by the persuader to make strategic choices of argument to present ."}
{"title": "sigma point belief propagation", "abstract": "the sigma point ( sp ) filter , also known as unscented kalman filter , is an attractive alternative to the extended kalman filter and the particle filter . here , we extend the sp filter to nonsequential bayesian inference corresponding to loopy factor graphs . we propose sigma point belief propagation ( spbp ) as a low-complexity approximation of the belief propagation ( bp ) message passing scheme . spbp achieves approximate marginalizations of posterior distributions corresponding to ( generally ) loopy factor graphs . it is well suited for decentralized inference because of its low communication requirements . for a decentralized , dynamic sensor localization problem , we demonstrate that spbp can outperform nonparametric ( particle-based ) bp while requiring significantly less computations and communications ."}
{"title": "on ullman 's theorem in computer vision", "abstract": "both in the plane and in space , we invert the nonlinear ullman transformation for 3 points and 3 orthographic cameras . while ullman 's theorem assures a unique reconstruction modulo a reflection for 3 cameras and 4 points , we find a locally unique reconstruction for 3 cameras and 3 points . explicit reconstruction formulas allow to decide whether picture data of three cameras seeing three points can be realized as a point-camera configuration ."}
{"title": "learning abduction under partial observability", "abstract": "juba recently proposed a formulation of learning abductive reasoning from examples , in which both the relative plausibility of various explanations , as well as which explanations are valid , are learned directly from data . the main shortcoming of this formulation of the task is that it assumes access to full-information ( i.e. , fully specified ) examples ; relatedly , it offers no role for declarative background knowledge , as such knowledge is rendered redundant in the abduction task by complete information . in this work , we extend the formulation to utilize such partially specified examples , along with declarative background knowledge about the missing data . we show that it is possible to use implicitly learned rules together with the explicitly given declarative knowledge to support hypotheses in the course of abduction . we observe that when a small explanation exists , it is possible to obtain a much-improved guarantee in the challenging exception-tolerant setting . such small , human-understandable explanations are of particular interest for potential applications of the task ."}
{"title": "mixed strategy for constrained stochastic optimal control", "abstract": "choosing control inputs randomly can result in a reduced expected cost in optimal control problems with stochastic constraints , such as stochastic model predictive control ( smpc ) . we consider a controller with initial randomization , meaning that the controller randomly chooses from k+1 control sequences at the beginning ( called k-randimization ) .it is known that , for a finite-state , finite-action markov decision process ( mdp ) with k constraints , k-randimization is sufficient to achieve the minimum cost . we found that the same result holds for stochastic optimal control problems with continuous state and action spaces.furthermore , we show the randomization of control input can result in reduced cost when the optimization problem is nonconvex , and the cost reduction is equal to the duality gap . we then provide the necessary and sufficient conditions for the optimality of a randomized solution , and develop an efficient solution method based on dual optimization . furthermore , in a special case with k=1 such as a joint chance-constrained problem , the dual optimization can be solved even more efficiently by root finding . finally , we test the theories and demonstrate the solution method on multiple practical problems ranging from path planning to the planning of entry , descent , and landing ( edl ) for future mars missions ."}
{"title": "indoor uav scheduling with restful task assignment algorithm", "abstract": "research in uav scheduling has obtained an emerging interest from scientists in the optimization field . when the scheduling itself has established a strong root since the 19th century , works on uav scheduling in indoor environment has come forth in the latest decade . several works on scheduling uav operations in indoor ( two and three dimensional ) and outdoor environments are reported . in this paper , a further study on uav scheduling in three dimensional indoor environment is investigated . dealing with indoor environment\\textemdash where humans , uavs , and other elements or infrastructures are likely to coexist in the same space\\textemdash draws attention towards the safety of the operations . in relation to the battery level , a preserved battery level leads to safer operations , promoting the uav to have a decent remaining power level . a methodology which consists of a heuristic approach based on restful task assignment algorithm , incorporated with particle swarm optimization algorithm , is proposed . the motivation is to preserve the battery level throughout the operations , which promotes less possibility in having failed uavs on duty . this methodology is tested with 54 benchmark datasets stressing on 4 different aspects : geographical distance , number of tasks , number of predecessors , and slack time . the test results and their characteristics in regard to the proposed methodology are discussed and presented ."}
{"title": "the shape of art history in the eyes of the machine", "abstract": "how does the machine classify styles in art ? and how does it relate to art historians ' methods for analyzing style ? several studies have shown the ability of the machine to learn and predict style categories , such as renaissance , baroque , impressionism , etc. , from images of paintings . this implies that the machine can learn an internal representation encoding discriminative features through its visual analysis . however , such a representation is not necessarily interpretable . we conducted a comprehensive study of several of the state-of-the-art convolutional neural networks applied to the task of style classification on 77k images of paintings , and analyzed the learned representation through correlation analysis with concepts derived from art history . surprisingly , the networks could place the works of art in a smooth temporal arrangement mainly based on learning style labels , without any a priori knowledge of time of creation , the historical time and context of styles , or relations between styles . the learned representations showed that there are few underlying factors that explain the visual variations of style in art . some of these factors were found to correlate with style patterns suggested by heinrich w\\ '' olfflin ( 1846-1945 ) . the learned representations also consistently highlighted certain artists as the extreme distinctive representative of their styles , which quantitatively confirms art historian observations ."}
{"title": "beating the world 's best at super smash bros. with deep reinforcement learning", "abstract": "there has been a recent explosion in the capabilities of game-playing artificial intelligence . many classes of rl tasks , from atari games to motor control to board games , are now solvable by fairly generic algorithms , based on deep learning , that learn to play from experience with minimal knowledge of the specific domain of interest . in this work , we will investigate the performance of these methods on super smash bros. melee ( ssbm ) , a popular console fighting game . the ssbm environment has complex dynamics and partial observability , making it challenging for human and machine alike . the multi-player aspect poses an additional challenge , as the vast majority of recent advances in rl have focused on single-agent environments . nonetheless , we will show that it is possible to train agents that are competitive against and even surpass human professionals , a new result for the multi-player video game setting ."}
{"title": "autonomous quadrotor landing using deep reinforcement learning", "abstract": "landing an unmanned aerial vehicle ( uav ) on a ground marker is an open problem despite the effort of the research community . previous attempts mostly focused on the analysis of hand-crafted geometric features and the use of external sensors in order to allow the vehicle to approach the land-pad . in this article , we propose a method based on deep reinforcement learning that only requires low-resolution images taken from a down-looking camera in order to identify the position of the marker and land the uav on it . the proposed approach is based on a hierarchy of deep q-networks ( dqns ) used as high-level control policy for the navigation toward the marker . we implemented different technical solutions , such as the combination of vanilla and double dqns , and a partitioned buffer replay . using domain randomization we trained the vehicle on uniform textures and we tested it on a large variety of simulated and real-world environments . the overall performance is comparable with a state-of-the-art algorithm and human pilots ."}
{"title": "spatio-temporal graphical model selection", "abstract": "we consider the problem of estimating the topology of spatial interactions in a discrete state , discrete time spatio-temporal graphical model where the interactions affect the temporal evolution of each agent in a network . among other models , the susceptible , infected , recovered ( $ sir $ ) model for interaction events fall into this framework . we pose the problem as a structure learning problem and solve it using an $ \\ell_1 $ -penalized likelihood convex program . we evaluate the solution on a simulated spread of infectious over a complex network . our topology estimates outperform those of a standard spatial markov random field graphical model selection using $ \\ell_1 $ -regularized logistic regression ."}
{"title": "a characterization of monotone influence measures for data classification", "abstract": "in this work we focus on the following question : how important was the i-th feature in determining the outcome for a given datapoint ? we identify a family of influence measures ; functions that , given a datapoint x , assign a value phi_i ( x ) to every feature i , which roughly corresponds to that i 's importance in determining the outcome for x. this family is uniquely derived from a set of axioms : desirable properties that any reasonable influence measure should satisfy . departing from prior work on influence measures , we assume no knowledge of - or access to - the underlying classifier labelling the dataset . in other words , our influence measures are based on the dataset alone , and do not make any queries to the classifier . while this requirement naturally limits the scope of explanations we provide , we show that it is effective on real datasets ."}
{"title": "an ontology of preference-based multiobjective metaheuristics", "abstract": "user preference integration is of great importance in multi-objective optimization , in particular in many objective optimization . preferences have long been considered in traditional multicriteria decision making ( mcdm ) which is based on mathematical programming . recently , it is integrated in multi-objective metaheuristics ( momh ) , resulting in focus on preferred parts of the pareto front instead of the whole pareto front . the number of publications on preference-based multi-objective metaheuristics has increased rapidly over the past decades . there already exist various preference handling methods and momh methods , which have been combined in diverse ways . this article proposes to use the web ontology language ( owl ) to model and systematize the results developed in this field . a review of the existing work is provided , based on which an ontology is built and instantiated with state-of-the-art results . the owl ontology is made public and open to future extension . moreover , the usage of the ontology is exemplified for different use-cases , including querying for methods that match an engineering application , bibliometric analysis , checking existence of combinations of preference models and momh techniques , and discovering opportunities for new research and open research questions ."}
{"title": "towards monetary incentives in social q & a services", "abstract": "community-based question answering ( cqa ) services are facing key challenges to motivate domain experts to provide timely answers . recently , cqa services are exploring new incentive models to engage experts and celebrities by allowing them to set a price on their answers . in this paper , we perform a data-driven analysis on two emerging payment-based cqa systems : fenda ( china ) and whale ( us ) . by analyzing a large dataset of 220k questions ( worth 1 million usd collectively ) , we examine how monetary incentives affect different players in the system . we find that , while monetary incentive enables quick answers from experts , it also drives certain users to aggressively game the system for profits . in addition , in this supplier-driven marketplace , users need to proactively adjust their price to make profits . famous people are unwilling to lower their price , which in turn hurts their income and engagement over time . finally , we discuss the key implications to future cqa design ."}
{"title": "design and software implementation of subsystems for creating and using the ontological base of a research scientist", "abstract": "creation of the information systems and tools for scientific research and development support has always been one of the central directions of the development of computer science . the main features of the modern evolution of scientific research and development are the transdisciplinary approach and the deep intellectualisation of all stages of the life cycle of formulation and solution of scientific problems . the theoretical and practical aspects of the development of perspective complex knowledge-oriented information systems and their components are considered in the paper . the analysis of existing scientific information systems ( or current research information systems , cris ) and synthesis of general principles of design of the research and development workstation environment of a researcher and its components are carried out in the work . the functional components of knowledge-oriented information system research and development workstation environment of a researcher are designed . designed and developed functional components of knowledge-oriented information system developing research and development workstation environment , including functional models and software implementation of the software subsystem for creation and use of ontological knowledge base for research fellow publications , as part of personalized knowledge base of scientific researcher . research in modern conditions of e-science paradigm requires pooling scientific community and intensive exchange of research results that may be achieved through the use of scientific information systems . research and development workstation environment allows to solve problems of contructivisation and formalisation of knowledge representation , obtained during the research process and collective accomplices interaction ."}
{"title": "rule reasoning for legal norm validation of fstp facts", "abstract": "non-obviousness or inventive step is a general requirement for patentability in most patent law systems . an invention should be at an adequate distance beyond its prior art in order to be patented . this short paper provides an overview on a methodology proposed for legal norm validation of fstp facts using rule reasoning approach ."}
{"title": "proje : embedding projection for knowledge graph completion", "abstract": "with the large volume of new information created every day , determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners . to address this challenge , a number of knowledge graph completion methods have been developed using low-dimensional graph embeddings . although researchers continue to improve these models using an increasingly complex feature space , we show that simple changes in the architecture of the underlying model can outperform state-of-the-art models without the need for complex feature engineering . in this work , we present a shared variable neural network model called proje that fills-in missing information in a knowledge graph by learning joint embeddings of the knowledge graph 's entities and edges , and through subtle , but important , changes to the standard loss function . in doing so , proje has a parameter size that is smaller than 11 out of 15 existing methods while performing $ 37\\ % $ better than the current-best method on standard datasets . we also show , via a new fact checking task , that proje is capable of accurately determining the veracity of many declarative statements ."}
{"title": "dire n'est pas concevoir", "abstract": "the conceptual modelling built from text is rarely an ontology . as a matter of fact , such a conceptualization is corpus-dependent and does not offer the main properties we expect from ontology . furthermore , ontology extracted from text in general does not match ontology defined by expert using a formal language . it is not surprising since ontology is an extra-linguistic conceptualization whereas knowledge extracted from text is the concern of textual linguistics . incompleteness of text and using rhetorical figures , like ellipsis , modify the perception of the conceptualization we may have . ontological knowledge , which is necessary for text understanding , is not in general embedded into documents ."}
{"title": "scaling up greedy causal search for continuous variables", "abstract": "as standardly implemented in r or the tetrad program , causal search algorithms used most widely or effectively by scientists have severe dimensionality constraints that make them inappropriate for big data problems without sacrificing accuracy . however , implementation improvements are possible . we explore optimizations for the greedy equivalence search that allow search on 50,000-variable problems in 13 minutes for sparse models with 1000 samples on a four-processor , 16g laptop computer . we finish a problem with 1000 samples on 1,000,000 variables in 18 hours for sparse models on a supercomputer node at the pittsburgh supercomputing center with 40 processors and 384 g ram . the same algorithm can be applied to discrete data , with a slower discrete score , though the discrete implementation currently does not scale as well in our experiments ; we have managed to scale up to about 10,000 variables in sparse models with 1000 samples ."}
{"title": "an integrated framework for diagnosis and prognosis of hybrid systems", "abstract": "complex systems are naturally hybrid : their dynamic behavior is both continuous and discrete . for these systems , maintenance and repair are an increasing part of the total cost of final product . efficient diagnosis and prognosis techniques have to be adopted to detect , isolate and anticipate faults . this paper presents an original integrated theoretical framework for diagnosis and prognosis of hybrid systems . the formalism used for hybrid diagnosis is enriched in order to be able to follow the evolution of an aging law for each fault of the system . the paper presents a methodology for interleaving diagnosis and prognosis in a hybrid framework ."}
{"title": "safety-aware apprenticeship learning", "abstract": "apprenticeship learning ( al ) is a class of `` learning from demonstrations '' techniques where the reward function of a markov decision process ( mdp ) is unknown to the learning agent and the agent has to derive a good policy by observing an expert 's demonstrations . in this paper , we study the problem of how to make al algorithms inherently safe while still meeting its learning objective . we consider a setting where the unknown reward function is assumed to be a linear combination of a set of state features , and the safety property is specified in probabilistic computation tree logic ( pctl ) . by embedding probabilistic model checking inside al , we propose a novel counterexample-guided approach that can ensure both safety and performance of the learnt policy . we demonstrate the effectiveness of our approach on several challenging al scenarios where safety is essential ."}
{"title": "field geology with a wearable computer : 1st results of the cyborg astrobiologist system", "abstract": "we present results from the first geological field tests of the ` cyborg astrobiologist ' , which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist . the cyborg astrobiologist platform has thus far been used for testing and development of these algorithms and systems : robotic acquisition of quasi-mosaics of images , real-time image segmentation , and real-time determination of interesting points in the image mosaics . this work is more of a test of the whole system , rather than of any one part of the system . however , beyond the concept of the system itself , the uncommon map ( despite its simplicity ) is the main innovative part of the system . the uncommon map helps to determine interest-points in a context-free manner . overall , the hardware and software systems function reliably , and the computer-vision algorithms are adequate for the first field tests . in addition to the proof-of-concept aspect of these field tests , the main result of these field tests is the enumeration of those issues that we can improve in the future , including : dealing with structural shadow and microtexture , and also , controlling the camera 's zoom lens in an intelligent manner . nonetheless , despite these and other technical inadequacies , this cyborg astrobiologist system , consisting of a camera-equipped wearable-computer and its computer-vision algorithms , has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery , and then gathering more information about these interest points in an automated manner . we use these capabilities for autonomous guidance towards geological points-of-interest ."}
{"title": "rhog : a refinement-operator library for directed labeled graphs", "abstract": "this document provides the foundations behind the functionality provided by the $ \\rho $ g library ( https : //github.com/santiontanon/rhog ) , focusing on the basic operations the library provides : subsumption , refinement of directed labeled graphs , and distance/similarity assessment between directed labeled graphs . $ \\rho $ g development was initially supported by the national science foundation , by the eager grant iis-1551338 ."}
{"title": "graying the black box : understanding dqns", "abstract": "in recent years there is a growing interest in using deep representations for reinforcement learning . in this paper , we present a methodology and tools to analyze deep q-networks ( dqns ) in a non-blind matter . moreover , we propose a new model , the semi aggregated markov decision process ( samdp ) , and an algorithm that learns it automatically . the samdp model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work . using our tools we reveal that the features learned by dqns aggregate the state space in a hierarchical fashion , explaining its success . moreover , we are able to understand and describe the policies learned by dqns for three different atari2600 games and suggest ways to interpret , debug and optimize deep neural networks in reinforcement learning ."}
{"title": "refractor importance sampling", "abstract": "in this paper we introduce refractor importance sampling ( ris ) , an improvement to reduce error variance in bayesian network importance sampling propagation under evidential reasoning . we prove the existence of a collection of importance functions that are close to the optimal importance function under evidential reasoning . based on this theoretic result we derive the ris algorithm . ris approaches the optimal importance function by applying localized arc changes to minimize the divergence between the evidence-adjusted importance function and the optimal importance function . the validity and performance of ris is empirically tested with a large setof synthetic bayesian networks and two real-world networks ."}
{"title": "attribute value weighting in k-modes clustering", "abstract": "in this paper , the traditional k-modes clustering algorithm is extended by weighting attribute value matches in dissimilarity computation . the use of attribute value weighting technique makes it possible to generate clusters with stronger intra-similarities , and therefore achieve better clustering performance . experimental results on real life datasets show that these value weighting based k-modes algorithms are superior to the standard k-modes algorithm with respect to clustering accuracy ."}
{"title": "determinism in the certification of unsat proofs", "abstract": "the search for increased trustworthiness of sat solvers is very active and uses various methods . some of these methods obtain a proof from the provers then check it , normally by replicating the search based on the proof 's information . because the certification process involves another nontrivial proof search , the trust we can place in it is decreased . some attempts to amend this use certifiers which have been verified by proofs assistants such as isabelle/hol and coq . our approach is different because it is based on an extremely simplified certifier . this certifier enjoys a very high level of trust but is very inefficient . in this paper , we experiment with this approach and conclude that by placing some restrictions on the formats , one can mostly eliminate the need for search and in principle , can certify proofs of arbitrary size ."}
{"title": "explainable artificial intelligence : understanding , visualizing and interpreting deep learning models", "abstract": "with the availability of large databases and recent improvements in deep learning methodology , the performance of ai systems is reaching or even exceeding the human level on an increasing number of complex tasks . impressive examples of this development can be found in domains such as image classification , sentiment analysis , speech understanding or strategic game playing . however , because of their nested non-linear structure , these highly successful machine learning and artificial intelligence models are usually applied in a black box manner , i.e. , no information is provided about what exactly makes them arrive at their predictions . since this lack of transparency can be a major drawback , e.g. , in medical applications , the development of methods for visualizing , explaining and interpreting deep learning models has recently attracted increasing attention . this paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence . furthermore , it presents two approaches to explaining predictions of deep learning models , one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables . these methods are evaluated on three classification tasks ."}
{"title": "active learning for matching problems", "abstract": "effective learning of user preferences is critical to easing user burden in various types of matching problems . equally important is active query selection to further reduce the amount of preference information users must provide . we address the problem of active learning of user preferences for matching problems , introducing a novel method for determining probabilistic matchings , and developing several new active learning strategies that are sensitive to the specific matching objective . experiments with real-world data sets spanning diverse domains demonstrate that matching-sensitive active learning"}
{"title": "extending fo ( id ) with knowledge producing definitions : preliminary results", "abstract": "previous research into the relation between asp and classical logic has identified at least two different ways in which the former extends the latter . first , asp program typically contain sets of rules that can be naturally interpreted as inductive definitions , and the language fo ( id ) has shown that such inductive definitions can elegantly be added to classical logic in a modular way . second , there is of course also the well-known epistemic component of asp , which was mainly emphasized in the early papers on stable model semantics . to investigate whether this kind of knowledge can also , and in a similarly modular way , be added to classical logic , the language of ordered epistemic logic was presented in recent work . however , this logic views the epistemic component as entirely separate from the inductive definition component , thus ignoring any possible interplay between the two . in this paper , we present a language that extends the inductive definition construct found in fo ( id ) with an epistemic component , making such interplay possible . the eventual goal of this work is to discover whether it is really appropriate to view the epistemic component and the inductive definition component of asp as two separate extensions of classical logic , or whether there is also something of importance in the combination of the two ."}
{"title": "probability aggregates in probability answer set programming", "abstract": "probability answer set programming is a declarative programming that has been shown effective for representing and reasoning about a variety of probability reasoning tasks . however , the lack of probability aggregates , e.g . { \\em expected values } , in the language of disjunctive hybrid probability logic programs ( dhpp ) disallows the natural and concise representation of many interesting problems . in this paper , we extend dhpp to allow arbitrary probability aggregates . we introduce two types of probability aggregates ; a type that computes the expected value of a classical aggregate , e.g. , the expected value of the minimum , and a type that computes the probability of a classical aggregate , e.g , the probability of sum of values . in addition , we define a probability answer set semantics for dhpp with arbitrary probability aggregates including monotone , antimonotone , and nonmonotone probability aggregates . we show that the proposed probability answer set semantics of dhpp subsumes both the original probability answer set semantics of dhpp and the classical answer set semantics of classical disjunctive logic programs with classical aggregates , and consequently subsumes the classical answer set semantics of the original disjunctive logic programs . we show that the proposed probability answer sets of dhpp with probability aggregates are minimal probability models and hence incomparable , which is an important property for nonmonotonic probability reasoning ."}
{"title": "sharing deep generative representation for perceived image reconstruction from human brain activity", "abstract": "decoding human brain activities via functional magnetic resonance imaging ( fmri ) has gained increasing attention in recent years . while encouraging results have been reported in brain states classification tasks , reconstructing the details of human visual experience still remains difficult . two main challenges that hinder the development of effective models are the perplexing fmri measurement noise and the high dimensionality of limited data instances . existing methods generally suffer from one or both of these issues and yield dissatisfactory results . in this paper , we tackle this problem by casting the reconstruction of visual stimulus as the bayesian inference of missing view in a multiview latent variable model . sharing a common latent representation , our joint generative model of external stimulus and brain response is not only `` deep '' in extracting nonlinear features from visual images , but also powerful in capturing correlations among voxel activities of fmri recordings . the nonlinearity and deep structure endow our model with strong representation ability , while the correlations of voxel activities are critical for suppressing noise and improving prediction . we devise an efficient variational bayesian method to infer the latent variables and the model parameters . to further improve the reconstruction accuracy , the latent representations of testing instances are enforced to be close to that of their neighbours from the training set via posterior regularization . experiments on three fmri recording datasets demonstrate that our approach can more accurately reconstruct visual stimuli ."}
{"title": "some properties of joint probability distributions", "abstract": "several artificial intelligence schemes for reasoning under uncertainty explore either explicitly or implicitly asymmetries among probabilities of various states of their uncertain domain models . even though the correct working of these schemes is practically contingent upon the existence of a small number of probable states , no formal justification has been proposed of why this should be the case . this paper attempts to fill this apparent gap by studying asymmetries among probabilities of various states of uncertain models . by rewriting the joint probability distribution over a model 's variables into a product of individual variables ' prior and conditional probability distributions , and applying central limit theorem to this product , we can demonstrate that the probabilities of individual states of the model can be expected to be drawn from highly skewed , log-normal distributions . with sufficient asymmetry in individual prior and conditional probability distributions , a small fraction of states can be expected to cover a large portion of the total probability space with the remaining states having practically negligible probability . theoretical discussion is supplemented by simulation results and an illustrative real-world example ."}
{"title": "study on feature subspace of archetypal emotions for speech emotion recognition", "abstract": "feature subspace selection is an important part in speech emotion recognition . most of the studies are devoted to finding a feature subspace for representing all emotions . however , some studies have indicated that the features associated with different emotions are not exactly the same . hence , traditional methods may fail to distinguish some of the emotions with just one global feature subspace . in this work , we propose a new divide and conquer idea to solve the problem . first , the feature subspaces are constructed for all the combinations of every two different emotions ( emotion-pair ) . bi-classifiers are then trained on these feature subspaces respectively . the final emotion recognition result is derived by the voting and competition method . experimental results demonstrate that the proposed method can get better results than the traditional multi-classification method ."}
{"title": "an evaluation of naive bayesian anti-spam filtering", "abstract": "it has recently been argued that a naive bayesian classifier can be used to filter unsolicited bulk e-mail ( `` spam '' ) . we conduct a thorough evaluation of this proposal on a corpus that we make publicly available , contributing towards standard benchmarks . at the same time we investigate the effect of attribute-set size , training-corpus size , lemmatization , and stop-lists on the filter 's performance , issues that had not been previously explored . after introducing appropriate cost-sensitive evaluation measures , we reach the conclusion that additional safety nets are needed for the naive bayesian anti-spam filter to be viable in practice ."}
{"title": "unsupervised neural-symbolic integration", "abstract": "symbolic has been long considered as a language of human intelligence while neural networks have advantages of robust computation and dealing with noisy data . the integration of neural-symbolic can offer better learning and reasoning while providing a means for interpretability through the representation of symbolic knowledge . although previous works focus intensively on supervised feedforward neural networks , little has been done for the unsupervised counterparts . in this paper we show how to integrate symbolic knowledge into unsupervised neural networks . we exemplify our approach with knowledge in different forms , including propositional logic for dna promoter prediction and first-order logic for understanding family relationship ."}
{"title": "a survey on resilient machine learning", "abstract": "machine learning based system are increasingly being used for sensitive tasks such as security surveillance , guiding autonomous vehicle , taking investment decisions , detecting and blocking network intrusion and malware etc . however , recent research has shown that machine learning models are venerable to attacks by adversaries at all phases of machine learning ( eg , training data collection , training , operation ) . all model classes of machine learning systems can be misled by providing carefully crafted inputs making them wrongly classify inputs . maliciously created input samples can affect the learning process of a ml system by either slowing down the learning process , or affecting the performance of the learned mode , or causing the system make error ( s ) only in attacker 's planned scenario . because of these developments , understanding security of machine learning algorithms and systems is emerging as an important research area among computer security and machine learning researchers and practitioners . we present a survey of this emerging area in machine learning ."}
{"title": "using discourse signals for robust instructor intervention prediction", "abstract": "we tackle the prediction of instructor intervention in student posts from discussion forums in massive open online courses ( moocs ) . our key finding is that using automatically obtained discourse relations improves the prediction of when instructors intervene in student discussions , when compared with a state-of-the-art , feature-rich baseline . our supervised classifier makes use of an automatic discourse parser which outputs penn discourse treebank ( pdtb ) tags that represent in-post discourse features . we show pdtb relation-based features increase the robustness of the classifier and complement baseline features in recalling more diverse instructor intervention patterns . in comprehensive experiments over 14 mooc offerings from several disciplines , the pdtb discourse features improve performance on average . the resultant models are less dependent on domain-specific vocabulary , allowing them to better generalize to new courses ."}
{"title": "fast dempster-shafer clustering using a neural network structure", "abstract": "in this article we study a problem within dempster-shafer theory where 2**n - 1 pieces of evidence are clustered by a neural structure into n clusters . the clustering is done by minimizing a metaconflict function . previously we developed a method based on iterative optimization . however , for large scale problems we need a method with lower computational complexity . the neural structure was found to be effective and much faster than iterative optimization for larger problems . while the growth in metaconflict was faster for the neural structure compared with iterative optimization in medium sized problems , the metaconflict per cluster and evidence was moderate . the neural structure was able to find a global minimum over ten runs for problem sizes up to six clusters ."}
{"title": "causal interfaces", "abstract": "the interaction of two binary variables , assumed to be empirical observations , has three degrees of freedom when expressed as a matrix of frequencies . usually , the size of causal influence of one variable on the other is calculated as a single value , as increase in recovery rate for a medical treatment , for example . we examine what is lost in this simplification , and propose using two interface constants to represent positive and negative implications separately . given certain assumptions about non-causal outcomes , the set of resulting epistemologies is a continuum . we derive a variety of particular measures and contrast them with the one-dimensional index ."}
{"title": "tight regret bounds for stochastic combinatorial semi-bandits", "abstract": "a stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to constraints , and then observes stochastic weights of these items and receives their sum as a payoff . in this paper , we close the problem of computationally and sample efficient learning in stochastic combinatorial semi-bandits . in particular , we analyze a ucb-like algorithm for solving the problem , which is known to be computationally efficient ; and prove $ o ( k l ( 1 / \\delta ) \\log n ) $ and $ o ( \\sqrt { k l n \\log n } ) $ upper bounds on its $ n $ -step regret , where $ l $ is the number of ground items , $ k $ is the maximum number of chosen items , and $ \\delta $ is the gap between the expected returns of the optimal and best suboptimal solutions . the gap-dependent bound is tight up to a constant factor and the gap-free bound is tight up to a polylogarithmic factor ."}
{"title": "data mining and privacy in public sector using intelligent agents ( discussion paper )", "abstract": "the public sector comprises government agencies , ministries , education institutions , health providers and other types of government , commercial and not-for-profit organisations . unlike commercial enterprises , this environment is highly heterogeneous in all aspects . this forms a complex network which is not always optimised . a lack of optimisation and communication hinders information sharing between the network nodes limiting the flow of information . another limiting aspect is privacy of personal information and security of operations of some nodes or segments of the network . attempts to reorganise the network or improve communications to make more information available for sharing and analysis may be hindered or completely halted by public concerns over privacy , political agendas , social and technological barriers . this paper discusses a technical solution for information sharing while addressing the privacy concerns with no need for reorganisation of the existing public sector infrastructure . the solution is based on imposing an additional layer of intelligent software agents and knowledge bases for data mining and analysis ."}
{"title": "speed optimization in unplanned traffic using bio-inspired computing and population knowledge base", "abstract": "bio-inspired algorithms on road traffic congestion and safety is a very promising research problem . searching for an efficient optimization method to increase the degree of speed optimization and thereby increasing the traffic flow in an unplanned zone is a widely concerning issue . however , there has been a limited research effort on the optimization of the lane usage with speed optimization . the main objective of this article is to find avenues or techniques in a novel way to solve the problem optimally using the knowledge from analysis of speeds of vehicles , which , in turn will act as a guide for design of lanes optimally to provide better optimized traffic . the accident factors adjust the base model estimates for individual geometric design element dimensions and for traffic control features . the application of these algorithms in partially modified form in accordance of this novel speed optimization technique in an unplanned traffic analysis technique is applied to the proposed design and speed optimization plan . the experimental results based on real life data are quite encouraging ."}
{"title": "interactive debugging of asp programs", "abstract": "broad application of answer set programming ( asp ) for declarative problem solving requires the development of tools supporting the coding process . program debugging is one of the crucial activities within this process . recently suggested asp debugging approaches allow efficient computation of possible explanations of a fault . however , even for a small program a debugger might return a large number of possible explanations and selection of the correct one must be done manually . in this paper we present an interactive query-based asp debugging method which extends previous approaches and finds a preferred explanation by means of observations . the system queries a programmer whether a set of ground atoms must be true in all ( cautiously ) or some ( bravely ) answer sets of the program . since some queries can be more informative than the others , we discuss query selection strategies which , given user 's preferences for an explanation , can find the best query . that is , the query an answer of which reduces the overall number of queries required for the identification of a preferred explanation ."}
{"title": "combinatorial structure of the deterministic seriation method with multiple subset solutions", "abstract": "seriation methods order a set of descriptions given some criterion ( e.g. , unimodality or minimum distance between similarity scores ) . seriation is thus inherently a problem of finding the optimal solution among a set of permutations of objects . in this short technical note , we review the combinatorial structure of the classical seriation problem , which seeks a single solution out of a set of objects . we then extend those results to the iterative frequency seriation approach introduced by lipo ( 1997 ) , which finds optimal subsets of objects which each satisfy the unimodality criterion within each subset . the number of possible solutions across multiple solution subsets is larger than $ n ! $ , which underscores the need to find new algorithms and heuristics to assist in the deterministic frequency seriation problem ."}
{"title": "constructive decision theory", "abstract": "in most contemporary approaches to decision making , a decision problem is described by a sets of states and set of outcomes , and a rich set of acts , which are functions from states to outcomes over which the decision maker ( dm ) has preferences . most interesting decision problems , however , do not come with a state space and an outcome space . indeed , in complex problems it is often far from clear what the state and outcome spaces would be . we present an alternative foundation for decision making , in which the primitive objects of choice are syntactic programs . a representation theorem is proved in the spirit of standard representation theorems , showing that if the dm 's preference relation on objects of choice satisfies appropriate axioms , then there exist a set s of states , a set o of outcomes , a way of interpreting the objects of choice as functions from s to o , a probability on s , and a utility function on o , such that the dm prefers choice a to choice b if and only if the expected utility of a is higher than that of b. thus , the state space and outcome space are subjective , just like the probability and utility ; they are not part of the description of the problem . in principle , a modeler can test for seu behavior without having access to states or outcomes . we illustrate the power of our approach by showing that it can capture decision makers who are subject to framing effects ."}
{"title": "indian regional movie dataset for recommender systems", "abstract": "indian regional movie dataset is the first database of regional indian movies , users and their ratings . it consists of movies belonging to 18 different indian regional languages and metadata of users with varying demographics . through this dataset , the diversity of indian regional cinema and its huge viewership is captured . we analyze the dataset that contains roughly 10k ratings of 919 users and 2,851 movies using some supervised and unsupervised collaborative filtering techniques like probabilistic matrix factorization , matrix completion , blind compressed sensing etc . the dataset consists of metadata information of users like age , occupation , home state and known languages . it also consists of metadata of movies like genre , language , release year and cast . india has a wide base of viewers which is evident by the large number of movies released every year and the huge box-office revenue . this dataset can be used for designing recommendation systems for indian users and regional movies , which do not , yet , exist . the dataset can be downloaded from \\href { https : //goo.gl/emtpv6 } { https : //goo.gl/emtpv6 } ."}
{"title": "frustratingly short attention spans in neural language modeling", "abstract": "neural language models predict the next token using a latent representation of the immediate token history . recently , various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed . for predicting the next token , these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies . however , conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step . this vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history . in this paper , we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory , as well as for encoding the next-word distribution . this model outperforms existing memory-augmented neural language models on two corpora . yet , we found that our method mainly utilizes a memory of the five most recent output representations . this led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models ."}
{"title": "ordering-based representations of rational inference", "abstract": "rational inference relations were introduced by lehmann and magidor as the ideal systems for drawing conclusions from a conditional base . however , there has been no simple characterization of these relations , other than its original representation by preferential models . in this paper , we shall characterize them with a class of total preorders of formulas by improving and extending gardenfors and makinson 's results for expectation inference relations . a second representation is application-oriented and is obtained by considering a class of consequence operators that grade sets of defaults according to our reliance on them . the finitary fragment of this class of consequence operators has been employed by recent default logic formalisms based on maxiconsistency ."}
{"title": "maximin safety : when failing to lose is preferable to trying to win", "abstract": "we present a new decision rule , \\emph { maximin safety } , that seeks to maintain a large margin from the worst outcome , in much the same way minimax regret seeks to minimize distance from the best . we argue that maximin safety is valuable both descriptively and normatively . descriptively , maximin safety explains the well-known \\emph { decoy effect } , in which the introduction of a dominated option changes preferences among the other options . normatively , we provide an axiomatization that characterizes preferences induced by maximin safety , and show that maximin safety shares much of the same behavioral basis with minimax regret ."}
{"title": "elicitation strategies for fuzzy constraint problems with missing preferences : algorithms and experimental studies", "abstract": "fuzzy constraints are a popular approach to handle preferences and over-constrained problems in scenarios where one needs to be cautious , such as in medical or space applications . we consider here fuzzy constraint problems where some of the preferences may be missing . this models , for example , settings where agents are distributed and have privacy issues , or where there is an ongoing preference elicitation process . in this setting , we study how to find a solution which is optimal irrespective of the missing preferences . in the process of finding such a solution , we may elicit preferences from the user if necessary . however , our goal is to ask the user as little as possible . we define a combined solving and preference elicitation scheme with a large number of different instantiations , each corresponding to a concrete algorithm which we compare experimentally . we compute both the number of elicited preferences and the `` user effort '' , which may be larger , as it contains all the preference values the user has to compute to be able to respond to the elicitation requests . while the number of elicited preferences is important when the concern is to communicate as little information as possible , the user effort measures also the hidden work the user has to do to be able to communicate the elicited preferences . our experimental results show that some of our algorithms are very good at finding a necessarily optimal solution while asking the user for only a very small fraction of the missing preferences . the user effort is also very small for the best algorithms . finally , we test these algorithms on hard constraint problems with possibly missing constraints , where the aim is to find feasible solutions irrespective of the missing constraints ."}
{"title": "challenges on probabilistic modeling for evolving networks", "abstract": "with the emerging of new networks , such as wireless sensor networks , vehicle networks , p2p networks , cloud computing , mobile internet , or social networks , the network dynamics and complexity expands from system design , hardware , software , protocols , structures , integration , evolution , application , even to business goals . thus the dynamics and uncertainty are unavoidable characteristics , which come from the regular network evolution and unexpected hardware defects , unavoidable software errors , incomplete management information and dependency relationship between the entities among the emerging complex networks . due to the complexity of emerging networks , it is not always possible to build precise models in modeling and optimization ( local and global ) for networks . this paper presents a survey on probabilistic modeling for evolving networks and identifies the new challenges which emerge on the probabilistic models and optimization strategies in the potential application areas of network performance , network management and network security for evolving networks ."}
{"title": "knowledge as a teacher : knowledge-guided structural attention networks", "abstract": "natural language understanding ( nlu ) is a core component of a spoken dialogue system . recently recurrent neural networks ( rnn ) obtained strong results on nlu due to their superior ability of preserving sequential information over time . traditionally , the nlu module tags semantic slots for utterances considering their flat structures , as the underlying rnn structure is a linear chain . however , natural language exhibits linguistic properties that provide rich , structured information for better understanding . this paper introduces a novel model , knowledge-guided structural attention networks ( k-san ) , a generalization of rnn to additionally incorporate non-flat network topologies guided by prior knowledge . there are two characteristics : 1 ) important substructures can be captured from small training data , allowing the model to generalize to previously unseen test data ; 2 ) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences , so that the understanding performance can be improved . the experiments on the benchmark air travel information system ( atis ) data show that the proposed k-san architecture can effectively extract salient knowledge from substructures with an attention mechanism , and outperform the performance of the state-of-the-art neural network based frameworks ."}
{"title": "proceedings of the twenty-sixth conference on uncertainty in artificial intelligence ( 2010 )", "abstract": "this is the proceedings of the twenty-sixth conference on uncertainty in artificial intelligence , which was held on catalina island , ca , july 8 - 11 2010 ."}
{"title": "utility elicitation as a classification problem", "abstract": "we investigate the application of classification techniques to utility elicitation . in a decision problem , two sets of parameters must generally be elicited : the probabilities and the utilities . while the prior and conditional probabilities in the model do not change from user to user , the utility models do . thus it is necessary to elicit a utility model separately for each new user . elicitation is long and tedious , particularly if the outcome space is large and not decomposable . there are two common approaches to utility function elicitation . the first is to base the determination of the users utility function solely on elicitation of qualitative preferences.the second makes assumptions about the form and decomposability of the utility function.here we take a different approach : we attempt to identify the new users utility function based on classification relative to a database of previously collected utility functions . we do this by identifying clusters of utility functions that minimize an appropriate distance measure . having identified the clusters , we develop a classification scheme that requires many fewer and simpler assessments than full utility elicitation and is more robust than utility elicitation based solely on preferences . we have tested our algorithm on a small database of utility functions in a prenatal diagnosis domain and the results are quite promising ."}
{"title": "relaxed survey propagation for the weighted maximum satisfiability problem", "abstract": "the survey propagation ( sp ) algorithm has been shown to work well on large instances of the random 3-sat problem near its phase transition . it was shown that sp estimates marginals over covers that represent clusters of solutions . the sp-y algorithm generalizes sp to work on the maximum satisfiability ( max-sat ) problem , but the cover interpretation of sp does not generalize to sp-y . in this paper , we formulate the relaxed survey propagation ( rsp ) algorithm , which extends the sp algorithm to apply to the weighted max-sat problem . we show that rsp has an interpretation of estimating marginals over covers violating a set of clauses with minimal weight . this naturally generalizes the cover interpretation of sp . empirically , we show that rsp outperforms sp-y and other state-of-the-art max-sat solvers on random max-sat instances . rsp also outperforms state-of-the-art weighted max-sat solvers on random weighted max-sat instances ."}
{"title": "mazebase : a sandbox for learning from games", "abstract": "this paper introduces mazebase : an environment for simple 2d games , designed as a sandbox for machine learning approaches to reasoning and planning . within it , we create 10 simple games embodying a range of algorithmic tasks ( e.g . if-then statements or set negation ) . a variety of neural models ( fully connected , convolutional network , memory network ) are deployed via reinforcement learning on these games , with and without a procedurally generated curriculum . despite the tasks ' simplicity , the performance of the models is far from optimal , suggesting directions for future development . we also demonstrate the versatility of mazebase by using it to emulate small combat scenarios from starcraft . models trained on the mazebase version can be directly applied to starcraft , where they consistently beat the in-game ai ."}
{"title": "optimal choice : new machine learning problem and its solution", "abstract": "the task of learning to pick a single preferred example out a finite set of examples , an `` optimal choice problem '' , is a supervised machine learning problem with complex , structured input . problems of optimal choice emerge often in various practical applications . we formalize the problem , show that it does not satisfy the assumptions of statistical learning theory , yet it can be solved efficiently in some cases . we propose two approaches to solve the problem . both of them reach good solutions on real life data from a signal processing application ."}
{"title": "task interaction in an htn planner", "abstract": "hierarchical task network ( htn ) planning uses task decomposition to plan for an executable sequence of actions as a solution to a problem . in order to reason effectively , an htn planner needs expressive domain knowledge . for instance , a simplified htn planning system such as jshop2 uses such expressivity and avoids some task interactions due to the increased complexity of the planning process . we address the possibility of simplifying the domain representation needed for an htn planner to find good solutions , especially in real-world domains describing home and building automation environments . we extend the jshop2 planner to reason about task interaction that happens when task 's effects are already achieved by other tasks . the planner then prunes some of the redundant searches that can occur due to the planning process 's interleaving nature . we evaluate the original and our improved planner on two benchmark domains . we show that our planner behaves better by using simplified domain knowledge and outperforms jshop2 in a number of relevant cases ."}
{"title": "ontology module extraction via datalog reasoning", "abstract": "module extraction - the task of computing a ( preferably small ) fragment m of an ontology t that preserves entailments over a signature s - has found many applications in recent years . extracting modules of minimal size is , however , computationally hard , and often algorithmically infeasible . thus , practical techniques are based on approximations , where m provably captures the relevant entailments , but is not guaranteed to be minimal . existing approximations , however , ensure that m preserves all second-order entailments of t w.r.t . s , which is stronger than is required in many applications , and may lead to large modules in practice . in this paper we propose a novel approach in which module extraction is reduced to a reasoning problem in datalog . our approach not only generalises existing approximations in an elegant way , but it can also be tailored to preserve only specific kinds of entailments , which allows us to extract significantly smaller modules . an evaluation on widely-used ontologies has shown very encouraging results ."}
{"title": "emotion : appraisal-coping model for the `` cascades '' problem", "abstract": "modelling emotion has become a challenge nowadays . therefore , several models have been produced in order to express human emotional activity . however , only a few of them are currently able to express the close relationship existing between emotion and cognition . an appraisal-coping model is presented here , with the aim to simulate the emotional impact caused by the evaluation of a particular situation ( appraisal ) , along with the consequent cognitive reaction intended to face the situation ( coping ) . this model is applied to the `` cascades '' problem , a small arithmetical exercise designed for ten-year-old pupils . the goal is to create a model corresponding to a child 's behaviour when solving the problem using his own strategies ."}
{"title": "what is an optimal diagnosis ?", "abstract": "within diagnostic reasoning there have been a number of proposed definitions of a diagnosis , and thus of the most likely diagnosis , including most probable posterior hypothesis , most probable interpretation , most probable covering hypothesis , etc . most of these approaches assume that the most likely diagnosis must be computed , and that a definition of what should be computed can be made a priori , independent of what the diagnosis is used for . we argue that the diagnostic problem , as currently posed , is incomplete : it does not consider how the diagnosis is to be used , or the utility associated with the treatment of the abnormalities . in this paper we analyze several well-known definitions of diagnosis , showing that the different definitions of the most likely diagnosis have different qualitative meanings , even given the same input data . we argue that the most appropriate definition of ( optimal ) diagnosis needs to take into account the utility of outcomes and what the diagnosis is used for ."}
{"title": "using answer set programming for pattern mining", "abstract": "serial pattern mining consists in extracting the frequent sequential patterns from a unique sequence of itemsets . this paper explores the ability of a declarative language , such as answer set programming ( asp ) , to solve this issue efficiently . we propose several asp implementations of the frequent sequential pattern mining task : a non-incremental and an incremental resolution . the results show that the incremental resolution is more efficient than the non-incremental one , but both asp programs are less efficient than dedicated algorithms . nonetheless , this approach can be seen as a first step toward a generic framework for sequential pattern mining with constraints ."}
{"title": "on reasoning with rdf statements about statements using singleton property triples", "abstract": "the singleton property ( sp ) approach has been proposed for representing and querying metadata about rdf triples such as provenance , time , location , and evidence . in this approach , one singleton property is created to uniquely represent a relationship in a particular context , and in general , generates a large property hierarchy in the schema . it has become the subject of important questions from semantic web practitioners . can an existing reasoner recognize the singleton property triples ? and how ? if the singleton property triples describe a data triple , then how can a reasoner infer this data triple from the singleton property triples ? or would the large property hierarchy affect the reasoners in some way ? we address these questions in this paper and present our study about the reasoning aspects of the singleton properties . we propose a simple mechanism to enable existing reasoners to recognize the singleton property triples , as well as to infer the data triples described by the singleton property triples . we evaluate the effect of the singleton property triples in the reasoning processes by comparing the performance on rdf datasets with and without singleton properties . our evaluation uses as benchmark the lubm datasets and the lubm-sp datasets derived from lubm with temporal information added through singleton properties ."}
{"title": "arguments using ontological and causal knowledge", "abstract": "we investigate an approach to reasoning about causes through argumentation . we consider a causal model for a physical system , and look for arguments about facts . some arguments are meant to provide explanations of facts whereas some challenge these explanations and so on . at the root of argumentation here , are causal links ( { a_1 , ... , a_n } causes b ) and ontological links ( o_1 is_a o_2 ) . we present a system that provides a candidate explanation ( { a_1 , ... , a_n } explains { b_1 , ... , b_m } ) by resorting to an underlying causal link substantiated with appropriate ontological links . argumentation is then at work from these various explaining links . a case study is developed : a severe storm xynthia that devastated part of france in 2010 , with an unaccountably high number of casualties ."}
{"title": "belief change with noisy sensing in the situation calculus", "abstract": "situation calculus has been applied widely in artificial intelligence to model and reason about actions and changes in dynamic systems . since actions carried out by agents will cause constant changes of the agents ' beliefs , how to manage these changes is a very important issue . shapiro et al . [ 22 ] is one of the studies that considered this issue . however , in this framework , the problem of noisy sensing , which often presents in real-world applications , is not considered . as a consequence , noisy sensing actions in this framework will lead to an agent facing inconsistent situation and subsequently the agent can not proceed further . in this paper , we investigate how noisy sensing actions can be handled in iterated belief change within the situation calculus formalism . we extend the framework proposed in [ 22 ] with the capability of managing noisy sensings . we demonstrate that an agent can still detect the actual situation when the ratio of noisy sensing actions vs. accurate sensing actions is limited . we prove that our framework subsumes the iterated belief change strategy in [ 22 ] when all sensing actions are accurate . furthermore , we prove that our framework can adequately handle belief introspection , mistaken beliefs , belief revision and belief update even with noisy sensing , as done in [ 22 ] with accurate sensing actions only ."}
{"title": "a randomized approximation algorithm of logic sampling", "abstract": "in recent years , researchers in decision analysis and artificial intelligence ( ai ) have used bayesian belief networks to build models of expert opinion . using standard methods drawn from the theory of computational complexity , workers in the field have shown that the problem of exact probabilistic inference on belief networks almost certainly requires exponential computation in the worst ease [ 3 ] . we have previously described a randomized approximation scheme , called bn-ras , for computation on belief networks [ 1 , 2 , 4 ] . we gave precise analytic bounds on the convergence of bn-ras and showed how to trade running time for accuracy in the evaluation of posterior marginal probabilities . we now extend our previous results and demonstrate the generality of our framework by applying similar mathematical techniques to the analysis of convergence for logic sampling [ 7 ] , an alternative simulation algorithm for probabilistic inference ."}
{"title": "agent based tools for modeling and simulation of self-organization in peer-to-peer , ad-hoc and other complex networks", "abstract": "agent-based modeling and simulation tools provide a mature platform for development of complex simulations . they however , have not been applied much in the domain of mainstream modeling and simulation of computer networks . in this article , we evaluate how and if these tools can offer any value-addition in the modeling & simulation of complex networks such as pervasive computing , large-scale peer-to-peer systems , and networks involving considerable environment and human/animal/habitat interaction . specifically , we demonstrate the effectiveness of netlogo - a tool that has been widely used in the area of agent-based social simulation ."}
{"title": "the effect of biased communications on both trusting and suspicious voters", "abstract": "in recent studies of political decision-making , apparently anomalous behavior has been observed on the part of voters , in which negative information about a candidate strengthens , rather than weakens , a prior positive opinion about the candidate . this behavior appears to run counter to rational models of decision making , and it is sometimes interpreted as evidence of non-rational `` motivated reasoning '' . we consider scenarios in which this effect arises in a model of rational decision making which includes the possibility of deceptive information . in particular , we will consider a model in which there are two classes of voters , which we will call trusting voters and suspicious voters , and two types of information sources , which we will call unbiased sources and biased sources . in our model , new data about a candidate can be efficiently incorporated by a trusting voter , and anomalous updates are impossible ; however , anomalous updates can be made by suspicious voters , if the information source mistakenly plans for an audience of trusting voters , and if the partisan goals of the information source are known by the suspicious voter to be `` opposite '' to his own . our model is based on a formalism introduced by the artificial intelligence community called `` multi-agent influence diagrams '' , which generalize bayesian networks to settings involving multiple agents with distinct goals ."}
{"title": "a heuristic search algorithm for solving first-order mdps", "abstract": "we present a heuristic search algorithm for solving first-order mdps ( fomdps ) . our approach combines first-order state abstraction that avoids evaluating states individually , and heuristic search that avoids evaluating all states . firstly , we apply state abstraction directly on the fomdp avoiding propositionalization . such kind of abstraction is referred to as firstorder state abstraction . secondly , guided by an admissible heuristic , the search is restricted only to those states that are reachable from the initial state . we demonstrate the usefullness of the above techniques for solving fomdps on a system , referred to as fcplanner , that entered the probabilistic track of the international planning competition ( ipc'2004 ) ."}
{"title": "elementos de ingenier\u00eda de explotaci\u00f3n de la informaci\u00f3n aplicados a la investigaci\u00f3n tributaria fiscal", "abstract": "by introducing elements of information mining to tax analysis , by means of data mining software and advanced computational concepts of artificial intelligence , the problem of tax evader 's crime against public property has been addressed . through an empirical approach from a hypothetical case of use , induction algorithms , neural networks and bayesian networks are applied to determine the feasibility of its heuristic application by the tax public administrator . different strategies are explored to facilitate the work of local and regional federal tax inspectors , considering their limited computational capabilities , but equally effective for those social scientist committed to handcrafting tax research . -- -- - apresentando a introdu\\c { c } \\~ao de elementos de explora\\c { c } \\~ao de informa\\c { c } \\~oes para an\\'alise fiscal , por meio de software de minera\\c { c } \\~ao de dados e conceitos avan\\c { c } ados computacionais de intelig\\^encia artificial , foi abordado o problema do crime de sonegador fiscal contra o patrim\\^onio p\\'ublico . atrav\\'es de uma abordagem emp\\'irica a partir de um caso hipot\\'etico de uso , os algoritmos de indu\\c { c } \\~ao , redes neurais e redes bayesianas s\\~ao aplicados para determinar a viabilidade de sua aplica\\c { c } \\~ao heur\\'istica pelo administrador p\\'ublico tribut\\'ario . diferentes estrat\\'egias s\\~ao exploradas para facilitar o trabalho dos inspectores tribut\\'arios federais locais e regionais , tendo em conta as suas capacidades computacionais limitados , mas igualmente eficaz para aqueles cientista social comprometido com a investiga\\c { c } \\~ao fiscal ."}
{"title": "decentralized constraint satisfaction", "abstract": "we show that several important resource allocation problems in wireless networks fit within the common framework of constraint satisfaction problems ( csps ) . inspired by the requirements of these applications , where variables are located at distinct network devices that may not be able to communicate but may interfere , we define natural criteria that a csp solver must possess in order to be practical . we term these algorithms decentralized csp solvers . the best known csp solvers were designed for centralized problems and do not meet these criteria . we introduce a stochastic decentralized csp solver and prove that it will find a solution in almost surely finite time , should one exist , also showing it has many practically desirable properties . we benchmark the algorithm 's performance on a well-studied class of csps , random k-sat , illustrating that the time the algorithm takes to find a satisfying assignment is competitive with stochastic centralized solvers on problems with order a thousand variables despite its decentralized nature . we demonstrate the solver 's practical utility for the problems that motivated its introduction by using it to find a non-interfering channel allocation for a network formed from data from downtown manhattan ."}
{"title": "the thermodynamic cost of fast thought", "abstract": "after more than sixty years , shannon 's research [ 1-3 ] continues to raise fundamental questions , such as the one formulated by luce [ 4,5 ] , which is still unanswered : `` why is information theory not very applicable to psychological problems , despite apparent similarities of concepts ? '' on this topic , pinker [ 6 ] , one of the foremost defenders of the computational theory of mind [ 6 ] , has argued that thought is simply a type of computation , and that the gap between human cognition and computational models may be illusory . in this context , in his latest book , titled thinking fast and slow [ 8 ] , kahneman [ 7,8 ] provides further theoretical interpretation by differentiating the two assumed systems of the cognitive functioning of the human mind . he calls them intuition ( system 1 ) determined to be an associative ( automatic , fast and perceptual ) machine , and reasoning ( system 2 ) required to be voluntary and to operate logical- deductively . in this paper , we propose an ansatz inspired by ausubel 's learning theory for investigating , from the constructivist perspective [ 9-12 ] , information processing in the working memory of cognizers . specifically , a thought experiment is performed utilizing the mind of a dual-natured creature known as maxwell 's demon : a tiny `` man-machine '' solely equipped with the characteristics of system 1 , which prevents it from reasoning . the calculation presented here shows that [ ... ] . this result indicates that when the system 2 is shut down , both an intelligent being , as well as a binary machine , incur the same energy cost per unit of information processed , which mathematically proves the computational attribute of the system 1 , as kahneman [ 7,8 ] theorized . this finding links information theory to human psychological features and opens a new path toward the conception of a multi-bit reasoning machine ."}
{"title": "generalized thompson sampling for contextual bandits", "abstract": "thompson sampling , one of the oldest heuristics for solving multi-armed bandits , has recently been shown to demonstrate state-of-the-art performance . the empirical success has led to great interests in theoretical understanding of this heuristic . in this paper , we approach this problem in a way very different from existing efforts . in particular , motivated by the connection between thompson sampling and exponentiated updates , we propose a new family of algorithms called generalized thompson sampling in the expert-learning framework , which includes thompson sampling as a special case . similar to most expert-learning algorithms , generalized thompson sampling uses a loss function to adjust the experts ' weights . general regret bounds are derived , which are also instantiated to two important loss functions : square loss and logarithmic loss . in contrast to existing bounds , our results apply to quite general contextual bandits . more importantly , they quantify the effect of the `` prior '' distribution on the regret bounds ."}
{"title": "join-graph propagation algorithms", "abstract": "the paper investigates parameterized approximate message-passing schemes that are based on bounded inference and are inspired by pearl 's belief propagation algorithm ( bp ) . we start with the bounded inference mini-clustering algorithm and then move to the iterative scheme called iterative join-graph propagation ( ijgp ) , that combines both iteration and bounded inference . algorithm ijgp belongs to the class of generalized belief propagation algorithms , a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini-clustering and belief propagation , as well as a number of other state-of-the-art algorithms on several classes of networks . we also provide insight into the accuracy of iterative bp and ijgp by relating these algorithms to well known classes of constraint propagation schemes ."}
{"title": "solving wcsp by extraction of minimal unsatisfiable cores", "abstract": "usual techniques to solve wcsp are based on cost transfer operations coupled with a branch and bound algorithm . in this paper , we focus on an approach integrating extraction and relaxation of minimal unsatisfiable cores in order to solve this problem . we decline our approach in two ways : an incomplete , greedy , algorithm and a complete one ."}
{"title": "ontohub : a semantic repository for heterogeneous ontologies", "abstract": "ontohub is a repository engine for managing distributed heterogeneous ontologies . the distributed nature enables communities to share and exchange their contributions easily . the heterogeneous nature makes it possible to integrate ontologies written in various ontology languages . ontohub supports a wide range of formal logical and ontology languages , as well as various structuring and modularity constructs and inter-theory ( concept ) mappings , building on the omg-standardized dol language . ontohub repositories are organised as git repositories , thus inheriting all features of this popular version control system . moreover , ontohub is the first repository engine meeting a substantial amount of the requirements formulated in the context of the open ontology repository ( oor ) initiative , including an api for federation as well as support for logical inference and axiom selection ."}
{"title": "discovering general partial orders in event streams", "abstract": "frequent episode discovery is a popular framework for pattern discovery in event streams . an episode is a partially ordered set of nodes with each node associated with an event type . efficient ( and separate ) algorithms exist for episode discovery when the associated partial order is total ( serial episode ) and trivial ( parallel episode ) . in this paper , we propose efficient algorithms for discovering frequent episodes with general partial orders . these algorithms can be easily specialized to discover serial or parallel episodes . also , the algorithms are flexible enough to be specialized for mining in the space of certain interesting subclasses of partial orders . we point out that there is an inherent combinatorial explosion in frequent partial order mining and most importantly , frequency alone is not a sufficient measure of interestingness . we propose a new interestingness measure for general partial order episodes and a discovery method based on this measure , for filtering out uninteresting partial orders . simulations demonstrate the effectiveness of our algorithms ."}
{"title": "online tool condition monitoring based on parsimonious ensemble+", "abstract": "accurate diagnosis of tool wear in metal turning process remains an open challenge for both scientists and industrial practitioners because of inhomogeneities in workpiece material , nonstationary machining settings to suit production requirements , and nonlinear relations between measured variables and tool wear . common methodologies for tool condition monitoring still rely on batch approaches which can not cope with a fast sampling rate of metal cutting process . furthermore they require a retraining process to be completed from scratch when dealing with a new set of machining parameters . this paper presents an online tool condition monitoring approach based on parsimonious ensemble+ , pensemble+ . the unique feature of pensemble+ lies in its highly flexible principle where both ensemble structure and base-classifier structure can automatically grow and shrink on the fly based on the characteristics of data streams . moreover , the online feature selection scenario is integrated to actively sample relevant input attributes . the paper presents advancement of a newly developed ensemble learning algorithm , pensemble+ , where online active learning scenario is incorporated to reduce operator labelling effort . the ensemble merging scenario is proposed which allows reduction of ensemble complexity while retaining its diversity . experimental studies utilising real-world manufacturing data streams and comparisons with well known algorithms were carried out . furthermore , the efficacy of pensemble was examined using benchmark concept drift data streams . it has been found that pensemble+ incurs low structural complexity and results in a significant reduction of operator labelling effort ."}
{"title": "integrating fuzzy and ant colony system for fuzzy vehicle routing problem with time windows", "abstract": "in this paper fuzzy vrptw with an uncertain travel time is considered . credibility theory is used to model the problem and specifies a preference index at which it is desired that the travel times to reach the customers fall into their time windows . we propose the integration of fuzzy and ant colony system based evolutionary algorithm to solve the problem while preserving the constraints . computational results for certain benchmark problems having short and long time horizons are presented to show the effectiveness of the algorithm . comparison between different preferences indexes have been obtained to help the user in making suitable decisions ."}
{"title": "pose estimation from a single depth image for arbitrary kinematic skeletons", "abstract": "we present a method for estimating pose information from a single depth image given an arbitrary kinematic structure without prior training . for an arbitrary skeleton and depth image , an evolutionary algorithm is used to find the optimal kinematic configuration to explain the observed image . results show that our approach can correctly estimate poses of 39 and 78 degree-of-freedom models from a single depth image , even in cases of significant self-occlusion ."}
{"title": "learning and real-time classification of hand-written digits with spiking neural networks", "abstract": "we describe a novel spiking neural network ( snn ) for automated , real-time handwritten digit classification and its implementation on a gp-gpu platform . information processing within the network , from feature extraction to classification is implemented by mimicking the basic aspects of neuronal spike initiation and propagation in the brain . the feature extraction layer of the snn uses fixed synaptic weight maps to extract the key features of the image and the classifier layer uses the recently developed normad approximate gradient descent based supervised learning algorithm for spiking neural networks to adjust the synaptic weights . on the standard mnist database images of handwritten digits , our network achieves an accuracy of 99.80 % on the training set and 98.06 % on the test set , with nearly 7x fewer parameters compared to the state-of-the-art spiking networks . we further use this network in a gpu based user-interface system demonstrating real-time snn simulation to infer digits written by different users . on a test set of 500 such images , this real-time platform achieves an accuracy exceeding 97 % while making a prediction within an snn emulation time of less than 100ms ."}
{"title": "dendritic cells for anomaly detection", "abstract": "artificial immune systems , more specifically the negative selection algorithm , have previously been applied to intrusion detection . the aim of this research is to develop an intrusion detection system based on a novel concept in immunology , the danger theory . dendritic cells ( dcs ) are antigen presenting cells and key to the activation of the human signals from the host tissue and correlate these signals with proteins know as antigens . in algorithmic terms , individual dcs perform multi-sensor data fusion based on time-windows . the whole population of dcs asynchronously correlates the fused signals with a secondary data stream . the behaviour of human dcs is abstracted to form the dc algorithm ( dca ) , which is implemented using an immune inspired framework , libtissue . this system is used to detect context switching for a basic machine learning dataset and to detect outgoing portscans in real-time . experimental results show a significant difference between an outgoing portscan and normal traffic ."}
{"title": "from end-user 's requirements to web services retrieval : a semantic and intention-driven approach", "abstract": "in this paper , we present satis , a framework to derive web service specifications from end-user 's requirements in order to opera-tionalise business processes in the context of a specific application domain . the aim of satis is to provide to neuroscientists , which are not familiar with computer science , a complete solution to easily find a set of web services to implement an image processing pipeline . more precisely , our framework offers the capability to capture high-level end-user 's requirements in an iterative and incremental way and to turn them into queries to retrieve web services description . the whole framework relies on reusable and combinable elements which can be shared out by a community of users sharing some interest or problems for a given topic . in our approach , we adopt web semantic languages and models as a unified framework to deal with end-user 's requirements and web service descriptions in order to take advantage of their reasoning and traceability capabilities ."}
{"title": "bounding search space size via ( hyper ) tree decompositions", "abstract": "this paper develops a measure for bounding the performance of and/or search algorithms for solving a variety of queries over graphical models . we show how drawing a connection to the recent notion of hypertree decompositions allows to exploit determinism in the problem specification and produce tighter bounds . we demonstrate on a variety of practical problem instances that we are often able to improve upon existing bounds by several orders of magnitude ."}
{"title": "belief revision in the propositional closure of a qualitative algebra ( extended version )", "abstract": "belief revision is an operation that aims at modifying old beliefs so that they become consistent with new ones . the issue of belief revision has been studied in various formalisms , in particular , in qualitative algebras ( qas ) in which the result is a disjunction of belief bases that is not necessarily representable in a qa . this motivates the study of belief revision in formalisms extending qas , namely , their propositional closures : in such a closure , the result of belief revision belongs to the formalism . moreover , this makes it possible to define a contraction operator thanks to the harper identity . belief revision in the propositional closure of qas is studied , an algorithm for a family of revision operators is designed , and an open-source implementation is made freely available on the web . ( this is the extended version of an article originally presented at the 14th international conference on principles of knowledge representation and reasoning . )"}
{"title": "cognitive surveillance : why does it never appear among the avss conferences topics ?", "abstract": "video surveillance is a fast evolving field of research and development ( r & d ) driven by the urgent need for public security and safety ( due to the growing threats of terrorism , vandalism , and anti-social behavior ) . traditionally , surveillance systems are comprised of two components - video cameras distributed over the guarded area and human observer watching and analyzing the incoming video . explosive growth of installed cameras and limited human operator 's ability to process the delivered video content raise an urgent demand for developing surveillance systems with human like cognitive capabilities , that is - cognitive surveillance systems . the growing interest in this issue is testified by the tens of workshops , symposiums and conferences held over the world each year . the ieee international conference on advanced video and signal-based surveillance ( avss ) is certainly one of them . however , for unknown reasons , the term cognitive surveillance does never appear among its topics . as to me , the explanation for this is simple - the complexity and the indefinable nature of the term `` cognition '' . in this paper , i am trying to resolve the problem providing a novel definition of cognition equally suitable for biological as well as technological applications . i hope my humble efforts will be helpful ."}
{"title": "survey of reasoning using neural networks", "abstract": "reason and inference require process as well as memory skills by humans . neural networks are able to process tasks like image recognition ( better than humans ) but in memory aspects are still limited ( by attention mechanism , size ) . recurrent neural network ( rnn ) and it 's modified version lstm are able to solve small memory contexts , but as context becomes larger than a threshold , it is difficult to use them . the solution is to use large external memory . still , it poses many challenges like , how to train neural networks for discrete memory representation , how to describe long term dependencies in sequential data etc . most prominent neural architectures for such tasks are memory networks : inference components combined with long term memory and neural turing machines : neural networks using external memory resources . also , additional techniques like attention mechanism , end to end gradient descent on discrete memory representation are needed to support these solutions . preliminary results of above neural architectures on simple algorithms ( sorting , copying ) and question answering ( based on story , dialogs ) application are comparable with the state of the art . in this paper , i explain these architectures ( in general ) , the additional techniques used and the results of their application ."}
{"title": "exact indexing for massive time series databases under time warping distance", "abstract": "among many existing distance measures for time series data , dynamic time warping ( dtw ) distance has been recognized as one of the most accurate and suitable distance measures due to its flexibility in sequence alignment . however , dtw distance calculation is computationally intensive . especially in very large time series databases , sequential scan through the entire database is definitely impractical , even with random access that exploits some index structures since high dimensionality of time series data incurs extremely high i/o cost . more specifically , a sequential structure consumes high cpu but low i/o costs , while an index structure requires low cpu but high i/o costs . in this work , we therefore propose a novel indexed sequential structure called twist ( time warping in indexed sequential structure ) which benefits from both sequential access and index structure . when a query sequence is issued , twist calculates lower bounding distances between a group of candidate sequences and the query sequence , and then identifies the data access order in advance , hence reducing a great number of both sequential and random accesses . impressively , our indexed sequential structure achieves significant speedup in a querying process by a few orders of magnitude . in addition , our method shows superiority over existing rival methods in terms of query processing time , number of page accesses , and storage requirement with no false dismissal guaranteed ."}
{"title": "a hybrid anytime algorithm for the constructiion of causal models from sparse data", "abstract": "we present a hybrid constraint-based/bayesian algorithm for learning causal networks in the presence of sparse data . the algorithm searches the space of equivalence classes of models ( essential graphs ) using a heuristic based on conventional constraint-based techniques . each essential graph is then converted into a directed acyclic graph and scored using a bayesian scoring metric . two variants of the algorithm are developed and tested using data from randomly generated networks of sizes from 15 to 45 nodes with data sizes ranging from 250 to 2000 records . both variations are compared to , and found to consistently outperform two variations of greedy search with restarts ."}
{"title": "dual decomposition from the perspective of relax , compensate and then recover", "abstract": "relax , compensate and then recover ( rcr ) is a paradigm for approximate inference in probabilistic graphical models that has previously provided theoretical and practical insights on iterative belief propagation and some of its generalizations . in this paper , we characterize the technique of dual decomposition in the terms of rcr , viewing it as a specific way to compensate for relaxed equivalence constraints . among other insights gathered from this perspective , we propose novel heuristics for recovering relaxed equivalence constraints with the goal of incrementally tightening dual decomposition approximations , all the way to reaching exact solutions . we also show empirically that recovering equivalence constraints can sometimes tighten the corresponding approximation ( and obtaining exact results ) , without increasing much the complexity of inference ."}
{"title": "learning from noisy label distributions", "abstract": "in this paper , we consider a novel machine learning problem , that is , learning a classifier from noisy label distributions . in this problem , each instance with a feature vector belongs to at least one group . then , instead of the true label of each instance , we observe the label distribution of the instances associated with a group , where the label distribution is distorted by an unknown noise . our goals are to ( 1 ) estimate the true label of each instance , and ( 2 ) learn a classifier that predicts the true label of a new instance . we propose a probabilistic model that considers true label distributions of groups and parameters that represent the noise as hidden variables . the model can be learned based on a variational bayesian method . in numerical experiments , we show that the proposed model outperforms existing methods in terms of the estimation of the true labels of instances ."}
{"title": "mandolin : a knowledge discovery framework for the web of data", "abstract": "markov logic networks join probabilistic modeling with first-order logic and have been shown to integrate well with the semantic web foundations . while several approaches have been devised to tackle the subproblems of rule mining , grounding , and inference , no comprehensive workflow has been proposed so far . in this paper , we fill this gap by introducing a framework called mandolin , which implements a workflow for knowledge discovery specifically on rdf datasets . our framework imports knowledge from referenced graphs , creates similarity relationships among similar literals , and relies on state-of-the-art techniques for rule mining , grounding , and inference computation . we show that our best configuration scales well and achieves at least comparable results with respect to other statistical-relational-learning algorithms on link prediction ."}
{"title": "tact : a transfer actor-critic learning framework for energy saving in cellular radio access networks", "abstract": "recent works have validated the possibility of improving energy efficiency in radio access networks ( rans ) , achieved by dynamically turning on/off some base stations ( bss ) . in this paper , we extend the research over bs switching operations , which should match up with traffic load variations . instead of depending on the dynamic traffic loads which are still quite challenging to precisely forecast , we firstly formulate the traffic variations as a markov decision process . afterwards , in order to foresightedly minimize the energy consumption of rans , we design a reinforcement learning framework based bs switching operation scheme . furthermore , to avoid the underlying curse of dimensionality in reinforcement learning , a transfer actor-critic algorithm ( tact ) , which utilizes the transferred learning expertise in historical periods or neighboring regions , is proposed and provably converges . in the end , we evaluate our proposed scheme by extensive simulations under various practical configurations and show that the proposed tact algorithm contributes to a performance jumpstart and demonstrates the feasibility of significant energy efficiency improvement at the expense of tolerable delay performance ."}
{"title": "planning by rewriting", "abstract": "domain-independent planning is a hard combinatorial problem . taking into account plan quality makes the task even more difficult . this article introduces planning by rewriting ( pbr ) , a new paradigm for efficient high-quality domain-independent planning . pbr exploits declarative plan-rewriting rules and efficient local search techniques to transform an easy-to-generate , but possibly suboptimal , initial plan into a high-quality plan . in addition to addressing the issues of planning efficiency and plan quality , this framework offers a new anytime planning algorithm . we have implemented this planner and applied it to several existing domains . the experimental results show that the pbr approach provides significant savings in planning effort while generating high-quality plans ."}
{"title": "composition of credal sets via polyhedral geometry", "abstract": "recently introduced composition operator for credal sets is an analogy of such operators in probability , possibility , evidence and valuation-based systems theories . it was designed to construct multidimensional models ( in the framework of credal sets ) from a system of low- dimensional credal sets . in this paper we study its potential from the computational point of view utilizing methods of polyhedral geometry ."}
{"title": "huto : an human time ontology for semantic web applications", "abstract": "the temporal phenomena have many facets that are studied by different communities . in semantic web , large heterogeneous data are handled and produced . these data often have informal , semi-formal or formal temporal information which must be interpreted by software agents . in this paper we present human time ontology ( huto ) an rdfs ontology to annotate and represent temporal data . a major contribution of huto is the modeling of non-convex intervals giving the ability to write queries for this kind of interval . huto also incorporates normalization and reasoning rules to explicit certain information . huto also proposes an approach which associates a temporal dimension to the knowledge base content . this facilitates information retrieval by considering or not the temporal aspect ."}
{"title": "hyperprofile-based computation offloading for mobile edge networks", "abstract": "in recent studies , researchers have developed various computation offloading frameworks for bringing cloud services closer to the user via edge networks . specifically , an edge device needs to offload computationally intensive tasks because of energy and processing constraints . these constraints present the challenge of identifying which edge nodes should receive tasks to reduce overall resource consumption . we propose a unique solution to this problem which incorporates elements from knowledge-defined networking ( kdn ) to make intelligent predictions about offloading costs based on historical data . each server instance can be represented in a multidimensional feature space where each dimension corresponds to a predicted metric . we compute features for a `` hyperprofile '' and position nodes based on the predicted costs of offloading a particular task . we then perform a k-nearest neighbor ( knn ) query within the hyperprofile to select nodes for offloading computation . this paper formalizes our hyperprofile-based solution and explores the viability of using machine learning ( ml ) techniques to predict metrics useful for computation offloading . we also investigate the effects of using different distance metrics for the queries . our results show various network metrics can be modeled accurately with regression , and there are circumstances where knn queries using euclidean distance as opposed to rectilinear distance is more favorable ."}
{"title": "towards an integrated visualization of semantically enriched 3d city models : an ontology of 3d visualization techniques", "abstract": "3d city models - which represent in 3 dimensions the geometric elements of a city - are increasingly used for an intended wide range of applications . such uses are made possible by using semantically enriched 3d city models and by presenting such enriched 3d city models in a way that allows decision-making processes to be carried out from the best choices among sets of objectives , and across issues and scales . in order to help in such a decision-making process we have defined a framework to find the best visualization technique ( s ) for a set of potentially heterogeneous data that have to be visualized within the same 3d city model , in order to perform a given task in a specific context . we have chosen an ontology-based approach . this approach and the specification and use of the resulting ontology of 3d visualization techniques are described in this paper ."}
{"title": "reply with : proactive recommendation of email attachments", "abstract": "email responses often contain items-such as a file or a hyperlink to an external document-that are attached to or included inline in the body of the message . analysis of an enterprise email corpus reveals that 35 % of the time when users include these items as part of their response , the attachable item is already present in their inbox or sent folder . a modern email client can proactively retrieve relevant attachable items from the user 's past emails based on the context of the current conversation , and recommend them for inclusion , to reduce the time and effort involved in composing the response . in this paper , we propose a weakly supervised learning framework for recommending attachable items to the user . as email search systems are commonly available , we constrain the recommendation task to formulating effective search queries from the context of the conversations . the query is submitted to an existing ir system to retrieve relevant items for attachment . we also present a novel strategy for generating labels from an email corpus -- -without the need for manual annotations -- -that can be used to train and evaluate the query formulation model . in addition , we describe a deep convolutional neural network that demonstrates satisfactory performance on this query formulation task when evaluated on the publicly available avocado dataset and a proprietary dataset of internal emails obtained through an employee participation program ."}
{"title": "assessment , criticism and improvement of imprecise subjective probabilities for a medical expert system", "abstract": "three paediatric cardiologists assessed nearly 1000 imprecise subjective conditional probabilities for a simple belief network representing congenital heart disease , and the quality of the assessments has been measured using prospective data on 200 babies . quality has been assessed by a brier scoring rule , which decomposes into terms measuring lack of discrimination and reliability . the results are displayed for each of 27 diseases and 24 questions , and generally the assessments are reliable although there was a tendency for the probabilities to be too extreme . the imprecision allows the judgements to be converted to implicit samples , and by combining with the observed data the probabilities naturally adapt with experience . this appears to be a practical procedure even for reasonably large expert systems ."}
{"title": "decoding-history-based adaptive control of attention for neural machine translation", "abstract": "attention-based sequence-to-sequence model has proved successful in neural machine translation ( nmt ) . however , the attention without consideration of decoding history , which includes the past information in the decoder and the attention mechanism , often causes much repetition . to address this problem , we propose the decoding-history-based adaptive control of attention ( aca ) for the nmt model . aca learns to control the attention by keeping track of the decoding history and the current information with a memory vector , so that the model can take the translated contents and the current information into consideration . experiments on chinese-english translation and the english-vietnamese translation have demonstrated that our model significantly outperforms the strong baselines . the analysis shows that our model is capable of generating translation with less repetition and higher accuracy . the code will be available at https : //github.com/lancopku"}
{"title": "decomposition during search for propagation-based constraint solvers", "abstract": "we describe decomposition during search ( dds ) , an integration of and/or tree search into propagation-based constraint solvers . the presented search algorithm dynamically decomposes sub-problems of a constraint satisfaction problem into independent partial problems , avoiding redundant work . the paper discusses how dds interacts with key features that make propagation-based solvers successful : constraint propagation , especially for global constraints , and dynamic search heuristics . we have implemented dds for the gecode constraint programming library . two applications , solution counting in graph coloring and protein structure prediction , exemplify the benefits of dds in practice ."}
{"title": "solving composed first-order constraints from discrete-time robust control", "abstract": "this paper deals with a problem from discrete-time robust control which requires the solution of constraints over the reals that contain both universal and existential quantifiers . for solving this problem we formulate it as a program in a ( fictitious ) constraint logic programming language with explicit quantifier notation . this allows us to clarify the special structure of the problem , and to extend an algorithm for computing approximate solution sets of first-order constraints over the reals to exploit this structure . as a result we can deal with inputs that are clearly out of reach for current symbolic solvers ."}
{"title": "evaluation of the causal effect of control plans in nonrecursive structural equation models", "abstract": "when observational data is available from practical studies and a directed cyclic graph for how various variables affect each other is known based on substantive understanding of the process , we consider a problem in which a control plan of a treatment variable is conducted in order to bring a response variable close to a target value with variation reduction . we formulate an optimal control plan concerning a certain treatment variable through path coefficients in the framework of linear nonrecursive structural equation models . based on the formulation , we clarify the properties of causal effects when conducting a control plan . the results enable us to evaluate the effect of a control plan on the variance from observational data ."}
{"title": "causes and explanations : a structural-model approach . part ii : explanations", "abstract": "we propose new definitions of ( causal ) explanation , using structural equations to model counterfactuals . the definition is based on the notion of actual cause , as defined and motivated in a companion paper . essentially , an explanation is a fact that is not known for certain but , if found to be true , would constitute an actual cause of the fact to be explained , regardless of the agent 's initial uncertainty . we show that the definition handles well a number of problematic examples from the literature ."}
{"title": "customizable contraction hierarchies", "abstract": "we consider the problem of quickly computing shortest paths in weighted graphs given auxiliary data derived in an expensive preprocessing phase . by adding a fast weight-customization phase , we extend contraction hierarchies by geisberger et al to support the three-phase workflow introduced by delling et al . our customizable contraction hierarchies use nested dissection orders as suggested by bauer et al . we provide an in-depth experimental analysis on large road and game maps that clearly shows that customizable contraction hierarchies are a very practicable solution in scenarios where edge weights often change ."}
{"title": "multilingual twitter sentiment classification : the role of human annotators", "abstract": "what are the limits of automated twitter sentiment classification ? we analyze a large set of manually labeled tweets in different languages , use them as training data , and construct automated classification models . it turns out that the quality of classification models depends much more on the quality and size of training data than on the type of the model trained . experimental results indicate that there is no statistically significant difference between the performance of the top classification models . we quantify the quality of training data by applying various annotator agreement measures , and identify the weakest points of different datasets . we show that the model performance approaches the inter-annotator agreement when the size of the training set is sufficiently large . however , it is crucial to regularly monitor the self- and inter-annotator agreements since this improves the training datasets and consequently the model performance . finally , we show that there is strong evidence that humans perceive the sentiment classes ( negative , neutral , and positive ) as ordered ."}
{"title": "solving map exactly using systematic search", "abstract": "map is the problem of finding a most probable instantiation of a set of variables in a bayesian network given some evidence . unlike computing posterior probabilities , or mpe ( a special case of map ) , the time and space complexity of structural solutions for map are not only exponential in the network treewidth , but in a larger parameter known as the `` constrained '' treewidth . in practice , this means that computing map can be orders of magnitude more expensive than computing posterior probabilities or mpe . this paper introduces a new , simple upper bound on the probability of a map solution , which admits a tradeoff between the bound quality and the time needed to compute it . the bound is shown to be generally much tighter than those of other methods of comparable complexity . we use this proposed upper bound to develop a branch-and-bound search algorithm for solving map exactly . experimental results demonstrate that the search algorithm is able to solve many problems that are far beyond the reach of any structure-based method for map . for example , we show that the proposed algorithm can compute map exactly and efficiently for some networks whose constrained treewidth is more than 40 ."}
{"title": "bayesian poisson tucker decomposition for learning the structure of international relations", "abstract": "we introduce bayesian poisson tucker decomposition ( bptd ) for modeling country -- country interaction event data . these data consist of interaction events of the form `` country $ i $ took action $ a $ toward country $ j $ at time $ t $ . '' bptd discovers overlapping country -- community memberships , including the number of latent communities . in addition , it discovers directed community -- community interaction networks that are specific to `` topics '' of action types and temporal `` regimes . '' we show that bptd yields an efficient mcmc inference algorithm and achieves better predictive performance than related models . we also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations ."}
{"title": "attacker and defender counting approach for abstract argumentation", "abstract": "in dung 's abstract argumentation , arguments are either acceptable or unacceptable , given a chosen notion of acceptability . this gives a coarse way to compare arguments . in this paper , we propose a counting approach for a more fine-gained assessment to arguments by counting the number of their respective attackers and defenders based on argument graph and argument game . an argument is more acceptable if the proponent puts forward more number of defenders for it and the opponent puts forward less number of attackers against it . we show that our counting model has two well-behaved properties : normalization and convergence . then , we define a counting semantics based on this model , and investigate some general properties of the semantics ."}
{"title": "large margin boltzmann machines and large margin sigmoid belief networks", "abstract": "current statistical models for structured prediction make simplifying assumptions about the underlying output graph structure , such as assuming a low-order markov chain , because exact inference becomes intractable as the tree-width of the underlying graph increases . approximate inference algorithms , on the other hand , force one to trade off representational power with computational efficiency . in this paper , we propose two new types of probabilistic graphical models , large margin boltzmann machines ( lmbms ) and large margin sigmoid belief networks ( lmsbns ) , for structured prediction . lmsbns in particular allow a very fast inference algorithm for arbitrary graph structures that runs in polynomial time with a high probability . this probability is data-distribution dependent and is maximized in learning . the new approach overcomes the representation-efficiency trade-off in previous models and allows fast structured prediction with complicated graph structures . we present results from applying a fully connected model to multi-label scene classification and demonstrate that the proposed approach can yield significant performance gains over current state-of-the-art methods ."}
{"title": "a low-cost ethics shaping approach for designing reinforcement learning agents", "abstract": "this paper proposes a low-cost , easily realizable strategy to equip a reinforcement learning ( rl ) agent the capability of behaving ethically . our model allows the designers of rl agents to solely focus on the task to achieve , without having to worry about the implementation of multiple trivial ethical patterns to follow . based on the assumption that the majority of human behavior , regardless which goals they are achieving , is ethical , our design integrates human policy with the rl policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey ."}
{"title": "answering conjunctive queries over $ \\mathcal { el } $ knowledge bases with transitive and reflexive roles", "abstract": "answering conjunctive queries ( cqs ) over $ \\mathcal { el } $ knowledge bases ( kbs ) with complex role inclusions is pspace-hard and in pspace in certain cases ; however , if complex role inclusions are restricted to role transitivity , the tight upper complexity bound has so far been unknown . furthermore , the existing algorithms can not handle reflexive roles , and they are not practicable . finally , the problem is tractable for acyclic cqs and $ \\mathcal { elh } $ , and np-complete for unrestricted cqs and $ \\mathcal { elho } $ kbs . in this paper we complete the complexity landscape of cq answering for several important cases . in particular , we present a practicable np algorithm for answering cqs over $ \\mathcal { elho } ^s $ kbs -- -a logic containing all of owl 2 el , but with complex role inclusions restricted to role transitivity . our preliminary evaluation suggests that the algorithm can be suitable for practical use . moreover , we show that , even for a restricted class of so-called arborescent acyclic queries , cq answering over $ \\mathcal { el } $ kbs becomes np-hard in the presence of either transitive or reflexive roles . finally , we show that answering arborescent cqs over $ \\mathcal { elho } $ kbs is tractable , whereas answering acyclic cqs is np-hard ."}
