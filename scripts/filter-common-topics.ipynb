{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "* So we have all our data in the form of JSON objects (one on each line) in the file `ARXIV_TOPICS_TITLES.txt` and we load it in memory. \n",
    "* A single JSON object contains a `title`, `abstract` and `topics` which is basically all the topics for that title. \n",
    "* We have a dictionary called `title_to_topics` which stores topics corresponding to a given title.\n",
    "* then we have a frequency dictionary called `idict` for storing frequencies of individual topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idict = {}\n",
    "title_to_topics = {}\n",
    "with open(\"ARXIV_TOPICS_TITLES.txt\") as f:\n",
    "    for line in f:\n",
    "        j = json.loads(line)\n",
    "        title = \" \".join(nltk.word_tokenize(j[\"title\"].lower()))\n",
    "        title_to_topics[title] = j[\"topics\"]\n",
    "        for t in j[\"topics\"]:\n",
    "            idict[t] = idict.get(t, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning the topics\n",
    "We only keep topics which have atleast 501 frequency amongst the entire dataset. Also, we want to trim the too frequent topics, so we sort by frequency and trim the top 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [(k, v) for k, v in idict.items() if v > 500]\n",
    "topics.sort(reverse=True, key=lambda x: x[1])\n",
    "topics = topics[8:]\n",
    "topics = [k for k, _ in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Computer vision'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stringify topics and sanity print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list(map(str, topics))\n",
    "topics[:5], len(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out important topics\n",
    "* Get all the titles and abstracts from our original dataset `ARXIV-CORPUS-COMPLETE-50k.txt`.\n",
    "* Then, we iterate over all these titles that we just loaded and for every title, we first check if its there in the Semantic Scholar dataset we loaded previously. \n",
    "* For the titles that exist, we filter out topics from the important list of 76 topics we found earlier. \n",
    "* We write this data to a new file `FINAL-DATA-WITH-TOPICS.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t = []\n",
    "f_a = []\n",
    "with open(\"FINAL-DATA-WITH-TOPICS.txt\", \"w\") as f1:\n",
    "    with open(\"ARXIV-CORPUS-COMPLETE-50k.txt\") as f2:\n",
    "        i = 0\n",
    "        for line in f2:\n",
    "            if i % 2 == 0:\n",
    "                f_t.append(line.strip())\n",
    "            else:\n",
    "                f_a.append(line.strip())\n",
    "            i += 1\n",
    "\n",
    "        for t, a in zip(f_t, f_a):\n",
    "\n",
    "            t = \" \".join(nltk.word_tokenize(t.lower()))\n",
    "            a = \" \".join(nltk.word_tokenize(a.lower()))\n",
    "\n",
    "            if t in title_to_topics:\n",
    "                curr_topics = title_to_topics[t]\n",
    "                final_topics = []\n",
    "                for to in curr_topics:\n",
    "                    if to in topics:\n",
    "                        final_topics.append(to.lower())\n",
    "\n",
    "                if final_topics:\n",
    "                    val1 = {\"title\": t, \"abstract\": a, \"topics\": final_topics}\n",
    "                    val2 = {\"title\": t, \"abstract\": a}\n",
    "                    f1.write(json.dumps(val1))\n",
    "                    f1.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
