{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from eval import Evaluate\n",
    "import torch\n",
    "eval_f = Evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_samples(path, is_unk):\n",
    "    abstracts = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            j = json.loads(line.strip())\n",
    "            if not is_unk:\n",
    "                abstracts[j[\"title\"]] = j[\"abstract\"]\n",
    "            else:\n",
    "                abstracts[j[\"abstract\"]] = j[\"title\"]\n",
    "    return abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated_samples(path):\n",
    "    abstracts = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            j = json.loads(line.strip())\n",
    "            abstracts[j[\"original\"]] = j[\"generated\"]\n",
    "    return abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstract_metrics(gen, org):\n",
    "    ref = {0 : [org]}\n",
    "    cand = {0 : gen}\n",
    "    final_scores = eval_f.evaluate(live=True, cand=cand, ref=ref)\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_abstracts = get_original_samples(\"acl-dataset/acl_original.txt\", is_unk=False)\n",
    "original_unk = get_original_samples(\"acl-dataset/acl-UNK-abstracts.txt\", is_unk=True)\n",
    "generated = get_generated_samples(\"acl-dataset/WEPGen/ACL-WE-Topics-Structure-IntraAttention-Polishing-BLEU-4=2.733333.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 200\n",
      "Done 400\n",
      "Done 600\n",
      "Done 800\n",
      "Done 1000\n"
     ]
    }
   ],
   "source": [
    "ratings = []\n",
    "c = 0\n",
    "for key in generated.keys():\n",
    "    original_abs_UNK = key\n",
    "    title = original_unk[original_abs_UNK]\n",
    "    org = original_abstracts[title]\n",
    "    gen = generated[key]\n",
    "    metrics = get_abstract_metrics(gen, org)\n",
    "    ratings.append((title, org, gen, metrics.values()))\n",
    "    c += 1\n",
    "    \n",
    "    if c % 200 == 0:\n",
    "        print(\"Done {}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_samples = sorted(ratings, reverse=True, key=lambda x: sum(list(x[-1])[:4]))[:100]\n",
    "meteor_samples = sorted(ratings, reverse=True, key=lambda x: list(x[-1])[4])[:100]\n",
    "rouge_samples = sorted(ratings, reverse=True, key=lambda x: list(x[-1])[5])[:100]\n",
    "combined_samples = sorted(ratings, reverse=True, key=lambda x: sum(x[-1]))[:100]\n",
    "\n",
    "bleu_set = set([b[:3] for b in bleu_samples])\n",
    "meteor_set = set([b[:3] for b in meteor_samples])\n",
    "rouge_set = set([b[:3] for b in rouge_samples])\n",
    "combined_set = set([b[:3] for b in combined_samples])\n",
    "\n",
    "A = bleu_set.intersection(meteor_set)\n",
    "B = A.intersection(rouge_set)\n",
    "final_set = list(B.intersection(combined_set))\n",
    "len(final_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative = {}\n",
    "i = 0\n",
    "for title, _, generated in final_set:\n",
    "    qualitative[i] = (title, generated)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"acl-dataset/Qualitative/W.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(qualitative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
