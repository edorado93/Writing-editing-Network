{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word Embeddings\n",
    "* We load pretrained word embeddings. Note, these word embeddings should be trained on the same corpus or else they won't be of any use. \n",
    "* `complete-512.vec` is the 512-dim word embeddings that was trained on the 50,000 PDFs we downloaded fromt arxiv. \n",
    "* We use gensim to load these word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = \"complete-512.vec\"\n",
    "en_model = KeyedVectors.load_word2vec_format(filename)\n",
    "\n",
    "pretrained_words = set()\n",
    "for word in en_model.vocab:\n",
    "    pretrained_words.add(word)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain top-k most relevant topics\n",
    "* So, we use these pretrained word embeddings and we find average WE for a given abstract using the words for whom word embeddings are available in the pretrained model. \n",
    "* Similarly, an average word embedding for each topic of this abstract. \n",
    "* Then, we sort the topics in accordance with their cosine similarity score with the abstract. \n",
    "* Pick the top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "def filter_relevant_topics(j, k):\n",
    "    abstract = j[\"abstract\"].split()\n",
    "    a_emb = []\n",
    "    for a in abstract:\n",
    "        if a in pretrained_words:\n",
    "            a_emb.append(en_model[a])\n",
    "    a_emb = np.array(sum(a_emb) / len(a_emb)).reshape(1,-1)\n",
    "    \n",
    "    topics = j[\"topics\"]      \n",
    "    topics_emb = []\n",
    "    for t in topics:\n",
    "        t = t.split()\n",
    "        t_emb = []\n",
    "        for atom in t:\n",
    "            if atom in pretrained_words:\n",
    "                t_emb.append(en_model[atom])        \n",
    "        if t_emb:        \n",
    "            topics_emb.append((np.array(sum(t_emb) / len(t_emb)).reshape(1, -1), \" \".join(t)))\n",
    "    \n",
    "    ans = []\n",
    "    for te, topic in topics_emb:\n",
    "        ans.append((topic, cosine_similarity(a_emb, te)))\n",
    "        \n",
    "    ans = sorted(ans, key=lambda x: x[1], reverse=True)\n",
    "    if len(ans) >= k:\n",
    "        return ans[:k]\n",
    "    \n",
    "    return ans    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process entire dataset\n",
    "* For every abstract in our corpus, obtain the top-k topics by cosine similarity relevance to the average abstract embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "k = 3\n",
    "found_k = []\n",
    "with open(\"LARGE-CORPUS.txt\") as f:\n",
    "    for line in f:\n",
    "        j = json.loads(line)\n",
    "        most_relevant = filter_relevant_topics(j)\n",
    "        if len(most_relevant) == k:\n",
    "            found_k.append((j[\"title\"], [t for t,_ in most_relevant]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly viewing chosen topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('visual scene representations : contrast , scaling and occlusion',\n",
       " ['sampling ( signal processing )', 'computer vision', 'approximation'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(found_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
